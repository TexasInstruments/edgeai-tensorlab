{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a0491009/.pyenv/versions/quant/lib/python3.10/site-packages/tqdm-4.66.2-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from scipy.special import softmax\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import onnx.checker\n",
    "import onnx.helper\n",
    "import onnx.shape_inference\n",
    "from onnx import FunctionProto, ModelProto, NodeProto, TensorProto, ValueInfoProto\n",
    "\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "def generate_tokenized_input(input_str, tokenizer, max_seq_len):\n",
    "    inputs = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    fixed_shape_input = tokenizer.pad_token_id*torch.ones(1, max_seq_len).to(device=\"cuda\", dtype=torch.int32)\n",
    "    fixed_shape_input[:,-len(inputs[0]):] = inputs[0]\n",
    "\n",
    "    pad_token_id = torch.tensor(tokenizer.pad_token_id).to(device='cuda')\n",
    "\n",
    "    attention_mask_from_padding = fixed_shape_input.ne(pad_token_id).long()\n",
    "    attention_mask = attention_mask_from_padding\n",
    "\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids = position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    return fixed_shape_input, attention_mask, position_ids\n",
    "\n",
    "def push_to_tensor_alternative(tensor, x):\n",
    "    return torch.cat((tensor[1:], torch.Tensor([x]).to(device=tensor.device)))\n",
    "\n",
    "def simplify_model_and_save(filename):\n",
    "    model = onnx.load(filename)\n",
    "    model_simp, check = simplify(model)\n",
    "    out_name = filename[:-5] + '_simp.onnx'\n",
    "    onnx.save(model_simp, out_name)\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self, input_path: str) -> None:\n",
    "        # inferred_model_path = input_path[:-5] + \"_inferred.onnx\"\n",
    "        inferred_model_path = input_path\n",
    "        onnx.shape_inference.infer_shapes_path(input_path, inferred_model_path)\n",
    "        self.model = onnx.load(inferred_model_path)\n",
    "        self.graph = self.model.graph\n",
    "        self.wmap = self._build_name2obj_dict(self.graph.initializer)\n",
    "        self.vimap = self._build_name2obj_dict(self.graph.value_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_name2obj_dict(objs):  # type: ignore\n",
    "        return {obj.name: obj for obj in objs}\n",
    "\n",
    "    def _collect_new_io_core(self, original_io, io_names_to_extract):  # type: ignore\n",
    "        original_io_map = self._build_name2obj_dict(original_io)\n",
    "        original_io_names = set(original_io_map.keys())\n",
    "        s_io_names_to_extract = set(io_names_to_extract)\n",
    "        io_names_to_keep = s_io_names_to_extract & original_io_names\n",
    "        new_io_names_to_add = s_io_names_to_extract - original_io_names\n",
    "\n",
    "        new_io_tensors = []\n",
    "        for name in io_names_to_keep:\n",
    "            new_io_tensors.append(original_io_map[name])\n",
    "        for name in new_io_names_to_add:\n",
    "            # activation become input or output\n",
    "            new_io_tensors.append(self.vimap[name])\n",
    "\n",
    "        # adjust sequence\n",
    "        new_io_tensors_map = self._build_name2obj_dict(new_io_tensors)\n",
    "        return [new_io_tensors_map[name] for name in io_names_to_extract]\n",
    "\n",
    "    def _collect_new_inputs(self, names: List[str]) -> List[ValueInfoProto]:\n",
    "        return self._collect_new_io_core(self.graph.input, names)  # type: ignore\n",
    "\n",
    "    def _collect_new_outputs(self, names: List[str]) -> List[ValueInfoProto]:\n",
    "        return self._collect_new_io_core(self.graph.output, names)  # type: ignore\n",
    "\n",
    "    def _dfs_search_reachable_nodes(\n",
    "        self,\n",
    "        node_output_name: str,\n",
    "        graph_input_names: List[str],\n",
    "        reachable_nodes: List[NodeProto],\n",
    "    ) -> None:\n",
    "        if node_output_name in graph_input_names:\n",
    "            return\n",
    "        for node in self.graph.node:\n",
    "            # check output_name first to reduce run time\n",
    "            if node_output_name not in node.output:\n",
    "                continue\n",
    "            if node in reachable_nodes:\n",
    "                continue\n",
    "            reachable_nodes.append(node)\n",
    "            for name in node.input:\n",
    "                self._dfs_search_reachable_nodes(\n",
    "                    name, graph_input_names, reachable_nodes\n",
    "                )\n",
    "\n",
    "    def _collect_reachable_nodes(\n",
    "        self,\n",
    "        input_names: List[str],\n",
    "        output_names: List[str],\n",
    "    ) -> List[NodeProto]:\n",
    "        reachable_nodes = list()  # type: ignore\n",
    "        for name in output_names:\n",
    "            self._dfs_search_reachable_nodes(name, input_names, reachable_nodes)\n",
    "        # needs to be topology sorted.\n",
    "        nodes = [n for n in self.graph.node if n in reachable_nodes]\n",
    "        return nodes\n",
    "\n",
    "    def _collect_referred_local_functions(\n",
    "        self,\n",
    "        nodes,  # type: List[NodeProto]\n",
    "    ):  # type: (...) -> List[FunctionProto]\n",
    "        # a node in a model graph may refer a function.\n",
    "        # a function contains nodes, some of which may in turn refer a function.\n",
    "        # we need to find functions referred by graph nodes and\n",
    "        # by nodes used to define functions.\n",
    "        def find_referred_funcs(nodes, referred_local_functions):  # type: ignore\n",
    "            new_nodes = []  # type: List[NodeProto]\n",
    "            for node in nodes:\n",
    "                # check if the node is a function op\n",
    "                match_function = next(\n",
    "                    (\n",
    "                        f\n",
    "                        for f in self.model.functions\n",
    "                        if f.name == node.op_type and f.domain == node.domain\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "                if match_function and match_function not in referred_local_functions:\n",
    "                    referred_local_functions.append(match_function)\n",
    "                    new_nodes.extend(match_function.node)\n",
    "\n",
    "            return new_nodes\n",
    "\n",
    "        referred_local_functions = []  # type: List[FunctionProto]\n",
    "        new_nodes = find_referred_funcs(nodes, referred_local_functions)\n",
    "        while new_nodes:\n",
    "            new_nodes = find_referred_funcs(new_nodes, referred_local_functions)\n",
    "\n",
    "        return referred_local_functions\n",
    "\n",
    "    def _collect_reachable_tensors(\n",
    "        self,\n",
    "        nodes: List[NodeProto],\n",
    "    ) -> Tuple[List[TensorProto], List[ValueInfoProto]]:\n",
    "        all_tensors_name = set()\n",
    "        for node in nodes:\n",
    "            for name in node.input:\n",
    "                all_tensors_name.add(name)\n",
    "            for name in node.output:\n",
    "                all_tensors_name.add(name)\n",
    "\n",
    "        initializer = [self.wmap[t] for t in self.wmap.keys() if t in all_tensors_name]\n",
    "        value_info = [self.vimap[t] for t in self.vimap.keys() if t in all_tensors_name]\n",
    "        assert len(self.graph.sparse_initializer) == 0\n",
    "        assert len(self.graph.quantization_annotation) == 0\n",
    "        return initializer, value_info\n",
    "\n",
    "    def _make_model(\n",
    "        self,\n",
    "        nodes: List[NodeProto],\n",
    "        inputs: List[ValueInfoProto],\n",
    "        outputs: List[ValueInfoProto],\n",
    "        initializer: List[TensorProto],\n",
    "        value_info: List[ValueInfoProto],\n",
    "        local_functions: List[FunctionProto],\n",
    "    ) -> ModelProto:\n",
    "        name = \"Extracted from {\" + self.graph.name + \"}\"\n",
    "        graph = onnx.helper.make_graph(\n",
    "            nodes, name, inputs, outputs, initializer=initializer, value_info=value_info\n",
    "        )\n",
    "\n",
    "        meta = {\n",
    "            \"ir_version\": self.model.ir_version,\n",
    "            \"opset_imports\": self.model.opset_import,\n",
    "            \"producer_name\": \"onnx.utils.extract_model\",\n",
    "            \"functions\": local_functions,\n",
    "        }\n",
    "        return onnx.helper.make_model(graph, **meta)\n",
    "\n",
    "    def extract_model(\n",
    "        self,\n",
    "        input_names: List[str],\n",
    "        output_names: List[str],\n",
    "    ) -> ModelProto:\n",
    "        inputs = self._collect_new_inputs(input_names)\n",
    "        outputs = self._collect_new_outputs(output_names)\n",
    "        nodes = self._collect_reachable_nodes(input_names, output_names)\n",
    "        initializer, value_info = self._collect_reachable_tensors(nodes)\n",
    "        local_functions = self._collect_referred_local_functions(nodes)\n",
    "        model = self._make_model(\n",
    "            nodes, inputs, outputs, initializer, value_info, local_functions\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def extract_model(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    input_names: List[str],\n",
    "    output_names: List[str],\n",
    "    check_model: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Extracts sub-model from an ONNX model.\n",
    "\n",
    "    The sub-model is defined by the names of the input and output tensors *exactly*.\n",
    "\n",
    "    Note: For control-flow operators, e.g. If and Loop, the _boundary of sub-model_,\n",
    "    which is defined by the input and output tensors, should not _cut through_ the\n",
    "    subgraph that is connected to the _main graph_ as attributes of these operators.\n",
    "\n",
    "    Arguments:\n",
    "        input_path (string): The path to original ONNX model.\n",
    "        output_path (string): The path to save the extracted ONNX model.\n",
    "        input_names (list of string): The names of the input tensors that to be extracted.\n",
    "        output_names (list of string): The names of the output tensors that to be extracted.\n",
    "        check_model (bool): Whether to run model checker on the extracted model.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        raise ValueError(f\"Invalid input model path: {input_path}\")\n",
    "    if not output_path:\n",
    "        raise ValueError(\"Output model path shall not be empty!\")\n",
    "    if not output_names:\n",
    "        raise ValueError(\"Output tensor names shall not be empty!\")\n",
    "\n",
    "    onnx.checker.check_model(input_path)\n",
    "    # model = onnx.load(input_path)\n",
    "\n",
    "    e = Extractor(input_path)\n",
    "    extracted = e.extract_model(input_names, output_names)\n",
    "\n",
    "    onnx.save(extracted, output_path)\n",
    "    if check_model:\n",
    "        onnx.checker.check_model(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# checkpoint = \"Qwen/Qwen1.5-0.5B\"\n",
    "max_seq_len = 64\n",
    "model_name = checkpoint.split('/')[-1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# create onnx_files folder in the directory\n",
    "directory = \"./onnx_files\"\n",
    "model_path = \"{}/{}_{}.onnx\".format(directory, checkpoint, max_seq_len)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_names  = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, _attn_implementation=\"eager\", use_cache=False).to(device)\n",
    "# input_question = \"When?\"\n",
    "# fixed_shape_input, attention_mask, position_ids = generate_tokenized_input(input_question, tokenizer, max_seq_len)\n",
    "# torch.onnx.export(model, tuple([fixed_shape_input, attention_mask, position_ids]), model_path, input_names=input_names, do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M and are newly initialized: ['model.layers.0.input_layernorm.bias', 'model.layers.0.post_attention_layernorm.bias', 'model.layers.1.input_layernorm.bias', 'model.layers.1.post_attention_layernorm.bias', 'model.layers.10.input_layernorm.bias', 'model.layers.10.post_attention_layernorm.bias', 'model.layers.11.input_layernorm.bias', 'model.layers.11.post_attention_layernorm.bias', 'model.layers.12.input_layernorm.bias', 'model.layers.12.post_attention_layernorm.bias', 'model.layers.13.input_layernorm.bias', 'model.layers.13.post_attention_layernorm.bias', 'model.layers.14.input_layernorm.bias', 'model.layers.14.post_attention_layernorm.bias', 'model.layers.15.input_layernorm.bias', 'model.layers.15.post_attention_layernorm.bias', 'model.layers.16.input_layernorm.bias', 'model.layers.16.post_attention_layernorm.bias', 'model.layers.17.input_layernorm.bias', 'model.layers.17.post_attention_layernorm.bias', 'model.layers.18.input_layernorm.bias', 'model.layers.18.post_attention_layernorm.bias', 'model.layers.19.input_layernorm.bias', 'model.layers.19.post_attention_layernorm.bias', 'model.layers.2.input_layernorm.bias', 'model.layers.2.post_attention_layernorm.bias', 'model.layers.20.input_layernorm.bias', 'model.layers.20.post_attention_layernorm.bias', 'model.layers.21.input_layernorm.bias', 'model.layers.21.post_attention_layernorm.bias', 'model.layers.22.input_layernorm.bias', 'model.layers.22.post_attention_layernorm.bias', 'model.layers.23.input_layernorm.bias', 'model.layers.23.post_attention_layernorm.bias', 'model.layers.24.input_layernorm.bias', 'model.layers.24.post_attention_layernorm.bias', 'model.layers.25.input_layernorm.bias', 'model.layers.25.post_attention_layernorm.bias', 'model.layers.26.input_layernorm.bias', 'model.layers.26.post_attention_layernorm.bias', 'model.layers.27.input_layernorm.bias', 'model.layers.27.post_attention_layernorm.bias', 'model.layers.28.input_layernorm.bias', 'model.layers.28.post_attention_layernorm.bias', 'model.layers.29.input_layernorm.bias', 'model.layers.29.post_attention_layernorm.bias', 'model.layers.3.input_layernorm.bias', 'model.layers.3.post_attention_layernorm.bias', 'model.layers.4.input_layernorm.bias', 'model.layers.4.post_attention_layernorm.bias', 'model.layers.5.input_layernorm.bias', 'model.layers.5.post_attention_layernorm.bias', 'model.layers.6.input_layernorm.bias', 'model.layers.6.post_attention_layernorm.bias', 'model.layers.7.input_layernorm.bias', 'model.layers.7.post_attention_layernorm.bias', 'model.layers.8.input_layernorm.bias', 'model.layers.8.post_attention_layernorm.bias', 'model.layers.9.input_layernorm.bias', 'model.layers.9.post_attention_layernorm.bias', 'model.norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/a0491009/quantization/edgeai-transformers/src/transformers/models/llama/modeling_llama.py:1056: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/a0491009/quantization/edgeai-transformers/src/transformers/models/llama/modeling_llama.py:397: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "input_names  = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"logits\"]\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, _attn_implementation=\"eager\", use_cache=False).to(device)\n",
    "input_question = \"When?\"\n",
    "fixed_shape_input, attention_mask, position_ids = generate_tokenized_input(input_question, tokenizer, max_seq_len)\n",
    "torch.onnx.export(model, tuple([fixed_shape_input, attention_mask]), model_path, input_names=input_names, output_names=output_names, do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking larger onnx models into smaller parts\n",
    "from onnx_utils_extract_large_model import extract_model\n",
    "input_path = model_path\n",
    "\n",
    "# output_path = model_path[:-5] + '_gather.onnx'\n",
    "# input_names = [\"input_ids\"]\n",
    "# output_names = [\"/model/embed_tokens/Gather_output_0\"]\n",
    "\n",
    "# output_path = model_path[:-5] + '_transformer.onnx'\n",
    "# input_names = [\"/model/embed_tokens/Gather_output_0\", \"attention_mask\"]\n",
    "# output_names = [\"/model/norm/LayerNormalization_output_0\"]\n",
    "\n",
    "# output_path = model_path[:-5] + '_fc.onnx'\n",
    "\n",
    "# input_names = [\"/model/norm/LayerNormalization_output_0\"]\n",
    "# output_names = [\"logits\"]\n",
    "\n",
    "output_path = model_path[:-5] + '_transformer_fc.onnx'\n",
    "input_names = [\"/model/embed_tokens/Gather_output_0\", \"attention_mask\"]\n",
    "output_names = [\"logits\"]\n",
    "\n",
    "extract_model(input_path, output_path, input_names, output_names)\n",
    "simplify_model_and_save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options = onnxruntime.SessionOptions()\n",
    "sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "session = onnxruntime.InferenceSession(model_path, sess_options, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you doing?\n",
      "\"Is it deep enough? More than ten inches?\" said the larger vision. \"Well, of course it's deep yet.\"\n",
      "\"Is it within the horizon? Television, you say?\" said a voice within the mask. In the last\n",
      "words the wideness of the face gave\n"
     ]
    }
   ],
   "source": [
    "input_question = \"How are you doing?\"\n",
    "fixed_shape_input, attention_mask, position_ids = generate_tokenized_input(input_question, tokenizer, max_seq_len)\n",
    "inp_token_len = torch.sum(attention_mask).item()\n",
    "do_sample = True\n",
    "min_p = 0.1 # With `min_p` sampling, the output gets restricted to high-probability tokens.\n",
    "# add n-gram banning as well \n",
    "# set_seed(1)\n",
    "for i in range(max_seq_len-inp_token_len):\n",
    "    output = session.run([], \n",
    "                     {\"input_ids\" : fixed_shape_input.cpu().numpy(),\n",
    "                      \"attention_mask\" : attention_mask.cpu().numpy() ,\n",
    "                    #   \"position_ids\" : position_ids.cpu().numpy() \n",
    "                      })\n",
    "    if do_sample:\n",
    "        probs = softmax(output[0][0][-1].astype(np.float64), axis=-1)\n",
    "        if min_p:\n",
    "            min_val_filter = min_p*probs.max()\n",
    "            tokens_to_remove = probs < min_val_filter\n",
    "            output[0][0][-1][tokens_to_remove] = -float('inf')\n",
    "        next_token = torch.multinomial(torch.tensor(probs), num_samples=1).item()\n",
    "    else:\n",
    "        next_token = np.argmax(output[0][0][-1])\n",
    "    fixed_shape_input[0] = push_to_tensor_alternative(fixed_shape_input[0], next_token)\n",
    "    attention_mask[0] = push_to_tensor_alternative(attention_mask[0], 1)\n",
    "    # position_ids = push_to_tensor_alternative(position_ids, i+inp_token_len)\n",
    "    if next_token == tokenizer.eos_token_id:\n",
    "        break\n",
    "print(tokenizer.decode(fixed_shape_input[0].tolist(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
