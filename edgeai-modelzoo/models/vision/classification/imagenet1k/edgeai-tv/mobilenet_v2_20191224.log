[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_classes': 1000}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv2_tv_x1', 'dataset_name': 'image_folder_classification', 'save_path': None, 'phase': 'training', 'date': '2019-12-24_15-32-12', 'workers': 8, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fb5372d92b0>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'generate_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'freeze_bn': False, 'solver': 'sgd', 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
=> resize resolution: 256
=> crop resolution  : 224
=> creating model 'mobilenetv2_tv_x1'
[33m=> weights could not be loaded. pretrained data given is None[39m
=> feature size is:  torch.Size([1, 1280, 7, 7])
=> Resize = 256, Crop = 224, GFLOPs = 0.598988544, GMACs = 0.299494272
MobileNetV2TV(
  (classifier): Sequential(
    (0): Identity()
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (add): AddBlock(inplace=False, signed=True)
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_classes': 1000}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv2_tv_x1', 'dataset_name': 'image_folder_classification', 'save_path': None, 'phase': 'training', 'date': '2019-12-24_15-32-12', 'workers': 8, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fb5372d92b0>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'generate_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'freeze_bn': False, 'solver': 'sgd', 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
=> optimizer type   : sgd
=> learning rate    : 0.1
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 150
[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST
=> validation 0.00% of 1x98...Epoch=1/150 LR=0.02000 Time=13.900 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST
=> validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=13.900 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=3056.19 Hz, eta=0:00:00, total=0:00:00, wall=15:32 IST
** validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=13.900 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=3056.19 Hz, eta=0:00:00, total=0:00:00, wall=15:33 IST
** validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=0.593 Loss=6.908 Prec@1=0.098 Prec@5=0.500 rate=3056.19 Hz, eta=0:00:00, total=0:00:00, wall=15:33 IST
** validation 100.00% of 1x98...Epoch=1/150 LR=0.02000 Time=0.593 Loss=6.908 Prec@1=0.098 Prec@5=0.500 rate=2.22 Hz, eta=0:00:00, total=0:00:44, wall=15:33 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:33 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:33 IST
=> training   0.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=7.698 DataTime=4.690 Loss=6.924 Prec@1=0.000 Prec@5=0.195 rate=0 Hz, eta=?, total=0:00:00, wall=15:33 IST
=> training   0.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=7.698 DataTime=4.690 Loss=6.924 Prec@1=0.000 Prec@5=0.195 rate=4616.57 Hz, eta=0:00:00, total=0:00:00, wall=15:33 IST
=> training   0.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=7.698 DataTime=4.690 Loss=6.924 Prec@1=0.000 Prec@5=0.195 rate=4616.57 Hz, eta=0:00:00, total=0:00:00, wall=15:34 IST
=> training   0.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.517 DataTime=0.288 Loss=6.890 Prec@1=0.201 Prec@5=0.795 rate=4616.57 Hz, eta=0:00:00, total=0:00:00, wall=15:34 IST
=> training   4.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.517 DataTime=0.288 Loss=6.890 Prec@1=0.201 Prec@5=0.795 rate=2.26 Hz, eta=0:17:42, total=0:00:44, wall=15:34 IST
=> training   4.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.517 DataTime=0.288 Loss=6.890 Prec@1=0.201 Prec@5=0.795 rate=2.26 Hz, eta=0:17:42, total=0:00:44, wall=15:34 IST
=> training   4.04% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.498 DataTime=0.284 Loss=6.817 Prec@1=0.352 Prec@5=1.391 rate=2.26 Hz, eta=0:17:42, total=0:00:44, wall=15:34 IST
=> training   8.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.498 DataTime=0.284 Loss=6.817 Prec@1=0.352 Prec@5=1.391 rate=2.17 Hz, eta=0:17:39, total=0:01:32, wall=15:34 IST
=> training   8.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.498 DataTime=0.284 Loss=6.817 Prec@1=0.352 Prec@5=1.391 rate=2.17 Hz, eta=0:17:39, total=0:01:32, wall=15:35 IST
=> training   8.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.489 DataTime=0.282 Loss=6.707 Prec@1=0.548 Prec@5=2.168 rate=2.17 Hz, eta=0:17:39, total=0:01:32, wall=15:35 IST
=> training   12.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.489 DataTime=0.282 Loss=6.707 Prec@1=0.548 Prec@5=2.168 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=15:35 IST
=> training   12.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.489 DataTime=0.282 Loss=6.707 Prec@1=0.548 Prec@5=2.168 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=15:36 IST
=> training   12.03% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.486 DataTime=0.281 Loss=6.597 Prec@1=0.773 Prec@5=2.924 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=15:36 IST
=> training   16.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.486 DataTime=0.281 Loss=6.597 Prec@1=0.773 Prec@5=2.924 rate=2.14 Hz, eta=0:16:21, total=0:03:07, wall=15:36 IST
=> training   16.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.486 DataTime=0.281 Loss=6.597 Prec@1=0.773 Prec@5=2.924 rate=2.14 Hz, eta=0:16:21, total=0:03:07, wall=15:37 IST
=> training   16.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.484 DataTime=0.281 Loss=6.497 Prec@1=0.998 Prec@5=3.670 rate=2.14 Hz, eta=0:16:21, total=0:03:07, wall=15:37 IST
=> training   20.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.484 DataTime=0.281 Loss=6.497 Prec@1=0.998 Prec@5=3.670 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=15:37 IST
=> training   20.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.484 DataTime=0.281 Loss=6.497 Prec@1=0.998 Prec@5=3.670 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=15:38 IST
=> training   20.02% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.483 DataTime=0.282 Loss=6.409 Prec@1=1.219 Prec@5=4.403 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=15:38 IST
=> training   24.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.483 DataTime=0.282 Loss=6.409 Prec@1=1.219 Prec@5=4.403 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=15:38 IST
=> training   24.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.483 DataTime=0.282 Loss=6.409 Prec@1=1.219 Prec@5=4.403 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=15:38 IST
=> training   24.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.280 Loss=6.327 Prec@1=1.465 Prec@5=5.172 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=15:38 IST
=> training   28.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.280 Loss=6.327 Prec@1=1.465 Prec@5=5.172 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=15:38 IST
=> training   28.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.280 Loss=6.327 Prec@1=1.465 Prec@5=5.172 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=15:39 IST
=> training   28.01% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.250 Prec@1=1.747 Prec@5=6.005 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=15:39 IST
=> training   32.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.250 Prec@1=1.747 Prec@5=6.005 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=15:39 IST
=> training   32.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.250 Prec@1=1.747 Prec@5=6.005 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=15:40 IST
=> training   32.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.281 Loss=6.179 Prec@1=2.006 Prec@5=6.774 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=15:40 IST
=> training   36.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.281 Loss=6.179 Prec@1=2.006 Prec@5=6.774 rate=2.12 Hz, eta=0:12:37, total=0:07:05, wall=15:40 IST
=> training   36.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.481 DataTime=0.281 Loss=6.179 Prec@1=2.006 Prec@5=6.774 rate=2.12 Hz, eta=0:12:37, total=0:07:05, wall=15:41 IST
=> training   36.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.113 Prec@1=2.270 Prec@5=7.552 rate=2.12 Hz, eta=0:12:37, total=0:07:05, wall=15:41 IST
=> training   39.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.113 Prec@1=2.270 Prec@5=7.552 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=15:41 IST
=> training   39.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.280 Loss=6.113 Prec@1=2.270 Prec@5=7.552 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=15:42 IST
=> training   39.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.281 Loss=6.050 Prec@1=2.548 Prec@5=8.337 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=15:42 IST
=> training   43.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.281 Loss=6.050 Prec@1=2.548 Prec@5=8.337 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=15:42 IST
=> training   43.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.480 DataTime=0.281 Loss=6.050 Prec@1=2.548 Prec@5=8.337 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=15:42 IST
=> training   43.99% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.280 Loss=5.991 Prec@1=2.815 Prec@5=9.066 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=15:42 IST
=> training   47.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.280 Loss=5.991 Prec@1=2.815 Prec@5=9.066 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=15:42 IST
=> training   47.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.280 Loss=5.991 Prec@1=2.815 Prec@5=9.066 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=15:43 IST
=> training   47.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.935 Prec@1=3.104 Prec@5=9.788 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=15:43 IST
=> training   51.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.935 Prec@1=3.104 Prec@5=9.788 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=15:43 IST
=> training   51.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.935 Prec@1=3.104 Prec@5=9.788 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=15:44 IST
=> training   51.98% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.881 Prec@1=3.383 Prec@5=10.514 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=15:44 IST
=> training   55.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.881 Prec@1=3.383 Prec@5=10.514 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=15:44 IST
=> training   55.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.881 Prec@1=3.383 Prec@5=10.514 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=15:45 IST
=> training   55.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.831 Prec@1=3.650 Prec@5=11.212 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=15:45 IST
=> training   59.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.831 Prec@1=3.650 Prec@5=11.212 rate=2.11 Hz, eta=0:07:55, total=0:11:51, wall=15:45 IST
=> training   59.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.831 Prec@1=3.650 Prec@5=11.212 rate=2.11 Hz, eta=0:07:55, total=0:11:51, wall=15:46 IST
=> training   59.97% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.783 Prec@1=3.929 Prec@5=11.892 rate=2.11 Hz, eta=0:07:55, total=0:11:51, wall=15:46 IST
=> training   63.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.783 Prec@1=3.929 Prec@5=11.892 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=15:46 IST
=> training   63.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.281 Loss=5.783 Prec@1=3.929 Prec@5=11.892 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=15:46 IST
=> training   63.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.736 Prec@1=4.219 Prec@5=12.577 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=15:46 IST
=> training   67.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.736 Prec@1=4.219 Prec@5=12.577 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=15:46 IST
=> training   67.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.736 Prec@1=4.219 Prec@5=12.577 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=15:47 IST
=> training   67.96% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.690 Prec@1=4.512 Prec@5=13.247 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=15:47 IST
=> training   71.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.690 Prec@1=4.512 Prec@5=13.247 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=15:47 IST
=> training   71.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.690 Prec@1=4.512 Prec@5=13.247 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=15:48 IST
=> training   71.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.280 Loss=5.647 Prec@1=4.786 Prec@5=13.890 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=15:48 IST
=> training   75.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.280 Loss=5.647 Prec@1=4.786 Prec@5=13.890 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=15:48 IST
=> training   75.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.280 Loss=5.647 Prec@1=4.786 Prec@5=13.890 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=15:49 IST
=> training   75.95% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.605 Prec@1=5.070 Prec@5=14.519 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=15:49 IST
=> training   79.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.605 Prec@1=5.070 Prec@5=14.519 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=15:49 IST
=> training   79.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.605 Prec@1=5.070 Prec@5=14.519 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=15:50 IST
=> training   79.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.566 Prec@1=5.342 Prec@5=15.141 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=15:50 IST
=> training   83.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.566 Prec@1=5.342 Prec@5=15.141 rate=2.11 Hz, eta=0:03:10, total=0:16:36, wall=15:50 IST
=> training   83.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.478 DataTime=0.281 Loss=5.566 Prec@1=5.342 Prec@5=15.141 rate=2.11 Hz, eta=0:03:10, total=0:16:36, wall=15:50 IST
=> training   83.94% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.527 Prec@1=5.611 Prec@5=15.739 rate=2.11 Hz, eta=0:03:10, total=0:16:36, wall=15:50 IST
=> training   87.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.527 Prec@1=5.611 Prec@5=15.739 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=15:50 IST
=> training   87.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.527 Prec@1=5.611 Prec@5=15.739 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=15:51 IST
=> training   87.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.489 Prec@1=5.896 Prec@5=16.349 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=15:51 IST
=> training   91.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.489 Prec@1=5.896 Prec@5=16.349 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=15:51 IST
=> training   91.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.489 Prec@1=5.896 Prec@5=16.349 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=15:52 IST
=> training   91.93% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.452 Prec@1=6.172 Prec@5=16.945 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=15:52 IST
=> training   95.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.452 Prec@1=6.172 Prec@5=16.945 rate=2.11 Hz, eta=0:00:48, total=0:18:58, wall=15:52 IST
=> training   95.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.452 Prec@1=6.172 Prec@5=16.945 rate=2.11 Hz, eta=0:00:48, total=0:18:58, wall=15:53 IST
=> training   95.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.416 Prec@1=6.446 Prec@5=17.518 rate=2.11 Hz, eta=0:00:48, total=0:18:58, wall=15:53 IST
=> training   99.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.416 Prec@1=6.446 Prec@5=17.518 rate=2.11 Hz, eta=0:00:00, total=0:19:45, wall=15:53 IST
=> training   99.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.477 DataTime=0.280 Loss=5.416 Prec@1=6.446 Prec@5=17.518 rate=2.11 Hz, eta=0:00:00, total=0:19:45, wall=15:53 IST
=> training   99.92% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.280 Loss=5.415 Prec@1=6.449 Prec@5=17.526 rate=2.11 Hz, eta=0:00:00, total=0:19:45, wall=15:53 IST
=> training   100.00% of 1x2503...Epoch=1/150 LR=0.02000 Time=0.479 DataTime=0.280 Loss=5.415 Prec@1=6.449 Prec@5=17.526 rate=2.10 Hz, eta=0:00:00, total=0:19:50, wall=15:53 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:53 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:53 IST
=> validation 0.00% of 1x98...Epoch=1/150 LR=0.02000 Time=6.267 Loss=3.760 Prec@1=20.508 Prec@5=52.734 rate=0 Hz, eta=?, total=0:00:00, wall=15:53 IST
=> validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=6.267 Loss=3.760 Prec@1=20.508 Prec@5=52.734 rate=12364.91 Hz, eta=0:00:00, total=0:00:00, wall=15:53 IST
** validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=6.267 Loss=3.760 Prec@1=20.508 Prec@5=52.734 rate=12364.91 Hz, eta=0:00:00, total=0:00:00, wall=15:54 IST
** validation 1.02% of 1x98...Epoch=1/150 LR=0.02000 Time=0.547 Loss=4.478 Prec@1=13.680 Prec@5=32.380 rate=12364.91 Hz, eta=0:00:00, total=0:00:00, wall=15:54 IST
** validation 100.00% of 1x98...Epoch=1/150 LR=0.02000 Time=0.547 Loss=4.478 Prec@1=13.680 Prec@5=32.380 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=15:54 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:54 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:54 IST
=> training   0.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=4.995 DataTime=4.207 Loss=4.574 Prec@1=13.281 Prec@5=35.156 rate=0 Hz, eta=?, total=0:00:00, wall=15:54 IST
=> training   0.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=4.995 DataTime=4.207 Loss=4.574 Prec@1=13.281 Prec@5=35.156 rate=8195.51 Hz, eta=0:00:00, total=0:00:00, wall=15:54 IST
=> training   0.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=4.995 DataTime=4.207 Loss=4.574 Prec@1=13.281 Prec@5=35.156 rate=8195.51 Hz, eta=0:00:00, total=0:00:00, wall=15:55 IST
=> training   0.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.515 DataTime=0.315 Loss=4.645 Prec@1=12.024 Prec@5=29.595 rate=8195.51 Hz, eta=0:00:00, total=0:00:00, wall=15:55 IST
=> training   4.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.515 DataTime=0.315 Loss=4.645 Prec@1=12.024 Prec@5=29.595 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=15:55 IST
=> training   4.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.515 DataTime=0.315 Loss=4.645 Prec@1=12.024 Prec@5=29.595 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=15:55 IST
=> training   4.04% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.491 DataTime=0.294 Loss=4.601 Prec@1=12.571 Prec@5=30.461 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=15:55 IST
=> training   8.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.491 DataTime=0.294 Loss=4.601 Prec@1=12.571 Prec@5=30.461 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=15:55 IST
=> training   8.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.491 DataTime=0.294 Loss=4.601 Prec@1=12.571 Prec@5=30.461 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=15:56 IST
=> training   8.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.488 DataTime=0.291 Loss=4.564 Prec@1=12.982 Prec@5=31.232 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=15:56 IST
=> training   12.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.488 DataTime=0.291 Loss=4.564 Prec@1=12.982 Prec@5=31.232 rate=2.12 Hz, eta=0:17:17, total=0:02:21, wall=15:56 IST
=> training   12.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.488 DataTime=0.291 Loss=4.564 Prec@1=12.982 Prec@5=31.232 rate=2.12 Hz, eta=0:17:17, total=0:02:21, wall=15:57 IST
=> training   12.03% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.482 DataTime=0.286 Loss=4.528 Prec@1=13.328 Prec@5=31.875 rate=2.12 Hz, eta=0:17:17, total=0:02:21, wall=15:57 IST
=> training   16.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.482 DataTime=0.286 Loss=4.528 Prec@1=13.328 Prec@5=31.875 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=15:57 IST
=> training   16.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.482 DataTime=0.286 Loss=4.528 Prec@1=13.328 Prec@5=31.875 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=15:58 IST
=> training   16.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.484 DataTime=0.287 Loss=4.494 Prec@1=13.669 Prec@5=32.479 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=15:58 IST
=> training   20.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.484 DataTime=0.287 Loss=4.494 Prec@1=13.669 Prec@5=32.479 rate=2.11 Hz, eta=0:15:48, total=0:03:57, wall=15:58 IST
=> training   20.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.484 DataTime=0.287 Loss=4.494 Prec@1=13.669 Prec@5=32.479 rate=2.11 Hz, eta=0:15:48, total=0:03:57, wall=15:59 IST
=> training   20.02% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.461 Prec@1=14.058 Prec@5=33.116 rate=2.11 Hz, eta=0:15:48, total=0:03:57, wall=15:59 IST
=> training   24.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.461 Prec@1=14.058 Prec@5=33.116 rate=2.12 Hz, eta=0:14:56, total=0:04:43, wall=15:59 IST
=> training   24.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.461 Prec@1=14.058 Prec@5=33.116 rate=2.12 Hz, eta=0:14:56, total=0:04:43, wall=15:59 IST
=> training   24.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.428 Prec@1=14.502 Prec@5=33.757 rate=2.12 Hz, eta=0:14:56, total=0:04:43, wall=15:59 IST
=> training   28.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.428 Prec@1=14.502 Prec@5=33.757 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=15:59 IST
=> training   28.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.283 Loss=4.428 Prec@1=14.502 Prec@5=33.757 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=16:00 IST
=> training   28.01% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.397 Prec@1=14.873 Prec@5=34.362 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=16:00 IST
=> training   32.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.397 Prec@1=14.873 Prec@5=34.362 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=16:00 IST
=> training   32.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.397 Prec@1=14.873 Prec@5=34.362 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=16:01 IST
=> training   32.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.367 Prec@1=15.225 Prec@5=34.945 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=16:01 IST
=> training   36.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.367 Prec@1=15.225 Prec@5=34.945 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=16:01 IST
=> training   36.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.367 Prec@1=15.225 Prec@5=34.945 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=16:02 IST
=> training   36.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.340 Prec@1=15.558 Prec@5=35.474 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=16:02 IST
=> training   39.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.340 Prec@1=15.558 Prec@5=35.474 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=16:02 IST
=> training   39.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.340 Prec@1=15.558 Prec@5=35.474 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=16:03 IST
=> training   39.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.314 Prec@1=15.891 Prec@5=36.001 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=16:03 IST
=> training   43.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.314 Prec@1=15.891 Prec@5=36.001 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=16:03 IST
=> training   43.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.314 Prec@1=15.891 Prec@5=36.001 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=16:03 IST
=> training   43.99% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.286 Prec@1=16.270 Prec@5=36.567 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=16:03 IST
=> training   47.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.286 Prec@1=16.270 Prec@5=36.567 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:03 IST
=> training   47.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.286 Prec@1=16.270 Prec@5=36.567 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:04 IST
=> training   47.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.260 Prec@1=16.601 Prec@5=37.070 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:04 IST
=> training   51.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.260 Prec@1=16.601 Prec@5=37.070 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=16:04 IST
=> training   51.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.260 Prec@1=16.601 Prec@5=37.070 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=16:05 IST
=> training   51.98% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.235 Prec@1=16.927 Prec@5=37.553 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=16:05 IST
=> training   55.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.235 Prec@1=16.927 Prec@5=37.553 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=16:05 IST
=> training   55.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.235 Prec@1=16.927 Prec@5=37.553 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=16:06 IST
=> training   55.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.211 Prec@1=17.247 Prec@5=38.040 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=16:06 IST
=> training   59.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.211 Prec@1=17.247 Prec@5=38.040 rate=2.10 Hz, eta=0:07:57, total=0:11:56, wall=16:06 IST
=> training   59.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.211 Prec@1=17.247 Prec@5=38.040 rate=2.10 Hz, eta=0:07:57, total=0:11:56, wall=16:07 IST
=> training   59.97% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.187 Prec@1=17.572 Prec@5=38.500 rate=2.10 Hz, eta=0:07:57, total=0:11:56, wall=16:07 IST
=> training   63.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.187 Prec@1=17.572 Prec@5=38.500 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=16:07 IST
=> training   63.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.187 Prec@1=17.572 Prec@5=38.500 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=16:07 IST
=> training   63.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.164 Prec@1=17.889 Prec@5=38.952 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=16:07 IST
=> training   67.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.164 Prec@1=17.889 Prec@5=38.952 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=16:07 IST
=> training   67.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.164 Prec@1=17.889 Prec@5=38.952 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=16:08 IST
=> training   67.96% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.142 Prec@1=18.173 Prec@5=39.379 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=16:08 IST
=> training   71.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.142 Prec@1=18.173 Prec@5=39.379 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=16:08 IST
=> training   71.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.481 DataTime=0.285 Loss=4.142 Prec@1=18.173 Prec@5=39.379 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=16:09 IST
=> training   71.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.120 Prec@1=18.462 Prec@5=39.805 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=16:09 IST
=> training   75.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.120 Prec@1=18.462 Prec@5=39.805 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=16:09 IST
=> training   75.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.285 Loss=4.120 Prec@1=18.462 Prec@5=39.805 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=16:10 IST
=> training   75.95% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.099 Prec@1=18.754 Prec@5=40.227 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=16:10 IST
=> training   79.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.099 Prec@1=18.754 Prec@5=40.227 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=16:10 IST
=> training   79.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.099 Prec@1=18.754 Prec@5=40.227 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=16:10 IST
=> training   79.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.078 Prec@1=19.055 Prec@5=40.638 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=16:11 IST
=> training   83.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.078 Prec@1=19.055 Prec@5=40.638 rate=2.09 Hz, eta=0:03:11, total=0:16:43, wall=16:11 IST
=> training   83.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.078 Prec@1=19.055 Prec@5=40.638 rate=2.09 Hz, eta=0:03:11, total=0:16:43, wall=16:11 IST
=> training   83.94% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.058 Prec@1=19.337 Prec@5=41.026 rate=2.09 Hz, eta=0:03:11, total=0:16:43, wall=16:11 IST
=> training   87.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.058 Prec@1=19.337 Prec@5=41.026 rate=2.10 Hz, eta=0:02:24, total=0:17:30, wall=16:11 IST
=> training   87.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.480 DataTime=0.284 Loss=4.058 Prec@1=19.337 Prec@5=41.026 rate=2.10 Hz, eta=0:02:24, total=0:17:30, wall=16:12 IST
=> training   87.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.038 Prec@1=19.616 Prec@5=41.416 rate=2.10 Hz, eta=0:02:24, total=0:17:30, wall=16:12 IST
=> training   91.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.038 Prec@1=19.616 Prec@5=41.416 rate=2.10 Hz, eta=0:01:36, total=0:18:18, wall=16:12 IST
=> training   91.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.038 Prec@1=19.616 Prec@5=41.416 rate=2.10 Hz, eta=0:01:36, total=0:18:18, wall=16:13 IST
=> training   91.93% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.019 Prec@1=19.879 Prec@5=41.792 rate=2.10 Hz, eta=0:01:36, total=0:18:18, wall=16:13 IST
=> training   95.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.019 Prec@1=19.879 Prec@5=41.792 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=16:13 IST
=> training   95.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.019 Prec@1=19.879 Prec@5=41.792 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=16:14 IST
=> training   95.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.000 Prec@1=20.133 Prec@5=42.156 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=16:14 IST
=> training   99.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.000 Prec@1=20.133 Prec@5=42.156 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=16:14 IST
=> training   99.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.000 Prec@1=20.133 Prec@5=42.156 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=16:14 IST
=> training   99.92% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.000 Prec@1=20.137 Prec@5=42.162 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=16:14 IST
=> training   100.00% of 1x2503...Epoch=2/150 LR=0.04000 Time=0.479 DataTime=0.284 Loss=4.000 Prec@1=20.137 Prec@5=42.162 rate=2.10 Hz, eta=0:00:00, total=0:19:53, wall=16:14 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> validation 0.00% of 1x98...Epoch=2/150 LR=0.04000 Time=6.850 Loss=2.895 Prec@1=34.961 Prec@5=65.625 rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> validation 1.02% of 1x98...Epoch=2/150 LR=0.04000 Time=6.850 Loss=2.895 Prec@1=34.961 Prec@5=65.625 rate=7955.39 Hz, eta=0:00:00, total=0:00:00, wall=16:14 IST
** validation 1.02% of 1x98...Epoch=2/150 LR=0.04000 Time=6.850 Loss=2.895 Prec@1=34.961 Prec@5=65.625 rate=7955.39 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
** validation 1.02% of 1x98...Epoch=2/150 LR=0.04000 Time=0.553 Loss=3.467 Prec@1=26.892 Prec@5=52.168 rate=7955.39 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
** validation 100.00% of 1x98...Epoch=2/150 LR=0.04000 Time=0.553 Loss=3.467 Prec@1=26.892 Prec@5=52.168 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=16:15 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> training   0.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=4.597 DataTime=4.343 Loss=3.606 Prec@1=25.391 Prec@5=51.172 rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> training   0.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=4.597 DataTime=4.343 Loss=3.606 Prec@1=25.391 Prec@5=51.172 rate=4079.48 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
=> training   0.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=4.597 DataTime=4.343 Loss=3.606 Prec@1=25.391 Prec@5=51.172 rate=4079.48 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
=> training   0.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.506 DataTime=0.311 Loss=3.618 Prec@1=25.050 Prec@5=49.319 rate=4079.48 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
=> training   4.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.506 DataTime=0.311 Loss=3.618 Prec@1=25.050 Prec@5=49.319 rate=2.17 Hz, eta=0:18:25, total=0:00:46, wall=16:15 IST
=> training   4.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.506 DataTime=0.311 Loss=3.618 Prec@1=25.050 Prec@5=49.319 rate=2.17 Hz, eta=0:18:25, total=0:00:46, wall=16:16 IST
=> training   4.04% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.493 DataTime=0.297 Loss=3.604 Prec@1=25.437 Prec@5=49.777 rate=2.17 Hz, eta=0:18:25, total=0:00:46, wall=16:16 IST
=> training   8.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.493 DataTime=0.297 Loss=3.604 Prec@1=25.437 Prec@5=49.777 rate=2.12 Hz, eta=0:18:03, total=0:01:34, wall=16:16 IST
=> training   8.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.493 DataTime=0.297 Loss=3.604 Prec@1=25.437 Prec@5=49.777 rate=2.12 Hz, eta=0:18:03, total=0:01:34, wall=16:17 IST
=> training   8.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.490 DataTime=0.294 Loss=3.595 Prec@1=25.609 Prec@5=49.981 rate=2.12 Hz, eta=0:18:03, total=0:01:34, wall=16:17 IST
=> training   12.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.490 DataTime=0.294 Loss=3.595 Prec@1=25.609 Prec@5=49.981 rate=2.11 Hz, eta=0:17:25, total=0:02:22, wall=16:17 IST
=> training   12.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.490 DataTime=0.294 Loss=3.595 Prec@1=25.609 Prec@5=49.981 rate=2.11 Hz, eta=0:17:25, total=0:02:22, wall=16:18 IST
=> training   12.03% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.486 DataTime=0.290 Loss=3.575 Prec@1=25.871 Prec@5=50.372 rate=2.11 Hz, eta=0:17:25, total=0:02:22, wall=16:18 IST
=> training   16.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.486 DataTime=0.290 Loss=3.575 Prec@1=25.871 Prec@5=50.372 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=16:18 IST
=> training   16.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.486 DataTime=0.290 Loss=3.575 Prec@1=25.871 Prec@5=50.372 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=16:19 IST
=> training   16.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.561 Prec@1=26.108 Prec@5=50.627 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=16:19 IST
=> training   20.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.561 Prec@1=26.108 Prec@5=50.627 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=16:19 IST
=> training   20.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.561 Prec@1=26.108 Prec@5=50.627 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=16:19 IST
=> training   20.02% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.547 Prec@1=26.323 Prec@5=50.916 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=16:19 IST
=> training   24.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.547 Prec@1=26.323 Prec@5=50.916 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=16:19 IST
=> training   24.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.484 DataTime=0.289 Loss=3.547 Prec@1=26.323 Prec@5=50.916 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=16:20 IST
=> training   24.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.287 Loss=3.531 Prec@1=26.579 Prec@5=51.228 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=16:20 IST
=> training   28.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.287 Loss=3.531 Prec@1=26.579 Prec@5=51.228 rate=2.10 Hz, eta=0:14:17, total=0:05:33, wall=16:20 IST
=> training   28.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.287 Loss=3.531 Prec@1=26.579 Prec@5=51.228 rate=2.10 Hz, eta=0:14:17, total=0:05:33, wall=16:21 IST
=> training   28.01% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.286 Loss=3.515 Prec@1=26.843 Prec@5=51.537 rate=2.10 Hz, eta=0:14:17, total=0:05:33, wall=16:21 IST
=> training   32.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.286 Loss=3.515 Prec@1=26.843 Prec@5=51.537 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=16:21 IST
=> training   32.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.482 DataTime=0.286 Loss=3.515 Prec@1=26.843 Prec@5=51.537 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=16:22 IST
=> training   32.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.500 Prec@1=27.063 Prec@5=51.797 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=16:22 IST
=> training   36.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.500 Prec@1=27.063 Prec@5=51.797 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=16:22 IST
=> training   36.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.500 Prec@1=27.063 Prec@5=51.797 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=16:23 IST
=> training   36.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.487 Prec@1=27.288 Prec@5=52.051 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=16:23 IST
=> training   39.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.487 Prec@1=27.288 Prec@5=52.051 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=16:23 IST
=> training   39.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.487 Prec@1=27.288 Prec@5=52.051 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=16:23 IST
=> training   39.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.472 Prec@1=27.515 Prec@5=52.324 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=16:23 IST
=> training   43.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.472 Prec@1=27.515 Prec@5=52.324 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=16:23 IST
=> training   43.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.472 Prec@1=27.515 Prec@5=52.324 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=16:24 IST
=> training   43.99% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.460 Prec@1=27.741 Prec@5=52.563 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=16:24 IST
=> training   47.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.460 Prec@1=27.741 Prec@5=52.563 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:24 IST
=> training   47.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.460 Prec@1=27.741 Prec@5=52.563 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:25 IST
=> training   47.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.447 Prec@1=27.941 Prec@5=52.802 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=16:25 IST
=> training   51.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.447 Prec@1=27.941 Prec@5=52.802 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=16:25 IST
=> training   51.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.447 Prec@1=27.941 Prec@5=52.802 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=16:26 IST
=> training   51.98% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.434 Prec@1=28.164 Prec@5=53.061 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=16:26 IST
=> training   55.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.434 Prec@1=28.164 Prec@5=53.061 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=16:26 IST
=> training   55.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.285 Loss=3.434 Prec@1=28.164 Prec@5=53.061 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=16:27 IST
=> training   55.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.286 Loss=3.420 Prec@1=28.366 Prec@5=53.320 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=16:27 IST
=> training   59.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.286 Loss=3.420 Prec@1=28.366 Prec@5=53.320 rate=2.09 Hz, eta=0:07:59, total=0:11:57, wall=16:27 IST
=> training   59.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.286 Loss=3.420 Prec@1=28.366 Prec@5=53.320 rate=2.09 Hz, eta=0:07:59, total=0:11:57, wall=16:27 IST
=> training   59.97% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.408 Prec@1=28.565 Prec@5=53.543 rate=2.09 Hz, eta=0:07:59, total=0:11:57, wall=16:27 IST
=> training   63.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.408 Prec@1=28.565 Prec@5=53.543 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=16:27 IST
=> training   63.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.481 DataTime=0.285 Loss=3.408 Prec@1=28.565 Prec@5=53.543 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=16:28 IST
=> training   63.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.395 Prec@1=28.774 Prec@5=53.784 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=16:28 IST
=> training   67.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.395 Prec@1=28.774 Prec@5=53.784 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=16:28 IST
=> training   67.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.480 DataTime=0.284 Loss=3.395 Prec@1=28.774 Prec@5=53.784 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=16:29 IST
=> training   67.96% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.479 DataTime=0.283 Loss=3.383 Prec@1=28.969 Prec@5=54.011 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=16:29 IST
=> training   71.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.479 DataTime=0.283 Loss=3.383 Prec@1=28.969 Prec@5=54.011 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=16:29 IST
=> training   71.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.479 DataTime=0.283 Loss=3.383 Prec@1=28.969 Prec@5=54.011 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=16:30 IST
=> training   71.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.478 DataTime=0.282 Loss=3.372 Prec@1=29.170 Prec@5=54.232 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=16:30 IST
=> training   75.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.478 DataTime=0.282 Loss=3.372 Prec@1=29.170 Prec@5=54.232 rate=2.10 Hz, eta=0:04:46, total=0:15:03, wall=16:30 IST
=> training   75.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.478 DataTime=0.282 Loss=3.372 Prec@1=29.170 Prec@5=54.232 rate=2.10 Hz, eta=0:04:46, total=0:15:03, wall=16:31 IST
=> training   75.95% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.360 Prec@1=29.355 Prec@5=54.448 rate=2.10 Hz, eta=0:04:46, total=0:15:03, wall=16:31 IST
=> training   79.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.360 Prec@1=29.355 Prec@5=54.448 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=16:31 IST
=> training   79.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.360 Prec@1=29.355 Prec@5=54.448 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=16:31 IST
=> training   79.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.348 Prec@1=29.553 Prec@5=54.671 rate=2.11 Hz, eta=0:03:58, total=0:15:49, wall=16:31 IST
=> training   83.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.348 Prec@1=29.553 Prec@5=54.671 rate=2.11 Hz, eta=0:03:10, total=0:16:37, wall=16:31 IST
=> training   83.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.477 DataTime=0.281 Loss=3.348 Prec@1=29.553 Prec@5=54.671 rate=2.11 Hz, eta=0:03:10, total=0:16:37, wall=16:32 IST
=> training   83.94% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.336 Prec@1=29.745 Prec@5=54.886 rate=2.11 Hz, eta=0:03:10, total=0:16:37, wall=16:32 IST
=> training   87.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.336 Prec@1=29.745 Prec@5=54.886 rate=2.11 Hz, eta=0:02:23, total=0:17:23, wall=16:32 IST
=> training   87.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.336 Prec@1=29.745 Prec@5=54.886 rate=2.11 Hz, eta=0:02:23, total=0:17:23, wall=16:33 IST
=> training   87.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.325 Prec@1=29.938 Prec@5=55.092 rate=2.11 Hz, eta=0:02:23, total=0:17:23, wall=16:33 IST
=> training   91.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.325 Prec@1=29.938 Prec@5=55.092 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=16:33 IST
=> training   91.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.325 Prec@1=29.938 Prec@5=55.092 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=16:34 IST
=> training   91.93% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.314 Prec@1=30.122 Prec@5=55.286 rate=2.11 Hz, eta=0:01:35, total=0:18:10, wall=16:34 IST
=> training   95.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.314 Prec@1=30.122 Prec@5=55.286 rate=2.11 Hz, eta=0:00:48, total=0:18:57, wall=16:34 IST
=> training   95.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.476 DataTime=0.280 Loss=3.314 Prec@1=30.122 Prec@5=55.286 rate=2.11 Hz, eta=0:00:48, total=0:18:57, wall=16:34 IST
=> training   95.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.475 DataTime=0.279 Loss=3.304 Prec@1=30.296 Prec@5=55.480 rate=2.11 Hz, eta=0:00:48, total=0:18:57, wall=16:34 IST
=> training   99.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.475 DataTime=0.279 Loss=3.304 Prec@1=30.296 Prec@5=55.480 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=16:34 IST
=> training   99.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.475 DataTime=0.279 Loss=3.304 Prec@1=30.296 Prec@5=55.480 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=16:34 IST
=> training   99.92% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.475 DataTime=0.279 Loss=3.304 Prec@1=30.299 Prec@5=55.481 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=16:34 IST
=> training   100.00% of 1x2503...Epoch=3/150 LR=0.06000 Time=0.475 DataTime=0.279 Loss=3.304 Prec@1=30.299 Prec@5=55.481 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=16:34 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> validation 0.00% of 1x98...Epoch=3/150 LR=0.06000 Time=6.916 Loss=2.200 Prec@1=48.242 Prec@5=78.711 rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> validation 1.02% of 1x98...Epoch=3/150 LR=0.06000 Time=6.916 Loss=2.200 Prec@1=48.242 Prec@5=78.711 rate=7642.40 Hz, eta=0:00:00, total=0:00:00, wall=16:35 IST
** validation 1.02% of 1x98...Epoch=3/150 LR=0.06000 Time=6.916 Loss=2.200 Prec@1=48.242 Prec@5=78.711 rate=7642.40 Hz, eta=0:00:00, total=0:00:00, wall=16:35 IST
** validation 1.02% of 1x98...Epoch=3/150 LR=0.06000 Time=0.548 Loss=2.950 Prec@1=35.476 Prec@5=62.028 rate=7642.40 Hz, eta=0:00:00, total=0:00:00, wall=16:35 IST
** validation 100.00% of 1x98...Epoch=3/150 LR=0.06000 Time=0.548 Loss=2.950 Prec@1=35.476 Prec@5=62.028 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=16:35 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> training   0.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=5.154 DataTime=4.964 Loss=2.980 Prec@1=34.961 Prec@5=60.352 rate=0 Hz, eta=?, total=0:00:00, wall=16:35 IST
=> training   0.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=5.154 DataTime=4.964 Loss=2.980 Prec@1=34.961 Prec@5=60.352 rate=7726.90 Hz, eta=0:00:00, total=0:00:00, wall=16:35 IST
=> training   0.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=5.154 DataTime=4.964 Loss=2.980 Prec@1=34.961 Prec@5=60.352 rate=7726.90 Hz, eta=0:00:00, total=0:00:00, wall=16:36 IST
=> training   0.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.504 DataTime=0.307 Loss=3.083 Prec@1=33.984 Prec@5=59.677 rate=7726.90 Hz, eta=0:00:00, total=0:00:00, wall=16:36 IST
=> training   4.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.504 DataTime=0.307 Loss=3.083 Prec@1=33.984 Prec@5=59.677 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:36 IST
=> training   4.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.504 DataTime=0.307 Loss=3.083 Prec@1=33.984 Prec@5=59.677 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:37 IST
=> training   4.04% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.481 DataTime=0.284 Loss=3.080 Prec@1=33.958 Prec@5=59.762 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:37 IST
=> training   8.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.481 DataTime=0.284 Loss=3.080 Prec@1=33.958 Prec@5=59.762 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:37 IST
=> training   8.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.481 DataTime=0.284 Loss=3.080 Prec@1=33.958 Prec@5=59.762 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:38 IST
=> training   8.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.473 DataTime=0.278 Loss=3.080 Prec@1=33.913 Prec@5=59.738 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:38 IST
=> training   12.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.473 DataTime=0.278 Loss=3.080 Prec@1=33.913 Prec@5=59.738 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=16:38 IST
=> training   12.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.473 DataTime=0.278 Loss=3.080 Prec@1=33.913 Prec@5=59.738 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=16:39 IST
=> training   12.03% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.470 DataTime=0.274 Loss=3.071 Prec@1=34.036 Prec@5=59.835 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=16:39 IST
=> training   16.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.470 DataTime=0.274 Loss=3.071 Prec@1=34.036 Prec@5=59.835 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=16:39 IST
=> training   16.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.470 DataTime=0.274 Loss=3.071 Prec@1=34.036 Prec@5=59.835 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=16:39 IST
=> training   16.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.469 DataTime=0.273 Loss=3.065 Prec@1=34.136 Prec@5=59.963 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=16:39 IST
=> training   20.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.469 DataTime=0.273 Loss=3.065 Prec@1=34.136 Prec@5=59.963 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=16:39 IST
=> training   20.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.469 DataTime=0.273 Loss=3.065 Prec@1=34.136 Prec@5=59.963 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=16:40 IST
=> training   20.02% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.055 Prec@1=34.357 Prec@5=60.162 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=16:40 IST
=> training   24.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.055 Prec@1=34.357 Prec@5=60.162 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:40 IST
=> training   24.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.055 Prec@1=34.357 Prec@5=60.162 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:41 IST
=> training   24.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.047 Prec@1=34.479 Prec@5=60.336 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:41 IST
=> training   28.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.047 Prec@1=34.479 Prec@5=60.336 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=16:41 IST
=> training   28.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.047 Prec@1=34.479 Prec@5=60.336 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=16:42 IST
=> training   28.01% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=3.038 Prec@1=34.647 Prec@5=60.453 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=16:42 IST
=> training   32.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=3.038 Prec@1=34.647 Prec@5=60.453 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:42 IST
=> training   32.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=3.038 Prec@1=34.647 Prec@5=60.453 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:42 IST
=> training   32.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.030 Prec@1=34.786 Prec@5=60.592 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:42 IST
=> training   36.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.030 Prec@1=34.786 Prec@5=60.592 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=16:42 IST
=> training   36.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.030 Prec@1=34.786 Prec@5=60.592 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=16:43 IST
=> training   36.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.022 Prec@1=34.938 Prec@5=60.753 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=16:43 IST
=> training   39.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.022 Prec@1=34.938 Prec@5=60.753 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=16:43 IST
=> training   39.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=3.022 Prec@1=34.938 Prec@5=60.753 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=16:44 IST
=> training   39.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.014 Prec@1=35.068 Prec@5=60.875 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=16:44 IST
=> training   43.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.014 Prec@1=35.068 Prec@5=60.875 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=16:44 IST
=> training   43.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.014 Prec@1=35.068 Prec@5=60.875 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=16:45 IST
=> training   43.99% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.006 Prec@1=35.217 Prec@5=61.013 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=16:45 IST
=> training   47.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.006 Prec@1=35.217 Prec@5=61.013 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=16:45 IST
=> training   47.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.467 DataTime=0.272 Loss=3.006 Prec@1=35.217 Prec@5=61.013 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=16:46 IST
=> training   47.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.998 Prec@1=35.354 Prec@5=61.149 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=16:46 IST
=> training   51.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.998 Prec@1=35.354 Prec@5=61.149 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=16:46 IST
=> training   51.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.998 Prec@1=35.354 Prec@5=61.149 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=16:46 IST
=> training   51.98% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.991 Prec@1=35.488 Prec@5=61.281 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=16:46 IST
=> training   55.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.991 Prec@1=35.488 Prec@5=61.281 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=16:46 IST
=> training   55.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.991 Prec@1=35.488 Prec@5=61.281 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=16:47 IST
=> training   55.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.983 Prec@1=35.633 Prec@5=61.434 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=16:47 IST
=> training   59.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.983 Prec@1=35.633 Prec@5=61.434 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=16:47 IST
=> training   59.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.272 Loss=2.983 Prec@1=35.633 Prec@5=61.434 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=16:48 IST
=> training   59.97% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.976 Prec@1=35.771 Prec@5=61.553 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=16:48 IST
=> training   63.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.976 Prec@1=35.771 Prec@5=61.553 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=16:48 IST
=> training   63.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.976 Prec@1=35.771 Prec@5=61.553 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=16:49 IST
=> training   63.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.969 Prec@1=35.918 Prec@5=61.689 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=16:49 IST
=> training   67.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.969 Prec@1=35.918 Prec@5=61.689 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=16:49 IST
=> training   67.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.969 Prec@1=35.918 Prec@5=61.689 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=16:49 IST
=> training   67.96% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.962 Prec@1=36.023 Prec@5=61.801 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=16:49 IST
=> training   71.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.962 Prec@1=36.023 Prec@5=61.801 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=16:49 IST
=> training   71.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.962 Prec@1=36.023 Prec@5=61.801 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=16:50 IST
=> training   71.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.274 Loss=2.954 Prec@1=36.160 Prec@5=61.940 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=16:50 IST
=> training   75.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.274 Loss=2.954 Prec@1=36.160 Prec@5=61.940 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=16:50 IST
=> training   75.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.274 Loss=2.954 Prec@1=36.160 Prec@5=61.940 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=16:51 IST
=> training   75.95% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.947 Prec@1=36.277 Prec@5=62.061 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=16:51 IST
=> training   79.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.947 Prec@1=36.277 Prec@5=62.061 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=16:51 IST
=> training   79.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.947 Prec@1=36.277 Prec@5=62.061 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=16:52 IST
=> training   79.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.940 Prec@1=36.402 Prec@5=62.180 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=16:52 IST
=> training   83.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.940 Prec@1=36.402 Prec@5=62.180 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=16:52 IST
=> training   83.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.940 Prec@1=36.402 Prec@5=62.180 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=16:53 IST
=> training   83.94% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.933 Prec@1=36.530 Prec@5=62.309 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=16:53 IST
=> training   87.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.933 Prec@1=36.530 Prec@5=62.309 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=16:53 IST
=> training   87.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.933 Prec@1=36.530 Prec@5=62.309 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=16:53 IST
=> training   87.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.926 Prec@1=36.654 Prec@5=62.446 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=16:53 IST
=> training   91.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.926 Prec@1=36.654 Prec@5=62.446 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=16:53 IST
=> training   91.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.926 Prec@1=36.654 Prec@5=62.446 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=16:54 IST
=> training   91.93% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.920 Prec@1=36.768 Prec@5=62.561 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=16:54 IST
=> training   95.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.920 Prec@1=36.768 Prec@5=62.561 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=16:54 IST
=> training   95.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.920 Prec@1=36.768 Prec@5=62.561 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=16:55 IST
=> training   95.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.913 Prec@1=36.880 Prec@5=62.674 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=16:55 IST
=> training   99.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.913 Prec@1=36.880 Prec@5=62.674 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:55 IST
=> training   99.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.913 Prec@1=36.880 Prec@5=62.674 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:55 IST
=> training   99.92% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.913 Prec@1=36.885 Prec@5=62.678 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:55 IST
=> training   100.00% of 1x2503...Epoch=4/150 LR=0.08000 Time=0.468 DataTime=0.273 Loss=2.913 Prec@1=36.885 Prec@5=62.678 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=16:55 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> validation 0.00% of 1x98...Epoch=4/150 LR=0.08000 Time=6.694 Loss=1.782 Prec@1=55.859 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> validation 1.02% of 1x98...Epoch=4/150 LR=0.08000 Time=6.694 Loss=1.782 Prec@1=55.859 Prec@5=82.812 rate=4283.28 Hz, eta=0:00:00, total=0:00:00, wall=16:55 IST
** validation 1.02% of 1x98...Epoch=4/150 LR=0.08000 Time=6.694 Loss=1.782 Prec@1=55.859 Prec@5=82.812 rate=4283.28 Hz, eta=0:00:00, total=0:00:00, wall=16:56 IST
** validation 1.02% of 1x98...Epoch=4/150 LR=0.08000 Time=0.551 Loss=2.670 Prec@1=40.238 Prec@5=67.034 rate=4283.28 Hz, eta=0:00:00, total=0:00:00, wall=16:56 IST
** validation 100.00% of 1x98...Epoch=4/150 LR=0.08000 Time=0.551 Loss=2.670 Prec@1=40.238 Prec@5=67.034 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=16:56 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:56 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:56 IST
=> training   0.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=5.169 DataTime=4.982 Loss=2.752 Prec@1=38.672 Prec@5=65.234 rate=0 Hz, eta=?, total=0:00:00, wall=16:56 IST
=> training   0.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=5.169 DataTime=4.982 Loss=2.752 Prec@1=38.672 Prec@5=65.234 rate=9975.56 Hz, eta=0:00:00, total=0:00:00, wall=16:56 IST
=> training   0.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=5.169 DataTime=4.982 Loss=2.752 Prec@1=38.672 Prec@5=65.234 rate=9975.56 Hz, eta=0:00:00, total=0:00:00, wall=16:57 IST
=> training   0.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.498 DataTime=0.306 Loss=2.776 Prec@1=39.179 Prec@5=65.078 rate=9975.56 Hz, eta=0:00:00, total=0:00:00, wall=16:57 IST
=> training   4.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.498 DataTime=0.306 Loss=2.776 Prec@1=39.179 Prec@5=65.078 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=16:57 IST
=> training   4.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.498 DataTime=0.306 Loss=2.776 Prec@1=39.179 Prec@5=65.078 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=16:57 IST
=> training   4.04% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.481 DataTime=0.286 Loss=2.777 Prec@1=39.185 Prec@5=64.958 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=16:57 IST
=> training   8.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.481 DataTime=0.286 Loss=2.777 Prec@1=39.185 Prec@5=64.958 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:57 IST
=> training   8.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.481 DataTime=0.286 Loss=2.777 Prec@1=39.185 Prec@5=64.958 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:58 IST
=> training   8.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.479 DataTime=0.284 Loss=2.772 Prec@1=39.218 Prec@5=65.116 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:58 IST
=> training   12.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.479 DataTime=0.284 Loss=2.772 Prec@1=39.218 Prec@5=65.116 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=16:58 IST
=> training   12.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.479 DataTime=0.284 Loss=2.772 Prec@1=39.218 Prec@5=65.116 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=16:59 IST
=> training   12.03% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.475 DataTime=0.279 Loss=2.768 Prec@1=39.372 Prec@5=65.229 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=16:59 IST
=> training   16.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.475 DataTime=0.279 Loss=2.768 Prec@1=39.372 Prec@5=65.229 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=16:59 IST
=> training   16.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.475 DataTime=0.279 Loss=2.768 Prec@1=39.372 Prec@5=65.229 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=17:00 IST
=> training   16.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.474 DataTime=0.279 Loss=2.761 Prec@1=39.517 Prec@5=65.303 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=17:00 IST
=> training   20.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.474 DataTime=0.279 Loss=2.761 Prec@1=39.517 Prec@5=65.303 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=17:00 IST
=> training   20.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.474 DataTime=0.279 Loss=2.761 Prec@1=39.517 Prec@5=65.303 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=17:01 IST
=> training   20.02% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.757 Prec@1=39.566 Prec@5=65.336 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=17:01 IST
=> training   24.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.757 Prec@1=39.566 Prec@5=65.336 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:01 IST
=> training   24.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.757 Prec@1=39.566 Prec@5=65.336 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:01 IST
=> training   24.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.274 Loss=2.753 Prec@1=39.641 Prec@5=65.413 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:01 IST
=> training   28.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.274 Loss=2.753 Prec@1=39.641 Prec@5=65.413 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=17:01 IST
=> training   28.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.274 Loss=2.753 Prec@1=39.641 Prec@5=65.413 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=17:02 IST
=> training   28.01% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.746 Prec@1=39.753 Prec@5=65.536 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=17:02 IST
=> training   32.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.746 Prec@1=39.753 Prec@5=65.536 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=17:02 IST
=> training   32.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.471 DataTime=0.276 Loss=2.746 Prec@1=39.753 Prec@5=65.536 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=17:03 IST
=> training   32.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.275 Loss=2.742 Prec@1=39.830 Prec@5=65.607 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=17:03 IST
=> training   36.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.275 Loss=2.742 Prec@1=39.830 Prec@5=65.607 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=17:03 IST
=> training   36.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.470 DataTime=0.275 Loss=2.742 Prec@1=39.830 Prec@5=65.607 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=17:04 IST
=> training   36.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.736 Prec@1=39.966 Prec@5=65.734 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=17:04 IST
=> training   39.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.736 Prec@1=39.966 Prec@5=65.734 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=17:04 IST
=> training   39.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.736 Prec@1=39.966 Prec@5=65.734 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=17:04 IST
=> training   39.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.729 Prec@1=40.088 Prec@5=65.852 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=17:04 IST
=> training   43.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.729 Prec@1=40.088 Prec@5=65.852 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=17:04 IST
=> training   43.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.729 Prec@1=40.088 Prec@5=65.852 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=17:05 IST
=> training   43.99% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.726 Prec@1=40.146 Prec@5=65.925 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=17:05 IST
=> training   47.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.726 Prec@1=40.146 Prec@5=65.925 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=17:05 IST
=> training   47.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.726 Prec@1=40.146 Prec@5=65.925 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=17:06 IST
=> training   47.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.721 Prec@1=40.220 Prec@5=66.002 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=17:06 IST
=> training   51.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.721 Prec@1=40.220 Prec@5=66.002 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=17:06 IST
=> training   51.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.721 Prec@1=40.220 Prec@5=66.002 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=17:07 IST
=> training   51.98% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.716 Prec@1=40.300 Prec@5=66.100 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=17:07 IST
=> training   55.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.716 Prec@1=40.300 Prec@5=66.100 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=17:07 IST
=> training   55.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.716 Prec@1=40.300 Prec@5=66.100 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=17:08 IST
=> training   55.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.710 Prec@1=40.395 Prec@5=66.192 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=17:08 IST
=> training   59.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.710 Prec@1=40.395 Prec@5=66.192 rate=2.15 Hz, eta=0:07:45, total=0:11:38, wall=17:08 IST
=> training   59.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.710 Prec@1=40.395 Prec@5=66.192 rate=2.15 Hz, eta=0:07:45, total=0:11:38, wall=17:08 IST
=> training   59.97% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.705 Prec@1=40.480 Prec@5=66.264 rate=2.15 Hz, eta=0:07:45, total=0:11:38, wall=17:08 IST
=> training   63.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.705 Prec@1=40.480 Prec@5=66.264 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=17:08 IST
=> training   63.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.705 Prec@1=40.480 Prec@5=66.264 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=17:09 IST
=> training   63.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.700 Prec@1=40.556 Prec@5=66.345 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=17:09 IST
=> training   67.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.700 Prec@1=40.556 Prec@5=66.345 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=17:09 IST
=> training   67.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.700 Prec@1=40.556 Prec@5=66.345 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=17:10 IST
=> training   67.96% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.695 Prec@1=40.651 Prec@5=66.429 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=17:10 IST
=> training   71.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.695 Prec@1=40.651 Prec@5=66.429 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=17:10 IST
=> training   71.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.695 Prec@1=40.651 Prec@5=66.429 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=17:11 IST
=> training   71.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.691 Prec@1=40.740 Prec@5=66.513 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=17:11 IST
=> training   75.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.691 Prec@1=40.740 Prec@5=66.513 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:11 IST
=> training   75.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.691 Prec@1=40.740 Prec@5=66.513 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:11 IST
=> training   75.95% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.686 Prec@1=40.823 Prec@5=66.590 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:11 IST
=> training   79.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.686 Prec@1=40.823 Prec@5=66.590 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=17:11 IST
=> training   79.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.686 Prec@1=40.823 Prec@5=66.590 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=17:12 IST
=> training   79.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.682 Prec@1=40.894 Prec@5=66.657 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=17:12 IST
=> training   83.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.682 Prec@1=40.894 Prec@5=66.657 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=17:12 IST
=> training   83.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.274 Loss=2.682 Prec@1=40.894 Prec@5=66.657 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=17:13 IST
=> training   83.94% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.678 Prec@1=40.962 Prec@5=66.727 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=17:13 IST
=> training   87.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.678 Prec@1=40.962 Prec@5=66.727 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=17:13 IST
=> training   87.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.678 Prec@1=40.962 Prec@5=66.727 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=17:14 IST
=> training   87.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.674 Prec@1=41.038 Prec@5=66.804 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=17:14 IST
=> training   91.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.674 Prec@1=41.038 Prec@5=66.804 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=17:14 IST
=> training   91.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.674 Prec@1=41.038 Prec@5=66.804 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=17:15 IST
=> training   91.93% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.669 Prec@1=41.116 Prec@5=66.886 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=17:15 IST
=> training   95.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.669 Prec@1=41.116 Prec@5=66.886 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:15 IST
=> training   95.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.469 DataTime=0.273 Loss=2.669 Prec@1=41.116 Prec@5=66.886 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:15 IST
=> training   95.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.664 Prec@1=41.213 Prec@5=66.966 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:15 IST
=> training   99.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.664 Prec@1=41.213 Prec@5=66.966 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=17:15 IST
=> training   99.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.664 Prec@1=41.213 Prec@5=66.966 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=17:15 IST
=> training   99.92% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.664 Prec@1=41.214 Prec@5=66.966 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=17:15 IST
=> training   100.00% of 1x2503...Epoch=5/150 LR=0.09982 Time=0.468 DataTime=0.273 Loss=2.664 Prec@1=41.214 Prec@5=66.966 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=17:15 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:15 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:15 IST
=> validation 0.00% of 1x98...Epoch=5/150 LR=0.09982 Time=6.971 Loss=1.620 Prec@1=59.961 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=17:15 IST
=> validation 1.02% of 1x98...Epoch=5/150 LR=0.09982 Time=6.971 Loss=1.620 Prec@1=59.961 Prec@5=84.180 rate=7780.71 Hz, eta=0:00:00, total=0:00:00, wall=17:15 IST
** validation 1.02% of 1x98...Epoch=5/150 LR=0.09982 Time=6.971 Loss=1.620 Prec@1=59.961 Prec@5=84.180 rate=7780.71 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
** validation 1.02% of 1x98...Epoch=5/150 LR=0.09982 Time=0.551 Loss=2.489 Prec@1=43.738 Prec@5=70.256 rate=7780.71 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
** validation 100.00% of 1x98...Epoch=5/150 LR=0.09982 Time=0.551 Loss=2.489 Prec@1=43.738 Prec@5=70.256 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=17:16 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> training   0.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=5.030 DataTime=4.771 Loss=2.498 Prec@1=43.359 Prec@5=69.727 rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> training   0.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=5.030 DataTime=4.771 Loss=2.498 Prec@1=43.359 Prec@5=69.727 rate=7971.69 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
=> training   0.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=5.030 DataTime=4.771 Loss=2.498 Prec@1=43.359 Prec@5=69.727 rate=7971.69 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST
=> training   0.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.498 DataTime=0.302 Loss=2.493 Prec@1=44.361 Prec@5=69.825 rate=7971.69 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST
=> training   4.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.498 DataTime=0.302 Loss=2.493 Prec@1=44.361 Prec@5=69.825 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=17:17 IST
=> training   4.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.498 DataTime=0.302 Loss=2.493 Prec@1=44.361 Prec@5=69.825 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=17:18 IST
=> training   4.04% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.488 DataTime=0.294 Loss=2.488 Prec@1=44.409 Prec@5=69.944 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=17:18 IST
=> training   8.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.488 DataTime=0.294 Loss=2.488 Prec@1=44.409 Prec@5=69.944 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=17:18 IST
=> training   8.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.488 DataTime=0.294 Loss=2.488 Prec@1=44.409 Prec@5=69.944 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=17:19 IST
=> training   8.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.480 DataTime=0.283 Loss=2.486 Prec@1=44.435 Prec@5=70.030 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=17:19 IST
=> training   12.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.480 DataTime=0.283 Loss=2.486 Prec@1=44.435 Prec@5=70.030 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:19 IST
=> training   12.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.480 DataTime=0.283 Loss=2.486 Prec@1=44.435 Prec@5=70.030 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:20 IST
=> training   12.03% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.476 DataTime=0.281 Loss=2.484 Prec@1=44.510 Prec@5=70.030 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:20 IST
=> training   16.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.476 DataTime=0.281 Loss=2.484 Prec@1=44.510 Prec@5=70.030 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=17:20 IST
=> training   16.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.476 DataTime=0.281 Loss=2.484 Prec@1=44.510 Prec@5=70.030 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=17:20 IST
=> training   16.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.486 Prec@1=44.511 Prec@5=70.046 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=17:20 IST
=> training   20.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.486 Prec@1=44.511 Prec@5=70.046 rate=2.15 Hz, eta=0:15:31, total=0:03:52, wall=17:20 IST
=> training   20.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.486 Prec@1=44.511 Prec@5=70.046 rate=2.15 Hz, eta=0:15:31, total=0:03:52, wall=17:21 IST
=> training   20.02% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.487 Prec@1=44.507 Prec@5=70.027 rate=2.15 Hz, eta=0:15:31, total=0:03:52, wall=17:21 IST
=> training   24.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.487 Prec@1=44.507 Prec@5=70.027 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=17:21 IST
=> training   24.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.487 Prec@1=44.507 Prec@5=70.027 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=17:22 IST
=> training   24.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.484 Prec@1=44.520 Prec@5=70.111 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=17:22 IST
=> training   28.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.484 Prec@1=44.520 Prec@5=70.111 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=17:22 IST
=> training   28.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.484 Prec@1=44.520 Prec@5=70.111 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=17:23 IST
=> training   28.01% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.480 Prec@1=44.617 Prec@5=70.172 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=17:23 IST
=> training   32.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.480 Prec@1=44.617 Prec@5=70.172 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=17:23 IST
=> training   32.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.480 Prec@1=44.617 Prec@5=70.172 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=17:23 IST
=> training   32.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.477 Prec@1=44.683 Prec@5=70.225 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=17:23 IST
=> training   36.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.477 Prec@1=44.683 Prec@5=70.225 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=17:23 IST
=> training   36.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.279 Loss=2.477 Prec@1=44.683 Prec@5=70.225 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=17:24 IST
=> training   36.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.475 Prec@1=44.705 Prec@5=70.252 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=17:24 IST
=> training   39.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.475 Prec@1=44.705 Prec@5=70.252 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:24 IST
=> training   39.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.475 DataTime=0.279 Loss=2.475 Prec@1=44.705 Prec@5=70.252 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:25 IST
=> training   39.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.474 Prec@1=44.723 Prec@5=70.253 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:25 IST
=> training   43.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.474 Prec@1=44.723 Prec@5=70.253 rate=2.13 Hz, eta=0:10:58, total=0:08:36, wall=17:25 IST
=> training   43.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.474 DataTime=0.278 Loss=2.474 Prec@1=44.723 Prec@5=70.253 rate=2.13 Hz, eta=0:10:58, total=0:08:36, wall=17:26 IST
=> training   43.99% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.473 DataTime=0.277 Loss=2.472 Prec@1=44.768 Prec@5=70.304 rate=2.13 Hz, eta=0:10:58, total=0:08:36, wall=17:26 IST
=> training   47.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.473 DataTime=0.277 Loss=2.472 Prec@1=44.768 Prec@5=70.304 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=17:26 IST
=> training   47.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.473 DataTime=0.277 Loss=2.472 Prec@1=44.768 Prec@5=70.304 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=17:27 IST
=> training   47.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.472 DataTime=0.276 Loss=2.471 Prec@1=44.801 Prec@5=70.318 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=17:27 IST
=> training   51.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.472 DataTime=0.276 Loss=2.471 Prec@1=44.801 Prec@5=70.318 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=17:27 IST
=> training   51.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.472 DataTime=0.276 Loss=2.471 Prec@1=44.801 Prec@5=70.318 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=17:27 IST
=> training   51.98% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.469 Prec@1=44.838 Prec@5=70.355 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=17:27 IST
=> training   55.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.469 Prec@1=44.838 Prec@5=70.355 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=17:27 IST
=> training   55.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.469 Prec@1=44.838 Prec@5=70.355 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=17:28 IST
=> training   55.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.467 Prec@1=44.874 Prec@5=70.387 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=17:28 IST
=> training   59.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.467 Prec@1=44.874 Prec@5=70.387 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=17:28 IST
=> training   59.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.467 Prec@1=44.874 Prec@5=70.387 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=17:29 IST
=> training   59.97% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.464 Prec@1=44.923 Prec@5=70.445 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=17:29 IST
=> training   63.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.464 Prec@1=44.923 Prec@5=70.445 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=17:29 IST
=> training   63.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.464 Prec@1=44.923 Prec@5=70.445 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=17:30 IST
=> training   63.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.462 Prec@1=44.964 Prec@5=70.483 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=17:30 IST
=> training   67.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.462 Prec@1=44.964 Prec@5=70.483 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=17:30 IST
=> training   67.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.462 Prec@1=44.964 Prec@5=70.483 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=17:30 IST
=> training   67.96% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.459 Prec@1=45.015 Prec@5=70.500 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=17:30 IST
=> training   71.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.459 Prec@1=45.015 Prec@5=70.500 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=17:30 IST
=> training   71.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.276 Loss=2.459 Prec@1=45.015 Prec@5=70.500 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=17:31 IST
=> training   71.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.275 Loss=2.457 Prec@1=45.063 Prec@5=70.530 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=17:31 IST
=> training   75.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.275 Loss=2.457 Prec@1=45.063 Prec@5=70.530 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=17:31 IST
=> training   75.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.471 DataTime=0.275 Loss=2.457 Prec@1=45.063 Prec@5=70.530 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=17:32 IST
=> training   75.95% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.275 Loss=2.455 Prec@1=45.105 Prec@5=70.561 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=17:32 IST
=> training   79.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.275 Loss=2.455 Prec@1=45.105 Prec@5=70.561 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=17:32 IST
=> training   79.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.275 Loss=2.455 Prec@1=45.105 Prec@5=70.561 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=17:33 IST
=> training   79.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.453 Prec@1=45.139 Prec@5=70.594 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=17:33 IST
=> training   83.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.453 Prec@1=45.139 Prec@5=70.594 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=17:33 IST
=> training   83.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.453 Prec@1=45.139 Prec@5=70.594 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=17:34 IST
=> training   83.94% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.451 Prec@1=45.190 Prec@5=70.637 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=17:34 IST
=> training   87.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.451 Prec@1=45.190 Prec@5=70.637 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=17:34 IST
=> training   87.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.451 Prec@1=45.190 Prec@5=70.637 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=17:34 IST
=> training   87.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.448 Prec@1=45.239 Prec@5=70.678 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=17:34 IST
=> training   91.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.448 Prec@1=45.239 Prec@5=70.678 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=17:34 IST
=> training   91.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.448 Prec@1=45.239 Prec@5=70.678 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=17:35 IST
=> training   91.93% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.447 Prec@1=45.289 Prec@5=70.706 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=17:35 IST
=> training   95.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.447 Prec@1=45.289 Prec@5=70.706 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=17:35 IST
=> training   95.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.447 Prec@1=45.289 Prec@5=70.706 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=17:36 IST
=> training   95.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.444 Prec@1=45.351 Prec@5=70.757 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=17:36 IST
=> training   99.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.444 Prec@1=45.351 Prec@5=70.757 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=17:36 IST
=> training   99.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.444 Prec@1=45.351 Prec@5=70.757 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=17:36 IST
=> training   99.92% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.444 Prec@1=45.353 Prec@5=70.757 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=17:36 IST
=> training   100.00% of 1x2503...Epoch=6/150 LR=0.09973 Time=0.470 DataTime=0.274 Loss=2.444 Prec@1=45.353 Prec@5=70.757 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=17:36 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:36 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:36 IST
=> validation 0.00% of 1x98...Epoch=6/150 LR=0.09973 Time=6.391 Loss=1.534 Prec@1=60.742 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=17:36 IST
=> validation 1.02% of 1x98...Epoch=6/150 LR=0.09973 Time=6.391 Loss=1.534 Prec@1=60.742 Prec@5=86.328 rate=8306.54 Hz, eta=0:00:00, total=0:00:00, wall=17:36 IST
** validation 1.02% of 1x98...Epoch=6/150 LR=0.09973 Time=6.391 Loss=1.534 Prec@1=60.742 Prec@5=86.328 rate=8306.54 Hz, eta=0:00:00, total=0:00:00, wall=17:37 IST
** validation 1.02% of 1x98...Epoch=6/150 LR=0.09973 Time=0.545 Loss=2.265 Prec@1=47.870 Prec@5=74.064 rate=8306.54 Hz, eta=0:00:00, total=0:00:00, wall=17:37 IST
** validation 100.00% of 1x98...Epoch=6/150 LR=0.09973 Time=0.545 Loss=2.265 Prec@1=47.870 Prec@5=74.064 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=17:37 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:37 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:37 IST
=> training   0.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=4.709 DataTime=4.444 Loss=2.174 Prec@1=49.609 Prec@5=73.047 rate=0 Hz, eta=?, total=0:00:00, wall=17:37 IST
=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=4.709 DataTime=4.444 Loss=2.174 Prec@1=49.609 Prec@5=73.047 rate=5046.83 Hz, eta=0:00:00, total=0:00:00, wall=17:37 IST
=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=4.709 DataTime=4.444 Loss=2.174 Prec@1=49.609 Prec@5=73.047 rate=5046.83 Hz, eta=0:00:00, total=0:00:00, wall=17:38 IST
=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.502 DataTime=0.307 Loss=2.322 Prec@1=47.436 Prec@5=72.699 rate=5046.83 Hz, eta=0:00:00, total=0:00:00, wall=17:38 IST
=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.502 DataTime=0.307 Loss=2.322 Prec@1=47.436 Prec@5=72.699 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=17:38 IST
=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.502 DataTime=0.307 Loss=2.322 Prec@1=47.436 Prec@5=72.699 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=17:38 IST
=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.487 DataTime=0.291 Loss=2.324 Prec@1=47.481 Prec@5=72.729 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=17:38 IST
=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.487 DataTime=0.291 Loss=2.324 Prec@1=47.481 Prec@5=72.729 rate=2.16 Hz, eta=0:17:47, total=0:01:33, wall=17:38 IST
=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.487 DataTime=0.291 Loss=2.324 Prec@1=47.481 Prec@5=72.729 rate=2.16 Hz, eta=0:17:47, total=0:01:33, wall=17:39 IST
=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.481 DataTime=0.286 Loss=2.328 Prec@1=47.475 Prec@5=72.647 rate=2.16 Hz, eta=0:17:47, total=0:01:33, wall=17:39 IST
=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.481 DataTime=0.286 Loss=2.328 Prec@1=47.475 Prec@5=72.647 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=17:39 IST
=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.481 DataTime=0.286 Loss=2.328 Prec@1=47.475 Prec@5=72.647 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=17:40 IST
=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.482 DataTime=0.286 Loss=2.327 Prec@1=47.531 Prec@5=72.673 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=17:40 IST
=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.482 DataTime=0.286 Loss=2.327 Prec@1=47.531 Prec@5=72.673 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=17:40 IST
=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.482 DataTime=0.286 Loss=2.327 Prec@1=47.531 Prec@5=72.673 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=17:41 IST
=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.479 DataTime=0.282 Loss=2.324 Prec@1=47.529 Prec@5=72.749 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=17:41 IST
=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.479 DataTime=0.282 Loss=2.324 Prec@1=47.529 Prec@5=72.749 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=17:41 IST
=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.479 DataTime=0.282 Loss=2.324 Prec@1=47.529 Prec@5=72.749 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=17:42 IST
=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.477 DataTime=0.281 Loss=2.323 Prec@1=47.537 Prec@5=72.753 rate=2.13 Hz, eta=0:15:39, total=0:03:55, wall=17:42 IST
=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.477 DataTime=0.281 Loss=2.323 Prec@1=47.537 Prec@5=72.753 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=17:42 IST
=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.477 DataTime=0.281 Loss=2.323 Prec@1=47.537 Prec@5=72.753 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=17:42 IST
=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.322 Prec@1=47.549 Prec@5=72.731 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=17:42 IST
=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.322 Prec@1=47.549 Prec@5=72.731 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=17:42 IST
=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.322 Prec@1=47.549 Prec@5=72.731 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=17:43 IST
=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.476 DataTime=0.280 Loss=2.321 Prec@1=47.554 Prec@5=72.746 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=17:43 IST
=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.476 DataTime=0.280 Loss=2.321 Prec@1=47.554 Prec@5=72.746 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=17:43 IST
=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.476 DataTime=0.280 Loss=2.321 Prec@1=47.554 Prec@5=72.746 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=17:44 IST
=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.319 Prec@1=47.570 Prec@5=72.790 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=17:44 IST
=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.319 Prec@1=47.570 Prec@5=72.790 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=17:44 IST
=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.319 Prec@1=47.570 Prec@5=72.790 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=17:45 IST
=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.626 Prec@5=72.817 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=17:45 IST
=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.626 Prec@5=72.817 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:45 IST
=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.626 Prec@5=72.817 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:46 IST
=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.642 Prec@5=72.825 rate=2.13 Hz, eta=0:11:46, total=0:07:50, wall=17:46 IST
=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.642 Prec@5=72.825 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=17:46 IST
=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.317 Prec@1=47.642 Prec@5=72.825 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=17:46 IST
=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.669 Prec@5=72.854 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=17:46 IST
=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.669 Prec@5=72.854 rate=2.12 Hz, eta=0:10:13, total=0:09:25, wall=17:46 IST
=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.669 Prec@5=72.854 rate=2.12 Hz, eta=0:10:13, total=0:09:25, wall=17:47 IST
=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.673 Prec@5=72.863 rate=2.12 Hz, eta=0:10:13, total=0:09:25, wall=17:47 IST
=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.673 Prec@5=72.863 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=17:47 IST
=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.279 Loss=2.315 Prec@1=47.673 Prec@5=72.863 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=17:48 IST
=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.278 Loss=2.314 Prec@1=47.698 Prec@5=72.873 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=17:48 IST
=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.278 Loss=2.314 Prec@1=47.698 Prec@5=72.873 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=17:48 IST
=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.475 DataTime=0.278 Loss=2.314 Prec@1=47.698 Prec@5=72.873 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=17:49 IST
=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.312 Prec@1=47.712 Prec@5=72.890 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=17:49 IST
=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.312 Prec@1=47.712 Prec@5=72.890 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=17:49 IST
=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.312 Prec@1=47.712 Prec@5=72.890 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=17:49 IST
=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.277 Loss=2.310 Prec@1=47.758 Prec@5=72.924 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=17:49 IST
=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.277 Loss=2.310 Prec@1=47.758 Prec@5=72.924 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=17:49 IST
=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.277 Loss=2.310 Prec@1=47.758 Prec@5=72.924 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=17:50 IST
=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.309 Prec@1=47.800 Prec@5=72.958 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=17:50 IST
=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.309 Prec@1=47.800 Prec@5=72.958 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=17:50 IST
=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.309 Prec@1=47.800 Prec@5=72.958 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=17:51 IST
=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.307 Prec@1=47.825 Prec@5=72.978 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=17:51 IST
=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.307 Prec@1=47.825 Prec@5=72.978 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=17:51 IST
=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.307 Prec@1=47.825 Prec@5=72.978 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=17:52 IST
=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.307 Prec@1=47.846 Prec@5=72.995 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=17:52 IST
=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.307 Prec@1=47.846 Prec@5=72.995 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=17:52 IST
=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.307 Prec@1=47.846 Prec@5=72.995 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=17:53 IST
=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.305 Prec@1=47.883 Prec@5=73.014 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=17:53 IST
=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.305 Prec@1=47.883 Prec@5=73.014 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=17:53 IST
=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.305 Prec@1=47.883 Prec@5=73.014 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=17:53 IST
=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.304 Prec@1=47.893 Prec@5=73.029 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=17:53 IST
=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.304 Prec@1=47.893 Prec@5=73.029 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=17:53 IST
=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.474 DataTime=0.278 Loss=2.304 Prec@1=47.893 Prec@5=73.029 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=17:54 IST
=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.303 Prec@1=47.928 Prec@5=73.042 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=17:54 IST
=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.303 Prec@1=47.928 Prec@5=73.042 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=17:54 IST
=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.303 Prec@1=47.928 Prec@5=73.042 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=17:55 IST
=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.302 Prec@1=47.956 Prec@5=73.054 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=17:55 IST
=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.302 Prec@1=47.956 Prec@5=73.054 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=17:55 IST
=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.302 Prec@1=47.956 Prec@5=73.054 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=17:56 IST
=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.301 Prec@1=47.980 Prec@5=73.070 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=17:56 IST
=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.301 Prec@1=47.980 Prec@5=73.070 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=17:56 IST
=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.301 Prec@1=47.980 Prec@5=73.070 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=17:57 IST
=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.299 Prec@1=48.016 Prec@5=73.090 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=17:57 IST
=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.299 Prec@1=48.016 Prec@5=73.090 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=17:57 IST
=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.299 Prec@1=48.016 Prec@5=73.090 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=17:57 IST
=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.299 Prec@1=48.016 Prec@5=73.089 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=17:57 IST
=> training   100.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.473 DataTime=0.277 Loss=2.299 Prec@1=48.016 Prec@5=73.089 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=17:57 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST
=> validation 0.00% of 1x98...Epoch=7/150 LR=0.09961 Time=6.891 Loss=1.499 Prec@1=63.672 Prec@5=85.352 rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST
=> validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=6.891 Loss=1.499 Prec@1=63.672 Prec@5=85.352 rate=8913.69 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST
** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=6.891 Loss=1.499 Prec@1=63.672 Prec@5=85.352 rate=8913.69 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST
** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=0.550 Loss=2.129 Prec@1=50.686 Prec@5=76.034 rate=8913.69 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST
** validation 100.00% of 1x98...Epoch=7/150 LR=0.09961 Time=0.550 Loss=2.129 Prec@1=50.686 Prec@5=76.034 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=17:57 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST
=> training   0.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.794 DataTime=4.422 Loss=2.129 Prec@1=51.758 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST
=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.794 DataTime=4.422 Loss=2.129 Prec@1=51.758 Prec@5=77.148 rate=6864.50 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST
=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.794 DataTime=4.422 Loss=2.129 Prec@1=51.758 Prec@5=77.148 rate=6864.50 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST
=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.505 DataTime=0.308 Loss=2.204 Prec@1=49.691 Prec@5=74.486 rate=6864.50 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST
=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.505 DataTime=0.308 Loss=2.204 Prec@1=49.691 Prec@5=74.486 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=17:58 IST
=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.505 DataTime=0.308 Loss=2.204 Prec@1=49.691 Prec@5=74.486 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=17:59 IST
=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.483 DataTime=0.286 Loss=2.204 Prec@1=49.699 Prec@5=74.574 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=17:59 IST
=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.483 DataTime=0.286 Loss=2.204 Prec@1=49.699 Prec@5=74.574 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=17:59 IST
=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.483 DataTime=0.286 Loss=2.204 Prec@1=49.699 Prec@5=74.574 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=18:00 IST
=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.476 DataTime=0.278 Loss=2.209 Prec@1=49.655 Prec@5=74.519 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=18:00 IST
=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.476 DataTime=0.278 Loss=2.209 Prec@1=49.655 Prec@5=74.519 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=18:00 IST
=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.476 DataTime=0.278 Loss=2.209 Prec@1=49.655 Prec@5=74.519 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=18:01 IST
=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.273 Loss=2.211 Prec@1=49.607 Prec@5=74.527 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=18:01 IST
=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.273 Loss=2.211 Prec@1=49.607 Prec@5=74.527 rate=2.18 Hz, eta=0:16:04, total=0:03:04, wall=18:01 IST
=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.273 Loss=2.211 Prec@1=49.607 Prec@5=74.527 rate=2.18 Hz, eta=0:16:04, total=0:03:04, wall=18:01 IST
=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.274 Loss=2.212 Prec@1=49.673 Prec@5=74.489 rate=2.18 Hz, eta=0:16:04, total=0:03:04, wall=18:01 IST
=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.274 Loss=2.212 Prec@1=49.673 Prec@5=74.489 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=18:01 IST
=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.471 DataTime=0.274 Loss=2.212 Prec@1=49.673 Prec@5=74.489 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=18:02 IST
=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.470 DataTime=0.274 Loss=2.211 Prec@1=49.703 Prec@5=74.494 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=18:02 IST
=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.470 DataTime=0.274 Loss=2.211 Prec@1=49.703 Prec@5=74.494 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=18:02 IST
=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.470 DataTime=0.274 Loss=2.211 Prec@1=49.703 Prec@5=74.494 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=18:03 IST
=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.711 Prec@5=74.494 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=18:03 IST
=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.711 Prec@5=74.494 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:03 IST
=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.711 Prec@5=74.494 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:04 IST
=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.723 Prec@5=74.483 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:04 IST
=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.723 Prec@5=74.483 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:04 IST
=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.723 Prec@5=74.483 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:05 IST
=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.692 Prec@5=74.487 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:05 IST
=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.692 Prec@5=74.487 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=18:05 IST
=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.211 Prec@1=49.692 Prec@5=74.487 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=18:05 IST
=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.210 Prec@1=49.695 Prec@5=74.510 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=18:05 IST
=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.210 Prec@1=49.695 Prec@5=74.510 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=18:05 IST
=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.210 Prec@1=49.695 Prec@5=74.510 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=18:06 IST
=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.209 Prec@1=49.722 Prec@5=74.539 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=18:06 IST
=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.209 Prec@1=49.722 Prec@5=74.539 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:06 IST
=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.209 Prec@1=49.722 Prec@5=74.539 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:07 IST
=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.208 Prec@1=49.736 Prec@5=74.569 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:07 IST
=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.208 Prec@1=49.736 Prec@5=74.569 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=18:07 IST
=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.208 Prec@1=49.736 Prec@5=74.569 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=18:08 IST
=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.752 Prec@5=74.598 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=18:08 IST
=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.752 Prec@5=74.598 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=18:08 IST
=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.752 Prec@5=74.598 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=18:08 IST
=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.749 Prec@5=74.612 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=18:08 IST
=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.749 Prec@5=74.612 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:08 IST
=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.206 Prec@1=49.749 Prec@5=74.612 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:09 IST
=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.206 Prec@1=49.777 Prec@5=74.615 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:09 IST
=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.206 Prec@1=49.777 Prec@5=74.615 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:09 IST
=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.206 Prec@1=49.777 Prec@5=74.615 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:10 IST
=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.204 Prec@1=49.800 Prec@5=74.640 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:10 IST
=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.204 Prec@1=49.800 Prec@5=74.640 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=18:10 IST
=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.273 Loss=2.204 Prec@1=49.800 Prec@5=74.640 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=18:11 IST
=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.202 Prec@1=49.829 Prec@5=74.666 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=18:11 IST
=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.202 Prec@1=49.829 Prec@5=74.666 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=18:11 IST
=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.469 DataTime=0.272 Loss=2.202 Prec@1=49.829 Prec@5=74.666 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=18:12 IST
=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.203 Prec@1=49.828 Prec@5=74.654 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=18:12 IST
=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.203 Prec@1=49.828 Prec@5=74.654 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:12 IST
=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.203 Prec@1=49.828 Prec@5=74.654 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:12 IST
=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.202 Prec@1=49.847 Prec@5=74.671 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:12 IST
=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.202 Prec@1=49.847 Prec@5=74.671 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=18:12 IST
=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.202 Prec@1=49.847 Prec@5=74.671 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=18:13 IST
=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.201 Prec@1=49.850 Prec@5=74.682 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=18:13 IST
=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.201 Prec@1=49.850 Prec@5=74.682 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:13 IST
=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.201 Prec@1=49.850 Prec@5=74.682 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:14 IST
=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.200 Prec@1=49.876 Prec@5=74.700 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:14 IST
=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.200 Prec@1=49.876 Prec@5=74.700 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=18:14 IST
=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.200 Prec@1=49.876 Prec@5=74.700 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=18:15 IST
=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.199 Prec@1=49.898 Prec@5=74.721 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=18:15 IST
=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.199 Prec@1=49.898 Prec@5=74.721 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:15 IST
=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.272 Loss=2.199 Prec@1=49.898 Prec@5=74.721 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:15 IST
=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.198 Prec@1=49.914 Prec@5=74.736 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:15 IST
=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.198 Prec@1=49.914 Prec@5=74.736 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=18:15 IST
=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.468 DataTime=0.271 Loss=2.198 Prec@1=49.914 Prec@5=74.736 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=18:16 IST
=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.197 Prec@1=49.924 Prec@5=74.753 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=18:16 IST
=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.197 Prec@1=49.924 Prec@5=74.753 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=18:16 IST
=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.197 Prec@1=49.924 Prec@5=74.753 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=18:17 IST
=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.195 Prec@1=49.953 Prec@5=74.788 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=18:17 IST
=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.195 Prec@1=49.953 Prec@5=74.788 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=18:17 IST
=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.195 Prec@1=49.953 Prec@5=74.788 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=18:17 IST
=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.195 Prec@1=49.953 Prec@5=74.788 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=18:17 IST
=> training   100.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.467 DataTime=0.271 Loss=2.195 Prec@1=49.953 Prec@5=74.788 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=18:17 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:17 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:17 IST
=> validation 0.00% of 1x98...Epoch=8/150 LR=0.09946 Time=6.932 Loss=1.453 Prec@1=63.477 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=18:17 IST
=> validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=6.932 Loss=1.453 Prec@1=63.477 Prec@5=87.109 rate=6866.06 Hz, eta=0:00:00, total=0:00:00, wall=18:17 IST
** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=6.932 Loss=1.453 Prec@1=63.477 Prec@5=87.109 rate=6866.06 Hz, eta=0:00:00, total=0:00:00, wall=18:18 IST
** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=0.546 Loss=2.150 Prec@1=50.326 Prec@5=75.980 rate=6866.06 Hz, eta=0:00:00, total=0:00:00, wall=18:18 IST
** validation 100.00% of 1x98...Epoch=8/150 LR=0.09946 Time=0.546 Loss=2.150 Prec@1=50.326 Prec@5=75.980 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=18:18 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> training   0.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=4.919 DataTime=4.655 Loss=2.151 Prec@1=52.148 Prec@5=74.219 rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=4.919 DataTime=4.655 Loss=2.151 Prec@1=52.148 Prec@5=74.219 rate=7693.73 Hz, eta=0:00:00, total=0:00:00, wall=18:18 IST
=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=4.919 DataTime=4.655 Loss=2.151 Prec@1=52.148 Prec@5=74.219 rate=7693.73 Hz, eta=0:00:00, total=0:00:00, wall=18:19 IST
=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.498 DataTime=0.300 Loss=2.129 Prec@1=51.228 Prec@5=75.870 rate=7693.73 Hz, eta=0:00:00, total=0:00:00, wall=18:19 IST
=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.498 DataTime=0.300 Loss=2.129 Prec@1=51.228 Prec@5=75.870 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=18:19 IST
=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.498 DataTime=0.300 Loss=2.129 Prec@1=51.228 Prec@5=75.870 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=18:20 IST
=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.482 DataTime=0.285 Loss=2.129 Prec@1=51.311 Prec@5=75.806 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=18:20 IST
=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.482 DataTime=0.285 Loss=2.129 Prec@1=51.311 Prec@5=75.806 rate=2.18 Hz, eta=0:17:33, total=0:01:31, wall=18:20 IST
=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.482 DataTime=0.285 Loss=2.129 Prec@1=51.311 Prec@5=75.806 rate=2.18 Hz, eta=0:17:33, total=0:01:31, wall=18:20 IST
=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.479 DataTime=0.282 Loss=2.123 Prec@1=51.377 Prec@5=75.899 rate=2.18 Hz, eta=0:17:33, total=0:01:31, wall=18:20 IST
=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.479 DataTime=0.282 Loss=2.123 Prec@1=51.377 Prec@5=75.899 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=18:20 IST
=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.479 DataTime=0.282 Loss=2.123 Prec@1=51.377 Prec@5=75.899 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=18:21 IST
=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.478 DataTime=0.282 Loss=2.123 Prec@1=51.427 Prec@5=75.887 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=18:21 IST
=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.478 DataTime=0.282 Loss=2.123 Prec@1=51.427 Prec@5=75.887 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=18:21 IST
=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.478 DataTime=0.282 Loss=2.123 Prec@1=51.427 Prec@5=75.887 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=18:22 IST
=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.475 DataTime=0.279 Loss=2.128 Prec@1=51.293 Prec@5=75.796 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=18:22 IST
=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.475 DataTime=0.279 Loss=2.128 Prec@1=51.293 Prec@5=75.796 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=18:22 IST
=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.475 DataTime=0.279 Loss=2.128 Prec@1=51.293 Prec@5=75.796 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=18:23 IST
=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.473 DataTime=0.277 Loss=2.127 Prec@1=51.277 Prec@5=75.824 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=18:23 IST
=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.473 DataTime=0.277 Loss=2.127 Prec@1=51.277 Prec@5=75.824 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=18:23 IST
=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.473 DataTime=0.277 Loss=2.127 Prec@1=51.277 Prec@5=75.824 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=18:23 IST
=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.472 DataTime=0.276 Loss=2.128 Prec@1=51.242 Prec@5=75.835 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=18:23 IST
=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.472 DataTime=0.276 Loss=2.128 Prec@1=51.242 Prec@5=75.835 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=18:23 IST
=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.472 DataTime=0.276 Loss=2.128 Prec@1=51.242 Prec@5=75.835 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=18:24 IST
=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.125 Prec@1=51.274 Prec@5=75.874 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=18:24 IST
=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.125 Prec@1=51.274 Prec@5=75.874 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=18:24 IST
=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.125 Prec@1=51.274 Prec@5=75.874 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=18:25 IST
=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.252 Prec@5=75.883 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=18:25 IST
=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.252 Prec@5=75.883 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=18:25 IST
=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.252 Prec@5=75.883 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=18:26 IST
=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.126 Prec@1=51.238 Prec@5=75.887 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=18:26 IST
=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.126 Prec@1=51.238 Prec@5=75.887 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=18:26 IST
=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.276 Loss=2.126 Prec@1=51.238 Prec@5=75.887 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=18:27 IST
=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.126 Prec@1=51.224 Prec@5=75.890 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=18:27 IST
=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.126 Prec@1=51.224 Prec@5=75.890 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=18:27 IST
=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.126 Prec@1=51.224 Prec@5=75.890 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=18:27 IST
=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.253 Prec@5=75.900 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=18:27 IST
=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.253 Prec@5=75.900 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=18:27 IST
=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.125 Prec@1=51.253 Prec@5=75.900 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=18:28 IST
=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.248 Prec@5=75.907 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=18:28 IST
=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.248 Prec@5=75.907 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=18:28 IST
=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.248 Prec@5=75.907 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=18:29 IST
=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.280 Prec@5=75.906 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=18:29 IST
=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.280 Prec@5=75.906 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:29 IST
=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.280 Prec@5=75.906 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:30 IST
=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.311 Prec@5=75.923 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:30 IST
=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.311 Prec@5=75.923 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:30 IST
=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.311 Prec@5=75.923 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:30 IST
=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.329 Prec@5=75.921 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:30 IST
=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.329 Prec@5=75.921 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:30 IST
=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.123 Prec@1=51.329 Prec@5=75.921 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:31 IST
=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.320 Prec@5=75.909 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:31 IST
=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.320 Prec@5=75.909 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=18:31 IST
=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.471 DataTime=0.275 Loss=2.124 Prec@1=51.320 Prec@5=75.909 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=18:32 IST
=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.275 Loss=2.123 Prec@1=51.345 Prec@5=75.933 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=18:32 IST
=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.275 Loss=2.123 Prec@1=51.345 Prec@5=75.933 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=18:32 IST
=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.275 Loss=2.123 Prec@1=51.345 Prec@5=75.933 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=18:33 IST
=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.348 Prec@5=75.939 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=18:33 IST
=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.348 Prec@5=75.939 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=18:33 IST
=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.348 Prec@5=75.939 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=18:34 IST
=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.367 Prec@5=75.938 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=18:34 IST
=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.367 Prec@5=75.938 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:34 IST
=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.470 DataTime=0.274 Loss=2.122 Prec@1=51.367 Prec@5=75.938 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:34 IST
=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.379 Prec@5=75.949 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:34 IST
=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.379 Prec@5=75.949 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:34 IST
=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.379 Prec@5=75.949 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:35 IST
=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.950 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:35 IST
=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.950 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:35 IST
=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.950 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:36 IST
=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.961 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:36 IST
=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.961 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=18:36 IST
=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.121 Prec@1=51.408 Prec@5=75.961 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=18:37 IST
=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.119 Prec@1=51.438 Prec@5=75.985 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=18:37 IST
=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.119 Prec@1=51.438 Prec@5=75.985 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=18:37 IST
=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.274 Loss=2.119 Prec@1=51.438 Prec@5=75.985 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=18:37 IST
=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.273 Loss=2.118 Prec@1=51.465 Prec@5=76.007 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=18:37 IST
=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.273 Loss=2.118 Prec@1=51.465 Prec@5=76.007 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=18:37 IST
=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.273 Loss=2.118 Prec@1=51.465 Prec@5=76.007 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=18:37 IST
=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.273 Loss=2.118 Prec@1=51.464 Prec@5=76.006 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=18:37 IST
=> training   100.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.469 DataTime=0.273 Loss=2.118 Prec@1=51.464 Prec@5=76.006 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=18:37 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> validation 0.00% of 1x98...Epoch=9/150 LR=0.09930 Time=6.134 Loss=1.285 Prec@1=67.773 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.134 Loss=1.285 Prec@1=67.773 Prec@5=88.477 rate=6334.09 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST
** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.134 Loss=1.285 Prec@1=67.773 Prec@5=88.477 rate=6334.09 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST
** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=0.540 Loss=1.994 Prec@1=53.238 Prec@5=78.268 rate=6334.09 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST
** validation 100.00% of 1x98...Epoch=9/150 LR=0.09930 Time=0.540 Loss=1.994 Prec@1=53.238 Prec@5=78.268 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=18:38 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> training   0.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=4.705 DataTime=4.444 Loss=2.123 Prec@1=53.320 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=18:38 IST
=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=4.705 DataTime=4.444 Loss=2.123 Prec@1=53.320 Prec@5=77.148 rate=7441.09 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST
=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=4.705 DataTime=4.444 Loss=2.123 Prec@1=53.320 Prec@5=77.148 rate=7441.09 Hz, eta=0:00:00, total=0:00:00, wall=18:39 IST
=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.507 DataTime=0.313 Loss=2.039 Prec@1=52.885 Prec@5=77.377 rate=7441.09 Hz, eta=0:00:00, total=0:00:00, wall=18:39 IST
=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.507 DataTime=0.313 Loss=2.039 Prec@1=52.885 Prec@5=77.377 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=18:39 IST
=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.507 DataTime=0.313 Loss=2.039 Prec@1=52.885 Prec@5=77.377 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=18:40 IST
=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.486 DataTime=0.291 Loss=2.048 Prec@1=52.784 Prec@5=77.188 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=18:40 IST
=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.486 DataTime=0.291 Loss=2.048 Prec@1=52.784 Prec@5=77.188 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=18:40 IST
=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.486 DataTime=0.291 Loss=2.048 Prec@1=52.784 Prec@5=77.188 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=18:41 IST
=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.476 DataTime=0.281 Loss=2.049 Prec@1=52.733 Prec@5=77.172 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=18:41 IST
=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.476 DataTime=0.281 Loss=2.049 Prec@1=52.733 Prec@5=77.172 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=18:41 IST
=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.476 DataTime=0.281 Loss=2.049 Prec@1=52.733 Prec@5=77.172 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=18:42 IST
=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.277 Loss=2.047 Prec@1=52.796 Prec@5=77.230 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=18:42 IST
=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.277 Loss=2.047 Prec@1=52.796 Prec@5=77.230 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=18:42 IST
=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.277 Loss=2.047 Prec@1=52.796 Prec@5=77.230 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=18:42 IST
=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.052 Prec@1=52.734 Prec@5=77.112 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=18:42 IST
=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.052 Prec@1=52.734 Prec@5=77.112 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:42 IST
=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.052 Prec@1=52.734 Prec@5=77.112 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:43 IST
=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.693 Prec@5=77.021 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:43 IST
=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.693 Prec@5=77.021 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:43 IST
=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.693 Prec@5=77.021 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:44 IST
=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.666 Prec@5=77.004 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:44 IST
=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.666 Prec@5=77.004 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=18:44 IST
=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.666 Prec@5=77.004 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=18:45 IST
=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.670 Prec@5=77.014 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=18:45 IST
=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.670 Prec@5=77.014 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=18:45 IST
=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.670 Prec@5=77.014 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=18:45 IST
=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.056 Prec@1=52.653 Prec@5=77.012 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=18:45 IST
=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.056 Prec@1=52.653 Prec@5=77.012 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=18:45 IST
=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.056 Prec@1=52.653 Prec@5=77.012 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=18:46 IST
=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.676 Prec@5=77.004 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=18:46 IST
=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.676 Prec@5=77.004 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=18:46 IST
=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.676 Prec@5=77.004 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=18:47 IST
=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.275 Loss=2.056 Prec@1=52.661 Prec@5=77.009 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=18:47 IST
=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.275 Loss=2.056 Prec@1=52.661 Prec@5=77.009 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=18:47 IST
=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.275 Loss=2.056 Prec@1=52.661 Prec@5=77.009 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=18:48 IST
=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.057 Prec@1=52.646 Prec@5=76.986 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=18:48 IST
=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.057 Prec@1=52.646 Prec@5=76.986 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=18:48 IST
=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.472 DataTime=0.276 Loss=2.057 Prec@1=52.646 Prec@5=76.986 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=18:49 IST
=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.658 Prec@5=77.006 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=18:49 IST
=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.658 Prec@5=77.006 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=18:49 IST
=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.658 Prec@5=77.006 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=18:49 IST
=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.650 Prec@5=76.998 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=18:49 IST
=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.650 Prec@5=76.998 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:49 IST
=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.650 Prec@5=76.998 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:50 IST
=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.641 Prec@5=76.985 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=18:50 IST
=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.641 Prec@5=76.985 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:50 IST
=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.057 Prec@1=52.641 Prec@5=76.985 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:51 IST
=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.652 Prec@5=76.993 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=18:51 IST
=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.652 Prec@5=76.993 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:51 IST
=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.057 Prec@1=52.652 Prec@5=76.993 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:52 IST
=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.673 Prec@5=76.994 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=18:52 IST
=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.673 Prec@5=76.994 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=18:52 IST
=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.673 Prec@5=76.994 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=18:53 IST
=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.671 Prec@5=76.998 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=18:53 IST
=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.671 Prec@5=76.998 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=18:53 IST
=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.671 Prec@5=76.998 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=18:53 IST
=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.666 Prec@5=76.997 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=18:53 IST
=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.666 Prec@5=76.997 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=18:53 IST
=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.666 Prec@5=76.997 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=18:54 IST
=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.659 Prec@5=76.998 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=18:54 IST
=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.659 Prec@5=76.998 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:54 IST
=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.659 Prec@5=76.998 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:55 IST
=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.664 Prec@5=77.005 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=18:55 IST
=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.664 Prec@5=77.005 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:55 IST
=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.056 Prec@1=52.664 Prec@5=77.005 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:56 IST
=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.660 Prec@5=76.999 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=18:56 IST
=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.660 Prec@5=76.999 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:56 IST
=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.273 Loss=2.056 Prec@1=52.660 Prec@5=76.999 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:56 IST
=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.055 Prec@1=52.670 Prec@5=77.010 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=18:56 IST
=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.055 Prec@1=52.670 Prec@5=77.010 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=18:56 IST
=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.469 DataTime=0.274 Loss=2.055 Prec@1=52.670 Prec@5=77.010 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=18:57 IST
=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.055 Prec@1=52.678 Prec@5=77.017 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=18:57 IST
=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.055 Prec@1=52.678 Prec@5=77.017 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=18:57 IST
=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.470 DataTime=0.274 Loss=2.055 Prec@1=52.678 Prec@5=77.017 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=18:58 IST
=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.055 Prec@1=52.678 Prec@5=77.018 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=18:58 IST
=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.055 Prec@1=52.678 Prec@5=77.018 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=18:58 IST
=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.055 Prec@1=52.678 Prec@5=77.018 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=18:58 IST
=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.055 Prec@1=52.677 Prec@5=77.017 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=18:58 IST
=> training   100.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.471 DataTime=0.275 Loss=2.055 Prec@1=52.677 Prec@5=77.017 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=18:58 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:58 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:58 IST
=> validation 0.00% of 1x98...Epoch=10/150 LR=0.09911 Time=6.772 Loss=1.229 Prec@1=68.750 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=18:58 IST
=> validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=6.772 Loss=1.229 Prec@1=68.750 Prec@5=89.258 rate=8611.93 Hz, eta=0:00:00, total=0:00:00, wall=18:58 IST
** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=6.772 Loss=1.229 Prec@1=68.750 Prec@5=89.258 rate=8611.93 Hz, eta=0:00:00, total=0:00:00, wall=18:59 IST
** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=0.553 Loss=1.924 Prec@1=55.112 Prec@5=79.270 rate=8611.93 Hz, eta=0:00:00, total=0:00:00, wall=18:59 IST
** validation 100.00% of 1x98...Epoch=10/150 LR=0.09911 Time=0.553 Loss=1.924 Prec@1=55.112 Prec@5=79.270 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=18:59 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:59 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:59 IST
=> training   0.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=4.550 DataTime=4.269 Loss=1.883 Prec@1=56.836 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=18:59 IST
=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=4.550 DataTime=4.269 Loss=1.883 Prec@1=56.836 Prec@5=80.469 rate=5492.75 Hz, eta=0:00:00, total=0:00:00, wall=18:59 IST
=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=4.550 DataTime=4.269 Loss=1.883 Prec@1=56.836 Prec@5=80.469 rate=5492.75 Hz, eta=0:00:00, total=0:00:00, wall=19:00 IST
=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.514 DataTime=0.320 Loss=1.985 Prec@1=54.131 Prec@5=78.001 rate=5492.75 Hz, eta=0:00:00, total=0:00:00, wall=19:00 IST
=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.514 DataTime=0.320 Loss=1.985 Prec@1=54.131 Prec@5=78.001 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=19:00 IST
=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.514 DataTime=0.320 Loss=1.985 Prec@1=54.131 Prec@5=78.001 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=19:01 IST
=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.500 DataTime=0.306 Loss=1.989 Prec@1=53.943 Prec@5=78.027 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=19:01 IST
=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.500 DataTime=0.306 Loss=1.989 Prec@1=53.943 Prec@5=78.027 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=19:01 IST
=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.500 DataTime=0.306 Loss=1.989 Prec@1=53.943 Prec@5=78.027 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=19:01 IST
=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.489 DataTime=0.294 Loss=1.992 Prec@1=53.981 Prec@5=77.945 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=19:01 IST
=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.489 DataTime=0.294 Loss=1.992 Prec@1=53.981 Prec@5=77.945 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=19:01 IST
=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.489 DataTime=0.294 Loss=1.992 Prec@1=53.981 Prec@5=77.945 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=19:02 IST
=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.482 DataTime=0.287 Loss=1.995 Prec@1=53.922 Prec@5=77.867 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=19:02 IST
=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.482 DataTime=0.287 Loss=1.995 Prec@1=53.922 Prec@5=77.867 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=19:02 IST
=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.482 DataTime=0.287 Loss=1.995 Prec@1=53.922 Prec@5=77.867 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=19:03 IST
=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.480 DataTime=0.284 Loss=1.997 Prec@1=53.831 Prec@5=77.853 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=19:03 IST
=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.480 DataTime=0.284 Loss=1.997 Prec@1=53.831 Prec@5=77.853 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=19:03 IST
=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.480 DataTime=0.284 Loss=1.997 Prec@1=53.831 Prec@5=77.853 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=19:04 IST
=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.750 Prec@5=77.816 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=19:04 IST
=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.750 Prec@5=77.816 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=19:04 IST
=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.750 Prec@5=77.816 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=19:05 IST
=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.479 DataTime=0.283 Loss=2.000 Prec@1=53.761 Prec@5=77.820 rate=2.12 Hz, eta=0:14:55, total=0:04:42, wall=19:05 IST
=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.479 DataTime=0.283 Loss=2.000 Prec@1=53.761 Prec@5=77.820 rate=2.12 Hz, eta=0:14:10, total=0:05:31, wall=19:05 IST
=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.479 DataTime=0.283 Loss=2.000 Prec@1=53.761 Prec@5=77.820 rate=2.12 Hz, eta=0:14:10, total=0:05:31, wall=19:05 IST
=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.725 Prec@5=77.818 rate=2.12 Hz, eta=0:14:10, total=0:05:31, wall=19:05 IST
=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.725 Prec@5=77.818 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=19:05 IST
=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.478 DataTime=0.283 Loss=2.001 Prec@1=53.725 Prec@5=77.818 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=19:06 IST
=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.003 Prec@1=53.693 Prec@5=77.801 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=19:06 IST
=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.003 Prec@1=53.693 Prec@5=77.801 rate=2.12 Hz, eta=0:12:36, total=0:07:05, wall=19:06 IST
=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.003 Prec@1=53.693 Prec@5=77.801 rate=2.12 Hz, eta=0:12:36, total=0:07:05, wall=19:07 IST
=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.005 Prec@1=53.695 Prec@5=77.792 rate=2.12 Hz, eta=0:12:36, total=0:07:05, wall=19:07 IST
=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.005 Prec@1=53.695 Prec@5=77.792 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=19:07 IST
=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.005 Prec@1=53.695 Prec@5=77.792 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=19:08 IST
=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.005 Prec@1=53.681 Prec@5=77.791 rate=2.12 Hz, eta=0:11:49, total=0:07:52, wall=19:08 IST
=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.005 Prec@1=53.681 Prec@5=77.791 rate=2.11 Hz, eta=0:11:02, total=0:08:40, wall=19:08 IST
=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.282 Loss=2.005 Prec@1=53.681 Prec@5=77.791 rate=2.11 Hz, eta=0:11:02, total=0:08:40, wall=19:09 IST
=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.004 Prec@1=53.660 Prec@5=77.784 rate=2.11 Hz, eta=0:11:02, total=0:08:40, wall=19:09 IST
=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.004 Prec@1=53.660 Prec@5=77.784 rate=2.11 Hz, eta=0:10:15, total=0:09:27, wall=19:09 IST
=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.004 Prec@1=53.660 Prec@5=77.784 rate=2.11 Hz, eta=0:10:15, total=0:09:27, wall=19:09 IST
=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.003 Prec@1=53.693 Prec@5=77.803 rate=2.11 Hz, eta=0:10:15, total=0:09:27, wall=19:09 IST
=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.003 Prec@1=53.693 Prec@5=77.803 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=19:09 IST
=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.477 DataTime=0.281 Loss=2.003 Prec@1=53.693 Prec@5=77.803 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=19:10 IST
=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.695 Prec@5=77.800 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=19:10 IST
=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.695 Prec@5=77.800 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=19:10 IST
=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.695 Prec@5=77.800 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=19:11 IST
=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.710 Prec@5=77.798 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=19:11 IST
=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.710 Prec@5=77.798 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=19:11 IST
=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.710 Prec@5=77.798 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=19:12 IST
=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.733 Prec@5=77.810 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=19:12 IST
=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.733 Prec@5=77.810 rate=2.11 Hz, eta=0:07:06, total=0:12:37, wall=19:12 IST
=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.733 Prec@5=77.810 rate=2.11 Hz, eta=0:07:06, total=0:12:37, wall=19:12 IST
=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.716 Prec@5=77.815 rate=2.11 Hz, eta=0:07:06, total=0:12:37, wall=19:12 IST
=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.716 Prec@5=77.815 rate=2.12 Hz, eta=0:06:19, total=0:13:23, wall=19:12 IST
=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.716 Prec@5=77.815 rate=2.12 Hz, eta=0:06:19, total=0:13:23, wall=19:13 IST
=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.002 Prec@1=53.720 Prec@5=77.820 rate=2.12 Hz, eta=0:06:19, total=0:13:23, wall=19:13 IST
=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.002 Prec@1=53.720 Prec@5=77.820 rate=2.11 Hz, eta=0:05:32, total=0:14:12, wall=19:13 IST
=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.002 Prec@1=53.720 Prec@5=77.820 rate=2.11 Hz, eta=0:05:32, total=0:14:12, wall=19:14 IST
=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.695 Prec@5=77.818 rate=2.11 Hz, eta=0:05:32, total=0:14:12, wall=19:14 IST
=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.695 Prec@5=77.818 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=19:14 IST
=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.280 Loss=2.003 Prec@1=53.695 Prec@5=77.818 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=19:15 IST
=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.673 Prec@5=77.805 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=19:15 IST
=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.673 Prec@5=77.805 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=19:15 IST
=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.003 Prec@1=53.673 Prec@5=77.805 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=19:16 IST
=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.684 Prec@5=77.801 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=19:16 IST
=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.684 Prec@5=77.801 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=19:16 IST
=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.684 Prec@5=77.801 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=19:16 IST
=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.803 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=19:16 IST
=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.803 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=19:16 IST
=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.803 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=19:17 IST
=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.795 rate=2.11 Hz, eta=0:02:23, total=0:17:22, wall=19:17 IST
=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.795 rate=2.11 Hz, eta=0:01:35, total=0:18:09, wall=19:17 IST
=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.476 DataTime=0.281 Loss=2.004 Prec@1=53.687 Prec@5=77.795 rate=2.11 Hz, eta=0:01:35, total=0:18:09, wall=19:18 IST
=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.688 Prec@5=77.782 rate=2.11 Hz, eta=0:01:35, total=0:18:09, wall=19:18 IST
=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.688 Prec@5=77.782 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=19:18 IST
=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.688 Prec@5=77.782 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=19:19 IST
=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.710 Prec@5=77.794 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=19:19 IST
=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.710 Prec@5=77.794 rate=2.11 Hz, eta=0:00:00, total=0:19:44, wall=19:19 IST
=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.710 Prec@5=77.794 rate=2.11 Hz, eta=0:00:00, total=0:19:44, wall=19:19 IST
=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.710 Prec@5=77.795 rate=2.11 Hz, eta=0:00:00, total=0:19:44, wall=19:19 IST
=> training   100.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.475 DataTime=0.281 Loss=2.004 Prec@1=53.710 Prec@5=77.795 rate=2.11 Hz, eta=0:00:00, total=0:19:44, wall=19:19 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> validation 0.00% of 1x98...Epoch=11/150 LR=0.09891 Time=6.045 Loss=1.183 Prec@1=68.555 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.045 Loss=1.183 Prec@1=68.555 Prec@5=90.039 rate=8480.18 Hz, eta=0:00:00, total=0:00:00, wall=19:19 IST
** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.045 Loss=1.183 Prec@1=68.555 Prec@5=90.039 rate=8480.18 Hz, eta=0:00:00, total=0:00:00, wall=19:20 IST
** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=0.543 Loss=1.929 Prec@1=54.610 Prec@5=79.124 rate=8480.18 Hz, eta=0:00:00, total=0:00:00, wall=19:20 IST
** validation 100.00% of 1x98...Epoch=11/150 LR=0.09891 Time=0.543 Loss=1.929 Prec@1=54.610 Prec@5=79.124 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=19:20 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:20 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:20 IST
=> training   0.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.822 DataTime=4.591 Loss=1.832 Prec@1=59.766 Prec@5=78.906 rate=0 Hz, eta=?, total=0:00:00, wall=19:20 IST
=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.822 DataTime=4.591 Loss=1.832 Prec@1=59.766 Prec@5=78.906 rate=9076.72 Hz, eta=0:00:00, total=0:00:00, wall=19:20 IST
=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.822 DataTime=4.591 Loss=1.832 Prec@1=59.766 Prec@5=78.906 rate=9076.72 Hz, eta=0:00:00, total=0:00:00, wall=19:21 IST
=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.505 DataTime=0.311 Loss=1.948 Prec@1=54.771 Prec@5=78.744 rate=9076.72 Hz, eta=0:00:00, total=0:00:00, wall=19:21 IST
=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.505 DataTime=0.311 Loss=1.948 Prec@1=54.771 Prec@5=78.744 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=19:21 IST
=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.505 DataTime=0.311 Loss=1.948 Prec@1=54.771 Prec@5=78.744 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=19:21 IST
=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.489 DataTime=0.296 Loss=1.950 Prec@1=54.798 Prec@5=78.669 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=19:21 IST
=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.489 DataTime=0.296 Loss=1.950 Prec@1=54.798 Prec@5=78.669 rate=2.15 Hz, eta=0:17:49, total=0:01:33, wall=19:21 IST
=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.489 DataTime=0.296 Loss=1.950 Prec@1=54.798 Prec@5=78.669 rate=2.15 Hz, eta=0:17:49, total=0:01:33, wall=19:22 IST
=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.479 DataTime=0.285 Loss=1.956 Prec@1=54.659 Prec@5=78.581 rate=2.15 Hz, eta=0:17:49, total=0:01:33, wall=19:22 IST
=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.479 DataTime=0.285 Loss=1.956 Prec@1=54.659 Prec@5=78.581 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:22 IST
=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.479 DataTime=0.285 Loss=1.956 Prec@1=54.659 Prec@5=78.581 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:23 IST
=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.474 DataTime=0.279 Loss=1.956 Prec@1=54.631 Prec@5=78.633 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:23 IST
=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.474 DataTime=0.279 Loss=1.956 Prec@1=54.631 Prec@5=78.633 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=19:23 IST
=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.474 DataTime=0.279 Loss=1.956 Prec@1=54.631 Prec@5=78.633 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=19:24 IST
=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.473 DataTime=0.279 Loss=1.953 Prec@1=54.661 Prec@5=78.674 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=19:24 IST
=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.473 DataTime=0.279 Loss=1.953 Prec@1=54.661 Prec@5=78.674 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:24 IST
=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.473 DataTime=0.279 Loss=1.953 Prec@1=54.661 Prec@5=78.674 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:24 IST
=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.472 DataTime=0.277 Loss=1.956 Prec@1=54.577 Prec@5=78.618 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:24 IST
=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.472 DataTime=0.277 Loss=1.956 Prec@1=54.577 Prec@5=78.618 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:24 IST
=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.472 DataTime=0.277 Loss=1.956 Prec@1=54.577 Prec@5=78.618 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:25 IST
=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.956 Prec@1=54.567 Prec@5=78.605 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:25 IST
=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.956 Prec@1=54.567 Prec@5=78.605 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=19:25 IST
=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.956 Prec@1=54.567 Prec@5=78.605 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=19:26 IST
=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.959 Prec@1=54.509 Prec@5=78.557 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=19:26 IST
=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.959 Prec@1=54.509 Prec@5=78.557 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=19:26 IST
=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.959 Prec@1=54.509 Prec@5=78.557 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=19:27 IST
=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.961 Prec@1=54.492 Prec@5=78.521 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=19:27 IST
=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.961 Prec@1=54.492 Prec@5=78.521 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:27 IST
=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.961 Prec@1=54.492 Prec@5=78.521 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:28 IST
=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.963 Prec@1=54.470 Prec@5=78.472 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:28 IST
=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.963 Prec@1=54.470 Prec@5=78.472 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=19:28 IST
=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.963 Prec@1=54.470 Prec@5=78.472 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=19:28 IST
=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.465 Prec@5=78.450 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=19:28 IST
=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.465 Prec@5=78.450 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=19:28 IST
=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.465 Prec@5=78.450 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=19:29 IST
=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.445 Prec@5=78.437 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=19:29 IST
=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.445 Prec@5=78.437 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=19:29 IST
=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.445 Prec@5=78.437 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=19:30 IST
=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.451 Prec@5=78.446 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=19:30 IST
=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.451 Prec@5=78.446 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=19:30 IST
=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.276 Loss=1.965 Prec@1=54.451 Prec@5=78.446 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=19:31 IST
=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.475 Prec@5=78.445 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=19:31 IST
=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.475 Prec@5=78.445 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=19:31 IST
=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.964 Prec@1=54.475 Prec@5=78.445 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=19:32 IST
=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.508 Prec@5=78.470 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=19:32 IST
=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.508 Prec@5=78.470 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=19:32 IST
=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.508 Prec@5=78.470 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=19:32 IST
=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.962 Prec@1=54.517 Prec@5=78.465 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=19:32 IST
=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.962 Prec@1=54.517 Prec@5=78.465 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=19:32 IST
=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.962 Prec@1=54.517 Prec@5=78.465 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=19:33 IST
=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.515 Prec@5=78.463 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=19:33 IST
=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.515 Prec@5=78.463 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=19:33 IST
=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.515 Prec@5=78.463 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=19:34 IST
=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.963 Prec@1=54.497 Prec@5=78.453 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=19:34 IST
=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.963 Prec@1=54.497 Prec@5=78.453 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=19:34 IST
=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.963 Prec@1=54.497 Prec@5=78.453 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=19:35 IST
=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.498 Prec@5=78.465 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=19:35 IST
=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.498 Prec@5=78.465 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:35 IST
=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.276 Loss=1.962 Prec@1=54.498 Prec@5=78.465 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:35 IST
=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.491 Prec@5=78.468 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:35 IST
=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.491 Prec@5=78.468 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=19:35 IST
=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.491 Prec@5=78.468 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=19:36 IST
=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.489 Prec@5=78.469 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=19:36 IST
=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.489 Prec@5=78.469 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=19:36 IST
=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.489 Prec@5=78.469 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=19:37 IST
=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.496 Prec@5=78.466 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=19:37 IST
=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.496 Prec@5=78.466 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=19:37 IST
=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.962 Prec@1=54.496 Prec@5=78.466 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=19:38 IST
=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.961 Prec@1=54.518 Prec@5=78.480 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=19:38 IST
=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.961 Prec@1=54.518 Prec@5=78.480 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=19:38 IST
=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.471 DataTime=0.277 Loss=1.961 Prec@1=54.518 Prec@5=78.480 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=19:39 IST
=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.528 Prec@5=78.487 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=19:39 IST
=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.528 Prec@5=78.487 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=19:39 IST
=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.528 Prec@5=78.487 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=19:39 IST
=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.520 Prec@5=78.480 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=19:39 IST
=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.520 Prec@5=78.480 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=19:39 IST
=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.520 Prec@5=78.480 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=19:39 IST
=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.518 Prec@5=78.479 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=19:39 IST
=> training   100.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.470 DataTime=0.277 Loss=1.961 Prec@1=54.518 Prec@5=78.479 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=19:39 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> validation 0.00% of 1x98...Epoch=12/150 LR=0.09868 Time=7.151 Loss=1.223 Prec@1=67.383 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=7.151 Loss=1.223 Prec@1=67.383 Prec@5=90.430 rate=7470.27 Hz, eta=0:00:00, total=0:00:00, wall=19:40 IST
** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=7.151 Loss=1.223 Prec@1=67.383 Prec@5=90.430 rate=7470.27 Hz, eta=0:00:00, total=0:00:00, wall=19:40 IST
** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=0.554 Loss=1.856 Prec@1=55.938 Prec@5=80.310 rate=7470.27 Hz, eta=0:00:00, total=0:00:00, wall=19:40 IST
** validation 100.00% of 1x98...Epoch=12/150 LR=0.09868 Time=0.554 Loss=1.856 Prec@1=55.938 Prec@5=80.310 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=19:40 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> training   0.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.090 DataTime=4.863 Loss=2.015 Prec@1=54.297 Prec@5=76.562 rate=0 Hz, eta=?, total=0:00:00, wall=19:40 IST
=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.090 DataTime=4.863 Loss=2.015 Prec@1=54.297 Prec@5=76.562 rate=5597.25 Hz, eta=0:00:00, total=0:00:00, wall=19:40 IST
=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.090 DataTime=4.863 Loss=2.015 Prec@1=54.297 Prec@5=76.562 rate=5597.25 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST
=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.498 DataTime=0.304 Loss=1.903 Prec@1=55.507 Prec@5=79.328 rate=5597.25 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST
=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.498 DataTime=0.304 Loss=1.903 Prec@1=55.507 Prec@5=79.328 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=19:41 IST
=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.498 DataTime=0.304 Loss=1.903 Prec@1=55.507 Prec@5=79.328 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=19:42 IST
=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.478 DataTime=0.284 Loss=1.895 Prec@1=55.791 Prec@5=79.481 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=19:42 IST
=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.478 DataTime=0.284 Loss=1.895 Prec@1=55.791 Prec@5=79.481 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=19:42 IST
=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.478 DataTime=0.284 Loss=1.895 Prec@1=55.791 Prec@5=79.481 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=19:43 IST
=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.472 DataTime=0.277 Loss=1.900 Prec@1=55.730 Prec@5=79.433 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=19:43 IST
=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.472 DataTime=0.277 Loss=1.900 Prec@1=55.730 Prec@5=79.433 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=19:43 IST
=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.472 DataTime=0.277 Loss=1.900 Prec@1=55.730 Prec@5=79.433 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=19:43 IST
=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.470 DataTime=0.276 Loss=1.904 Prec@1=55.661 Prec@5=79.381 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=19:43 IST
=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.470 DataTime=0.276 Loss=1.904 Prec@1=55.661 Prec@5=79.381 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:43 IST
=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.470 DataTime=0.276 Loss=1.904 Prec@1=55.661 Prec@5=79.381 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:44 IST
=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.468 DataTime=0.274 Loss=1.908 Prec@1=55.603 Prec@5=79.321 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:44 IST
=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.468 DataTime=0.274 Loss=1.908 Prec@1=55.603 Prec@5=79.321 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=19:44 IST
=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.468 DataTime=0.274 Loss=1.908 Prec@1=55.603 Prec@5=79.321 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=19:45 IST
=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.911 Prec@1=55.561 Prec@5=79.262 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=19:45 IST
=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.911 Prec@1=55.561 Prec@5=79.262 rate=2.19 Hz, eta=0:14:30, total=0:04:34, wall=19:45 IST
=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.911 Prec@1=55.561 Prec@5=79.262 rate=2.19 Hz, eta=0:14:30, total=0:04:34, wall=19:46 IST
=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.911 Prec@1=55.559 Prec@5=79.218 rate=2.19 Hz, eta=0:14:30, total=0:04:34, wall=19:46 IST
=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.911 Prec@1=55.559 Prec@5=79.218 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=19:46 IST
=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.911 Prec@1=55.559 Prec@5=79.218 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=19:47 IST
=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.912 Prec@1=55.519 Prec@5=79.192 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=19:47 IST
=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.912 Prec@1=55.519 Prec@5=79.192 rate=2.18 Hz, eta=0:13:01, total=0:06:08, wall=19:47 IST
=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.912 Prec@1=55.519 Prec@5=79.192 rate=2.18 Hz, eta=0:13:01, total=0:06:08, wall=19:47 IST
=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.913 Prec@1=55.487 Prec@5=79.187 rate=2.18 Hz, eta=0:13:01, total=0:06:08, wall=19:47 IST
=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.913 Prec@1=55.487 Prec@5=79.187 rate=2.17 Hz, eta=0:12:18, total=0:06:55, wall=19:47 IST
=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.272 Loss=1.913 Prec@1=55.487 Prec@5=79.187 rate=2.17 Hz, eta=0:12:18, total=0:06:55, wall=19:48 IST
=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.914 Prec@1=55.448 Prec@5=79.178 rate=2.17 Hz, eta=0:12:18, total=0:06:55, wall=19:48 IST
=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.914 Prec@1=55.448 Prec@5=79.178 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=19:48 IST
=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.914 Prec@1=55.448 Prec@5=79.178 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=19:49 IST
=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.915 Prec@1=55.466 Prec@5=79.176 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=19:49 IST
=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.915 Prec@1=55.466 Prec@5=79.176 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=19:49 IST
=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.915 Prec@1=55.466 Prec@5=79.176 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=19:50 IST
=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.915 Prec@1=55.473 Prec@5=79.176 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=19:50 IST
=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.915 Prec@1=55.473 Prec@5=79.176 rate=2.17 Hz, eta=0:10:00, total=0:09:14, wall=19:50 IST
=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.466 DataTime=0.271 Loss=1.915 Prec@1=55.473 Prec@5=79.176 rate=2.17 Hz, eta=0:10:00, total=0:09:14, wall=19:50 IST
=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.271 Loss=1.916 Prec@1=55.441 Prec@5=79.164 rate=2.17 Hz, eta=0:10:00, total=0:09:14, wall=19:50 IST
=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.271 Loss=1.916 Prec@1=55.441 Prec@5=79.164 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=19:50 IST
=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.271 Loss=1.916 Prec@1=55.441 Prec@5=79.164 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=19:51 IST
=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.916 Prec@1=55.434 Prec@5=79.153 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=19:51 IST
=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.916 Prec@1=55.434 Prec@5=79.153 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=19:51 IST
=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.916 Prec@1=55.434 Prec@5=79.153 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=19:52 IST
=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.918 Prec@1=55.414 Prec@5=79.134 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=19:52 IST
=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.918 Prec@1=55.414 Prec@5=79.134 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=19:52 IST
=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.918 Prec@1=55.414 Prec@5=79.134 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=19:53 IST
=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.371 Prec@5=79.118 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=19:53 IST
=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.371 Prec@5=79.118 rate=2.17 Hz, eta=0:06:56, total=0:12:18, wall=19:53 IST
=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.371 Prec@5=79.118 rate=2.17 Hz, eta=0:06:56, total=0:12:18, wall=19:54 IST
=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.345 Prec@5=79.103 rate=2.17 Hz, eta=0:06:56, total=0:12:18, wall=19:54 IST
=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.345 Prec@5=79.103 rate=2.16 Hz, eta=0:06:10, total=0:13:05, wall=19:54 IST
=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.345 Prec@5=79.103 rate=2.16 Hz, eta=0:06:10, total=0:13:05, wall=19:54 IST
=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.334 Prec@5=79.110 rate=2.16 Hz, eta=0:06:10, total=0:13:05, wall=19:54 IST
=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.334 Prec@5=79.110 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=19:54 IST
=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.919 Prec@1=55.334 Prec@5=79.110 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=19:55 IST
=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.920 Prec@1=55.325 Prec@5=79.105 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=19:55 IST
=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.920 Prec@1=55.325 Prec@5=79.105 rate=2.16 Hz, eta=0:04:38, total=0:14:38, wall=19:55 IST
=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.920 Prec@1=55.325 Prec@5=79.105 rate=2.16 Hz, eta=0:04:38, total=0:14:38, wall=19:56 IST
=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.306 Prec@5=79.083 rate=2.16 Hz, eta=0:04:38, total=0:14:38, wall=19:56 IST
=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.306 Prec@5=79.083 rate=2.16 Hz, eta=0:03:51, total=0:15:24, wall=19:56 IST
=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.306 Prec@5=79.083 rate=2.16 Hz, eta=0:03:51, total=0:15:24, wall=19:57 IST
=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.305 Prec@5=79.084 rate=2.16 Hz, eta=0:03:51, total=0:15:24, wall=19:57 IST
=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.305 Prec@5=79.084 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=19:57 IST
=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.921 Prec@1=55.305 Prec@5=79.084 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=19:57 IST
=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.922 Prec@1=55.296 Prec@5=79.073 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=19:57 IST
=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.922 Prec@1=55.296 Prec@5=79.073 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=19:57 IST
=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.465 DataTime=0.270 Loss=1.922 Prec@1=55.296 Prec@5=79.073 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=19:58 IST
=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.294 Prec@5=79.074 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=19:58 IST
=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.294 Prec@5=79.074 rate=2.16 Hz, eta=0:01:33, total=0:17:42, wall=19:58 IST
=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.294 Prec@5=79.074 rate=2.16 Hz, eta=0:01:33, total=0:17:42, wall=19:59 IST
=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.296 Prec@5=79.065 rate=2.16 Hz, eta=0:01:33, total=0:17:42, wall=19:59 IST
=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.296 Prec@5=79.065 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=19:59 IST
=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.296 Prec@5=79.065 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=20:00 IST
=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.312 Prec@5=79.057 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=20:00 IST
=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.312 Prec@5=79.057 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=20:00 IST
=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.312 Prec@5=79.057 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=20:00 IST
=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.313 Prec@5=79.058 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=20:00 IST
=> training   100.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.464 DataTime=0.269 Loss=1.922 Prec@1=55.313 Prec@5=79.058 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=20:00 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:00 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:00 IST
=> validation 0.00% of 1x98...Epoch=13/150 LR=0.09843 Time=6.569 Loss=1.188 Prec@1=68.945 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=20:00 IST
=> validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.569 Loss=1.188 Prec@1=68.945 Prec@5=90.430 rate=3418.54 Hz, eta=0:00:00, total=0:00:00, wall=20:00 IST
** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.569 Loss=1.188 Prec@1=68.945 Prec@5=90.430 rate=3418.54 Hz, eta=0:00:00, total=0:00:00, wall=20:01 IST
** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=0.544 Loss=1.836 Prec@1=56.764 Prec@5=80.774 rate=3418.54 Hz, eta=0:00:00, total=0:00:00, wall=20:01 IST
** validation 100.00% of 1x98...Epoch=13/150 LR=0.09843 Time=0.544 Loss=1.836 Prec@1=56.764 Prec@5=80.774 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=20:01 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:01 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:01 IST
=> training   0.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.223 DataTime=4.971 Loss=1.944 Prec@1=54.688 Prec@5=80.078 rate=0 Hz, eta=?, total=0:00:00, wall=20:01 IST
=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.223 DataTime=4.971 Loss=1.944 Prec@1=54.688 Prec@5=80.078 rate=7486.94 Hz, eta=0:00:00, total=0:00:00, wall=20:01 IST
=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.223 DataTime=4.971 Loss=1.944 Prec@1=54.688 Prec@5=80.078 rate=7486.94 Hz, eta=0:00:00, total=0:00:00, wall=20:01 IST
=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.501 DataTime=0.308 Loss=1.867 Prec@1=56.260 Prec@5=79.962 rate=7486.94 Hz, eta=0:00:00, total=0:00:00, wall=20:01 IST
=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.501 DataTime=0.308 Loss=1.867 Prec@1=56.260 Prec@5=79.962 rate=2.22 Hz, eta=0:17:59, total=0:00:45, wall=20:01 IST
=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.501 DataTime=0.308 Loss=1.867 Prec@1=56.260 Prec@5=79.962 rate=2.22 Hz, eta=0:17:59, total=0:00:45, wall=20:02 IST
=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.484 DataTime=0.289 Loss=1.871 Prec@1=56.255 Prec@5=79.864 rate=2.22 Hz, eta=0:17:59, total=0:00:45, wall=20:02 IST
=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.484 DataTime=0.289 Loss=1.871 Prec@1=56.255 Prec@5=79.864 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=20:02 IST
=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.484 DataTime=0.289 Loss=1.871 Prec@1=56.255 Prec@5=79.864 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=20:03 IST
=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.476 DataTime=0.281 Loss=1.867 Prec@1=56.256 Prec@5=79.886 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=20:03 IST
=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.476 DataTime=0.281 Loss=1.867 Prec@1=56.256 Prec@5=79.886 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=20:03 IST
=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.476 DataTime=0.281 Loss=1.867 Prec@1=56.256 Prec@5=79.886 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=20:04 IST
=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.471 DataTime=0.276 Loss=1.872 Prec@1=56.175 Prec@5=79.831 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=20:04 IST
=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.471 DataTime=0.276 Loss=1.872 Prec@1=56.175 Prec@5=79.831 rate=2.18 Hz, eta=0:16:03, total=0:03:03, wall=20:04 IST
=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.471 DataTime=0.276 Loss=1.872 Prec@1=56.175 Prec@5=79.831 rate=2.18 Hz, eta=0:16:03, total=0:03:03, wall=20:05 IST
=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.468 DataTime=0.273 Loss=1.877 Prec@1=56.132 Prec@5=79.771 rate=2.18 Hz, eta=0:16:03, total=0:03:03, wall=20:05 IST
=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.468 DataTime=0.273 Loss=1.877 Prec@1=56.132 Prec@5=79.771 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=20:05 IST
=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.468 DataTime=0.273 Loss=1.877 Prec@1=56.132 Prec@5=79.771 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=20:05 IST
=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.878 Prec@1=56.089 Prec@5=79.746 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=20:05 IST
=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.878 Prec@1=56.089 Prec@5=79.746 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=20:05 IST
=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.878 Prec@1=56.089 Prec@5=79.746 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=20:06 IST
=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.878 Prec@1=56.112 Prec@5=79.741 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=20:06 IST
=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.878 Prec@1=56.112 Prec@5=79.741 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=20:06 IST
=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.878 Prec@1=56.112 Prec@5=79.741 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=20:07 IST
=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.881 Prec@1=56.060 Prec@5=79.707 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=20:07 IST
=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.881 Prec@1=56.060 Prec@5=79.707 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=20:07 IST
=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.881 Prec@1=56.060 Prec@5=79.707 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=20:08 IST
=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.883 Prec@1=56.080 Prec@5=79.684 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=20:08 IST
=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.883 Prec@1=56.080 Prec@5=79.684 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=20:08 IST
=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.883 Prec@1=56.080 Prec@5=79.684 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=20:08 IST
=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.885 Prec@1=56.066 Prec@5=79.643 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=20:08 IST
=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.885 Prec@1=56.066 Prec@5=79.643 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=20:08 IST
=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.271 Loss=1.885 Prec@1=56.066 Prec@5=79.643 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=20:09 IST
=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.886 Prec@1=56.044 Prec@5=79.642 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=20:09 IST
=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.886 Prec@1=56.044 Prec@5=79.642 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=20:09 IST
=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.886 Prec@1=56.044 Prec@5=79.642 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=20:10 IST
=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.886 Prec@1=56.032 Prec@5=79.626 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=20:10 IST
=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.886 Prec@1=56.032 Prec@5=79.626 rate=2.17 Hz, eta=0:10:00, total=0:09:13, wall=20:10 IST
=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.886 Prec@1=56.032 Prec@5=79.626 rate=2.17 Hz, eta=0:10:00, total=0:09:13, wall=20:11 IST
=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.887 Prec@1=56.025 Prec@5=79.612 rate=2.17 Hz, eta=0:10:00, total=0:09:13, wall=20:11 IST
=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.887 Prec@1=56.025 Prec@5=79.612 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:11 IST
=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.887 Prec@1=56.025 Prec@5=79.612 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:11 IST
=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.888 Prec@1=56.016 Prec@5=79.602 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:11 IST
=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.888 Prec@1=56.016 Prec@5=79.602 rate=2.17 Hz, eta=0:08:28, total=0:10:47, wall=20:11 IST
=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.466 DataTime=0.270 Loss=1.888 Prec@1=56.016 Prec@5=79.602 rate=2.17 Hz, eta=0:08:28, total=0:10:47, wall=20:12 IST
=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.986 Prec@5=79.577 rate=2.17 Hz, eta=0:08:28, total=0:10:47, wall=20:12 IST
=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.986 Prec@5=79.577 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=20:12 IST
=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.986 Prec@5=79.577 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=20:13 IST
=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.967 Prec@5=79.569 rate=2.17 Hz, eta=0:07:42, total=0:11:32, wall=20:13 IST
=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.967 Prec@5=79.569 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=20:13 IST
=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.967 Prec@5=79.569 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=20:14 IST
=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=20:14 IST
=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.16 Hz, eta=0:06:10, total=0:13:06, wall=20:14 IST
=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.16 Hz, eta=0:06:10, total=0:13:06, wall=20:15 IST
=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.16 Hz, eta=0:06:10, total=0:13:06, wall=20:15 IST
=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=20:15 IST
=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.961 Prec@5=79.576 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=20:15 IST
=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.963 Prec@5=79.595 rate=2.16 Hz, eta=0:05:24, total=0:13:52, wall=20:15 IST
=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.963 Prec@5=79.595 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=20:15 IST
=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.963 Prec@5=79.595 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=20:16 IST
=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.968 Prec@5=79.601 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=20:16 IST
=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.968 Prec@5=79.601 rate=2.16 Hz, eta=0:03:52, total=0:15:24, wall=20:16 IST
=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.889 Prec@1=55.968 Prec@5=79.601 rate=2.16 Hz, eta=0:03:52, total=0:15:24, wall=20:17 IST
=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.959 Prec@5=79.595 rate=2.16 Hz, eta=0:03:52, total=0:15:24, wall=20:17 IST
=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.959 Prec@5=79.595 rate=2.16 Hz, eta=0:03:05, total=0:16:10, wall=20:17 IST
=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.889 Prec@1=55.959 Prec@5=79.595 rate=2.16 Hz, eta=0:03:05, total=0:16:10, wall=20:18 IST
=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.942 Prec@5=79.572 rate=2.16 Hz, eta=0:03:05, total=0:16:10, wall=20:18 IST
=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.942 Prec@5=79.572 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=20:18 IST
=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.942 Prec@5=79.572 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=20:18 IST
=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.890 Prec@1=55.938 Prec@5=79.569 rate=2.16 Hz, eta=0:02:19, total=0:16:57, wall=20:18 IST
=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.890 Prec@1=55.938 Prec@5=79.569 rate=2.16 Hz, eta=0:01:33, total=0:17:43, wall=20:18 IST
=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.269 Loss=1.890 Prec@1=55.938 Prec@5=79.569 rate=2.16 Hz, eta=0:01:33, total=0:17:43, wall=20:19 IST
=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.927 Prec@5=79.564 rate=2.16 Hz, eta=0:01:33, total=0:17:43, wall=20:19 IST
=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.927 Prec@5=79.564 rate=2.16 Hz, eta=0:00:47, total=0:18:31, wall=20:19 IST
=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.927 Prec@5=79.564 rate=2.16 Hz, eta=0:00:47, total=0:18:31, wall=20:20 IST
=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.935 Prec@5=79.575 rate=2.16 Hz, eta=0:00:47, total=0:18:31, wall=20:20 IST
=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.935 Prec@5=79.575 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=20:20 IST
=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.935 Prec@5=79.575 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=20:20 IST
=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.935 Prec@5=79.574 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=20:20 IST
=> training   100.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.465 DataTime=0.270 Loss=1.890 Prec@1=55.935 Prec@5=79.574 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=20:20 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> validation 0.00% of 1x98...Epoch=14/150 LR=0.09816 Time=6.941 Loss=1.270 Prec@1=66.797 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=6.941 Loss=1.270 Prec@1=66.797 Prec@5=89.258 rate=8503.55 Hz, eta=0:00:00, total=0:00:00, wall=20:20 IST
** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=6.941 Loss=1.270 Prec@1=66.797 Prec@5=89.258 rate=8503.55 Hz, eta=0:00:00, total=0:00:00, wall=20:21 IST
** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=0.548 Loss=1.850 Prec@1=56.172 Prec@5=80.750 rate=8503.55 Hz, eta=0:00:00, total=0:00:00, wall=20:21 IST
** validation 100.00% of 1x98...Epoch=14/150 LR=0.09816 Time=0.548 Loss=1.850 Prec@1=56.172 Prec@5=80.750 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=20:21 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> training   0.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.178 DataTime=4.928 Loss=1.889 Prec@1=57.812 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.178 DataTime=4.928 Loss=1.889 Prec@1=57.812 Prec@5=80.273 rate=7469.88 Hz, eta=0:00:00, total=0:00:00, wall=20:21 IST
=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.178 DataTime=4.928 Loss=1.889 Prec@1=57.812 Prec@5=80.273 rate=7469.88 Hz, eta=0:00:00, total=0:00:00, wall=20:22 IST
=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.501 DataTime=0.304 Loss=1.841 Prec@1=56.625 Prec@5=80.287 rate=7469.88 Hz, eta=0:00:00, total=0:00:00, wall=20:22 IST
=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.501 DataTime=0.304 Loss=1.841 Prec@1=56.625 Prec@5=80.287 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=20:22 IST
=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.501 DataTime=0.304 Loss=1.841 Prec@1=56.625 Prec@5=80.287 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=20:23 IST
=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.481 DataTime=0.286 Loss=1.836 Prec@1=56.783 Prec@5=80.362 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=20:23 IST
=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.481 DataTime=0.286 Loss=1.836 Prec@1=56.783 Prec@5=80.362 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=20:23 IST
=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.481 DataTime=0.286 Loss=1.836 Prec@1=56.783 Prec@5=80.362 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=20:23 IST
=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.474 DataTime=0.278 Loss=1.837 Prec@1=56.835 Prec@5=80.317 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=20:23 IST
=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.474 DataTime=0.278 Loss=1.837 Prec@1=56.835 Prec@5=80.317 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=20:23 IST
=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.474 DataTime=0.278 Loss=1.837 Prec@1=56.835 Prec@5=80.317 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=20:24 IST
=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.472 DataTime=0.276 Loss=1.839 Prec@1=56.750 Prec@5=80.276 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=20:24 IST
=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.472 DataTime=0.276 Loss=1.839 Prec@1=56.750 Prec@5=80.276 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=20:24 IST
=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.472 DataTime=0.276 Loss=1.839 Prec@1=56.750 Prec@5=80.276 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=20:25 IST
=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.469 DataTime=0.272 Loss=1.841 Prec@1=56.708 Prec@5=80.266 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=20:25 IST
=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.469 DataTime=0.272 Loss=1.841 Prec@1=56.708 Prec@5=80.266 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=20:25 IST
=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.469 DataTime=0.272 Loss=1.841 Prec@1=56.708 Prec@5=80.266 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=20:26 IST
=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.693 Prec@5=80.219 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=20:26 IST
=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.693 Prec@5=80.219 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=20:26 IST
=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.693 Prec@5=80.219 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=20:26 IST
=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.672 Prec@5=80.238 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=20:26 IST
=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.672 Prec@5=80.238 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=20:26 IST
=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.470 DataTime=0.274 Loss=1.845 Prec@1=56.672 Prec@5=80.238 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=20:27 IST
=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.845 Prec@1=56.657 Prec@5=80.228 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=20:27 IST
=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.845 Prec@1=56.657 Prec@5=80.228 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=20:27 IST
=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.845 Prec@1=56.657 Prec@5=80.228 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=20:28 IST
=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.847 Prec@1=56.612 Prec@5=80.207 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=20:28 IST
=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.847 Prec@1=56.612 Prec@5=80.207 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=20:28 IST
=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.847 Prec@1=56.612 Prec@5=80.207 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=20:29 IST
=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.849 Prec@1=56.578 Prec@5=80.167 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=20:29 IST
=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.849 Prec@1=56.578 Prec@5=80.167 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=20:29 IST
=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.849 Prec@1=56.578 Prec@5=80.167 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=20:30 IST
=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.852 Prec@1=56.535 Prec@5=80.125 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=20:30 IST
=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.852 Prec@1=56.535 Prec@5=80.125 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=20:30 IST
=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.852 Prec@1=56.535 Prec@5=80.125 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=20:30 IST
=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.853 Prec@1=56.515 Prec@5=80.113 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=20:30 IST
=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.853 Prec@1=56.515 Prec@5=80.113 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=20:30 IST
=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.468 DataTime=0.273 Loss=1.853 Prec@1=56.515 Prec@5=80.113 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=20:31 IST
=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.512 Prec@5=80.103 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=20:31 IST
=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.512 Prec@5=80.103 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=20:31 IST
=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.512 Prec@5=80.103 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=20:32 IST
=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.506 Prec@5=80.091 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=20:32 IST
=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.506 Prec@5=80.091 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:32 IST
=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.273 Loss=1.855 Prec@1=56.506 Prec@5=80.091 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:33 IST
=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.856 Prec@1=56.489 Prec@5=80.078 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:33 IST
=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.856 Prec@1=56.489 Prec@5=80.078 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=20:33 IST
=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.856 Prec@1=56.489 Prec@5=80.078 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=20:33 IST
=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.472 Prec@5=80.055 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=20:33 IST
=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.472 Prec@5=80.055 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=20:33 IST
=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.472 Prec@5=80.055 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=20:34 IST
=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.471 Prec@5=80.053 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=20:34 IST
=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.471 Prec@5=80.053 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=20:34 IST
=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.471 Prec@5=80.053 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=20:35 IST
=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.480 Prec@5=80.052 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=20:35 IST
=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.480 Prec@5=80.052 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=20:35 IST
=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.480 Prec@5=80.052 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=20:36 IST
=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.486 Prec@5=80.054 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=20:36 IST
=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.486 Prec@5=80.054 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=20:36 IST
=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.857 Prec@1=56.486 Prec@5=80.054 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=20:37 IST
=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.858 Prec@1=56.480 Prec@5=80.045 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=20:37 IST
=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.858 Prec@1=56.480 Prec@5=80.045 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=20:37 IST
=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.858 Prec@1=56.480 Prec@5=80.045 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=20:37 IST
=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.467 Prec@5=80.035 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=20:37 IST
=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.467 Prec@5=80.035 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=20:37 IST
=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.467 Prec@5=80.035 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=20:38 IST
=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.453 Prec@5=80.016 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=20:38 IST
=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.453 Prec@5=80.016 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=20:38 IST
=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.467 DataTime=0.272 Loss=1.859 Prec@1=56.453 Prec@5=80.016 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=20:39 IST
=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.429 Prec@5=80.001 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=20:39 IST
=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.429 Prec@5=80.001 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=20:39 IST
=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.429 Prec@5=80.001 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=20:40 IST
=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.426 Prec@5=79.996 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=20:40 IST
=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.426 Prec@5=79.996 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=20:40 IST
=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.861 Prec@1=56.426 Prec@5=79.996 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=20:40 IST
=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.862 Prec@1=56.421 Prec@5=79.983 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=20:40 IST
=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.862 Prec@1=56.421 Prec@5=79.983 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=20:40 IST
=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.862 Prec@1=56.421 Prec@5=79.983 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=20:40 IST
=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.862 Prec@1=56.422 Prec@5=79.983 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=20:40 IST
=> training   100.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.466 DataTime=0.271 Loss=1.862 Prec@1=56.422 Prec@5=79.983 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=20:40 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> validation 0.00% of 1x98...Epoch=15/150 LR=0.09787 Time=6.702 Loss=1.101 Prec@1=73.242 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.702 Loss=1.101 Prec@1=73.242 Prec@5=91.211 rate=11015.04 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST
** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.702 Loss=1.101 Prec@1=73.242 Prec@5=91.211 rate=11015.04 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST
** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=0.558 Loss=1.803 Prec@1=57.286 Prec@5=81.258 rate=11015.04 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST
** validation 100.00% of 1x98...Epoch=15/150 LR=0.09787 Time=0.558 Loss=1.803 Prec@1=57.286 Prec@5=81.258 rate=2.04 Hz, eta=0:00:00, total=0:00:47, wall=20:41 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> training   0.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.927 DataTime=4.586 Loss=1.807 Prec@1=57.422 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST
=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.927 DataTime=4.586 Loss=1.807 Prec@1=57.422 Prec@5=80.469 rate=10129.04 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST
=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.927 DataTime=4.586 Loss=1.807 Prec@1=57.422 Prec@5=80.469 rate=10129.04 Hz, eta=0:00:00, total=0:00:00, wall=20:42 IST
=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.511 DataTime=0.315 Loss=1.802 Prec@1=57.832 Prec@5=80.958 rate=10129.04 Hz, eta=0:00:00, total=0:00:00, wall=20:42 IST
=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.511 DataTime=0.315 Loss=1.802 Prec@1=57.832 Prec@5=80.958 rate=2.16 Hz, eta=0:18:29, total=0:00:46, wall=20:42 IST
=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.511 DataTime=0.315 Loss=1.802 Prec@1=57.832 Prec@5=80.958 rate=2.16 Hz, eta=0:18:29, total=0:00:46, wall=20:43 IST
=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.491 DataTime=0.297 Loss=1.812 Prec@1=57.500 Prec@5=80.791 rate=2.16 Hz, eta=0:18:29, total=0:00:46, wall=20:43 IST
=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.491 DataTime=0.297 Loss=1.812 Prec@1=57.500 Prec@5=80.791 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=20:43 IST
=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.491 DataTime=0.297 Loss=1.812 Prec@1=57.500 Prec@5=80.791 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=20:44 IST
=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.485 DataTime=0.291 Loss=1.815 Prec@1=57.475 Prec@5=80.742 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=20:44 IST
=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.485 DataTime=0.291 Loss=1.815 Prec@1=57.475 Prec@5=80.742 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=20:44 IST
=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.485 DataTime=0.291 Loss=1.815 Prec@1=57.475 Prec@5=80.742 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=20:45 IST
=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.286 Loss=1.813 Prec@1=57.431 Prec@5=80.747 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=20:45 IST
=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.286 Loss=1.813 Prec@1=57.431 Prec@5=80.747 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=20:45 IST
=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.286 Loss=1.813 Prec@1=57.431 Prec@5=80.747 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=20:45 IST
=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.817 Prec@1=57.320 Prec@5=80.698 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=20:45 IST
=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.817 Prec@1=57.320 Prec@5=80.698 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=20:45 IST
=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.817 Prec@1=57.320 Prec@5=80.698 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=20:46 IST
=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.820 Prec@1=57.268 Prec@5=80.662 rate=2.12 Hz, eta=0:15:42, total=0:03:55, wall=20:46 IST
=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.820 Prec@1=57.268 Prec@5=80.662 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:46 IST
=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.820 Prec@1=57.268 Prec@5=80.662 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:47 IST
=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.821 Prec@1=57.237 Prec@5=80.654 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:47 IST
=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.821 Prec@1=57.237 Prec@5=80.654 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=20:47 IST
=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.821 Prec@1=57.237 Prec@5=80.654 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=20:48 IST
=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.823 Prec@1=57.172 Prec@5=80.618 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=20:48 IST
=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.823 Prec@1=57.172 Prec@5=80.618 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=20:48 IST
=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.823 Prec@1=57.172 Prec@5=80.618 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=20:49 IST
=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.286 Loss=1.824 Prec@1=57.158 Prec@5=80.590 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=20:49 IST
=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.286 Loss=1.824 Prec@1=57.158 Prec@5=80.590 rate=2.11 Hz, eta=0:12:39, total=0:07:06, wall=20:49 IST
=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.286 Loss=1.824 Prec@1=57.158 Prec@5=80.590 rate=2.11 Hz, eta=0:12:39, total=0:07:06, wall=20:49 IST
=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.826 Prec@1=57.119 Prec@5=80.544 rate=2.11 Hz, eta=0:12:39, total=0:07:06, wall=20:49 IST
=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.826 Prec@1=57.119 Prec@5=80.544 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=20:49 IST
=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.826 Prec@1=57.119 Prec@5=80.544 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=20:50 IST
=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.829 Prec@1=57.072 Prec@5=80.501 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=20:50 IST
=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.829 Prec@1=57.072 Prec@5=80.501 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=20:50 IST
=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.481 DataTime=0.287 Loss=1.829 Prec@1=57.072 Prec@5=80.501 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=20:51 IST
=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.025 Prec@5=80.492 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=20:51 IST
=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.025 Prec@5=80.492 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=20:51 IST
=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.025 Prec@5=80.492 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=20:52 IST
=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.019 Prec@5=80.476 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=20:52 IST
=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.019 Prec@5=80.476 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=20:52 IST
=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.480 DataTime=0.286 Loss=1.830 Prec@1=57.019 Prec@5=80.476 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=20:53 IST
=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.285 Loss=1.831 Prec@1=57.013 Prec@5=80.474 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=20:53 IST
=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.285 Loss=1.831 Prec@1=57.013 Prec@5=80.474 rate=2.10 Hz, eta=0:08:43, total=0:11:05, wall=20:53 IST
=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.479 DataTime=0.285 Loss=1.831 Prec@1=57.013 Prec@5=80.474 rate=2.10 Hz, eta=0:08:43, total=0:11:05, wall=20:53 IST
=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.478 DataTime=0.284 Loss=1.832 Prec@1=57.016 Prec@5=80.474 rate=2.10 Hz, eta=0:08:43, total=0:11:05, wall=20:53 IST
=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.478 DataTime=0.284 Loss=1.832 Prec@1=57.016 Prec@5=80.474 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=20:53 IST
=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.478 DataTime=0.284 Loss=1.832 Prec@1=57.016 Prec@5=80.474 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=20:54 IST
=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.833 Prec@1=57.000 Prec@5=80.460 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=20:54 IST
=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.833 Prec@1=57.000 Prec@5=80.460 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=20:54 IST
=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.833 Prec@1=57.000 Prec@5=80.460 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=20:55 IST
=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.834 Prec@1=57.007 Prec@5=80.444 rate=2.11 Hz, eta=0:07:07, total=0:12:39, wall=20:55 IST
=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.834 Prec@1=57.007 Prec@5=80.444 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=20:55 IST
=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.477 DataTime=0.283 Loss=1.834 Prec@1=57.007 Prec@5=80.444 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=20:56 IST
=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.283 Loss=1.834 Prec@1=56.996 Prec@5=80.421 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=20:56 IST
=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.283 Loss=1.834 Prec@1=56.996 Prec@5=80.421 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=20:56 IST
=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.283 Loss=1.834 Prec@1=56.996 Prec@5=80.421 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=20:56 IST
=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.282 Loss=1.836 Prec@1=56.954 Prec@5=80.397 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=20:56 IST
=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.282 Loss=1.836 Prec@1=56.954 Prec@5=80.397 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=20:56 IST
=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.476 DataTime=0.282 Loss=1.836 Prec@1=56.954 Prec@5=80.397 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=20:57 IST
=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.837 Prec@1=56.940 Prec@5=80.384 rate=2.11 Hz, eta=0:04:45, total=0:15:00, wall=20:57 IST
=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.837 Prec@1=56.940 Prec@5=80.384 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=20:57 IST
=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.837 Prec@1=56.940 Prec@5=80.384 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=20:58 IST
=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.836 Prec@1=56.937 Prec@5=80.389 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=20:58 IST
=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.836 Prec@1=56.937 Prec@5=80.389 rate=2.12 Hz, eta=0:03:09, total=0:16:32, wall=20:58 IST
=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.475 DataTime=0.281 Loss=1.836 Prec@1=56.937 Prec@5=80.389 rate=2.12 Hz, eta=0:03:09, total=0:16:32, wall=20:59 IST
=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.928 Prec@5=80.383 rate=2.12 Hz, eta=0:03:09, total=0:16:32, wall=20:59 IST
=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.928 Prec@5=80.383 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=20:59 IST
=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.928 Prec@5=80.383 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=21:00 IST
=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.911 Prec@5=80.373 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=21:00 IST
=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.911 Prec@5=80.373 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=21:00 IST
=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.911 Prec@5=80.373 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=21:00 IST
=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.910 Prec@5=80.369 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=21:00 IST
=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.910 Prec@5=80.369 rate=2.12 Hz, eta=0:00:48, total=0:18:53, wall=21:00 IST
=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.838 Prec@1=56.910 Prec@5=80.369 rate=2.12 Hz, eta=0:00:48, total=0:18:53, wall=21:01 IST
=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.919 Prec@5=80.375 rate=2.12 Hz, eta=0:00:48, total=0:18:53, wall=21:01 IST
=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.919 Prec@5=80.375 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=21:01 IST
=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.919 Prec@5=80.375 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=21:01 IST
=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.918 Prec@5=80.374 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=21:01 IST
=> training   100.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.474 DataTime=0.280 Loss=1.837 Prec@1=56.918 Prec@5=80.374 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=21:01 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST
=> validation 0.00% of 1x98...Epoch=16/150 LR=0.09755 Time=6.040 Loss=1.188 Prec@1=68.359 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST
=> validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.040 Loss=1.188 Prec@1=68.359 Prec@5=91.211 rate=7079.40 Hz, eta=0:00:00, total=0:00:00, wall=21:01 IST
** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.040 Loss=1.188 Prec@1=68.359 Prec@5=91.211 rate=7079.40 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST
** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=0.543 Loss=1.776 Prec@1=57.862 Prec@5=81.588 rate=7079.40 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST
** validation 100.00% of 1x98...Epoch=16/150 LR=0.09755 Time=0.543 Loss=1.776 Prec@1=57.862 Prec@5=81.588 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=21:02 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST
=> training   0.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=5.140 DataTime=4.865 Loss=1.813 Prec@1=56.836 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST
=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=5.140 DataTime=4.865 Loss=1.813 Prec@1=56.836 Prec@5=80.469 rate=7793.32 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST
=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=5.140 DataTime=4.865 Loss=1.813 Prec@1=56.836 Prec@5=80.469 rate=7793.32 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST
=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.495 DataTime=0.297 Loss=1.790 Prec@1=57.714 Prec@5=81.045 rate=7793.32 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST
=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.495 DataTime=0.297 Loss=1.790 Prec@1=57.714 Prec@5=81.045 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=21:03 IST
=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.495 DataTime=0.297 Loss=1.790 Prec@1=57.714 Prec@5=81.045 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=21:04 IST
=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.479 DataTime=0.283 Loss=1.785 Prec@1=57.949 Prec@5=81.130 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=21:04 IST
=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.479 DataTime=0.283 Loss=1.785 Prec@1=57.949 Prec@5=81.130 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=21:04 IST
=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.479 DataTime=0.283 Loss=1.785 Prec@1=57.949 Prec@5=81.130 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=21:04 IST
=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.786 Prec@1=57.853 Prec@5=81.178 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=21:04 IST
=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.786 Prec@1=57.853 Prec@5=81.178 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=21:04 IST
=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.786 Prec@1=57.853 Prec@5=81.178 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=21:05 IST
=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.786 Prec@1=57.897 Prec@5=81.182 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=21:05 IST
=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.786 Prec@1=57.897 Prec@5=81.182 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:05 IST
=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.786 Prec@1=57.897 Prec@5=81.182 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:06 IST
=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.476 DataTime=0.280 Loss=1.789 Prec@1=57.861 Prec@5=81.148 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:06 IST
=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.476 DataTime=0.280 Loss=1.789 Prec@1=57.861 Prec@5=81.148 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:06 IST
=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.476 DataTime=0.280 Loss=1.789 Prec@1=57.861 Prec@5=81.148 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:07 IST
=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.791 Prec@1=57.834 Prec@5=81.119 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:07 IST
=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.791 Prec@1=57.834 Prec@5=81.119 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=21:07 IST
=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.473 DataTime=0.277 Loss=1.791 Prec@1=57.834 Prec@5=81.119 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=21:08 IST
=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.793 Prec@1=57.799 Prec@5=81.088 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=21:08 IST
=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.793 Prec@1=57.799 Prec@5=81.088 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=21:08 IST
=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.474 DataTime=0.278 Loss=1.793 Prec@1=57.799 Prec@5=81.088 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=21:08 IST
=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.472 DataTime=0.277 Loss=1.796 Prec@1=57.720 Prec@5=81.044 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=21:08 IST
=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.472 DataTime=0.277 Loss=1.796 Prec@1=57.720 Prec@5=81.044 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=21:08 IST
=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.472 DataTime=0.277 Loss=1.796 Prec@1=57.720 Prec@5=81.044 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=21:09 IST
=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.275 Loss=1.798 Prec@1=57.688 Prec@5=81.020 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=21:09 IST
=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.275 Loss=1.798 Prec@1=57.688 Prec@5=81.020 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=21:09 IST
=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.275 Loss=1.798 Prec@1=57.688 Prec@5=81.020 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=21:10 IST
=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.800 Prec@1=57.684 Prec@5=80.974 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=21:10 IST
=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.800 Prec@1=57.684 Prec@5=80.974 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=21:10 IST
=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.800 Prec@1=57.684 Prec@5=80.974 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=21:11 IST
=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.803 Prec@1=57.646 Prec@5=80.922 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=21:11 IST
=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.803 Prec@1=57.646 Prec@5=80.922 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=21:11 IST
=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.470 DataTime=0.274 Loss=1.803 Prec@1=57.646 Prec@5=80.922 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=21:11 IST
=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.469 DataTime=0.274 Loss=1.804 Prec@1=57.611 Prec@5=80.894 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=21:11 IST
=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.469 DataTime=0.274 Loss=1.804 Prec@1=57.611 Prec@5=80.894 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=21:11 IST
=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.469 DataTime=0.274 Loss=1.804 Prec@1=57.611 Prec@5=80.894 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=21:12 IST
=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.804 Prec@1=57.601 Prec@5=80.876 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=21:12 IST
=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.804 Prec@1=57.601 Prec@5=80.876 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=21:12 IST
=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.804 Prec@1=57.601 Prec@5=80.876 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=21:13 IST
=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.806 Prec@1=57.559 Prec@5=80.850 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=21:13 IST
=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.806 Prec@1=57.559 Prec@5=80.850 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=21:13 IST
=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.273 Loss=1.806 Prec@1=57.559 Prec@5=80.850 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=21:14 IST
=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.824 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=21:14 IST
=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.824 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=21:14 IST
=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.468 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.824 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=21:15 IST
=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.813 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=21:15 IST
=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.813 rate=2.15 Hz, eta=0:06:58, total=0:12:22, wall=21:15 IST
=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.522 Prec@5=80.813 rate=2.15 Hz, eta=0:06:58, total=0:12:22, wall=21:15 IST
=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.508 Prec@5=80.800 rate=2.15 Hz, eta=0:06:58, total=0:12:22, wall=21:15 IST
=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.508 Prec@5=80.800 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=21:15 IST
=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.808 Prec@1=57.508 Prec@5=80.800 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=21:16 IST
=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.811 Prec@1=57.458 Prec@5=80.768 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=21:16 IST
=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.811 Prec@1=57.458 Prec@5=80.768 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=21:16 IST
=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.811 Prec@1=57.458 Prec@5=80.768 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=21:17 IST
=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.812 Prec@1=57.457 Prec@5=80.764 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=21:17 IST
=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.812 Prec@1=57.457 Prec@5=80.764 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:17 IST
=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.272 Loss=1.812 Prec@1=57.457 Prec@5=80.764 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:18 IST
=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.447 Prec@5=80.761 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:18 IST
=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.447 Prec@5=80.761 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=21:18 IST
=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.447 Prec@5=80.761 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=21:18 IST
=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.446 Prec@5=80.753 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=21:18 IST
=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.446 Prec@5=80.753 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=21:18 IST
=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.812 Prec@1=57.446 Prec@5=80.753 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=21:19 IST
=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.435 Prec@5=80.753 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=21:19 IST
=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.435 Prec@5=80.753 rate=2.16 Hz, eta=0:02:20, total=0:17:00, wall=21:19 IST
=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.435 Prec@5=80.753 rate=2.16 Hz, eta=0:02:20, total=0:17:00, wall=21:20 IST
=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.421 Prec@5=80.743 rate=2.16 Hz, eta=0:02:20, total=0:17:00, wall=21:20 IST
=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.421 Prec@5=80.743 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=21:20 IST
=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.466 DataTime=0.271 Loss=1.813 Prec@1=57.421 Prec@5=80.743 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=21:21 IST
=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.813 Prec@1=57.432 Prec@5=80.740 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=21:21 IST
=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.813 Prec@1=57.432 Prec@5=80.740 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:21 IST
=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.813 Prec@1=57.432 Prec@5=80.740 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:22 IST
=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.814 Prec@1=57.424 Prec@5=80.731 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:22 IST
=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.814 Prec@1=57.424 Prec@5=80.731 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=21:22 IST
=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.814 Prec@1=57.424 Prec@5=80.731 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=21:22 IST
=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.814 Prec@1=57.423 Prec@5=80.731 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=21:22 IST
=> training   100.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.467 DataTime=0.271 Loss=1.814 Prec@1=57.423 Prec@5=80.731 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=21:22 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:22 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:22 IST
=> validation 0.00% of 1x98...Epoch=17/150 LR=0.09722 Time=6.892 Loss=1.120 Prec@1=73.828 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=21:22 IST
=> validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=6.892 Loss=1.120 Prec@1=73.828 Prec@5=90.430 rate=10765.07 Hz, eta=0:00:00, total=0:00:00, wall=21:22 IST
** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=6.892 Loss=1.120 Prec@1=73.828 Prec@5=90.430 rate=10765.07 Hz, eta=0:00:00, total=0:00:00, wall=21:22 IST
** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=0.554 Loss=1.755 Prec@1=58.478 Prec@5=81.936 rate=10765.07 Hz, eta=0:00:00, total=0:00:00, wall=21:22 IST
** validation 100.00% of 1x98...Epoch=17/150 LR=0.09722 Time=0.554 Loss=1.755 Prec@1=58.478 Prec@5=81.936 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=21:22 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> training   0.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=4.918 DataTime=4.669 Loss=1.876 Prec@1=57.031 Prec@5=80.078 rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=4.918 DataTime=4.669 Loss=1.876 Prec@1=57.031 Prec@5=80.078 rate=7394.15 Hz, eta=0:00:00, total=0:00:00, wall=21:23 IST
=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=4.918 DataTime=4.669 Loss=1.876 Prec@1=57.031 Prec@5=80.078 rate=7394.15 Hz, eta=0:00:00, total=0:00:00, wall=21:23 IST
=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.499 DataTime=0.306 Loss=1.775 Prec@1=58.226 Prec@5=81.484 rate=7394.15 Hz, eta=0:00:00, total=0:00:00, wall=21:23 IST
=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.499 DataTime=0.306 Loss=1.775 Prec@1=58.226 Prec@5=81.484 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=21:23 IST
=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.499 DataTime=0.306 Loss=1.775 Prec@1=58.226 Prec@5=81.484 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=21:24 IST
=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.482 DataTime=0.288 Loss=1.772 Prec@1=58.207 Prec@5=81.437 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=21:24 IST
=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.482 DataTime=0.288 Loss=1.772 Prec@1=58.207 Prec@5=81.437 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=21:24 IST
=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.482 DataTime=0.288 Loss=1.772 Prec@1=58.207 Prec@5=81.437 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=21:25 IST
=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.475 DataTime=0.280 Loss=1.774 Prec@1=58.153 Prec@5=81.383 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=21:25 IST
=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.475 DataTime=0.280 Loss=1.774 Prec@1=58.153 Prec@5=81.383 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=21:25 IST
=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.475 DataTime=0.280 Loss=1.774 Prec@1=58.153 Prec@5=81.383 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=21:26 IST
=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.154 Prec@5=81.389 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=21:26 IST
=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.154 Prec@5=81.389 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:26 IST
=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.154 Prec@5=81.389 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:26 IST
=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.772 Prec@1=58.197 Prec@5=81.376 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:26 IST
=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.772 Prec@1=58.197 Prec@5=81.376 rate=2.17 Hz, eta=0:15:23, total=0:03:50, wall=21:26 IST
=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.772 Prec@1=58.197 Prec@5=81.376 rate=2.17 Hz, eta=0:15:23, total=0:03:50, wall=21:27 IST
=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.773 Prec@1=58.190 Prec@5=81.356 rate=2.17 Hz, eta=0:15:23, total=0:03:50, wall=21:27 IST
=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.773 Prec@1=58.190 Prec@5=81.356 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=21:27 IST
=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.773 Prec@1=58.190 Prec@5=81.356 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=21:28 IST
=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.776 Prec@1=58.157 Prec@5=81.325 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=21:28 IST
=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.776 Prec@1=58.157 Prec@5=81.325 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:28 IST
=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.776 Prec@1=58.157 Prec@5=81.325 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:29 IST
=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.778 Prec@1=58.101 Prec@5=81.316 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:29 IST
=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.778 Prec@1=58.101 Prec@5=81.316 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=21:29 IST
=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.778 Prec@1=58.101 Prec@5=81.316 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=21:30 IST
=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.781 Prec@1=58.036 Prec@5=81.272 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=21:30 IST
=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.781 Prec@1=58.036 Prec@5=81.272 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:30 IST
=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.781 Prec@1=58.036 Prec@5=81.272 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:30 IST
=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.468 DataTime=0.272 Loss=1.782 Prec@1=58.002 Prec@5=81.239 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:30 IST
=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.468 DataTime=0.272 Loss=1.782 Prec@1=58.002 Prec@5=81.239 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:30 IST
=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.468 DataTime=0.272 Loss=1.782 Prec@1=58.002 Prec@5=81.239 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:31 IST
=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.783 Prec@1=57.987 Prec@5=81.217 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:31 IST
=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.783 Prec@1=57.987 Prec@5=81.217 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=21:31 IST
=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.783 Prec@1=57.987 Prec@5=81.217 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=21:32 IST
=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.784 Prec@1=57.974 Prec@5=81.195 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=21:32 IST
=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.784 Prec@1=57.974 Prec@5=81.195 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=21:32 IST
=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.784 Prec@1=57.974 Prec@5=81.195 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=21:33 IST
=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.949 Prec@5=81.164 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=21:33 IST
=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.949 Prec@5=81.164 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=21:33 IST
=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.949 Prec@5=81.164 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=21:33 IST
=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.945 Prec@5=81.160 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=21:33 IST
=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.945 Prec@5=81.160 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=21:33 IST
=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.785 Prec@1=57.945 Prec@5=81.160 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=21:34 IST
=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.786 Prec@1=57.925 Prec@5=81.146 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=21:34 IST
=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.786 Prec@1=57.925 Prec@5=81.146 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=21:34 IST
=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.786 Prec@1=57.925 Prec@5=81.146 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=21:35 IST
=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.786 Prec@1=57.925 Prec@5=81.151 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=21:35 IST
=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.786 Prec@1=57.925 Prec@5=81.151 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=21:35 IST
=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.273 Loss=1.786 Prec@1=57.925 Prec@5=81.151 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=21:36 IST
=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.787 Prec@1=57.905 Prec@5=81.128 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=21:36 IST
=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.787 Prec@1=57.905 Prec@5=81.128 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=21:36 IST
=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.469 DataTime=0.274 Loss=1.787 Prec@1=57.905 Prec@5=81.128 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=21:37 IST
=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.274 Loss=1.789 Prec@1=57.852 Prec@5=81.093 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=21:37 IST
=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.274 Loss=1.789 Prec@1=57.852 Prec@5=81.093 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:37 IST
=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.274 Loss=1.789 Prec@1=57.852 Prec@5=81.093 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:37 IST
=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.789 Prec@1=57.864 Prec@5=81.103 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:37 IST
=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.789 Prec@1=57.864 Prec@5=81.103 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=21:37 IST
=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.470 DataTime=0.275 Loss=1.789 Prec@1=57.864 Prec@5=81.103 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=21:38 IST
=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.275 Loss=1.789 Prec@1=57.870 Prec@5=81.094 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=21:38 IST
=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.275 Loss=1.789 Prec@1=57.870 Prec@5=81.094 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=21:38 IST
=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.275 Loss=1.789 Prec@1=57.870 Prec@5=81.094 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=21:39 IST
=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.790 Prec@1=57.861 Prec@5=81.082 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=21:39 IST
=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.790 Prec@1=57.861 Prec@5=81.082 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=21:39 IST
=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.790 Prec@1=57.861 Prec@5=81.082 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=21:40 IST
=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.791 Prec@1=57.857 Prec@5=81.073 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=21:40 IST
=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.791 Prec@1=57.857 Prec@5=81.073 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=21:40 IST
=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.791 Prec@1=57.857 Prec@5=81.073 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=21:41 IST
=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.792 Prec@1=57.841 Prec@5=81.060 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=21:41 IST
=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.792 Prec@1=57.841 Prec@5=81.060 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=21:41 IST
=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.792 Prec@1=57.841 Prec@5=81.060 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=21:41 IST
=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.793 Prec@1=57.822 Prec@5=81.048 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=21:41 IST
=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.793 Prec@1=57.822 Prec@5=81.048 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=21:41 IST
=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.471 DataTime=0.276 Loss=1.793 Prec@1=57.822 Prec@5=81.048 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=21:42 IST
=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.793 Prec@1=57.808 Prec@5=81.036 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=21:42 IST
=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.793 Prec@1=57.808 Prec@5=81.036 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=21:42 IST
=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.793 Prec@1=57.808 Prec@5=81.036 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=21:42 IST
=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.793 Prec@1=57.807 Prec@5=81.035 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=21:42 IST
=> training   100.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.472 DataTime=0.277 Loss=1.793 Prec@1=57.807 Prec@5=81.035 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=21:42 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST
=> validation 0.00% of 1x98...Epoch=18/150 LR=0.09686 Time=6.905 Loss=1.073 Prec@1=74.023 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST
=> validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=6.905 Loss=1.073 Prec@1=74.023 Prec@5=92.188 rate=2477.17 Hz, eta=0:00:00, total=0:00:00, wall=21:42 IST
** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=6.905 Loss=1.073 Prec@1=74.023 Prec@5=92.188 rate=2477.17 Hz, eta=0:00:00, total=0:00:00, wall=21:43 IST
** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=0.556 Loss=1.721 Prec@1=58.930 Prec@5=82.562 rate=2477.17 Hz, eta=0:00:00, total=0:00:00, wall=21:43 IST
** validation 100.00% of 1x98...Epoch=18/150 LR=0.09686 Time=0.556 Loss=1.721 Prec@1=58.930 Prec@5=82.562 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=21:43 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:43 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:43 IST
=> training   0.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=4.794 DataTime=4.531 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=21:43 IST
=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=4.794 DataTime=4.531 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=7561.21 Hz, eta=0:00:00, total=0:00:00, wall=21:43 IST
=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=4.794 DataTime=4.531 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=7561.21 Hz, eta=0:00:00, total=0:00:00, wall=21:44 IST
=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.515 DataTime=0.321 Loss=1.757 Prec@1=58.385 Prec@5=81.523 rate=7561.21 Hz, eta=0:00:00, total=0:00:00, wall=21:44 IST
=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.515 DataTime=0.321 Loss=1.757 Prec@1=58.385 Prec@5=81.523 rate=2.14 Hz, eta=0:18:42, total=0:00:47, wall=21:44 IST
=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.515 DataTime=0.321 Loss=1.757 Prec@1=58.385 Prec@5=81.523 rate=2.14 Hz, eta=0:18:42, total=0:00:47, wall=21:45 IST
=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.493 DataTime=0.299 Loss=1.753 Prec@1=58.546 Prec@5=81.584 rate=2.14 Hz, eta=0:18:42, total=0:00:47, wall=21:45 IST
=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.493 DataTime=0.299 Loss=1.753 Prec@1=58.546 Prec@5=81.584 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=21:45 IST
=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.493 DataTime=0.299 Loss=1.753 Prec@1=58.546 Prec@5=81.584 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=21:46 IST
=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.485 DataTime=0.289 Loss=1.754 Prec@1=58.506 Prec@5=81.603 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=21:46 IST
=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.485 DataTime=0.289 Loss=1.754 Prec@1=58.506 Prec@5=81.603 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=21:46 IST
=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.485 DataTime=0.289 Loss=1.754 Prec@1=58.506 Prec@5=81.603 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=21:46 IST
=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.480 DataTime=0.284 Loss=1.755 Prec@1=58.567 Prec@5=81.557 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=21:46 IST
=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.480 DataTime=0.284 Loss=1.755 Prec@1=58.567 Prec@5=81.557 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=21:46 IST
=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.480 DataTime=0.284 Loss=1.755 Prec@1=58.567 Prec@5=81.557 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=21:47 IST
=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.755 Prec@1=58.611 Prec@5=81.549 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=21:47 IST
=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.755 Prec@1=58.611 Prec@5=81.549 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=21:47 IST
=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.755 Prec@1=58.611 Prec@5=81.549 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=21:48 IST
=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.279 Loss=1.755 Prec@1=58.585 Prec@5=81.535 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=21:48 IST
=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.279 Loss=1.755 Prec@1=58.585 Prec@5=81.535 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=21:48 IST
=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.279 Loss=1.755 Prec@1=58.585 Prec@5=81.535 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=21:49 IST
=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.758 Prec@1=58.546 Prec@5=81.504 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=21:49 IST
=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.758 Prec@1=58.546 Prec@5=81.504 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=21:49 IST
=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.758 Prec@1=58.546 Prec@5=81.504 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=21:49 IST
=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.760 Prec@1=58.487 Prec@5=81.489 rate=2.13 Hz, eta=0:14:04, total=0:05:28, wall=21:49 IST
=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.760 Prec@1=58.487 Prec@5=81.489 rate=2.13 Hz, eta=0:13:19, total=0:06:16, wall=21:49 IST
=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.760 Prec@1=58.487 Prec@5=81.489 rate=2.13 Hz, eta=0:13:19, total=0:06:16, wall=21:50 IST
=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.762 Prec@1=58.434 Prec@5=81.452 rate=2.13 Hz, eta=0:13:19, total=0:06:16, wall=21:50 IST
=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.762 Prec@1=58.434 Prec@5=81.452 rate=2.12 Hz, eta=0:12:35, total=0:07:04, wall=21:50 IST
=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.762 Prec@1=58.434 Prec@5=81.452 rate=2.12 Hz, eta=0:12:35, total=0:07:04, wall=21:51 IST
=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.282 Loss=1.762 Prec@1=58.433 Prec@5=81.460 rate=2.12 Hz, eta=0:12:35, total=0:07:04, wall=21:51 IST
=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.282 Loss=1.762 Prec@1=58.433 Prec@5=81.460 rate=2.12 Hz, eta=0:11:50, total=0:07:53, wall=21:51 IST
=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.282 Loss=1.762 Prec@1=58.433 Prec@5=81.460 rate=2.12 Hz, eta=0:11:50, total=0:07:53, wall=21:52 IST
=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.283 Loss=1.763 Prec@1=58.408 Prec@5=81.456 rate=2.12 Hz, eta=0:11:50, total=0:07:53, wall=21:52 IST
=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.283 Loss=1.763 Prec@1=58.408 Prec@5=81.456 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=21:52 IST
=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.478 DataTime=0.283 Loss=1.763 Prec@1=58.408 Prec@5=81.456 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=21:53 IST
=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.764 Prec@1=58.388 Prec@5=81.455 rate=2.11 Hz, eta=0:11:03, total=0:08:40, wall=21:53 IST
=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.764 Prec@1=58.388 Prec@5=81.455 rate=2.11 Hz, eta=0:10:15, total=0:09:28, wall=21:53 IST
=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.764 Prec@1=58.388 Prec@5=81.455 rate=2.11 Hz, eta=0:10:15, total=0:09:28, wall=21:53 IST
=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.767 Prec@1=58.333 Prec@5=81.427 rate=2.11 Hz, eta=0:10:15, total=0:09:28, wall=21:53 IST
=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.767 Prec@1=58.333 Prec@5=81.427 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=21:53 IST
=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.477 DataTime=0.282 Loss=1.767 Prec@1=58.333 Prec@5=81.427 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=21:54 IST
=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.767 Prec@1=58.325 Prec@5=81.421 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=21:54 IST
=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.767 Prec@1=58.325 Prec@5=81.421 rate=2.12 Hz, eta=0:08:40, total=0:11:02, wall=21:54 IST
=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.476 DataTime=0.281 Loss=1.767 Prec@1=58.325 Prec@5=81.421 rate=2.12 Hz, eta=0:08:40, total=0:11:02, wall=21:55 IST
=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.768 Prec@1=58.310 Prec@5=81.406 rate=2.12 Hz, eta=0:08:40, total=0:11:02, wall=21:55 IST
=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.768 Prec@1=58.310 Prec@5=81.406 rate=2.12 Hz, eta=0:07:53, total=0:11:48, wall=21:55 IST
=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.475 DataTime=0.280 Loss=1.768 Prec@1=58.310 Prec@5=81.406 rate=2.12 Hz, eta=0:07:53, total=0:11:48, wall=21:56 IST
=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.769 Prec@1=58.298 Prec@5=81.386 rate=2.12 Hz, eta=0:07:53, total=0:11:48, wall=21:56 IST
=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.769 Prec@1=58.298 Prec@5=81.386 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=21:56 IST
=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.769 Prec@1=58.298 Prec@5=81.386 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=21:57 IST
=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.770 Prec@1=58.280 Prec@5=81.379 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=21:57 IST
=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.770 Prec@1=58.280 Prec@5=81.379 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=21:57 IST
=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.770 Prec@1=58.280 Prec@5=81.379 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=21:57 IST
=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.259 Prec@5=81.361 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=21:57 IST
=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.259 Prec@5=81.361 rate=2.12 Hz, eta=0:05:30, total=0:14:09, wall=21:57 IST
=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.259 Prec@5=81.361 rate=2.12 Hz, eta=0:05:30, total=0:14:09, wall=21:58 IST
=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.273 Prec@5=81.352 rate=2.12 Hz, eta=0:05:30, total=0:14:09, wall=21:58 IST
=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.273 Prec@5=81.352 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=21:58 IST
=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.273 Prec@5=81.352 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=21:59 IST
=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.267 Prec@5=81.353 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=21:59 IST
=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.267 Prec@5=81.353 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=21:59 IST
=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.771 Prec@1=58.267 Prec@5=81.353 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=22:00 IST
=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.772 Prec@1=58.264 Prec@5=81.349 rate=2.12 Hz, eta=0:03:56, total=0:15:43, wall=22:00 IST
=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.772 Prec@1=58.264 Prec@5=81.349 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:00 IST
=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.772 Prec@1=58.264 Prec@5=81.349 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:00 IST
=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.773 Prec@1=58.253 Prec@5=81.339 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:00 IST
=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.773 Prec@1=58.253 Prec@5=81.339 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:00 IST
=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.474 DataTime=0.279 Loss=1.773 Prec@1=58.253 Prec@5=81.339 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:01 IST
=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.231 Prec@5=81.328 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:01 IST
=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.231 Prec@5=81.328 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:01 IST
=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.774 Prec@1=58.231 Prec@5=81.328 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:02 IST
=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.775 Prec@1=58.215 Prec@5=81.304 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:02 IST
=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.775 Prec@1=58.215 Prec@5=81.304 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:02 IST
=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.775 Prec@1=58.215 Prec@5=81.304 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:03 IST
=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.776 Prec@1=58.197 Prec@5=81.298 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:03 IST
=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.776 Prec@1=58.197 Prec@5=81.298 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=22:03 IST
=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.776 Prec@1=58.197 Prec@5=81.298 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=22:03 IST
=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.776 Prec@1=58.198 Prec@5=81.298 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=22:03 IST
=> training   100.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.473 DataTime=0.278 Loss=1.776 Prec@1=58.198 Prec@5=81.298 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=22:03 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:03 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:03 IST
=> validation 0.00% of 1x98...Epoch=19/150 LR=0.09649 Time=6.811 Loss=1.141 Prec@1=71.094 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=22:03 IST
=> validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.811 Loss=1.141 Prec@1=71.094 Prec@5=91.602 rate=7135.11 Hz, eta=0:00:00, total=0:00:00, wall=22:03 IST
** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.811 Loss=1.141 Prec@1=71.094 Prec@5=91.602 rate=7135.11 Hz, eta=0:00:00, total=0:00:00, wall=22:04 IST
** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=0.553 Loss=1.768 Prec@1=57.974 Prec@5=81.698 rate=7135.11 Hz, eta=0:00:00, total=0:00:00, wall=22:04 IST
** validation 100.00% of 1x98...Epoch=19/150 LR=0.09649 Time=0.553 Loss=1.768 Prec@1=57.974 Prec@5=81.698 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=22:04 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:04 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:04 IST
=> training   0.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.038 DataTime=4.768 Loss=1.706 Prec@1=58.398 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=22:04 IST
=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.038 DataTime=4.768 Loss=1.706 Prec@1=58.398 Prec@5=81.641 rate=9369.35 Hz, eta=0:00:00, total=0:00:00, wall=22:04 IST
=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.038 DataTime=4.768 Loss=1.706 Prec@1=58.398 Prec@5=81.641 rate=9369.35 Hz, eta=0:00:00, total=0:00:00, wall=22:05 IST
=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.498 DataTime=0.302 Loss=1.720 Prec@1=59.182 Prec@5=82.161 rate=9369.35 Hz, eta=0:00:00, total=0:00:00, wall=22:05 IST
=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.498 DataTime=0.302 Loss=1.720 Prec@1=59.182 Prec@5=82.161 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:05 IST
=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.498 DataTime=0.302 Loss=1.720 Prec@1=59.182 Prec@5=82.161 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:05 IST
=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.480 DataTime=0.284 Loss=1.724 Prec@1=59.186 Prec@5=82.031 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:05 IST
=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.480 DataTime=0.284 Loss=1.724 Prec@1=59.186 Prec@5=82.031 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=22:05 IST
=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.480 DataTime=0.284 Loss=1.724 Prec@1=59.186 Prec@5=82.031 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=22:06 IST
=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.477 DataTime=0.280 Loss=1.729 Prec@1=59.043 Prec@5=81.972 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=22:06 IST
=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.477 DataTime=0.280 Loss=1.729 Prec@1=59.043 Prec@5=81.972 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=22:06 IST
=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.477 DataTime=0.280 Loss=1.729 Prec@1=59.043 Prec@5=81.972 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=22:07 IST
=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.473 DataTime=0.276 Loss=1.734 Prec@1=58.901 Prec@5=81.940 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=22:07 IST
=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.473 DataTime=0.276 Loss=1.734 Prec@1=58.901 Prec@5=81.940 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=22:07 IST
=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.473 DataTime=0.276 Loss=1.734 Prec@1=58.901 Prec@5=81.940 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=22:08 IST
=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.471 DataTime=0.275 Loss=1.735 Prec@1=58.918 Prec@5=81.887 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=22:08 IST
=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.471 DataTime=0.275 Loss=1.735 Prec@1=58.918 Prec@5=81.887 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=22:08 IST
=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.471 DataTime=0.275 Loss=1.735 Prec@1=58.918 Prec@5=81.887 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=22:08 IST
=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.470 DataTime=0.274 Loss=1.737 Prec@1=58.914 Prec@5=81.881 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=22:08 IST
=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.470 DataTime=0.274 Loss=1.737 Prec@1=58.914 Prec@5=81.881 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=22:08 IST
=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.470 DataTime=0.274 Loss=1.737 Prec@1=58.914 Prec@5=81.881 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=22:09 IST
=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.740 Prec@1=58.887 Prec@5=81.829 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=22:09 IST
=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.740 Prec@1=58.887 Prec@5=81.829 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=22:09 IST
=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.740 Prec@1=58.887 Prec@5=81.829 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=22:10 IST
=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.744 Prec@1=58.810 Prec@5=81.780 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=22:10 IST
=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.744 Prec@1=58.810 Prec@5=81.780 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=22:10 IST
=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.744 Prec@1=58.810 Prec@5=81.780 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=22:11 IST
=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.745 Prec@1=58.780 Prec@5=81.756 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=22:11 IST
=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.745 Prec@1=58.780 Prec@5=81.756 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=22:11 IST
=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.745 Prec@1=58.780 Prec@5=81.756 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=22:12 IST
=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.746 Prec@1=58.768 Prec@5=81.739 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=22:12 IST
=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.746 Prec@1=58.768 Prec@5=81.739 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=22:12 IST
=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.746 Prec@1=58.768 Prec@5=81.739 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=22:12 IST
=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.748 Prec@1=58.729 Prec@5=81.723 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=22:12 IST
=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.748 Prec@1=58.729 Prec@5=81.723 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=22:12 IST
=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.748 Prec@1=58.729 Prec@5=81.723 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=22:13 IST
=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.751 Prec@1=58.669 Prec@5=81.674 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=22:13 IST
=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.751 Prec@1=58.669 Prec@5=81.674 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=22:13 IST
=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.751 Prec@1=58.669 Prec@5=81.674 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=22:14 IST
=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.752 Prec@1=58.631 Prec@5=81.651 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=22:14 IST
=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.752 Prec@1=58.631 Prec@5=81.651 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=22:14 IST
=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.752 Prec@1=58.631 Prec@5=81.651 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=22:15 IST
=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.753 Prec@1=58.635 Prec@5=81.648 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=22:15 IST
=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.753 Prec@1=58.635 Prec@5=81.648 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=22:15 IST
=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.753 Prec@1=58.635 Prec@5=81.648 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=22:16 IST
=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.754 Prec@1=58.599 Prec@5=81.638 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=22:16 IST
=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.754 Prec@1=58.599 Prec@5=81.638 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:16 IST
=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.754 Prec@1=58.599 Prec@5=81.638 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:16 IST
=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.754 Prec@1=58.592 Prec@5=81.629 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:16 IST
=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.754 Prec@1=58.592 Prec@5=81.629 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=22:16 IST
=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.754 Prec@1=58.592 Prec@5=81.629 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=22:17 IST
=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.756 Prec@1=58.560 Prec@5=81.612 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=22:17 IST
=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.756 Prec@1=58.560 Prec@5=81.612 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=22:17 IST
=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.273 Loss=1.756 Prec@1=58.560 Prec@5=81.612 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=22:18 IST
=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.757 Prec@1=58.530 Prec@5=81.585 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=22:18 IST
=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.757 Prec@1=58.530 Prec@5=81.585 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=22:18 IST
=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.469 DataTime=0.272 Loss=1.757 Prec@1=58.530 Prec@5=81.585 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=22:19 IST
=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.757 Prec@1=58.519 Prec@5=81.591 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=22:19 IST
=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.757 Prec@1=58.519 Prec@5=81.591 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:19 IST
=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.757 Prec@1=58.519 Prec@5=81.591 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:19 IST
=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.758 Prec@1=58.509 Prec@5=81.584 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:19 IST
=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.758 Prec@1=58.509 Prec@5=81.584 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=22:19 IST
=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.758 Prec@1=58.509 Prec@5=81.584 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=22:20 IST
=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.760 Prec@1=58.483 Prec@5=81.551 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=22:20 IST
=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.760 Prec@1=58.483 Prec@5=81.551 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=22:20 IST
=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.760 Prec@1=58.483 Prec@5=81.551 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=22:21 IST
=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.552 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=22:21 IST
=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.552 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=22:21 IST
=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.552 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=22:22 IST
=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.550 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=22:22 IST
=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.550 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=22:22 IST
=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.467 DataTime=0.271 Loss=1.760 Prec@1=58.484 Prec@5=81.550 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=22:22 IST
=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.271 Loss=1.761 Prec@1=58.484 Prec@5=81.547 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=22:22 IST
=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.271 Loss=1.761 Prec@1=58.484 Prec@5=81.547 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=22:22 IST
=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.271 Loss=1.761 Prec@1=58.484 Prec@5=81.547 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=22:23 IST
=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.761 Prec@1=58.464 Prec@5=81.537 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=22:23 IST
=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.761 Prec@1=58.464 Prec@5=81.537 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=22:23 IST
=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.761 Prec@1=58.464 Prec@5=81.537 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=22:23 IST
=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.761 Prec@1=58.464 Prec@5=81.538 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=22:23 IST
=> training   100.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.468 DataTime=0.272 Loss=1.761 Prec@1=58.464 Prec@5=81.538 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=22:23 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> validation 0.00% of 1x98...Epoch=20/150 LR=0.09609 Time=6.230 Loss=1.098 Prec@1=72.266 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=6.230 Loss=1.098 Prec@1=72.266 Prec@5=91.406 rate=3624.25 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST
** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=6.230 Loss=1.098 Prec@1=72.266 Prec@5=91.406 rate=3624.25 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST
** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=0.547 Loss=1.727 Prec@1=58.542 Prec@5=82.468 rate=3624.25 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST
** validation 100.00% of 1x98...Epoch=20/150 LR=0.09609 Time=0.547 Loss=1.727 Prec@1=58.542 Prec@5=82.468 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=22:24 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST
=> training   0.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.039 DataTime=4.812 Loss=1.669 Prec@1=57.617 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST
=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.039 DataTime=4.812 Loss=1.669 Prec@1=57.617 Prec@5=83.594 rate=7813.42 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST
=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.039 DataTime=4.812 Loss=1.669 Prec@1=57.617 Prec@5=83.594 rate=7813.42 Hz, eta=0:00:00, total=0:00:00, wall=22:25 IST
=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.498 DataTime=0.306 Loss=1.704 Prec@1=59.669 Prec@5=82.213 rate=7813.42 Hz, eta=0:00:00, total=0:00:00, wall=22:25 IST
=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.498 DataTime=0.306 Loss=1.704 Prec@1=59.669 Prec@5=82.213 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:25 IST
=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.498 DataTime=0.306 Loss=1.704 Prec@1=59.669 Prec@5=82.213 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:26 IST
=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.483 DataTime=0.286 Loss=1.712 Prec@1=59.462 Prec@5=82.170 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=22:26 IST
=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.483 DataTime=0.286 Loss=1.712 Prec@1=59.462 Prec@5=82.170 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=22:26 IST
=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.483 DataTime=0.286 Loss=1.712 Prec@1=59.462 Prec@5=82.170 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=22:27 IST
=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.475 DataTime=0.278 Loss=1.715 Prec@1=59.454 Prec@5=82.180 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=22:27 IST
=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.475 DataTime=0.278 Loss=1.715 Prec@1=59.454 Prec@5=82.180 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=22:27 IST
=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.475 DataTime=0.278 Loss=1.715 Prec@1=59.454 Prec@5=82.180 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=22:27 IST
=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.473 DataTime=0.277 Loss=1.715 Prec@1=59.382 Prec@5=82.202 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=22:27 IST
=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.473 DataTime=0.277 Loss=1.715 Prec@1=59.382 Prec@5=82.202 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=22:27 IST
=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.473 DataTime=0.277 Loss=1.715 Prec@1=59.382 Prec@5=82.202 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=22:28 IST
=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.719 Prec@1=59.321 Prec@5=82.126 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=22:28 IST
=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.719 Prec@1=59.321 Prec@5=82.126 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=22:28 IST
=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.719 Prec@1=59.321 Prec@5=82.126 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=22:29 IST
=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.719 Prec@1=59.310 Prec@5=82.123 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=22:29 IST
=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.719 Prec@1=59.310 Prec@5=82.123 rate=2.17 Hz, eta=0:14:36, total=0:04:36, wall=22:29 IST
=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.719 Prec@1=59.310 Prec@5=82.123 rate=2.17 Hz, eta=0:14:36, total=0:04:36, wall=22:30 IST
=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.720 Prec@1=59.288 Prec@5=82.144 rate=2.17 Hz, eta=0:14:36, total=0:04:36, wall=22:30 IST
=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.720 Prec@1=59.288 Prec@5=82.144 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=22:30 IST
=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.720 Prec@1=59.288 Prec@5=82.144 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=22:31 IST
=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.722 Prec@1=59.230 Prec@5=82.117 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=22:31 IST
=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.722 Prec@1=59.230 Prec@5=82.117 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=22:31 IST
=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.722 Prec@1=59.230 Prec@5=82.117 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=22:31 IST
=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.725 Prec@1=59.143 Prec@5=82.077 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=22:31 IST
=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.725 Prec@1=59.143 Prec@5=82.077 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=22:31 IST
=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.472 DataTime=0.276 Loss=1.725 Prec@1=59.143 Prec@5=82.077 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=22:32 IST
=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.276 Loss=1.727 Prec@1=59.117 Prec@5=82.031 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=22:32 IST
=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.276 Loss=1.727 Prec@1=59.117 Prec@5=82.031 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=22:32 IST
=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.276 Loss=1.727 Prec@1=59.117 Prec@5=82.031 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=22:33 IST
=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.728 Prec@1=59.088 Prec@5=82.010 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=22:33 IST
=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.728 Prec@1=59.088 Prec@5=82.010 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=22:33 IST
=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.728 Prec@1=59.088 Prec@5=82.010 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=22:34 IST
=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.729 Prec@1=59.066 Prec@5=82.013 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=22:34 IST
=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.729 Prec@1=59.066 Prec@5=82.013 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:34 IST
=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.729 Prec@1=59.066 Prec@5=82.013 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:34 IST
=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.730 Prec@1=59.038 Prec@5=81.998 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:34 IST
=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.730 Prec@1=59.038 Prec@5=81.998 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:34 IST
=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.730 Prec@1=59.038 Prec@5=81.998 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:35 IST
=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.731 Prec@1=59.015 Prec@5=81.979 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:35 IST
=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.731 Prec@1=59.015 Prec@5=81.979 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=22:35 IST
=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.471 DataTime=0.275 Loss=1.731 Prec@1=59.015 Prec@5=81.979 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=22:36 IST
=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.275 Loss=1.732 Prec@1=59.007 Prec@5=81.967 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=22:36 IST
=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.275 Loss=1.732 Prec@1=59.007 Prec@5=81.967 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=22:36 IST
=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.275 Loss=1.732 Prec@1=59.007 Prec@5=81.967 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=22:37 IST
=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.734 Prec@1=58.993 Prec@5=81.949 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=22:37 IST
=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.734 Prec@1=58.993 Prec@5=81.949 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=22:37 IST
=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.470 DataTime=0.274 Loss=1.734 Prec@1=58.993 Prec@5=81.949 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=22:38 IST
=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.735 Prec@1=58.988 Prec@5=81.934 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=22:38 IST
=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.735 Prec@1=58.988 Prec@5=81.934 rate=2.14 Hz, eta=0:06:13, total=0:13:13, wall=22:38 IST
=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.735 Prec@1=58.988 Prec@5=81.934 rate=2.14 Hz, eta=0:06:13, total=0:13:13, wall=22:38 IST
=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.736 Prec@1=58.960 Prec@5=81.904 rate=2.14 Hz, eta=0:06:13, total=0:13:13, wall=22:38 IST
=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.736 Prec@1=58.960 Prec@5=81.904 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=22:38 IST
=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.736 Prec@1=58.960 Prec@5=81.904 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=22:39 IST
=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.737 Prec@1=58.960 Prec@5=81.899 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=22:39 IST
=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.737 Prec@1=58.960 Prec@5=81.899 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:39 IST
=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.469 DataTime=0.273 Loss=1.737 Prec@1=58.960 Prec@5=81.899 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:40 IST
=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.273 Loss=1.738 Prec@1=58.924 Prec@5=81.870 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:40 IST
=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.273 Loss=1.738 Prec@1=58.924 Prec@5=81.870 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=22:40 IST
=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.273 Loss=1.738 Prec@1=58.924 Prec@5=81.870 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=22:41 IST
=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.905 Prec@5=81.842 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=22:41 IST
=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.905 Prec@5=81.842 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=22:41 IST
=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.905 Prec@5=81.842 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=22:41 IST
=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.884 Prec@5=81.837 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=22:41 IST
=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.884 Prec@5=81.837 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=22:41 IST
=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.740 Prec@1=58.884 Prec@5=81.837 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=22:42 IST
=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.741 Prec@1=58.849 Prec@5=81.814 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=22:42 IST
=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.741 Prec@1=58.849 Prec@5=81.814 rate=2.15 Hz, eta=0:01:34, total=0:17:50, wall=22:42 IST
=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.468 DataTime=0.272 Loss=1.741 Prec@1=58.849 Prec@5=81.814 rate=2.15 Hz, eta=0:01:34, total=0:17:50, wall=22:43 IST
=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.271 Loss=1.742 Prec@1=58.835 Prec@5=81.803 rate=2.15 Hz, eta=0:01:34, total=0:17:50, wall=22:43 IST
=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.271 Loss=1.742 Prec@1=58.835 Prec@5=81.803 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=22:43 IST
=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.271 Loss=1.742 Prec@1=58.835 Prec@5=81.803 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=22:44 IST
=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.272 Loss=1.743 Prec@1=58.830 Prec@5=81.797 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=22:44 IST
=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.272 Loss=1.743 Prec@1=58.830 Prec@5=81.797 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=22:44 IST
=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.272 Loss=1.743 Prec@1=58.830 Prec@5=81.797 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=22:44 IST
=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.272 Loss=1.743 Prec@1=58.828 Prec@5=81.796 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=22:44 IST
=> training   100.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.467 DataTime=0.272 Loss=1.743 Prec@1=58.828 Prec@5=81.796 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=22:44 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:44 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:44 IST
=> validation 0.00% of 1x98...Epoch=21/150 LR=0.09568 Time=6.952 Loss=1.200 Prec@1=70.898 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=22:44 IST
=> validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=6.952 Loss=1.200 Prec@1=70.898 Prec@5=90.234 rate=8282.95 Hz, eta=0:00:00, total=0:00:00, wall=22:44 IST
** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=6.952 Loss=1.200 Prec@1=70.898 Prec@5=90.234 rate=8282.95 Hz, eta=0:00:00, total=0:00:00, wall=22:45 IST
** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=0.550 Loss=1.743 Prec@1=58.398 Prec@5=82.184 rate=8282.95 Hz, eta=0:00:00, total=0:00:00, wall=22:45 IST
** validation 100.00% of 1x98...Epoch=21/150 LR=0.09568 Time=0.550 Loss=1.743 Prec@1=58.398 Prec@5=82.184 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=22:45 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:45 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:45 IST
=> training   0.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=4.630 DataTime=4.367 Loss=1.693 Prec@1=57.812 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=22:45 IST
=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=4.630 DataTime=4.367 Loss=1.693 Prec@1=57.812 Prec@5=83.008 rate=4623.12 Hz, eta=0:00:00, total=0:00:00, wall=22:45 IST
=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=4.630 DataTime=4.367 Loss=1.693 Prec@1=57.812 Prec@5=83.008 rate=4623.12 Hz, eta=0:00:00, total=0:00:00, wall=22:46 IST
=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.508 DataTime=0.314 Loss=1.686 Prec@1=59.814 Prec@5=82.664 rate=4623.12 Hz, eta=0:00:00, total=0:00:00, wall=22:46 IST
=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.508 DataTime=0.314 Loss=1.686 Prec@1=59.814 Prec@5=82.664 rate=2.16 Hz, eta=0:18:31, total=0:00:46, wall=22:46 IST
=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.508 DataTime=0.314 Loss=1.686 Prec@1=59.814 Prec@5=82.664 rate=2.16 Hz, eta=0:18:31, total=0:00:46, wall=22:46 IST
=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.489 DataTime=0.293 Loss=1.698 Prec@1=59.695 Prec@5=82.483 rate=2.16 Hz, eta=0:18:31, total=0:00:46, wall=22:46 IST
=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.489 DataTime=0.293 Loss=1.698 Prec@1=59.695 Prec@5=82.483 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=22:46 IST
=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.489 DataTime=0.293 Loss=1.698 Prec@1=59.695 Prec@5=82.483 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=22:47 IST
=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.478 DataTime=0.284 Loss=1.699 Prec@1=59.740 Prec@5=82.416 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=22:47 IST
=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.478 DataTime=0.284 Loss=1.699 Prec@1=59.740 Prec@5=82.416 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=22:47 IST
=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.478 DataTime=0.284 Loss=1.699 Prec@1=59.740 Prec@5=82.416 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=22:48 IST
=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.475 DataTime=0.280 Loss=1.701 Prec@1=59.666 Prec@5=82.400 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=22:48 IST
=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.475 DataTime=0.280 Loss=1.701 Prec@1=59.666 Prec@5=82.400 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=22:48 IST
=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.475 DataTime=0.280 Loss=1.701 Prec@1=59.666 Prec@5=82.400 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=22:49 IST
=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.700 Prec@1=59.724 Prec@5=82.441 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=22:49 IST
=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.700 Prec@1=59.724 Prec@5=82.441 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=22:49 IST
=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.700 Prec@1=59.724 Prec@5=82.441 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=22:49 IST
=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.704 Prec@1=59.624 Prec@5=82.375 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=22:49 IST
=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.704 Prec@1=59.624 Prec@5=82.375 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=22:49 IST
=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.704 Prec@1=59.624 Prec@5=82.375 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=22:50 IST
=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.706 Prec@1=59.595 Prec@5=82.347 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=22:50 IST
=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.706 Prec@1=59.595 Prec@5=82.347 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=22:50 IST
=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.473 DataTime=0.278 Loss=1.706 Prec@1=59.595 Prec@5=82.347 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=22:51 IST
=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.277 Loss=1.709 Prec@1=59.554 Prec@5=82.291 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=22:51 IST
=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.277 Loss=1.709 Prec@1=59.554 Prec@5=82.291 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=22:51 IST
=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.277 Loss=1.709 Prec@1=59.554 Prec@5=82.291 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=22:52 IST
=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.711 Prec@1=59.501 Prec@5=82.262 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=22:52 IST
=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.711 Prec@1=59.501 Prec@5=82.262 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=22:52 IST
=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.711 Prec@1=59.501 Prec@5=82.262 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=22:53 IST
=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.276 Loss=1.713 Prec@1=59.463 Prec@5=82.223 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=22:53 IST
=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.276 Loss=1.713 Prec@1=59.463 Prec@5=82.223 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=22:53 IST
=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.472 DataTime=0.276 Loss=1.713 Prec@1=59.463 Prec@5=82.223 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=22:53 IST
=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.716 Prec@1=59.400 Prec@5=82.185 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=22:53 IST
=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.716 Prec@1=59.400 Prec@5=82.185 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=22:53 IST
=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.716 Prec@1=59.400 Prec@5=82.185 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=22:54 IST
=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.716 Prec@1=59.380 Prec@5=82.190 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=22:54 IST
=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.716 Prec@1=59.380 Prec@5=82.190 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:54 IST
=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.716 Prec@1=59.380 Prec@5=82.190 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:55 IST
=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.717 Prec@1=59.372 Prec@5=82.173 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=22:55 IST
=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.717 Prec@1=59.372 Prec@5=82.173 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:55 IST
=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.717 Prec@1=59.372 Prec@5=82.173 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:56 IST
=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.719 Prec@1=59.330 Prec@5=82.142 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=22:56 IST
=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.719 Prec@1=59.330 Prec@5=82.142 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=22:56 IST
=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.719 Prec@1=59.330 Prec@5=82.142 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=22:56 IST
=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.721 Prec@1=59.291 Prec@5=82.130 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=22:56 IST
=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.721 Prec@1=59.291 Prec@5=82.130 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=22:56 IST
=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.721 Prec@1=59.291 Prec@5=82.130 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=22:57 IST
=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.273 Prec@5=82.122 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=22:57 IST
=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.273 Prec@5=82.122 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=22:57 IST
=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.273 Prec@5=82.122 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=22:58 IST
=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.245 Prec@5=82.120 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=22:58 IST
=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.245 Prec@5=82.120 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=22:58 IST
=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.722 Prec@1=59.245 Prec@5=82.120 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=22:59 IST
=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.723 Prec@1=59.247 Prec@5=82.099 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=22:59 IST
=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.723 Prec@1=59.247 Prec@5=82.099 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=22:59 IST
=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.275 Loss=1.723 Prec@1=59.247 Prec@5=82.099 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=23:00 IST
=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.724 Prec@1=59.217 Prec@5=82.090 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=23:00 IST
=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.724 Prec@1=59.217 Prec@5=82.090 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=23:00 IST
=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.724 Prec@1=59.217 Prec@5=82.090 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=23:00 IST
=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.208 Prec@5=82.069 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=23:00 IST
=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.208 Prec@5=82.069 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=23:00 IST
=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.208 Prec@5=82.069 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=23:01 IST
=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.209 Prec@5=82.072 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=23:01 IST
=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.209 Prec@5=82.072 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=23:01 IST
=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.725 Prec@1=59.209 Prec@5=82.072 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=23:02 IST
=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.726 Prec@1=59.193 Prec@5=82.046 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=23:02 IST
=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.726 Prec@1=59.193 Prec@5=82.046 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=23:02 IST
=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.471 DataTime=0.276 Loss=1.726 Prec@1=59.193 Prec@5=82.046 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=23:03 IST
=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.727 Prec@1=59.187 Prec@5=82.037 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=23:03 IST
=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.727 Prec@1=59.187 Prec@5=82.037 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=23:03 IST
=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.727 Prec@1=59.187 Prec@5=82.037 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=23:03 IST
=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.158 Prec@5=82.023 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=23:03 IST
=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.158 Prec@5=82.023 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=23:03 IST
=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.158 Prec@5=82.023 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=23:04 IST
=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.156 Prec@5=82.022 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=23:04 IST
=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.156 Prec@5=82.022 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=23:04 IST
=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.156 Prec@5=82.022 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=23:04 IST
=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.155 Prec@5=82.022 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=23:04 IST
=> training   100.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.470 DataTime=0.275 Loss=1.728 Prec@1=59.155 Prec@5=82.022 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=23:04 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:04 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:04 IST
=> validation 0.00% of 1x98...Epoch=22/150 LR=0.09524 Time=6.841 Loss=1.186 Prec@1=70.312 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=23:04 IST
=> validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=6.841 Loss=1.186 Prec@1=70.312 Prec@5=90.039 rate=4880.36 Hz, eta=0:00:00, total=0:00:00, wall=23:04 IST
** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=6.841 Loss=1.186 Prec@1=70.312 Prec@5=90.039 rate=4880.36 Hz, eta=0:00:00, total=0:00:00, wall=23:05 IST
** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=0.551 Loss=1.692 Prec@1=59.632 Prec@5=83.102 rate=4880.36 Hz, eta=0:00:00, total=0:00:00, wall=23:05 IST
** validation 100.00% of 1x98...Epoch=22/150 LR=0.09524 Time=0.551 Loss=1.692 Prec@1=59.632 Prec@5=83.102 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=23:05 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:05 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:05 IST
=> training   0.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=4.953 DataTime=4.708 Loss=1.618 Prec@1=62.305 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=23:05 IST
=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=4.953 DataTime=4.708 Loss=1.618 Prec@1=62.305 Prec@5=84.180 rate=7799.52 Hz, eta=0:00:00, total=0:00:00, wall=23:05 IST
=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=4.953 DataTime=4.708 Loss=1.618 Prec@1=62.305 Prec@5=84.180 rate=7799.52 Hz, eta=0:00:00, total=0:00:00, wall=23:06 IST
=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.502 DataTime=0.307 Loss=1.665 Prec@1=60.170 Prec@5=83.081 rate=7799.52 Hz, eta=0:00:00, total=0:00:00, wall=23:06 IST
=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.502 DataTime=0.307 Loss=1.665 Prec@1=60.170 Prec@5=83.081 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:06 IST
=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.502 DataTime=0.307 Loss=1.665 Prec@1=60.170 Prec@5=83.081 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:07 IST
=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.480 DataTime=0.284 Loss=1.674 Prec@1=60.051 Prec@5=82.936 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:07 IST
=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.480 DataTime=0.284 Loss=1.674 Prec@1=60.051 Prec@5=82.936 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=23:07 IST
=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.480 DataTime=0.284 Loss=1.674 Prec@1=60.051 Prec@5=82.936 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=23:08 IST
=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.472 DataTime=0.276 Loss=1.683 Prec@1=59.925 Prec@5=82.809 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=23:08 IST
=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.472 DataTime=0.276 Loss=1.683 Prec@1=59.925 Prec@5=82.809 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=23:08 IST
=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.472 DataTime=0.276 Loss=1.683 Prec@1=59.925 Prec@5=82.809 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=23:08 IST
=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.467 DataTime=0.272 Loss=1.684 Prec@1=59.851 Prec@5=82.763 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=23:08 IST
=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.467 DataTime=0.272 Loss=1.684 Prec@1=59.851 Prec@5=82.763 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=23:08 IST
=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.467 DataTime=0.272 Loss=1.684 Prec@1=59.851 Prec@5=82.763 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=23:09 IST
=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.466 DataTime=0.270 Loss=1.687 Prec@1=59.818 Prec@5=82.726 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=23:09 IST
=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.466 DataTime=0.270 Loss=1.687 Prec@1=59.818 Prec@5=82.726 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=23:09 IST
=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.466 DataTime=0.270 Loss=1.687 Prec@1=59.818 Prec@5=82.726 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=23:10 IST
=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.689 Prec@1=59.772 Prec@5=82.679 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=23:10 IST
=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.689 Prec@1=59.772 Prec@5=82.679 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=23:10 IST
=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.689 Prec@1=59.772 Prec@5=82.679 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=23:11 IST
=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.269 Loss=1.692 Prec@1=59.719 Prec@5=82.619 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=23:11 IST
=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.269 Loss=1.692 Prec@1=59.719 Prec@5=82.619 rate=2.19 Hz, eta=0:13:43, total=0:05:20, wall=23:11 IST
=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.269 Loss=1.692 Prec@1=59.719 Prec@5=82.619 rate=2.19 Hz, eta=0:13:43, total=0:05:20, wall=23:11 IST
=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.693 Prec@1=59.640 Prec@5=82.596 rate=2.19 Hz, eta=0:13:43, total=0:05:20, wall=23:11 IST
=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.693 Prec@1=59.640 Prec@5=82.596 rate=2.18 Hz, eta=0:12:58, total=0:06:06, wall=23:11 IST
=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.693 Prec@1=59.640 Prec@5=82.596 rate=2.18 Hz, eta=0:12:58, total=0:06:06, wall=23:12 IST
=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.695 Prec@1=59.618 Prec@5=82.550 rate=2.18 Hz, eta=0:12:58, total=0:06:06, wall=23:12 IST
=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.695 Prec@1=59.618 Prec@5=82.550 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=23:12 IST
=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.465 DataTime=0.269 Loss=1.695 Prec@1=59.618 Prec@5=82.550 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=23:13 IST
=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.697 Prec@1=59.599 Prec@5=82.503 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=23:13 IST
=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.697 Prec@1=59.599 Prec@5=82.503 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=23:13 IST
=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.697 Prec@1=59.599 Prec@5=82.503 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=23:14 IST
=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.700 Prec@1=59.567 Prec@5=82.460 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=23:14 IST
=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.700 Prec@1=59.567 Prec@5=82.460 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=23:14 IST
=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.700 Prec@1=59.567 Prec@5=82.460 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=23:14 IST
=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.700 Prec@1=59.539 Prec@5=82.450 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=23:14 IST
=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.700 Prec@1=59.539 Prec@5=82.450 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=23:14 IST
=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.700 Prec@1=59.539 Prec@5=82.450 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=23:15 IST
=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.702 Prec@1=59.505 Prec@5=82.425 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=23:15 IST
=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.702 Prec@1=59.505 Prec@5=82.425 rate=2.18 Hz, eta=0:09:12, total=0:09:57, wall=23:15 IST
=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.702 Prec@1=59.505 Prec@5=82.425 rate=2.18 Hz, eta=0:09:12, total=0:09:57, wall=23:16 IST
=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.703 Prec@1=59.501 Prec@5=82.413 rate=2.18 Hz, eta=0:09:12, total=0:09:57, wall=23:16 IST
=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.703 Prec@1=59.501 Prec@5=82.413 rate=2.17 Hz, eta=0:08:26, total=0:10:44, wall=23:16 IST
=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.703 Prec@1=59.501 Prec@5=82.413 rate=2.17 Hz, eta=0:08:26, total=0:10:44, wall=23:17 IST
=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.705 Prec@1=59.475 Prec@5=82.385 rate=2.17 Hz, eta=0:08:26, total=0:10:44, wall=23:17 IST
=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.705 Prec@1=59.475 Prec@5=82.385 rate=2.17 Hz, eta=0:07:40, total=0:11:30, wall=23:17 IST
=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.463 DataTime=0.267 Loss=1.705 Prec@1=59.475 Prec@5=82.385 rate=2.17 Hz, eta=0:07:40, total=0:11:30, wall=23:18 IST
=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.451 Prec@5=82.368 rate=2.17 Hz, eta=0:07:40, total=0:11:30, wall=23:18 IST
=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.451 Prec@5=82.368 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=23:18 IST
=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.451 Prec@5=82.368 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=23:18 IST
=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.442 Prec@5=82.346 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=23:18 IST
=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.442 Prec@5=82.346 rate=2.17 Hz, eta=0:06:09, total=0:13:04, wall=23:18 IST
=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.707 Prec@1=59.442 Prec@5=82.346 rate=2.17 Hz, eta=0:06:09, total=0:13:04, wall=23:19 IST
=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.709 Prec@1=59.436 Prec@5=82.325 rate=2.17 Hz, eta=0:06:09, total=0:13:04, wall=23:19 IST
=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.709 Prec@1=59.436 Prec@5=82.325 rate=2.17 Hz, eta=0:05:24, total=0:13:51, wall=23:19 IST
=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.709 Prec@1=59.436 Prec@5=82.325 rate=2.17 Hz, eta=0:05:24, total=0:13:51, wall=23:20 IST
=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.710 Prec@1=59.432 Prec@5=82.310 rate=2.17 Hz, eta=0:05:24, total=0:13:51, wall=23:20 IST
=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.710 Prec@1=59.432 Prec@5=82.310 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=23:20 IST
=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.710 Prec@1=59.432 Prec@5=82.310 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=23:21 IST
=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.267 Loss=1.711 Prec@1=59.421 Prec@5=82.287 rate=2.17 Hz, eta=0:04:37, total=0:14:37, wall=23:21 IST
=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.267 Loss=1.711 Prec@1=59.421 Prec@5=82.287 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=23:21 IST
=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.267 Loss=1.711 Prec@1=59.421 Prec@5=82.287 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=23:21 IST
=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.398 Prec@5=82.267 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=23:21 IST
=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.398 Prec@5=82.267 rate=2.17 Hz, eta=0:03:05, total=0:16:10, wall=23:21 IST
=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.398 Prec@5=82.267 rate=2.17 Hz, eta=0:03:05, total=0:16:10, wall=23:22 IST
=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.399 Prec@5=82.258 rate=2.17 Hz, eta=0:03:05, total=0:16:10, wall=23:22 IST
=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.399 Prec@5=82.258 rate=2.16 Hz, eta=0:02:19, total=0:16:56, wall=23:22 IST
=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.399 Prec@5=82.258 rate=2.16 Hz, eta=0:02:19, total=0:16:56, wall=23:23 IST
=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.401 Prec@5=82.249 rate=2.16 Hz, eta=0:02:19, total=0:16:56, wall=23:23 IST
=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.401 Prec@5=82.249 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=23:23 IST
=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.713 Prec@1=59.401 Prec@5=82.249 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=23:24 IST
=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.714 Prec@1=59.394 Prec@5=82.239 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=23:24 IST
=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.714 Prec@1=59.394 Prec@5=82.239 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=23:24 IST
=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.714 Prec@1=59.394 Prec@5=82.239 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=23:25 IST
=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.715 Prec@1=59.372 Prec@5=82.226 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=23:25 IST
=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.715 Prec@1=59.372 Prec@5=82.226 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=23:25 IST
=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.715 Prec@1=59.372 Prec@5=82.226 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=23:25 IST
=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.715 Prec@1=59.372 Prec@5=82.226 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=23:25 IST
=> training   100.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.464 DataTime=0.268 Loss=1.715 Prec@1=59.372 Prec@5=82.226 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=23:25 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> validation 0.00% of 1x98...Epoch=23/150 LR=0.09479 Time=7.232 Loss=1.114 Prec@1=72.461 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=7.232 Loss=1.114 Prec@1=72.461 Prec@5=90.234 rate=7393.55 Hz, eta=0:00:00, total=0:00:00, wall=23:25 IST
** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=7.232 Loss=1.114 Prec@1=72.461 Prec@5=90.234 rate=7393.55 Hz, eta=0:00:00, total=0:00:00, wall=23:25 IST
** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=0.549 Loss=1.684 Prec@1=59.764 Prec@5=82.990 rate=7393.55 Hz, eta=0:00:00, total=0:00:00, wall=23:25 IST
** validation 100.00% of 1x98...Epoch=23/150 LR=0.09479 Time=0.549 Loss=1.684 Prec@1=59.764 Prec@5=82.990 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=23:25 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> training   0.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.113 DataTime=4.814 Loss=1.615 Prec@1=61.523 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.113 DataTime=4.814 Loss=1.615 Prec@1=61.523 Prec@5=81.250 rate=12662.88 Hz, eta=0:00:00, total=0:00:00, wall=23:26 IST
=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.113 DataTime=4.814 Loss=1.615 Prec@1=61.523 Prec@5=81.250 rate=12662.88 Hz, eta=0:00:00, total=0:00:00, wall=23:26 IST
=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.503 DataTime=0.308 Loss=1.670 Prec@1=60.158 Prec@5=82.783 rate=12662.88 Hz, eta=0:00:00, total=0:00:00, wall=23:26 IST
=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.503 DataTime=0.308 Loss=1.670 Prec@1=60.158 Prec@5=82.783 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:26 IST
=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.503 DataTime=0.308 Loss=1.670 Prec@1=60.158 Prec@5=82.783 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:27 IST
=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.487 DataTime=0.292 Loss=1.673 Prec@1=60.144 Prec@5=82.737 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=23:27 IST
=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.487 DataTime=0.292 Loss=1.673 Prec@1=60.144 Prec@5=82.737 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=23:27 IST
=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.487 DataTime=0.292 Loss=1.673 Prec@1=60.144 Prec@5=82.737 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=23:28 IST
=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.678 Prec@1=60.139 Prec@5=82.700 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=23:28 IST
=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.678 Prec@1=60.139 Prec@5=82.700 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=23:28 IST
=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.678 Prec@1=60.139 Prec@5=82.700 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=23:29 IST
=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.679 Prec@1=60.052 Prec@5=82.648 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=23:29 IST
=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.679 Prec@1=60.052 Prec@5=82.648 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=23:29 IST
=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.679 Prec@1=60.052 Prec@5=82.648 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=23:29 IST
=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.277 Loss=1.684 Prec@1=60.009 Prec@5=82.598 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=23:29 IST
=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.277 Loss=1.684 Prec@1=60.009 Prec@5=82.598 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=23:29 IST
=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.277 Loss=1.684 Prec@1=60.009 Prec@5=82.598 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=23:30 IST
=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.685 Prec@1=60.049 Prec@5=82.587 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=23:30 IST
=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.685 Prec@1=60.049 Prec@5=82.587 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:30 IST
=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.685 Prec@1=60.049 Prec@5=82.587 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:31 IST
=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.687 Prec@1=60.003 Prec@5=82.563 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:31 IST
=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.687 Prec@1=60.003 Prec@5=82.563 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:31 IST
=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.472 DataTime=0.276 Loss=1.687 Prec@1=60.003 Prec@5=82.563 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:32 IST
=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.690 Prec@1=59.950 Prec@5=82.518 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:32 IST
=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.690 Prec@1=59.950 Prec@5=82.518 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=23:32 IST
=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.690 Prec@1=59.950 Prec@5=82.518 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=23:33 IST
=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.691 Prec@1=59.915 Prec@5=82.518 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=23:33 IST
=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.691 Prec@1=59.915 Prec@5=82.518 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=23:33 IST
=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.691 Prec@1=59.915 Prec@5=82.518 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=23:33 IST
=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.692 Prec@1=59.881 Prec@5=82.518 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=23:33 IST
=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.692 Prec@1=59.881 Prec@5=82.518 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=23:33 IST
=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.692 Prec@1=59.881 Prec@5=82.518 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=23:34 IST
=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.693 Prec@1=59.853 Prec@5=82.510 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=23:34 IST
=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.693 Prec@1=59.853 Prec@5=82.510 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=23:34 IST
=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.693 Prec@1=59.853 Prec@5=82.510 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=23:35 IST
=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.804 Prec@5=82.483 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=23:35 IST
=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.804 Prec@5=82.483 rate=2.12 Hz, eta=0:10:14, total=0:09:27, wall=23:35 IST
=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.804 Prec@5=82.483 rate=2.12 Hz, eta=0:10:14, total=0:09:27, wall=23:36 IST
=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.697 Prec@1=59.784 Prec@5=82.464 rate=2.12 Hz, eta=0:10:14, total=0:09:27, wall=23:36 IST
=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.697 Prec@1=59.784 Prec@5=82.464 rate=2.12 Hz, eta=0:09:27, total=0:10:13, wall=23:36 IST
=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.697 Prec@1=59.784 Prec@5=82.464 rate=2.12 Hz, eta=0:09:27, total=0:10:13, wall=23:37 IST
=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.811 Prec@5=82.461 rate=2.12 Hz, eta=0:09:27, total=0:10:13, wall=23:37 IST
=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.811 Prec@5=82.461 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=23:37 IST
=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.281 Loss=1.696 Prec@1=59.811 Prec@5=82.461 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=23:37 IST
=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.696 Prec@1=59.797 Prec@5=82.460 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=23:37 IST
=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.696 Prec@1=59.797 Prec@5=82.460 rate=2.12 Hz, eta=0:07:52, total=0:11:48, wall=23:37 IST
=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.696 Prec@1=59.797 Prec@5=82.460 rate=2.12 Hz, eta=0:07:52, total=0:11:48, wall=23:38 IST
=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.698 Prec@1=59.759 Prec@5=82.436 rate=2.12 Hz, eta=0:07:52, total=0:11:48, wall=23:38 IST
=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.698 Prec@1=59.759 Prec@5=82.436 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=23:38 IST
=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.698 Prec@1=59.759 Prec@5=82.436 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=23:39 IST
=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.698 Prec@1=59.773 Prec@5=82.431 rate=2.12 Hz, eta=0:07:05, total=0:12:34, wall=23:39 IST
=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.698 Prec@1=59.773 Prec@5=82.431 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=23:39 IST
=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.698 Prec@1=59.773 Prec@5=82.431 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=23:40 IST
=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.699 Prec@1=59.759 Prec@5=82.415 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=23:40 IST
=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.699 Prec@1=59.759 Prec@5=82.415 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:40 IST
=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.699 Prec@1=59.759 Prec@5=82.415 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:41 IST
=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.742 Prec@5=82.405 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:41 IST
=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.742 Prec@5=82.405 rate=2.11 Hz, eta=0:04:44, total=0:14:58, wall=23:41 IST
=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.742 Prec@5=82.405 rate=2.11 Hz, eta=0:04:44, total=0:14:58, wall=23:41 IST
=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.721 Prec@5=82.396 rate=2.11 Hz, eta=0:04:44, total=0:14:58, wall=23:41 IST
=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.721 Prec@5=82.396 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=23:41 IST
=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.700 Prec@1=59.721 Prec@5=82.396 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=23:42 IST
=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.701 Prec@1=59.714 Prec@5=82.388 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=23:42 IST
=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.701 Prec@1=59.714 Prec@5=82.388 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=23:42 IST
=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.476 DataTime=0.280 Loss=1.701 Prec@1=59.714 Prec@5=82.388 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=23:43 IST
=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.702 Prec@1=59.702 Prec@5=82.378 rate=2.11 Hz, eta=0:03:10, total=0:16:34, wall=23:43 IST
=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.702 Prec@1=59.702 Prec@5=82.378 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=23:43 IST
=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.280 Loss=1.702 Prec@1=59.702 Prec@5=82.378 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=23:44 IST
=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.703 Prec@1=59.694 Prec@5=82.362 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=23:44 IST
=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.703 Prec@1=59.694 Prec@5=82.362 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=23:44 IST
=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.475 DataTime=0.279 Loss=1.703 Prec@1=59.694 Prec@5=82.362 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=23:44 IST
=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=23:44 IST
=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:00:48, total=0:18:54, wall=23:44 IST
=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.279 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:00:48, total=0:18:54, wall=23:45 IST
=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:00:48, total=0:18:54, wall=23:45 IST
=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=23:45 IST
=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.703 Prec@1=59.685 Prec@5=82.359 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=23:45 IST
=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.703 Prec@1=59.684 Prec@5=82.359 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=23:45 IST
=> training   100.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.474 DataTime=0.278 Loss=1.703 Prec@1=59.684 Prec@5=82.359 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=23:45 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST
=> validation 0.00% of 1x98...Epoch=24/150 LR=0.09431 Time=6.834 Loss=1.140 Prec@1=69.922 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST
=> validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.834 Loss=1.140 Prec@1=69.922 Prec@5=91.797 rate=10801.70 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST
** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.834 Loss=1.140 Prec@1=69.922 Prec@5=91.797 rate=10801.70 Hz, eta=0:00:00, total=0:00:00, wall=23:46 IST
** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=0.551 Loss=1.704 Prec@1=59.240 Prec@5=82.616 rate=10801.70 Hz, eta=0:00:00, total=0:00:00, wall=23:46 IST
** validation 100.00% of 1x98...Epoch=24/150 LR=0.09431 Time=0.551 Loss=1.704 Prec@1=59.240 Prec@5=82.616 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=23:46 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:46 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:46 IST
=> training   0.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.700 DataTime=4.481 Loss=1.797 Prec@1=57.617 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=23:46 IST
=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.700 DataTime=4.481 Loss=1.797 Prec@1=57.617 Prec@5=80.273 rate=7484.75 Hz, eta=0:00:00, total=0:00:00, wall=23:46 IST
=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.700 DataTime=4.481 Loss=1.797 Prec@1=57.617 Prec@5=80.273 rate=7484.75 Hz, eta=0:00:00, total=0:00:00, wall=23:47 IST
=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.502 DataTime=0.308 Loss=1.665 Prec@1=60.332 Prec@5=83.033 rate=7484.75 Hz, eta=0:00:00, total=0:00:00, wall=23:47 IST
=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.502 DataTime=0.308 Loss=1.665 Prec@1=60.332 Prec@5=83.033 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=23:47 IST
=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.502 DataTime=0.308 Loss=1.665 Prec@1=60.332 Prec@5=83.033 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=23:48 IST
=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.484 DataTime=0.288 Loss=1.670 Prec@1=60.285 Prec@5=82.966 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=23:48 IST
=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.484 DataTime=0.288 Loss=1.670 Prec@1=60.285 Prec@5=82.966 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=23:48 IST
=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.484 DataTime=0.288 Loss=1.670 Prec@1=60.285 Prec@5=82.966 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=23:49 IST
=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.477 DataTime=0.283 Loss=1.667 Prec@1=60.352 Prec@5=82.920 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=23:49 IST
=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.477 DataTime=0.283 Loss=1.667 Prec@1=60.352 Prec@5=82.920 rate=2.17 Hz, eta=0:16:57, total=0:02:19, wall=23:49 IST
=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.477 DataTime=0.283 Loss=1.667 Prec@1=60.352 Prec@5=82.920 rate=2.17 Hz, eta=0:16:57, total=0:02:19, wall=23:49 IST
=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.475 DataTime=0.281 Loss=1.666 Prec@1=60.354 Prec@5=82.964 rate=2.17 Hz, eta=0:16:57, total=0:02:19, wall=23:49 IST
=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.475 DataTime=0.281 Loss=1.666 Prec@1=60.354 Prec@5=82.964 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=23:49 IST
=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.475 DataTime=0.281 Loss=1.666 Prec@1=60.354 Prec@5=82.964 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=23:50 IST
=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.473 DataTime=0.279 Loss=1.668 Prec@1=60.318 Prec@5=82.966 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=23:50 IST
=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.473 DataTime=0.279 Loss=1.668 Prec@1=60.318 Prec@5=82.966 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=23:50 IST
=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.473 DataTime=0.279 Loss=1.668 Prec@1=60.318 Prec@5=82.966 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=23:51 IST
=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.668 Prec@1=60.325 Prec@5=82.970 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=23:51 IST
=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.668 Prec@1=60.325 Prec@5=82.970 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:51 IST
=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.668 Prec@1=60.325 Prec@5=82.970 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:52 IST
=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.672 Prec@1=60.257 Prec@5=82.900 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=23:52 IST
=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.672 Prec@1=60.257 Prec@5=82.900 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:52 IST
=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.672 Prec@1=60.257 Prec@5=82.900 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:52 IST
=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.674 Prec@1=60.214 Prec@5=82.843 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=23:52 IST
=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.674 Prec@1=60.214 Prec@5=82.843 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=23:52 IST
=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.277 Loss=1.674 Prec@1=60.214 Prec@5=82.843 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=23:53 IST
=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.821 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=23:53 IST
=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.821 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=23:53 IST
=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.821 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=23:54 IST
=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.470 DataTime=0.275 Loss=1.679 Prec@1=60.136 Prec@5=82.777 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=23:54 IST
=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.470 DataTime=0.275 Loss=1.679 Prec@1=60.136 Prec@5=82.777 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=23:54 IST
=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.470 DataTime=0.275 Loss=1.679 Prec@1=60.136 Prec@5=82.777 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=23:55 IST
=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.469 DataTime=0.273 Loss=1.683 Prec@1=60.077 Prec@5=82.719 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=23:55 IST
=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.469 DataTime=0.273 Loss=1.683 Prec@1=60.077 Prec@5=82.719 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=23:55 IST
=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.469 DataTime=0.273 Loss=1.683 Prec@1=60.077 Prec@5=82.719 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=23:56 IST
=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.062 Prec@5=82.713 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=23:56 IST
=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.062 Prec@5=82.713 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=23:56 IST
=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.062 Prec@5=82.713 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=23:56 IST
=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.075 Prec@5=82.709 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=23:56 IST
=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.075 Prec@5=82.709 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=23:56 IST
=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.683 Prec@1=60.075 Prec@5=82.709 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=23:57 IST
=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.685 Prec@1=60.059 Prec@5=82.672 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=23:57 IST
=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.685 Prec@1=60.059 Prec@5=82.672 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=23:57 IST
=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.685 Prec@1=60.059 Prec@5=82.672 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=23:58 IST
=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.686 Prec@1=60.035 Prec@5=82.646 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=23:58 IST
=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.686 Prec@1=60.035 Prec@5=82.646 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=23:58 IST
=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.273 Loss=1.686 Prec@1=60.035 Prec@5=82.646 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=23:59 IST
=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.687 Prec@1=60.019 Prec@5=82.631 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=23:59 IST
=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.687 Prec@1=60.019 Prec@5=82.631 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=23:59 IST
=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.687 Prec@1=60.019 Prec@5=82.631 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=23:59 IST
=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.272 Loss=1.688 Prec@1=59.999 Prec@5=82.620 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=23:59 IST
=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.272 Loss=1.688 Prec@1=59.999 Prec@5=82.620 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=23:59 IST
=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.468 DataTime=0.272 Loss=1.688 Prec@1=59.999 Prec@5=82.620 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=00:00 IST
=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.688 Prec@1=59.989 Prec@5=82.615 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=00:00 IST
=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.688 Prec@1=59.989 Prec@5=82.615 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=00:00 IST
=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.272 Loss=1.688 Prec@1=59.989 Prec@5=82.615 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=00:01 IST
=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.689 Prec@1=59.985 Prec@5=82.609 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=00:01 IST
=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.689 Prec@1=59.985 Prec@5=82.609 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=00:01 IST
=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.689 Prec@1=59.985 Prec@5=82.609 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=00:02 IST
=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.947 Prec@5=82.585 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=00:02 IST
=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.947 Prec@5=82.585 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=00:02 IST
=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.947 Prec@5=82.585 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=00:03 IST
=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.946 Prec@5=82.587 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=00:03 IST
=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.946 Prec@5=82.587 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=00:03 IST
=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.690 Prec@1=59.946 Prec@5=82.587 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=00:03 IST
=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.931 Prec@5=82.580 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=00:03 IST
=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.931 Prec@5=82.580 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=00:03 IST
=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.931 Prec@5=82.580 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=00:04 IST
=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.920 Prec@5=82.567 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=00:04 IST
=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.920 Prec@5=82.567 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=00:04 IST
=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.691 Prec@1=59.920 Prec@5=82.567 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=00:05 IST
=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.921 Prec@5=82.561 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=00:05 IST
=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.921 Prec@5=82.561 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=00:05 IST
=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.921 Prec@5=82.561 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=00:06 IST
=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.911 Prec@5=82.558 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=00:06 IST
=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.911 Prec@5=82.558 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=00:06 IST
=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.911 Prec@5=82.558 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=00:06 IST
=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.910 Prec@5=82.556 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=00:06 IST
=> training   100.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.467 DataTime=0.271 Loss=1.692 Prec@1=59.910 Prec@5=82.556 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=00:06 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST
=> validation 0.00% of 1x98...Epoch=25/150 LR=0.09382 Time=6.828 Loss=0.994 Prec@1=75.195 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST
=> validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=6.828 Loss=0.994 Prec@1=75.195 Prec@5=92.578 rate=7938.02 Hz, eta=0:00:00, total=0:00:00, wall=00:06 IST
** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=6.828 Loss=0.994 Prec@1=75.195 Prec@5=92.578 rate=7938.02 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST
** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=0.557 Loss=1.660 Prec@1=60.082 Prec@5=83.320 rate=7938.02 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST
** validation 100.00% of 1x98...Epoch=25/150 LR=0.09382 Time=0.557 Loss=1.660 Prec@1=60.082 Prec@5=83.320 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=00:07 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST
=> training   0.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.739 DataTime=4.491 Loss=1.734 Prec@1=58.594 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST
=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.739 DataTime=4.491 Loss=1.734 Prec@1=58.594 Prec@5=81.250 rate=5233.44 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST
=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.739 DataTime=4.491 Loss=1.734 Prec@1=58.594 Prec@5=81.250 rate=5233.44 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST
=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.509 DataTime=0.314 Loss=1.644 Prec@1=60.821 Prec@5=83.414 rate=5233.44 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST
=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.509 DataTime=0.314 Loss=1.644 Prec@1=60.821 Prec@5=83.414 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=00:07 IST
=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.509 DataTime=0.314 Loss=1.644 Prec@1=60.821 Prec@5=83.414 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=00:08 IST
=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.490 DataTime=0.297 Loss=1.652 Prec@1=60.630 Prec@5=83.201 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=00:08 IST
=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.490 DataTime=0.297 Loss=1.652 Prec@1=60.630 Prec@5=83.201 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=00:08 IST
=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.490 DataTime=0.297 Loss=1.652 Prec@1=60.630 Prec@5=83.201 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=00:09 IST
=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.483 DataTime=0.288 Loss=1.652 Prec@1=60.677 Prec@5=83.185 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=00:09 IST
=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.483 DataTime=0.288 Loss=1.652 Prec@1=60.677 Prec@5=83.185 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=00:09 IST
=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.483 DataTime=0.288 Loss=1.652 Prec@1=60.677 Prec@5=83.185 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=00:10 IST
=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.477 DataTime=0.284 Loss=1.650 Prec@1=60.636 Prec@5=83.182 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=00:10 IST
=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.477 DataTime=0.284 Loss=1.650 Prec@1=60.636 Prec@5=83.182 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=00:10 IST
=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.477 DataTime=0.284 Loss=1.650 Prec@1=60.636 Prec@5=83.182 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=00:11 IST
=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.476 DataTime=0.281 Loss=1.656 Prec@1=60.552 Prec@5=83.083 rate=2.15 Hz, eta=0:16:18, total=0:03:06, wall=00:11 IST
=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.476 DataTime=0.281 Loss=1.656 Prec@1=60.552 Prec@5=83.083 rate=2.14 Hz, eta=0:15:34, total=0:03:53, wall=00:11 IST
=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.476 DataTime=0.281 Loss=1.656 Prec@1=60.552 Prec@5=83.083 rate=2.14 Hz, eta=0:15:34, total=0:03:53, wall=00:11 IST
=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.475 DataTime=0.280 Loss=1.658 Prec@1=60.516 Prec@5=83.067 rate=2.14 Hz, eta=0:15:34, total=0:03:53, wall=00:11 IST
=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.475 DataTime=0.280 Loss=1.658 Prec@1=60.516 Prec@5=83.067 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=00:11 IST
=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.475 DataTime=0.280 Loss=1.658 Prec@1=60.516 Prec@5=83.067 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=00:12 IST
=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.474 DataTime=0.279 Loss=1.660 Prec@1=60.469 Prec@5=83.012 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=00:12 IST
=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.474 DataTime=0.279 Loss=1.660 Prec@1=60.469 Prec@5=83.012 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=00:12 IST
=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.474 DataTime=0.279 Loss=1.660 Prec@1=60.469 Prec@5=83.012 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=00:13 IST
=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.662 Prec@1=60.435 Prec@5=82.971 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=00:13 IST
=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.662 Prec@1=60.435 Prec@5=82.971 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=00:13 IST
=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.662 Prec@1=60.435 Prec@5=82.971 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=00:14 IST
=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.665 Prec@1=60.365 Prec@5=82.926 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=00:14 IST
=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.665 Prec@1=60.365 Prec@5=82.926 rate=2.14 Hz, eta=0:12:26, total=0:07:00, wall=00:14 IST
=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.665 Prec@1=60.365 Prec@5=82.926 rate=2.14 Hz, eta=0:12:26, total=0:07:00, wall=00:14 IST
=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.667 Prec@1=60.365 Prec@5=82.885 rate=2.14 Hz, eta=0:12:26, total=0:07:00, wall=00:14 IST
=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.667 Prec@1=60.365 Prec@5=82.885 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=00:14 IST
=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.667 Prec@1=60.365 Prec@5=82.885 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=00:15 IST
=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.669 Prec@1=60.344 Prec@5=82.859 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=00:15 IST
=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.669 Prec@1=60.344 Prec@5=82.859 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=00:15 IST
=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.669 Prec@1=60.344 Prec@5=82.859 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=00:16 IST
=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.670 Prec@1=60.339 Prec@5=82.855 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=00:16 IST
=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.670 Prec@1=60.339 Prec@5=82.855 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=00:16 IST
=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.670 Prec@1=60.339 Prec@5=82.855 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=00:17 IST
=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.473 DataTime=0.278 Loss=1.670 Prec@1=60.321 Prec@5=82.845 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=00:17 IST
=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.473 DataTime=0.278 Loss=1.670 Prec@1=60.321 Prec@5=82.845 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=00:17 IST
=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.473 DataTime=0.278 Loss=1.670 Prec@1=60.321 Prec@5=82.845 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=00:18 IST
=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.672 Prec@1=60.276 Prec@5=82.825 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=00:18 IST
=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.672 Prec@1=60.276 Prec@5=82.825 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=00:18 IST
=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.672 Prec@1=60.276 Prec@5=82.825 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=00:18 IST
=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.674 Prec@1=60.240 Prec@5=82.805 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=00:18 IST
=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.674 Prec@1=60.240 Prec@5=82.805 rate=2.13 Hz, eta=0:07:49, total=0:11:44, wall=00:18 IST
=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.278 Loss=1.674 Prec@1=60.240 Prec@5=82.805 rate=2.13 Hz, eta=0:07:49, total=0:11:44, wall=00:19 IST
=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.674 Prec@1=60.212 Prec@5=82.787 rate=2.13 Hz, eta=0:07:49, total=0:11:44, wall=00:19 IST
=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.674 Prec@1=60.212 Prec@5=82.787 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=00:19 IST
=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.472 DataTime=0.277 Loss=1.674 Prec@1=60.212 Prec@5=82.787 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=00:20 IST
=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.675 Prec@1=60.211 Prec@5=82.785 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=00:20 IST
=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.675 Prec@1=60.211 Prec@5=82.785 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=00:20 IST
=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.675 Prec@1=60.211 Prec@5=82.785 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=00:21 IST
=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.676 Prec@1=60.183 Prec@5=82.776 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=00:21 IST
=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.676 Prec@1=60.183 Prec@5=82.776 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=00:21 IST
=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.676 Prec@1=60.183 Prec@5=82.776 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=00:22 IST
=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.758 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=00:22 IST
=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.758 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=00:22 IST
=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.471 DataTime=0.276 Loss=1.677 Prec@1=60.166 Prec@5=82.758 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=00:22 IST
=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.678 Prec@1=60.143 Prec@5=82.742 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=00:22 IST
=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.678 Prec@1=60.143 Prec@5=82.742 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=00:22 IST
=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.678 Prec@1=60.143 Prec@5=82.742 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=00:23 IST
=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.115 Prec@5=82.719 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=00:23 IST
=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.115 Prec@5=82.719 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=00:23 IST
=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.115 Prec@5=82.719 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=00:24 IST
=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.125 Prec@5=82.709 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=00:24 IST
=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.125 Prec@5=82.709 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=00:24 IST
=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.680 Prec@1=60.125 Prec@5=82.709 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=00:25 IST
=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.110 Prec@5=82.701 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=00:25 IST
=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.110 Prec@5=82.701 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=00:25 IST
=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.110 Prec@5=82.701 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=00:25 IST
=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.109 Prec@5=82.695 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=00:25 IST
=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.109 Prec@5=82.695 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=00:25 IST
=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.109 Prec@5=82.695 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=00:26 IST
=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.104 Prec@5=82.688 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=00:26 IST
=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.104 Prec@5=82.688 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=00:26 IST
=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.104 Prec@5=82.688 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=00:26 IST
=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.103 Prec@5=82.687 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=00:26 IST
=> training   100.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.470 DataTime=0.276 Loss=1.681 Prec@1=60.103 Prec@5=82.687 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=00:26 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:26 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:26 IST
=> validation 0.00% of 1x98...Epoch=26/150 LR=0.09330 Time=7.132 Loss=1.055 Prec@1=73.828 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=00:26 IST
=> validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=7.132 Loss=1.055 Prec@1=73.828 Prec@5=91.992 rate=7873.52 Hz, eta=0:00:00, total=0:00:00, wall=00:26 IST
** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=7.132 Loss=1.055 Prec@1=73.828 Prec@5=91.992 rate=7873.52 Hz, eta=0:00:00, total=0:00:00, wall=00:27 IST
** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=0.556 Loss=1.641 Prec@1=60.676 Prec@5=83.672 rate=7873.52 Hz, eta=0:00:00, total=0:00:00, wall=00:27 IST
** validation 100.00% of 1x98...Epoch=26/150 LR=0.09330 Time=0.556 Loss=1.641 Prec@1=60.676 Prec@5=83.672 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=00:27 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:27 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:27 IST
=> training   0.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=4.840 DataTime=4.583 Loss=1.650 Prec@1=59.766 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=00:27 IST
=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=4.840 DataTime=4.583 Loss=1.650 Prec@1=59.766 Prec@5=83.398 rate=8111.62 Hz, eta=0:00:00, total=0:00:00, wall=00:27 IST
=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=4.840 DataTime=4.583 Loss=1.650 Prec@1=59.766 Prec@5=83.398 rate=8111.62 Hz, eta=0:00:00, total=0:00:00, wall=00:28 IST
=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.509 DataTime=0.311 Loss=1.637 Prec@1=61.005 Prec@5=83.304 rate=8111.62 Hz, eta=0:00:00, total=0:00:00, wall=00:28 IST
=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.509 DataTime=0.311 Loss=1.637 Prec@1=61.005 Prec@5=83.304 rate=2.17 Hz, eta=0:18:26, total=0:00:46, wall=00:28 IST
=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.509 DataTime=0.311 Loss=1.637 Prec@1=61.005 Prec@5=83.304 rate=2.17 Hz, eta=0:18:26, total=0:00:46, wall=00:29 IST
=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.484 DataTime=0.288 Loss=1.641 Prec@1=61.002 Prec@5=83.285 rate=2.17 Hz, eta=0:18:26, total=0:00:46, wall=00:29 IST
=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.484 DataTime=0.288 Loss=1.641 Prec@1=61.002 Prec@5=83.285 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=00:29 IST
=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.484 DataTime=0.288 Loss=1.641 Prec@1=61.002 Prec@5=83.285 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=00:30 IST
=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.476 DataTime=0.281 Loss=1.642 Prec@1=60.886 Prec@5=83.257 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=00:30 IST
=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.476 DataTime=0.281 Loss=1.642 Prec@1=60.886 Prec@5=83.257 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=00:30 IST
=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.476 DataTime=0.281 Loss=1.642 Prec@1=60.886 Prec@5=83.257 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=00:30 IST
=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.852 Prec@5=83.335 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=00:30 IST
=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.852 Prec@5=83.335 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=00:30 IST
=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.852 Prec@5=83.335 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=00:31 IST
=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.278 Loss=1.646 Prec@1=60.747 Prec@5=83.289 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=00:31 IST
=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.278 Loss=1.646 Prec@1=60.747 Prec@5=83.289 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=00:31 IST
=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.474 DataTime=0.278 Loss=1.646 Prec@1=60.747 Prec@5=83.289 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=00:32 IST
=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.714 Prec@5=83.266 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=00:32 IST
=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.714 Prec@5=83.266 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=00:32 IST
=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.714 Prec@5=83.266 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=00:33 IST
=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.470 DataTime=0.276 Loss=1.649 Prec@1=60.612 Prec@5=83.213 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=00:33 IST
=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.470 DataTime=0.276 Loss=1.649 Prec@1=60.612 Prec@5=83.213 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=00:33 IST
=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.470 DataTime=0.276 Loss=1.649 Prec@1=60.612 Prec@5=83.213 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=00:33 IST
=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.651 Prec@1=60.591 Prec@5=83.155 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=00:33 IST
=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.651 Prec@1=60.591 Prec@5=83.155 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=00:33 IST
=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.651 Prec@1=60.591 Prec@5=83.155 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=00:34 IST
=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.562 Prec@5=83.113 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=00:34 IST
=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.562 Prec@5=83.113 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=00:34 IST
=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.562 Prec@5=83.113 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=00:35 IST
=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.654 Prec@1=60.537 Prec@5=83.084 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=00:35 IST
=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.654 Prec@1=60.537 Prec@5=83.084 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=00:35 IST
=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.654 Prec@1=60.537 Prec@5=83.084 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=00:36 IST
=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.494 Prec@5=83.062 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=00:36 IST
=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.494 Prec@5=83.062 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=00:36 IST
=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.494 Prec@5=83.062 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=00:37 IST
=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.658 Prec@1=60.452 Prec@5=83.021 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=00:37 IST
=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.658 Prec@1=60.452 Prec@5=83.021 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=00:37 IST
=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.658 Prec@1=60.452 Prec@5=83.021 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=00:37 IST
=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.467 DataTime=0.273 Loss=1.659 Prec@1=60.434 Prec@5=83.012 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=00:37 IST
=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.467 DataTime=0.273 Loss=1.659 Prec@1=60.434 Prec@5=83.012 rate=2.16 Hz, eta=0:09:17, total=0:10:02, wall=00:37 IST
=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.467 DataTime=0.273 Loss=1.659 Prec@1=60.434 Prec@5=83.012 rate=2.16 Hz, eta=0:09:17, total=0:10:02, wall=00:38 IST
=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.659 Prec@1=60.438 Prec@5=83.018 rate=2.16 Hz, eta=0:09:17, total=0:10:02, wall=00:38 IST
=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.659 Prec@1=60.438 Prec@5=83.018 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=00:38 IST
=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.659 Prec@1=60.438 Prec@5=83.018 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=00:39 IST
=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.662 Prec@1=60.401 Prec@5=82.979 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=00:39 IST
=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.662 Prec@1=60.401 Prec@5=82.979 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=00:39 IST
=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.662 Prec@1=60.401 Prec@5=82.979 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=00:40 IST
=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.662 Prec@1=60.407 Prec@5=82.982 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=00:40 IST
=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.662 Prec@1=60.407 Prec@5=82.982 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=00:40 IST
=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.273 Loss=1.662 Prec@1=60.407 Prec@5=82.982 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=00:40 IST
=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.664 Prec@1=60.371 Prec@5=82.949 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=00:40 IST
=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.664 Prec@1=60.371 Prec@5=82.949 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=00:40 IST
=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.274 Loss=1.664 Prec@1=60.371 Prec@5=82.949 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=00:41 IST
=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.665 Prec@1=60.342 Prec@5=82.921 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=00:41 IST
=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.665 Prec@1=60.342 Prec@5=82.921 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=00:41 IST
=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.665 Prec@1=60.342 Prec@5=82.921 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=00:42 IST
=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.666 Prec@1=60.330 Prec@5=82.906 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=00:42 IST
=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.666 Prec@1=60.330 Prec@5=82.906 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=00:42 IST
=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.666 Prec@1=60.330 Prec@5=82.906 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=00:43 IST
=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.667 Prec@1=60.303 Prec@5=82.892 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=00:43 IST
=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.667 Prec@1=60.303 Prec@5=82.892 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=00:43 IST
=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.468 DataTime=0.274 Loss=1.667 Prec@1=60.303 Prec@5=82.892 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=00:44 IST
=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.668 Prec@1=60.280 Prec@5=82.878 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=00:44 IST
=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.668 Prec@1=60.280 Prec@5=82.878 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=00:44 IST
=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.668 Prec@1=60.280 Prec@5=82.878 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=00:44 IST
=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.669 Prec@1=60.247 Prec@5=82.858 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=00:44 IST
=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.669 Prec@1=60.247 Prec@5=82.858 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=00:44 IST
=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.669 Prec@1=60.247 Prec@5=82.858 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=00:45 IST
=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.670 Prec@1=60.233 Prec@5=82.848 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=00:45 IST
=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.670 Prec@1=60.233 Prec@5=82.848 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=00:45 IST
=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.670 Prec@1=60.233 Prec@5=82.848 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=00:46 IST
=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.671 Prec@1=60.245 Prec@5=82.841 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=00:46 IST
=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.671 Prec@1=60.245 Prec@5=82.841 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=00:46 IST
=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.671 Prec@1=60.245 Prec@5=82.841 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=00:47 IST
=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.672 Prec@1=60.226 Prec@5=82.821 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=00:47 IST
=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.672 Prec@1=60.226 Prec@5=82.821 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=00:47 IST
=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.672 Prec@1=60.226 Prec@5=82.821 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=00:47 IST
=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.672 Prec@1=60.226 Prec@5=82.821 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=00:47 IST
=> training   100.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.469 DataTime=0.275 Loss=1.672 Prec@1=60.226 Prec@5=82.821 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=00:47 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST
=> validation 0.00% of 1x98...Epoch=27/150 LR=0.09277 Time=6.674 Loss=1.097 Prec@1=71.680 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST
=> validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=6.674 Loss=1.097 Prec@1=71.680 Prec@5=91.992 rate=6166.71 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST
** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=6.674 Loss=1.097 Prec@1=71.680 Prec@5=91.992 rate=6166.71 Hz, eta=0:00:00, total=0:00:00, wall=00:48 IST
** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=0.547 Loss=1.674 Prec@1=60.056 Prec@5=83.232 rate=6166.71 Hz, eta=0:00:00, total=0:00:00, wall=00:48 IST
** validation 100.00% of 1x98...Epoch=27/150 LR=0.09277 Time=0.547 Loss=1.674 Prec@1=60.056 Prec@5=83.232 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=00:48 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:48 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:48 IST
=> training   0.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.054 DataTime=4.804 Loss=1.708 Prec@1=60.547 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=00:48 IST
=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.054 DataTime=4.804 Loss=1.708 Prec@1=60.547 Prec@5=82.227 rate=4761.90 Hz, eta=0:00:00, total=0:00:00, wall=00:48 IST
=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.054 DataTime=4.804 Loss=1.708 Prec@1=60.547 Prec@5=82.227 rate=4761.90 Hz, eta=0:00:00, total=0:00:00, wall=00:49 IST
=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.502 DataTime=0.307 Loss=1.634 Prec@1=61.092 Prec@5=83.491 rate=4761.90 Hz, eta=0:00:00, total=0:00:00, wall=00:49 IST
=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.502 DataTime=0.307 Loss=1.634 Prec@1=61.092 Prec@5=83.491 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=00:49 IST
=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.502 DataTime=0.307 Loss=1.634 Prec@1=61.092 Prec@5=83.491 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=00:49 IST
=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.481 DataTime=0.284 Loss=1.633 Prec@1=61.230 Prec@5=83.373 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=00:49 IST
=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.481 DataTime=0.284 Loss=1.633 Prec@1=61.230 Prec@5=83.373 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=00:49 IST
=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.481 DataTime=0.284 Loss=1.633 Prec@1=61.230 Prec@5=83.373 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=00:50 IST
=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.473 DataTime=0.276 Loss=1.630 Prec@1=61.145 Prec@5=83.435 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=00:50 IST
=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.473 DataTime=0.276 Loss=1.630 Prec@1=61.145 Prec@5=83.435 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=00:50 IST
=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.473 DataTime=0.276 Loss=1.630 Prec@1=61.145 Prec@5=83.435 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=00:51 IST
=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.471 DataTime=0.274 Loss=1.633 Prec@1=61.106 Prec@5=83.417 rate=2.19 Hz, eta=0:16:43, total=0:02:17, wall=00:51 IST
=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.471 DataTime=0.274 Loss=1.633 Prec@1=61.106 Prec@5=83.417 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=00:51 IST
=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.471 DataTime=0.274 Loss=1.633 Prec@1=61.106 Prec@5=83.417 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=00:52 IST
=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.637 Prec@1=61.053 Prec@5=83.313 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=00:52 IST
=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.637 Prec@1=61.053 Prec@5=83.313 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=00:52 IST
=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.637 Prec@1=61.053 Prec@5=83.313 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=00:52 IST
=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.271 Loss=1.640 Prec@1=60.979 Prec@5=83.292 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=00:52 IST
=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.271 Loss=1.640 Prec@1=60.979 Prec@5=83.292 rate=2.18 Hz, eta=0:14:32, total=0:04:35, wall=00:52 IST
=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.271 Loss=1.640 Prec@1=60.979 Prec@5=83.292 rate=2.18 Hz, eta=0:14:32, total=0:04:35, wall=00:53 IST
=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.644 Prec@1=60.907 Prec@5=83.256 rate=2.18 Hz, eta=0:14:32, total=0:04:35, wall=00:53 IST
=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.644 Prec@1=60.907 Prec@5=83.256 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=00:53 IST
=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.272 Loss=1.644 Prec@1=60.907 Prec@5=83.256 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=00:54 IST
=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.272 Loss=1.644 Prec@1=60.890 Prec@5=83.243 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=00:54 IST
=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.272 Loss=1.644 Prec@1=60.890 Prec@5=83.243 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=00:54 IST
=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.467 DataTime=0.272 Loss=1.644 Prec@1=60.890 Prec@5=83.243 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=00:55 IST
=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.645 Prec@1=60.832 Prec@5=83.220 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=00:55 IST
=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.645 Prec@1=60.832 Prec@5=83.220 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=00:55 IST
=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.645 Prec@1=60.832 Prec@5=83.220 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=00:56 IST
=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.648 Prec@1=60.798 Prec@5=83.190 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=00:56 IST
=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.648 Prec@1=60.798 Prec@5=83.190 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=00:56 IST
=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.648 Prec@1=60.798 Prec@5=83.190 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=00:56 IST
=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.652 Prec@1=60.707 Prec@5=83.150 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=00:56 IST
=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.652 Prec@1=60.707 Prec@5=83.150 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=00:56 IST
=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.652 Prec@1=60.707 Prec@5=83.150 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=00:57 IST
=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.682 Prec@5=83.118 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=00:57 IST
=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.682 Prec@5=83.118 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=00:57 IST
=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.653 Prec@1=60.682 Prec@5=83.118 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=00:58 IST
=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.654 Prec@1=60.669 Prec@5=83.113 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=00:58 IST
=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.654 Prec@1=60.669 Prec@5=83.113 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=00:58 IST
=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.273 Loss=1.654 Prec@1=60.669 Prec@5=83.113 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=00:59 IST
=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.654 Prec@1=60.672 Prec@5=83.098 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=00:59 IST
=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.654 Prec@1=60.672 Prec@5=83.098 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=00:59 IST
=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.654 Prec@1=60.672 Prec@5=83.098 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=00:59 IST
=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.655 Prec@1=60.653 Prec@5=83.092 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=00:59 IST
=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.655 Prec@1=60.653 Prec@5=83.092 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=00:59 IST
=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.655 Prec@1=60.653 Prec@5=83.092 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=01:00 IST
=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.639 Prec@5=83.066 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=01:00 IST
=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.639 Prec@5=83.066 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=01:00 IST
=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.468 DataTime=0.273 Loss=1.656 Prec@1=60.639 Prec@5=83.066 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=01:01 IST
=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.656 Prec@1=60.631 Prec@5=83.071 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=01:01 IST
=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.656 Prec@1=60.631 Prec@5=83.071 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=01:01 IST
=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.656 Prec@1=60.631 Prec@5=83.071 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=01:02 IST
=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.657 Prec@1=60.624 Prec@5=83.065 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=01:02 IST
=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.657 Prec@1=60.624 Prec@5=83.065 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=01:02 IST
=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.657 Prec@1=60.624 Prec@5=83.065 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=01:03 IST
=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.657 Prec@1=60.626 Prec@5=83.065 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=01:03 IST
=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.657 Prec@1=60.626 Prec@5=83.065 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=01:03 IST
=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.657 Prec@1=60.626 Prec@5=83.065 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=01:03 IST
=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.584 Prec@5=83.038 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=01:03 IST
=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.584 Prec@5=83.038 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=01:03 IST
=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.584 Prec@5=83.038 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=01:04 IST
=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.659 Prec@1=60.581 Prec@5=83.030 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=01:04 IST
=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.659 Prec@1=60.581 Prec@5=83.030 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=01:04 IST
=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.659 Prec@1=60.581 Prec@5=83.030 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=01:05 IST
=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.579 Prec@5=83.022 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=01:05 IST
=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.579 Prec@5=83.022 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=01:05 IST
=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.659 Prec@1=60.579 Prec@5=83.022 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=01:06 IST
=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.557 Prec@5=82.999 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=01:06 IST
=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.557 Prec@5=82.999 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=01:06 IST
=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.557 Prec@5=82.999 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=01:07 IST
=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.553 Prec@5=83.003 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=01:07 IST
=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.553 Prec@5=83.003 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=01:07 IST
=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.470 DataTime=0.275 Loss=1.660 Prec@1=60.553 Prec@5=83.003 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=01:07 IST
=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.661 Prec@1=60.541 Prec@5=82.997 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=01:07 IST
=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.661 Prec@1=60.541 Prec@5=82.997 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=01:07 IST
=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.275 Loss=1.661 Prec@1=60.541 Prec@5=82.997 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=01:07 IST
=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.661 Prec@1=60.541 Prec@5=82.996 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=01:07 IST
=> training   100.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.469 DataTime=0.274 Loss=1.661 Prec@1=60.541 Prec@5=82.996 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=01:07 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> validation 0.00% of 1x98...Epoch=28/150 LR=0.09222 Time=7.122 Loss=1.049 Prec@1=74.219 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=7.122 Loss=1.049 Prec@1=74.219 Prec@5=91.211 rate=6260.49 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST
** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=7.122 Loss=1.049 Prec@1=74.219 Prec@5=91.211 rate=6260.49 Hz, eta=0:00:00, total=0:00:00, wall=01:08 IST
** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=0.557 Loss=1.665 Prec@1=60.094 Prec@5=83.426 rate=6260.49 Hz, eta=0:00:00, total=0:00:00, wall=01:08 IST
** validation 100.00% of 1x98...Epoch=28/150 LR=0.09222 Time=0.557 Loss=1.665 Prec@1=60.094 Prec@5=83.426 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=01:08 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:08 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:08 IST
=> training   0.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.087 DataTime=4.849 Loss=1.513 Prec@1=63.086 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=01:08 IST
=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.087 DataTime=4.849 Loss=1.513 Prec@1=63.086 Prec@5=84.961 rate=9322.88 Hz, eta=0:00:00, total=0:00:00, wall=01:08 IST
=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.087 DataTime=4.849 Loss=1.513 Prec@1=63.086 Prec@5=84.961 rate=9322.88 Hz, eta=0:00:00, total=0:00:00, wall=01:09 IST
=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.506 DataTime=0.312 Loss=1.632 Prec@1=60.872 Prec@5=83.524 rate=9322.88 Hz, eta=0:00:00, total=0:00:00, wall=01:09 IST
=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.506 DataTime=0.312 Loss=1.632 Prec@1=60.872 Prec@5=83.524 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=01:09 IST
=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.506 DataTime=0.312 Loss=1.632 Prec@1=60.872 Prec@5=83.524 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=01:10 IST
=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.490 DataTime=0.295 Loss=1.631 Prec@1=61.019 Prec@5=83.523 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=01:10 IST
=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.490 DataTime=0.295 Loss=1.631 Prec@1=61.019 Prec@5=83.523 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=01:10 IST
=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.490 DataTime=0.295 Loss=1.631 Prec@1=61.019 Prec@5=83.523 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=01:11 IST
=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.482 DataTime=0.288 Loss=1.622 Prec@1=61.260 Prec@5=83.639 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=01:11 IST
=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.482 DataTime=0.288 Loss=1.622 Prec@1=61.260 Prec@5=83.639 rate=2.15 Hz, eta=0:17:04, total=0:02:20, wall=01:11 IST
=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.482 DataTime=0.288 Loss=1.622 Prec@1=61.260 Prec@5=83.639 rate=2.15 Hz, eta=0:17:04, total=0:02:20, wall=01:11 IST
=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.476 DataTime=0.283 Loss=1.624 Prec@1=61.155 Prec@5=83.585 rate=2.15 Hz, eta=0:17:04, total=0:02:20, wall=01:11 IST
=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.476 DataTime=0.283 Loss=1.624 Prec@1=61.155 Prec@5=83.585 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=01:11 IST
=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.476 DataTime=0.283 Loss=1.624 Prec@1=61.155 Prec@5=83.585 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=01:12 IST
=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.624 Prec@1=61.156 Prec@5=83.622 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=01:12 IST
=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.624 Prec@1=61.156 Prec@5=83.622 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=01:12 IST
=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.624 Prec@1=61.156 Prec@5=83.622 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=01:13 IST
=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.626 Prec@1=61.108 Prec@5=83.586 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=01:13 IST
=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.626 Prec@1=61.108 Prec@5=83.586 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=01:13 IST
=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.626 Prec@1=61.108 Prec@5=83.586 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=01:14 IST
=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.628 Prec@1=61.100 Prec@5=83.562 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=01:14 IST
=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.628 Prec@1=61.100 Prec@5=83.562 rate=2.14 Hz, eta=0:14:03, total=0:05:27, wall=01:14 IST
=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.628 Prec@1=61.100 Prec@5=83.562 rate=2.14 Hz, eta=0:14:03, total=0:05:27, wall=01:15 IST
=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.282 Loss=1.630 Prec@1=61.071 Prec@5=83.527 rate=2.14 Hz, eta=0:14:03, total=0:05:27, wall=01:15 IST
=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.282 Loss=1.630 Prec@1=61.071 Prec@5=83.527 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=01:15 IST
=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.282 Loss=1.630 Prec@1=61.071 Prec@5=83.527 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=01:15 IST
=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.631 Prec@1=61.021 Prec@5=83.487 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=01:15 IST
=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.631 Prec@1=61.021 Prec@5=83.487 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=01:15 IST
=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.631 Prec@1=61.021 Prec@5=83.487 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=01:16 IST
=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.632 Prec@1=61.034 Prec@5=83.484 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=01:16 IST
=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.632 Prec@1=61.034 Prec@5=83.484 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=01:16 IST
=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.475 DataTime=0.281 Loss=1.632 Prec@1=61.034 Prec@5=83.484 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=01:17 IST
=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.635 Prec@1=60.989 Prec@5=83.425 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=01:17 IST
=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.635 Prec@1=60.989 Prec@5=83.425 rate=2.13 Hz, eta=0:10:58, total=0:08:37, wall=01:17 IST
=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.280 Loss=1.635 Prec@1=60.989 Prec@5=83.425 rate=2.13 Hz, eta=0:10:58, total=0:08:37, wall=01:18 IST
=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.638 Prec@1=60.941 Prec@5=83.398 rate=2.13 Hz, eta=0:10:58, total=0:08:37, wall=01:18 IST
=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.638 Prec@1=60.941 Prec@5=83.398 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=01:18 IST
=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.638 Prec@1=60.941 Prec@5=83.398 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=01:18 IST
=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.639 Prec@1=60.933 Prec@5=83.372 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=01:18 IST
=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.639 Prec@1=60.933 Prec@5=83.372 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=01:18 IST
=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.279 Loss=1.639 Prec@1=60.933 Prec@5=83.372 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=01:19 IST
=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.893 Prec@5=83.342 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=01:19 IST
=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.893 Prec@5=83.342 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=01:19 IST
=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.474 DataTime=0.279 Loss=1.641 Prec@1=60.893 Prec@5=83.342 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=01:20 IST
=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.278 Loss=1.641 Prec@1=60.901 Prec@5=83.315 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=01:20 IST
=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.278 Loss=1.641 Prec@1=60.901 Prec@5=83.315 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=01:20 IST
=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.473 DataTime=0.278 Loss=1.641 Prec@1=60.901 Prec@5=83.315 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=01:21 IST
=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.278 Loss=1.642 Prec@1=60.871 Prec@5=83.300 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=01:21 IST
=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.278 Loss=1.642 Prec@1=60.871 Prec@5=83.300 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=01:21 IST
=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.278 Loss=1.642 Prec@1=60.871 Prec@5=83.300 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=01:22 IST
=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.642 Prec@1=60.865 Prec@5=83.294 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=01:22 IST
=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.642 Prec@1=60.865 Prec@5=83.294 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=01:22 IST
=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.642 Prec@1=60.865 Prec@5=83.294 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=01:22 IST
=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.644 Prec@1=60.820 Prec@5=83.264 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=01:22 IST
=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.644 Prec@1=60.820 Prec@5=83.264 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=01:22 IST
=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.644 Prec@1=60.820 Prec@5=83.264 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=01:23 IST
=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.801 Prec@5=83.244 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=01:23 IST
=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.801 Prec@5=83.244 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=01:23 IST
=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.472 DataTime=0.277 Loss=1.645 Prec@1=60.801 Prec@5=83.244 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=01:24 IST
=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.773 Prec@5=83.225 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=01:24 IST
=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.773 Prec@5=83.225 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=01:24 IST
=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.773 Prec@5=83.225 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=01:25 IST
=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.777 Prec@5=83.223 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=01:25 IST
=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.777 Prec@5=83.223 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=01:25 IST
=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.777 Prec@5=83.223 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=01:25 IST
=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.768 Prec@5=83.209 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=01:25 IST
=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.768 Prec@5=83.209 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=01:25 IST
=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.647 Prec@1=60.768 Prec@5=83.209 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=01:26 IST
=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.648 Prec@1=60.747 Prec@5=83.207 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=01:26 IST
=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.648 Prec@1=60.747 Prec@5=83.207 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=01:26 IST
=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.648 Prec@1=60.747 Prec@5=83.207 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=01:27 IST
=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.649 Prec@1=60.741 Prec@5=83.180 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=01:27 IST
=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.649 Prec@1=60.741 Prec@5=83.180 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=01:27 IST
=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.277 Loss=1.649 Prec@1=60.741 Prec@5=83.180 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=01:28 IST
=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.276 Loss=1.650 Prec@1=60.733 Prec@5=83.170 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=01:28 IST
=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.276 Loss=1.650 Prec@1=60.733 Prec@5=83.170 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=01:28 IST
=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.471 DataTime=0.276 Loss=1.650 Prec@1=60.733 Prec@5=83.170 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=01:28 IST
=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.470 DataTime=0.276 Loss=1.650 Prec@1=60.733 Prec@5=83.171 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=01:28 IST
=> training   100.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.470 DataTime=0.276 Loss=1.650 Prec@1=60.733 Prec@5=83.171 rate=2.14 Hz, eta=0:00:00, total=0:19:32, wall=01:28 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> validation 0.00% of 1x98...Epoch=29/150 LR=0.09165 Time=6.732 Loss=1.083 Prec@1=73.828 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=6.732 Loss=1.083 Prec@1=73.828 Prec@5=90.430 rate=5266.71 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST
** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=6.732 Loss=1.083 Prec@1=73.828 Prec@5=90.430 rate=5266.71 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=0.557 Loss=1.660 Prec@1=60.340 Prec@5=83.444 rate=5266.71 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
** validation 100.00% of 1x98...Epoch=29/150 LR=0.09165 Time=0.557 Loss=1.660 Prec@1=60.340 Prec@5=83.444 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=01:29 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> training   0.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.104 DataTime=4.914 Loss=1.592 Prec@1=62.891 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.104 DataTime=4.914 Loss=1.592 Prec@1=62.891 Prec@5=84.180 rate=7287.14 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.104 DataTime=4.914 Loss=1.592 Prec@1=62.891 Prec@5=84.180 rate=7287.14 Hz, eta=0:00:00, total=0:00:00, wall=01:30 IST
=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.495 DataTime=0.299 Loss=1.607 Prec@1=61.786 Prec@5=83.760 rate=7287.14 Hz, eta=0:00:00, total=0:00:00, wall=01:30 IST
=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.495 DataTime=0.299 Loss=1.607 Prec@1=61.786 Prec@5=83.760 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=01:30 IST
=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.495 DataTime=0.299 Loss=1.607 Prec@1=61.786 Prec@5=83.760 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=01:30 IST
=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.477 DataTime=0.283 Loss=1.605 Prec@1=61.694 Prec@5=83.826 rate=2.25 Hz, eta=0:17:47, total=0:00:44, wall=01:30 IST
=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.477 DataTime=0.283 Loss=1.605 Prec@1=61.694 Prec@5=83.826 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=01:30 IST
=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.477 DataTime=0.283 Loss=1.605 Prec@1=61.694 Prec@5=83.826 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=01:31 IST
=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.282 Loss=1.613 Prec@1=61.429 Prec@5=83.740 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=01:31 IST
=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.282 Loss=1.613 Prec@1=61.429 Prec@5=83.740 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=01:31 IST
=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.282 Loss=1.613 Prec@1=61.429 Prec@5=83.740 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=01:32 IST
=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.279 Loss=1.619 Prec@1=61.326 Prec@5=83.672 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=01:32 IST
=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.279 Loss=1.619 Prec@1=61.326 Prec@5=83.672 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=01:32 IST
=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.279 Loss=1.619 Prec@1=61.326 Prec@5=83.672 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=01:33 IST
=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.621 Prec@1=61.237 Prec@5=83.635 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=01:33 IST
=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.621 Prec@1=61.237 Prec@5=83.635 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=01:33 IST
=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.621 Prec@1=61.237 Prec@5=83.635 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=01:33 IST
=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.622 Prec@1=61.246 Prec@5=83.610 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=01:33 IST
=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.622 Prec@1=61.246 Prec@5=83.610 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:33 IST
=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.622 Prec@1=61.246 Prec@5=83.610 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:34 IST
=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.214 Prec@5=83.555 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:34 IST
=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.214 Prec@5=83.555 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=01:34 IST
=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.214 Prec@5=83.555 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=01:35 IST
=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.626 Prec@1=61.194 Prec@5=83.534 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=01:35 IST
=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.626 Prec@1=61.194 Prec@5=83.534 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=01:35 IST
=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.626 Prec@1=61.194 Prec@5=83.534 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=01:36 IST
=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.628 Prec@1=61.171 Prec@5=83.515 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=01:36 IST
=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.628 Prec@1=61.171 Prec@5=83.515 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=01:36 IST
=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.628 Prec@1=61.171 Prec@5=83.515 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=01:37 IST
=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.630 Prec@1=61.108 Prec@5=83.481 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=01:37 IST
=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.630 Prec@1=61.108 Prec@5=83.481 rate=2.13 Hz, eta=0:11:45, total=0:07:49, wall=01:37 IST
=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.630 Prec@1=61.108 Prec@5=83.481 rate=2.13 Hz, eta=0:11:45, total=0:07:49, wall=01:38 IST
=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.630 Prec@1=61.116 Prec@5=83.480 rate=2.13 Hz, eta=0:11:45, total=0:07:49, wall=01:38 IST
=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.630 Prec@1=61.116 Prec@5=83.480 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=01:38 IST
=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.630 Prec@1=61.116 Prec@5=83.480 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=01:38 IST
=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.631 Prec@1=61.097 Prec@5=83.471 rate=2.12 Hz, eta=0:11:00, total=0:08:38, wall=01:38 IST
=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.631 Prec@1=61.097 Prec@5=83.471 rate=2.12 Hz, eta=0:10:14, total=0:09:26, wall=01:38 IST
=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.631 Prec@1=61.097 Prec@5=83.471 rate=2.12 Hz, eta=0:10:14, total=0:09:26, wall=01:39 IST
=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.633 Prec@1=61.080 Prec@5=83.439 rate=2.12 Hz, eta=0:10:14, total=0:09:26, wall=01:39 IST
=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.633 Prec@1=61.080 Prec@5=83.439 rate=2.12 Hz, eta=0:09:27, total=0:10:14, wall=01:39 IST
=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.633 Prec@1=61.080 Prec@5=83.439 rate=2.12 Hz, eta=0:09:27, total=0:10:14, wall=01:40 IST
=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.635 Prec@1=61.030 Prec@5=83.403 rate=2.12 Hz, eta=0:09:27, total=0:10:14, wall=01:40 IST
=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.635 Prec@1=61.030 Prec@5=83.403 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=01:40 IST
=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.476 DataTime=0.280 Loss=1.635 Prec@1=61.030 Prec@5=83.403 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=01:41 IST
=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.379 rate=2.12 Hz, eta=0:08:40, total=0:11:01, wall=01:41 IST
=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.379 rate=2.12 Hz, eta=0:07:52, total=0:11:47, wall=01:41 IST
=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.379 rate=2.12 Hz, eta=0:07:52, total=0:11:47, wall=01:41 IST
=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.973 Prec@5=83.375 rate=2.12 Hz, eta=0:07:52, total=0:11:47, wall=01:41 IST
=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.973 Prec@5=83.375 rate=2.12 Hz, eta=0:07:05, total=0:12:35, wall=01:41 IST
=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.973 Prec@5=83.375 rate=2.12 Hz, eta=0:07:05, total=0:12:35, wall=01:42 IST
=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.390 rate=2.12 Hz, eta=0:07:05, total=0:12:35, wall=01:42 IST
=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.390 rate=2.12 Hz, eta=0:06:18, total=0:13:22, wall=01:42 IST
=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.475 DataTime=0.279 Loss=1.637 Prec@1=60.991 Prec@5=83.390 rate=2.12 Hz, eta=0:06:18, total=0:13:22, wall=01:43 IST
=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.637 Prec@1=60.985 Prec@5=83.378 rate=2.12 Hz, eta=0:06:18, total=0:13:22, wall=01:43 IST
=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.637 Prec@1=60.985 Prec@5=83.378 rate=2.12 Hz, eta=0:05:30, total=0:14:08, wall=01:43 IST
=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.637 Prec@1=60.985 Prec@5=83.378 rate=2.12 Hz, eta=0:05:30, total=0:14:08, wall=01:44 IST
=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.279 Loss=1.638 Prec@1=60.980 Prec@5=83.357 rate=2.12 Hz, eta=0:05:30, total=0:14:08, wall=01:44 IST
=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.279 Loss=1.638 Prec@1=60.980 Prec@5=83.357 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=01:44 IST
=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.279 Loss=1.638 Prec@1=60.980 Prec@5=83.357 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=01:45 IST
=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.639 Prec@1=60.958 Prec@5=83.334 rate=2.12 Hz, eta=0:04:43, total=0:14:56, wall=01:45 IST
=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.639 Prec@1=60.958 Prec@5=83.334 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=01:45 IST
=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.474 DataTime=0.278 Loss=1.639 Prec@1=60.958 Prec@5=83.334 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=01:45 IST
=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.278 Loss=1.640 Prec@1=60.927 Prec@5=83.332 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=01:45 IST
=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.278 Loss=1.640 Prec@1=60.927 Prec@5=83.332 rate=2.12 Hz, eta=0:03:09, total=0:16:29, wall=01:45 IST
=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.278 Loss=1.640 Prec@1=60.927 Prec@5=83.332 rate=2.12 Hz, eta=0:03:09, total=0:16:29, wall=01:46 IST
=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.640 Prec@1=60.903 Prec@5=83.321 rate=2.12 Hz, eta=0:03:09, total=0:16:29, wall=01:46 IST
=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.640 Prec@1=60.903 Prec@5=83.321 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=01:46 IST
=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.473 DataTime=0.277 Loss=1.640 Prec@1=60.903 Prec@5=83.321 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=01:47 IST
=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.277 Loss=1.641 Prec@1=60.892 Prec@5=83.302 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=01:47 IST
=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.277 Loss=1.641 Prec@1=60.892 Prec@5=83.302 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=01:47 IST
=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.277 Loss=1.641 Prec@1=60.892 Prec@5=83.302 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=01:48 IST
=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.642 Prec@1=60.881 Prec@5=83.279 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=01:48 IST
=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.642 Prec@1=60.881 Prec@5=83.279 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=01:48 IST
=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.472 DataTime=0.276 Loss=1.642 Prec@1=60.881 Prec@5=83.279 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=01:48 IST
=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.643 Prec@1=60.866 Prec@5=83.267 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.643 Prec@1=60.866 Prec@5=83.267 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.643 Prec@1=60.866 Prec@5=83.267 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.643 Prec@1=60.864 Prec@5=83.266 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=01:48 IST
=> training   100.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.471 DataTime=0.276 Loss=1.643 Prec@1=60.864 Prec@5=83.266 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=01:48 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> validation 0.00% of 1x98...Epoch=30/150 LR=0.09106 Time=6.901 Loss=1.106 Prec@1=72.070 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.901 Loss=1.106 Prec@1=72.070 Prec@5=92.578 rate=5010.60 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.901 Loss=1.106 Prec@1=72.070 Prec@5=92.578 rate=5010.60 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=0.551 Loss=1.629 Prec@1=60.770 Prec@5=83.702 rate=5010.60 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** validation 100.00% of 1x98...Epoch=30/150 LR=0.09106 Time=0.551 Loss=1.629 Prec@1=60.770 Prec@5=83.702 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=01:49 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.127 DataTime=4.881 Loss=1.732 Prec@1=57.812 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.127 DataTime=4.881 Loss=1.732 Prec@1=57.812 Prec@5=82.812 rate=8271.37 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.127 DataTime=4.881 Loss=1.732 Prec@1=57.812 Prec@5=82.812 rate=8271.37 Hz, eta=0:00:00, total=0:00:00, wall=01:50 IST
=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.500 DataTime=0.303 Loss=1.602 Prec@1=61.599 Prec@5=83.934 rate=8271.37 Hz, eta=0:00:00, total=0:00:00, wall=01:50 IST
=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.500 DataTime=0.303 Loss=1.602 Prec@1=61.599 Prec@5=83.934 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=01:50 IST
=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.500 DataTime=0.303 Loss=1.602 Prec@1=61.599 Prec@5=83.934 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=01:51 IST
=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.485 DataTime=0.288 Loss=1.605 Prec@1=61.592 Prec@5=83.893 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=01:51 IST
=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.485 DataTime=0.288 Loss=1.605 Prec@1=61.592 Prec@5=83.893 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=01:51 IST
=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.485 DataTime=0.288 Loss=1.605 Prec@1=61.592 Prec@5=83.893 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=01:52 IST
=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.481 DataTime=0.284 Loss=1.604 Prec@1=61.682 Prec@5=83.881 rate=2.17 Hz, eta=0:17:38, total=0:01:32, wall=01:52 IST
=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.481 DataTime=0.284 Loss=1.604 Prec@1=61.682 Prec@5=83.881 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=01:52 IST
=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.481 DataTime=0.284 Loss=1.604 Prec@1=61.682 Prec@5=83.881 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=01:53 IST
=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.475 DataTime=0.278 Loss=1.610 Prec@1=61.589 Prec@5=83.785 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=01:53 IST
=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.475 DataTime=0.278 Loss=1.610 Prec@1=61.589 Prec@5=83.785 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=01:53 IST
=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.475 DataTime=0.278 Loss=1.610 Prec@1=61.589 Prec@5=83.785 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=01:53 IST
=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.472 DataTime=0.276 Loss=1.611 Prec@1=61.546 Prec@5=83.749 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=01:53 IST
=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.472 DataTime=0.276 Loss=1.611 Prec@1=61.546 Prec@5=83.749 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=01:53 IST
=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.472 DataTime=0.276 Loss=1.611 Prec@1=61.546 Prec@5=83.749 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=01:54 IST
=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.471 DataTime=0.275 Loss=1.611 Prec@1=61.533 Prec@5=83.755 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=01:54 IST
=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.471 DataTime=0.275 Loss=1.611 Prec@1=61.533 Prec@5=83.755 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:54 IST
=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.471 DataTime=0.275 Loss=1.611 Prec@1=61.533 Prec@5=83.755 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:55 IST
=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.613 Prec@1=61.466 Prec@5=83.721 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=01:55 IST
=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.613 Prec@1=61.466 Prec@5=83.721 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=01:55 IST
=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.613 Prec@1=61.466 Prec@5=83.721 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=01:56 IST
=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.275 Loss=1.614 Prec@1=61.416 Prec@5=83.702 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.275 Loss=1.614 Prec@1=61.416 Prec@5=83.702 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.275 Loss=1.614 Prec@1=61.416 Prec@5=83.702 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.616 Prec@1=61.386 Prec@5=83.652 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=01:56 IST
=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.616 Prec@1=61.386 Prec@5=83.652 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=01:56 IST
=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.470 DataTime=0.275 Loss=1.616 Prec@1=61.386 Prec@5=83.652 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=01:57 IST
=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.618 Prec@1=61.365 Prec@5=83.623 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=01:57 IST
=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.618 Prec@1=61.365 Prec@5=83.623 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=01:57 IST
=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.618 Prec@1=61.365 Prec@5=83.623 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=01:58 IST
=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.620 Prec@1=61.323 Prec@5=83.604 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=01:58 IST
=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.620 Prec@1=61.323 Prec@5=83.604 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=01:58 IST
=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.620 Prec@1=61.323 Prec@5=83.604 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=01:59 IST
=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.621 Prec@1=61.265 Prec@5=83.596 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=01:59 IST
=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.621 Prec@1=61.265 Prec@5=83.596 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=01:59 IST
=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.621 Prec@1=61.265 Prec@5=83.596 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=02:00 IST
=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.622 Prec@1=61.266 Prec@5=83.585 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.622 Prec@1=61.266 Prec@5=83.585 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.274 Loss=1.622 Prec@1=61.266 Prec@5=83.585 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.623 Prec@1=61.249 Prec@5=83.565 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:00 IST
=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.623 Prec@1=61.249 Prec@5=83.565 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=02:00 IST
=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.623 Prec@1=61.249 Prec@5=83.565 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=02:01 IST
=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.623 Prec@1=61.262 Prec@5=83.569 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=02:01 IST
=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.623 Prec@1=61.262 Prec@5=83.569 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=02:01 IST
=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.623 Prec@1=61.262 Prec@5=83.569 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=02:02 IST
=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.625 Prec@1=61.231 Prec@5=83.533 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=02:02 IST
=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.625 Prec@1=61.231 Prec@5=83.533 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=02:02 IST
=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.625 Prec@1=61.231 Prec@5=83.533 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=02:03 IST
=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.205 Prec@5=83.518 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=02:03 IST
=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.205 Prec@5=83.518 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=02:03 IST
=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.205 Prec@5=83.518 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=02:03 IST
=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.190 Prec@5=83.505 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=02:03 IST
=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.190 Prec@5=83.505 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=02:03 IST
=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.469 DataTime=0.273 Loss=1.627 Prec@1=61.190 Prec@5=83.505 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=02:04 IST
=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.629 Prec@1=61.161 Prec@5=83.487 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=02:04 IST
=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.629 Prec@1=61.161 Prec@5=83.487 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=02:04 IST
=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.629 Prec@1=61.161 Prec@5=83.487 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=02:05 IST
=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.630 Prec@1=61.154 Prec@5=83.478 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=02:05 IST
=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.630 Prec@1=61.154 Prec@5=83.478 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=02:05 IST
=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.630 Prec@1=61.154 Prec@5=83.478 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=02:06 IST
=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.631 Prec@1=61.128 Prec@5=83.457 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=02:06 IST
=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.631 Prec@1=61.128 Prec@5=83.457 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=02:06 IST
=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.631 Prec@1=61.128 Prec@5=83.457 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=02:07 IST
=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.633 Prec@1=61.107 Prec@5=83.443 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=02:07 IST
=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.633 Prec@1=61.107 Prec@5=83.443 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=02:07 IST
=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.273 Loss=1.633 Prec@1=61.107 Prec@5=83.443 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=02:07 IST
=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.088 Prec@5=83.423 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=02:07 IST
=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.088 Prec@5=83.423 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=02:07 IST
=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.088 Prec@5=83.423 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=02:08 IST
=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.075 Prec@5=83.415 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=02:08 IST
=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.075 Prec@5=83.415 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=02:08 IST
=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.634 Prec@1=61.075 Prec@5=83.415 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=02:09 IST
=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.635 Prec@1=61.054 Prec@5=83.408 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.635 Prec@1=61.054 Prec@5=83.408 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.635 Prec@1=61.054 Prec@5=83.408 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.635 Prec@1=61.053 Prec@5=83.408 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=02:09 IST
=> training   100.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.468 DataTime=0.272 Loss=1.635 Prec@1=61.053 Prec@5=83.408 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=02:09 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 0.00% of 1x98...Epoch=31/150 LR=0.09045 Time=6.996 Loss=1.183 Prec@1=72.461 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=6.996 Loss=1.183 Prec@1=72.461 Prec@5=89.453 rate=9048.38 Hz, eta=0:00:00, total=0:00:00, wall=02:09 IST
** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=6.996 Loss=1.183 Prec@1=72.461 Prec@5=89.453 rate=9048.38 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=0.557 Loss=1.645 Prec@1=60.544 Prec@5=83.556 rate=9048.38 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
** validation 100.00% of 1x98...Epoch=31/150 LR=0.09045 Time=0.557 Loss=1.645 Prec@1=60.544 Prec@5=83.556 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=02:10 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.098 DataTime=4.712 Loss=1.429 Prec@1=68.164 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.098 DataTime=4.712 Loss=1.429 Prec@1=68.164 Prec@5=86.133 rate=9473.82 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.098 DataTime=4.712 Loss=1.429 Prec@1=68.164 Prec@5=86.133 rate=9473.82 Hz, eta=0:00:00, total=0:00:00, wall=02:11 IST
=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.513 DataTime=0.314 Loss=1.595 Prec@1=61.870 Prec@5=83.996 rate=9473.82 Hz, eta=0:00:00, total=0:00:00, wall=02:11 IST
=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.513 DataTime=0.314 Loss=1.595 Prec@1=61.870 Prec@5=83.996 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:11 IST
=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.513 DataTime=0.314 Loss=1.595 Prec@1=61.870 Prec@5=83.996 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:11 IST
=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.487 DataTime=0.289 Loss=1.592 Prec@1=61.979 Prec@5=84.074 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:11 IST
=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.487 DataTime=0.289 Loss=1.592 Prec@1=61.979 Prec@5=84.074 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=02:11 IST
=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.487 DataTime=0.289 Loss=1.592 Prec@1=61.979 Prec@5=84.074 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=02:12 IST
=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.591 Prec@1=61.980 Prec@5=84.067 rate=2.16 Hz, eta=0:17:43, total=0:01:32, wall=02:12 IST
=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.591 Prec@1=61.980 Prec@5=84.067 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=02:12 IST
=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.591 Prec@1=61.980 Prec@5=84.067 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=02:13 IST
=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.280 Loss=1.600 Prec@1=61.796 Prec@5=83.907 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=02:13 IST
=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.280 Loss=1.600 Prec@1=61.796 Prec@5=83.907 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=02:13 IST
=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.280 Loss=1.600 Prec@1=61.796 Prec@5=83.907 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=02:14 IST
=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.481 DataTime=0.284 Loss=1.603 Prec@1=61.721 Prec@5=83.838 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=02:14 IST
=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.481 DataTime=0.284 Loss=1.603 Prec@1=61.721 Prec@5=83.838 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=02:14 IST
=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.481 DataTime=0.284 Loss=1.603 Prec@1=61.721 Prec@5=83.838 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=02:15 IST
=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.606 Prec@1=61.645 Prec@5=83.809 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=02:15 IST
=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.606 Prec@1=61.645 Prec@5=83.809 rate=2.13 Hz, eta=0:14:54, total=0:04:42, wall=02:15 IST
=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.282 Loss=1.606 Prec@1=61.645 Prec@5=83.809 rate=2.13 Hz, eta=0:14:54, total=0:04:42, wall=02:15 IST
=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.607 Prec@1=61.588 Prec@5=83.795 rate=2.13 Hz, eta=0:14:54, total=0:04:42, wall=02:15 IST
=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.607 Prec@1=61.588 Prec@5=83.795 rate=2.13 Hz, eta=0:14:07, total=0:05:29, wall=02:15 IST
=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.607 Prec@1=61.588 Prec@5=83.795 rate=2.13 Hz, eta=0:14:07, total=0:05:29, wall=02:16 IST
=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.609 Prec@1=61.542 Prec@5=83.788 rate=2.13 Hz, eta=0:14:07, total=0:05:29, wall=02:16 IST
=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.609 Prec@1=61.542 Prec@5=83.788 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=02:16 IST
=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.609 Prec@1=61.542 Prec@5=83.788 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=02:17 IST
=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.612 Prec@1=61.499 Prec@5=83.736 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=02:17 IST
=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.612 Prec@1=61.499 Prec@5=83.736 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=02:17 IST
=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.479 DataTime=0.283 Loss=1.612 Prec@1=61.499 Prec@5=83.736 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=02:18 IST
=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.282 Loss=1.613 Prec@1=61.489 Prec@5=83.735 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=02:18 IST
=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.282 Loss=1.613 Prec@1=61.489 Prec@5=83.735 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=02:18 IST
=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.478 DataTime=0.282 Loss=1.613 Prec@1=61.489 Prec@5=83.735 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=02:19 IST
=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.614 Prec@1=61.478 Prec@5=83.722 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=02:19 IST
=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.614 Prec@1=61.478 Prec@5=83.722 rate=2.12 Hz, eta=0:11:02, total=0:08:39, wall=02:19 IST
=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.281 Loss=1.614 Prec@1=61.478 Prec@5=83.722 rate=2.12 Hz, eta=0:11:02, total=0:08:39, wall=02:19 IST
=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.280 Loss=1.613 Prec@1=61.477 Prec@5=83.719 rate=2.12 Hz, eta=0:11:02, total=0:08:39, wall=02:19 IST
=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.280 Loss=1.613 Prec@1=61.477 Prec@5=83.719 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=02:19 IST
=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.477 DataTime=0.280 Loss=1.613 Prec@1=61.477 Prec@5=83.719 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=02:20 IST
=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.279 Loss=1.616 Prec@1=61.433 Prec@5=83.699 rate=2.12 Hz, eta=0:10:15, total=0:09:27, wall=02:20 IST
=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.279 Loss=1.616 Prec@1=61.433 Prec@5=83.699 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=02:20 IST
=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.279 Loss=1.616 Prec@1=61.433 Prec@5=83.699 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=02:21 IST
=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.278 Loss=1.617 Prec@1=61.412 Prec@5=83.676 rate=2.12 Hz, eta=0:09:26, total=0:10:13, wall=02:21 IST
=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.278 Loss=1.617 Prec@1=61.412 Prec@5=83.676 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=02:21 IST
=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.475 DataTime=0.278 Loss=1.617 Prec@1=61.412 Prec@5=83.676 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=02:22 IST
=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.474 DataTime=0.278 Loss=1.617 Prec@1=61.391 Prec@5=83.661 rate=2.12 Hz, eta=0:08:39, total=0:11:00, wall=02:22 IST
=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.474 DataTime=0.278 Loss=1.617 Prec@1=61.391 Prec@5=83.661 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=02:22 IST
=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.474 DataTime=0.278 Loss=1.617 Prec@1=61.391 Prec@5=83.661 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=02:22 IST
=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.620 Prec@1=61.344 Prec@5=83.620 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=02:22 IST
=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.620 Prec@1=61.344 Prec@5=83.620 rate=2.13 Hz, eta=0:07:04, total=0:12:33, wall=02:22 IST
=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.620 Prec@1=61.344 Prec@5=83.620 rate=2.13 Hz, eta=0:07:04, total=0:12:33, wall=02:23 IST
=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.621 Prec@1=61.321 Prec@5=83.604 rate=2.13 Hz, eta=0:07:04, total=0:12:33, wall=02:23 IST
=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.621 Prec@1=61.321 Prec@5=83.604 rate=2.13 Hz, eta=0:06:17, total=0:13:20, wall=02:23 IST
=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.621 Prec@1=61.321 Prec@5=83.604 rate=2.13 Hz, eta=0:06:17, total=0:13:20, wall=02:24 IST
=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.622 Prec@1=61.304 Prec@5=83.589 rate=2.13 Hz, eta=0:06:17, total=0:13:20, wall=02:24 IST
=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.622 Prec@1=61.304 Prec@5=83.589 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=02:24 IST
=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.622 Prec@1=61.304 Prec@5=83.589 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=02:25 IST
=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.623 Prec@1=61.278 Prec@5=83.580 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=02:25 IST
=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.623 Prec@1=61.278 Prec@5=83.580 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=02:25 IST
=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.623 Prec@1=61.278 Prec@5=83.580 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=02:26 IST
=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.624 Prec@1=61.275 Prec@5=83.567 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=02:26 IST
=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.624 Prec@1=61.275 Prec@5=83.567 rate=2.13 Hz, eta=0:03:55, total=0:15:40, wall=02:26 IST
=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.624 Prec@1=61.275 Prec@5=83.567 rate=2.13 Hz, eta=0:03:55, total=0:15:40, wall=02:26 IST
=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.257 Prec@5=83.552 rate=2.13 Hz, eta=0:03:55, total=0:15:40, wall=02:26 IST
=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.257 Prec@5=83.552 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=02:26 IST
=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.625 Prec@1=61.257 Prec@5=83.552 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=02:27 IST
=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.626 Prec@1=61.229 Prec@5=83.538 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=02:27 IST
=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.626 Prec@1=61.229 Prec@5=83.538 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=02:27 IST
=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.472 DataTime=0.276 Loss=1.626 Prec@1=61.229 Prec@5=83.538 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=02:28 IST
=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.226 Prec@5=83.527 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=02:28 IST
=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.226 Prec@5=83.527 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=02:28 IST
=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.226 Prec@5=83.527 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=02:29 IST
=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.216 Prec@5=83.520 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=02:29 IST
=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.216 Prec@5=83.520 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=02:29 IST
=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.627 Prec@1=61.216 Prec@5=83.520 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=02:30 IST
=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.628 Prec@1=61.202 Prec@5=83.514 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.628 Prec@1=61.202 Prec@5=83.514 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.628 Prec@1=61.202 Prec@5=83.514 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.628 Prec@1=61.200 Prec@5=83.513 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=02:30 IST
=> training   100.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.473 DataTime=0.277 Loss=1.628 Prec@1=61.200 Prec@5=83.513 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=02:30 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:30 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:30 IST
=> validation 0.00% of 1x98...Epoch=32/150 LR=0.08983 Time=7.012 Loss=1.039 Prec@1=72.461 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=02:30 IST
=> validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=7.012 Loss=1.039 Prec@1=72.461 Prec@5=92.578 rate=8325.08 Hz, eta=0:00:00, total=0:00:00, wall=02:30 IST
** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=7.012 Loss=1.039 Prec@1=72.461 Prec@5=92.578 rate=8325.08 Hz, eta=0:00:00, total=0:00:00, wall=02:30 IST
** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=0.547 Loss=1.625 Prec@1=61.028 Prec@5=84.040 rate=8325.08 Hz, eta=0:00:00, total=0:00:00, wall=02:30 IST
** validation 100.00% of 1x98...Epoch=32/150 LR=0.08983 Time=0.547 Loss=1.625 Prec@1=61.028 Prec@5=84.040 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=02:30 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> training   0.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=4.611 DataTime=4.341 Loss=1.657 Prec@1=59.375 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=4.611 DataTime=4.341 Loss=1.657 Prec@1=59.375 Prec@5=83.789 rate=5425.11 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=4.611 DataTime=4.341 Loss=1.657 Prec@1=59.375 Prec@5=83.789 rate=5425.11 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.503 DataTime=0.305 Loss=1.586 Prec@1=61.984 Prec@5=83.984 rate=5425.11 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.503 DataTime=0.305 Loss=1.586 Prec@1=61.984 Prec@5=83.984 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=02:31 IST
=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.503 DataTime=0.305 Loss=1.586 Prec@1=61.984 Prec@5=83.984 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=02:32 IST
=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.488 DataTime=0.291 Loss=1.585 Prec@1=62.029 Prec@5=84.047 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=02:32 IST
=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.488 DataTime=0.291 Loss=1.585 Prec@1=62.029 Prec@5=84.047 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=02:32 IST
=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.488 DataTime=0.291 Loss=1.585 Prec@1=62.029 Prec@5=84.047 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=02:33 IST
=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.480 DataTime=0.283 Loss=1.592 Prec@1=61.943 Prec@5=83.981 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=02:33 IST
=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.480 DataTime=0.283 Loss=1.592 Prec@1=61.943 Prec@5=83.981 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=02:33 IST
=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.480 DataTime=0.283 Loss=1.592 Prec@1=61.943 Prec@5=83.981 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=02:34 IST
=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.473 DataTime=0.277 Loss=1.593 Prec@1=61.929 Prec@5=83.986 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=02:34 IST
=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.473 DataTime=0.277 Loss=1.593 Prec@1=61.929 Prec@5=83.986 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=02:34 IST
=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.473 DataTime=0.277 Loss=1.593 Prec@1=61.929 Prec@5=83.986 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=02:34 IST
=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.474 DataTime=0.277 Loss=1.594 Prec@1=61.921 Prec@5=83.964 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=02:34 IST
=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.474 DataTime=0.277 Loss=1.594 Prec@1=61.921 Prec@5=83.964 rate=2.15 Hz, eta=0:15:29, total=0:03:52, wall=02:34 IST
=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.474 DataTime=0.277 Loss=1.594 Prec@1=61.921 Prec@5=83.964 rate=2.15 Hz, eta=0:15:29, total=0:03:52, wall=02:35 IST
=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.595 Prec@1=61.914 Prec@5=83.918 rate=2.15 Hz, eta=0:15:29, total=0:03:52, wall=02:35 IST
=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.595 Prec@1=61.914 Prec@5=83.918 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=02:35 IST
=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.595 Prec@1=61.914 Prec@5=83.918 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=02:36 IST
=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.860 Prec@5=83.924 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=02:36 IST
=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.860 Prec@5=83.924 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=02:36 IST
=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.860 Prec@5=83.924 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=02:37 IST
=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.599 Prec@1=61.837 Prec@5=83.879 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=02:37 IST
=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.599 Prec@1=61.837 Prec@5=83.879 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=02:37 IST
=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.472 DataTime=0.275 Loss=1.599 Prec@1=61.837 Prec@5=83.879 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=02:38 IST
=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.601 Prec@1=61.784 Prec@5=83.850 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=02:38 IST
=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.601 Prec@1=61.784 Prec@5=83.850 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=02:38 IST
=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.601 Prec@1=61.784 Prec@5=83.850 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=02:38 IST
=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.601 Prec@1=61.761 Prec@5=83.842 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=02:38 IST
=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.601 Prec@1=61.761 Prec@5=83.842 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=02:38 IST
=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.601 Prec@1=61.761 Prec@5=83.842 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=02:39 IST
=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.603 Prec@1=61.729 Prec@5=83.821 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=02:39 IST
=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.603 Prec@1=61.729 Prec@5=83.821 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=02:39 IST
=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.603 Prec@1=61.729 Prec@5=83.821 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=02:40 IST
=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.273 Loss=1.604 Prec@1=61.709 Prec@5=83.810 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=02:40 IST
=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.273 Loss=1.604 Prec@1=61.709 Prec@5=83.810 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=02:40 IST
=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.273 Loss=1.604 Prec@1=61.709 Prec@5=83.810 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=02:41 IST
=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.605 Prec@1=61.682 Prec@5=83.785 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=02:41 IST
=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.605 Prec@1=61.682 Prec@5=83.785 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:41 IST
=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.605 Prec@1=61.682 Prec@5=83.785 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:41 IST
=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.607 Prec@1=61.623 Prec@5=83.763 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=02:41 IST
=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.607 Prec@1=61.623 Prec@5=83.763 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=02:41 IST
=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.607 Prec@1=61.623 Prec@5=83.763 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=02:42 IST
=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.608 Prec@1=61.619 Prec@5=83.750 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=02:42 IST
=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.608 Prec@1=61.619 Prec@5=83.750 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=02:42 IST
=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.273 Loss=1.608 Prec@1=61.619 Prec@5=83.750 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=02:43 IST
=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.609 Prec@1=61.610 Prec@5=83.736 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=02:43 IST
=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.609 Prec@1=61.610 Prec@5=83.736 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=02:43 IST
=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.609 Prec@1=61.610 Prec@5=83.736 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=02:44 IST
=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.611 Prec@1=61.574 Prec@5=83.703 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=02:44 IST
=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.611 Prec@1=61.574 Prec@5=83.703 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=02:44 IST
=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.611 Prec@1=61.574 Prec@5=83.703 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=02:45 IST
=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.612 Prec@1=61.541 Prec@5=83.694 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=02:45 IST
=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.612 Prec@1=61.541 Prec@5=83.694 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=02:45 IST
=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.469 DataTime=0.274 Loss=1.612 Prec@1=61.541 Prec@5=83.694 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=02:45 IST
=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.613 Prec@1=61.536 Prec@5=83.684 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=02:45 IST
=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.613 Prec@1=61.536 Prec@5=83.684 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=02:45 IST
=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.613 Prec@1=61.536 Prec@5=83.684 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=02:46 IST
=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.614 Prec@1=61.508 Prec@5=83.654 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=02:46 IST
=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.614 Prec@1=61.508 Prec@5=83.654 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=02:46 IST
=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.471 DataTime=0.275 Loss=1.614 Prec@1=61.508 Prec@5=83.654 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=02:47 IST
=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.615 Prec@1=61.491 Prec@5=83.646 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=02:47 IST
=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.615 Prec@1=61.491 Prec@5=83.646 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=02:47 IST
=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.615 Prec@1=61.491 Prec@5=83.646 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=02:48 IST
=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.616 Prec@1=61.471 Prec@5=83.635 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=02:48 IST
=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.616 Prec@1=61.471 Prec@5=83.635 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=02:48 IST
=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.616 Prec@1=61.471 Prec@5=83.635 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=02:49 IST
=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.617 Prec@1=61.440 Prec@5=83.624 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=02:49 IST
=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.617 Prec@1=61.440 Prec@5=83.624 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=02:49 IST
=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.617 Prec@1=61.440 Prec@5=83.624 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=02:49 IST
=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.417 Prec@5=83.610 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=02:49 IST
=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.417 Prec@5=83.610 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=02:49 IST
=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.417 Prec@5=83.610 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=02:50 IST
=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.416 Prec@5=83.601 rate=2.14 Hz, eta=0:00:47, total=0:18:43, wall=02:50 IST
=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.416 Prec@5=83.601 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=02:50 IST
=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.416 Prec@5=83.601 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=02:50 IST
=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.416 Prec@5=83.602 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=02:50 IST
=> training   100.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.470 DataTime=0.274 Loss=1.619 Prec@1=61.416 Prec@5=83.602 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=02:50 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST
=> validation 0.00% of 1x98...Epoch=33/150 LR=0.08918 Time=6.913 Loss=1.095 Prec@1=72.852 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST
=> validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=6.913 Loss=1.095 Prec@1=72.852 Prec@5=91.992 rate=8893.08 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST
** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=6.913 Loss=1.095 Prec@1=72.852 Prec@5=91.992 rate=8893.08 Hz, eta=0:00:00, total=0:00:00, wall=02:51 IST
** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=0.550 Loss=1.611 Prec@1=61.142 Prec@5=84.174 rate=8893.08 Hz, eta=0:00:00, total=0:00:00, wall=02:51 IST
** validation 100.00% of 1x98...Epoch=33/150 LR=0.08918 Time=0.550 Loss=1.611 Prec@1=61.142 Prec@5=84.174 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=02:51 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> training   0.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=5.029 DataTime=4.782 Loss=1.526 Prec@1=68.164 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=5.029 DataTime=4.782 Loss=1.526 Prec@1=68.164 Prec@5=84.570 rate=6517.97 Hz, eta=0:00:00, total=0:00:00, wall=02:51 IST
=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=5.029 DataTime=4.782 Loss=1.526 Prec@1=68.164 Prec@5=84.570 rate=6517.97 Hz, eta=0:00:00, total=0:00:00, wall=02:52 IST
=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.510 DataTime=0.314 Loss=1.585 Prec@1=61.974 Prec@5=84.037 rate=6517.97 Hz, eta=0:00:00, total=0:00:00, wall=02:52 IST
=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.510 DataTime=0.314 Loss=1.585 Prec@1=61.974 Prec@5=84.037 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=02:52 IST
=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.510 DataTime=0.314 Loss=1.585 Prec@1=61.974 Prec@5=84.037 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=02:53 IST
=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.488 DataTime=0.290 Loss=1.579 Prec@1=62.078 Prec@5=84.245 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=02:53 IST
=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.488 DataTime=0.290 Loss=1.579 Prec@1=62.078 Prec@5=84.245 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=02:53 IST
=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.488 DataTime=0.290 Loss=1.579 Prec@1=62.078 Prec@5=84.245 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=02:53 IST
=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.477 DataTime=0.280 Loss=1.582 Prec@1=62.046 Prec@5=84.148 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=02:53 IST
=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.477 DataTime=0.280 Loss=1.582 Prec@1=62.046 Prec@5=84.148 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=02:53 IST
=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.477 DataTime=0.280 Loss=1.582 Prec@1=62.046 Prec@5=84.148 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=02:54 IST
=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.472 DataTime=0.276 Loss=1.583 Prec@1=62.034 Prec@5=84.121 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=02:54 IST
=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.472 DataTime=0.276 Loss=1.583 Prec@1=62.034 Prec@5=84.121 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=02:54 IST
=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.472 DataTime=0.276 Loss=1.583 Prec@1=62.034 Prec@5=84.121 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=02:55 IST
=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.585 Prec@1=61.984 Prec@5=84.101 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=02:55 IST
=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.585 Prec@1=61.984 Prec@5=84.101 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=02:55 IST
=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.585 Prec@1=61.984 Prec@5=84.101 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=02:56 IST
=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.588 Prec@1=61.913 Prec@5=84.038 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=02:56 IST
=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.588 Prec@1=61.913 Prec@5=84.038 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=02:56 IST
=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.470 DataTime=0.274 Loss=1.588 Prec@1=61.913 Prec@5=84.038 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=02:57 IST
=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.469 DataTime=0.273 Loss=1.590 Prec@1=61.884 Prec@5=84.000 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=02:57 IST
=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.469 DataTime=0.273 Loss=1.590 Prec@1=61.884 Prec@5=84.000 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=02:57 IST
=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.469 DataTime=0.273 Loss=1.590 Prec@1=61.884 Prec@5=84.000 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=02:57 IST
=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.846 Prec@5=83.998 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=02:57 IST
=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.846 Prec@5=83.998 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=02:57 IST
=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.846 Prec@5=83.998 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=02:58 IST
=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.592 Prec@1=61.855 Prec@5=83.981 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=02:58 IST
=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.592 Prec@1=61.855 Prec@5=83.981 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=02:58 IST
=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.592 Prec@1=61.855 Prec@5=83.981 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=02:59 IST
=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.594 Prec@1=61.855 Prec@5=83.958 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=02:59 IST
=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.594 Prec@1=61.855 Prec@5=83.958 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=02:59 IST
=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.594 Prec@1=61.855 Prec@5=83.958 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=03:00 IST
=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.595 Prec@1=61.817 Prec@5=83.938 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=03:00 IST
=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.595 Prec@1=61.817 Prec@5=83.938 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=03:00 IST
=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.595 Prec@1=61.817 Prec@5=83.938 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=03:00 IST
=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.596 Prec@1=61.810 Prec@5=83.919 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=03:00 IST
=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.596 Prec@1=61.810 Prec@5=83.919 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=03:00 IST
=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.596 Prec@1=61.810 Prec@5=83.919 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=03:01 IST
=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.598 Prec@1=61.778 Prec@5=83.897 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=03:01 IST
=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.598 Prec@1=61.778 Prec@5=83.897 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=03:01 IST
=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.598 Prec@1=61.778 Prec@5=83.897 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=03:02 IST
=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.599 Prec@1=61.751 Prec@5=83.880 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=03:02 IST
=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.599 Prec@1=61.751 Prec@5=83.880 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=03:02 IST
=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.599 Prec@1=61.751 Prec@5=83.880 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=03:03 IST
=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.599 Prec@1=61.754 Prec@5=83.884 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=03:03 IST
=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.599 Prec@1=61.754 Prec@5=83.884 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=03:03 IST
=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.599 Prec@1=61.754 Prec@5=83.884 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=03:04 IST
=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.600 Prec@1=61.731 Prec@5=83.866 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=03:04 IST
=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.600 Prec@1=61.731 Prec@5=83.866 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=03:04 IST
=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.600 Prec@1=61.731 Prec@5=83.866 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=03:04 IST
=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.602 Prec@1=61.695 Prec@5=83.841 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=03:04 IST
=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.602 Prec@1=61.695 Prec@5=83.841 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=03:04 IST
=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.602 Prec@1=61.695 Prec@5=83.841 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=03:05 IST
=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.603 Prec@1=61.671 Prec@5=83.837 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=03:05 IST
=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.603 Prec@1=61.671 Prec@5=83.837 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=03:05 IST
=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.603 Prec@1=61.671 Prec@5=83.837 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=03:06 IST
=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.603 Prec@1=61.659 Prec@5=83.833 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=03:06 IST
=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.603 Prec@1=61.659 Prec@5=83.833 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=03:06 IST
=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.603 Prec@1=61.659 Prec@5=83.833 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=03:07 IST
=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.605 Prec@1=61.634 Prec@5=83.807 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=03:07 IST
=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.605 Prec@1=61.634 Prec@5=83.807 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=03:07 IST
=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.605 Prec@1=61.634 Prec@5=83.807 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=03:07 IST
=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.606 Prec@1=61.604 Prec@5=83.787 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=03:07 IST
=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.606 Prec@1=61.604 Prec@5=83.787 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=03:07 IST
=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.606 Prec@1=61.604 Prec@5=83.787 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=03:08 IST
=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.607 Prec@1=61.580 Prec@5=83.765 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=03:08 IST
=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.607 Prec@1=61.580 Prec@5=83.765 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=03:08 IST
=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.466 DataTime=0.270 Loss=1.607 Prec@1=61.580 Prec@5=83.765 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=03:09 IST
=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.608 Prec@1=61.553 Prec@5=83.753 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=03:09 IST
=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.608 Prec@1=61.553 Prec@5=83.753 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=03:09 IST
=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.608 Prec@1=61.553 Prec@5=83.753 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=03:10 IST
=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.609 Prec@1=61.535 Prec@5=83.742 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=03:10 IST
=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.609 Prec@1=61.535 Prec@5=83.742 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=03:10 IST
=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.270 Loss=1.609 Prec@1=61.535 Prec@5=83.742 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=03:11 IST
=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.610 Prec@1=61.520 Prec@5=83.730 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=03:11 IST
=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.610 Prec@1=61.520 Prec@5=83.730 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=03:11 IST
=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.610 Prec@1=61.520 Prec@5=83.730 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=03:11 IST
=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.610 Prec@1=61.518 Prec@5=83.729 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=03:11 IST
=> training   100.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.467 DataTime=0.271 Loss=1.610 Prec@1=61.518 Prec@5=83.729 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=03:11 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST
=> validation 0.00% of 1x98...Epoch=34/150 LR=0.08853 Time=7.019 Loss=1.233 Prec@1=66.797 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST
=> validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=7.019 Loss=1.233 Prec@1=66.797 Prec@5=91.016 rate=7374.36 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST
** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=7.019 Loss=1.233 Prec@1=66.797 Prec@5=91.016 rate=7374.36 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST
** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=0.555 Loss=1.696 Prec@1=59.396 Prec@5=82.892 rate=7374.36 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST
** validation 100.00% of 1x98...Epoch=34/150 LR=0.08853 Time=0.555 Loss=1.696 Prec@1=59.396 Prec@5=82.892 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=03:11 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST
=> training   0.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.984 DataTime=4.744 Loss=1.573 Prec@1=61.133 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST
=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.984 DataTime=4.744 Loss=1.573 Prec@1=61.133 Prec@5=85.547 rate=7486.43 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST
=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.984 DataTime=4.744 Loss=1.573 Prec@1=61.133 Prec@5=85.547 rate=7486.43 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST
=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.510 DataTime=0.314 Loss=1.555 Prec@1=62.657 Prec@5=84.667 rate=7486.43 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST
=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.510 DataTime=0.314 Loss=1.555 Prec@1=62.657 Prec@5=84.667 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=03:12 IST
=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.510 DataTime=0.314 Loss=1.555 Prec@1=62.657 Prec@5=84.667 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=03:13 IST
=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.491 DataTime=0.294 Loss=1.563 Prec@1=62.329 Prec@5=84.503 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=03:13 IST
=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.491 DataTime=0.294 Loss=1.563 Prec@1=62.329 Prec@5=84.503 rate=2.15 Hz, eta=0:17:53, total=0:01:33, wall=03:13 IST
=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.491 DataTime=0.294 Loss=1.563 Prec@1=62.329 Prec@5=84.503 rate=2.15 Hz, eta=0:17:53, total=0:01:33, wall=03:14 IST
=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.485 DataTime=0.288 Loss=1.567 Prec@1=62.207 Prec@5=84.386 rate=2.15 Hz, eta=0:17:53, total=0:01:33, wall=03:14 IST
=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.485 DataTime=0.288 Loss=1.567 Prec@1=62.207 Prec@5=84.386 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=03:14 IST
=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.485 DataTime=0.288 Loss=1.567 Prec@1=62.207 Prec@5=84.386 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=03:15 IST
=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.482 DataTime=0.285 Loss=1.570 Prec@1=62.206 Prec@5=84.314 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=03:15 IST
=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.482 DataTime=0.285 Loss=1.570 Prec@1=62.206 Prec@5=84.314 rate=2.13 Hz, eta=0:16:26, total=0:03:08, wall=03:15 IST
=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.482 DataTime=0.285 Loss=1.570 Prec@1=62.206 Prec@5=84.314 rate=2.13 Hz, eta=0:16:26, total=0:03:08, wall=03:15 IST
=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.478 DataTime=0.281 Loss=1.571 Prec@1=62.203 Prec@5=84.322 rate=2.13 Hz, eta=0:16:26, total=0:03:08, wall=03:15 IST
=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.478 DataTime=0.281 Loss=1.571 Prec@1=62.203 Prec@5=84.322 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=03:15 IST
=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.478 DataTime=0.281 Loss=1.571 Prec@1=62.203 Prec@5=84.322 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=03:16 IST
=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.477 DataTime=0.281 Loss=1.576 Prec@1=62.135 Prec@5=84.262 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=03:16 IST
=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.477 DataTime=0.281 Loss=1.576 Prec@1=62.135 Prec@5=84.262 rate=2.13 Hz, eta=0:14:52, total=0:04:41, wall=03:16 IST
=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.477 DataTime=0.281 Loss=1.576 Prec@1=62.135 Prec@5=84.262 rate=2.13 Hz, eta=0:14:52, total=0:04:41, wall=03:17 IST
=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.476 DataTime=0.280 Loss=1.581 Prec@1=62.021 Prec@5=84.188 rate=2.13 Hz, eta=0:14:52, total=0:04:41, wall=03:17 IST
=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.476 DataTime=0.280 Loss=1.581 Prec@1=62.021 Prec@5=84.188 rate=2.13 Hz, eta=0:14:05, total=0:05:28, wall=03:17 IST
=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.476 DataTime=0.280 Loss=1.581 Prec@1=62.021 Prec@5=84.188 rate=2.13 Hz, eta=0:14:05, total=0:05:28, wall=03:18 IST
=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.475 DataTime=0.279 Loss=1.583 Prec@1=61.988 Prec@5=84.165 rate=2.13 Hz, eta=0:14:05, total=0:05:28, wall=03:18 IST
=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.475 DataTime=0.279 Loss=1.583 Prec@1=61.988 Prec@5=84.165 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=03:18 IST
=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.475 DataTime=0.279 Loss=1.583 Prec@1=61.988 Prec@5=84.165 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=03:19 IST
=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.474 DataTime=0.277 Loss=1.584 Prec@1=61.937 Prec@5=84.137 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=03:19 IST
=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.474 DataTime=0.277 Loss=1.584 Prec@1=61.937 Prec@5=84.137 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=03:19 IST
=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.474 DataTime=0.277 Loss=1.584 Prec@1=61.937 Prec@5=84.137 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=03:19 IST
=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.586 Prec@1=61.923 Prec@5=84.117 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=03:19 IST
=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.586 Prec@1=61.923 Prec@5=84.117 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=03:19 IST
=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.586 Prec@1=61.923 Prec@5=84.117 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=03:20 IST
=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.587 Prec@1=61.902 Prec@5=84.088 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=03:20 IST
=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.587 Prec@1=61.902 Prec@5=84.088 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=03:20 IST
=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.587 Prec@1=61.902 Prec@5=84.088 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=03:21 IST
=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.589 Prec@1=61.870 Prec@5=84.065 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=03:21 IST
=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.589 Prec@1=61.870 Prec@5=84.065 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=03:21 IST
=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.589 Prec@1=61.870 Prec@5=84.065 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=03:22 IST
=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.591 Prec@1=61.848 Prec@5=84.015 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=03:22 IST
=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.591 Prec@1=61.848 Prec@5=84.015 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=03:22 IST
=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.591 Prec@1=61.848 Prec@5=84.015 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=03:22 IST
=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.592 Prec@1=61.833 Prec@5=84.005 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=03:22 IST
=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.592 Prec@1=61.833 Prec@5=84.005 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=03:22 IST
=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.592 Prec@1=61.833 Prec@5=84.005 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=03:23 IST
=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.275 Loss=1.593 Prec@1=61.797 Prec@5=83.987 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=03:23 IST
=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.275 Loss=1.593 Prec@1=61.797 Prec@5=83.987 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=03:23 IST
=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.275 Loss=1.593 Prec@1=61.797 Prec@5=83.987 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=03:24 IST
=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.274 Loss=1.595 Prec@1=61.766 Prec@5=83.969 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=03:24 IST
=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.274 Loss=1.595 Prec@1=61.766 Prec@5=83.969 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=03:24 IST
=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.471 DataTime=0.274 Loss=1.595 Prec@1=61.766 Prec@5=83.969 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=03:25 IST
=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.596 Prec@1=61.743 Prec@5=83.949 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=03:25 IST
=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.596 Prec@1=61.743 Prec@5=83.949 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=03:25 IST
=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.275 Loss=1.596 Prec@1=61.743 Prec@5=83.949 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=03:26 IST
=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.746 Prec@5=83.940 rate=2.13 Hz, eta=0:06:15, total=0:13:17, wall=03:26 IST
=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.746 Prec@5=83.940 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=03:26 IST
=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.596 Prec@1=61.746 Prec@5=83.940 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=03:26 IST
=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.597 Prec@1=61.731 Prec@5=83.917 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=03:26 IST
=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.597 Prec@1=61.731 Prec@5=83.917 rate=2.13 Hz, eta=0:04:43, total=0:14:53, wall=03:26 IST
=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.473 DataTime=0.277 Loss=1.597 Prec@1=61.731 Prec@5=83.917 rate=2.13 Hz, eta=0:04:43, total=0:14:53, wall=03:27 IST
=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.598 Prec@1=61.716 Prec@5=83.898 rate=2.13 Hz, eta=0:04:43, total=0:14:53, wall=03:27 IST
=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.598 Prec@1=61.716 Prec@5=83.898 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=03:27 IST
=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.598 Prec@1=61.716 Prec@5=83.898 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=03:28 IST
=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.698 Prec@5=83.883 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=03:28 IST
=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.698 Prec@5=83.883 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=03:28 IST
=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.698 Prec@5=83.883 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=03:29 IST
=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.668 Prec@5=83.881 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=03:29 IST
=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.668 Prec@5=83.881 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=03:29 IST
=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.600 Prec@1=61.668 Prec@5=83.881 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=03:30 IST
=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.601 Prec@1=61.657 Prec@5=83.865 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=03:30 IST
=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.601 Prec@1=61.657 Prec@5=83.865 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=03:30 IST
=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.601 Prec@1=61.657 Prec@5=83.865 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=03:30 IST
=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.657 Prec@5=83.863 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=03:30 IST
=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.657 Prec@5=83.863 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=03:30 IST
=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.657 Prec@5=83.863 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=03:31 IST
=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.660 Prec@5=83.860 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=03:31 IST
=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.660 Prec@5=83.860 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=03:31 IST
=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.660 Prec@5=83.860 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=03:31 IST
=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.660 Prec@5=83.860 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=03:31 IST
=> training   100.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.472 DataTime=0.276 Loss=1.602 Prec@1=61.660 Prec@5=83.860 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=03:31 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> validation 0.00% of 1x98...Epoch=35/150 LR=0.08785 Time=6.671 Loss=0.946 Prec@1=75.977 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.671 Loss=0.946 Prec@1=75.977 Prec@5=92.578 rate=4609.17 Hz, eta=0:00:00, total=0:00:00, wall=03:31 IST
** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.671 Loss=0.946 Prec@1=75.977 Prec@5=92.578 rate=4609.17 Hz, eta=0:00:00, total=0:00:00, wall=03:32 IST
** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=0.548 Loss=1.597 Prec@1=61.666 Prec@5=84.298 rate=4609.17 Hz, eta=0:00:00, total=0:00:00, wall=03:32 IST
** validation 100.00% of 1x98...Epoch=35/150 LR=0.08785 Time=0.548 Loss=1.597 Prec@1=61.666 Prec@5=84.298 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=03:32 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:32 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:32 IST
=> training   0.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.087 DataTime=4.847 Loss=1.449 Prec@1=65.430 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=03:32 IST
=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.087 DataTime=4.847 Loss=1.449 Prec@1=65.430 Prec@5=86.133 rate=7406.91 Hz, eta=0:00:00, total=0:00:00, wall=03:32 IST
=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.087 DataTime=4.847 Loss=1.449 Prec@1=65.430 Prec@5=86.133 rate=7406.91 Hz, eta=0:00:00, total=0:00:00, wall=03:33 IST
=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.509 DataTime=0.311 Loss=1.563 Prec@1=62.483 Prec@5=84.470 rate=7406.91 Hz, eta=0:00:00, total=0:00:00, wall=03:33 IST
=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.509 DataTime=0.311 Loss=1.563 Prec@1=62.483 Prec@5=84.470 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=03:33 IST
=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.509 DataTime=0.311 Loss=1.563 Prec@1=62.483 Prec@5=84.470 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=03:34 IST
=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.486 DataTime=0.287 Loss=1.568 Prec@1=62.488 Prec@5=84.344 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=03:34 IST
=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.486 DataTime=0.287 Loss=1.568 Prec@1=62.488 Prec@5=84.344 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=03:34 IST
=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.486 DataTime=0.287 Loss=1.568 Prec@1=62.488 Prec@5=84.344 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=03:34 IST
=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.476 DataTime=0.279 Loss=1.574 Prec@1=62.349 Prec@5=84.319 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=03:34 IST
=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.476 DataTime=0.279 Loss=1.574 Prec@1=62.349 Prec@5=84.319 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:34 IST
=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.476 DataTime=0.279 Loss=1.574 Prec@1=62.349 Prec@5=84.319 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:35 IST
=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.472 DataTime=0.274 Loss=1.575 Prec@1=62.306 Prec@5=84.289 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:35 IST
=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.472 DataTime=0.274 Loss=1.575 Prec@1=62.306 Prec@5=84.289 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:35 IST
=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.472 DataTime=0.274 Loss=1.575 Prec@1=62.306 Prec@5=84.289 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:36 IST
=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.575 Prec@1=62.300 Prec@5=84.256 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:36 IST
=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.575 Prec@1=62.300 Prec@5=84.256 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=03:36 IST
=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.575 Prec@1=62.300 Prec@5=84.256 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=03:37 IST
=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.264 Prec@5=84.179 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=03:37 IST
=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.264 Prec@5=84.179 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=03:37 IST
=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.264 Prec@5=84.179 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=03:38 IST
=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.576 Prec@1=62.261 Prec@5=84.211 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=03:38 IST
=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.576 Prec@1=62.261 Prec@5=84.211 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=03:38 IST
=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.576 Prec@1=62.261 Prec@5=84.211 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=03:38 IST
=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.273 Prec@5=84.185 rate=2.17 Hz, eta=0:13:49, total=0:05:22, wall=03:38 IST
=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.273 Prec@5=84.185 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=03:38 IST
=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.577 Prec@1=62.273 Prec@5=84.185 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=03:39 IST
=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.580 Prec@1=62.207 Prec@5=84.162 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=03:39 IST
=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.580 Prec@1=62.207 Prec@5=84.162 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=03:39 IST
=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.580 Prec@1=62.207 Prec@5=84.162 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=03:40 IST
=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.582 Prec@1=62.175 Prec@5=84.123 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=03:40 IST
=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.582 Prec@1=62.175 Prec@5=84.123 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=03:40 IST
=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.582 Prec@1=62.175 Prec@5=84.123 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=03:41 IST
=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.583 Prec@1=62.128 Prec@5=84.098 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=03:41 IST
=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.583 Prec@1=62.128 Prec@5=84.098 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=03:41 IST
=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.583 Prec@1=62.128 Prec@5=84.098 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=03:41 IST
=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.100 Prec@5=84.075 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=03:41 IST
=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.100 Prec@5=84.075 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=03:41 IST
=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.100 Prec@5=84.075 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=03:42 IST
=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.586 Prec@1=62.067 Prec@5=84.052 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=03:42 IST
=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.586 Prec@1=62.067 Prec@5=84.052 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=03:42 IST
=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.586 Prec@1=62.067 Prec@5=84.052 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=03:43 IST
=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.587 Prec@1=62.060 Prec@5=84.031 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=03:43 IST
=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.587 Prec@1=62.060 Prec@5=84.031 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=03:43 IST
=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.272 Loss=1.587 Prec@1=62.060 Prec@5=84.031 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=03:44 IST
=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.588 Prec@1=62.026 Prec@5=84.018 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=03:44 IST
=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.588 Prec@1=62.026 Prec@5=84.018 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=03:44 IST
=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.588 Prec@1=62.026 Prec@5=84.018 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=03:45 IST
=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.590 Prec@1=61.988 Prec@5=83.995 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=03:45 IST
=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.590 Prec@1=61.988 Prec@5=83.995 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=03:45 IST
=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.590 Prec@1=61.988 Prec@5=83.995 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=03:45 IST
=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.591 Prec@1=61.985 Prec@5=83.995 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=03:45 IST
=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.591 Prec@1=61.985 Prec@5=83.995 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=03:45 IST
=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.271 Loss=1.591 Prec@1=61.985 Prec@5=83.995 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=03:46 IST
=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.960 Prec@5=83.993 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=03:46 IST
=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.960 Prec@5=83.993 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=03:46 IST
=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.960 Prec@5=83.993 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=03:47 IST
=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.957 Prec@5=83.994 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=03:47 IST
=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.957 Prec@5=83.994 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=03:47 IST
=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.591 Prec@1=61.957 Prec@5=83.994 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=03:48 IST
=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.592 Prec@1=61.933 Prec@5=83.989 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=03:48 IST
=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.592 Prec@1=61.933 Prec@5=83.989 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=03:48 IST
=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.468 DataTime=0.272 Loss=1.592 Prec@1=61.933 Prec@5=83.989 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=03:49 IST
=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.593 Prec@1=61.910 Prec@5=83.970 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=03:49 IST
=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.593 Prec@1=61.910 Prec@5=83.970 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=03:49 IST
=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.593 Prec@1=61.910 Prec@5=83.970 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=03:49 IST
=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.594 Prec@1=61.882 Prec@5=83.957 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=03:49 IST
=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.594 Prec@1=61.882 Prec@5=83.957 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=03:49 IST
=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.469 DataTime=0.273 Loss=1.594 Prec@1=61.882 Prec@5=83.957 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=03:50 IST
=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.273 Loss=1.594 Prec@1=61.874 Prec@5=83.959 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=03:50 IST
=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.273 Loss=1.594 Prec@1=61.874 Prec@5=83.959 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=03:50 IST
=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.273 Loss=1.594 Prec@1=61.874 Prec@5=83.959 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=03:51 IST
=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.595 Prec@1=61.853 Prec@5=83.943 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=03:51 IST
=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.595 Prec@1=61.853 Prec@5=83.943 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=03:51 IST
=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.595 Prec@1=61.853 Prec@5=83.943 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=03:52 IST
=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.596 Prec@1=61.834 Prec@5=83.934 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=03:52 IST
=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.596 Prec@1=61.834 Prec@5=83.934 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=03:52 IST
=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.596 Prec@1=61.834 Prec@5=83.934 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=03:52 IST
=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.596 Prec@1=61.837 Prec@5=83.935 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=03:52 IST
=> training   100.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.470 DataTime=0.274 Loss=1.596 Prec@1=61.837 Prec@5=83.935 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=03:52 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> validation 0.00% of 1x98...Epoch=36/150 LR=0.08716 Time=6.240 Loss=0.998 Prec@1=77.539 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.240 Loss=0.998 Prec@1=77.539 Prec@5=92.383 rate=8443.17 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST
** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.240 Loss=0.998 Prec@1=77.539 Prec@5=92.383 rate=8443.17 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST
** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=0.539 Loss=1.604 Prec@1=61.594 Prec@5=84.310 rate=8443.17 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST
** validation 100.00% of 1x98...Epoch=36/150 LR=0.08716 Time=0.539 Loss=1.604 Prec@1=61.594 Prec@5=84.310 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=03:53 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:53 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:53 IST
=> training   0.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.010 DataTime=4.780 Loss=1.649 Prec@1=60.547 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=03:53 IST
=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.010 DataTime=4.780 Loss=1.649 Prec@1=60.547 Prec@5=83.984 rate=9826.95 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST
=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.010 DataTime=4.780 Loss=1.649 Prec@1=60.547 Prec@5=83.984 rate=9826.95 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST
=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.507 DataTime=0.314 Loss=1.542 Prec@1=62.943 Prec@5=84.735 rate=9826.95 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST
=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.507 DataTime=0.314 Loss=1.542 Prec@1=62.943 Prec@5=84.735 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=03:53 IST
=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.507 DataTime=0.314 Loss=1.542 Prec@1=62.943 Prec@5=84.735 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=03:54 IST
=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.483 DataTime=0.290 Loss=1.555 Prec@1=62.669 Prec@5=84.615 rate=2.19 Hz, eta=0:18:19, total=0:00:46, wall=03:54 IST
=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.483 DataTime=0.290 Loss=1.555 Prec@1=62.669 Prec@5=84.615 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=03:54 IST
=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.483 DataTime=0.290 Loss=1.555 Prec@1=62.669 Prec@5=84.615 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=03:55 IST
=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.476 DataTime=0.281 Loss=1.558 Prec@1=62.573 Prec@5=84.537 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=03:55 IST
=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.476 DataTime=0.281 Loss=1.558 Prec@1=62.573 Prec@5=84.537 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:55 IST
=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.476 DataTime=0.281 Loss=1.558 Prec@1=62.573 Prec@5=84.537 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:56 IST
=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.565 Prec@1=62.463 Prec@5=84.389 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=03:56 IST
=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.565 Prec@1=62.463 Prec@5=84.389 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:56 IST
=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.565 Prec@1=62.463 Prec@5=84.389 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:57 IST
=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.460 Prec@5=84.421 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=03:57 IST
=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.460 Prec@5=84.421 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=03:57 IST
=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.460 Prec@5=84.421 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=03:57 IST
=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.471 DataTime=0.276 Loss=1.566 Prec@1=62.472 Prec@5=84.401 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=03:57 IST
=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.471 DataTime=0.276 Loss=1.566 Prec@1=62.472 Prec@5=84.401 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=03:57 IST
=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.471 DataTime=0.276 Loss=1.566 Prec@1=62.472 Prec@5=84.401 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=03:58 IST
=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.470 Prec@5=84.386 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=03:58 IST
=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.470 Prec@5=84.386 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=03:58 IST
=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.470 Prec@5=84.386 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=03:59 IST
=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.412 Prec@5=84.372 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=03:59 IST
=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.412 Prec@5=84.372 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=03:59 IST
=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.412 Prec@5=84.372 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=04:00 IST
=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.570 Prec@1=62.386 Prec@5=84.337 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=04:00 IST
=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.570 Prec@1=62.386 Prec@5=84.337 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=04:00 IST
=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.570 Prec@1=62.386 Prec@5=84.337 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=04:00 IST
=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.571 Prec@1=62.367 Prec@5=84.311 rate=2.16 Hz, eta=0:12:23, total=0:06:58, wall=04:00 IST
=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.571 Prec@1=62.367 Prec@5=84.311 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=04:00 IST
=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.275 Loss=1.571 Prec@1=62.367 Prec@5=84.311 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=04:01 IST
=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.573 Prec@1=62.339 Prec@5=84.273 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=04:01 IST
=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.573 Prec@1=62.339 Prec@5=84.273 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=04:01 IST
=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.470 DataTime=0.274 Loss=1.573 Prec@1=62.339 Prec@5=84.273 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=04:02 IST
=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.574 Prec@1=62.315 Prec@5=84.267 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=04:02 IST
=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.574 Prec@1=62.315 Prec@5=84.267 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=04:02 IST
=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.574 Prec@1=62.315 Prec@5=84.267 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=04:03 IST
=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.575 Prec@1=62.286 Prec@5=84.254 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=04:03 IST
=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.575 Prec@1=62.286 Prec@5=84.254 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=04:03 IST
=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.575 Prec@1=62.286 Prec@5=84.254 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=04:04 IST
=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.578 Prec@1=62.218 Prec@5=84.225 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=04:04 IST
=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.578 Prec@1=62.218 Prec@5=84.225 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=04:04 IST
=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.578 Prec@1=62.218 Prec@5=84.225 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=04:04 IST
=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.580 Prec@1=62.190 Prec@5=84.201 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=04:04 IST
=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.580 Prec@1=62.190 Prec@5=84.201 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=04:04 IST
=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.580 Prec@1=62.190 Prec@5=84.201 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=04:05 IST
=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.468 DataTime=0.273 Loss=1.581 Prec@1=62.169 Prec@5=84.172 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=04:05 IST
=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.468 DataTime=0.273 Loss=1.581 Prec@1=62.169 Prec@5=84.172 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=04:05 IST
=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.468 DataTime=0.273 Loss=1.581 Prec@1=62.169 Prec@5=84.172 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=04:06 IST
=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.148 Prec@5=84.146 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=04:06 IST
=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.148 Prec@5=84.146 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:06 IST
=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.148 Prec@5=84.146 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:07 IST
=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.584 Prec@1=62.108 Prec@5=84.114 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:07 IST
=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.584 Prec@1=62.108 Prec@5=84.114 rate=2.15 Hz, eta=0:05:27, total=0:13:58, wall=04:07 IST
=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.273 Loss=1.584 Prec@1=62.108 Prec@5=84.114 rate=2.15 Hz, eta=0:05:27, total=0:13:58, wall=04:07 IST
=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.585 Prec@1=62.090 Prec@5=84.095 rate=2.15 Hz, eta=0:05:27, total=0:13:58, wall=04:07 IST
=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.585 Prec@1=62.090 Prec@5=84.095 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:07 IST
=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.585 Prec@1=62.090 Prec@5=84.095 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:08 IST
=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.059 Prec@5=84.067 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:08 IST
=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.059 Prec@5=84.067 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=04:08 IST
=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.059 Prec@5=84.067 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=04:09 IST
=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.050 Prec@5=84.066 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=04:09 IST
=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.050 Prec@5=84.066 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:09 IST
=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.050 Prec@5=84.066 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:10 IST
=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.045 Prec@5=84.066 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:10 IST
=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.045 Prec@5=84.066 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:10 IST
=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.586 Prec@1=62.045 Prec@5=84.066 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:11 IST
=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.587 Prec@1=62.020 Prec@5=84.056 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:11 IST
=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.587 Prec@1=62.020 Prec@5=84.056 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:11 IST
=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.587 Prec@1=62.020 Prec@5=84.056 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:11 IST
=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.588 Prec@1=62.001 Prec@5=84.041 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:11 IST
=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.588 Prec@1=62.001 Prec@5=84.041 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=04:11 IST
=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.588 Prec@1=62.001 Prec@5=84.041 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=04:12 IST
=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.589 Prec@1=61.993 Prec@5=84.032 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=04:12 IST
=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.589 Prec@1=61.993 Prec@5=84.032 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=04:12 IST
=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.589 Prec@1=61.993 Prec@5=84.032 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=04:12 IST
=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.589 Prec@1=61.993 Prec@5=84.031 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=04:12 IST
=> training   100.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.469 DataTime=0.274 Loss=1.589 Prec@1=61.993 Prec@5=84.031 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=04:12 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:12 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:12 IST
=> validation 0.00% of 1x98...Epoch=37/150 LR=0.08645 Time=6.735 Loss=0.978 Prec@1=73.242 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=04:12 IST
=> validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=6.735 Loss=0.978 Prec@1=73.242 Prec@5=92.773 rate=6922.86 Hz, eta=0:00:00, total=0:00:00, wall=04:12 IST
** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=6.735 Loss=0.978 Prec@1=73.242 Prec@5=92.773 rate=6922.86 Hz, eta=0:00:00, total=0:00:00, wall=04:13 IST
** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=0.544 Loss=1.592 Prec@1=61.438 Prec@5=84.376 rate=6922.86 Hz, eta=0:00:00, total=0:00:00, wall=04:13 IST
** validation 100.00% of 1x98...Epoch=37/150 LR=0.08645 Time=0.544 Loss=1.592 Prec@1=61.438 Prec@5=84.376 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=04:13 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:13 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:13 IST
=> training   0.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=4.626 DataTime=4.387 Loss=1.562 Prec@1=62.891 Prec@5=85.352 rate=0 Hz, eta=?, total=0:00:00, wall=04:13 IST
=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=4.626 DataTime=4.387 Loss=1.562 Prec@1=62.891 Prec@5=85.352 rate=7034.77 Hz, eta=0:00:00, total=0:00:00, wall=04:13 IST
=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=4.626 DataTime=4.387 Loss=1.562 Prec@1=62.891 Prec@5=85.352 rate=7034.77 Hz, eta=0:00:00, total=0:00:00, wall=04:14 IST
=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.503 DataTime=0.309 Loss=1.557 Prec@1=62.312 Prec@5=84.435 rate=7034.77 Hz, eta=0:00:00, total=0:00:00, wall=04:14 IST
=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.503 DataTime=0.309 Loss=1.557 Prec@1=62.312 Prec@5=84.435 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=04:14 IST
=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.503 DataTime=0.309 Loss=1.557 Prec@1=62.312 Prec@5=84.435 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=04:15 IST
=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.486 DataTime=0.291 Loss=1.552 Prec@1=62.626 Prec@5=84.546 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=04:15 IST
=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.486 DataTime=0.291 Loss=1.552 Prec@1=62.626 Prec@5=84.546 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=04:15 IST
=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.486 DataTime=0.291 Loss=1.552 Prec@1=62.626 Prec@5=84.546 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=04:15 IST
=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.477 DataTime=0.281 Loss=1.554 Prec@1=62.501 Prec@5=84.569 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=04:15 IST
=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.477 DataTime=0.281 Loss=1.554 Prec@1=62.501 Prec@5=84.569 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=04:15 IST
=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.477 DataTime=0.281 Loss=1.554 Prec@1=62.501 Prec@5=84.569 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=04:16 IST
=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.471 DataTime=0.276 Loss=1.557 Prec@1=62.451 Prec@5=84.560 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=04:16 IST
=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.471 DataTime=0.276 Loss=1.557 Prec@1=62.451 Prec@5=84.560 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=04:16 IST
=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.471 DataTime=0.276 Loss=1.557 Prec@1=62.451 Prec@5=84.560 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=04:17 IST
=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.470 DataTime=0.274 Loss=1.559 Prec@1=62.438 Prec@5=84.507 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=04:17 IST
=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.470 DataTime=0.274 Loss=1.559 Prec@1=62.438 Prec@5=84.507 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=04:17 IST
=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.470 DataTime=0.274 Loss=1.559 Prec@1=62.438 Prec@5=84.507 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=04:18 IST
=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.562 Prec@1=62.409 Prec@5=84.454 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=04:18 IST
=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.562 Prec@1=62.409 Prec@5=84.454 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=04:18 IST
=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.562 Prec@1=62.409 Prec@5=84.454 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=04:19 IST
=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.564 Prec@1=62.393 Prec@5=84.400 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=04:19 IST
=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.564 Prec@1=62.393 Prec@5=84.400 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=04:19 IST
=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.564 Prec@1=62.393 Prec@5=84.400 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=04:19 IST
=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.566 Prec@1=62.364 Prec@5=84.384 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=04:19 IST
=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.566 Prec@1=62.364 Prec@5=84.384 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=04:19 IST
=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.566 Prec@1=62.364 Prec@5=84.384 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=04:20 IST
=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.566 Prec@1=62.328 Prec@5=84.380 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=04:20 IST
=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.566 Prec@1=62.328 Prec@5=84.380 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=04:20 IST
=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.566 Prec@1=62.328 Prec@5=84.380 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=04:21 IST
=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.569 Prec@1=62.276 Prec@5=84.331 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=04:21 IST
=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.569 Prec@1=62.276 Prec@5=84.331 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=04:21 IST
=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.569 Prec@1=62.276 Prec@5=84.331 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=04:22 IST
=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.570 Prec@1=62.274 Prec@5=84.312 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=04:22 IST
=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.570 Prec@1=62.274 Prec@5=84.312 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=04:22 IST
=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.570 Prec@1=62.274 Prec@5=84.312 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=04:22 IST
=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.570 Prec@1=62.284 Prec@5=84.324 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=04:22 IST
=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.570 Prec@1=62.284 Prec@5=84.324 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=04:22 IST
=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.570 Prec@1=62.284 Prec@5=84.324 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=04:23 IST
=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.571 Prec@1=62.249 Prec@5=84.305 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=04:23 IST
=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.571 Prec@1=62.249 Prec@5=84.305 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=04:23 IST
=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.571 Prec@1=62.249 Prec@5=84.305 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=04:24 IST
=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.573 Prec@1=62.231 Prec@5=84.289 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=04:24 IST
=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.573 Prec@1=62.231 Prec@5=84.289 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=04:24 IST
=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.573 Prec@1=62.231 Prec@5=84.289 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=04:25 IST
=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.574 Prec@1=62.209 Prec@5=84.254 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=04:25 IST
=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.574 Prec@1=62.209 Prec@5=84.254 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=04:25 IST
=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.574 Prec@1=62.209 Prec@5=84.254 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=04:26 IST
=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.203 Prec@5=84.240 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=04:26 IST
=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.203 Prec@5=84.240 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=04:26 IST
=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.203 Prec@5=84.240 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=04:26 IST
=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.186 Prec@5=84.209 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=04:26 IST
=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.186 Prec@5=84.209 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=04:26 IST
=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.186 Prec@5=84.209 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=04:27 IST
=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.579 Prec@1=62.159 Prec@5=84.172 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=04:27 IST
=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.579 Prec@1=62.159 Prec@5=84.172 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=04:27 IST
=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.272 Loss=1.579 Prec@1=62.159 Prec@5=84.172 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=04:28 IST
=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.579 Prec@1=62.155 Prec@5=84.169 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=04:28 IST
=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.579 Prec@1=62.155 Prec@5=84.169 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=04:28 IST
=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.467 DataTime=0.271 Loss=1.579 Prec@1=62.155 Prec@5=84.169 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=04:29 IST
=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.580 Prec@1=62.148 Prec@5=84.158 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=04:29 IST
=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.580 Prec@1=62.148 Prec@5=84.158 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=04:29 IST
=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.580 Prec@1=62.148 Prec@5=84.158 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=04:30 IST
=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.581 Prec@1=62.119 Prec@5=84.146 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=04:30 IST
=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.581 Prec@1=62.119 Prec@5=84.146 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:30 IST
=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.581 Prec@1=62.119 Prec@5=84.146 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:30 IST
=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.098 Prec@5=84.139 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=04:30 IST
=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.098 Prec@5=84.139 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:30 IST
=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.098 Prec@5=84.139 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:31 IST
=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.092 Prec@5=84.138 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=04:31 IST
=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.092 Prec@5=84.138 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:31 IST
=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.469 DataTime=0.273 Loss=1.582 Prec@1=62.092 Prec@5=84.138 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:32 IST
=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.583 Prec@1=62.080 Prec@5=84.121 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=04:32 IST
=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.583 Prec@1=62.080 Prec@5=84.121 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=04:32 IST
=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.273 Loss=1.583 Prec@1=62.080 Prec@5=84.121 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=04:33 IST
=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.078 Prec@5=84.108 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=04:33 IST
=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.078 Prec@5=84.108 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=04:33 IST
=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.078 Prec@5=84.108 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=04:33 IST
=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.076 Prec@5=84.107 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=04:33 IST
=> training   100.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.468 DataTime=0.272 Loss=1.584 Prec@1=62.076 Prec@5=84.107 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=04:33 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST
=> validation 0.00% of 1x98...Epoch=38/150 LR=0.08572 Time=7.042 Loss=1.068 Prec@1=71.094 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST
=> validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=7.042 Loss=1.068 Prec@1=71.094 Prec@5=92.773 rate=7818.61 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST
** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=7.042 Loss=1.068 Prec@1=71.094 Prec@5=92.773 rate=7818.61 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST
** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=0.553 Loss=1.619 Prec@1=61.158 Prec@5=84.004 rate=7818.61 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST
** validation 100.00% of 1x98...Epoch=38/150 LR=0.08572 Time=0.553 Loss=1.619 Prec@1=61.158 Prec@5=84.004 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=04:34 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:34 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:34 IST
=> training   0.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=4.840 DataTime=4.592 Loss=1.479 Prec@1=63.477 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=04:34 IST
=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=4.840 DataTime=4.592 Loss=1.479 Prec@1=63.477 Prec@5=85.938 rate=6161.69 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST
=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=4.840 DataTime=4.592 Loss=1.479 Prec@1=63.477 Prec@5=85.938 rate=6161.69 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST
=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.501 DataTime=0.308 Loss=1.545 Prec@1=62.910 Prec@5=84.799 rate=6161.69 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST
=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.501 DataTime=0.308 Loss=1.545 Prec@1=62.910 Prec@5=84.799 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=04:34 IST
=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.501 DataTime=0.308 Loss=1.545 Prec@1=62.910 Prec@5=84.799 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=04:35 IST
=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.484 DataTime=0.290 Loss=1.543 Prec@1=62.820 Prec@5=84.890 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=04:35 IST
=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.484 DataTime=0.290 Loss=1.543 Prec@1=62.820 Prec@5=84.890 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:35 IST
=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.484 DataTime=0.290 Loss=1.543 Prec@1=62.820 Prec@5=84.890 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:36 IST
=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.546 Prec@1=62.730 Prec@5=84.770 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:36 IST
=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.546 Prec@1=62.730 Prec@5=84.770 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=04:36 IST
=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.546 Prec@1=62.730 Prec@5=84.770 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=04:37 IST
=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.550 Prec@1=62.678 Prec@5=84.671 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=04:37 IST
=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.550 Prec@1=62.678 Prec@5=84.671 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=04:37 IST
=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.550 Prec@1=62.678 Prec@5=84.671 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=04:38 IST
=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.282 Loss=1.550 Prec@1=62.693 Prec@5=84.646 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=04:38 IST
=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.282 Loss=1.550 Prec@1=62.693 Prec@5=84.646 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=04:38 IST
=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.282 Loss=1.550 Prec@1=62.693 Prec@5=84.646 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=04:38 IST
=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.551 Prec@1=62.687 Prec@5=84.606 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=04:38 IST
=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.551 Prec@1=62.687 Prec@5=84.606 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=04:38 IST
=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.477 DataTime=0.282 Loss=1.551 Prec@1=62.687 Prec@5=84.606 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=04:39 IST
=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.553 Prec@1=62.637 Prec@5=84.592 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=04:39 IST
=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.553 Prec@1=62.637 Prec@5=84.592 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=04:39 IST
=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.478 DataTime=0.283 Loss=1.553 Prec@1=62.637 Prec@5=84.592 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=04:40 IST
=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.476 DataTime=0.280 Loss=1.556 Prec@1=62.561 Prec@5=84.551 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=04:40 IST
=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.476 DataTime=0.280 Loss=1.556 Prec@1=62.561 Prec@5=84.551 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=04:40 IST
=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.476 DataTime=0.280 Loss=1.556 Prec@1=62.561 Prec@5=84.551 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=04:41 IST
=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.475 DataTime=0.280 Loss=1.556 Prec@1=62.548 Prec@5=84.554 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=04:41 IST
=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.475 DataTime=0.280 Loss=1.556 Prec@1=62.548 Prec@5=84.554 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=04:41 IST
=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.475 DataTime=0.280 Loss=1.556 Prec@1=62.548 Prec@5=84.554 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=04:41 IST
=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.474 DataTime=0.278 Loss=1.558 Prec@1=62.509 Prec@5=84.531 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=04:41 IST
=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.474 DataTime=0.278 Loss=1.558 Prec@1=62.509 Prec@5=84.531 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=04:41 IST
=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.474 DataTime=0.278 Loss=1.558 Prec@1=62.509 Prec@5=84.531 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=04:42 IST
=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.278 Loss=1.560 Prec@1=62.477 Prec@5=84.499 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=04:42 IST
=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.278 Loss=1.560 Prec@1=62.477 Prec@5=84.499 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=04:42 IST
=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.278 Loss=1.560 Prec@1=62.477 Prec@5=84.499 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=04:43 IST
=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.277 Loss=1.563 Prec@1=62.437 Prec@5=84.456 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=04:43 IST
=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.277 Loss=1.563 Prec@1=62.437 Prec@5=84.456 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=04:43 IST
=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.473 DataTime=0.277 Loss=1.563 Prec@1=62.437 Prec@5=84.456 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=04:44 IST
=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.400 Prec@5=84.436 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=04:44 IST
=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.400 Prec@5=84.436 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=04:44 IST
=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.472 DataTime=0.276 Loss=1.564 Prec@1=62.400 Prec@5=84.436 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=04:45 IST
=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.378 Prec@5=84.402 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=04:45 IST
=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.378 Prec@5=84.402 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=04:45 IST
=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.378 Prec@5=84.402 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=04:45 IST
=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.377 Prec@5=84.398 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=04:45 IST
=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.377 Prec@5=84.398 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=04:45 IST
=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.275 Loss=1.566 Prec@1=62.377 Prec@5=84.398 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=04:46 IST
=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.338 Prec@5=84.376 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=04:46 IST
=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.338 Prec@5=84.376 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=04:46 IST
=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.470 DataTime=0.274 Loss=1.568 Prec@1=62.338 Prec@5=84.376 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=04:47 IST
=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.568 Prec@1=62.327 Prec@5=84.382 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=04:47 IST
=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.568 Prec@1=62.327 Prec@5=84.382 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:47 IST
=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.568 Prec@1=62.327 Prec@5=84.382 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:48 IST
=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.569 Prec@1=62.300 Prec@5=84.371 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=04:48 IST
=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.569 Prec@1=62.300 Prec@5=84.371 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=04:48 IST
=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.569 Prec@1=62.300 Prec@5=84.371 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=04:48 IST
=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.571 Prec@1=62.259 Prec@5=84.340 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=04:48 IST
=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.571 Prec@1=62.259 Prec@5=84.340 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:48 IST
=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.469 DataTime=0.273 Loss=1.571 Prec@1=62.259 Prec@5=84.340 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:49 IST
=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.572 Prec@1=62.227 Prec@5=84.317 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=04:49 IST
=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.572 Prec@1=62.227 Prec@5=84.317 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=04:49 IST
=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.572 Prec@1=62.227 Prec@5=84.317 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=04:50 IST
=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.573 Prec@1=62.201 Prec@5=84.305 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=04:50 IST
=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.573 Prec@1=62.201 Prec@5=84.305 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=04:50 IST
=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.273 Loss=1.573 Prec@1=62.201 Prec@5=84.305 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=04:51 IST
=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.574 Prec@1=62.202 Prec@5=84.289 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=04:51 IST
=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.574 Prec@1=62.202 Prec@5=84.289 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=04:51 IST
=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.574 Prec@1=62.202 Prec@5=84.289 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=04:51 IST
=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.187 Prec@5=84.270 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=04:51 IST
=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.187 Prec@5=84.270 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=04:51 IST
=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.468 DataTime=0.272 Loss=1.575 Prec@1=62.187 Prec@5=84.270 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=04:52 IST
=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.576 Prec@1=62.172 Prec@5=84.261 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=04:52 IST
=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.576 Prec@1=62.172 Prec@5=84.261 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=04:52 IST
=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.576 Prec@1=62.172 Prec@5=84.261 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=04:53 IST
=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.169 Prec@5=84.253 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=04:53 IST
=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.169 Prec@5=84.253 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=04:53 IST
=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.169 Prec@5=84.253 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=04:53 IST
=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.168 Prec@5=84.252 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=04:53 IST
=> training   100.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.467 DataTime=0.272 Loss=1.577 Prec@1=62.168 Prec@5=84.252 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=04:53 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> validation 0.00% of 1x98...Epoch=39/150 LR=0.08498 Time=7.086 Loss=1.014 Prec@1=72.852 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.086 Loss=1.014 Prec@1=72.852 Prec@5=92.188 rate=7803.17 Hz, eta=0:00:00, total=0:00:00, wall=04:53 IST
** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.086 Loss=1.014 Prec@1=72.852 Prec@5=92.188 rate=7803.17 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST
** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=0.552 Loss=1.600 Prec@1=61.424 Prec@5=84.340 rate=7803.17 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST
** validation 100.00% of 1x98...Epoch=39/150 LR=0.08498 Time=0.552 Loss=1.600 Prec@1=61.424 Prec@5=84.340 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=04:54 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> training   0.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.789 DataTime=4.372 Loss=1.534 Prec@1=60.156 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.789 DataTime=4.372 Loss=1.534 Prec@1=60.156 Prec@5=86.133 rate=8398.35 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST
=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.789 DataTime=4.372 Loss=1.534 Prec@1=60.156 Prec@5=86.133 rate=8398.35 Hz, eta=0:00:00, total=0:00:00, wall=04:55 IST
=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.509 DataTime=0.315 Loss=1.544 Prec@1=62.947 Prec@5=84.688 rate=8398.35 Hz, eta=0:00:00, total=0:00:00, wall=04:55 IST
=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.509 DataTime=0.315 Loss=1.544 Prec@1=62.947 Prec@5=84.688 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=04:55 IST
=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.509 DataTime=0.315 Loss=1.544 Prec@1=62.947 Prec@5=84.688 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=04:56 IST
=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.483 DataTime=0.288 Loss=1.540 Prec@1=63.045 Prec@5=84.810 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=04:56 IST
=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.483 DataTime=0.288 Loss=1.540 Prec@1=63.045 Prec@5=84.810 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:56 IST
=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.483 DataTime=0.288 Loss=1.540 Prec@1=63.045 Prec@5=84.810 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:56 IST
=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.480 DataTime=0.285 Loss=1.539 Prec@1=62.956 Prec@5=84.789 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=04:56 IST
=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.480 DataTime=0.285 Loss=1.539 Prec@1=62.956 Prec@5=84.789 rate=2.15 Hz, eta=0:17:01, total=0:02:19, wall=04:56 IST
=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.480 DataTime=0.285 Loss=1.539 Prec@1=62.956 Prec@5=84.789 rate=2.15 Hz, eta=0:17:01, total=0:02:19, wall=04:57 IST
=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.478 DataTime=0.283 Loss=1.542 Prec@1=62.871 Prec@5=84.765 rate=2.15 Hz, eta=0:17:01, total=0:02:19, wall=04:57 IST
=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.478 DataTime=0.283 Loss=1.542 Prec@1=62.871 Prec@5=84.765 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=04:57 IST
=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.478 DataTime=0.283 Loss=1.542 Prec@1=62.871 Prec@5=84.765 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=04:58 IST
=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.545 Prec@1=62.811 Prec@5=84.711 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=04:58 IST
=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.545 Prec@1=62.811 Prec@5=84.711 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=04:58 IST
=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.545 Prec@1=62.811 Prec@5=84.711 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=04:59 IST
=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.549 Prec@1=62.740 Prec@5=84.637 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=04:59 IST
=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.549 Prec@1=62.740 Prec@5=84.637 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=04:59 IST
=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.475 DataTime=0.280 Loss=1.549 Prec@1=62.740 Prec@5=84.637 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=05:00 IST
=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.474 DataTime=0.279 Loss=1.551 Prec@1=62.691 Prec@5=84.601 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=05:00 IST
=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.474 DataTime=0.279 Loss=1.551 Prec@1=62.691 Prec@5=84.601 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=05:00 IST
=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.474 DataTime=0.279 Loss=1.551 Prec@1=62.691 Prec@5=84.601 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=05:00 IST
=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.277 Loss=1.553 Prec@1=62.630 Prec@5=84.576 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=05:00 IST
=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.277 Loss=1.553 Prec@1=62.630 Prec@5=84.576 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=05:00 IST
=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.277 Loss=1.553 Prec@1=62.630 Prec@5=84.576 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=05:01 IST
=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.278 Loss=1.556 Prec@1=62.579 Prec@5=84.541 rate=2.14 Hz, eta=0:13:14, total=0:06:13, wall=05:01 IST
=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.278 Loss=1.556 Prec@1=62.579 Prec@5=84.541 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=05:01 IST
=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.473 DataTime=0.278 Loss=1.556 Prec@1=62.579 Prec@5=84.541 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=05:02 IST
=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.558 Prec@1=62.528 Prec@5=84.534 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=05:02 IST
=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.558 Prec@1=62.528 Prec@5=84.534 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=05:02 IST
=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.558 Prec@1=62.528 Prec@5=84.534 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=05:03 IST
=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.559 Prec@1=62.515 Prec@5=84.529 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=05:03 IST
=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.559 Prec@1=62.515 Prec@5=84.529 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=05:03 IST
=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.277 Loss=1.559 Prec@1=62.515 Prec@5=84.529 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=05:03 IST
=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.276 Loss=1.560 Prec@1=62.471 Prec@5=84.504 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=05:03 IST
=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.276 Loss=1.560 Prec@1=62.471 Prec@5=84.504 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=05:03 IST
=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.472 DataTime=0.276 Loss=1.560 Prec@1=62.471 Prec@5=84.504 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=05:04 IST
=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.561 Prec@1=62.464 Prec@5=84.480 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=05:04 IST
=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.561 Prec@1=62.464 Prec@5=84.480 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=05:04 IST
=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.561 Prec@1=62.464 Prec@5=84.480 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=05:05 IST
=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.563 Prec@1=62.438 Prec@5=84.440 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=05:05 IST
=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.563 Prec@1=62.438 Prec@5=84.440 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=05:05 IST
=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.563 Prec@1=62.438 Prec@5=84.440 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=05:06 IST
=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.564 Prec@1=62.423 Prec@5=84.426 rate=2.14 Hz, eta=0:08:35, total=0:10:54, wall=05:06 IST
=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.564 Prec@1=62.423 Prec@5=84.426 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=05:06 IST
=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.564 Prec@1=62.423 Prec@5=84.426 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=05:07 IST
=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.416 Prec@5=84.416 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=05:07 IST
=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.416 Prec@5=84.416 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=05:07 IST
=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.416 Prec@5=84.416 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=05:07 IST
=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.401 Prec@5=84.403 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=05:07 IST
=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.401 Prec@5=84.403 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=05:07 IST
=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.471 DataTime=0.276 Loss=1.565 Prec@1=62.401 Prec@5=84.403 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=05:08 IST
=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.566 Prec@1=62.390 Prec@5=84.396 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=05:08 IST
=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.566 Prec@1=62.390 Prec@5=84.396 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=05:08 IST
=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.566 Prec@1=62.390 Prec@5=84.396 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=05:09 IST
=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.374 Prec@5=84.373 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=05:09 IST
=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.374 Prec@5=84.373 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=05:09 IST
=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.567 Prec@1=62.374 Prec@5=84.373 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=05:10 IST
=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.568 Prec@1=62.362 Prec@5=84.364 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=05:10 IST
=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.568 Prec@1=62.362 Prec@5=84.364 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=05:10 IST
=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.568 Prec@1=62.362 Prec@5=84.364 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=05:10 IST
=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.569 Prec@1=62.344 Prec@5=84.341 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=05:10 IST
=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.569 Prec@1=62.344 Prec@5=84.341 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=05:10 IST
=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.569 Prec@1=62.344 Prec@5=84.341 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=05:11 IST
=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.570 Prec@1=62.326 Prec@5=84.334 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=05:11 IST
=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.570 Prec@1=62.326 Prec@5=84.334 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=05:11 IST
=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.275 Loss=1.570 Prec@1=62.326 Prec@5=84.334 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=05:12 IST
=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.571 Prec@1=62.302 Prec@5=84.310 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=05:12 IST
=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.571 Prec@1=62.302 Prec@5=84.310 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=05:12 IST
=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.571 Prec@1=62.302 Prec@5=84.310 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=05:13 IST
=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.298 Prec@5=84.291 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=05:13 IST
=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.298 Prec@5=84.291 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=05:13 IST
=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.298 Prec@5=84.291 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=05:14 IST
=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.302 Prec@5=84.296 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=05:14 IST
=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.302 Prec@5=84.296 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=05:14 IST
=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.302 Prec@5=84.296 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=05:14 IST
=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.301 Prec@5=84.295 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=05:14 IST
=> training   100.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.470 DataTime=0.276 Loss=1.572 Prec@1=62.301 Prec@5=84.295 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=05:14 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:14 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:14 IST
=> validation 0.00% of 1x98...Epoch=40/150 LR=0.08423 Time=6.782 Loss=1.016 Prec@1=74.414 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=05:14 IST
=> validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=6.782 Loss=1.016 Prec@1=74.414 Prec@5=92.188 rate=7386.67 Hz, eta=0:00:00, total=0:00:00, wall=05:14 IST
** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=6.782 Loss=1.016 Prec@1=74.414 Prec@5=92.188 rate=7386.67 Hz, eta=0:00:00, total=0:00:00, wall=05:15 IST
** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=0.551 Loss=1.589 Prec@1=61.480 Prec@5=84.274 rate=7386.67 Hz, eta=0:00:00, total=0:00:00, wall=05:15 IST
** validation 100.00% of 1x98...Epoch=40/150 LR=0.08423 Time=0.551 Loss=1.589 Prec@1=61.480 Prec@5=84.274 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=05:15 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:15 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:15 IST
=> training   0.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.691 DataTime=4.418 Loss=1.381 Prec@1=66.016 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=05:15 IST
=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.691 DataTime=4.418 Loss=1.381 Prec@1=66.016 Prec@5=87.695 rate=6301.48 Hz, eta=0:00:00, total=0:00:00, wall=05:15 IST
=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.691 DataTime=4.418 Loss=1.381 Prec@1=66.016 Prec@5=87.695 rate=6301.48 Hz, eta=0:00:00, total=0:00:00, wall=05:15 IST
=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.495 DataTime=0.300 Loss=1.531 Prec@1=63.264 Prec@5=84.920 rate=6301.48 Hz, eta=0:00:00, total=0:00:00, wall=05:15 IST
=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.495 DataTime=0.300 Loss=1.531 Prec@1=63.264 Prec@5=84.920 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=05:15 IST
=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.495 DataTime=0.300 Loss=1.531 Prec@1=63.264 Prec@5=84.920 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=05:16 IST
=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.485 DataTime=0.289 Loss=1.525 Prec@1=63.238 Prec@5=84.961 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=05:16 IST
=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.485 DataTime=0.289 Loss=1.525 Prec@1=63.238 Prec@5=84.961 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=05:16 IST
=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.485 DataTime=0.289 Loss=1.525 Prec@1=63.238 Prec@5=84.961 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=05:17 IST
=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.475 DataTime=0.279 Loss=1.524 Prec@1=63.367 Prec@5=85.004 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=05:17 IST
=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.475 DataTime=0.279 Loss=1.524 Prec@1=63.367 Prec@5=85.004 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=05:17 IST
=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.475 DataTime=0.279 Loss=1.524 Prec@1=63.367 Prec@5=85.004 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=05:18 IST
=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.472 DataTime=0.277 Loss=1.527 Prec@1=63.289 Prec@5=84.948 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=05:18 IST
=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.472 DataTime=0.277 Loss=1.527 Prec@1=63.289 Prec@5=84.948 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=05:18 IST
=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.472 DataTime=0.277 Loss=1.527 Prec@1=63.289 Prec@5=84.948 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=05:18 IST
=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.471 DataTime=0.275 Loss=1.529 Prec@1=63.211 Prec@5=84.894 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=05:18 IST
=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.471 DataTime=0.275 Loss=1.529 Prec@1=63.211 Prec@5=84.894 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=05:18 IST
=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.471 DataTime=0.275 Loss=1.529 Prec@1=63.211 Prec@5=84.894 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=05:19 IST
=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.137 Prec@5=84.813 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=05:19 IST
=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.137 Prec@5=84.813 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=05:19 IST
=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.137 Prec@5=84.813 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=05:20 IST
=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.273 Loss=1.536 Prec@1=63.112 Prec@5=84.772 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=05:20 IST
=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.273 Loss=1.536 Prec@1=63.112 Prec@5=84.772 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=05:20 IST
=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.273 Loss=1.536 Prec@1=63.112 Prec@5=84.772 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=05:21 IST
=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.539 Prec@1=63.030 Prec@5=84.735 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=05:21 IST
=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.539 Prec@1=63.030 Prec@5=84.735 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=05:21 IST
=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.539 Prec@1=63.030 Prec@5=84.735 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=05:22 IST
=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.541 Prec@1=62.986 Prec@5=84.673 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=05:22 IST
=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.541 Prec@1=62.986 Prec@5=84.673 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:22 IST
=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.541 Prec@1=62.986 Prec@5=84.673 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:22 IST
=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.542 Prec@1=62.966 Prec@5=84.680 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:22 IST
=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.542 Prec@1=62.966 Prec@5=84.680 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=05:22 IST
=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.542 Prec@1=62.966 Prec@5=84.680 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=05:23 IST
=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.274 Loss=1.545 Prec@1=62.905 Prec@5=84.648 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=05:23 IST
=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.274 Loss=1.545 Prec@1=62.905 Prec@5=84.648 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=05:23 IST
=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.470 DataTime=0.274 Loss=1.545 Prec@1=62.905 Prec@5=84.648 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=05:24 IST
=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.546 Prec@1=62.857 Prec@5=84.627 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=05:24 IST
=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.546 Prec@1=62.857 Prec@5=84.627 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=05:24 IST
=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.546 Prec@1=62.857 Prec@5=84.627 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=05:25 IST
=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.549 Prec@1=62.826 Prec@5=84.597 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=05:25 IST
=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.549 Prec@1=62.826 Prec@5=84.597 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=05:25 IST
=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.549 Prec@1=62.826 Prec@5=84.597 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=05:25 IST
=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.550 Prec@1=62.815 Prec@5=84.580 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=05:25 IST
=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.550 Prec@1=62.815 Prec@5=84.580 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=05:25 IST
=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.550 Prec@1=62.815 Prec@5=84.580 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=05:26 IST
=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.551 Prec@1=62.791 Prec@5=84.563 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=05:26 IST
=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.551 Prec@1=62.791 Prec@5=84.563 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=05:26 IST
=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.551 Prec@1=62.791 Prec@5=84.563 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=05:27 IST
=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.552 Prec@1=62.761 Prec@5=84.553 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=05:27 IST
=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.552 Prec@1=62.761 Prec@5=84.553 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=05:27 IST
=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.552 Prec@1=62.761 Prec@5=84.553 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=05:28 IST
=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.553 Prec@1=62.736 Prec@5=84.546 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=05:28 IST
=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.553 Prec@1=62.736 Prec@5=84.546 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=05:28 IST
=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.553 Prec@1=62.736 Prec@5=84.546 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=05:29 IST
=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.554 Prec@1=62.717 Prec@5=84.532 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=05:29 IST
=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.554 Prec@1=62.717 Prec@5=84.532 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=05:29 IST
=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.554 Prec@1=62.717 Prec@5=84.532 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=05:29 IST
=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.555 Prec@1=62.685 Prec@5=84.507 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=05:29 IST
=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.555 Prec@1=62.685 Prec@5=84.507 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=05:29 IST
=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.555 Prec@1=62.685 Prec@5=84.507 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=05:30 IST
=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.556 Prec@1=62.674 Prec@5=84.495 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=05:30 IST
=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.556 Prec@1=62.674 Prec@5=84.495 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=05:30 IST
=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.556 Prec@1=62.674 Prec@5=84.495 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=05:31 IST
=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.557 Prec@1=62.647 Prec@5=84.473 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=05:31 IST
=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.557 Prec@1=62.647 Prec@5=84.473 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=05:31 IST
=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.557 Prec@1=62.647 Prec@5=84.473 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=05:32 IST
=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.558 Prec@1=62.644 Prec@5=84.464 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=05:32 IST
=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.558 Prec@1=62.644 Prec@5=84.464 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=05:32 IST
=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.558 Prec@1=62.644 Prec@5=84.464 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=05:33 IST
=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.609 Prec@5=84.441 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=05:33 IST
=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.609 Prec@5=84.441 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=05:33 IST
=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.609 Prec@5=84.441 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=05:33 IST
=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.602 Prec@5=84.436 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=05:33 IST
=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.602 Prec@5=84.436 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=05:33 IST
=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.560 Prec@1=62.602 Prec@5=84.436 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=05:34 IST
=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.561 Prec@1=62.575 Prec@5=84.425 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=05:34 IST
=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.561 Prec@1=62.575 Prec@5=84.425 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=05:34 IST
=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.469 DataTime=0.273 Loss=1.561 Prec@1=62.575 Prec@5=84.425 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=05:34 IST
=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.561 Prec@1=62.575 Prec@5=84.425 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=05:34 IST
=> training   100.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.468 DataTime=0.273 Loss=1.561 Prec@1=62.575 Prec@5=84.425 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=05:34 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> validation 0.00% of 1x98...Epoch=41/150 LR=0.08346 Time=7.074 Loss=0.923 Prec@1=77.344 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=7.074 Loss=0.923 Prec@1=77.344 Prec@5=93.555 rate=7349.27 Hz, eta=0:00:00, total=0:00:00, wall=05:34 IST
** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=7.074 Loss=0.923 Prec@1=77.344 Prec@5=93.555 rate=7349.27 Hz, eta=0:00:00, total=0:00:00, wall=05:35 IST
** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=0.554 Loss=1.549 Prec@1=62.690 Prec@5=85.058 rate=7349.27 Hz, eta=0:00:00, total=0:00:00, wall=05:35 IST
** validation 100.00% of 1x98...Epoch=41/150 LR=0.08346 Time=0.554 Loss=1.549 Prec@1=62.690 Prec@5=85.058 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=05:35 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:35 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:35 IST
=> training   0.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=5.101 DataTime=4.910 Loss=1.670 Prec@1=59.570 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=05:35 IST
=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=5.101 DataTime=4.910 Loss=1.670 Prec@1=59.570 Prec@5=83.008 rate=10041.17 Hz, eta=0:00:00, total=0:00:00, wall=05:35 IST
=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=5.101 DataTime=4.910 Loss=1.670 Prec@1=59.570 Prec@5=83.008 rate=10041.17 Hz, eta=0:00:00, total=0:00:00, wall=05:36 IST
=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.509 DataTime=0.313 Loss=1.520 Prec@1=63.384 Prec@5=84.886 rate=10041.17 Hz, eta=0:00:00, total=0:00:00, wall=05:36 IST
=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.509 DataTime=0.313 Loss=1.520 Prec@1=63.384 Prec@5=84.886 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=05:36 IST
=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.509 DataTime=0.313 Loss=1.520 Prec@1=63.384 Prec@5=84.886 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=05:37 IST
=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.483 DataTime=0.287 Loss=1.526 Prec@1=63.374 Prec@5=84.809 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=05:37 IST
=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.483 DataTime=0.287 Loss=1.526 Prec@1=63.374 Prec@5=84.809 rate=2.18 Hz, eta=0:17:33, total=0:01:32, wall=05:37 IST
=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.483 DataTime=0.287 Loss=1.526 Prec@1=63.374 Prec@5=84.809 rate=2.18 Hz, eta=0:17:33, total=0:01:32, wall=05:37 IST
=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.475 DataTime=0.279 Loss=1.528 Prec@1=63.275 Prec@5=84.845 rate=2.18 Hz, eta=0:17:33, total=0:01:32, wall=05:37 IST
=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.475 DataTime=0.279 Loss=1.528 Prec@1=63.275 Prec@5=84.845 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=05:37 IST
=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.475 DataTime=0.279 Loss=1.528 Prec@1=63.275 Prec@5=84.845 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=05:38 IST
=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.474 DataTime=0.279 Loss=1.532 Prec@1=63.187 Prec@5=84.831 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=05:38 IST
=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.474 DataTime=0.279 Loss=1.532 Prec@1=63.187 Prec@5=84.831 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=05:38 IST
=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.474 DataTime=0.279 Loss=1.532 Prec@1=63.187 Prec@5=84.831 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=05:39 IST
=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.473 DataTime=0.278 Loss=1.532 Prec@1=63.156 Prec@5=84.855 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=05:39 IST
=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.473 DataTime=0.278 Loss=1.532 Prec@1=63.156 Prec@5=84.855 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=05:39 IST
=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.473 DataTime=0.278 Loss=1.532 Prec@1=63.156 Prec@5=84.855 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=05:40 IST
=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.277 Loss=1.533 Prec@1=63.093 Prec@5=84.839 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=05:40 IST
=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.277 Loss=1.533 Prec@1=63.093 Prec@5=84.839 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=05:40 IST
=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.277 Loss=1.533 Prec@1=63.093 Prec@5=84.839 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=05:41 IST
=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.276 Loss=1.535 Prec@1=63.051 Prec@5=84.818 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=05:41 IST
=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.276 Loss=1.535 Prec@1=63.051 Prec@5=84.818 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=05:41 IST
=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.471 DataTime=0.276 Loss=1.535 Prec@1=63.051 Prec@5=84.818 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=05:41 IST
=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.470 DataTime=0.275 Loss=1.534 Prec@1=63.041 Prec@5=84.810 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=05:41 IST
=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.470 DataTime=0.275 Loss=1.534 Prec@1=63.041 Prec@5=84.810 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=05:41 IST
=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.470 DataTime=0.275 Loss=1.534 Prec@1=63.041 Prec@5=84.810 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=05:42 IST
=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.469 DataTime=0.274 Loss=1.536 Prec@1=63.031 Prec@5=84.764 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=05:42 IST
=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.469 DataTime=0.274 Loss=1.536 Prec@1=63.031 Prec@5=84.764 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:42 IST
=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.469 DataTime=0.274 Loss=1.536 Prec@1=63.031 Prec@5=84.764 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:43 IST
=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.540 Prec@1=62.992 Prec@5=84.718 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=05:43 IST
=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.540 Prec@1=62.992 Prec@5=84.718 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=05:43 IST
=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.540 Prec@1=62.992 Prec@5=84.718 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=05:44 IST
=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.542 Prec@1=62.967 Prec@5=84.698 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=05:44 IST
=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.542 Prec@1=62.967 Prec@5=84.698 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=05:44 IST
=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.542 Prec@1=62.967 Prec@5=84.698 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=05:44 IST
=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.543 Prec@1=62.940 Prec@5=84.669 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=05:44 IST
=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.543 Prec@1=62.940 Prec@5=84.669 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=05:44 IST
=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.543 Prec@1=62.940 Prec@5=84.669 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=05:45 IST
=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.546 Prec@1=62.879 Prec@5=84.637 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=05:45 IST
=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.546 Prec@1=62.879 Prec@5=84.637 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=05:45 IST
=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.273 Loss=1.546 Prec@1=62.879 Prec@5=84.637 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=05:46 IST
=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.272 Loss=1.547 Prec@1=62.842 Prec@5=84.621 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=05:46 IST
=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.272 Loss=1.547 Prec@1=62.842 Prec@5=84.621 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=05:46 IST
=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.468 DataTime=0.272 Loss=1.547 Prec@1=62.842 Prec@5=84.621 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=05:47 IST
=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.548 Prec@1=62.832 Prec@5=84.591 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=05:47 IST
=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.548 Prec@1=62.832 Prec@5=84.591 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=05:47 IST
=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.548 Prec@1=62.832 Prec@5=84.591 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=05:47 IST
=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.550 Prec@1=62.809 Prec@5=84.568 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=05:47 IST
=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.550 Prec@1=62.809 Prec@5=84.568 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=05:47 IST
=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.550 Prec@1=62.809 Prec@5=84.568 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=05:48 IST
=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.272 Loss=1.550 Prec@1=62.796 Prec@5=84.555 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=05:48 IST
=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.272 Loss=1.550 Prec@1=62.796 Prec@5=84.555 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=05:48 IST
=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.272 Loss=1.550 Prec@1=62.796 Prec@5=84.555 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=05:49 IST
=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.552 Prec@1=62.768 Prec@5=84.534 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=05:49 IST
=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.552 Prec@1=62.768 Prec@5=84.534 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=05:49 IST
=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.552 Prec@1=62.768 Prec@5=84.534 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=05:50 IST
=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.723 Prec@5=84.524 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=05:50 IST
=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.723 Prec@5=84.524 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=05:50 IST
=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.723 Prec@5=84.524 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=05:51 IST
=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.724 Prec@5=84.523 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=05:51 IST
=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.724 Prec@5=84.523 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=05:51 IST
=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.553 Prec@1=62.724 Prec@5=84.523 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=05:51 IST
=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.554 Prec@1=62.709 Prec@5=84.507 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=05:51 IST
=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.554 Prec@1=62.709 Prec@5=84.507 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=05:51 IST
=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.467 DataTime=0.271 Loss=1.554 Prec@1=62.709 Prec@5=84.507 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=05:52 IST
=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.555 Prec@1=62.684 Prec@5=84.491 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=05:52 IST
=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.555 Prec@1=62.684 Prec@5=84.491 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=05:52 IST
=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.555 Prec@1=62.684 Prec@5=84.491 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=05:53 IST
=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.556 Prec@1=62.659 Prec@5=84.481 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=05:53 IST
=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.556 Prec@1=62.659 Prec@5=84.481 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=05:53 IST
=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.556 Prec@1=62.659 Prec@5=84.481 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=05:54 IST
=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.557 Prec@1=62.650 Prec@5=84.471 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=05:54 IST
=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.557 Prec@1=62.650 Prec@5=84.471 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=05:54 IST
=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.557 Prec@1=62.650 Prec@5=84.471 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=05:54 IST
=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.558 Prec@1=62.634 Prec@5=84.459 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=05:54 IST
=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.558 Prec@1=62.634 Prec@5=84.459 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=05:54 IST
=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.558 Prec@1=62.634 Prec@5=84.459 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=05:54 IST
=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.558 Prec@1=62.633 Prec@5=84.458 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=05:54 IST
=> training   100.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.466 DataTime=0.271 Loss=1.558 Prec@1=62.633 Prec@5=84.458 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=05:54 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> validation 0.00% of 1x98...Epoch=42/150 LR=0.08267 Time=7.091 Loss=1.012 Prec@1=75.000 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=7.091 Loss=1.012 Prec@1=75.000 Prec@5=91.797 rate=4672.26 Hz, eta=0:00:00, total=0:00:00, wall=05:55 IST
** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=7.091 Loss=1.012 Prec@1=75.000 Prec@5=91.797 rate=4672.26 Hz, eta=0:00:00, total=0:00:00, wall=05:55 IST
** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=0.552 Loss=1.543 Prec@1=62.804 Prec@5=85.152 rate=4672.26 Hz, eta=0:00:00, total=0:00:00, wall=05:55 IST
** validation 100.00% of 1x98...Epoch=42/150 LR=0.08267 Time=0.552 Loss=1.543 Prec@1=62.804 Prec@5=85.152 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=05:55 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> training   0.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.523 DataTime=4.282 Loss=1.441 Prec@1=64.453 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=05:55 IST
=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.523 DataTime=4.282 Loss=1.441 Prec@1=64.453 Prec@5=85.938 rate=6562.28 Hz, eta=0:00:00, total=0:00:00, wall=05:55 IST
=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.523 DataTime=4.282 Loss=1.441 Prec@1=64.453 Prec@5=85.938 rate=6562.28 Hz, eta=0:00:00, total=0:00:00, wall=05:56 IST
=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.492 DataTime=0.296 Loss=1.510 Prec@1=63.478 Prec@5=85.176 rate=6562.28 Hz, eta=0:00:00, total=0:00:00, wall=05:56 IST
=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.492 DataTime=0.296 Loss=1.510 Prec@1=63.478 Prec@5=85.176 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=05:56 IST
=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.492 DataTime=0.296 Loss=1.510 Prec@1=63.478 Prec@5=85.176 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=05:57 IST
=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.476 DataTime=0.278 Loss=1.508 Prec@1=63.635 Prec@5=85.158 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=05:57 IST
=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.476 DataTime=0.278 Loss=1.508 Prec@1=63.635 Prec@5=85.158 rate=2.21 Hz, eta=0:17:23, total=0:01:31, wall=05:57 IST
=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.476 DataTime=0.278 Loss=1.508 Prec@1=63.635 Prec@5=85.158 rate=2.21 Hz, eta=0:17:23, total=0:01:31, wall=05:58 IST
=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.472 DataTime=0.275 Loss=1.516 Prec@1=63.546 Prec@5=85.039 rate=2.21 Hz, eta=0:17:23, total=0:01:31, wall=05:58 IST
=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.472 DataTime=0.275 Loss=1.516 Prec@1=63.546 Prec@5=85.039 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=05:58 IST
=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.472 DataTime=0.275 Loss=1.516 Prec@1=63.546 Prec@5=85.039 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=05:59 IST
=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.470 DataTime=0.274 Loss=1.520 Prec@1=63.418 Prec@5=85.007 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=05:59 IST
=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.470 DataTime=0.274 Loss=1.520 Prec@1=63.418 Prec@5=85.007 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=05:59 IST
=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.470 DataTime=0.274 Loss=1.520 Prec@1=63.418 Prec@5=85.007 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=05:59 IST
=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.469 DataTime=0.273 Loss=1.526 Prec@1=63.256 Prec@5=84.945 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=05:59 IST
=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.469 DataTime=0.273 Loss=1.526 Prec@1=63.256 Prec@5=84.945 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=05:59 IST
=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.469 DataTime=0.273 Loss=1.526 Prec@1=63.256 Prec@5=84.945 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=06:00 IST
=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.528 Prec@1=63.242 Prec@5=84.912 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=06:00 IST
=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.528 Prec@1=63.242 Prec@5=84.912 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=06:00 IST
=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.528 Prec@1=63.242 Prec@5=84.912 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=06:01 IST
=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.529 Prec@1=63.222 Prec@5=84.875 rate=2.18 Hz, eta=0:14:34, total=0:04:36, wall=06:01 IST
=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.529 Prec@1=63.222 Prec@5=84.875 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=06:01 IST
=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.529 Prec@1=63.222 Prec@5=84.875 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=06:02 IST
=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.531 Prec@1=63.183 Prec@5=84.840 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=06:02 IST
=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.531 Prec@1=63.183 Prec@5=84.840 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=06:02 IST
=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.531 Prec@1=63.183 Prec@5=84.840 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=06:02 IST
=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.533 Prec@1=63.192 Prec@5=84.823 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=06:02 IST
=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.533 Prec@1=63.192 Prec@5=84.823 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=06:02 IST
=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.533 Prec@1=63.192 Prec@5=84.823 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=06:03 IST
=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.536 Prec@1=63.096 Prec@5=84.774 rate=2.17 Hz, eta=0:12:19, total=0:06:56, wall=06:03 IST
=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.536 Prec@1=63.096 Prec@5=84.774 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=06:03 IST
=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.536 Prec@1=63.096 Prec@5=84.774 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=06:04 IST
=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.537 Prec@1=63.087 Prec@5=84.750 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=06:04 IST
=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.537 Prec@1=63.087 Prec@5=84.750 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=06:04 IST
=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.537 Prec@1=63.087 Prec@5=84.750 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=06:05 IST
=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.271 Loss=1.537 Prec@1=63.071 Prec@5=84.737 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=06:05 IST
=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.271 Loss=1.537 Prec@1=63.071 Prec@5=84.737 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=06:05 IST
=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.271 Loss=1.537 Prec@1=63.071 Prec@5=84.737 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=06:06 IST
=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.539 Prec@1=63.038 Prec@5=84.724 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=06:06 IST
=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.539 Prec@1=63.038 Prec@5=84.724 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=06:06 IST
=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.539 Prec@1=63.038 Prec@5=84.724 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=06:06 IST
=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.540 Prec@1=63.015 Prec@5=84.700 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=06:06 IST
=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.540 Prec@1=63.015 Prec@5=84.700 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=06:06 IST
=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.467 DataTime=0.271 Loss=1.540 Prec@1=63.015 Prec@5=84.700 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=06:07 IST
=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.541 Prec@1=62.992 Prec@5=84.683 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=06:07 IST
=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.541 Prec@1=62.992 Prec@5=84.683 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=06:07 IST
=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.541 Prec@1=62.992 Prec@5=84.683 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=06:08 IST
=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.543 Prec@1=62.956 Prec@5=84.656 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=06:08 IST
=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.543 Prec@1=62.956 Prec@5=84.656 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=06:08 IST
=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.543 Prec@1=62.956 Prec@5=84.656 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=06:09 IST
=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.544 Prec@1=62.946 Prec@5=84.644 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=06:09 IST
=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.544 Prec@1=62.946 Prec@5=84.644 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=06:09 IST
=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.544 Prec@1=62.946 Prec@5=84.644 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=06:09 IST
=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.545 Prec@1=62.928 Prec@5=84.620 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=06:09 IST
=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.545 Prec@1=62.928 Prec@5=84.620 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=06:09 IST
=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.466 DataTime=0.270 Loss=1.545 Prec@1=62.928 Prec@5=84.620 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=06:10 IST
=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.546 Prec@1=62.898 Prec@5=84.617 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=06:10 IST
=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.546 Prec@1=62.898 Prec@5=84.617 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=06:10 IST
=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.546 Prec@1=62.898 Prec@5=84.617 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=06:11 IST
=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.547 Prec@1=62.888 Prec@5=84.598 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=06:11 IST
=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.547 Prec@1=62.888 Prec@5=84.598 rate=2.16 Hz, eta=0:03:52, total=0:15:26, wall=06:11 IST
=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.547 Prec@1=62.888 Prec@5=84.598 rate=2.16 Hz, eta=0:03:52, total=0:15:26, wall=06:12 IST
=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.548 Prec@1=62.851 Prec@5=84.579 rate=2.16 Hz, eta=0:03:52, total=0:15:26, wall=06:12 IST
=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.548 Prec@1=62.851 Prec@5=84.579 rate=2.16 Hz, eta=0:03:06, total=0:16:13, wall=06:12 IST
=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.548 Prec@1=62.851 Prec@5=84.579 rate=2.16 Hz, eta=0:03:06, total=0:16:13, wall=06:12 IST
=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.269 Loss=1.549 Prec@1=62.837 Prec@5=84.578 rate=2.16 Hz, eta=0:03:06, total=0:16:13, wall=06:12 IST
=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.269 Loss=1.549 Prec@1=62.837 Prec@5=84.578 rate=2.16 Hz, eta=0:02:19, total=0:16:59, wall=06:12 IST
=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.269 Loss=1.549 Prec@1=62.837 Prec@5=84.578 rate=2.16 Hz, eta=0:02:19, total=0:16:59, wall=06:13 IST
=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.549 Prec@1=62.832 Prec@5=84.572 rate=2.16 Hz, eta=0:02:19, total=0:16:59, wall=06:13 IST
=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.549 Prec@1=62.832 Prec@5=84.572 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=06:13 IST
=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.549 Prec@1=62.832 Prec@5=84.572 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=06:14 IST
=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.816 Prec@5=84.551 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=06:14 IST
=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.816 Prec@5=84.551 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=06:14 IST
=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.816 Prec@5=84.551 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=06:15 IST
=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.806 Prec@5=84.543 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=06:15 IST
=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.806 Prec@5=84.543 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=06:15 IST
=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.806 Prec@5=84.543 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=06:15 IST
=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.807 Prec@5=84.544 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=06:15 IST
=> training   100.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.465 DataTime=0.270 Loss=1.551 Prec@1=62.807 Prec@5=84.544 rate=2.16 Hz, eta=0:00:00, total=0:19:19, wall=06:15 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:15 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:15 IST
=> validation 0.00% of 1x98...Epoch=43/150 LR=0.08187 Time=6.398 Loss=1.001 Prec@1=74.219 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=06:15 IST
=> validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.398 Loss=1.001 Prec@1=74.219 Prec@5=92.773 rate=8537.30 Hz, eta=0:00:00, total=0:00:00, wall=06:15 IST
** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.398 Loss=1.001 Prec@1=74.219 Prec@5=92.773 rate=8537.30 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST
** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=0.551 Loss=1.588 Prec@1=61.930 Prec@5=84.388 rate=8537.30 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST
** validation 100.00% of 1x98...Epoch=43/150 LR=0.08187 Time=0.551 Loss=1.588 Prec@1=61.930 Prec@5=84.388 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=06:16 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST
=> training   0.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.085 DataTime=4.819 Loss=1.461 Prec@1=63.867 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST
=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.085 DataTime=4.819 Loss=1.461 Prec@1=63.867 Prec@5=87.305 rate=7798.12 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST
=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.085 DataTime=4.819 Loss=1.461 Prec@1=63.867 Prec@5=87.305 rate=7798.12 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST
=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.499 DataTime=0.302 Loss=1.497 Prec@1=64.051 Prec@5=85.323 rate=7798.12 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST
=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.499 DataTime=0.302 Loss=1.497 Prec@1=64.051 Prec@5=85.323 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=06:17 IST
=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.499 DataTime=0.302 Loss=1.497 Prec@1=64.051 Prec@5=85.323 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=06:17 IST
=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.480 DataTime=0.284 Loss=1.504 Prec@1=63.890 Prec@5=85.218 rate=2.23 Hz, eta=0:17:57, total=0:00:45, wall=06:17 IST
=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.480 DataTime=0.284 Loss=1.504 Prec@1=63.890 Prec@5=85.218 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=06:17 IST
=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.480 DataTime=0.284 Loss=1.504 Prec@1=63.890 Prec@5=85.218 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=06:18 IST
=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.472 DataTime=0.276 Loss=1.503 Prec@1=63.810 Prec@5=85.237 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=06:18 IST
=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.472 DataTime=0.276 Loss=1.503 Prec@1=63.810 Prec@5=85.237 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=06:18 IST
=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.472 DataTime=0.276 Loss=1.503 Prec@1=63.810 Prec@5=85.237 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=06:19 IST
=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.272 Loss=1.502 Prec@1=63.780 Prec@5=85.294 rate=2.20 Hz, eta=0:16:41, total=0:02:16, wall=06:19 IST
=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.272 Loss=1.502 Prec@1=63.780 Prec@5=85.294 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=06:19 IST
=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.272 Loss=1.502 Prec@1=63.780 Prec@5=85.294 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=06:20 IST
=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.271 Loss=1.510 Prec@1=63.632 Prec@5=85.148 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=06:20 IST
=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.271 Loss=1.510 Prec@1=63.632 Prec@5=85.148 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=06:20 IST
=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.467 DataTime=0.271 Loss=1.510 Prec@1=63.632 Prec@5=85.148 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=06:20 IST
=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.513 Prec@1=63.564 Prec@5=85.122 rate=2.19 Hz, eta=0:15:14, total=0:03:48, wall=06:20 IST
=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.513 Prec@1=63.564 Prec@5=85.122 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=06:20 IST
=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.513 Prec@1=63.564 Prec@5=85.122 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=06:21 IST
=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.466 DataTime=0.270 Loss=1.514 Prec@1=63.509 Prec@5=85.127 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=06:21 IST
=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.466 DataTime=0.270 Loss=1.514 Prec@1=63.509 Prec@5=85.127 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=06:21 IST
=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.466 DataTime=0.270 Loss=1.514 Prec@1=63.509 Prec@5=85.127 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=06:22 IST
=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.518 Prec@1=63.418 Prec@5=85.099 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=06:22 IST
=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.518 Prec@1=63.418 Prec@5=85.099 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=06:22 IST
=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.518 Prec@1=63.418 Prec@5=85.099 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=06:23 IST
=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.519 Prec@1=63.385 Prec@5=85.086 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=06:23 IST
=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.519 Prec@1=63.385 Prec@5=85.086 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=06:23 IST
=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.519 Prec@1=63.385 Prec@5=85.086 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=06:24 IST
=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.521 Prec@1=63.326 Prec@5=85.048 rate=2.18 Hz, eta=0:12:16, total=0:06:54, wall=06:24 IST
=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.521 Prec@1=63.326 Prec@5=85.048 rate=2.17 Hz, eta=0:11:30, total=0:07:40, wall=06:24 IST
=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.521 Prec@1=63.326 Prec@5=85.048 rate=2.17 Hz, eta=0:11:30, total=0:07:40, wall=06:24 IST
=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.523 Prec@1=63.283 Prec@5=85.014 rate=2.17 Hz, eta=0:11:30, total=0:07:40, wall=06:24 IST
=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.523 Prec@1=63.283 Prec@5=85.014 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=06:24 IST
=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.523 Prec@1=63.283 Prec@5=85.014 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=06:25 IST
=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.524 Prec@1=63.257 Prec@5=84.994 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=06:25 IST
=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.524 Prec@1=63.257 Prec@5=84.994 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=06:25 IST
=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.524 Prec@1=63.257 Prec@5=84.994 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=06:26 IST
=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.526 Prec@1=63.224 Prec@5=84.963 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=06:26 IST
=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.526 Prec@1=63.224 Prec@5=84.963 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=06:26 IST
=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.526 Prec@1=63.224 Prec@5=84.963 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=06:27 IST
=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.528 Prec@1=63.175 Prec@5=84.936 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=06:27 IST
=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.528 Prec@1=63.175 Prec@5=84.936 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=06:27 IST
=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.528 Prec@1=63.175 Prec@5=84.936 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=06:27 IST
=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.529 Prec@1=63.135 Prec@5=84.923 rate=2.17 Hz, eta=0:08:28, total=0:10:46, wall=06:27 IST
=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.529 Prec@1=63.135 Prec@5=84.923 rate=2.16 Hz, eta=0:07:42, total=0:11:33, wall=06:27 IST
=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.529 Prec@1=63.135 Prec@5=84.923 rate=2.16 Hz, eta=0:07:42, total=0:11:33, wall=06:28 IST
=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.531 Prec@1=63.127 Prec@5=84.901 rate=2.16 Hz, eta=0:07:42, total=0:11:33, wall=06:28 IST
=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.531 Prec@1=63.127 Prec@5=84.901 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=06:28 IST
=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.531 Prec@1=63.127 Prec@5=84.901 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=06:29 IST
=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.533 Prec@1=63.093 Prec@5=84.874 rate=2.17 Hz, eta=0:06:56, total=0:12:19, wall=06:29 IST
=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.533 Prec@1=63.093 Prec@5=84.874 rate=2.17 Hz, eta=0:06:10, total=0:13:05, wall=06:29 IST
=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.533 Prec@1=63.093 Prec@5=84.874 rate=2.17 Hz, eta=0:06:10, total=0:13:05, wall=06:30 IST
=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.533 Prec@1=63.101 Prec@5=84.865 rate=2.17 Hz, eta=0:06:10, total=0:13:05, wall=06:30 IST
=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.533 Prec@1=63.101 Prec@5=84.865 rate=2.16 Hz, eta=0:05:24, total=0:13:53, wall=06:30 IST
=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.533 Prec@1=63.101 Prec@5=84.865 rate=2.16 Hz, eta=0:05:24, total=0:13:53, wall=06:31 IST
=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.535 Prec@1=63.070 Prec@5=84.839 rate=2.16 Hz, eta=0:05:24, total=0:13:53, wall=06:31 IST
=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.535 Prec@1=63.070 Prec@5=84.839 rate=2.16 Hz, eta=0:04:38, total=0:14:39, wall=06:31 IST
=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.270 Loss=1.535 Prec@1=63.070 Prec@5=84.839 rate=2.16 Hz, eta=0:04:38, total=0:14:39, wall=06:31 IST
=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.535 Prec@1=63.070 Prec@5=84.830 rate=2.16 Hz, eta=0:04:38, total=0:14:39, wall=06:31 IST
=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.535 Prec@1=63.070 Prec@5=84.830 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=06:31 IST
=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.535 Prec@1=63.070 Prec@5=84.830 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=06:32 IST
=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.063 Prec@5=84.813 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=06:32 IST
=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.063 Prec@5=84.813 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=06:32 IST
=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.063 Prec@5=84.813 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=06:33 IST
=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.046 Prec@5=84.811 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=06:33 IST
=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.046 Prec@5=84.811 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=06:33 IST
=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.536 Prec@1=63.046 Prec@5=84.811 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=06:34 IST
=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.538 Prec@1=63.010 Prec@5=84.792 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=06:34 IST
=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.538 Prec@1=63.010 Prec@5=84.792 rate=2.16 Hz, eta=0:01:33, total=0:17:44, wall=06:34 IST
=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.465 DataTime=0.269 Loss=1.538 Prec@1=63.010 Prec@5=84.792 rate=2.16 Hz, eta=0:01:33, total=0:17:44, wall=06:34 IST
=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.269 Loss=1.540 Prec@1=62.976 Prec@5=84.767 rate=2.16 Hz, eta=0:01:33, total=0:17:44, wall=06:34 IST
=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.269 Loss=1.540 Prec@1=62.976 Prec@5=84.767 rate=2.16 Hz, eta=0:00:47, total=0:18:30, wall=06:34 IST
=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.269 Loss=1.540 Prec@1=62.976 Prec@5=84.767 rate=2.16 Hz, eta=0:00:47, total=0:18:30, wall=06:35 IST
=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.268 Loss=1.541 Prec@1=62.966 Prec@5=84.754 rate=2.16 Hz, eta=0:00:47, total=0:18:30, wall=06:35 IST
=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.268 Loss=1.541 Prec@1=62.966 Prec@5=84.754 rate=2.16 Hz, eta=0:00:00, total=0:19:16, wall=06:35 IST
=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.268 Loss=1.541 Prec@1=62.966 Prec@5=84.754 rate=2.16 Hz, eta=0:00:00, total=0:19:16, wall=06:35 IST
=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.268 Loss=1.541 Prec@1=62.965 Prec@5=84.753 rate=2.16 Hz, eta=0:00:00, total=0:19:16, wall=06:35 IST
=> training   100.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.464 DataTime=0.268 Loss=1.541 Prec@1=62.965 Prec@5=84.753 rate=2.16 Hz, eta=0:00:00, total=0:19:16, wall=06:35 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:35 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:35 IST
=> validation 0.00% of 1x98...Epoch=44/150 LR=0.08106 Time=7.182 Loss=0.893 Prec@1=76.758 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=06:35 IST
=> validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=7.182 Loss=0.893 Prec@1=76.758 Prec@5=93.359 rate=5286.33 Hz, eta=0:00:00, total=0:00:00, wall=06:35 IST
** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=7.182 Loss=0.893 Prec@1=76.758 Prec@5=93.359 rate=5286.33 Hz, eta=0:00:00, total=0:00:00, wall=06:36 IST
** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=0.551 Loss=1.559 Prec@1=62.532 Prec@5=84.708 rate=5286.33 Hz, eta=0:00:00, total=0:00:00, wall=06:36 IST
** validation 100.00% of 1x98...Epoch=44/150 LR=0.08106 Time=0.551 Loss=1.559 Prec@1=62.532 Prec@5=84.708 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=06:36 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:36 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:36 IST
=> training   0.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=4.531 DataTime=4.245 Loss=1.519 Prec@1=61.914 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=06:36 IST
=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=4.531 DataTime=4.245 Loss=1.519 Prec@1=61.914 Prec@5=86.133 rate=7496.42 Hz, eta=0:00:00, total=0:00:00, wall=06:36 IST
=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=4.531 DataTime=4.245 Loss=1.519 Prec@1=61.914 Prec@5=86.133 rate=7496.42 Hz, eta=0:00:00, total=0:00:00, wall=06:37 IST
=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.499 DataTime=0.305 Loss=1.500 Prec@1=63.919 Prec@5=85.371 rate=7496.42 Hz, eta=0:00:00, total=0:00:00, wall=06:37 IST
=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.499 DataTime=0.305 Loss=1.500 Prec@1=63.919 Prec@5=85.371 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=06:37 IST
=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.499 DataTime=0.305 Loss=1.500 Prec@1=63.919 Prec@5=85.371 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=06:38 IST
=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.481 DataTime=0.286 Loss=1.505 Prec@1=63.831 Prec@5=85.195 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=06:38 IST
=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.481 DataTime=0.286 Loss=1.505 Prec@1=63.831 Prec@5=85.195 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=06:38 IST
=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.481 DataTime=0.286 Loss=1.505 Prec@1=63.831 Prec@5=85.195 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=06:38 IST
=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.476 DataTime=0.280 Loss=1.508 Prec@1=63.765 Prec@5=85.118 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=06:38 IST
=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.476 DataTime=0.280 Loss=1.508 Prec@1=63.765 Prec@5=85.118 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=06:38 IST
=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.476 DataTime=0.280 Loss=1.508 Prec@1=63.765 Prec@5=85.118 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=06:39 IST
=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.472 DataTime=0.276 Loss=1.509 Prec@1=63.699 Prec@5=85.099 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=06:39 IST
=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.472 DataTime=0.276 Loss=1.509 Prec@1=63.699 Prec@5=85.099 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=06:39 IST
=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.472 DataTime=0.276 Loss=1.509 Prec@1=63.699 Prec@5=85.099 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=06:40 IST
=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.514 Prec@1=63.589 Prec@5=85.028 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=06:40 IST
=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.514 Prec@1=63.589 Prec@5=85.028 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=06:40 IST
=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.514 Prec@1=63.589 Prec@5=85.028 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=06:41 IST
=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.517 Prec@1=63.514 Prec@5=84.983 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=06:41 IST
=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.517 Prec@1=63.514 Prec@5=84.983 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=06:41 IST
=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.471 DataTime=0.275 Loss=1.517 Prec@1=63.514 Prec@5=84.983 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=06:42 IST
=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.470 DataTime=0.274 Loss=1.519 Prec@1=63.487 Prec@5=84.988 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=06:42 IST
=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.470 DataTime=0.274 Loss=1.519 Prec@1=63.487 Prec@5=84.988 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=06:42 IST
=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.470 DataTime=0.274 Loss=1.519 Prec@1=63.487 Prec@5=84.988 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=06:42 IST
=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.459 Prec@5=84.962 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=06:42 IST
=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.459 Prec@5=84.962 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=06:42 IST
=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.459 Prec@5=84.962 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=06:43 IST
=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.410 Prec@5=84.949 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=06:43 IST
=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.410 Prec@5=84.949 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=06:43 IST
=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.410 Prec@5=84.949 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=06:44 IST
=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.271 Loss=1.523 Prec@1=63.388 Prec@5=84.946 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=06:44 IST
=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.271 Loss=1.523 Prec@1=63.388 Prec@5=84.946 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=06:44 IST
=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.271 Loss=1.523 Prec@1=63.388 Prec@5=84.946 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=06:45 IST
=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.523 Prec@1=63.358 Prec@5=84.941 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=06:45 IST
=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.523 Prec@1=63.358 Prec@5=84.941 rate=2.15 Hz, eta=0:10:50, total=0:08:30, wall=06:45 IST
=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.523 Prec@1=63.358 Prec@5=84.941 rate=2.15 Hz, eta=0:10:50, total=0:08:30, wall=06:45 IST
=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.310 Prec@5=84.905 rate=2.15 Hz, eta=0:10:50, total=0:08:30, wall=06:45 IST
=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.310 Prec@5=84.905 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=06:45 IST
=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.310 Prec@5=84.905 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=06:46 IST
=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.315 Prec@5=84.907 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=06:46 IST
=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.315 Prec@5=84.907 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=06:46 IST
=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.525 Prec@1=63.315 Prec@5=84.907 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=06:47 IST
=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.527 Prec@1=63.289 Prec@5=84.886 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=06:47 IST
=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.527 Prec@1=63.289 Prec@5=84.886 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=06:47 IST
=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.527 Prec@1=63.289 Prec@5=84.886 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=06:48 IST
=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.528 Prec@1=63.255 Prec@5=84.873 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=06:48 IST
=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.528 Prec@1=63.255 Prec@5=84.873 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=06:48 IST
=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.528 Prec@1=63.255 Prec@5=84.873 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=06:49 IST
=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.528 Prec@1=63.250 Prec@5=84.871 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=06:49 IST
=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.528 Prec@1=63.250 Prec@5=84.871 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=06:49 IST
=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.528 Prec@1=63.250 Prec@5=84.871 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=06:49 IST
=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.529 Prec@1=63.215 Prec@5=84.855 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=06:49 IST
=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.529 Prec@1=63.215 Prec@5=84.855 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=06:49 IST
=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.529 Prec@1=63.215 Prec@5=84.855 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=06:50 IST
=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.530 Prec@1=63.199 Prec@5=84.840 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=06:50 IST
=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.530 Prec@1=63.199 Prec@5=84.840 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=06:50 IST
=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.530 Prec@1=63.199 Prec@5=84.840 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=06:51 IST
=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.177 Prec@5=84.830 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=06:51 IST
=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.177 Prec@5=84.830 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=06:51 IST
=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.177 Prec@5=84.830 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=06:52 IST
=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.169 Prec@5=84.812 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=06:52 IST
=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.169 Prec@5=84.812 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=06:52 IST
=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.532 Prec@1=63.169 Prec@5=84.812 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=06:52 IST
=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.147 Prec@5=84.788 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=06:52 IST
=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.147 Prec@5=84.788 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=06:52 IST
=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.469 DataTime=0.273 Loss=1.534 Prec@1=63.147 Prec@5=84.788 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=06:53 IST
=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.535 Prec@1=63.130 Prec@5=84.770 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=06:53 IST
=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.535 Prec@1=63.130 Prec@5=84.770 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=06:53 IST
=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.535 Prec@1=63.130 Prec@5=84.770 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=06:54 IST
=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.114 Prec@5=84.756 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=06:54 IST
=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.114 Prec@5=84.756 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=06:54 IST
=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.114 Prec@5=84.756 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=06:55 IST
=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.107 Prec@5=84.754 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=06:55 IST
=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.107 Prec@5=84.754 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=06:55 IST
=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.107 Prec@5=84.754 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=06:56 IST
=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.103 Prec@5=84.752 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=06:56 IST
=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.103 Prec@5=84.752 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=06:56 IST
=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.103 Prec@5=84.752 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=06:56 IST
=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.102 Prec@5=84.752 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=06:56 IST
=> training   100.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.468 DataTime=0.272 Loss=1.536 Prec@1=63.102 Prec@5=84.752 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=06:56 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> validation 0.00% of 1x98...Epoch=45/150 LR=0.08023 Time=6.889 Loss=0.861 Prec@1=78.711 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.889 Loss=0.861 Prec@1=78.711 Prec@5=94.336 rate=7607.34 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.889 Loss=0.861 Prec@1=78.711 Prec@5=94.336 rate=7607.34 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=0.553 Loss=1.558 Prec@1=62.314 Prec@5=84.836 rate=7607.34 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
** validation 100.00% of 1x98...Epoch=45/150 LR=0.08023 Time=0.553 Loss=1.558 Prec@1=62.314 Prec@5=84.836 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=06:56 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST
=> training   0.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.150 DataTime=4.921 Loss=1.713 Prec@1=59.570 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST
=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.150 DataTime=4.921 Loss=1.713 Prec@1=59.570 Prec@5=82.227 rate=7770.25 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST
=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.150 DataTime=4.921 Loss=1.713 Prec@1=59.570 Prec@5=82.227 rate=7770.25 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST
=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.498 DataTime=0.302 Loss=1.483 Prec@1=64.219 Prec@5=85.671 rate=7770.25 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST
=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.498 DataTime=0.302 Loss=1.483 Prec@1=64.219 Prec@5=85.671 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=06:57 IST
=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.498 DataTime=0.302 Loss=1.483 Prec@1=64.219 Prec@5=85.671 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=06:58 IST
=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.480 DataTime=0.283 Loss=1.483 Prec@1=64.173 Prec@5=85.563 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=06:58 IST
=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.480 DataTime=0.283 Loss=1.483 Prec@1=64.173 Prec@5=85.563 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=06:58 IST
=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.480 DataTime=0.283 Loss=1.483 Prec@1=64.173 Prec@5=85.563 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=06:59 IST
=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.473 DataTime=0.275 Loss=1.496 Prec@1=63.920 Prec@5=85.369 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=06:59 IST
=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.473 DataTime=0.275 Loss=1.496 Prec@1=63.920 Prec@5=85.369 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=06:59 IST
=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.473 DataTime=0.275 Loss=1.496 Prec@1=63.920 Prec@5=85.369 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=07:00 IST
=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.470 DataTime=0.273 Loss=1.500 Prec@1=63.788 Prec@5=85.298 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=07:00 IST
=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.470 DataTime=0.273 Loss=1.500 Prec@1=63.788 Prec@5=85.298 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=07:00 IST
=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.470 DataTime=0.273 Loss=1.500 Prec@1=63.788 Prec@5=85.298 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=07:00 IST
=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.468 DataTime=0.271 Loss=1.506 Prec@1=63.711 Prec@5=85.215 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=07:00 IST
=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.468 DataTime=0.271 Loss=1.506 Prec@1=63.711 Prec@5=85.215 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=07:00 IST
=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.468 DataTime=0.271 Loss=1.506 Prec@1=63.711 Prec@5=85.215 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=07:01 IST
=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.466 DataTime=0.269 Loss=1.506 Prec@1=63.710 Prec@5=85.213 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=07:01 IST
=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.466 DataTime=0.269 Loss=1.506 Prec@1=63.710 Prec@5=85.213 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=07:01 IST
=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.466 DataTime=0.269 Loss=1.506 Prec@1=63.710 Prec@5=85.213 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=07:02 IST
=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.269 Loss=1.507 Prec@1=63.708 Prec@5=85.186 rate=2.19 Hz, eta=0:14:29, total=0:04:34, wall=07:02 IST
=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.269 Loss=1.507 Prec@1=63.708 Prec@5=85.186 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=07:02 IST
=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.269 Loss=1.507 Prec@1=63.708 Prec@5=85.186 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=07:03 IST
=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.510 Prec@1=63.631 Prec@5=85.145 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=07:03 IST
=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.510 Prec@1=63.631 Prec@5=85.145 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=07:03 IST
=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.510 Prec@1=63.631 Prec@5=85.145 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=07:04 IST
=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.267 Loss=1.512 Prec@1=63.607 Prec@5=85.115 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=07:04 IST
=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.267 Loss=1.512 Prec@1=63.607 Prec@5=85.115 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=07:04 IST
=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.267 Loss=1.512 Prec@1=63.607 Prec@5=85.115 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=07:04 IST
=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.513 Prec@1=63.602 Prec@5=85.083 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=07:04 IST
=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.513 Prec@1=63.602 Prec@5=85.083 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=07:04 IST
=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.513 Prec@1=63.602 Prec@5=85.083 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=07:05 IST
=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.515 Prec@1=63.564 Prec@5=85.061 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=07:05 IST
=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.515 Prec@1=63.564 Prec@5=85.061 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=07:05 IST
=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.515 Prec@1=63.564 Prec@5=85.061 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=07:06 IST
=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.517 Prec@1=63.507 Prec@5=85.027 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=07:06 IST
=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.517 Prec@1=63.507 Prec@5=85.027 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=07:06 IST
=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.465 DataTime=0.268 Loss=1.517 Prec@1=63.507 Prec@5=85.027 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=07:07 IST
=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.520 Prec@1=63.452 Prec@5=84.981 rate=2.17 Hz, eta=0:09:59, total=0:09:13, wall=07:07 IST
=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.520 Prec@1=63.452 Prec@5=84.981 rate=2.17 Hz, eta=0:09:13, total=0:09:59, wall=07:07 IST
=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.520 Prec@1=63.452 Prec@5=84.981 rate=2.17 Hz, eta=0:09:13, total=0:09:59, wall=07:07 IST
=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.400 Prec@5=84.962 rate=2.17 Hz, eta=0:09:13, total=0:09:59, wall=07:07 IST
=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.400 Prec@5=84.962 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=07:07 IST
=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.400 Prec@5=84.962 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=07:08 IST
=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.398 Prec@5=84.966 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=07:08 IST
=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.398 Prec@5=84.966 rate=2.17 Hz, eta=0:07:41, total=0:11:30, wall=07:08 IST
=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.522 Prec@1=63.398 Prec@5=84.966 rate=2.17 Hz, eta=0:07:41, total=0:11:30, wall=07:09 IST
=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.523 Prec@1=63.373 Prec@5=84.944 rate=2.17 Hz, eta=0:07:41, total=0:11:30, wall=07:09 IST
=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.523 Prec@1=63.373 Prec@5=84.944 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=07:09 IST
=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.523 Prec@1=63.373 Prec@5=84.944 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=07:10 IST
=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.463 DataTime=0.267 Loss=1.523 Prec@1=63.376 Prec@5=84.949 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=07:10 IST
=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.463 DataTime=0.267 Loss=1.523 Prec@1=63.376 Prec@5=84.949 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=07:10 IST
=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.463 DataTime=0.267 Loss=1.523 Prec@1=63.376 Prec@5=84.949 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=07:10 IST
=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.525 Prec@1=63.353 Prec@5=84.932 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=07:10 IST
=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.525 Prec@1=63.353 Prec@5=84.932 rate=2.17 Hz, eta=0:05:23, total=0:13:50, wall=07:10 IST
=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.525 Prec@1=63.353 Prec@5=84.932 rate=2.17 Hz, eta=0:05:23, total=0:13:50, wall=07:11 IST
=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.526 Prec@1=63.318 Prec@5=84.911 rate=2.17 Hz, eta=0:05:23, total=0:13:50, wall=07:11 IST
=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.526 Prec@1=63.318 Prec@5=84.911 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=07:11 IST
=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.526 Prec@1=63.318 Prec@5=84.911 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=07:12 IST
=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.527 Prec@1=63.314 Prec@5=84.904 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=07:12 IST
=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.527 Prec@1=63.314 Prec@5=84.904 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=07:12 IST
=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.527 Prec@1=63.314 Prec@5=84.904 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=07:13 IST
=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.528 Prec@1=63.281 Prec@5=84.887 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=07:13 IST
=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.528 Prec@1=63.281 Prec@5=84.887 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=07:13 IST
=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.528 Prec@1=63.281 Prec@5=84.887 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=07:14 IST
=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.529 Prec@1=63.258 Prec@5=84.876 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=07:14 IST
=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.529 Prec@1=63.258 Prec@5=84.876 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=07:14 IST
=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.267 Loss=1.529 Prec@1=63.258 Prec@5=84.876 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=07:14 IST
=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.530 Prec@1=63.223 Prec@5=84.858 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=07:14 IST
=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.530 Prec@1=63.223 Prec@5=84.858 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=07:14 IST
=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.530 Prec@1=63.223 Prec@5=84.858 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=07:15 IST
=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.531 Prec@1=63.212 Prec@5=84.843 rate=2.17 Hz, eta=0:01:33, total=0:17:42, wall=07:15 IST
=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.531 Prec@1=63.212 Prec@5=84.843 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=07:15 IST
=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.531 Prec@1=63.212 Prec@5=84.843 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=07:16 IST
=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.532 Prec@1=63.187 Prec@5=84.825 rate=2.16 Hz, eta=0:00:47, total=0:18:29, wall=07:16 IST
=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.532 Prec@1=63.187 Prec@5=84.825 rate=2.16 Hz, eta=0:00:00, total=0:19:15, wall=07:16 IST
=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.532 Prec@1=63.187 Prec@5=84.825 rate=2.16 Hz, eta=0:00:00, total=0:19:15, wall=07:16 IST
=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.532 Prec@1=63.186 Prec@5=84.824 rate=2.16 Hz, eta=0:00:00, total=0:19:15, wall=07:16 IST
=> training   100.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.464 DataTime=0.268 Loss=1.532 Prec@1=63.186 Prec@5=84.824 rate=2.17 Hz, eta=0:00:00, total=0:19:16, wall=07:16 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST
=> validation 0.00% of 1x98...Epoch=46/150 LR=0.07939 Time=7.061 Loss=0.997 Prec@1=74.805 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST
=> validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=7.061 Loss=0.997 Prec@1=74.805 Prec@5=92.578 rate=4428.87 Hz, eta=0:00:00, total=0:00:00, wall=07:16 IST
** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=7.061 Loss=0.997 Prec@1=74.805 Prec@5=92.578 rate=4428.87 Hz, eta=0:00:00, total=0:00:00, wall=07:17 IST
** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=0.551 Loss=1.605 Prec@1=61.428 Prec@5=84.214 rate=4428.87 Hz, eta=0:00:00, total=0:00:00, wall=07:17 IST
** validation 100.00% of 1x98...Epoch=46/150 LR=0.07939 Time=0.551 Loss=1.605 Prec@1=61.428 Prec@5=84.214 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=07:17 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:17 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:17 IST
=> training   0.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.932 DataTime=4.699 Loss=1.411 Prec@1=66.797 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=07:17 IST
=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.932 DataTime=4.699 Loss=1.411 Prec@1=66.797 Prec@5=86.133 rate=8504.34 Hz, eta=0:00:00, total=0:00:00, wall=07:17 IST
=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.932 DataTime=4.699 Loss=1.411 Prec@1=66.797 Prec@5=86.133 rate=8504.34 Hz, eta=0:00:00, total=0:00:00, wall=07:18 IST
=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.508 DataTime=0.312 Loss=1.482 Prec@1=64.250 Prec@5=85.622 rate=8504.34 Hz, eta=0:00:00, total=0:00:00, wall=07:18 IST
=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.508 DataTime=0.312 Loss=1.482 Prec@1=64.250 Prec@5=85.622 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=07:18 IST
=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.508 DataTime=0.312 Loss=1.482 Prec@1=64.250 Prec@5=85.622 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=07:18 IST
=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.491 DataTime=0.294 Loss=1.489 Prec@1=64.103 Prec@5=85.480 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=07:18 IST
=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.491 DataTime=0.294 Loss=1.489 Prec@1=64.103 Prec@5=85.480 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=07:18 IST
=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.491 DataTime=0.294 Loss=1.489 Prec@1=64.103 Prec@5=85.480 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=07:19 IST
=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.480 DataTime=0.283 Loss=1.494 Prec@1=63.890 Prec@5=85.379 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=07:19 IST
=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.480 DataTime=0.283 Loss=1.494 Prec@1=63.890 Prec@5=85.379 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=07:19 IST
=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.480 DataTime=0.283 Loss=1.494 Prec@1=63.890 Prec@5=85.379 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=07:20 IST
=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.475 DataTime=0.278 Loss=1.496 Prec@1=63.776 Prec@5=85.405 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=07:20 IST
=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.475 DataTime=0.278 Loss=1.496 Prec@1=63.776 Prec@5=85.405 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=07:20 IST
=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.475 DataTime=0.278 Loss=1.496 Prec@1=63.776 Prec@5=85.405 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=07:21 IST
=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.474 DataTime=0.277 Loss=1.498 Prec@1=63.752 Prec@5=85.335 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=07:21 IST
=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.474 DataTime=0.277 Loss=1.498 Prec@1=63.752 Prec@5=85.335 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=07:21 IST
=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.474 DataTime=0.277 Loss=1.498 Prec@1=63.752 Prec@5=85.335 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=07:22 IST
=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.473 DataTime=0.276 Loss=1.501 Prec@1=63.712 Prec@5=85.338 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=07:22 IST
=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.473 DataTime=0.276 Loss=1.501 Prec@1=63.712 Prec@5=85.338 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=07:22 IST
=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.473 DataTime=0.276 Loss=1.501 Prec@1=63.712 Prec@5=85.338 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=07:22 IST
=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.472 DataTime=0.275 Loss=1.504 Prec@1=63.652 Prec@5=85.297 rate=2.15 Hz, eta=0:14:44, total=0:04:39, wall=07:22 IST
=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.472 DataTime=0.275 Loss=1.504 Prec@1=63.652 Prec@5=85.297 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=07:22 IST
=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.472 DataTime=0.275 Loss=1.504 Prec@1=63.652 Prec@5=85.297 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=07:23 IST
=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.505 Prec@1=63.627 Prec@5=85.261 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=07:23 IST
=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.505 Prec@1=63.627 Prec@5=85.261 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=07:23 IST
=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.505 Prec@1=63.627 Prec@5=85.261 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=07:24 IST
=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.507 Prec@1=63.573 Prec@5=85.229 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=07:24 IST
=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.507 Prec@1=63.573 Prec@5=85.229 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=07:24 IST
=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.507 Prec@1=63.573 Prec@5=85.229 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=07:25 IST
=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.510 Prec@1=63.541 Prec@5=85.190 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=07:25 IST
=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.510 Prec@1=63.541 Prec@5=85.190 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=07:25 IST
=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.510 Prec@1=63.541 Prec@5=85.190 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=07:25 IST
=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.511 Prec@1=63.531 Prec@5=85.162 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=07:25 IST
=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.511 Prec@1=63.531 Prec@5=85.162 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=07:25 IST
=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.511 Prec@1=63.531 Prec@5=85.162 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=07:26 IST
=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.512 Prec@1=63.491 Prec@5=85.152 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=07:26 IST
=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.512 Prec@1=63.491 Prec@5=85.152 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=07:26 IST
=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.512 Prec@1=63.491 Prec@5=85.152 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=07:27 IST
=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.478 Prec@5=85.114 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=07:27 IST
=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.478 Prec@5=85.114 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=07:27 IST
=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.478 Prec@5=85.114 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=07:28 IST
=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.515 Prec@1=63.469 Prec@5=85.101 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=07:28 IST
=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.515 Prec@1=63.469 Prec@5=85.101 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=07:28 IST
=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.515 Prec@1=63.469 Prec@5=85.101 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=07:29 IST
=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.454 Prec@5=85.085 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=07:29 IST
=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.454 Prec@5=85.085 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=07:29 IST
=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.454 Prec@5=85.085 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=07:29 IST
=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.443 Prec@5=85.065 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=07:29 IST
=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.443 Prec@5=85.065 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=07:29 IST
=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.517 Prec@1=63.443 Prec@5=85.065 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=07:30 IST
=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.518 Prec@1=63.434 Prec@5=85.054 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=07:30 IST
=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.518 Prec@1=63.434 Prec@5=85.054 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=07:30 IST
=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.470 DataTime=0.273 Loss=1.518 Prec@1=63.434 Prec@5=85.054 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=07:31 IST
=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.519 Prec@1=63.410 Prec@5=85.036 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=07:31 IST
=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.519 Prec@1=63.410 Prec@5=85.036 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=07:31 IST
=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.519 Prec@1=63.410 Prec@5=85.036 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=07:32 IST
=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.387 Prec@5=85.030 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=07:32 IST
=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.387 Prec@5=85.030 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=07:32 IST
=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.520 Prec@1=63.387 Prec@5=85.030 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=07:32 IST
=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.521 Prec@1=63.367 Prec@5=85.019 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=07:32 IST
=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.521 Prec@1=63.367 Prec@5=85.019 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=07:32 IST
=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.521 Prec@1=63.367 Prec@5=85.019 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=07:33 IST
=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.341 Prec@5=85.006 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=07:33 IST
=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.341 Prec@5=85.006 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=07:33 IST
=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.522 Prec@1=63.341 Prec@5=85.006 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=07:34 IST
=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.523 Prec@1=63.308 Prec@5=84.990 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=07:34 IST
=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.523 Prec@1=63.308 Prec@5=84.990 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=07:34 IST
=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.523 Prec@1=63.308 Prec@5=84.990 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=07:35 IST
=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.524 Prec@1=63.282 Prec@5=84.978 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=07:35 IST
=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.524 Prec@1=63.282 Prec@5=84.978 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=07:35 IST
=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.524 Prec@1=63.282 Prec@5=84.978 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=07:36 IST
=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.281 Prec@5=84.969 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=07:36 IST
=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.281 Prec@5=84.969 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=07:36 IST
=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.281 Prec@5=84.969 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=07:36 IST
=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.271 Prec@5=84.961 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=07:36 IST
=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.271 Prec@5=84.961 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=07:36 IST
=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.271 Prec@5=84.961 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=07:36 IST
=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.271 Prec@5=84.960 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=07:36 IST
=> training   100.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.469 DataTime=0.273 Loss=1.525 Prec@1=63.271 Prec@5=84.960 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=07:36 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> validation 0.00% of 1x98...Epoch=47/150 LR=0.07854 Time=6.745 Loss=0.961 Prec@1=75.586 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=6.745 Loss=0.961 Prec@1=75.586 Prec@5=93.750 rate=5011.90 Hz, eta=0:00:00, total=0:00:00, wall=07:36 IST
** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=6.745 Loss=0.961 Prec@1=75.586 Prec@5=93.750 rate=5011.90 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST
** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=0.551 Loss=1.558 Prec@1=62.636 Prec@5=84.876 rate=5011.90 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST
** validation 100.00% of 1x98...Epoch=47/150 LR=0.07854 Time=0.551 Loss=1.558 Prec@1=62.636 Prec@5=84.876 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=07:37 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST
=> training   0.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.322 DataTime=4.962 Loss=1.496 Prec@1=63.672 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST
=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.322 DataTime=4.962 Loss=1.496 Prec@1=63.672 Prec@5=85.742 rate=9802.77 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST
=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.322 DataTime=4.962 Loss=1.496 Prec@1=63.672 Prec@5=85.742 rate=9802.77 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST
=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.505 DataTime=0.310 Loss=1.478 Prec@1=64.215 Prec@5=85.471 rate=9802.77 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST
=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.505 DataTime=0.310 Loss=1.478 Prec@1=64.215 Prec@5=85.471 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=07:38 IST
=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.505 DataTime=0.310 Loss=1.478 Prec@1=64.215 Prec@5=85.471 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=07:39 IST
=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.481 DataTime=0.287 Loss=1.468 Prec@1=64.412 Prec@5=85.708 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=07:39 IST
=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.481 DataTime=0.287 Loss=1.468 Prec@1=64.412 Prec@5=85.708 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=07:39 IST
=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.481 DataTime=0.287 Loss=1.468 Prec@1=64.412 Prec@5=85.708 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=07:40 IST
=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.476 DataTime=0.281 Loss=1.477 Prec@1=64.188 Prec@5=85.610 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=07:40 IST
=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.476 DataTime=0.281 Loss=1.477 Prec@1=64.188 Prec@5=85.610 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=07:40 IST
=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.476 DataTime=0.281 Loss=1.477 Prec@1=64.188 Prec@5=85.610 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=07:40 IST
=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.473 DataTime=0.278 Loss=1.481 Prec@1=64.155 Prec@5=85.549 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=07:40 IST
=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.473 DataTime=0.278 Loss=1.481 Prec@1=64.155 Prec@5=85.549 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=07:40 IST
=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.473 DataTime=0.278 Loss=1.481 Prec@1=64.155 Prec@5=85.549 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=07:41 IST
=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.471 DataTime=0.277 Loss=1.484 Prec@1=64.139 Prec@5=85.483 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=07:41 IST
=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.471 DataTime=0.277 Loss=1.484 Prec@1=64.139 Prec@5=85.483 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=07:41 IST
=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.471 DataTime=0.277 Loss=1.484 Prec@1=64.139 Prec@5=85.483 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=07:42 IST
=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.274 Loss=1.485 Prec@1=64.084 Prec@5=85.466 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=07:42 IST
=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.274 Loss=1.485 Prec@1=64.084 Prec@5=85.466 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=07:42 IST
=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.274 Loss=1.485 Prec@1=64.084 Prec@5=85.466 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=07:43 IST
=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.487 Prec@1=64.066 Prec@5=85.487 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=07:43 IST
=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.487 Prec@1=64.066 Prec@5=85.487 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=07:43 IST
=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.487 Prec@1=64.066 Prec@5=85.487 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=07:44 IST
=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.488 Prec@1=64.059 Prec@5=85.475 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=07:44 IST
=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.488 Prec@1=64.059 Prec@5=85.475 rate=2.17 Hz, eta=0:13:06, total=0:06:09, wall=07:44 IST
=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.488 Prec@1=64.059 Prec@5=85.475 rate=2.17 Hz, eta=0:13:06, total=0:06:09, wall=07:44 IST
=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=64.010 Prec@5=85.429 rate=2.17 Hz, eta=0:13:06, total=0:06:09, wall=07:44 IST
=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=64.010 Prec@5=85.429 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=07:44 IST
=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=64.010 Prec@5=85.429 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=07:45 IST
=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.494 Prec@1=63.968 Prec@5=85.417 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=07:45 IST
=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.494 Prec@1=63.968 Prec@5=85.417 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=07:45 IST
=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.494 Prec@1=63.968 Prec@5=85.417 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=07:46 IST
=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.498 Prec@1=63.878 Prec@5=85.375 rate=2.16 Hz, eta=0:11:34, total=0:07:43, wall=07:46 IST
=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.498 Prec@1=63.878 Prec@5=85.375 rate=2.16 Hz, eta=0:10:49, total=0:08:29, wall=07:46 IST
=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.498 Prec@1=63.878 Prec@5=85.375 rate=2.16 Hz, eta=0:10:49, total=0:08:29, wall=07:47 IST
=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.272 Loss=1.500 Prec@1=63.818 Prec@5=85.343 rate=2.16 Hz, eta=0:10:49, total=0:08:29, wall=07:47 IST
=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.272 Loss=1.500 Prec@1=63.818 Prec@5=85.343 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=07:47 IST
=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.272 Loss=1.500 Prec@1=63.818 Prec@5=85.343 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=07:47 IST
=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.502 Prec@1=63.803 Prec@5=85.324 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=07:47 IST
=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.502 Prec@1=63.803 Prec@5=85.324 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=07:47 IST
=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.502 Prec@1=63.803 Prec@5=85.324 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=07:48 IST
=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.503 Prec@1=63.785 Prec@5=85.298 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=07:48 IST
=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.503 Prec@1=63.785 Prec@5=85.298 rate=2.16 Hz, eta=0:08:30, total=0:10:48, wall=07:48 IST
=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.503 Prec@1=63.785 Prec@5=85.298 rate=2.16 Hz, eta=0:08:30, total=0:10:48, wall=07:49 IST
=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.271 Loss=1.506 Prec@1=63.733 Prec@5=85.248 rate=2.16 Hz, eta=0:08:30, total=0:10:48, wall=07:49 IST
=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.271 Loss=1.506 Prec@1=63.733 Prec@5=85.248 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=07:49 IST
=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.271 Loss=1.506 Prec@1=63.733 Prec@5=85.248 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=07:50 IST
=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.270 Loss=1.507 Prec@1=63.708 Prec@5=85.228 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=07:50 IST
=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.270 Loss=1.507 Prec@1=63.708 Prec@5=85.228 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=07:50 IST
=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.466 DataTime=0.270 Loss=1.507 Prec@1=63.708 Prec@5=85.228 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=07:51 IST
=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.509 Prec@1=63.678 Prec@5=85.203 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=07:51 IST
=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.509 Prec@1=63.678 Prec@5=85.203 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=07:51 IST
=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.467 DataTime=0.271 Loss=1.509 Prec@1=63.678 Prec@5=85.203 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=07:51 IST
=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.510 Prec@1=63.651 Prec@5=85.192 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=07:51 IST
=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.510 Prec@1=63.651 Prec@5=85.192 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=07:51 IST
=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.272 Loss=1.510 Prec@1=63.651 Prec@5=85.192 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=07:52 IST
=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.512 Prec@1=63.608 Prec@5=85.170 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=07:52 IST
=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.512 Prec@1=63.608 Prec@5=85.170 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=07:52 IST
=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.512 Prec@1=63.608 Prec@5=85.170 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=07:53 IST
=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.145 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=07:53 IST
=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.145 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=07:53 IST
=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.145 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=07:54 IST
=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.566 Prec@5=85.125 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=07:54 IST
=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.566 Prec@5=85.125 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=07:54 IST
=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.514 Prec@1=63.566 Prec@5=85.125 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=07:54 IST
=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.515 Prec@1=63.546 Prec@5=85.110 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=07:54 IST
=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.515 Prec@1=63.546 Prec@5=85.110 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=07:54 IST
=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.515 Prec@1=63.546 Prec@5=85.110 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=07:55 IST
=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.516 Prec@1=63.540 Prec@5=85.099 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=07:55 IST
=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.516 Prec@1=63.540 Prec@5=85.099 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=07:55 IST
=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.516 Prec@1=63.540 Prec@5=85.099 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=07:56 IST
=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.517 Prec@1=63.508 Prec@5=85.075 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=07:56 IST
=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.517 Prec@1=63.508 Prec@5=85.075 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=07:56 IST
=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.468 DataTime=0.273 Loss=1.517 Prec@1=63.508 Prec@5=85.075 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=07:57 IST
=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.518 Prec@1=63.476 Prec@5=85.056 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=07:57 IST
=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.518 Prec@1=63.476 Prec@5=85.056 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=07:57 IST
=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.518 Prec@1=63.476 Prec@5=85.056 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=07:57 IST
=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.518 Prec@1=63.474 Prec@5=85.056 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=07:57 IST
=> training   100.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.469 DataTime=0.273 Loss=1.518 Prec@1=63.474 Prec@5=85.056 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=07:57 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:57 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:57 IST
=> validation 0.00% of 1x98...Epoch=48/150 LR=0.07767 Time=6.981 Loss=1.006 Prec@1=73.047 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=07:57 IST
=> validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=6.981 Loss=1.006 Prec@1=73.047 Prec@5=92.969 rate=7483.35 Hz, eta=0:00:00, total=0:00:00, wall=07:57 IST
** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=6.981 Loss=1.006 Prec@1=73.047 Prec@5=92.969 rate=7483.35 Hz, eta=0:00:00, total=0:00:00, wall=07:58 IST
** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=0.547 Loss=1.535 Prec@1=62.764 Prec@5=85.090 rate=7483.35 Hz, eta=0:00:00, total=0:00:00, wall=07:58 IST
** validation 100.00% of 1x98...Epoch=48/150 LR=0.07767 Time=0.547 Loss=1.535 Prec@1=62.764 Prec@5=85.090 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=07:58 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:58 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:58 IST
=> training   0.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=4.522 DataTime=4.236 Loss=1.439 Prec@1=64.648 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=07:58 IST
=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=4.522 DataTime=4.236 Loss=1.439 Prec@1=64.648 Prec@5=86.328 rate=5844.67 Hz, eta=0:00:00, total=0:00:00, wall=07:58 IST
=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=4.522 DataTime=4.236 Loss=1.439 Prec@1=64.648 Prec@5=86.328 rate=5844.67 Hz, eta=0:00:00, total=0:00:00, wall=07:59 IST
=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.503 DataTime=0.310 Loss=1.480 Prec@1=64.103 Prec@5=85.558 rate=5844.67 Hz, eta=0:00:00, total=0:00:00, wall=07:59 IST
=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.503 DataTime=0.310 Loss=1.480 Prec@1=64.103 Prec@5=85.558 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=07:59 IST
=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.503 DataTime=0.310 Loss=1.480 Prec@1=64.103 Prec@5=85.558 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=07:59 IST
=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.488 DataTime=0.294 Loss=1.483 Prec@1=64.079 Prec@5=85.425 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=07:59 IST
=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.488 DataTime=0.294 Loss=1.483 Prec@1=64.079 Prec@5=85.425 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=07:59 IST
=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.488 DataTime=0.294 Loss=1.483 Prec@1=64.079 Prec@5=85.425 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=08:00 IST
=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.478 DataTime=0.283 Loss=1.485 Prec@1=64.052 Prec@5=85.422 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=08:00 IST
=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.478 DataTime=0.283 Loss=1.485 Prec@1=64.052 Prec@5=85.422 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=08:00 IST
=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.478 DataTime=0.283 Loss=1.485 Prec@1=64.052 Prec@5=85.422 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=08:01 IST
=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.474 DataTime=0.278 Loss=1.488 Prec@1=63.952 Prec@5=85.412 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=08:01 IST
=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.474 DataTime=0.278 Loss=1.488 Prec@1=63.952 Prec@5=85.412 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=08:01 IST
=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.474 DataTime=0.278 Loss=1.488 Prec@1=63.952 Prec@5=85.412 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=08:02 IST
=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.470 DataTime=0.275 Loss=1.491 Prec@1=63.927 Prec@5=85.382 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=08:02 IST
=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.470 DataTime=0.275 Loss=1.491 Prec@1=63.927 Prec@5=85.382 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=08:02 IST
=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.470 DataTime=0.275 Loss=1.491 Prec@1=63.927 Prec@5=85.382 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=08:02 IST
=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.865 Prec@5=85.366 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=08:02 IST
=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.865 Prec@5=85.366 rate=2.17 Hz, eta=0:14:38, total=0:04:37, wall=08:02 IST
=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.865 Prec@5=85.366 rate=2.17 Hz, eta=0:14:38, total=0:04:37, wall=08:03 IST
=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.496 Prec@1=63.878 Prec@5=85.360 rate=2.17 Hz, eta=0:14:38, total=0:04:37, wall=08:03 IST
=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.496 Prec@1=63.878 Prec@5=85.360 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=08:03 IST
=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.496 Prec@1=63.878 Prec@5=85.360 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=08:04 IST
=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.876 Prec@5=85.368 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=08:04 IST
=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.876 Prec@5=85.368 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=08:04 IST
=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.876 Prec@5=85.368 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=08:05 IST
=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.881 Prec@5=85.360 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=08:05 IST
=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.881 Prec@5=85.360 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=08:05 IST
=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.495 Prec@1=63.881 Prec@5=85.360 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=08:06 IST
=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.496 Prec@1=63.860 Prec@5=85.342 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=08:06 IST
=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.496 Prec@1=63.860 Prec@5=85.342 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=08:06 IST
=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.496 Prec@1=63.860 Prec@5=85.342 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=08:06 IST
=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.498 Prec@1=63.826 Prec@5=85.317 rate=2.16 Hz, eta=0:11:36, total=0:07:43, wall=08:06 IST
=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.498 Prec@1=63.826 Prec@5=85.317 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=08:06 IST
=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.498 Prec@1=63.826 Prec@5=85.317 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=08:07 IST
=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.498 Prec@1=63.844 Prec@5=85.297 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=08:07 IST
=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.498 Prec@1=63.844 Prec@5=85.297 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=08:07 IST
=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.498 Prec@1=63.844 Prec@5=85.297 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=08:08 IST
=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.829 Prec@5=85.284 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=08:08 IST
=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.829 Prec@5=85.284 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=08:08 IST
=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.829 Prec@5=85.284 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=08:09 IST
=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.501 Prec@1=63.808 Prec@5=85.272 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=08:09 IST
=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.501 Prec@1=63.808 Prec@5=85.272 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=08:09 IST
=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.467 DataTime=0.272 Loss=1.501 Prec@1=63.808 Prec@5=85.272 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=08:09 IST
=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.502 Prec@1=63.783 Prec@5=85.255 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=08:09 IST
=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.502 Prec@1=63.783 Prec@5=85.255 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=08:09 IST
=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.502 Prec@1=63.783 Prec@5=85.255 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=08:10 IST
=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.504 Prec@1=63.758 Prec@5=85.233 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=08:10 IST
=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.504 Prec@1=63.758 Prec@5=85.233 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=08:10 IST
=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.504 Prec@1=63.758 Prec@5=85.233 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=08:11 IST
=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.505 Prec@1=63.745 Prec@5=85.225 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=08:11 IST
=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.505 Prec@1=63.745 Prec@5=85.225 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=08:11 IST
=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.272 Loss=1.505 Prec@1=63.745 Prec@5=85.225 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=08:12 IST
=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.726 Prec@5=85.207 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=08:12 IST
=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.726 Prec@5=85.207 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=08:12 IST
=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.726 Prec@5=85.207 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=08:13 IST
=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.723 Prec@5=85.213 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=08:13 IST
=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.723 Prec@5=85.213 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=08:13 IST
=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.723 Prec@5=85.213 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=08:13 IST
=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.507 Prec@1=63.707 Prec@5=85.189 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=08:13 IST
=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.507 Prec@1=63.707 Prec@5=85.189 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:13 IST
=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.507 Prec@1=63.707 Prec@5=85.189 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:14 IST
=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.508 Prec@1=63.695 Prec@5=85.178 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:14 IST
=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.508 Prec@1=63.695 Prec@5=85.178 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=08:14 IST
=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.274 Loss=1.508 Prec@1=63.695 Prec@5=85.178 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=08:15 IST
=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.508 Prec@1=63.680 Prec@5=85.166 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=08:15 IST
=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.508 Prec@1=63.680 Prec@5=85.166 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:15 IST
=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.508 Prec@1=63.680 Prec@5=85.166 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:16 IST
=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.510 Prec@1=63.650 Prec@5=85.151 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:16 IST
=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.510 Prec@1=63.650 Prec@5=85.151 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:16 IST
=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.510 Prec@1=63.650 Prec@5=85.151 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:17 IST
=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.511 Prec@1=63.622 Prec@5=85.131 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:17 IST
=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.511 Prec@1=63.622 Prec@5=85.131 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=08:17 IST
=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.468 DataTime=0.273 Loss=1.511 Prec@1=63.622 Prec@5=85.131 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=08:17 IST
=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.111 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=08:17 IST
=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.111 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=08:17 IST
=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.594 Prec@5=85.111 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=08:17 IST
=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.592 Prec@5=85.110 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=08:17 IST
=> training   100.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.469 DataTime=0.273 Loss=1.513 Prec@1=63.592 Prec@5=85.110 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=08:17 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:17 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:17 IST
=> validation 0.00% of 1x98...Epoch=49/150 LR=0.07679 Time=6.461 Loss=0.956 Prec@1=76.367 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=08:17 IST
=> validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.461 Loss=0.956 Prec@1=76.367 Prec@5=92.188 rate=7603.40 Hz, eta=0:00:00, total=0:00:00, wall=08:17 IST
** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.461 Loss=0.956 Prec@1=76.367 Prec@5=92.188 rate=7603.40 Hz, eta=0:00:00, total=0:00:00, wall=08:18 IST
** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=0.552 Loss=1.526 Prec@1=63.146 Prec@5=85.312 rate=7603.40 Hz, eta=0:00:00, total=0:00:00, wall=08:18 IST
** validation 100.00% of 1x98...Epoch=49/150 LR=0.07679 Time=0.552 Loss=1.526 Prec@1=63.146 Prec@5=85.312 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=08:18 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:18 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:18 IST
=> training   0.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=4.859 DataTime=4.608 Loss=1.516 Prec@1=61.523 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=08:18 IST
=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=4.859 DataTime=4.608 Loss=1.516 Prec@1=61.523 Prec@5=86.133 rate=8133.92 Hz, eta=0:00:00, total=0:00:00, wall=08:18 IST
=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=4.859 DataTime=4.608 Loss=1.516 Prec@1=61.523 Prec@5=86.133 rate=8133.92 Hz, eta=0:00:00, total=0:00:00, wall=08:19 IST
=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.502 DataTime=0.307 Loss=1.472 Prec@1=64.442 Prec@5=85.756 rate=8133.92 Hz, eta=0:00:00, total=0:00:00, wall=08:19 IST
=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.502 DataTime=0.307 Loss=1.472 Prec@1=64.442 Prec@5=85.756 rate=2.20 Hz, eta=0:18:09, total=0:00:45, wall=08:19 IST
=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.502 DataTime=0.307 Loss=1.472 Prec@1=64.442 Prec@5=85.756 rate=2.20 Hz, eta=0:18:09, total=0:00:45, wall=08:20 IST
=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.481 DataTime=0.286 Loss=1.469 Prec@1=64.500 Prec@5=85.805 rate=2.20 Hz, eta=0:18:09, total=0:00:45, wall=08:20 IST
=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.481 DataTime=0.286 Loss=1.469 Prec@1=64.500 Prec@5=85.805 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=08:20 IST
=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.481 DataTime=0.286 Loss=1.469 Prec@1=64.500 Prec@5=85.805 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=08:21 IST
=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.478 DataTime=0.283 Loss=1.471 Prec@1=64.513 Prec@5=85.691 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=08:21 IST
=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.478 DataTime=0.283 Loss=1.471 Prec@1=64.513 Prec@5=85.691 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=08:21 IST
=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.478 DataTime=0.283 Loss=1.471 Prec@1=64.513 Prec@5=85.691 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=08:21 IST
=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.474 DataTime=0.278 Loss=1.477 Prec@1=64.380 Prec@5=85.594 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=08:21 IST
=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.474 DataTime=0.278 Loss=1.477 Prec@1=64.380 Prec@5=85.594 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=08:21 IST
=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.474 DataTime=0.278 Loss=1.477 Prec@1=64.380 Prec@5=85.594 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=08:22 IST
=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.316 Prec@5=85.587 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=08:22 IST
=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.316 Prec@5=85.587 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=08:22 IST
=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.316 Prec@5=85.587 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=08:23 IST
=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.470 DataTime=0.275 Loss=1.484 Prec@1=64.174 Prec@5=85.505 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=08:23 IST
=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.470 DataTime=0.275 Loss=1.484 Prec@1=64.174 Prec@5=85.505 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=08:23 IST
=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.470 DataTime=0.275 Loss=1.484 Prec@1=64.174 Prec@5=85.505 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=08:24 IST
=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.485 Prec@1=64.166 Prec@5=85.487 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=08:24 IST
=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.485 Prec@1=64.166 Prec@5=85.487 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=08:24 IST
=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.485 Prec@1=64.166 Prec@5=85.487 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=08:25 IST
=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.487 Prec@1=64.106 Prec@5=85.453 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=08:25 IST
=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.487 Prec@1=64.106 Prec@5=85.453 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=08:25 IST
=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.487 Prec@1=64.106 Prec@5=85.453 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=08:25 IST
=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.488 Prec@1=64.084 Prec@5=85.451 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=08:25 IST
=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.488 Prec@1=64.084 Prec@5=85.451 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:25 IST
=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.488 Prec@1=64.084 Prec@5=85.451 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:26 IST
=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.489 Prec@1=64.048 Prec@5=85.450 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:26 IST
=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.489 Prec@1=64.048 Prec@5=85.450 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=08:26 IST
=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.489 Prec@1=64.048 Prec@5=85.450 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=08:27 IST
=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.491 Prec@1=64.012 Prec@5=85.428 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=08:27 IST
=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.491 Prec@1=64.012 Prec@5=85.428 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:27 IST
=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.491 Prec@1=64.012 Prec@5=85.428 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:28 IST
=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=63.984 Prec@5=85.414 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:28 IST
=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=63.984 Prec@5=85.414 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=08:28 IST
=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.492 Prec@1=63.984 Prec@5=85.414 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=08:28 IST
=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.494 Prec@1=63.954 Prec@5=85.372 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=08:28 IST
=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.494 Prec@1=63.954 Prec@5=85.372 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=08:28 IST
=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.494 Prec@1=63.954 Prec@5=85.372 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=08:29 IST
=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.495 Prec@1=63.935 Prec@5=85.365 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=08:29 IST
=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.495 Prec@1=63.935 Prec@5=85.365 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=08:29 IST
=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.272 Loss=1.495 Prec@1=63.935 Prec@5=85.365 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=08:30 IST
=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.902 Prec@5=85.360 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=08:30 IST
=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.902 Prec@5=85.360 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=08:30 IST
=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.496 Prec@1=63.902 Prec@5=85.360 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=08:31 IST
=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.498 Prec@1=63.855 Prec@5=85.324 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=08:31 IST
=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.498 Prec@1=63.855 Prec@5=85.324 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=08:31 IST
=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.498 Prec@1=63.855 Prec@5=85.324 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=08:32 IST
=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.500 Prec@1=63.827 Prec@5=85.290 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=08:32 IST
=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.500 Prec@1=63.827 Prec@5=85.290 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=08:32 IST
=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.500 Prec@1=63.827 Prec@5=85.290 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=08:32 IST
=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.501 Prec@1=63.795 Prec@5=85.262 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=08:32 IST
=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.501 Prec@1=63.795 Prec@5=85.262 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=08:32 IST
=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.501 Prec@1=63.795 Prec@5=85.262 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=08:33 IST
=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.502 Prec@1=63.797 Prec@5=85.246 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=08:33 IST
=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.502 Prec@1=63.797 Prec@5=85.246 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=08:33 IST
=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.502 Prec@1=63.797 Prec@5=85.246 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=08:34 IST
=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.503 Prec@1=63.778 Prec@5=85.234 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=08:34 IST
=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.503 Prec@1=63.778 Prec@5=85.234 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:34 IST
=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.503 Prec@1=63.778 Prec@5=85.234 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:35 IST
=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.504 Prec@1=63.761 Prec@5=85.232 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=08:35 IST
=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.504 Prec@1=63.761 Prec@5=85.232 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=08:35 IST
=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.504 Prec@1=63.761 Prec@5=85.232 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=08:35 IST
=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.505 Prec@1=63.741 Prec@5=85.224 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=08:35 IST
=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.505 Prec@1=63.741 Prec@5=85.224 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:35 IST
=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.505 Prec@1=63.741 Prec@5=85.224 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:36 IST
=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.707 Prec@5=85.207 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=08:36 IST
=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.707 Prec@5=85.207 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:36 IST
=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.469 DataTime=0.273 Loss=1.506 Prec@1=63.707 Prec@5=85.207 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:37 IST
=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.701 Prec@5=85.202 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=08:37 IST
=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.701 Prec@5=85.202 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=08:37 IST
=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.506 Prec@1=63.701 Prec@5=85.202 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=08:38 IST
=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.508 Prec@1=63.665 Prec@5=85.187 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=08:38 IST
=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.508 Prec@1=63.665 Prec@5=85.187 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=08:38 IST
=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.508 Prec@1=63.665 Prec@5=85.187 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=08:38 IST
=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.508 Prec@1=63.666 Prec@5=85.186 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=08:38 IST
=> training   100.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.468 DataTime=0.273 Loss=1.508 Prec@1=63.666 Prec@5=85.186 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=08:38 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:38 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:38 IST
=> validation 0.00% of 1x98...Epoch=50/150 LR=0.07590 Time=7.172 Loss=0.954 Prec@1=75.391 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=08:38 IST
=> validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=7.172 Loss=0.954 Prec@1=75.391 Prec@5=93.164 rate=6234.88 Hz, eta=0:00:00, total=0:00:00, wall=08:38 IST
** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=7.172 Loss=0.954 Prec@1=75.391 Prec@5=93.164 rate=6234.88 Hz, eta=0:00:00, total=0:00:00, wall=08:39 IST
** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=0.553 Loss=1.529 Prec@1=62.998 Prec@5=85.306 rate=6234.88 Hz, eta=0:00:00, total=0:00:00, wall=08:39 IST
** validation 100.00% of 1x98...Epoch=50/150 LR=0.07590 Time=0.553 Loss=1.529 Prec@1=62.998 Prec@5=85.306 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=08:39 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:39 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:39 IST
=> training   0.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=4.682 DataTime=4.461 Loss=1.341 Prec@1=65.820 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=08:39 IST
=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=4.682 DataTime=4.461 Loss=1.341 Prec@1=65.820 Prec@5=89.648 rate=8470.70 Hz, eta=0:00:00, total=0:00:00, wall=08:39 IST
=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=4.682 DataTime=4.461 Loss=1.341 Prec@1=65.820 Prec@5=89.648 rate=8470.70 Hz, eta=0:00:00, total=0:00:00, wall=08:40 IST
=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.501 DataTime=0.305 Loss=1.472 Prec@1=64.482 Prec@5=85.657 rate=8470.70 Hz, eta=0:00:00, total=0:00:00, wall=08:40 IST
=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.501 DataTime=0.305 Loss=1.472 Prec@1=64.482 Prec@5=85.657 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=08:40 IST
=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.501 DataTime=0.305 Loss=1.472 Prec@1=64.482 Prec@5=85.657 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=08:40 IST
=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.482 DataTime=0.287 Loss=1.463 Prec@1=64.674 Prec@5=85.852 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=08:40 IST
=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.482 DataTime=0.287 Loss=1.463 Prec@1=64.674 Prec@5=85.852 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=08:40 IST
=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.482 DataTime=0.287 Loss=1.463 Prec@1=64.674 Prec@5=85.852 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=08:41 IST
=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.475 DataTime=0.279 Loss=1.466 Prec@1=64.584 Prec@5=85.875 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=08:41 IST
=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.475 DataTime=0.279 Loss=1.466 Prec@1=64.584 Prec@5=85.875 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=08:41 IST
=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.475 DataTime=0.279 Loss=1.466 Prec@1=64.584 Prec@5=85.875 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=08:42 IST
=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.473 DataTime=0.278 Loss=1.470 Prec@1=64.470 Prec@5=85.810 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=08:42 IST
=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.473 DataTime=0.278 Loss=1.470 Prec@1=64.470 Prec@5=85.810 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=08:42 IST
=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.473 DataTime=0.278 Loss=1.470 Prec@1=64.470 Prec@5=85.810 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=08:43 IST
=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.473 Prec@1=64.424 Prec@5=85.737 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=08:43 IST
=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.473 Prec@1=64.424 Prec@5=85.737 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=08:43 IST
=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.473 Prec@1=64.424 Prec@5=85.737 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=08:43 IST
=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.477 Prec@1=64.282 Prec@5=85.674 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=08:43 IST
=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.477 Prec@1=64.282 Prec@5=85.674 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=08:43 IST
=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.471 DataTime=0.276 Loss=1.477 Prec@1=64.282 Prec@5=85.674 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=08:44 IST
=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.481 Prec@1=64.254 Prec@5=85.621 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=08:44 IST
=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.481 Prec@1=64.254 Prec@5=85.621 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=08:44 IST
=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.481 Prec@1=64.254 Prec@5=85.621 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=08:45 IST
=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.482 Prec@1=64.223 Prec@5=85.599 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=08:45 IST
=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.482 Prec@1=64.223 Prec@5=85.599 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=08:45 IST
=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.470 DataTime=0.274 Loss=1.482 Prec@1=64.223 Prec@5=85.599 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=08:46 IST
=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.484 Prec@1=64.178 Prec@5=85.554 rate=2.16 Hz, eta=0:13:09, total=0:06:11, wall=08:46 IST
=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.484 Prec@1=64.178 Prec@5=85.554 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:46 IST
=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.484 Prec@1=64.178 Prec@5=85.554 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:47 IST
=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.484 Prec@1=64.157 Prec@5=85.550 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=08:47 IST
=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.484 Prec@1=64.157 Prec@5=85.550 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=08:47 IST
=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.484 Prec@1=64.157 Prec@5=85.550 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=08:47 IST
=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.486 Prec@1=64.131 Prec@5=85.524 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=08:47 IST
=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.486 Prec@1=64.131 Prec@5=85.524 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:47 IST
=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.469 DataTime=0.273 Loss=1.486 Prec@1=64.131 Prec@5=85.524 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:48 IST
=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.487 Prec@1=64.100 Prec@5=85.490 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=08:48 IST
=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.487 Prec@1=64.100 Prec@5=85.490 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=08:48 IST
=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.487 Prec@1=64.100 Prec@5=85.490 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=08:49 IST
=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.074 Prec@5=85.470 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=08:49 IST
=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.074 Prec@5=85.470 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=08:49 IST
=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.074 Prec@5=85.470 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=08:50 IST
=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.071 Prec@5=85.474 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=08:50 IST
=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.071 Prec@5=85.474 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=08:50 IST
=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.489 Prec@1=64.071 Prec@5=85.474 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=08:50 IST
=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.491 Prec@1=64.033 Prec@5=85.442 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=08:50 IST
=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.491 Prec@1=64.033 Prec@5=85.442 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=08:50 IST
=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.491 Prec@1=64.033 Prec@5=85.442 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=08:51 IST
=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.492 Prec@1=64.006 Prec@5=85.418 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=08:51 IST
=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.492 Prec@1=64.006 Prec@5=85.418 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=08:51 IST
=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.492 Prec@1=64.006 Prec@5=85.418 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=08:52 IST
=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.466 DataTime=0.270 Loss=1.494 Prec@1=63.982 Prec@5=85.383 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=08:52 IST
=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.466 DataTime=0.270 Loss=1.494 Prec@1=63.982 Prec@5=85.383 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=08:52 IST
=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.466 DataTime=0.270 Loss=1.494 Prec@1=63.982 Prec@5=85.383 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=08:53 IST
=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.495 Prec@1=63.950 Prec@5=85.368 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=08:53 IST
=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.495 Prec@1=63.950 Prec@5=85.368 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=08:53 IST
=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.495 Prec@1=63.950 Prec@5=85.368 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=08:54 IST
=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.917 Prec@5=85.340 rate=2.16 Hz, eta=0:05:25, total=0:13:55, wall=08:54 IST
=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.917 Prec@5=85.340 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=08:54 IST
=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.917 Prec@5=85.340 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=08:54 IST
=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.912 Prec@5=85.333 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=08:54 IST
=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.912 Prec@5=85.333 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=08:54 IST
=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.497 Prec@1=63.912 Prec@5=85.333 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=08:55 IST
=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.498 Prec@1=63.894 Prec@5=85.315 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=08:55 IST
=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.498 Prec@1=63.894 Prec@5=85.315 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=08:55 IST
=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.498 Prec@1=63.894 Prec@5=85.315 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=08:56 IST
=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.499 Prec@1=63.875 Prec@5=85.309 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=08:56 IST
=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.499 Prec@1=63.875 Prec@5=85.309 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=08:56 IST
=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.467 DataTime=0.271 Loss=1.499 Prec@1=63.875 Prec@5=85.309 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=08:57 IST
=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.861 Prec@5=85.294 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=08:57 IST
=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.861 Prec@5=85.294 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=08:57 IST
=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.861 Prec@5=85.294 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=08:57 IST
=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.857 Prec@5=85.296 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=08:57 IST
=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.857 Prec@5=85.296 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=08:57 IST
=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.500 Prec@1=63.857 Prec@5=85.296 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=08:58 IST
=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.501 Prec@1=63.833 Prec@5=85.279 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=08:58 IST
=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.501 Prec@1=63.833 Prec@5=85.279 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=08:58 IST
=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.501 Prec@1=63.833 Prec@5=85.279 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=08:58 IST
=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.501 Prec@1=63.834 Prec@5=85.279 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=08:58 IST
=> training   100.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.468 DataTime=0.272 Loss=1.501 Prec@1=63.834 Prec@5=85.279 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=08:58 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> validation 0.00% of 1x98...Epoch=51/150 LR=0.07500 Time=6.934 Loss=1.001 Prec@1=73.438 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.934 Loss=1.001 Prec@1=73.438 Prec@5=92.578 rate=7908.76 Hz, eta=0:00:00, total=0:00:00, wall=08:58 IST
** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.934 Loss=1.001 Prec@1=73.438 Prec@5=92.578 rate=7908.76 Hz, eta=0:00:00, total=0:00:00, wall=08:59 IST
** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=0.557 Loss=1.534 Prec@1=63.092 Prec@5=85.212 rate=7908.76 Hz, eta=0:00:00, total=0:00:00, wall=08:59 IST
** validation 100.00% of 1x98...Epoch=51/150 LR=0.07500 Time=0.557 Loss=1.534 Prec@1=63.092 Prec@5=85.212 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=08:59 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:59 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:59 IST
=> training   0.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.763 DataTime=4.510 Loss=1.329 Prec@1=68.359 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=08:59 IST
=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.763 DataTime=4.510 Loss=1.329 Prec@1=68.359 Prec@5=87.109 rate=4658.62 Hz, eta=0:00:00, total=0:00:00, wall=08:59 IST
=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.763 DataTime=4.510 Loss=1.329 Prec@1=68.359 Prec@5=87.109 rate=4658.62 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST
=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.525 DataTime=0.331 Loss=1.465 Prec@1=64.548 Prec@5=85.665 rate=4658.62 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST
=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.525 DataTime=0.331 Loss=1.465 Prec@1=64.548 Prec@5=85.665 rate=2.09 Hz, eta=0:19:08, total=0:00:48, wall=09:00 IST
=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.525 DataTime=0.331 Loss=1.465 Prec@1=64.548 Prec@5=85.665 rate=2.09 Hz, eta=0:19:08, total=0:00:48, wall=09:01 IST
=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.508 DataTime=0.313 Loss=1.465 Prec@1=64.504 Prec@5=85.756 rate=2.09 Hz, eta=0:19:08, total=0:00:48, wall=09:01 IST
=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.508 DataTime=0.313 Loss=1.465 Prec@1=64.504 Prec@5=85.756 rate=2.07 Hz, eta=0:18:34, total=0:01:37, wall=09:01 IST
=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.508 DataTime=0.313 Loss=1.465 Prec@1=64.504 Prec@5=85.756 rate=2.07 Hz, eta=0:18:34, total=0:01:37, wall=09:02 IST
=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.496 DataTime=0.302 Loss=1.468 Prec@1=64.432 Prec@5=85.801 rate=2.07 Hz, eta=0:18:34, total=0:01:37, wall=09:02 IST
=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.496 DataTime=0.302 Loss=1.468 Prec@1=64.432 Prec@5=85.801 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=09:02 IST
=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.496 DataTime=0.302 Loss=1.468 Prec@1=64.432 Prec@5=85.801 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=09:02 IST
=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.491 DataTime=0.296 Loss=1.474 Prec@1=64.291 Prec@5=85.713 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=09:02 IST
=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.491 DataTime=0.296 Loss=1.474 Prec@1=64.291 Prec@5=85.713 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=09:02 IST
=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.491 DataTime=0.296 Loss=1.474 Prec@1=64.291 Prec@5=85.713 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=09:03 IST
=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.488 DataTime=0.293 Loss=1.476 Prec@1=64.244 Prec@5=85.661 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=09:03 IST
=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.488 DataTime=0.293 Loss=1.476 Prec@1=64.244 Prec@5=85.661 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=09:03 IST
=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.488 DataTime=0.293 Loss=1.476 Prec@1=64.244 Prec@5=85.661 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=09:04 IST
=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.288 Loss=1.478 Prec@1=64.240 Prec@5=85.639 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=09:04 IST
=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.288 Loss=1.478 Prec@1=64.240 Prec@5=85.639 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=09:04 IST
=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.288 Loss=1.478 Prec@1=64.240 Prec@5=85.639 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=09:05 IST
=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.289 Loss=1.479 Prec@1=64.253 Prec@5=85.606 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=09:05 IST
=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.289 Loss=1.479 Prec@1=64.253 Prec@5=85.606 rate=2.10 Hz, eta=0:14:18, total=0:05:33, wall=09:05 IST
=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.483 DataTime=0.289 Loss=1.479 Prec@1=64.253 Prec@5=85.606 rate=2.10 Hz, eta=0:14:18, total=0:05:33, wall=09:06 IST
=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.482 DataTime=0.287 Loss=1.479 Prec@1=64.245 Prec@5=85.597 rate=2.10 Hz, eta=0:14:18, total=0:05:33, wall=09:06 IST
=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.482 DataTime=0.287 Loss=1.479 Prec@1=64.245 Prec@5=85.597 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=09:06 IST
=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.482 DataTime=0.287 Loss=1.479 Prec@1=64.245 Prec@5=85.597 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=09:06 IST
=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.480 Prec@1=64.215 Prec@5=85.580 rate=2.10 Hz, eta=0:13:29, total=0:06:21, wall=09:06 IST
=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.480 Prec@1=64.215 Prec@5=85.580 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=09:06 IST
=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.480 Prec@1=64.215 Prec@5=85.580 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=09:07 IST
=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.286 Loss=1.482 Prec@1=64.153 Prec@5=85.549 rate=2.11 Hz, eta=0:12:40, total=0:07:07, wall=09:07 IST
=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.286 Loss=1.482 Prec@1=64.153 Prec@5=85.549 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=09:07 IST
=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.286 Loss=1.482 Prec@1=64.153 Prec@5=85.549 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=09:08 IST
=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.482 Prec@1=64.154 Prec@5=85.544 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=09:08 IST
=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.482 Prec@1=64.154 Prec@5=85.544 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=09:08 IST
=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.480 DataTime=0.285 Loss=1.482 Prec@1=64.154 Prec@5=85.544 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=09:09 IST
=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.479 DataTime=0.284 Loss=1.483 Prec@1=64.139 Prec@5=85.543 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=09:09 IST
=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.479 DataTime=0.284 Loss=1.483 Prec@1=64.139 Prec@5=85.543 rate=2.11 Hz, eta=0:10:18, total=0:09:30, wall=09:09 IST
=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.479 DataTime=0.284 Loss=1.483 Prec@1=64.139 Prec@5=85.543 rate=2.11 Hz, eta=0:10:18, total=0:09:30, wall=09:10 IST
=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.478 DataTime=0.283 Loss=1.483 Prec@1=64.134 Prec@5=85.532 rate=2.11 Hz, eta=0:10:18, total=0:09:30, wall=09:10 IST
=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.478 DataTime=0.283 Loss=1.483 Prec@1=64.134 Prec@5=85.532 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=09:10 IST
=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.478 DataTime=0.283 Loss=1.483 Prec@1=64.134 Prec@5=85.532 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=09:10 IST
=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.283 Loss=1.485 Prec@1=64.111 Prec@5=85.514 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=09:10 IST
=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.283 Loss=1.485 Prec@1=64.111 Prec@5=85.514 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=09:10 IST
=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.283 Loss=1.485 Prec@1=64.111 Prec@5=85.514 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=09:11 IST
=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.485 Prec@1=64.116 Prec@5=85.507 rate=2.11 Hz, eta=0:08:42, total=0:11:04, wall=09:11 IST
=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.485 Prec@1=64.116 Prec@5=85.507 rate=2.11 Hz, eta=0:07:54, total=0:11:50, wall=09:11 IST
=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.485 Prec@1=64.116 Prec@5=85.507 rate=2.11 Hz, eta=0:07:54, total=0:11:50, wall=09:12 IST
=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.487 Prec@1=64.088 Prec@5=85.493 rate=2.11 Hz, eta=0:07:54, total=0:11:50, wall=09:12 IST
=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.487 Prec@1=64.088 Prec@5=85.493 rate=2.11 Hz, eta=0:07:07, total=0:12:38, wall=09:12 IST
=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.487 Prec@1=64.088 Prec@5=85.493 rate=2.11 Hz, eta=0:07:07, total=0:12:38, wall=09:13 IST
=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.056 Prec@5=85.460 rate=2.11 Hz, eta=0:07:07, total=0:12:38, wall=09:13 IST
=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.056 Prec@5=85.460 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=09:13 IST
=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.056 Prec@5=85.460 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=09:14 IST
=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.041 Prec@5=85.449 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=09:14 IST
=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.041 Prec@5=85.449 rate=2.11 Hz, eta=0:05:32, total=0:14:14, wall=09:14 IST
=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.489 Prec@1=64.041 Prec@5=85.449 rate=2.11 Hz, eta=0:05:32, total=0:14:14, wall=09:14 IST
=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.490 Prec@1=64.008 Prec@5=85.433 rate=2.11 Hz, eta=0:05:32, total=0:14:14, wall=09:14 IST
=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.490 Prec@1=64.008 Prec@5=85.433 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=09:14 IST
=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.477 DataTime=0.282 Loss=1.490 Prec@1=64.008 Prec@5=85.433 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=09:15 IST
=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.476 DataTime=0.281 Loss=1.492 Prec@1=63.972 Prec@5=85.408 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=09:15 IST
=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.476 DataTime=0.281 Loss=1.492 Prec@1=63.972 Prec@5=85.408 rate=2.11 Hz, eta=0:03:57, total=0:15:48, wall=09:15 IST
=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.476 DataTime=0.281 Loss=1.492 Prec@1=63.972 Prec@5=85.408 rate=2.11 Hz, eta=0:03:57, total=0:15:48, wall=09:16 IST
=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.494 Prec@1=63.946 Prec@5=85.379 rate=2.11 Hz, eta=0:03:57, total=0:15:48, wall=09:16 IST
=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.494 Prec@1=63.946 Prec@5=85.379 rate=2.11 Hz, eta=0:03:10, total=0:16:33, wall=09:16 IST
=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.494 Prec@1=63.946 Prec@5=85.379 rate=2.11 Hz, eta=0:03:10, total=0:16:33, wall=09:17 IST
=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.495 Prec@1=63.922 Prec@5=85.362 rate=2.11 Hz, eta=0:03:10, total=0:16:33, wall=09:17 IST
=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.495 Prec@1=63.922 Prec@5=85.362 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=09:17 IST
=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.495 Prec@1=63.922 Prec@5=85.362 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=09:17 IST
=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.496 Prec@1=63.895 Prec@5=85.343 rate=2.11 Hz, eta=0:02:22, total=0:17:21, wall=09:17 IST
=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.496 Prec@1=63.895 Prec@5=85.343 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=09:17 IST
=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.496 Prec@1=63.895 Prec@5=85.343 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=09:18 IST
=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.898 Prec@5=85.334 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=09:18 IST
=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.898 Prec@5=85.334 rate=2.11 Hz, eta=0:00:48, total=0:18:55, wall=09:18 IST
=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.898 Prec@5=85.334 rate=2.11 Hz, eta=0:00:48, total=0:18:55, wall=09:19 IST
=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.880 Prec@5=85.325 rate=2.11 Hz, eta=0:00:48, total=0:18:55, wall=09:19 IST
=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.880 Prec@5=85.325 rate=2.11 Hz, eta=0:00:00, total=0:19:42, wall=09:19 IST
=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.880 Prec@5=85.325 rate=2.11 Hz, eta=0:00:00, total=0:19:42, wall=09:19 IST
=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.878 Prec@5=85.325 rate=2.11 Hz, eta=0:00:00, total=0:19:42, wall=09:19 IST
=> training   100.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.475 DataTime=0.280 Loss=1.497 Prec@1=63.878 Prec@5=85.325 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=09:19 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:19 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:19 IST
=> validation 0.00% of 1x98...Epoch=52/150 LR=0.07409 Time=6.942 Loss=0.958 Prec@1=73.828 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=09:19 IST
=> validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.942 Loss=0.958 Prec@1=73.828 Prec@5=93.359 rate=10015.52 Hz, eta=0:00:00, total=0:00:00, wall=09:19 IST
** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.942 Loss=0.958 Prec@1=73.828 Prec@5=93.359 rate=10015.52 Hz, eta=0:00:00, total=0:00:00, wall=09:20 IST
** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=0.556 Loss=1.520 Prec@1=63.254 Prec@5=85.366 rate=10015.52 Hz, eta=0:00:00, total=0:00:00, wall=09:20 IST
** validation 100.00% of 1x98...Epoch=52/150 LR=0.07409 Time=0.556 Loss=1.520 Prec@1=63.254 Prec@5=85.366 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=09:20 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST
=> training   0.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.682 DataTime=4.403 Loss=1.444 Prec@1=64.844 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST
=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.682 DataTime=4.403 Loss=1.444 Prec@1=64.844 Prec@5=85.742 rate=5290.28 Hz, eta=0:00:00, total=0:00:00, wall=09:20 IST
=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.682 DataTime=4.403 Loss=1.444 Prec@1=64.844 Prec@5=85.742 rate=5290.28 Hz, eta=0:00:00, total=0:00:00, wall=09:21 IST
=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.502 DataTime=0.306 Loss=1.453 Prec@1=64.919 Prec@5=85.856 rate=5290.28 Hz, eta=0:00:00, total=0:00:00, wall=09:21 IST
=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.502 DataTime=0.306 Loss=1.453 Prec@1=64.919 Prec@5=85.856 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=09:21 IST
=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.502 DataTime=0.306 Loss=1.453 Prec@1=64.919 Prec@5=85.856 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=09:22 IST
=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.486 DataTime=0.290 Loss=1.456 Prec@1=64.945 Prec@5=85.968 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=09:22 IST
=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.486 DataTime=0.290 Loss=1.456 Prec@1=64.945 Prec@5=85.968 rate=2.16 Hz, eta=0:17:44, total=0:01:32, wall=09:22 IST
=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.486 DataTime=0.290 Loss=1.456 Prec@1=64.945 Prec@5=85.968 rate=2.16 Hz, eta=0:17:44, total=0:01:32, wall=09:22 IST
=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.480 DataTime=0.284 Loss=1.458 Prec@1=64.805 Prec@5=85.948 rate=2.16 Hz, eta=0:17:44, total=0:01:32, wall=09:22 IST
=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.480 DataTime=0.284 Loss=1.458 Prec@1=64.805 Prec@5=85.948 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=09:22 IST
=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.480 DataTime=0.284 Loss=1.458 Prec@1=64.805 Prec@5=85.948 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=09:23 IST
=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.476 DataTime=0.281 Loss=1.462 Prec@1=64.732 Prec@5=85.891 rate=2.15 Hz, eta=0:17:02, total=0:02:19, wall=09:23 IST
=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.476 DataTime=0.281 Loss=1.462 Prec@1=64.732 Prec@5=85.891 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=09:23 IST
=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.476 DataTime=0.281 Loss=1.462 Prec@1=64.732 Prec@5=85.891 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=09:24 IST
=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.477 DataTime=0.281 Loss=1.464 Prec@1=64.703 Prec@5=85.825 rate=2.15 Hz, eta=0:16:17, total=0:03:06, wall=09:24 IST
=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.477 DataTime=0.281 Loss=1.464 Prec@1=64.703 Prec@5=85.825 rate=2.14 Hz, eta=0:15:35, total=0:03:54, wall=09:24 IST
=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.477 DataTime=0.281 Loss=1.464 Prec@1=64.703 Prec@5=85.825 rate=2.14 Hz, eta=0:15:35, total=0:03:54, wall=09:25 IST
=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.475 DataTime=0.279 Loss=1.464 Prec@1=64.670 Prec@5=85.825 rate=2.14 Hz, eta=0:15:35, total=0:03:54, wall=09:25 IST
=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.475 DataTime=0.279 Loss=1.464 Prec@1=64.670 Prec@5=85.825 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=09:25 IST
=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.475 DataTime=0.279 Loss=1.464 Prec@1=64.670 Prec@5=85.825 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=09:25 IST
=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.610 Prec@5=85.808 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=09:25 IST
=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.610 Prec@5=85.808 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=09:25 IST
=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.610 Prec@5=85.808 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=09:26 IST
=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.532 Prec@5=85.749 rate=2.14 Hz, eta=0:14:00, total=0:05:26, wall=09:26 IST
=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.532 Prec@5=85.749 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=09:26 IST
=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.532 Prec@5=85.749 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=09:27 IST
=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.474 Prec@1=64.450 Prec@5=85.708 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=09:27 IST
=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.474 Prec@1=64.450 Prec@5=85.708 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=09:27 IST
=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.277 Loss=1.474 Prec@1=64.450 Prec@5=85.708 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=09:28 IST
=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.278 Loss=1.474 Prec@1=64.456 Prec@5=85.697 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=09:28 IST
=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.278 Loss=1.474 Prec@1=64.456 Prec@5=85.697 rate=2.13 Hz, eta=0:11:43, total=0:07:49, wall=09:28 IST
=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.473 DataTime=0.278 Loss=1.474 Prec@1=64.456 Prec@5=85.697 rate=2.13 Hz, eta=0:11:43, total=0:07:49, wall=09:29 IST
=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.476 Prec@1=64.395 Prec@5=85.665 rate=2.13 Hz, eta=0:11:43, total=0:07:49, wall=09:29 IST
=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.476 Prec@1=64.395 Prec@5=85.665 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=09:29 IST
=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.476 Prec@1=64.395 Prec@5=85.665 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=09:29 IST
=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.478 Prec@1=64.333 Prec@5=85.650 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=09:29 IST
=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.478 Prec@1=64.333 Prec@5=85.650 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=09:29 IST
=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.478 Prec@1=64.333 Prec@5=85.650 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=09:30 IST
=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.479 Prec@1=64.304 Prec@5=85.627 rate=2.14 Hz, eta=0:10:09, total=0:09:21, wall=09:30 IST
=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.479 Prec@1=64.304 Prec@5=85.627 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=09:30 IST
=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.479 Prec@1=64.304 Prec@5=85.627 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=09:31 IST
=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.268 Prec@5=85.606 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=09:31 IST
=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.268 Prec@5=85.606 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=09:31 IST
=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.480 Prec@1=64.268 Prec@5=85.606 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=09:32 IST
=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.471 DataTime=0.276 Loss=1.481 Prec@1=64.233 Prec@5=85.587 rate=2.14 Hz, eta=0:08:36, total=0:10:56, wall=09:32 IST
=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.471 DataTime=0.276 Loss=1.481 Prec@1=64.233 Prec@5=85.587 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=09:32 IST
=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.471 DataTime=0.276 Loss=1.481 Prec@1=64.233 Prec@5=85.587 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=09:33 IST
=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.483 Prec@1=64.211 Prec@5=85.559 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=09:33 IST
=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.483 Prec@1=64.211 Prec@5=85.559 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=09:33 IST
=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.483 Prec@1=64.211 Prec@5=85.559 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=09:33 IST
=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.485 Prec@1=64.179 Prec@5=85.525 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=09:33 IST
=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.485 Prec@1=64.179 Prec@5=85.525 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=09:33 IST
=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.485 Prec@1=64.179 Prec@5=85.525 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=09:34 IST
=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.486 Prec@1=64.152 Prec@5=85.500 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=09:34 IST
=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.486 Prec@1=64.152 Prec@5=85.500 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=09:34 IST
=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.486 Prec@1=64.152 Prec@5=85.500 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=09:35 IST
=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.487 Prec@1=64.135 Prec@5=85.498 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=09:35 IST
=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.487 Prec@1=64.135 Prec@5=85.498 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=09:35 IST
=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.487 Prec@1=64.135 Prec@5=85.498 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=09:36 IST
=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.487 Prec@1=64.121 Prec@5=85.494 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=09:36 IST
=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.487 Prec@1=64.121 Prec@5=85.494 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=09:36 IST
=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.487 Prec@1=64.121 Prec@5=85.494 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=09:36 IST
=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.488 Prec@1=64.103 Prec@5=85.482 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=09:36 IST
=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.488 Prec@1=64.103 Prec@5=85.482 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=09:36 IST
=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.488 Prec@1=64.103 Prec@5=85.482 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=09:37 IST
=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.489 Prec@1=64.087 Prec@5=85.470 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=09:37 IST
=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.489 Prec@1=64.087 Prec@5=85.470 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=09:37 IST
=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.489 Prec@1=64.087 Prec@5=85.470 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=09:38 IST
=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.489 Prec@1=64.073 Prec@5=85.461 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=09:38 IST
=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.489 Prec@1=64.073 Prec@5=85.461 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=09:38 IST
=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.489 Prec@1=64.073 Prec@5=85.461 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=09:39 IST
=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.490 Prec@1=64.055 Prec@5=85.450 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=09:39 IST
=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.490 Prec@1=64.055 Prec@5=85.450 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=09:39 IST
=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.277 Loss=1.490 Prec@1=64.055 Prec@5=85.450 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=09:40 IST
=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.491 Prec@1=64.040 Prec@5=85.437 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=09:40 IST
=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.491 Prec@1=64.040 Prec@5=85.437 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=09:40 IST
=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.491 Prec@1=64.040 Prec@5=85.437 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=09:40 IST
=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.491 Prec@1=64.039 Prec@5=85.435 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=09:40 IST
=> training   100.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.472 DataTime=0.276 Loss=1.491 Prec@1=64.039 Prec@5=85.435 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=09:40 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:40 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:40 IST
=> validation 0.00% of 1x98...Epoch=53/150 LR=0.07316 Time=6.141 Loss=0.845 Prec@1=77.344 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=09:40 IST
=> validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=6.141 Loss=0.845 Prec@1=77.344 Prec@5=95.117 rate=4213.95 Hz, eta=0:00:00, total=0:00:00, wall=09:40 IST
** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=6.141 Loss=0.845 Prec@1=77.344 Prec@5=95.117 rate=4213.95 Hz, eta=0:00:00, total=0:00:00, wall=09:41 IST
** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=0.544 Loss=1.494 Prec@1=63.962 Prec@5=85.732 rate=4213.95 Hz, eta=0:00:00, total=0:00:00, wall=09:41 IST
** validation 100.00% of 1x98...Epoch=53/150 LR=0.07316 Time=0.544 Loss=1.494 Prec@1=63.962 Prec@5=85.732 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=09:41 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:41 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:41 IST
=> training   0.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.265 DataTime=5.007 Loss=1.486 Prec@1=62.695 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=09:41 IST
=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.265 DataTime=5.007 Loss=1.486 Prec@1=62.695 Prec@5=84.766 rate=7595.38 Hz, eta=0:00:00, total=0:00:00, wall=09:41 IST
=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.265 DataTime=5.007 Loss=1.486 Prec@1=62.695 Prec@5=84.766 rate=7595.38 Hz, eta=0:00:00, total=0:00:00, wall=09:41 IST
=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.506 DataTime=0.309 Loss=1.450 Prec@1=64.720 Prec@5=85.961 rate=7595.38 Hz, eta=0:00:00, total=0:00:00, wall=09:41 IST
=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.506 DataTime=0.309 Loss=1.450 Prec@1=64.720 Prec@5=85.961 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=09:41 IST
=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.506 DataTime=0.309 Loss=1.450 Prec@1=64.720 Prec@5=85.961 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=09:42 IST
=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.483 DataTime=0.286 Loss=1.454 Prec@1=64.709 Prec@5=85.889 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=09:42 IST
=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.483 DataTime=0.286 Loss=1.454 Prec@1=64.709 Prec@5=85.889 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=09:42 IST
=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.483 DataTime=0.286 Loss=1.454 Prec@1=64.709 Prec@5=85.889 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=09:43 IST
=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.478 DataTime=0.281 Loss=1.456 Prec@1=64.715 Prec@5=85.830 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=09:43 IST
=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.478 DataTime=0.281 Loss=1.456 Prec@1=64.715 Prec@5=85.830 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=09:43 IST
=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.478 DataTime=0.281 Loss=1.456 Prec@1=64.715 Prec@5=85.830 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=09:44 IST
=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.474 DataTime=0.277 Loss=1.463 Prec@1=64.580 Prec@5=85.776 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=09:44 IST
=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.474 DataTime=0.277 Loss=1.463 Prec@1=64.580 Prec@5=85.776 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=09:44 IST
=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.474 DataTime=0.277 Loss=1.463 Prec@1=64.580 Prec@5=85.776 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=09:45 IST
=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.275 Loss=1.463 Prec@1=64.568 Prec@5=85.796 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=09:45 IST
=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.275 Loss=1.463 Prec@1=64.568 Prec@5=85.796 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=09:45 IST
=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.275 Loss=1.463 Prec@1=64.568 Prec@5=85.796 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=09:45 IST
=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.274 Loss=1.464 Prec@1=64.545 Prec@5=85.794 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=09:45 IST
=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.274 Loss=1.464 Prec@1=64.545 Prec@5=85.794 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=09:45 IST
=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.472 DataTime=0.274 Loss=1.464 Prec@1=64.545 Prec@5=85.794 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=09:46 IST
=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.471 DataTime=0.274 Loss=1.465 Prec@1=64.516 Prec@5=85.785 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=09:46 IST
=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.471 DataTime=0.274 Loss=1.465 Prec@1=64.516 Prec@5=85.785 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=09:46 IST
=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.471 DataTime=0.274 Loss=1.465 Prec@1=64.516 Prec@5=85.785 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=09:47 IST
=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.470 DataTime=0.273 Loss=1.465 Prec@1=64.524 Prec@5=85.777 rate=2.16 Hz, eta=0:13:55, total=0:05:24, wall=09:47 IST
=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.470 DataTime=0.273 Loss=1.465 Prec@1=64.524 Prec@5=85.777 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=09:47 IST
=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.470 DataTime=0.273 Loss=1.465 Prec@1=64.524 Prec@5=85.777 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=09:48 IST
=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.468 Prec@1=64.468 Prec@5=85.733 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=09:48 IST
=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.468 Prec@1=64.468 Prec@5=85.733 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=09:48 IST
=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.468 Prec@1=64.468 Prec@5=85.733 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=09:48 IST
=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.470 Prec@1=64.419 Prec@5=85.706 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=09:48 IST
=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.470 Prec@1=64.419 Prec@5=85.706 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=09:48 IST
=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.470 Prec@1=64.419 Prec@5=85.706 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=09:49 IST
=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.471 Prec@1=64.375 Prec@5=85.688 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=09:49 IST
=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.471 Prec@1=64.375 Prec@5=85.688 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=09:49 IST
=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.471 Prec@1=64.375 Prec@5=85.688 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=09:50 IST
=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.473 Prec@1=64.342 Prec@5=85.665 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=09:50 IST
=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.473 Prec@1=64.342 Prec@5=85.665 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=09:50 IST
=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.473 Prec@1=64.342 Prec@5=85.665 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=09:51 IST
=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.474 Prec@1=64.346 Prec@5=85.656 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=09:51 IST
=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.474 Prec@1=64.346 Prec@5=85.656 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=09:51 IST
=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.474 Prec@1=64.346 Prec@5=85.656 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=09:52 IST
=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.475 Prec@1=64.311 Prec@5=85.648 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=09:52 IST
=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.475 Prec@1=64.311 Prec@5=85.648 rate=2.16 Hz, eta=0:08:31, total=0:10:50, wall=09:52 IST
=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.475 Prec@1=64.311 Prec@5=85.648 rate=2.16 Hz, eta=0:08:31, total=0:10:50, wall=09:52 IST
=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.476 Prec@1=64.294 Prec@5=85.634 rate=2.16 Hz, eta=0:08:31, total=0:10:50, wall=09:52 IST
=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.476 Prec@1=64.294 Prec@5=85.634 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=09:52 IST
=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.476 Prec@1=64.294 Prec@5=85.634 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=09:53 IST
=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.477 Prec@1=64.267 Prec@5=85.619 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=09:53 IST
=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.477 Prec@1=64.267 Prec@5=85.619 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=09:53 IST
=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.468 DataTime=0.272 Loss=1.477 Prec@1=64.267 Prec@5=85.619 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=09:54 IST
=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.477 Prec@1=64.271 Prec@5=85.617 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=09:54 IST
=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.477 Prec@1=64.271 Prec@5=85.617 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=09:54 IST
=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.477 Prec@1=64.271 Prec@5=85.617 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=09:55 IST
=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.478 Prec@1=64.260 Prec@5=85.591 rate=2.16 Hz, eta=0:06:12, total=0:13:09, wall=09:55 IST
=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.478 Prec@1=64.260 Prec@5=85.591 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=09:55 IST
=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.478 Prec@1=64.260 Prec@5=85.591 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=09:55 IST
=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.479 Prec@1=64.247 Prec@5=85.580 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=09:55 IST
=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.479 Prec@1=64.247 Prec@5=85.580 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=09:55 IST
=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.479 Prec@1=64.247 Prec@5=85.580 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=09:56 IST
=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.480 Prec@1=64.233 Prec@5=85.565 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=09:56 IST
=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.480 Prec@1=64.233 Prec@5=85.565 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=09:56 IST
=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.480 Prec@1=64.233 Prec@5=85.565 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=09:57 IST
=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.481 Prec@1=64.224 Prec@5=85.550 rate=2.16 Hz, eta=0:03:52, total=0:15:28, wall=09:57 IST
=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.481 Prec@1=64.224 Prec@5=85.550 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=09:57 IST
=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.271 Loss=1.481 Prec@1=64.224 Prec@5=85.550 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=09:58 IST
=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.482 Prec@1=64.177 Prec@5=85.528 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=09:58 IST
=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.482 Prec@1=64.177 Prec@5=85.528 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=09:58 IST
=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.467 DataTime=0.270 Loss=1.482 Prec@1=64.177 Prec@5=85.528 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=09:58 IST
=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.168 Prec@5=85.523 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=09:58 IST
=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.168 Prec@5=85.523 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=09:58 IST
=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.168 Prec@5=85.523 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=09:59 IST
=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.153 Prec@5=85.521 rate=2.15 Hz, eta=0:01:33, total=0:17:47, wall=09:59 IST
=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.153 Prec@5=85.521 rate=2.16 Hz, eta=0:00:47, total=0:18:34, wall=09:59 IST
=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.483 Prec@1=64.153 Prec@5=85.521 rate=2.16 Hz, eta=0:00:47, total=0:18:34, wall=10:00 IST
=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.484 Prec@1=64.136 Prec@5=85.511 rate=2.16 Hz, eta=0:00:47, total=0:18:34, wall=10:00 IST
=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.484 Prec@1=64.136 Prec@5=85.511 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=10:00 IST
=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.484 Prec@1=64.136 Prec@5=85.511 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=10:00 IST
=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.484 Prec@1=64.135 Prec@5=85.511 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=10:00 IST
=> training   100.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.466 DataTime=0.270 Loss=1.484 Prec@1=64.135 Prec@5=85.511 rate=2.16 Hz, eta=0:00:00, total=0:19:20, wall=10:00 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:00 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:00 IST
=> validation 0.00% of 1x98...Epoch=54/150 LR=0.07223 Time=6.822 Loss=0.988 Prec@1=74.219 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=10:00 IST
=> validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=6.822 Loss=0.988 Prec@1=74.219 Prec@5=92.578 rate=7741.02 Hz, eta=0:00:00, total=0:00:00, wall=10:00 IST
** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=6.822 Loss=0.988 Prec@1=74.219 Prec@5=92.578 rate=7741.02 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST
** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=0.543 Loss=1.515 Prec@1=63.136 Prec@5=85.512 rate=7741.02 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST
** validation 100.00% of 1x98...Epoch=54/150 LR=0.07223 Time=0.543 Loss=1.515 Prec@1=63.136 Prec@5=85.512 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=10:01 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST
=> training   0.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=4.906 DataTime=4.649 Loss=1.455 Prec@1=64.844 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST
=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=4.906 DataTime=4.649 Loss=1.455 Prec@1=64.844 Prec@5=84.766 rate=7799.03 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST
=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=4.906 DataTime=4.649 Loss=1.455 Prec@1=64.844 Prec@5=84.766 rate=7799.03 Hz, eta=0:00:00, total=0:00:00, wall=10:02 IST
=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.498 DataTime=0.304 Loss=1.457 Prec@1=64.581 Prec@5=85.932 rate=7799.03 Hz, eta=0:00:00, total=0:00:00, wall=10:02 IST
=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.498 DataTime=0.304 Loss=1.457 Prec@1=64.581 Prec@5=85.932 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=10:02 IST
=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.498 DataTime=0.304 Loss=1.457 Prec@1=64.581 Prec@5=85.932 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=10:03 IST
=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.479 DataTime=0.284 Loss=1.452 Prec@1=64.651 Prec@5=86.028 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=10:03 IST
=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.479 DataTime=0.284 Loss=1.452 Prec@1=64.651 Prec@5=86.028 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=10:03 IST
=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.479 DataTime=0.284 Loss=1.452 Prec@1=64.651 Prec@5=86.028 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=10:03 IST
=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.451 Prec@1=64.743 Prec@5=86.022 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=10:03 IST
=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.451 Prec@1=64.743 Prec@5=86.022 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=10:03 IST
=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.451 Prec@1=64.743 Prec@5=86.022 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=10:04 IST
=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.477 DataTime=0.281 Loss=1.454 Prec@1=64.680 Prec@5=86.016 rate=2.18 Hz, eta=0:16:50, total=0:02:18, wall=10:04 IST
=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.477 DataTime=0.281 Loss=1.454 Prec@1=64.680 Prec@5=86.016 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=10:04 IST
=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.477 DataTime=0.281 Loss=1.454 Prec@1=64.680 Prec@5=86.016 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=10:05 IST
=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.455 Prec@1=64.710 Prec@5=85.990 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=10:05 IST
=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.455 Prec@1=64.710 Prec@5=85.990 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=10:05 IST
=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.475 DataTime=0.279 Loss=1.455 Prec@1=64.710 Prec@5=85.990 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=10:06 IST
=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.718 Prec@5=85.967 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=10:06 IST
=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.718 Prec@5=85.967 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=10:06 IST
=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.718 Prec@5=85.967 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=10:06 IST
=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.689 Prec@5=85.940 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=10:06 IST
=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.689 Prec@5=85.940 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=10:06 IST
=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.689 Prec@5=85.940 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=10:07 IST
=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.457 Prec@1=64.683 Prec@5=85.954 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=10:07 IST
=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.457 Prec@1=64.683 Prec@5=85.954 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=10:07 IST
=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.457 Prec@1=64.683 Prec@5=85.954 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=10:08 IST
=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.459 Prec@1=64.634 Prec@5=85.910 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=10:08 IST
=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.459 Prec@1=64.634 Prec@5=85.910 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=10:08 IST
=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.459 Prec@1=64.634 Prec@5=85.910 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=10:09 IST
=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.464 Prec@1=64.549 Prec@5=85.826 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=10:09 IST
=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.464 Prec@1=64.549 Prec@5=85.826 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=10:09 IST
=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.464 Prec@1=64.549 Prec@5=85.826 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=10:10 IST
=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.465 Prec@1=64.507 Prec@5=85.804 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=10:10 IST
=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.465 Prec@1=64.507 Prec@5=85.804 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=10:10 IST
=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.465 Prec@1=64.507 Prec@5=85.804 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=10:10 IST
=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.466 Prec@1=64.492 Prec@5=85.782 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=10:10 IST
=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.466 Prec@1=64.492 Prec@5=85.782 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:10 IST
=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.273 Loss=1.466 Prec@1=64.492 Prec@5=85.782 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:11 IST
=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.467 Prec@1=64.464 Prec@5=85.752 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:11 IST
=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.467 Prec@1=64.464 Prec@5=85.752 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=10:11 IST
=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.469 DataTime=0.273 Loss=1.467 Prec@1=64.464 Prec@5=85.752 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=10:12 IST
=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.468 Prec@1=64.462 Prec@5=85.749 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=10:12 IST
=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.468 Prec@1=64.462 Prec@5=85.749 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=10:12 IST
=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.468 Prec@1=64.462 Prec@5=85.749 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=10:13 IST
=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.470 Prec@1=64.422 Prec@5=85.722 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=10:13 IST
=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.470 Prec@1=64.422 Prec@5=85.722 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=10:13 IST
=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.470 Prec@1=64.422 Prec@5=85.722 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=10:13 IST
=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.407 Prec@5=85.706 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=10:13 IST
=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.407 Prec@5=85.706 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=10:13 IST
=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.407 Prec@5=85.706 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=10:14 IST
=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.403 Prec@5=85.707 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=10:14 IST
=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.403 Prec@5=85.707 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=10:14 IST
=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.470 DataTime=0.274 Loss=1.471 Prec@1=64.403 Prec@5=85.707 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=10:15 IST
=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.472 Prec@1=64.400 Prec@5=85.698 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=10:15 IST
=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.472 Prec@1=64.400 Prec@5=85.698 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=10:15 IST
=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.472 Prec@1=64.400 Prec@5=85.698 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=10:16 IST
=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.473 Prec@1=64.386 Prec@5=85.687 rate=2.14 Hz, eta=0:05:28, total=0:14:03, wall=10:16 IST
=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.473 Prec@1=64.386 Prec@5=85.687 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=10:16 IST
=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.473 Prec@1=64.386 Prec@5=85.687 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=10:17 IST
=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.474 Prec@1=64.363 Prec@5=85.669 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=10:17 IST
=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.474 Prec@1=64.363 Prec@5=85.669 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=10:17 IST
=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.474 Prec@1=64.363 Prec@5=85.669 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=10:17 IST
=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.275 Loss=1.476 Prec@1=64.324 Prec@5=85.645 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=10:17 IST
=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.275 Loss=1.476 Prec@1=64.324 Prec@5=85.645 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=10:17 IST
=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.275 Loss=1.476 Prec@1=64.324 Prec@5=85.645 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=10:18 IST
=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.476 Prec@1=64.315 Prec@5=85.636 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=10:18 IST
=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.476 Prec@1=64.315 Prec@5=85.636 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=10:18 IST
=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.476 Prec@1=64.315 Prec@5=85.636 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=10:19 IST
=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.477 Prec@1=64.296 Prec@5=85.627 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=10:19 IST
=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.477 Prec@1=64.296 Prec@5=85.627 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=10:19 IST
=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.477 Prec@1=64.296 Prec@5=85.627 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=10:20 IST
=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.478 Prec@1=64.287 Prec@5=85.622 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=10:20 IST
=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.478 Prec@1=64.287 Prec@5=85.622 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=10:20 IST
=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.472 DataTime=0.276 Loss=1.478 Prec@1=64.287 Prec@5=85.622 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=10:21 IST
=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.479 Prec@1=64.269 Prec@5=85.608 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=10:21 IST
=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.479 Prec@1=64.269 Prec@5=85.608 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=10:21 IST
=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.479 Prec@1=64.269 Prec@5=85.608 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=10:21 IST
=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.479 Prec@1=64.267 Prec@5=85.606 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=10:21 IST
=> training   100.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.471 DataTime=0.275 Loss=1.479 Prec@1=64.267 Prec@5=85.606 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=10:21 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:21 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:21 IST
=> validation 0.00% of 1x98...Epoch=55/150 LR=0.07129 Time=6.922 Loss=0.950 Prec@1=75.195 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=10:21 IST
=> validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.922 Loss=0.950 Prec@1=75.195 Prec@5=92.578 rate=5798.55 Hz, eta=0:00:00, total=0:00:00, wall=10:21 IST
** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.922 Loss=0.950 Prec@1=75.195 Prec@5=92.578 rate=5798.55 Hz, eta=0:00:00, total=0:00:00, wall=10:21 IST
** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=0.551 Loss=1.480 Prec@1=64.058 Prec@5=85.914 rate=5798.55 Hz, eta=0:00:00, total=0:00:00, wall=10:21 IST
** validation 100.00% of 1x98...Epoch=55/150 LR=0.07129 Time=0.551 Loss=1.480 Prec@1=64.058 Prec@5=85.914 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=10:21 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> training   0.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.088 DataTime=4.751 Loss=1.513 Prec@1=61.914 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.088 DataTime=4.751 Loss=1.513 Prec@1=61.914 Prec@5=83.789 rate=9979.94 Hz, eta=0:00:00, total=0:00:00, wall=10:22 IST
=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.088 DataTime=4.751 Loss=1.513 Prec@1=61.914 Prec@5=83.789 rate=9979.94 Hz, eta=0:00:00, total=0:00:00, wall=10:22 IST
=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.513 DataTime=0.320 Loss=1.445 Prec@1=65.031 Prec@5=86.189 rate=9979.94 Hz, eta=0:00:00, total=0:00:00, wall=10:22 IST
=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.513 DataTime=0.320 Loss=1.445 Prec@1=65.031 Prec@5=86.189 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=10:22 IST
=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.513 DataTime=0.320 Loss=1.445 Prec@1=65.031 Prec@5=86.189 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=10:23 IST
=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.487 DataTime=0.292 Loss=1.444 Prec@1=64.880 Prec@5=86.190 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=10:23 IST
=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.487 DataTime=0.292 Loss=1.444 Prec@1=64.880 Prec@5=86.190 rate=2.17 Hz, eta=0:17:43, total=0:01:32, wall=10:23 IST
=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.487 DataTime=0.292 Loss=1.444 Prec@1=64.880 Prec@5=86.190 rate=2.17 Hz, eta=0:17:43, total=0:01:32, wall=10:24 IST
=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.482 DataTime=0.286 Loss=1.447 Prec@1=64.832 Prec@5=86.155 rate=2.17 Hz, eta=0:17:43, total=0:01:32, wall=10:24 IST
=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.482 DataTime=0.286 Loss=1.447 Prec@1=64.832 Prec@5=86.155 rate=2.15 Hz, eta=0:17:04, total=0:02:19, wall=10:24 IST
=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.482 DataTime=0.286 Loss=1.447 Prec@1=64.832 Prec@5=86.155 rate=2.15 Hz, eta=0:17:04, total=0:02:19, wall=10:25 IST
=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.477 DataTime=0.281 Loss=1.446 Prec@1=64.907 Prec@5=86.136 rate=2.15 Hz, eta=0:17:04, total=0:02:19, wall=10:25 IST
=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.477 DataTime=0.281 Loss=1.446 Prec@1=64.907 Prec@5=86.136 rate=2.15 Hz, eta=0:16:15, total=0:03:06, wall=10:25 IST
=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.477 DataTime=0.281 Loss=1.446 Prec@1=64.907 Prec@5=86.136 rate=2.15 Hz, eta=0:16:15, total=0:03:06, wall=10:25 IST
=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.278 Loss=1.448 Prec@1=64.886 Prec@5=86.089 rate=2.15 Hz, eta=0:16:15, total=0:03:06, wall=10:25 IST
=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.278 Loss=1.448 Prec@1=64.886 Prec@5=86.089 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=10:25 IST
=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.278 Loss=1.448 Prec@1=64.886 Prec@5=86.089 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=10:26 IST
=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.279 Loss=1.450 Prec@1=64.862 Prec@5=86.050 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=10:26 IST
=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.279 Loss=1.450 Prec@1=64.862 Prec@5=86.050 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=10:26 IST
=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.475 DataTime=0.279 Loss=1.450 Prec@1=64.862 Prec@5=86.050 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=10:27 IST
=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.450 Prec@1=64.881 Prec@5=86.025 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=10:27 IST
=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.450 Prec@1=64.881 Prec@5=86.025 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=10:27 IST
=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.450 Prec@1=64.881 Prec@5=86.025 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=10:28 IST
=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.279 Loss=1.451 Prec@1=64.846 Prec@5=85.988 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=10:28 IST
=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.279 Loss=1.451 Prec@1=64.846 Prec@5=85.988 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=10:28 IST
=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.279 Loss=1.451 Prec@1=64.846 Prec@5=85.988 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=10:29 IST
=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.454 Prec@1=64.790 Prec@5=85.927 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=10:29 IST
=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.454 Prec@1=64.790 Prec@5=85.927 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=10:29 IST
=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.454 Prec@1=64.790 Prec@5=85.927 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=10:29 IST
=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.762 Prec@5=85.905 rate=2.13 Hz, eta=0:12:30, total=0:07:02, wall=10:29 IST
=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.762 Prec@5=85.905 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=10:29 IST
=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.456 Prec@1=64.762 Prec@5=85.905 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=10:30 IST
=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.733 Prec@5=85.907 rate=2.14 Hz, eta=0:11:43, total=0:07:48, wall=10:30 IST
=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.733 Prec@5=85.907 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=10:30 IST
=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.276 Loss=1.457 Prec@1=64.733 Prec@5=85.907 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=10:31 IST
=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.460 Prec@1=64.678 Prec@5=85.872 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=10:31 IST
=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.460 Prec@1=64.678 Prec@5=85.872 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=10:31 IST
=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.460 Prec@1=64.678 Prec@5=85.872 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=10:32 IST
=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.461 Prec@1=64.643 Prec@5=85.856 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=10:32 IST
=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.461 Prec@1=64.643 Prec@5=85.856 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=10:32 IST
=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.461 Prec@1=64.643 Prec@5=85.856 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=10:33 IST
=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.462 Prec@1=64.603 Prec@5=85.834 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=10:33 IST
=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.462 Prec@1=64.603 Prec@5=85.834 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=10:33 IST
=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.474 DataTime=0.278 Loss=1.462 Prec@1=64.603 Prec@5=85.834 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=10:33 IST
=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.278 Loss=1.464 Prec@1=64.580 Prec@5=85.820 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=10:33 IST
=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.278 Loss=1.464 Prec@1=64.580 Prec@5=85.820 rate=2.13 Hz, eta=0:07:50, total=0:11:45, wall=10:33 IST
=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.278 Loss=1.464 Prec@1=64.580 Prec@5=85.820 rate=2.13 Hz, eta=0:07:50, total=0:11:45, wall=10:34 IST
=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.465 Prec@1=64.549 Prec@5=85.806 rate=2.13 Hz, eta=0:07:50, total=0:11:45, wall=10:34 IST
=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.465 Prec@1=64.549 Prec@5=85.806 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=10:34 IST
=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.465 Prec@1=64.549 Prec@5=85.806 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=10:35 IST
=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.521 Prec@5=85.784 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=10:35 IST
=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.521 Prec@5=85.784 rate=2.13 Hz, eta=0:06:16, total=0:13:19, wall=10:35 IST
=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.466 Prec@1=64.521 Prec@5=85.784 rate=2.13 Hz, eta=0:06:16, total=0:13:19, wall=10:36 IST
=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.467 Prec@1=64.511 Prec@5=85.765 rate=2.13 Hz, eta=0:06:16, total=0:13:19, wall=10:36 IST
=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.467 Prec@1=64.511 Prec@5=85.765 rate=2.13 Hz, eta=0:05:29, total=0:14:06, wall=10:36 IST
=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.473 DataTime=0.277 Loss=1.467 Prec@1=64.511 Prec@5=85.765 rate=2.13 Hz, eta=0:05:29, total=0:14:06, wall=10:36 IST
=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.468 Prec@1=64.492 Prec@5=85.758 rate=2.13 Hz, eta=0:05:29, total=0:14:06, wall=10:36 IST
=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.468 Prec@1=64.492 Prec@5=85.758 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=10:36 IST
=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.468 Prec@1=64.492 Prec@5=85.758 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=10:37 IST
=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.469 Prec@1=64.462 Prec@5=85.738 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=10:37 IST
=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.469 Prec@1=64.462 Prec@5=85.738 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=10:37 IST
=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.469 Prec@1=64.462 Prec@5=85.738 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=10:38 IST
=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.460 Prec@5=85.723 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=10:38 IST
=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.460 Prec@5=85.723 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=10:38 IST
=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.470 Prec@1=64.460 Prec@5=85.723 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=10:39 IST
=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.444 Prec@5=85.709 rate=2.13 Hz, eta=0:03:08, total=0:16:27, wall=10:39 IST
=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.444 Prec@5=85.709 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=10:39 IST
=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.444 Prec@5=85.709 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=10:40 IST
=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.438 Prec@5=85.697 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=10:40 IST
=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.438 Prec@5=85.697 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=10:40 IST
=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.471 Prec@1=64.438 Prec@5=85.697 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=10:40 IST
=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.412 Prec@5=85.683 rate=2.13 Hz, eta=0:01:34, total=0:18:01, wall=10:40 IST
=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.412 Prec@5=85.683 rate=2.13 Hz, eta=0:00:47, total=0:18:49, wall=10:40 IST
=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.412 Prec@5=85.683 rate=2.13 Hz, eta=0:00:47, total=0:18:49, wall=10:41 IST
=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.397 Prec@5=85.670 rate=2.13 Hz, eta=0:00:47, total=0:18:49, wall=10:41 IST
=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.397 Prec@5=85.670 rate=2.13 Hz, eta=0:00:00, total=0:19:36, wall=10:41 IST
=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.397 Prec@5=85.670 rate=2.13 Hz, eta=0:00:00, total=0:19:36, wall=10:41 IST
=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.399 Prec@5=85.670 rate=2.13 Hz, eta=0:00:00, total=0:19:36, wall=10:41 IST
=> training   100.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.472 DataTime=0.277 Loss=1.473 Prec@1=64.399 Prec@5=85.670 rate=2.13 Hz, eta=0:00:00, total=0:19:37, wall=10:41 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST
=> validation 0.00% of 1x98...Epoch=56/150 LR=0.07034 Time=6.300 Loss=0.992 Prec@1=73.828 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST
=> validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.300 Loss=0.992 Prec@1=73.828 Prec@5=91.992 rate=9181.72 Hz, eta=0:00:00, total=0:00:00, wall=10:41 IST
** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.300 Loss=0.992 Prec@1=73.828 Prec@5=91.992 rate=9181.72 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST
** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=0.550 Loss=1.520 Prec@1=63.180 Prec@5=85.476 rate=9181.72 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST
** validation 100.00% of 1x98...Epoch=56/150 LR=0.07034 Time=0.550 Loss=1.520 Prec@1=63.180 Prec@5=85.476 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=10:42 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST
=> training   0.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.666 DataTime=4.425 Loss=1.354 Prec@1=66.797 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST
=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.666 DataTime=4.425 Loss=1.354 Prec@1=66.797 Prec@5=86.719 rate=8050.43 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST
=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.666 DataTime=4.425 Loss=1.354 Prec@1=66.797 Prec@5=86.719 rate=8050.43 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST
=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.504 DataTime=0.306 Loss=1.427 Prec@1=65.395 Prec@5=86.324 rate=8050.43 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST
=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.504 DataTime=0.306 Loss=1.427 Prec@1=65.395 Prec@5=86.324 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=10:43 IST
=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.504 DataTime=0.306 Loss=1.427 Prec@1=65.395 Prec@5=86.324 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=10:44 IST
=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.481 DataTime=0.284 Loss=1.429 Prec@1=65.259 Prec@5=86.342 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=10:44 IST
=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.481 DataTime=0.284 Loss=1.429 Prec@1=65.259 Prec@5=86.342 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=10:44 IST
=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.481 DataTime=0.284 Loss=1.429 Prec@1=65.259 Prec@5=86.342 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=10:45 IST
=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.478 DataTime=0.282 Loss=1.435 Prec@1=65.149 Prec@5=86.233 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=10:45 IST
=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.478 DataTime=0.282 Loss=1.435 Prec@1=65.149 Prec@5=86.233 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=10:45 IST
=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.478 DataTime=0.282 Loss=1.435 Prec@1=65.149 Prec@5=86.233 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=10:45 IST
=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.475 DataTime=0.279 Loss=1.439 Prec@1=65.125 Prec@5=86.183 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=10:45 IST
=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.475 DataTime=0.279 Loss=1.439 Prec@1=65.125 Prec@5=86.183 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=10:45 IST
=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.475 DataTime=0.279 Loss=1.439 Prec@1=65.125 Prec@5=86.183 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=10:46 IST
=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.441 Prec@1=65.065 Prec@5=86.145 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=10:46 IST
=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.441 Prec@1=65.065 Prec@5=86.145 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=10:46 IST
=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.441 Prec@1=65.065 Prec@5=86.145 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=10:47 IST
=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.041 Prec@5=86.121 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=10:47 IST
=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.041 Prec@5=86.121 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=10:47 IST
=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.041 Prec@5=86.121 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=10:48 IST
=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.444 Prec@1=65.005 Prec@5=86.055 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=10:48 IST
=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.444 Prec@1=65.005 Prec@5=86.055 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=10:48 IST
=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.444 Prec@1=65.005 Prec@5=86.055 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=10:48 IST
=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.447 Prec@1=64.947 Prec@5=86.024 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=10:48 IST
=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.447 Prec@1=64.947 Prec@5=86.024 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=10:48 IST
=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.447 Prec@1=64.947 Prec@5=86.024 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=10:49 IST
=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.451 Prec@1=64.873 Prec@5=85.998 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=10:49 IST
=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.451 Prec@1=64.873 Prec@5=85.998 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=10:49 IST
=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.451 Prec@1=64.873 Prec@5=85.998 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=10:50 IST
=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.453 Prec@1=64.833 Prec@5=85.942 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=10:50 IST
=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.453 Prec@1=64.833 Prec@5=85.942 rate=2.15 Hz, eta=0:11:40, total=0:07:46, wall=10:50 IST
=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.453 Prec@1=64.833 Prec@5=85.942 rate=2.15 Hz, eta=0:11:40, total=0:07:46, wall=10:51 IST
=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.454 Prec@1=64.798 Prec@5=85.923 rate=2.15 Hz, eta=0:11:40, total=0:07:46, wall=10:51 IST
=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.454 Prec@1=64.798 Prec@5=85.923 rate=2.15 Hz, eta=0:10:53, total=0:08:32, wall=10:51 IST
=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.454 Prec@1=64.798 Prec@5=85.923 rate=2.15 Hz, eta=0:10:53, total=0:08:32, wall=10:52 IST
=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.455 Prec@1=64.765 Prec@5=85.898 rate=2.15 Hz, eta=0:10:53, total=0:08:32, wall=10:52 IST
=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.455 Prec@1=64.765 Prec@5=85.898 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:52 IST
=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.455 Prec@1=64.765 Prec@5=85.898 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:52 IST
=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.456 Prec@1=64.740 Prec@5=85.885 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=10:52 IST
=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.456 Prec@1=64.740 Prec@5=85.885 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=10:52 IST
=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.456 Prec@1=64.740 Prec@5=85.885 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=10:53 IST
=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.457 Prec@1=64.706 Prec@5=85.871 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=10:53 IST
=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.457 Prec@1=64.706 Prec@5=85.871 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=10:53 IST
=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.457 Prec@1=64.706 Prec@5=85.871 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=10:54 IST
=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.459 Prec@1=64.682 Prec@5=85.859 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=10:54 IST
=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.459 Prec@1=64.682 Prec@5=85.859 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=10:54 IST
=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.459 Prec@1=64.682 Prec@5=85.859 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=10:55 IST
=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.460 Prec@1=64.659 Prec@5=85.845 rate=2.14 Hz, eta=0:07:48, total=0:11:42, wall=10:55 IST
=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.460 Prec@1=64.659 Prec@5=85.845 rate=2.13 Hz, eta=0:07:02, total=0:12:29, wall=10:55 IST
=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.460 Prec@1=64.659 Prec@5=85.845 rate=2.13 Hz, eta=0:07:02, total=0:12:29, wall=10:56 IST
=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.461 Prec@1=64.626 Prec@5=85.841 rate=2.13 Hz, eta=0:07:02, total=0:12:29, wall=10:56 IST
=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.461 Prec@1=64.626 Prec@5=85.841 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=10:56 IST
=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.461 Prec@1=64.626 Prec@5=85.841 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=10:56 IST
=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.462 Prec@1=64.595 Prec@5=85.824 rate=2.14 Hz, eta=0:06:15, total=0:13:16, wall=10:56 IST
=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.462 Prec@1=64.595 Prec@5=85.824 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=10:56 IST
=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.275 Loss=1.462 Prec@1=64.595 Prec@5=85.824 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=10:57 IST
=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.463 Prec@1=64.587 Prec@5=85.814 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=10:57 IST
=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.463 Prec@1=64.587 Prec@5=85.814 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=10:57 IST
=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.472 DataTime=0.276 Loss=1.463 Prec@1=64.587 Prec@5=85.814 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=10:58 IST
=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.464 Prec@1=64.582 Prec@5=85.799 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=10:58 IST
=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.464 Prec@1=64.582 Prec@5=85.799 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=10:58 IST
=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.464 Prec@1=64.582 Prec@5=85.799 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=10:59 IST
=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.565 Prec@5=85.803 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=10:59 IST
=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.565 Prec@5=85.803 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=10:59 IST
=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.565 Prec@5=85.803 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=10:59 IST
=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.551 Prec@5=85.789 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=10:59 IST
=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.551 Prec@5=85.789 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=10:59 IST
=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.465 Prec@1=64.551 Prec@5=85.789 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=11:00 IST
=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.466 Prec@1=64.545 Prec@5=85.790 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=11:00 IST
=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.466 Prec@1=64.545 Prec@5=85.790 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=11:00 IST
=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.471 DataTime=0.276 Loss=1.466 Prec@1=64.545 Prec@5=85.790 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=11:01 IST
=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.527 Prec@5=85.774 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=11:01 IST
=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.527 Prec@5=85.774 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=11:01 IST
=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.527 Prec@5=85.774 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=11:02 IST
=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.520 Prec@5=85.768 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=11:02 IST
=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.520 Prec@5=85.768 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=11:02 IST
=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.520 Prec@5=85.768 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=11:02 IST
=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.520 Prec@5=85.767 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=11:02 IST
=> training   100.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.470 DataTime=0.275 Loss=1.467 Prec@1=64.520 Prec@5=85.767 rate=2.14 Hz, eta=0:00:00, total=0:19:32, wall=11:02 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:02 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:02 IST
=> validation 0.00% of 1x98...Epoch=57/150 LR=0.06938 Time=6.822 Loss=0.969 Prec@1=74.219 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=11:02 IST
=> validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=6.822 Loss=0.969 Prec@1=74.219 Prec@5=93.750 rate=7750.61 Hz, eta=0:00:00, total=0:00:00, wall=11:02 IST
** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=6.822 Loss=0.969 Prec@1=74.219 Prec@5=93.750 rate=7750.61 Hz, eta=0:00:00, total=0:00:00, wall=11:03 IST
** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=0.549 Loss=1.508 Prec@1=63.428 Prec@5=85.664 rate=7750.61 Hz, eta=0:00:00, total=0:00:00, wall=11:03 IST
** validation 100.00% of 1x98...Epoch=57/150 LR=0.06938 Time=0.549 Loss=1.508 Prec@1=63.428 Prec@5=85.664 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=11:03 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:03 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:03 IST
=> training   0.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.105 DataTime=4.885 Loss=1.467 Prec@1=65.039 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=11:03 IST
=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.105 DataTime=4.885 Loss=1.467 Prec@1=65.039 Prec@5=83.984 rate=8327.36 Hz, eta=0:00:00, total=0:00:00, wall=11:03 IST
=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.105 DataTime=4.885 Loss=1.467 Prec@1=65.039 Prec@5=83.984 rate=8327.36 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST
=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.505 DataTime=0.313 Loss=1.411 Prec@1=65.635 Prec@5=86.487 rate=8327.36 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST
=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.505 DataTime=0.313 Loss=1.411 Prec@1=65.635 Prec@5=86.487 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=11:04 IST
=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.505 DataTime=0.313 Loss=1.411 Prec@1=65.635 Prec@5=86.487 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=11:04 IST
=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.482 DataTime=0.288 Loss=1.422 Prec@1=65.448 Prec@5=86.385 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=11:04 IST
=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.482 DataTime=0.288 Loss=1.422 Prec@1=65.448 Prec@5=86.385 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=11:04 IST
=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.482 DataTime=0.288 Loss=1.422 Prec@1=65.448 Prec@5=86.385 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=11:05 IST
=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.477 DataTime=0.282 Loss=1.434 Prec@1=65.182 Prec@5=86.218 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=11:05 IST
=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.477 DataTime=0.282 Loss=1.434 Prec@1=65.182 Prec@5=86.218 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:05 IST
=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.477 DataTime=0.282 Loss=1.434 Prec@1=65.182 Prec@5=86.218 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:06 IST
=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.474 DataTime=0.278 Loss=1.433 Prec@1=65.267 Prec@5=86.204 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:06 IST
=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.474 DataTime=0.278 Loss=1.433 Prec@1=65.267 Prec@5=86.204 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=11:06 IST
=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.474 DataTime=0.278 Loss=1.433 Prec@1=65.267 Prec@5=86.204 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=11:07 IST
=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.437 Prec@1=65.179 Prec@5=86.140 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=11:07 IST
=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.437 Prec@1=65.179 Prec@5=86.140 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=11:07 IST
=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.437 Prec@1=65.179 Prec@5=86.140 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=11:07 IST
=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.473 DataTime=0.276 Loss=1.438 Prec@1=65.132 Prec@5=86.121 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=11:07 IST
=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.473 DataTime=0.276 Loss=1.438 Prec@1=65.132 Prec@5=86.121 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=11:07 IST
=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.473 DataTime=0.276 Loss=1.438 Prec@1=65.132 Prec@5=86.121 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=11:08 IST
=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.441 Prec@1=65.090 Prec@5=86.075 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=11:08 IST
=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.441 Prec@1=65.090 Prec@5=86.075 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=11:08 IST
=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.441 Prec@1=65.090 Prec@5=86.075 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=11:09 IST
=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.440 Prec@1=65.097 Prec@5=86.089 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=11:09 IST
=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.440 Prec@1=65.097 Prec@5=86.089 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=11:09 IST
=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.275 Loss=1.440 Prec@1=65.097 Prec@5=86.089 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=11:10 IST
=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.276 Loss=1.442 Prec@1=65.060 Prec@5=86.081 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=11:10 IST
=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.276 Loss=1.442 Prec@1=65.060 Prec@5=86.081 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=11:10 IST
=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.472 DataTime=0.276 Loss=1.442 Prec@1=65.060 Prec@5=86.081 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=11:11 IST
=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.471 DataTime=0.275 Loss=1.445 Prec@1=65.002 Prec@5=86.045 rate=2.14 Hz, eta=0:12:27, total=0:07:00, wall=11:11 IST
=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.471 DataTime=0.275 Loss=1.445 Prec@1=65.002 Prec@5=86.045 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:11 IST
=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.471 DataTime=0.275 Loss=1.445 Prec@1=65.002 Prec@5=86.045 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:11 IST
=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.022 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:11 IST
=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.022 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:11 IST
=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.022 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:12 IST
=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.273 Loss=1.447 Prec@1=64.957 Prec@5=86.034 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:12 IST
=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.273 Loss=1.447 Prec@1=64.957 Prec@5=86.034 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=11:12 IST
=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.273 Loss=1.447 Prec@1=64.957 Prec@5=86.034 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=11:13 IST
=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.028 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=11:13 IST
=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.028 rate=2.15 Hz, eta=0:09:20, total=0:10:06, wall=11:13 IST
=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.959 Prec@5=86.028 rate=2.15 Hz, eta=0:09:20, total=0:10:06, wall=11:14 IST
=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.449 Prec@1=64.914 Prec@5=86.001 rate=2.15 Hz, eta=0:09:20, total=0:10:06, wall=11:14 IST
=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.449 Prec@1=64.914 Prec@5=86.001 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=11:14 IST
=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.470 DataTime=0.274 Loss=1.449 Prec@1=64.914 Prec@5=86.001 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=11:14 IST
=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.450 Prec@1=64.874 Prec@5=85.986 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=11:14 IST
=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.450 Prec@1=64.874 Prec@5=85.986 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=11:14 IST
=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.450 Prec@1=64.874 Prec@5=85.986 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=11:15 IST
=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.452 Prec@1=64.841 Prec@5=85.957 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=11:15 IST
=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.452 Prec@1=64.841 Prec@5=85.957 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:15 IST
=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.452 Prec@1=64.841 Prec@5=85.957 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:16 IST
=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.453 Prec@1=64.822 Prec@5=85.944 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:16 IST
=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.453 Prec@1=64.822 Prec@5=85.944 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=11:16 IST
=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.453 Prec@1=64.822 Prec@5=85.944 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=11:17 IST
=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.454 Prec@1=64.802 Prec@5=85.935 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=11:17 IST
=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.454 Prec@1=64.802 Prec@5=85.935 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=11:17 IST
=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.273 Loss=1.454 Prec@1=64.802 Prec@5=85.935 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=11:18 IST
=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.272 Loss=1.455 Prec@1=64.781 Prec@5=85.920 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=11:18 IST
=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.272 Loss=1.455 Prec@1=64.781 Prec@5=85.920 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=11:18 IST
=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.469 DataTime=0.272 Loss=1.455 Prec@1=64.781 Prec@5=85.920 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=11:18 IST
=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.747 Prec@5=85.901 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=11:18 IST
=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.747 Prec@5=85.901 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:18 IST
=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.747 Prec@5=85.901 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:19 IST
=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.740 Prec@5=85.895 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:19 IST
=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.740 Prec@5=85.895 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=11:19 IST
=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.456 Prec@1=64.740 Prec@5=85.895 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=11:20 IST
=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.458 Prec@1=64.723 Prec@5=85.873 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=11:20 IST
=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.458 Prec@1=64.723 Prec@5=85.873 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:20 IST
=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.458 Prec@1=64.723 Prec@5=85.873 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:21 IST
=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.459 Prec@1=64.687 Prec@5=85.860 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:21 IST
=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.459 Prec@1=64.687 Prec@5=85.860 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=11:21 IST
=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.459 Prec@1=64.687 Prec@5=85.860 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=11:21 IST
=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.460 Prec@1=64.664 Prec@5=85.840 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=11:21 IST
=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.460 Prec@1=64.664 Prec@5=85.840 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=11:21 IST
=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.460 Prec@1=64.664 Prec@5=85.840 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=11:22 IST
=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.461 Prec@1=64.637 Prec@5=85.830 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=11:22 IST
=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.461 Prec@1=64.637 Prec@5=85.830 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=11:22 IST
=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.461 Prec@1=64.637 Prec@5=85.830 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=11:22 IST
=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.461 Prec@1=64.636 Prec@5=85.828 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=11:22 IST
=> training   100.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.468 DataTime=0.272 Loss=1.461 Prec@1=64.636 Prec@5=85.828 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=11:22 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:22 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:22 IST
=> validation 0.00% of 1x98...Epoch=58/150 LR=0.06841 Time=7.183 Loss=0.935 Prec@1=74.805 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=11:22 IST
=> validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=7.183 Loss=0.935 Prec@1=74.805 Prec@5=93.164 rate=7428.98 Hz, eta=0:00:00, total=0:00:00, wall=11:22 IST
** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=7.183 Loss=0.935 Prec@1=74.805 Prec@5=93.164 rate=7428.98 Hz, eta=0:00:00, total=0:00:00, wall=11:23 IST
** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=0.547 Loss=1.507 Prec@1=63.522 Prec@5=85.572 rate=7428.98 Hz, eta=0:00:00, total=0:00:00, wall=11:23 IST
** validation 100.00% of 1x98...Epoch=58/150 LR=0.06841 Time=0.547 Loss=1.507 Prec@1=63.522 Prec@5=85.572 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=11:23 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST
=> training   0.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.787 DataTime=4.549 Loss=1.497 Prec@1=65.625 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST
=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.787 DataTime=4.549 Loss=1.497 Prec@1=65.625 Prec@5=83.203 rate=9722.42 Hz, eta=0:00:00, total=0:00:00, wall=11:23 IST
=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.787 DataTime=4.549 Loss=1.497 Prec@1=65.625 Prec@5=83.203 rate=9722.42 Hz, eta=0:00:00, total=0:00:00, wall=11:24 IST
=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.495 DataTime=0.299 Loss=1.437 Prec@1=65.167 Prec@5=86.218 rate=9722.42 Hz, eta=0:00:00, total=0:00:00, wall=11:24 IST
=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.495 DataTime=0.299 Loss=1.437 Prec@1=65.167 Prec@5=86.218 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=11:24 IST
=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.495 DataTime=0.299 Loss=1.437 Prec@1=65.167 Prec@5=86.218 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=11:25 IST
=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.480 DataTime=0.284 Loss=1.427 Prec@1=65.459 Prec@5=86.314 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=11:25 IST
=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.480 DataTime=0.284 Loss=1.427 Prec@1=65.459 Prec@5=86.314 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=11:25 IST
=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.480 DataTime=0.284 Loss=1.427 Prec@1=65.459 Prec@5=86.314 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=11:26 IST
=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.476 DataTime=0.280 Loss=1.431 Prec@1=65.293 Prec@5=86.281 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=11:26 IST
=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.476 DataTime=0.280 Loss=1.431 Prec@1=65.293 Prec@5=86.281 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:26 IST
=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.476 DataTime=0.280 Loss=1.431 Prec@1=65.293 Prec@5=86.281 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:26 IST
=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.276 Prec@5=86.232 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=11:26 IST
=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.276 Prec@5=86.232 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=11:26 IST
=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.276 Prec@5=86.232 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=11:27 IST
=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.473 DataTime=0.277 Loss=1.434 Prec@1=65.235 Prec@5=86.241 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=11:27 IST
=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.473 DataTime=0.277 Loss=1.434 Prec@1=65.235 Prec@5=86.241 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=11:27 IST
=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.473 DataTime=0.277 Loss=1.434 Prec@1=65.235 Prec@5=86.241 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=11:28 IST
=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.222 Prec@5=86.201 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=11:28 IST
=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.222 Prec@5=86.201 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=11:28 IST
=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.434 Prec@1=65.222 Prec@5=86.201 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=11:29 IST
=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.437 Prec@1=65.151 Prec@5=86.150 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=11:29 IST
=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.437 Prec@1=65.151 Prec@5=86.150 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=11:29 IST
=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.437 Prec@1=65.151 Prec@5=86.150 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=11:29 IST
=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.101 Prec@5=86.127 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=11:29 IST
=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.101 Prec@5=86.127 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=11:29 IST
=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.101 Prec@5=86.127 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=11:30 IST
=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.471 DataTime=0.275 Loss=1.441 Prec@1=65.066 Prec@5=86.115 rate=2.15 Hz, eta=0:13:12, total=0:06:13, wall=11:30 IST
=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.471 DataTime=0.275 Loss=1.441 Prec@1=65.066 Prec@5=86.115 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=11:30 IST
=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.471 DataTime=0.275 Loss=1.441 Prec@1=65.066 Prec@5=86.115 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=11:31 IST
=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=65.023 Prec@5=86.079 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=11:31 IST
=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=65.023 Prec@5=86.079 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:31 IST
=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=65.023 Prec@5=86.079 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:32 IST
=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.445 Prec@1=65.001 Prec@5=86.066 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=11:32 IST
=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.445 Prec@1=65.001 Prec@5=86.066 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:32 IST
=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.445 Prec@1=65.001 Prec@5=86.066 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:33 IST
=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.446 Prec@1=64.962 Prec@5=86.045 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=11:33 IST
=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.446 Prec@1=64.962 Prec@5=86.045 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=11:33 IST
=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.446 Prec@1=64.962 Prec@5=86.045 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=11:33 IST
=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.939 Prec@5=86.032 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=11:33 IST
=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.939 Prec@5=86.032 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=11:33 IST
=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.939 Prec@5=86.032 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=11:34 IST
=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.940 Prec@5=86.034 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=11:34 IST
=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.940 Prec@5=86.034 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=11:34 IST
=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.940 Prec@5=86.034 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=11:35 IST
=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.943 Prec@5=86.044 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=11:35 IST
=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.943 Prec@5=86.044 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=11:35 IST
=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.470 DataTime=0.274 Loss=1.447 Prec@1=64.943 Prec@5=86.044 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=11:36 IST
=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.904 Prec@5=86.012 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=11:36 IST
=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.904 Prec@5=86.012 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=11:36 IST
=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.904 Prec@5=86.012 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=11:36 IST
=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.449 Prec@1=64.887 Prec@5=86.008 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=11:36 IST
=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.449 Prec@1=64.887 Prec@5=86.008 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=11:36 IST
=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.449 Prec@1=64.887 Prec@5=86.008 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=11:37 IST
=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.875 Prec@5=85.999 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=11:37 IST
=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.875 Prec@5=85.999 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=11:37 IST
=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.875 Prec@5=85.999 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=11:38 IST
=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.854 Prec@5=85.993 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=11:38 IST
=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.854 Prec@5=85.993 rate=2.14 Hz, eta=0:04:40, total=0:14:47, wall=11:38 IST
=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.450 Prec@1=64.854 Prec@5=85.993 rate=2.14 Hz, eta=0:04:40, total=0:14:47, wall=11:39 IST
=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.451 Prec@1=64.849 Prec@5=85.981 rate=2.14 Hz, eta=0:04:40, total=0:14:47, wall=11:39 IST
=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.451 Prec@1=64.849 Prec@5=85.981 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=11:39 IST
=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.451 Prec@1=64.849 Prec@5=85.981 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=11:40 IST
=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.452 Prec@1=64.823 Prec@5=85.959 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=11:40 IST
=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.452 Prec@1=64.823 Prec@5=85.959 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=11:40 IST
=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.452 Prec@1=64.823 Prec@5=85.959 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=11:40 IST
=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.453 Prec@1=64.790 Prec@5=85.947 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=11:40 IST
=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.453 Prec@1=64.790 Prec@5=85.947 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=11:40 IST
=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.274 Loss=1.453 Prec@1=64.790 Prec@5=85.947 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=11:41 IST
=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.762 Prec@5=85.925 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=11:41 IST
=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.762 Prec@5=85.925 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=11:41 IST
=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.762 Prec@5=85.925 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=11:42 IST
=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.753 Prec@5=85.927 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=11:42 IST
=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.753 Prec@5=85.927 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=11:42 IST
=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.469 DataTime=0.273 Loss=1.455 Prec@1=64.753 Prec@5=85.927 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=11:43 IST
=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.468 DataTime=0.273 Loss=1.455 Prec@1=64.750 Prec@5=85.912 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=11:43 IST
=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.468 DataTime=0.273 Loss=1.455 Prec@1=64.750 Prec@5=85.912 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=11:43 IST
=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.468 DataTime=0.273 Loss=1.455 Prec@1=64.750 Prec@5=85.912 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=11:43 IST
=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.468 DataTime=0.273 Loss=1.455 Prec@1=64.751 Prec@5=85.913 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=11:43 IST
=> training   100.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.468 DataTime=0.273 Loss=1.455 Prec@1=64.751 Prec@5=85.913 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=11:43 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST
=> validation 0.00% of 1x98...Epoch=59/150 LR=0.06743 Time=6.871 Loss=0.930 Prec@1=78.320 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST
=> validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.871 Loss=0.930 Prec@1=78.320 Prec@5=93.164 rate=7512.30 Hz, eta=0:00:00, total=0:00:00, wall=11:43 IST
** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.871 Loss=0.930 Prec@1=78.320 Prec@5=93.164 rate=7512.30 Hz, eta=0:00:00, total=0:00:00, wall=11:44 IST
** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=0.551 Loss=1.501 Prec@1=63.804 Prec@5=85.876 rate=7512.30 Hz, eta=0:00:00, total=0:00:00, wall=11:44 IST
** validation 100.00% of 1x98...Epoch=59/150 LR=0.06743 Time=0.551 Loss=1.501 Prec@1=63.804 Prec@5=85.876 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=11:44 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:44 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:44 IST
=> training   0.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=4.780 DataTime=4.543 Loss=1.482 Prec@1=64.062 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=11:44 IST
=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=4.780 DataTime=4.543 Loss=1.482 Prec@1=64.062 Prec@5=85.156 rate=6392.02 Hz, eta=0:00:00, total=0:00:00, wall=11:44 IST
=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=4.780 DataTime=4.543 Loss=1.482 Prec@1=64.062 Prec@5=85.156 rate=6392.02 Hz, eta=0:00:00, total=0:00:00, wall=11:44 IST
=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.499 DataTime=0.304 Loss=1.402 Prec@1=65.772 Prec@5=86.597 rate=6392.02 Hz, eta=0:00:00, total=0:00:00, wall=11:44 IST
=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.499 DataTime=0.304 Loss=1.402 Prec@1=65.772 Prec@5=86.597 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=11:44 IST
=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.499 DataTime=0.304 Loss=1.402 Prec@1=65.772 Prec@5=86.597 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=11:45 IST
=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.485 DataTime=0.289 Loss=1.408 Prec@1=65.594 Prec@5=86.560 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=11:45 IST
=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.485 DataTime=0.289 Loss=1.408 Prec@1=65.594 Prec@5=86.560 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=11:45 IST
=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.485 DataTime=0.289 Loss=1.408 Prec@1=65.594 Prec@5=86.560 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=11:46 IST
=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.483 DataTime=0.287 Loss=1.416 Prec@1=65.439 Prec@5=86.417 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=11:46 IST
=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.483 DataTime=0.287 Loss=1.416 Prec@1=65.439 Prec@5=86.417 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=11:46 IST
=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.483 DataTime=0.287 Loss=1.416 Prec@1=65.439 Prec@5=86.417 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=11:47 IST
=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.420 Prec@1=65.405 Prec@5=86.379 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=11:47 IST
=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.420 Prec@1=65.405 Prec@5=86.379 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=11:47 IST
=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.420 Prec@1=65.405 Prec@5=86.379 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=11:48 IST
=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.479 DataTime=0.283 Loss=1.420 Prec@1=65.436 Prec@5=86.382 rate=2.14 Hz, eta=0:16:24, total=0:03:07, wall=11:48 IST
=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.479 DataTime=0.283 Loss=1.420 Prec@1=65.436 Prec@5=86.382 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=11:48 IST
=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.479 DataTime=0.283 Loss=1.420 Prec@1=65.436 Prec@5=86.382 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=11:48 IST
=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.426 Prec@1=65.316 Prec@5=86.305 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=11:48 IST
=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.426 Prec@1=65.316 Prec@5=86.305 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=11:48 IST
=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.480 DataTime=0.284 Loss=1.426 Prec@1=65.316 Prec@5=86.305 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=11:49 IST
=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.478 DataTime=0.282 Loss=1.430 Prec@1=65.256 Prec@5=86.260 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=11:49 IST
=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.478 DataTime=0.282 Loss=1.430 Prec@1=65.256 Prec@5=86.260 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=11:49 IST
=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.478 DataTime=0.282 Loss=1.430 Prec@1=65.256 Prec@5=86.260 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=11:50 IST
=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.476 DataTime=0.281 Loss=1.431 Prec@1=65.210 Prec@5=86.225 rate=2.12 Hz, eta=0:14:09, total=0:05:30, wall=11:50 IST
=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.476 DataTime=0.281 Loss=1.431 Prec@1=65.210 Prec@5=86.225 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=11:50 IST
=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.476 DataTime=0.281 Loss=1.431 Prec@1=65.210 Prec@5=86.225 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=11:51 IST
=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.475 DataTime=0.280 Loss=1.431 Prec@1=65.196 Prec@5=86.244 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=11:51 IST
=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.475 DataTime=0.280 Loss=1.431 Prec@1=65.196 Prec@5=86.244 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=11:51 IST
=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.475 DataTime=0.280 Loss=1.431 Prec@1=65.196 Prec@5=86.244 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=11:52 IST
=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.474 DataTime=0.279 Loss=1.432 Prec@1=65.223 Prec@5=86.227 rate=2.13 Hz, eta=0:12:32, total=0:07:03, wall=11:52 IST
=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.474 DataTime=0.279 Loss=1.432 Prec@1=65.223 Prec@5=86.227 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=11:52 IST
=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.474 DataTime=0.279 Loss=1.432 Prec@1=65.223 Prec@5=86.227 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=11:52 IST
=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.432 Prec@1=65.213 Prec@5=86.209 rate=2.13 Hz, eta=0:11:45, total=0:07:50, wall=11:52 IST
=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.432 Prec@1=65.213 Prec@5=86.209 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=11:52 IST
=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.432 Prec@1=65.213 Prec@5=86.209 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=11:53 IST
=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.434 Prec@1=65.195 Prec@5=86.199 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=11:53 IST
=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.434 Prec@1=65.195 Prec@5=86.199 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=11:53 IST
=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.434 Prec@1=65.195 Prec@5=86.199 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=11:54 IST
=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.435 Prec@1=65.144 Prec@5=86.192 rate=2.13 Hz, eta=0:10:10, total=0:09:23, wall=11:54 IST
=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.435 Prec@1=65.144 Prec@5=86.192 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=11:54 IST
=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.278 Loss=1.435 Prec@1=65.144 Prec@5=86.192 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=11:55 IST
=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.277 Loss=1.437 Prec@1=65.110 Prec@5=86.177 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=11:55 IST
=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.277 Loss=1.437 Prec@1=65.110 Prec@5=86.177 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=11:55 IST
=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.277 Loss=1.437 Prec@1=65.110 Prec@5=86.177 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=11:55 IST
=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.277 Loss=1.437 Prec@1=65.087 Prec@5=86.177 rate=2.13 Hz, eta=0:08:36, total=0:10:56, wall=11:55 IST
=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.277 Loss=1.437 Prec@1=65.087 Prec@5=86.177 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=11:55 IST
=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.473 DataTime=0.277 Loss=1.437 Prec@1=65.087 Prec@5=86.177 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=11:56 IST
=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.061 Prec@5=86.145 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=11:56 IST
=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.061 Prec@5=86.145 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=11:56 IST
=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.472 DataTime=0.276 Loss=1.439 Prec@1=65.061 Prec@5=86.145 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=11:57 IST
=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.440 Prec@1=65.038 Prec@5=86.135 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=11:57 IST
=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.440 Prec@1=65.038 Prec@5=86.135 rate=2.13 Hz, eta=0:06:15, total=0:13:16, wall=11:57 IST
=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.440 Prec@1=65.038 Prec@5=86.135 rate=2.13 Hz, eta=0:06:15, total=0:13:16, wall=11:58 IST
=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.018 Prec@5=86.121 rate=2.13 Hz, eta=0:06:15, total=0:13:16, wall=11:58 IST
=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.018 Prec@5=86.121 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=11:58 IST
=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.441 Prec@1=65.018 Prec@5=86.121 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=11:59 IST
=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.443 Prec@1=64.995 Prec@5=86.109 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=11:59 IST
=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.443 Prec@1=64.995 Prec@5=86.109 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=11:59 IST
=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.471 DataTime=0.276 Loss=1.443 Prec@1=64.995 Prec@5=86.109 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=11:59 IST
=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=64.970 Prec@5=86.092 rate=2.13 Hz, eta=0:04:42, total=0:14:50, wall=11:59 IST
=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=64.970 Prec@5=86.092 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=11:59 IST
=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.444 Prec@1=64.970 Prec@5=86.092 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=12:00 IST
=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.445 Prec@1=64.942 Prec@5=86.072 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=12:00 IST
=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.445 Prec@1=64.942 Prec@5=86.072 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=12:00 IST
=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.445 Prec@1=64.942 Prec@5=86.072 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=12:01 IST
=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.446 Prec@1=64.930 Prec@5=86.061 rate=2.14 Hz, eta=0:03:08, total=0:16:22, wall=12:01 IST
=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.446 Prec@1=64.930 Prec@5=86.061 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=12:01 IST
=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.275 Loss=1.446 Prec@1=64.930 Prec@5=86.061 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=12:02 IST
=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.274 Loss=1.448 Prec@1=64.902 Prec@5=86.046 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=12:02 IST
=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.274 Loss=1.448 Prec@1=64.902 Prec@5=86.046 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=12:02 IST
=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.470 DataTime=0.274 Loss=1.448 Prec@1=64.902 Prec@5=86.046 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=12:02 IST
=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.887 Prec@5=86.032 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=12:02 IST
=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.887 Prec@5=86.032 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=12:02 IST
=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.887 Prec@5=86.032 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=12:03 IST
=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.882 Prec@5=86.027 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=12:03 IST
=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.882 Prec@5=86.027 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=12:03 IST
=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.882 Prec@5=86.027 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=12:03 IST
=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.881 Prec@5=86.027 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=12:03 IST
=> training   100.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.469 DataTime=0.274 Loss=1.448 Prec@1=64.881 Prec@5=86.027 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=12:03 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST
=> validation 0.00% of 1x98...Epoch=60/150 LR=0.06644 Time=6.882 Loss=1.081 Prec@1=71.484 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST
=> validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=6.882 Loss=1.081 Prec@1=71.484 Prec@5=91.602 rate=8001.15 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST
** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=6.882 Loss=1.081 Prec@1=71.484 Prec@5=91.602 rate=8001.15 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST
** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=0.545 Loss=1.526 Prec@1=63.114 Prec@5=85.434 rate=8001.15 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST
** validation 100.00% of 1x98...Epoch=60/150 LR=0.06644 Time=0.545 Loss=1.526 Prec@1=63.114 Prec@5=85.434 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=12:04 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> training   0.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.708 DataTime=4.440 Loss=1.402 Prec@1=66.406 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.708 DataTime=4.440 Loss=1.402 Prec@1=66.406 Prec@5=86.328 rate=2707.82 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST
=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.708 DataTime=4.440 Loss=1.402 Prec@1=66.406 Prec@5=86.328 rate=2707.82 Hz, eta=0:00:00, total=0:00:00, wall=12:05 IST
=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.504 DataTime=0.307 Loss=1.412 Prec@1=65.664 Prec@5=86.460 rate=2707.82 Hz, eta=0:00:00, total=0:00:00, wall=12:05 IST
=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.504 DataTime=0.307 Loss=1.412 Prec@1=65.664 Prec@5=86.460 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=12:05 IST
=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.504 DataTime=0.307 Loss=1.412 Prec@1=65.664 Prec@5=86.460 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=12:06 IST
=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.486 DataTime=0.288 Loss=1.416 Prec@1=65.611 Prec@5=86.419 rate=2.19 Hz, eta=0:18:17, total=0:00:46, wall=12:06 IST
=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.486 DataTime=0.288 Loss=1.416 Prec@1=65.611 Prec@5=86.419 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=12:06 IST
=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.486 DataTime=0.288 Loss=1.416 Prec@1=65.611 Prec@5=86.419 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=12:07 IST
=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.476 DataTime=0.278 Loss=1.420 Prec@1=65.447 Prec@5=86.377 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=12:07 IST
=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.476 DataTime=0.278 Loss=1.420 Prec@1=65.447 Prec@5=86.377 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:07 IST
=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.476 DataTime=0.278 Loss=1.420 Prec@1=65.447 Prec@5=86.377 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:07 IST
=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.472 DataTime=0.275 Loss=1.420 Prec@1=65.470 Prec@5=86.356 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:07 IST
=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.472 DataTime=0.275 Loss=1.420 Prec@1=65.470 Prec@5=86.356 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=12:07 IST
=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.472 DataTime=0.275 Loss=1.420 Prec@1=65.470 Prec@5=86.356 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=12:08 IST
=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.471 DataTime=0.275 Loss=1.420 Prec@1=65.491 Prec@5=86.375 rate=2.17 Hz, eta=0:16:07, total=0:03:04, wall=12:08 IST
=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.471 DataTime=0.275 Loss=1.420 Prec@1=65.491 Prec@5=86.375 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:08 IST
=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.471 DataTime=0.275 Loss=1.420 Prec@1=65.491 Prec@5=86.375 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:09 IST
=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.274 Loss=1.418 Prec@1=65.589 Prec@5=86.390 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:09 IST
=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.274 Loss=1.418 Prec@1=65.589 Prec@5=86.390 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=12:09 IST
=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.274 Loss=1.418 Prec@1=65.589 Prec@5=86.390 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=12:10 IST
=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.275 Loss=1.420 Prec@1=65.549 Prec@5=86.362 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=12:10 IST
=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.275 Loss=1.420 Prec@1=65.549 Prec@5=86.362 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=12:10 IST
=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.470 DataTime=0.275 Loss=1.420 Prec@1=65.549 Prec@5=86.362 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=12:10 IST
=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.450 Prec@5=86.319 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=12:10 IST
=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.450 Prec@5=86.319 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=12:10 IST
=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.450 Prec@5=86.319 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=12:11 IST
=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.420 Prec@5=86.298 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=12:11 IST
=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.420 Prec@5=86.298 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=12:11 IST
=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.420 Prec@5=86.298 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=12:12 IST
=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.426 Prec@1=65.377 Prec@5=86.286 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=12:12 IST
=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.426 Prec@1=65.377 Prec@5=86.286 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:12 IST
=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.426 Prec@1=65.377 Prec@5=86.286 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:13 IST
=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.357 Prec@5=86.270 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:13 IST
=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.357 Prec@5=86.270 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:13 IST
=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.357 Prec@5=86.270 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:14 IST
=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.429 Prec@1=65.337 Prec@5=86.261 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:14 IST
=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.429 Prec@1=65.337 Prec@5=86.261 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=12:14 IST
=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.429 Prec@1=65.337 Prec@5=86.261 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=12:14 IST
=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.345 Prec@5=86.262 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=12:14 IST
=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.345 Prec@5=86.262 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:14 IST
=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.469 DataTime=0.273 Loss=1.428 Prec@1=65.345 Prec@5=86.262 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:15 IST
=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.430 Prec@1=65.303 Prec@5=86.234 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:15 IST
=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.430 Prec@1=65.303 Prec@5=86.234 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=12:15 IST
=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.430 Prec@1=65.303 Prec@5=86.234 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=12:16 IST
=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.433 Prec@1=65.277 Prec@5=86.200 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=12:16 IST
=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.433 Prec@1=65.277 Prec@5=86.200 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=12:16 IST
=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.433 Prec@1=65.277 Prec@5=86.200 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=12:17 IST
=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.254 Prec@5=86.182 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=12:17 IST
=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.254 Prec@5=86.182 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=12:17 IST
=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.254 Prec@5=86.182 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=12:17 IST
=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.234 Prec@5=86.180 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=12:17 IST
=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.234 Prec@5=86.180 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=12:17 IST
=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.434 Prec@1=65.234 Prec@5=86.180 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=12:18 IST
=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.436 Prec@1=65.197 Prec@5=86.150 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=12:18 IST
=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.436 Prec@1=65.197 Prec@5=86.150 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=12:18 IST
=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.436 Prec@1=65.197 Prec@5=86.150 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=12:19 IST
=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.437 Prec@1=65.168 Prec@5=86.132 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=12:19 IST
=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.437 Prec@1=65.168 Prec@5=86.132 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=12:19 IST
=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.437 Prec@1=65.168 Prec@5=86.132 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=12:20 IST
=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.438 Prec@1=65.141 Prec@5=86.114 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=12:20 IST
=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.438 Prec@1=65.141 Prec@5=86.114 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=12:20 IST
=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.468 DataTime=0.273 Loss=1.438 Prec@1=65.141 Prec@5=86.114 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=12:20 IST
=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.112 Prec@5=86.095 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=12:20 IST
=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.112 Prec@5=86.095 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=12:20 IST
=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.112 Prec@5=86.095 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=12:21 IST
=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.105 Prec@5=86.088 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=12:21 IST
=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.105 Prec@5=86.088 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=12:21 IST
=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.440 Prec@1=65.105 Prec@5=86.088 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=12:22 IST
=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.082 Prec@5=86.090 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=12:22 IST
=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.082 Prec@5=86.090 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=12:22 IST
=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.082 Prec@5=86.090 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=12:23 IST
=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.071 Prec@5=86.076 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=12:23 IST
=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.071 Prec@5=86.076 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=12:23 IST
=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.441 Prec@1=65.071 Prec@5=86.076 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=12:24 IST
=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.443 Prec@1=65.047 Prec@5=86.057 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=12:24 IST
=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.443 Prec@1=65.047 Prec@5=86.057 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=12:24 IST
=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.443 Prec@1=65.047 Prec@5=86.057 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=12:24 IST
=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.443 Prec@1=65.047 Prec@5=86.056 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=12:24 IST
=> training   100.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.467 DataTime=0.272 Loss=1.443 Prec@1=65.047 Prec@5=86.056 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=12:24 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> validation 0.00% of 1x98...Epoch=61/150 LR=0.06545 Time=6.955 Loss=0.901 Prec@1=75.977 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.955 Loss=0.901 Prec@1=75.977 Prec@5=93.164 rate=8143.32 Hz, eta=0:00:00, total=0:00:00, wall=12:24 IST
** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.955 Loss=0.901 Prec@1=75.977 Prec@5=93.164 rate=8143.32 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=0.553 Loss=1.463 Prec@1=64.434 Prec@5=86.150 rate=8143.32 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
** validation 100.00% of 1x98...Epoch=61/150 LR=0.06545 Time=0.553 Loss=1.463 Prec@1=64.434 Prec@5=86.150 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=12:25 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> training   0.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=4.851 DataTime=4.572 Loss=1.347 Prec@1=68.164 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=4.851 DataTime=4.572 Loss=1.347 Prec@1=68.164 Prec@5=86.328 rate=5856.04 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=4.851 DataTime=4.572 Loss=1.347 Prec@1=68.164 Prec@5=86.328 rate=5856.04 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.494 DataTime=0.299 Loss=1.420 Prec@1=65.602 Prec@5=86.438 rate=5856.04 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.494 DataTime=0.299 Loss=1.420 Prec@1=65.602 Prec@5=86.438 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=12:25 IST
=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.494 DataTime=0.299 Loss=1.420 Prec@1=65.602 Prec@5=86.438 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=12:26 IST
=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.482 DataTime=0.287 Loss=1.403 Prec@1=65.852 Prec@5=86.721 rate=2.24 Hz, eta=0:17:52, total=0:00:45, wall=12:26 IST
=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.482 DataTime=0.287 Loss=1.403 Prec@1=65.852 Prec@5=86.721 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=12:26 IST
=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.482 DataTime=0.287 Loss=1.403 Prec@1=65.852 Prec@5=86.721 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=12:27 IST
=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.405 Prec@1=65.828 Prec@5=86.664 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=12:27 IST
=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.405 Prec@1=65.828 Prec@5=86.664 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=12:27 IST
=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.405 Prec@1=65.828 Prec@5=86.664 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=12:28 IST
=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.408 Prec@1=65.758 Prec@5=86.596 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=12:28 IST
=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.408 Prec@1=65.758 Prec@5=86.596 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=12:28 IST
=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.279 Loss=1.408 Prec@1=65.758 Prec@5=86.596 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=12:29 IST
=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.414 Prec@1=65.665 Prec@5=86.516 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=12:29 IST
=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.414 Prec@1=65.665 Prec@5=86.516 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:29 IST
=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.414 Prec@1=65.665 Prec@5=86.516 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:29 IST
=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.574 Prec@5=86.466 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:29 IST
=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.574 Prec@5=86.466 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=12:29 IST
=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.574 Prec@5=86.466 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=12:30 IST
=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.541 Prec@5=86.493 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=12:30 IST
=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.541 Prec@5=86.493 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=12:30 IST
=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.416 Prec@1=65.541 Prec@5=86.493 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=12:31 IST
=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.274 Loss=1.417 Prec@1=65.542 Prec@5=86.488 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=12:31 IST
=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.274 Loss=1.417 Prec@1=65.542 Prec@5=86.488 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=12:31 IST
=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.274 Loss=1.417 Prec@1=65.542 Prec@5=86.488 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=12:32 IST
=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.276 Loss=1.418 Prec@1=65.498 Prec@5=86.461 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=12:32 IST
=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.276 Loss=1.418 Prec@1=65.498 Prec@5=86.461 rate=2.14 Hz, eta=0:12:28, total=0:07:00, wall=12:32 IST
=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.276 Loss=1.418 Prec@1=65.498 Prec@5=86.461 rate=2.14 Hz, eta=0:12:28, total=0:07:00, wall=12:32 IST
=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.418 Prec@1=65.498 Prec@5=86.447 rate=2.14 Hz, eta=0:12:28, total=0:07:00, wall=12:32 IST
=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.418 Prec@1=65.498 Prec@5=86.447 rate=2.13 Hz, eta=0:11:43, total=0:07:48, wall=12:32 IST
=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.418 Prec@1=65.498 Prec@5=86.447 rate=2.13 Hz, eta=0:11:43, total=0:07:48, wall=12:33 IST
=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.475 DataTime=0.279 Loss=1.419 Prec@1=65.471 Prec@5=86.451 rate=2.13 Hz, eta=0:11:43, total=0:07:48, wall=12:33 IST
=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.475 DataTime=0.279 Loss=1.419 Prec@1=65.471 Prec@5=86.451 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=12:33 IST
=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.475 DataTime=0.279 Loss=1.419 Prec@1=65.471 Prec@5=86.451 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=12:34 IST
=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.421 Prec@1=65.423 Prec@5=86.422 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=12:34 IST
=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.421 Prec@1=65.423 Prec@5=86.422 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=12:34 IST
=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.421 Prec@1=65.423 Prec@5=86.422 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=12:35 IST
=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.423 Prec@1=65.390 Prec@5=86.385 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=12:35 IST
=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.423 Prec@1=65.390 Prec@5=86.385 rate=2.13 Hz, eta=0:09:24, total=0:10:11, wall=12:35 IST
=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.474 DataTime=0.278 Loss=1.423 Prec@1=65.390 Prec@5=86.385 rate=2.13 Hz, eta=0:09:24, total=0:10:11, wall=12:36 IST
=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.425 Prec@1=65.353 Prec@5=86.356 rate=2.13 Hz, eta=0:09:24, total=0:10:11, wall=12:36 IST
=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.425 Prec@1=65.353 Prec@5=86.356 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=12:36 IST
=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.277 Loss=1.425 Prec@1=65.353 Prec@5=86.356 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=12:36 IST
=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.278 Loss=1.427 Prec@1=65.330 Prec@5=86.334 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=12:36 IST
=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.278 Loss=1.427 Prec@1=65.330 Prec@5=86.334 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=12:36 IST
=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.473 DataTime=0.278 Loss=1.427 Prec@1=65.330 Prec@5=86.334 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=12:37 IST
=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.428 Prec@1=65.322 Prec@5=86.311 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=12:37 IST
=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.428 Prec@1=65.322 Prec@5=86.311 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=12:37 IST
=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.428 Prec@1=65.322 Prec@5=86.311 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=12:38 IST
=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.429 Prec@1=65.324 Prec@5=86.295 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=12:38 IST
=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.429 Prec@1=65.324 Prec@5=86.295 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:38 IST
=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.277 Loss=1.429 Prec@1=65.324 Prec@5=86.295 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:39 IST
=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.288 Prec@5=86.271 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:39 IST
=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.288 Prec@5=86.271 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=12:39 IST
=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.288 Prec@5=86.271 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=12:40 IST
=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.431 Prec@1=65.275 Prec@5=86.263 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=12:40 IST
=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.431 Prec@1=65.275 Prec@5=86.263 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=12:40 IST
=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.431 Prec@1=65.275 Prec@5=86.263 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=12:40 IST
=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.263 Prec@5=86.250 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=12:40 IST
=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.263 Prec@5=86.250 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=12:40 IST
=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.263 Prec@5=86.250 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=12:41 IST
=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.433 Prec@1=65.221 Prec@5=86.227 rate=2.13 Hz, eta=0:03:55, total=0:15:38, wall=12:41 IST
=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.433 Prec@1=65.221 Prec@5=86.227 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=12:41 IST
=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.472 DataTime=0.276 Loss=1.433 Prec@1=65.221 Prec@5=86.227 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=12:42 IST
=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.225 Prec@5=86.223 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=12:42 IST
=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.225 Prec@5=86.223 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=12:42 IST
=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.225 Prec@5=86.223 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=12:43 IST
=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.209 Prec@5=86.204 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=12:43 IST
=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.209 Prec@5=86.204 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=12:43 IST
=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.434 Prec@1=65.209 Prec@5=86.204 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=12:43 IST
=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.275 Loss=1.434 Prec@1=65.208 Prec@5=86.210 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=12:43 IST
=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.275 Loss=1.434 Prec@1=65.208 Prec@5=86.210 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=12:43 IST
=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.470 DataTime=0.275 Loss=1.434 Prec@1=65.208 Prec@5=86.210 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=12:44 IST
=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.435 Prec@1=65.184 Prec@5=86.198 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=12:44 IST
=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.435 Prec@1=65.184 Prec@5=86.198 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=12:44 IST
=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.435 Prec@1=65.184 Prec@5=86.198 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=12:44 IST
=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.435 Prec@1=65.183 Prec@5=86.198 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=12:44 IST
=> training   100.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.471 DataTime=0.275 Loss=1.435 Prec@1=65.183 Prec@5=86.198 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=12:44 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> validation 0.00% of 1x98...Epoch=62/150 LR=0.06445 Time=7.062 Loss=0.970 Prec@1=72.266 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=7.062 Loss=0.970 Prec@1=72.266 Prec@5=93.750 rate=7507.00 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=7.062 Loss=0.970 Prec@1=72.266 Prec@5=93.750 rate=7507.00 Hz, eta=0:00:00, total=0:00:00, wall=12:45 IST
** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=0.551 Loss=1.468 Prec@1=64.362 Prec@5=86.140 rate=7507.00 Hz, eta=0:00:00, total=0:00:00, wall=12:45 IST
** validation 100.00% of 1x98...Epoch=62/150 LR=0.06445 Time=0.551 Loss=1.468 Prec@1=64.362 Prec@5=86.140 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=12:45 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:45 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:45 IST
=> training   0.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=4.374 DataTime=4.076 Loss=1.120 Prec@1=70.898 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=12:45 IST
=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=4.374 DataTime=4.076 Loss=1.120 Prec@1=70.898 Prec@5=91.406 rate=6946.33 Hz, eta=0:00:00, total=0:00:00, wall=12:45 IST
=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=4.374 DataTime=4.076 Loss=1.120 Prec@1=70.898 Prec@5=91.406 rate=6946.33 Hz, eta=0:00:00, total=0:00:00, wall=12:46 IST
=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.497 DataTime=0.303 Loss=1.406 Prec@1=65.944 Prec@5=86.527 rate=6946.33 Hz, eta=0:00:00, total=0:00:00, wall=12:46 IST
=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.497 DataTime=0.303 Loss=1.406 Prec@1=65.944 Prec@5=86.527 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=12:46 IST
=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.497 DataTime=0.303 Loss=1.406 Prec@1=65.944 Prec@5=86.527 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=12:47 IST
=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.478 DataTime=0.284 Loss=1.400 Prec@1=65.931 Prec@5=86.634 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=12:47 IST
=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.478 DataTime=0.284 Loss=1.400 Prec@1=65.931 Prec@5=86.634 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=12:47 IST
=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.478 DataTime=0.284 Loss=1.400 Prec@1=65.931 Prec@5=86.634 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=12:48 IST
=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.474 DataTime=0.280 Loss=1.402 Prec@1=65.848 Prec@5=86.657 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=12:48 IST
=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.474 DataTime=0.280 Loss=1.402 Prec@1=65.848 Prec@5=86.657 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:48 IST
=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.474 DataTime=0.280 Loss=1.402 Prec@1=65.848 Prec@5=86.657 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:48 IST
=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.472 DataTime=0.277 Loss=1.404 Prec@1=65.808 Prec@5=86.643 rate=2.17 Hz, eta=0:16:52, total=0:02:18, wall=12:48 IST
=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.472 DataTime=0.277 Loss=1.404 Prec@1=65.808 Prec@5=86.643 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=12:48 IST
=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.472 DataTime=0.277 Loss=1.404 Prec@1=65.808 Prec@5=86.643 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=12:49 IST
=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.408 Prec@1=65.746 Prec@5=86.558 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=12:49 IST
=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.408 Prec@1=65.746 Prec@5=86.558 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:49 IST
=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.408 Prec@1=65.746 Prec@5=86.558 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:50 IST
=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.411 Prec@1=65.680 Prec@5=86.538 rate=2.16 Hz, eta=0:15:24, total=0:03:51, wall=12:50 IST
=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.411 Prec@1=65.680 Prec@5=86.538 rate=2.16 Hz, eta=0:14:39, total=0:04:38, wall=12:50 IST
=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.411 Prec@1=65.680 Prec@5=86.538 rate=2.16 Hz, eta=0:14:39, total=0:04:38, wall=12:51 IST
=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.412 Prec@1=65.675 Prec@5=86.516 rate=2.16 Hz, eta=0:14:39, total=0:04:38, wall=12:51 IST
=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.412 Prec@1=65.675 Prec@5=86.516 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=12:51 IST
=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.412 Prec@1=65.675 Prec@5=86.516 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=12:51 IST
=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.413 Prec@1=65.649 Prec@5=86.497 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=12:51 IST
=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.413 Prec@1=65.649 Prec@5=86.497 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=12:51 IST
=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.413 Prec@1=65.649 Prec@5=86.497 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=12:52 IST
=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.414 Prec@1=65.628 Prec@5=86.483 rate=2.15 Hz, eta=0:13:09, total=0:06:11, wall=12:52 IST
=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.414 Prec@1=65.628 Prec@5=86.483 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:52 IST
=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.414 Prec@1=65.628 Prec@5=86.483 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:53 IST
=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.616 Prec@5=86.471 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:53 IST
=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.616 Prec@5=86.471 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:53 IST
=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.616 Prec@5=86.471 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:54 IST
=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.574 Prec@5=86.464 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:54 IST
=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.574 Prec@5=86.464 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:54 IST
=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.415 Prec@1=65.574 Prec@5=86.464 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:55 IST
=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.418 Prec@1=65.548 Prec@5=86.434 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:55 IST
=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.418 Prec@1=65.548 Prec@5=86.434 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:55 IST
=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.418 Prec@1=65.548 Prec@5=86.434 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:55 IST
=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.468 DataTime=0.274 Loss=1.420 Prec@1=65.504 Prec@5=86.409 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:55 IST
=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.468 DataTime=0.274 Loss=1.420 Prec@1=65.504 Prec@5=86.409 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=12:55 IST
=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.468 DataTime=0.274 Loss=1.420 Prec@1=65.504 Prec@5=86.409 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=12:56 IST
=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.422 Prec@1=65.465 Prec@5=86.377 rate=2.15 Hz, eta=0:09:18, total=0:10:05, wall=12:56 IST
=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.422 Prec@1=65.465 Prec@5=86.377 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:56 IST
=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.422 Prec@1=65.465 Prec@5=86.377 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:57 IST
=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.440 Prec@5=86.355 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:57 IST
=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.440 Prec@5=86.355 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=12:57 IST
=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.423 Prec@1=65.440 Prec@5=86.355 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=12:58 IST
=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.414 Prec@5=86.353 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=12:58 IST
=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.414 Prec@5=86.353 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=12:58 IST
=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.274 Loss=1.424 Prec@1=65.414 Prec@5=86.353 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=12:58 IST
=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.390 Prec@5=86.337 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=12:58 IST
=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.390 Prec@5=86.337 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=12:58 IST
=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.390 Prec@5=86.337 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=12:59 IST
=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.385 Prec@5=86.316 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=12:59 IST
=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.385 Prec@5=86.316 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=12:59 IST
=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.469 DataTime=0.275 Loss=1.425 Prec@1=65.385 Prec@5=86.316 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=13:00 IST
=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.426 Prec@1=65.373 Prec@5=86.305 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=13:00 IST
=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.426 Prec@1=65.373 Prec@5=86.305 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=13:00 IST
=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.275 Loss=1.426 Prec@1=65.373 Prec@5=86.305 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=13:01 IST
=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.345 Prec@5=86.284 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=13:01 IST
=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.345 Prec@5=86.284 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=13:01 IST
=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.345 Prec@5=86.284 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=13:02 IST
=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.335 Prec@5=86.276 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=13:02 IST
=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.335 Prec@5=86.276 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=13:02 IST
=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.470 DataTime=0.276 Loss=1.428 Prec@1=65.335 Prec@5=86.276 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=13:02 IST
=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.429 Prec@1=65.328 Prec@5=86.252 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=13:02 IST
=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.429 Prec@1=65.328 Prec@5=86.252 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=13:02 IST
=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.429 Prec@1=65.328 Prec@5=86.252 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=13:03 IST
=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.430 Prec@1=65.313 Prec@5=86.237 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=13:03 IST
=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.430 Prec@1=65.313 Prec@5=86.237 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=13:03 IST
=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.430 Prec@1=65.313 Prec@5=86.237 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=13:04 IST
=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.431 Prec@1=65.308 Prec@5=86.239 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=13:04 IST
=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.431 Prec@1=65.308 Prec@5=86.239 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=13:04 IST
=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.277 Loss=1.431 Prec@1=65.308 Prec@5=86.239 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=13:05 IST
=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.302 Prec@5=86.229 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=13:05 IST
=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.302 Prec@5=86.229 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:05 IST
=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.302 Prec@5=86.229 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:05 IST
=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.302 Prec@5=86.229 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:05 IST
=> training   100.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.471 DataTime=0.276 Loss=1.431 Prec@1=65.302 Prec@5=86.229 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:05 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:05 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:05 IST
=> validation 0.00% of 1x98...Epoch=63/150 LR=0.06345 Time=6.772 Loss=0.875 Prec@1=77.734 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=13:05 IST
=> validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=6.772 Loss=0.875 Prec@1=77.734 Prec@5=94.336 rate=8429.71 Hz, eta=0:00:00, total=0:00:00, wall=13:05 IST
** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=6.772 Loss=0.875 Prec@1=77.734 Prec@5=94.336 rate=8429.71 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST
** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=0.552 Loss=1.455 Prec@1=64.544 Prec@5=86.190 rate=8429.71 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST
** validation 100.00% of 1x98...Epoch=63/150 LR=0.06345 Time=0.552 Loss=1.455 Prec@1=64.544 Prec@5=86.190 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=13:06 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST
=> training   0.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.300 DataTime=4.955 Loss=1.375 Prec@1=66.602 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST
=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.300 DataTime=4.955 Loss=1.375 Prec@1=66.602 Prec@5=87.695 rate=11011.65 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST
=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.300 DataTime=4.955 Loss=1.375 Prec@1=66.602 Prec@5=87.695 rate=11011.65 Hz, eta=0:00:00, total=0:00:00, wall=13:07 IST
=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.510 DataTime=0.311 Loss=1.388 Prec@1=66.321 Prec@5=86.856 rate=11011.65 Hz, eta=0:00:00, total=0:00:00, wall=13:07 IST
=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.510 DataTime=0.311 Loss=1.388 Prec@1=66.321 Prec@5=86.856 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=13:07 IST
=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.510 DataTime=0.311 Loss=1.388 Prec@1=66.321 Prec@5=86.856 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=13:07 IST
=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.492 DataTime=0.296 Loss=1.396 Prec@1=66.199 Prec@5=86.726 rate=2.18 Hz, eta=0:18:20, total=0:00:46, wall=13:07 IST
=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.492 DataTime=0.296 Loss=1.396 Prec@1=66.199 Prec@5=86.726 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:07 IST
=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.492 DataTime=0.296 Loss=1.396 Prec@1=66.199 Prec@5=86.726 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:08 IST
=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.481 DataTime=0.285 Loss=1.398 Prec@1=66.023 Prec@5=86.740 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:08 IST
=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.481 DataTime=0.285 Loss=1.398 Prec@1=66.023 Prec@5=86.740 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=13:08 IST
=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.481 DataTime=0.285 Loss=1.398 Prec@1=66.023 Prec@5=86.740 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=13:09 IST
=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.396 Prec@1=66.071 Prec@5=86.732 rate=2.16 Hz, eta=0:17:00, total=0:02:19, wall=13:09 IST
=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.396 Prec@1=66.071 Prec@5=86.732 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=13:09 IST
=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.396 Prec@1=66.071 Prec@5=86.732 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=13:10 IST
=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.395 Prec@1=66.073 Prec@5=86.760 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=13:10 IST
=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.395 Prec@1=66.073 Prec@5=86.760 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=13:10 IST
=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.395 Prec@1=66.073 Prec@5=86.760 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=13:10 IST
=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.273 Loss=1.398 Prec@1=66.053 Prec@5=86.713 rate=2.17 Hz, eta=0:15:24, total=0:03:51, wall=13:10 IST
=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.273 Loss=1.398 Prec@1=66.053 Prec@5=86.713 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=13:10 IST
=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.273 Loss=1.398 Prec@1=66.053 Prec@5=86.713 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=13:11 IST
=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.469 DataTime=0.273 Loss=1.399 Prec@1=65.991 Prec@5=86.683 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=13:11 IST
=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.469 DataTime=0.273 Loss=1.399 Prec@1=65.991 Prec@5=86.683 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=13:11 IST
=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.469 DataTime=0.273 Loss=1.399 Prec@1=65.991 Prec@5=86.683 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=13:12 IST
=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.274 Loss=1.401 Prec@1=65.951 Prec@5=86.652 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=13:12 IST
=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.274 Loss=1.401 Prec@1=65.951 Prec@5=86.652 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=13:12 IST
=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.274 Loss=1.401 Prec@1=65.951 Prec@5=86.652 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=13:13 IST
=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.275 Loss=1.404 Prec@1=65.909 Prec@5=86.617 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=13:13 IST
=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.275 Loss=1.404 Prec@1=65.909 Prec@5=86.617 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:13 IST
=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.275 Loss=1.404 Prec@1=65.909 Prec@5=86.617 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:14 IST
=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.275 Loss=1.407 Prec@1=65.827 Prec@5=86.581 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:14 IST
=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.275 Loss=1.407 Prec@1=65.827 Prec@5=86.581 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=13:14 IST
=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.470 DataTime=0.275 Loss=1.407 Prec@1=65.827 Prec@5=86.581 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=13:14 IST
=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.407 Prec@1=65.808 Prec@5=86.563 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=13:14 IST
=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.407 Prec@1=65.808 Prec@5=86.563 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=13:14 IST
=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.407 Prec@1=65.808 Prec@5=86.563 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=13:15 IST
=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.409 Prec@1=65.773 Prec@5=86.536 rate=2.14 Hz, eta=0:10:54, total=0:08:33, wall=13:15 IST
=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.409 Prec@1=65.773 Prec@5=86.536 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=13:15 IST
=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.409 Prec@1=65.773 Prec@5=86.536 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=13:16 IST
=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.411 Prec@1=65.747 Prec@5=86.524 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=13:16 IST
=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.411 Prec@1=65.747 Prec@5=86.524 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=13:16 IST
=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.411 Prec@1=65.747 Prec@5=86.524 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=13:17 IST
=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.412 Prec@1=65.728 Prec@5=86.521 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=13:17 IST
=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.412 Prec@1=65.728 Prec@5=86.521 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=13:17 IST
=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.412 Prec@1=65.728 Prec@5=86.521 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=13:18 IST
=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.413 Prec@1=65.711 Prec@5=86.501 rate=2.13 Hz, eta=0:08:37, total=0:10:58, wall=13:18 IST
=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.413 Prec@1=65.711 Prec@5=86.501 rate=2.13 Hz, eta=0:07:51, total=0:11:46, wall=13:18 IST
=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.413 Prec@1=65.711 Prec@5=86.501 rate=2.13 Hz, eta=0:07:51, total=0:11:46, wall=13:18 IST
=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.414 Prec@1=65.683 Prec@5=86.490 rate=2.13 Hz, eta=0:07:51, total=0:11:46, wall=13:18 IST
=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.414 Prec@1=65.683 Prec@5=86.490 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=13:18 IST
=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.278 Loss=1.414 Prec@1=65.683 Prec@5=86.490 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=13:19 IST
=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.415 Prec@1=65.675 Prec@5=86.485 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=13:19 IST
=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.415 Prec@1=65.675 Prec@5=86.485 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=13:19 IST
=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.415 Prec@1=65.675 Prec@5=86.485 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=13:20 IST
=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.416 Prec@1=65.668 Prec@5=86.463 rate=2.12 Hz, eta=0:06:17, total=0:13:21, wall=13:20 IST
=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.416 Prec@1=65.668 Prec@5=86.463 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=13:20 IST
=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.475 DataTime=0.279 Loss=1.416 Prec@1=65.668 Prec@5=86.463 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=13:21 IST
=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.417 Prec@1=65.642 Prec@5=86.447 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=13:21 IST
=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.417 Prec@1=65.642 Prec@5=86.447 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=13:21 IST
=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.474 DataTime=0.279 Loss=1.417 Prec@1=65.642 Prec@5=86.447 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=13:22 IST
=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.278 Loss=1.418 Prec@1=65.614 Prec@5=86.425 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=13:22 IST
=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.278 Loss=1.418 Prec@1=65.614 Prec@5=86.425 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=13:22 IST
=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.278 Loss=1.418 Prec@1=65.614 Prec@5=86.425 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=13:22 IST
=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.419 Prec@1=65.580 Prec@5=86.407 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=13:22 IST
=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.419 Prec@1=65.580 Prec@5=86.407 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=13:22 IST
=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.473 DataTime=0.277 Loss=1.419 Prec@1=65.580 Prec@5=86.407 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=13:23 IST
=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.420 Prec@1=65.561 Prec@5=86.392 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=13:23 IST
=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.420 Prec@1=65.561 Prec@5=86.392 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=13:23 IST
=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.420 Prec@1=65.561 Prec@5=86.392 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=13:24 IST
=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.421 Prec@1=65.545 Prec@5=86.389 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=13:24 IST
=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.421 Prec@1=65.545 Prec@5=86.389 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=13:24 IST
=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.277 Loss=1.421 Prec@1=65.545 Prec@5=86.389 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=13:25 IST
=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.422 Prec@1=65.520 Prec@5=86.374 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=13:25 IST
=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.422 Prec@1=65.520 Prec@5=86.374 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=13:25 IST
=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.472 DataTime=0.276 Loss=1.422 Prec@1=65.520 Prec@5=86.374 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=13:25 IST
=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.423 Prec@1=65.492 Prec@5=86.353 rate=2.13 Hz, eta=0:00:47, total=0:18:47, wall=13:25 IST
=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.423 Prec@1=65.492 Prec@5=86.353 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:25 IST
=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.423 Prec@1=65.492 Prec@5=86.353 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:25 IST
=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.423 Prec@1=65.493 Prec@5=86.353 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=13:25 IST
=> training   100.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.471 DataTime=0.276 Loss=1.423 Prec@1=65.493 Prec@5=86.353 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=13:25 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> validation 0.00% of 1x98...Epoch=64/150 LR=0.06243 Time=6.861 Loss=0.929 Prec@1=76.562 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.861 Loss=0.929 Prec@1=76.562 Prec@5=92.188 rate=8508.25 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.861 Loss=0.929 Prec@1=76.562 Prec@5=92.188 rate=8508.25 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=0.553 Loss=1.454 Prec@1=64.448 Prec@5=86.348 rate=8508.25 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
** validation 100.00% of 1x98...Epoch=64/150 LR=0.06243 Time=0.553 Loss=1.454 Prec@1=64.448 Prec@5=86.348 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=13:26 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> training   0.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=4.608 DataTime=4.373 Loss=1.287 Prec@1=69.336 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=13:26 IST
=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=4.608 DataTime=4.373 Loss=1.287 Prec@1=69.336 Prec@5=88.086 rate=5699.60 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=4.608 DataTime=4.373 Loss=1.287 Prec@1=69.336 Prec@5=88.086 rate=5699.60 Hz, eta=0:00:00, total=0:00:00, wall=13:27 IST
=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.493 DataTime=0.298 Loss=1.372 Prec@1=66.650 Prec@5=86.995 rate=5699.60 Hz, eta=0:00:00, total=0:00:00, wall=13:27 IST
=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.493 DataTime=0.298 Loss=1.372 Prec@1=66.650 Prec@5=86.995 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=13:27 IST
=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.493 DataTime=0.298 Loss=1.372 Prec@1=66.650 Prec@5=86.995 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=13:28 IST
=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.476 DataTime=0.279 Loss=1.374 Prec@1=66.575 Prec@5=86.971 rate=2.24 Hz, eta=0:17:54, total=0:00:45, wall=13:28 IST
=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.476 DataTime=0.279 Loss=1.374 Prec@1=66.575 Prec@5=86.971 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=13:28 IST
=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.476 DataTime=0.279 Loss=1.374 Prec@1=66.575 Prec@5=86.971 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=13:29 IST
=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.273 Loss=1.374 Prec@1=66.496 Prec@5=86.963 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=13:29 IST
=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.273 Loss=1.374 Prec@1=66.496 Prec@5=86.963 rate=2.21 Hz, eta=0:16:38, total=0:02:16, wall=13:29 IST
=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.273 Loss=1.374 Prec@1=66.496 Prec@5=86.963 rate=2.21 Hz, eta=0:16:38, total=0:02:16, wall=13:29 IST
=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.378 Prec@1=66.383 Prec@5=86.911 rate=2.21 Hz, eta=0:16:38, total=0:02:16, wall=13:29 IST
=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.378 Prec@1=66.383 Prec@5=86.911 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=13:29 IST
=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.378 Prec@1=66.383 Prec@5=86.911 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=13:30 IST
=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.380 Prec@1=66.301 Prec@5=86.894 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=13:30 IST
=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.380 Prec@1=66.301 Prec@5=86.894 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=13:30 IST
=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.380 Prec@1=66.301 Prec@5=86.894 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=13:31 IST
=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.274 Loss=1.387 Prec@1=66.166 Prec@5=86.851 rate=2.18 Hz, eta=0:15:16, total=0:03:49, wall=13:31 IST
=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.274 Loss=1.387 Prec@1=66.166 Prec@5=86.851 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=13:31 IST
=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.469 DataTime=0.274 Loss=1.387 Prec@1=66.166 Prec@5=86.851 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=13:32 IST
=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.273 Loss=1.388 Prec@1=66.105 Prec@5=86.841 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=13:32 IST
=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.273 Loss=1.388 Prec@1=66.105 Prec@5=86.841 rate=2.17 Hz, eta=0:13:52, total=0:05:23, wall=13:32 IST
=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.273 Loss=1.388 Prec@1=66.105 Prec@5=86.841 rate=2.17 Hz, eta=0:13:52, total=0:05:23, wall=13:33 IST
=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.392 Prec@1=66.030 Prec@5=86.801 rate=2.17 Hz, eta=0:13:52, total=0:05:23, wall=13:33 IST
=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.392 Prec@1=66.030 Prec@5=86.801 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=13:33 IST
=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.392 Prec@1=66.030 Prec@5=86.801 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=13:33 IST
=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.394 Prec@1=65.980 Prec@5=86.771 rate=2.17 Hz, eta=0:13:04, total=0:06:09, wall=13:33 IST
=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.394 Prec@1=65.980 Prec@5=86.771 rate=2.17 Hz, eta=0:12:19, total=0:06:55, wall=13:33 IST
=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.394 Prec@1=65.980 Prec@5=86.771 rate=2.17 Hz, eta=0:12:19, total=0:06:55, wall=13:34 IST
=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.396 Prec@1=65.949 Prec@5=86.736 rate=2.17 Hz, eta=0:12:19, total=0:06:55, wall=13:34 IST
=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.396 Prec@1=65.949 Prec@5=86.736 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=13:34 IST
=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.396 Prec@1=65.949 Prec@5=86.736 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=13:35 IST
=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.398 Prec@1=65.910 Prec@5=86.715 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=13:35 IST
=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.398 Prec@1=65.910 Prec@5=86.715 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=13:35 IST
=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.398 Prec@1=65.910 Prec@5=86.715 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=13:36 IST
=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.400 Prec@1=65.887 Prec@5=86.671 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=13:36 IST
=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.400 Prec@1=65.887 Prec@5=86.671 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=13:36 IST
=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.400 Prec@1=65.887 Prec@5=86.671 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=13:36 IST
=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.402 Prec@1=65.859 Prec@5=86.653 rate=2.16 Hz, eta=0:10:02, total=0:09:16, wall=13:36 IST
=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.402 Prec@1=65.859 Prec@5=86.653 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=13:36 IST
=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.402 Prec@1=65.859 Prec@5=86.653 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=13:37 IST
=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.403 Prec@1=65.830 Prec@5=86.639 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=13:37 IST
=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.403 Prec@1=65.830 Prec@5=86.639 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=13:37 IST
=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.403 Prec@1=65.830 Prec@5=86.639 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=13:38 IST
=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.785 Prec@5=86.607 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=13:38 IST
=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.785 Prec@5=86.607 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=13:38 IST
=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.785 Prec@5=86.607 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=13:39 IST
=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.408 Prec@1=65.737 Prec@5=86.567 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=13:39 IST
=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.408 Prec@1=65.737 Prec@5=86.567 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=13:39 IST
=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.272 Loss=1.408 Prec@1=65.737 Prec@5=86.567 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=13:40 IST
=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.409 Prec@1=65.703 Prec@5=86.546 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=13:40 IST
=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.409 Prec@1=65.703 Prec@5=86.546 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=13:40 IST
=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.409 Prec@1=65.703 Prec@5=86.546 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=13:40 IST
=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.411 Prec@1=65.664 Prec@5=86.531 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=13:40 IST
=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.411 Prec@1=65.664 Prec@5=86.531 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=13:40 IST
=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.411 Prec@1=65.664 Prec@5=86.531 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=13:41 IST
=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.412 Prec@1=65.642 Prec@5=86.527 rate=2.15 Hz, eta=0:05:25, total=0:13:55, wall=13:41 IST
=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.412 Prec@1=65.642 Prec@5=86.527 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=13:41 IST
=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.412 Prec@1=65.642 Prec@5=86.527 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=13:42 IST
=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.413 Prec@1=65.618 Prec@5=86.511 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=13:42 IST
=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.413 Prec@1=65.618 Prec@5=86.511 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=13:42 IST
=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.467 DataTime=0.271 Loss=1.413 Prec@1=65.618 Prec@5=86.511 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=13:43 IST
=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.606 Prec@5=86.496 rate=2.15 Hz, eta=0:03:53, total=0:15:29, wall=13:43 IST
=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.606 Prec@5=86.496 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=13:43 IST
=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.606 Prec@5=86.496 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=13:43 IST
=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.597 Prec@5=86.488 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=13:43 IST
=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.597 Prec@5=86.488 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=13:43 IST
=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.271 Loss=1.414 Prec@1=65.597 Prec@5=86.488 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=13:44 IST
=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.416 Prec@1=65.551 Prec@5=86.457 rate=2.16 Hz, eta=0:02:20, total=0:17:01, wall=13:44 IST
=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.416 Prec@1=65.551 Prec@5=86.457 rate=2.16 Hz, eta=0:01:33, total=0:17:46, wall=13:44 IST
=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.416 Prec@1=65.551 Prec@5=86.457 rate=2.16 Hz, eta=0:01:33, total=0:17:46, wall=13:45 IST
=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.417 Prec@1=65.542 Prec@5=86.438 rate=2.16 Hz, eta=0:01:33, total=0:17:46, wall=13:45 IST
=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.417 Prec@1=65.542 Prec@5=86.438 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=13:45 IST
=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.417 Prec@1=65.542 Prec@5=86.438 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=13:46 IST
=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.418 Prec@1=65.531 Prec@5=86.425 rate=2.16 Hz, eta=0:00:47, total=0:18:33, wall=13:46 IST
=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.418 Prec@1=65.531 Prec@5=86.425 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=13:46 IST
=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.418 Prec@1=65.531 Prec@5=86.425 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=13:46 IST
=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.418 Prec@1=65.530 Prec@5=86.424 rate=2.15 Hz, eta=0:00:00, total=0:19:20, wall=13:46 IST
=> training   100.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.466 DataTime=0.270 Loss=1.418 Prec@1=65.530 Prec@5=86.424 rate=2.16 Hz, eta=0:00:00, total=0:19:21, wall=13:46 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> validation 0.00% of 1x98...Epoch=65/150 LR=0.06142 Time=7.081 Loss=0.932 Prec@1=76.758 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.081 Loss=0.932 Prec@1=76.758 Prec@5=92.773 rate=7391.09 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST
** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.081 Loss=0.932 Prec@1=76.758 Prec@5=92.773 rate=7391.09 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST
** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=0.556 Loss=1.453 Prec@1=64.700 Prec@5=86.300 rate=7391.09 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST
** validation 100.00% of 1x98...Epoch=65/150 LR=0.06142 Time=0.556 Loss=1.453 Prec@1=64.700 Prec@5=86.300 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=13:47 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST
=> training   0.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.681 DataTime=4.439 Loss=1.364 Prec@1=64.844 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST
=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.681 DataTime=4.439 Loss=1.364 Prec@1=64.844 Prec@5=86.719 rate=7075.29 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST
=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.681 DataTime=4.439 Loss=1.364 Prec@1=64.844 Prec@5=86.719 rate=7075.29 Hz, eta=0:00:00, total=0:00:00, wall=13:48 IST
=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.502 DataTime=0.307 Loss=1.381 Prec@1=66.223 Prec@5=87.034 rate=7075.29 Hz, eta=0:00:00, total=0:00:00, wall=13:48 IST
=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.502 DataTime=0.307 Loss=1.381 Prec@1=66.223 Prec@5=87.034 rate=2.20 Hz, eta=0:18:14, total=0:00:46, wall=13:48 IST
=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.502 DataTime=0.307 Loss=1.381 Prec@1=66.223 Prec@5=87.034 rate=2.20 Hz, eta=0:18:14, total=0:00:46, wall=13:48 IST
=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.484 DataTime=0.289 Loss=1.381 Prec@1=66.312 Prec@5=86.985 rate=2.20 Hz, eta=0:18:14, total=0:00:46, wall=13:48 IST
=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.484 DataTime=0.289 Loss=1.381 Prec@1=66.312 Prec@5=86.985 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=13:48 IST
=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.484 DataTime=0.289 Loss=1.381 Prec@1=66.312 Prec@5=86.985 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=13:49 IST
=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.479 DataTime=0.283 Loss=1.381 Prec@1=66.326 Prec@5=87.038 rate=2.17 Hz, eta=0:17:40, total=0:01:32, wall=13:49 IST
=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.479 DataTime=0.283 Loss=1.381 Prec@1=66.326 Prec@5=87.038 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=13:49 IST
=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.479 DataTime=0.283 Loss=1.381 Prec@1=66.326 Prec@5=87.038 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=13:50 IST
=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.476 DataTime=0.281 Loss=1.382 Prec@1=66.322 Prec@5=87.019 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=13:50 IST
=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.476 DataTime=0.281 Loss=1.382 Prec@1=66.322 Prec@5=87.019 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=13:50 IST
=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.476 DataTime=0.281 Loss=1.382 Prec@1=66.322 Prec@5=87.019 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=13:51 IST
=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.473 DataTime=0.278 Loss=1.386 Prec@1=66.260 Prec@5=86.952 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=13:51 IST
=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.473 DataTime=0.278 Loss=1.386 Prec@1=66.260 Prec@5=86.952 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=13:51 IST
=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.473 DataTime=0.278 Loss=1.386 Prec@1=66.260 Prec@5=86.952 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=13:51 IST
=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.276 Loss=1.388 Prec@1=66.212 Prec@5=86.918 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=13:51 IST
=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.276 Loss=1.388 Prec@1=66.212 Prec@5=86.918 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:51 IST
=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.276 Loss=1.388 Prec@1=66.212 Prec@5=86.918 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:52 IST
=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.275 Loss=1.389 Prec@1=66.184 Prec@5=86.882 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:52 IST
=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.275 Loss=1.389 Prec@1=66.184 Prec@5=86.882 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=13:52 IST
=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.275 Loss=1.389 Prec@1=66.184 Prec@5=86.882 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=13:53 IST
=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.274 Loss=1.393 Prec@1=66.066 Prec@5=86.827 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=13:53 IST
=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.274 Loss=1.393 Prec@1=66.066 Prec@5=86.827 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=13:53 IST
=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.471 DataTime=0.274 Loss=1.393 Prec@1=66.066 Prec@5=86.827 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=13:54 IST
=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.039 Prec@5=86.801 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=13:54 IST
=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.039 Prec@5=86.801 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:54 IST
=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.039 Prec@5=86.801 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:55 IST
=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.397 Prec@1=66.001 Prec@5=86.759 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=13:55 IST
=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.397 Prec@1=66.001 Prec@5=86.759 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=13:55 IST
=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.397 Prec@1=66.001 Prec@5=86.759 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=13:55 IST
=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.398 Prec@1=65.992 Prec@5=86.749 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=13:55 IST
=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.398 Prec@1=65.992 Prec@5=86.749 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=13:55 IST
=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.468 DataTime=0.273 Loss=1.398 Prec@1=65.992 Prec@5=86.749 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=13:56 IST
=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.401 Prec@1=65.953 Prec@5=86.716 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=13:56 IST
=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.401 Prec@1=65.953 Prec@5=86.716 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=13:56 IST
=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.401 Prec@1=65.953 Prec@5=86.716 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=13:57 IST
=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.402 Prec@1=65.931 Prec@5=86.693 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=13:57 IST
=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.402 Prec@1=65.931 Prec@5=86.693 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=13:57 IST
=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.402 Prec@1=65.931 Prec@5=86.693 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=13:58 IST
=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.403 Prec@1=65.898 Prec@5=86.680 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=13:58 IST
=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.403 Prec@1=65.898 Prec@5=86.680 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=13:58 IST
=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.403 Prec@1=65.898 Prec@5=86.680 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=13:58 IST
=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.404 Prec@1=65.875 Prec@5=86.657 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=13:58 IST
=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.404 Prec@1=65.875 Prec@5=86.657 rate=2.14 Hz, eta=0:07:47, total=0:11:39, wall=13:58 IST
=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.404 Prec@1=65.875 Prec@5=86.657 rate=2.14 Hz, eta=0:07:47, total=0:11:39, wall=13:59 IST
=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.405 Prec@1=65.838 Prec@5=86.658 rate=2.14 Hz, eta=0:07:47, total=0:11:39, wall=13:59 IST
=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.405 Prec@1=65.838 Prec@5=86.658 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=13:59 IST
=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.405 Prec@1=65.838 Prec@5=86.658 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=14:00 IST
=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.406 Prec@1=65.808 Prec@5=86.635 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=14:00 IST
=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.406 Prec@1=65.808 Prec@5=86.635 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:00 IST
=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.406 Prec@1=65.808 Prec@5=86.635 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:01 IST
=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.407 Prec@1=65.767 Prec@5=86.608 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:01 IST
=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.407 Prec@1=65.767 Prec@5=86.608 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=14:01 IST
=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.407 Prec@1=65.767 Prec@5=86.608 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=14:02 IST
=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.408 Prec@1=65.759 Prec@5=86.587 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=14:02 IST
=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.408 Prec@1=65.759 Prec@5=86.587 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=14:02 IST
=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.408 Prec@1=65.759 Prec@5=86.587 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=14:02 IST
=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.409 Prec@1=65.736 Prec@5=86.584 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=14:02 IST
=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.409 Prec@1=65.736 Prec@5=86.584 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=14:02 IST
=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.409 Prec@1=65.736 Prec@5=86.584 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=14:03 IST
=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.409 Prec@1=65.750 Prec@5=86.572 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=14:03 IST
=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.409 Prec@1=65.750 Prec@5=86.572 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=14:03 IST
=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.409 Prec@1=65.750 Prec@5=86.572 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=14:04 IST
=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.410 Prec@1=65.731 Prec@5=86.564 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=14:04 IST
=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.410 Prec@1=65.731 Prec@5=86.564 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=14:04 IST
=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.410 Prec@1=65.731 Prec@5=86.564 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=14:05 IST
=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.411 Prec@1=65.718 Prec@5=86.551 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=14:05 IST
=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.411 Prec@1=65.718 Prec@5=86.551 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=14:05 IST
=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.411 Prec@1=65.718 Prec@5=86.551 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=14:06 IST
=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.412 Prec@1=65.698 Prec@5=86.533 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=14:06 IST
=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.412 Prec@1=65.698 Prec@5=86.533 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=14:06 IST
=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.470 DataTime=0.274 Loss=1.412 Prec@1=65.698 Prec@5=86.533 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=14:06 IST
=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.412 Prec@1=65.679 Prec@5=86.522 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.412 Prec@1=65.679 Prec@5=86.522 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.412 Prec@1=65.679 Prec@5=86.522 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.412 Prec@1=65.679 Prec@5=86.521 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=14:06 IST
=> training   100.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.469 DataTime=0.274 Loss=1.412 Prec@1=65.679 Prec@5=86.521 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=14:06 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 0.00% of 1x98...Epoch=66/150 LR=0.06040 Time=6.961 Loss=0.969 Prec@1=75.195 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.961 Loss=0.969 Prec@1=75.195 Prec@5=91.602 rate=9566.09 Hz, eta=0:00:00, total=0:00:00, wall=14:06 IST
** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.961 Loss=0.969 Prec@1=75.195 Prec@5=91.602 rate=9566.09 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=0.550 Loss=1.458 Prec@1=64.496 Prec@5=86.180 rate=9566.09 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
** validation 100.00% of 1x98...Epoch=66/150 LR=0.06040 Time=0.550 Loss=1.458 Prec@1=64.496 Prec@5=86.180 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=14:07 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=4.449 DataTime=4.165 Loss=1.373 Prec@1=66.211 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=4.449 DataTime=4.165 Loss=1.373 Prec@1=66.211 Prec@5=86.719 rate=5680.66 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=4.449 DataTime=4.165 Loss=1.373 Prec@1=66.211 Prec@5=86.719 rate=5680.66 Hz, eta=0:00:00, total=0:00:00, wall=14:08 IST
=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.496 DataTime=0.298 Loss=1.365 Prec@1=66.841 Prec@5=87.100 rate=5680.66 Hz, eta=0:00:00, total=0:00:00, wall=14:08 IST
=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.496 DataTime=0.298 Loss=1.365 Prec@1=66.841 Prec@5=87.100 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=14:08 IST
=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.496 DataTime=0.298 Loss=1.365 Prec@1=66.841 Prec@5=87.100 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=14:09 IST
=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.481 DataTime=0.283 Loss=1.368 Prec@1=66.660 Prec@5=87.053 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=14:09 IST
=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.481 DataTime=0.283 Loss=1.368 Prec@1=66.660 Prec@5=87.053 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:09 IST
=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.481 DataTime=0.283 Loss=1.368 Prec@1=66.660 Prec@5=87.053 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:10 IST
=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.475 DataTime=0.278 Loss=1.379 Prec@1=66.322 Prec@5=86.906 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:10 IST
=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.475 DataTime=0.278 Loss=1.379 Prec@1=66.322 Prec@5=86.906 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=14:10 IST
=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.475 DataTime=0.278 Loss=1.379 Prec@1=66.322 Prec@5=86.906 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=14:10 IST
=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.474 DataTime=0.276 Loss=1.377 Prec@1=66.382 Prec@5=86.996 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=14:10 IST
=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.474 DataTime=0.276 Loss=1.377 Prec@1=66.382 Prec@5=86.996 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=14:10 IST
=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.474 DataTime=0.276 Loss=1.377 Prec@1=66.382 Prec@5=86.996 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=14:11 IST
=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.274 Loss=1.380 Prec@1=66.334 Prec@5=86.920 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=14:11 IST
=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.274 Loss=1.380 Prec@1=66.334 Prec@5=86.920 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:11 IST
=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.274 Loss=1.380 Prec@1=66.334 Prec@5=86.920 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:12 IST
=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.322 Prec@5=86.938 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:12 IST
=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.322 Prec@5=86.938 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=14:12 IST
=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.322 Prec@5=86.938 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=14:13 IST
=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.250 Prec@5=86.928 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=14:13 IST
=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.250 Prec@5=86.928 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=14:13 IST
=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.250 Prec@5=86.928 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=14:14 IST
=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.199 Prec@5=86.877 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=14:14 IST
=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.199 Prec@5=86.877 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:14 IST
=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.199 Prec@5=86.877 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:14 IST
=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.388 Prec@1=66.161 Prec@5=86.833 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:14 IST
=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.388 Prec@1=66.161 Prec@5=86.833 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:14 IST
=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.471 DataTime=0.275 Loss=1.388 Prec@1=66.161 Prec@5=86.833 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:15 IST
=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.390 Prec@1=66.148 Prec@5=86.806 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:15 IST
=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.390 Prec@1=66.148 Prec@5=86.806 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=14:15 IST
=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.390 Prec@1=66.148 Prec@5=86.806 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=14:16 IST
=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.393 Prec@1=66.081 Prec@5=86.782 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=14:16 IST
=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.393 Prec@1=66.081 Prec@5=86.782 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=14:16 IST
=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.393 Prec@1=66.081 Prec@5=86.782 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=14:17 IST
=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.394 Prec@1=66.048 Prec@5=86.773 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.394 Prec@1=66.048 Prec@5=86.773 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.394 Prec@1=66.048 Prec@5=86.773 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.395 Prec@1=66.024 Prec@5=86.758 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=14:17 IST
=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.395 Prec@1=66.024 Prec@5=86.758 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=14:17 IST
=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.395 Prec@1=66.024 Prec@5=86.758 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=14:18 IST
=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.396 Prec@1=66.000 Prec@5=86.730 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=14:18 IST
=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.396 Prec@1=66.000 Prec@5=86.730 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:18 IST
=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.470 DataTime=0.274 Loss=1.396 Prec@1=66.000 Prec@5=86.730 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:19 IST
=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.398 Prec@1=65.960 Prec@5=86.704 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:19 IST
=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.398 Prec@1=65.960 Prec@5=86.704 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=14:19 IST
=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.398 Prec@1=65.960 Prec@5=86.704 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=14:20 IST
=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.399 Prec@1=65.947 Prec@5=86.690 rate=2.14 Hz, eta=0:07:47, total=0:11:40, wall=14:20 IST
=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.399 Prec@1=65.947 Prec@5=86.690 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=14:20 IST
=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.274 Loss=1.399 Prec@1=65.947 Prec@5=86.690 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=14:21 IST
=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.669 rate=2.14 Hz, eta=0:07:00, total=0:12:26, wall=14:21 IST
=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.669 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:21 IST
=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.669 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:21 IST
=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.674 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=14:21 IST
=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.674 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=14:21 IST
=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.401 Prec@1=65.925 Prec@5=86.674 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=14:22 IST
=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.915 Prec@5=86.677 rate=2.15 Hz, eta=0:05:27, total=0:13:59, wall=14:22 IST
=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.915 Prec@5=86.677 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=14:22 IST
=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.401 Prec@1=65.915 Prec@5=86.677 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=14:23 IST
=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.402 Prec@1=65.894 Prec@5=86.662 rate=2.14 Hz, eta=0:04:40, total=0:14:46, wall=14:23 IST
=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.402 Prec@1=65.894 Prec@5=86.662 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=14:23 IST
=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.273 Loss=1.402 Prec@1=65.894 Prec@5=86.662 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=14:24 IST
=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.404 Prec@1=65.847 Prec@5=86.633 rate=2.14 Hz, eta=0:03:54, total=0:15:32, wall=14:24 IST
=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.404 Prec@1=65.847 Prec@5=86.633 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=14:24 IST
=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.404 Prec@1=65.847 Prec@5=86.633 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=14:24 IST
=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.834 Prec@5=86.620 rate=2.15 Hz, eta=0:03:07, total=0:16:19, wall=14:24 IST
=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.834 Prec@5=86.620 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=14:24 IST
=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.405 Prec@1=65.834 Prec@5=86.620 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=14:25 IST
=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.406 Prec@1=65.819 Prec@5=86.603 rate=2.14 Hz, eta=0:02:20, total=0:17:06, wall=14:25 IST
=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.406 Prec@1=65.819 Prec@5=86.603 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=14:25 IST
=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.406 Prec@1=65.819 Prec@5=86.603 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=14:26 IST
=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.407 Prec@1=65.806 Prec@5=86.598 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=14:26 IST
=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.407 Prec@1=65.806 Prec@5=86.598 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=14:26 IST
=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.469 DataTime=0.273 Loss=1.407 Prec@1=65.806 Prec@5=86.598 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=14:27 IST
=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.408 Prec@1=65.777 Prec@5=86.571 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.408 Prec@1=65.777 Prec@5=86.571 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.408 Prec@1=65.777 Prec@5=86.571 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.408 Prec@1=65.777 Prec@5=86.573 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=14:27 IST
=> training   100.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.468 DataTime=0.272 Loss=1.408 Prec@1=65.777 Prec@5=86.573 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=14:27 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 0.00% of 1x98...Epoch=67/150 LR=0.05937 Time=7.022 Loss=0.972 Prec@1=75.195 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=7.022 Loss=0.972 Prec@1=75.195 Prec@5=92.188 rate=8909.24 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST
** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=7.022 Loss=0.972 Prec@1=75.195 Prec@5=92.188 rate=8909.24 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=0.546 Loss=1.442 Prec@1=64.928 Prec@5=86.434 rate=8909.24 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** validation 100.00% of 1x98...Epoch=67/150 LR=0.05937 Time=0.546 Loss=1.442 Prec@1=64.928 Prec@5=86.434 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=14:28 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=4.568 DataTime=4.313 Loss=1.336 Prec@1=67.578 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=4.568 DataTime=4.313 Loss=1.336 Prec@1=67.578 Prec@5=87.500 rate=3645.32 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=4.568 DataTime=4.313 Loss=1.336 Prec@1=67.578 Prec@5=87.500 rate=3645.32 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.495 DataTime=0.299 Loss=1.356 Prec@1=67.042 Prec@5=87.179 rate=3645.32 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.495 DataTime=0.299 Loss=1.356 Prec@1=67.042 Prec@5=87.179 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=14:29 IST
=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.495 DataTime=0.299 Loss=1.356 Prec@1=67.042 Prec@5=87.179 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=14:29 IST
=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.478 DataTime=0.282 Loss=1.359 Prec@1=66.900 Prec@5=87.216 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=14:29 IST
=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.478 DataTime=0.282 Loss=1.359 Prec@1=66.900 Prec@5=87.216 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=14:29 IST
=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.478 DataTime=0.282 Loss=1.359 Prec@1=66.900 Prec@5=87.216 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=14:30 IST
=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.276 Loss=1.359 Prec@1=66.807 Prec@5=87.209 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=14:30 IST
=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.276 Loss=1.359 Prec@1=66.807 Prec@5=87.209 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=14:30 IST
=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.276 Loss=1.359 Prec@1=66.807 Prec@5=87.209 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=14:31 IST
=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.277 Loss=1.366 Prec@1=66.664 Prec@5=87.138 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=14:31 IST
=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.277 Loss=1.366 Prec@1=66.664 Prec@5=87.138 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=14:31 IST
=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.472 DataTime=0.277 Loss=1.366 Prec@1=66.664 Prec@5=87.138 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=14:32 IST
=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.369 Prec@1=66.599 Prec@5=87.097 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=14:32 IST
=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.369 Prec@1=66.599 Prec@5=87.097 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:32 IST
=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.369 Prec@1=66.599 Prec@5=87.097 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:32 IST
=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.520 Prec@5=87.011 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=14:32 IST
=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.520 Prec@5=87.011 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:32 IST
=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.520 Prec@5=87.011 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:33 IST
=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.274 Loss=1.376 Prec@1=66.443 Prec@5=86.974 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:33 IST
=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.274 Loss=1.376 Prec@1=66.443 Prec@5=86.974 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=14:33 IST
=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.274 Loss=1.376 Prec@1=66.443 Prec@5=86.974 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=14:34 IST
=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.380 Prec@1=66.382 Prec@5=86.915 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=14:34 IST
=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.380 Prec@1=66.382 Prec@5=86.915 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=14:34 IST
=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.380 Prec@1=66.382 Prec@5=86.915 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=14:35 IST
=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.325 Prec@5=86.903 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=14:35 IST
=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.325 Prec@5=86.903 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:35 IST
=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.325 Prec@5=86.903 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:36 IST
=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.383 Prec@1=66.265 Prec@5=86.896 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:36 IST
=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.383 Prec@1=66.265 Prec@5=86.896 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:36 IST
=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.383 Prec@1=66.265 Prec@5=86.896 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:36 IST
=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.236 Prec@5=86.887 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:36 IST
=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.236 Prec@5=86.887 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=14:36 IST
=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.236 Prec@5=86.887 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=14:37 IST
=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.219 Prec@5=86.887 rate=2.14 Hz, eta=0:10:53, total=0:08:33, wall=14:37 IST
=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.219 Prec@5=86.887 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:37 IST
=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.219 Prec@5=86.887 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:38 IST
=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.198 Prec@5=86.872 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:38 IST
=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.198 Prec@5=86.872 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:38 IST
=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.386 Prec@1=66.198 Prec@5=86.872 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:39 IST
=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.175 Prec@5=86.847 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:39 IST
=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.175 Prec@5=86.847 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:39 IST
=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.175 Prec@5=86.847 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:39 IST
=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.157 Prec@5=86.813 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=14:39 IST
=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.157 Prec@5=86.813 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=14:39 IST
=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.157 Prec@5=86.813 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=14:40 IST
=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.132 Prec@5=86.788 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=14:40 IST
=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.132 Prec@5=86.788 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=14:40 IST
=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.132 Prec@5=86.788 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=14:41 IST
=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.114 Prec@5=86.786 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=14:41 IST
=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.114 Prec@5=86.786 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=14:41 IST
=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.391 Prec@1=66.114 Prec@5=86.786 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=14:42 IST
=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.392 Prec@1=66.098 Prec@5=86.772 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=14:42 IST
=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.392 Prec@1=66.098 Prec@5=86.772 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=14:42 IST
=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.392 Prec@1=66.098 Prec@5=86.772 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=14:43 IST
=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.394 Prec@1=66.081 Prec@5=86.754 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=14:43 IST
=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.394 Prec@1=66.081 Prec@5=86.754 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=14:43 IST
=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.394 Prec@1=66.081 Prec@5=86.754 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=14:43 IST
=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.395 Prec@1=66.051 Prec@5=86.739 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=14:43 IST
=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.395 Prec@1=66.051 Prec@5=86.739 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=14:43 IST
=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.395 Prec@1=66.051 Prec@5=86.739 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=14:44 IST
=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.396 Prec@1=66.031 Prec@5=86.729 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=14:44 IST
=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.396 Prec@1=66.031 Prec@5=86.729 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=14:44 IST
=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.396 Prec@1=66.031 Prec@5=86.729 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=14:45 IST
=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.396 Prec@1=66.026 Prec@5=86.714 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=14:45 IST
=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.396 Prec@1=66.026 Prec@5=86.714 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=14:45 IST
=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.276 Loss=1.396 Prec@1=66.026 Prec@5=86.714 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=14:46 IST
=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.397 Prec@1=66.005 Prec@5=86.707 rate=2.13 Hz, eta=0:02:21, total=0:17:13, wall=14:46 IST
=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.397 Prec@1=66.005 Prec@5=86.707 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=14:46 IST
=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.397 Prec@1=66.005 Prec@5=86.707 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=14:47 IST
=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.398 Prec@1=65.985 Prec@5=86.697 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=14:47 IST
=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.398 Prec@1=65.985 Prec@5=86.697 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=14:47 IST
=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.398 Prec@1=65.985 Prec@5=86.697 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=14:47 IST
=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.399 Prec@1=65.976 Prec@5=86.684 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=14:47 IST
=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.399 Prec@1=65.976 Prec@5=86.684 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=14:47 IST
=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.399 Prec@1=65.976 Prec@5=86.684 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=14:47 IST
=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.399 Prec@1=65.976 Prec@5=86.683 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=14:47 IST
=> training   100.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.471 DataTime=0.275 Loss=1.399 Prec@1=65.976 Prec@5=86.683 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=14:47 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:47 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:47 IST
=> validation 0.00% of 1x98...Epoch=68/150 LR=0.05834 Time=6.238 Loss=0.863 Prec@1=77.734 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=14:47 IST
=> validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.238 Loss=0.863 Prec@1=77.734 Prec@5=94.141 rate=11165.20 Hz, eta=0:00:00, total=0:00:00, wall=14:47 IST
** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.238 Loss=0.863 Prec@1=77.734 Prec@5=94.141 rate=11165.20 Hz, eta=0:00:00, total=0:00:00, wall=14:48 IST
** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=0.549 Loss=1.462 Prec@1=64.568 Prec@5=86.154 rate=11165.20 Hz, eta=0:00:00, total=0:00:00, wall=14:48 IST
** validation 100.00% of 1x98...Epoch=68/150 LR=0.05834 Time=0.549 Loss=1.462 Prec@1=64.568 Prec@5=86.154 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=14:48 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> training   0.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=4.940 DataTime=4.705 Loss=1.314 Prec@1=68.359 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=4.940 DataTime=4.705 Loss=1.314 Prec@1=68.359 Prec@5=86.328 rate=7479.49 Hz, eta=0:00:00, total=0:00:00, wall=14:48 IST
=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=4.940 DataTime=4.705 Loss=1.314 Prec@1=68.359 Prec@5=86.328 rate=7479.49 Hz, eta=0:00:00, total=0:00:00, wall=14:49 IST
=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.504 DataTime=0.310 Loss=1.367 Prec@1=66.747 Prec@5=87.179 rate=7479.49 Hz, eta=0:00:00, total=0:00:00, wall=14:49 IST
=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.504 DataTime=0.310 Loss=1.367 Prec@1=66.747 Prec@5=87.179 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=14:49 IST
=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.504 DataTime=0.310 Loss=1.367 Prec@1=66.747 Prec@5=87.179 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=14:50 IST
=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.483 DataTime=0.288 Loss=1.361 Prec@1=66.687 Prec@5=87.251 rate=2.20 Hz, eta=0:18:12, total=0:00:45, wall=14:50 IST
=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.483 DataTime=0.288 Loss=1.361 Prec@1=66.687 Prec@5=87.251 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:50 IST
=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.483 DataTime=0.288 Loss=1.361 Prec@1=66.687 Prec@5=87.251 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:51 IST
=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.479 DataTime=0.284 Loss=1.364 Prec@1=66.628 Prec@5=87.170 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=14:51 IST
=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.479 DataTime=0.284 Loss=1.364 Prec@1=66.628 Prec@5=87.170 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=14:51 IST
=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.479 DataTime=0.284 Loss=1.364 Prec@1=66.628 Prec@5=87.170 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=14:51 IST
=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.476 DataTime=0.281 Loss=1.364 Prec@1=66.684 Prec@5=87.148 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=14:51 IST
=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.476 DataTime=0.281 Loss=1.364 Prec@1=66.684 Prec@5=87.148 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=14:51 IST
=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.476 DataTime=0.281 Loss=1.364 Prec@1=66.684 Prec@5=87.148 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=14:52 IST
=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.473 DataTime=0.277 Loss=1.367 Prec@1=66.545 Prec@5=87.156 rate=2.16 Hz, eta=0:16:15, total=0:03:06, wall=14:52 IST
=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.473 DataTime=0.277 Loss=1.367 Prec@1=66.545 Prec@5=87.156 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=14:52 IST
=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.473 DataTime=0.277 Loss=1.367 Prec@1=66.545 Prec@5=87.156 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=14:53 IST
=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.462 Prec@5=87.115 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=14:53 IST
=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.462 Prec@5=87.115 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:53 IST
=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.462 Prec@5=87.115 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:54 IST
=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.472 DataTime=0.275 Loss=1.375 Prec@1=66.359 Prec@5=87.041 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=14:54 IST
=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.472 DataTime=0.275 Loss=1.375 Prec@1=66.359 Prec@5=87.041 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:54 IST
=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.472 DataTime=0.275 Loss=1.375 Prec@1=66.359 Prec@5=87.041 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:55 IST
=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.377 Prec@1=66.305 Prec@5=87.000 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:55 IST
=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.377 Prec@1=66.305 Prec@5=87.000 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:55 IST
=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.377 Prec@1=66.305 Prec@5=87.000 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:55 IST
=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.379 Prec@1=66.283 Prec@5=86.966 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=14:55 IST
=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.379 Prec@1=66.283 Prec@5=86.966 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=14:55 IST
=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.379 Prec@1=66.283 Prec@5=86.966 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=14:56 IST
=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.248 Prec@5=86.944 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=14:56 IST
=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.248 Prec@5=86.944 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:56 IST
=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.380 Prec@1=66.248 Prec@5=86.944 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:57 IST
=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.239 Prec@5=86.913 rate=2.15 Hz, eta=0:11:39, total=0:07:46, wall=14:57 IST
=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.239 Prec@5=86.913 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:57 IST
=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.239 Prec@5=86.913 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:58 IST
=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.382 Prec@1=66.231 Prec@5=86.909 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:58 IST
=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.382 Prec@1=66.231 Prec@5=86.909 rate=2.14 Hz, eta=0:10:07, total=0:09:19, wall=14:58 IST
=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.382 Prec@1=66.231 Prec@5=86.909 rate=2.14 Hz, eta=0:10:07, total=0:09:19, wall=14:58 IST
=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.211 Prec@5=86.915 rate=2.14 Hz, eta=0:10:07, total=0:09:19, wall=14:58 IST
=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.211 Prec@5=86.915 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:58 IST
=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.211 Prec@5=86.915 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:59 IST
=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.383 Prec@1=66.193 Prec@5=86.900 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:59 IST
=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.383 Prec@1=66.193 Prec@5=86.900 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=14:59 IST
=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.383 Prec@1=66.193 Prec@5=86.900 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=15:00 IST
=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.165 Prec@5=86.880 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=15:00 IST
=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.165 Prec@5=86.880 rate=2.14 Hz, eta=0:07:47, total=0:11:41, wall=15:00 IST
=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.165 Prec@5=86.880 rate=2.14 Hz, eta=0:07:47, total=0:11:41, wall=15:01 IST
=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.116 Prec@5=86.858 rate=2.14 Hz, eta=0:07:47, total=0:11:41, wall=15:01 IST
=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.116 Prec@5=86.858 rate=2.14 Hz, eta=0:07:00, total=0:12:27, wall=15:01 IST
=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.116 Prec@5=86.858 rate=2.14 Hz, eta=0:07:00, total=0:12:27, wall=15:02 IST
=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.119 Prec@5=86.841 rate=2.14 Hz, eta=0:07:00, total=0:12:27, wall=15:02 IST
=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.119 Prec@5=86.841 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=15:02 IST
=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.387 Prec@1=66.119 Prec@5=86.841 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=15:02 IST
=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.105 Prec@5=86.824 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=15:02 IST
=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.105 Prec@5=86.824 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:02 IST
=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.389 Prec@1=66.105 Prec@5=86.824 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:03 IST
=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.390 Prec@1=66.080 Prec@5=86.809 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:03 IST
=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.390 Prec@1=66.080 Prec@5=86.809 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=15:03 IST
=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.390 Prec@1=66.080 Prec@5=86.809 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=15:04 IST
=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.391 Prec@1=66.064 Prec@5=86.791 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=15:04 IST
=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.391 Prec@1=66.064 Prec@5=86.791 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=15:04 IST
=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.391 Prec@1=66.064 Prec@5=86.791 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=15:05 IST
=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.392 Prec@1=66.050 Prec@5=86.786 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=15:05 IST
=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.392 Prec@1=66.050 Prec@5=86.786 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:05 IST
=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.275 Loss=1.392 Prec@1=66.050 Prec@5=86.786 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:05 IST
=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.393 Prec@1=66.042 Prec@5=86.776 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:05 IST
=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.393 Prec@1=66.042 Prec@5=86.776 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:05 IST
=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.393 Prec@1=66.042 Prec@5=86.776 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:06 IST
=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.020 Prec@5=86.760 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:06 IST
=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.020 Prec@5=86.760 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=15:06 IST
=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.470 DataTime=0.274 Loss=1.394 Prec@1=66.020 Prec@5=86.760 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=15:07 IST
=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.394 Prec@1=66.015 Prec@5=86.752 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=15:07 IST
=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.394 Prec@1=66.015 Prec@5=86.752 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=15:07 IST
=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.394 Prec@1=66.015 Prec@5=86.752 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=15:08 IST
=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.395 Prec@1=65.999 Prec@5=86.745 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.395 Prec@1=65.999 Prec@5=86.745 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.395 Prec@1=65.999 Prec@5=86.745 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.395 Prec@1=65.999 Prec@5=86.745 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   100.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.469 DataTime=0.274 Loss=1.395 Prec@1=65.999 Prec@5=86.745 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:08 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:08 IST
=> validation 0.00% of 1x98...Epoch=69/150 LR=0.05730 Time=7.001 Loss=1.027 Prec@1=73.047 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=15:08 IST
=> validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=7.001 Loss=1.027 Prec@1=73.047 Prec@5=91.797 rate=8404.70 Hz, eta=0:00:00, total=0:00:00, wall=15:08 IST
** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=7.001 Loss=1.027 Prec@1=73.047 Prec@5=91.797 rate=8404.70 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=0.550 Loss=1.471 Prec@1=64.280 Prec@5=85.992 rate=8404.70 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
** validation 100.00% of 1x98...Epoch=69/150 LR=0.05730 Time=0.550 Loss=1.471 Prec@1=64.280 Prec@5=85.992 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=15:09 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=4.572 DataTime=4.273 Loss=1.395 Prec@1=64.062 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=4.572 DataTime=4.273 Loss=1.395 Prec@1=64.062 Prec@5=85.742 rate=4607.55 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=4.572 DataTime=4.273 Loss=1.395 Prec@1=64.062 Prec@5=85.742 rate=4607.55 Hz, eta=0:00:00, total=0:00:00, wall=15:10 IST
=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.501 DataTime=0.305 Loss=1.379 Prec@1=66.567 Prec@5=86.870 rate=4607.55 Hz, eta=0:00:00, total=0:00:00, wall=15:10 IST
=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.501 DataTime=0.305 Loss=1.379 Prec@1=66.567 Prec@5=86.870 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=15:10 IST
=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.501 DataTime=0.305 Loss=1.379 Prec@1=66.567 Prec@5=86.870 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=15:10 IST
=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.481 DataTime=0.284 Loss=1.371 Prec@1=66.577 Prec@5=87.094 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=15:10 IST
=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.481 DataTime=0.284 Loss=1.371 Prec@1=66.577 Prec@5=87.094 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=15:10 IST
=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.481 DataTime=0.284 Loss=1.371 Prec@1=66.577 Prec@5=87.094 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=15:11 IST
=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.476 DataTime=0.278 Loss=1.367 Prec@1=66.690 Prec@5=87.163 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=15:11 IST
=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.476 DataTime=0.278 Loss=1.367 Prec@1=66.690 Prec@5=87.163 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=15:11 IST
=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.476 DataTime=0.278 Loss=1.367 Prec@1=66.690 Prec@5=87.163 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=15:12 IST
=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.475 DataTime=0.278 Loss=1.368 Prec@1=66.647 Prec@5=87.146 rate=2.17 Hz, eta=0:16:54, total=0:02:18, wall=15:12 IST
=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.475 DataTime=0.278 Loss=1.368 Prec@1=66.647 Prec@5=87.146 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=15:12 IST
=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.475 DataTime=0.278 Loss=1.368 Prec@1=66.647 Prec@5=87.146 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=15:13 IST
=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.474 DataTime=0.277 Loss=1.368 Prec@1=66.637 Prec@5=87.146 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=15:13 IST
=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.474 DataTime=0.277 Loss=1.368 Prec@1=66.637 Prec@5=87.146 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=15:13 IST
=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.474 DataTime=0.277 Loss=1.368 Prec@1=66.637 Prec@5=87.146 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=15:13 IST
=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.274 Loss=1.368 Prec@1=66.611 Prec@5=87.124 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=15:13 IST
=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.274 Loss=1.368 Prec@1=66.611 Prec@5=87.124 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=15:13 IST
=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.274 Loss=1.368 Prec@1=66.611 Prec@5=87.124 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=15:14 IST
=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.274 Loss=1.369 Prec@1=66.618 Prec@5=87.083 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=15:14 IST
=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.274 Loss=1.369 Prec@1=66.618 Prec@5=87.083 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=15:14 IST
=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.274 Loss=1.369 Prec@1=66.618 Prec@5=87.083 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=15:15 IST
=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.580 Prec@5=87.062 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=15:15 IST
=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.580 Prec@5=87.062 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=15:15 IST
=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.371 Prec@1=66.580 Prec@5=87.062 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=15:16 IST
=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.372 Prec@1=66.581 Prec@5=87.056 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=15:16 IST
=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.372 Prec@1=66.581 Prec@5=87.056 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=15:16 IST
=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.372 Prec@1=66.581 Prec@5=87.056 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=15:17 IST
=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.587 Prec@5=87.043 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=15:17 IST
=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.587 Prec@5=87.043 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:17 IST
=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.587 Prec@5=87.043 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:17 IST
=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.560 Prec@5=87.038 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:17 IST
=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.560 Prec@5=87.038 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=15:17 IST
=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.372 Prec@1=66.560 Prec@5=87.038 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=15:18 IST
=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.374 Prec@1=66.528 Prec@5=87.020 rate=2.14 Hz, eta=0:10:56, total=0:08:35, wall=15:18 IST
=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.374 Prec@1=66.528 Prec@5=87.020 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=15:18 IST
=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.374 Prec@1=66.528 Prec@5=87.020 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=15:19 IST
=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.375 Prec@1=66.495 Prec@5=87.003 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=15:19 IST
=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.375 Prec@1=66.495 Prec@5=87.003 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=15:19 IST
=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.375 Prec@1=66.495 Prec@5=87.003 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=15:20 IST
=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.276 Loss=1.376 Prec@1=66.481 Prec@5=86.996 rate=2.14 Hz, eta=0:09:22, total=0:10:09, wall=15:20 IST
=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.276 Loss=1.376 Prec@1=66.481 Prec@5=86.996 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=15:20 IST
=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.276 Loss=1.376 Prec@1=66.481 Prec@5=86.996 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=15:21 IST
=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.378 Prec@1=66.441 Prec@5=86.967 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=15:21 IST
=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.378 Prec@1=66.441 Prec@5=86.967 rate=2.13 Hz, eta=0:07:49, total=0:11:43, wall=15:21 IST
=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.472 DataTime=0.276 Loss=1.378 Prec@1=66.441 Prec@5=86.967 rate=2.13 Hz, eta=0:07:49, total=0:11:43, wall=15:21 IST
=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.417 Prec@5=86.947 rate=2.13 Hz, eta=0:07:49, total=0:11:43, wall=15:21 IST
=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.417 Prec@5=86.947 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=15:21 IST
=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.417 Prec@5=86.947 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=15:22 IST
=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.404 Prec@5=86.940 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=15:22 IST
=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.404 Prec@5=86.940 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=15:22 IST
=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.379 Prec@1=66.404 Prec@5=86.940 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=15:23 IST
=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.381 Prec@1=66.374 Prec@5=86.920 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=15:23 IST
=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.381 Prec@1=66.374 Prec@5=86.920 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=15:23 IST
=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.381 Prec@1=66.374 Prec@5=86.920 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=15:24 IST
=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.349 Prec@5=86.920 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=15:24 IST
=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.349 Prec@5=86.920 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=15:24 IST
=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.381 Prec@1=66.349 Prec@5=86.920 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=15:24 IST
=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.329 Prec@5=86.903 rate=2.14 Hz, eta=0:04:41, total=0:14:50, wall=15:24 IST
=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.329 Prec@5=86.903 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=15:24 IST
=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.382 Prec@1=66.329 Prec@5=86.903 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=15:25 IST
=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.383 Prec@1=66.325 Prec@5=86.905 rate=2.14 Hz, eta=0:03:55, total=0:15:37, wall=15:25 IST
=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.383 Prec@1=66.325 Prec@5=86.905 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=15:25 IST
=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.383 Prec@1=66.325 Prec@5=86.905 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=15:26 IST
=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.305 Prec@5=86.896 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=15:26 IST
=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.305 Prec@5=86.896 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=15:26 IST
=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.384 Prec@1=66.305 Prec@5=86.896 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=15:27 IST
=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.283 Prec@5=86.886 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=15:27 IST
=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.283 Prec@5=86.886 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=15:27 IST
=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.471 DataTime=0.275 Loss=1.384 Prec@1=66.283 Prec@5=86.886 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=15:28 IST
=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.386 Prec@1=66.251 Prec@5=86.866 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=15:28 IST
=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.386 Prec@1=66.251 Prec@5=86.866 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=15:28 IST
=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.386 Prec@1=66.251 Prec@5=86.866 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=15:28 IST
=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.222 Prec@5=86.842 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=15:28 IST
=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.222 Prec@5=86.842 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=15:28 IST
=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.222 Prec@5=86.842 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=15:28 IST
=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.222 Prec@5=86.842 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=15:28 IST
=> training   100.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.470 DataTime=0.275 Loss=1.387 Prec@1=66.222 Prec@5=86.842 rate=2.14 Hz, eta=0:00:00, total=0:19:31, wall=15:28 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:28 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:28 IST
=> validation 0.00% of 1x98...Epoch=70/150 LR=0.05627 Time=7.029 Loss=0.931 Prec@1=74.609 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=15:28 IST
=> validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=7.029 Loss=0.931 Prec@1=74.609 Prec@5=93.750 rate=11000.38 Hz, eta=0:00:00, total=0:00:00, wall=15:28 IST
** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=7.029 Loss=0.931 Prec@1=74.609 Prec@5=93.750 rate=11000.38 Hz, eta=0:00:00, total=0:00:00, wall=15:29 IST
** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=0.552 Loss=1.445 Prec@1=64.948 Prec@5=86.518 rate=11000.38 Hz, eta=0:00:00, total=0:00:00, wall=15:29 IST
** validation 100.00% of 1x98...Epoch=70/150 LR=0.05627 Time=0.552 Loss=1.445 Prec@1=64.948 Prec@5=86.518 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=15:29 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> training   0.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.045 DataTime=4.802 Loss=1.466 Prec@1=61.328 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.045 DataTime=4.802 Loss=1.466 Prec@1=61.328 Prec@5=86.523 rate=9628.72 Hz, eta=0:00:00, total=0:00:00, wall=15:29 IST
=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.045 DataTime=4.802 Loss=1.466 Prec@1=61.328 Prec@5=86.523 rate=9628.72 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST
=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.509 DataTime=0.311 Loss=1.341 Prec@1=67.126 Prec@5=87.492 rate=9628.72 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST
=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.509 DataTime=0.311 Loss=1.341 Prec@1=67.126 Prec@5=87.492 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=15:30 IST
=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.509 DataTime=0.311 Loss=1.341 Prec@1=67.126 Prec@5=87.492 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=15:31 IST
=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.483 DataTime=0.286 Loss=1.343 Prec@1=67.103 Prec@5=87.424 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=15:31 IST
=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.483 DataTime=0.286 Loss=1.343 Prec@1=67.103 Prec@5=87.424 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=15:31 IST
=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.483 DataTime=0.286 Loss=1.343 Prec@1=67.103 Prec@5=87.424 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=15:32 IST
=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.478 DataTime=0.282 Loss=1.352 Prec@1=66.979 Prec@5=87.344 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=15:32 IST
=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.478 DataTime=0.282 Loss=1.352 Prec@1=66.979 Prec@5=87.344 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=15:32 IST
=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.478 DataTime=0.282 Loss=1.352 Prec@1=66.979 Prec@5=87.344 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=15:32 IST
=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.475 DataTime=0.279 Loss=1.355 Prec@1=66.908 Prec@5=87.288 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=15:32 IST
=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.475 DataTime=0.279 Loss=1.355 Prec@1=66.908 Prec@5=87.288 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=15:32 IST
=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.475 DataTime=0.279 Loss=1.355 Prec@1=66.908 Prec@5=87.288 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=15:33 IST
=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.354 Prec@1=66.956 Prec@5=87.276 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=15:33 IST
=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.354 Prec@1=66.956 Prec@5=87.276 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=15:33 IST
=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.354 Prec@1=66.956 Prec@5=87.276 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=15:34 IST
=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.356 Prec@1=66.874 Prec@5=87.241 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=15:34 IST
=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.356 Prec@1=66.874 Prec@5=87.241 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=15:34 IST
=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.356 Prec@1=66.874 Prec@5=87.241 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=15:35 IST
=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.360 Prec@1=66.789 Prec@5=87.195 rate=2.15 Hz, eta=0:14:45, total=0:04:39, wall=15:35 IST
=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.360 Prec@1=66.789 Prec@5=87.195 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=15:35 IST
=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.474 DataTime=0.279 Loss=1.360 Prec@1=66.789 Prec@5=87.195 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=15:36 IST
=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.363 Prec@1=66.713 Prec@5=87.204 rate=2.14 Hz, eta=0:14:01, total=0:05:27, wall=15:36 IST
=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.363 Prec@1=66.713 Prec@5=87.204 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=15:36 IST
=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.363 Prec@1=66.713 Prec@5=87.204 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=15:36 IST
=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.681 Prec@5=87.193 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=15:36 IST
=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.681 Prec@5=87.193 rate=2.14 Hz, eta=0:12:28, total=0:07:01, wall=15:36 IST
=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.681 Prec@5=87.193 rate=2.14 Hz, eta=0:12:28, total=0:07:01, wall=15:37 IST
=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.668 Prec@5=87.198 rate=2.14 Hz, eta=0:12:28, total=0:07:01, wall=15:37 IST
=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.668 Prec@5=87.198 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:37 IST
=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.364 Prec@1=66.668 Prec@5=87.198 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:38 IST
=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.365 Prec@1=66.633 Prec@5=87.166 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:38 IST
=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.365 Prec@1=66.633 Prec@5=87.166 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=15:38 IST
=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.365 Prec@1=66.633 Prec@5=87.166 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=15:39 IST
=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.277 Loss=1.368 Prec@1=66.595 Prec@5=87.135 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=15:39 IST
=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.277 Loss=1.368 Prec@1=66.595 Prec@5=87.135 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=15:39 IST
=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.277 Loss=1.368 Prec@1=66.595 Prec@5=87.135 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=15:40 IST
=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.369 Prec@1=66.570 Prec@5=87.116 rate=2.13 Hz, eta=0:10:09, total=0:09:22, wall=15:40 IST
=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.369 Prec@1=66.570 Prec@5=87.116 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:40 IST
=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.369 Prec@1=66.570 Prec@5=87.116 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:40 IST
=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.370 Prec@1=66.559 Prec@5=87.113 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:40 IST
=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.370 Prec@1=66.559 Prec@5=87.113 rate=2.13 Hz, eta=0:08:36, total=0:10:57, wall=15:40 IST
=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.473 DataTime=0.278 Loss=1.370 Prec@1=66.559 Prec@5=87.113 rate=2.13 Hz, eta=0:08:36, total=0:10:57, wall=15:41 IST
=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.370 Prec@1=66.555 Prec@5=87.112 rate=2.13 Hz, eta=0:08:36, total=0:10:57, wall=15:41 IST
=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.370 Prec@1=66.555 Prec@5=87.112 rate=2.14 Hz, eta=0:07:49, total=0:11:43, wall=15:41 IST
=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.370 Prec@1=66.555 Prec@5=87.112 rate=2.14 Hz, eta=0:07:49, total=0:11:43, wall=15:42 IST
=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.372 Prec@1=66.522 Prec@5=87.085 rate=2.14 Hz, eta=0:07:49, total=0:11:43, wall=15:42 IST
=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.372 Prec@1=66.522 Prec@5=87.085 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=15:42 IST
=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.372 Prec@1=66.522 Prec@5=87.085 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=15:43 IST
=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.373 Prec@1=66.504 Prec@5=87.064 rate=2.13 Hz, eta=0:07:02, total=0:12:30, wall=15:43 IST
=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.373 Prec@1=66.504 Prec@5=87.064 rate=2.13 Hz, eta=0:06:16, total=0:13:17, wall=15:43 IST
=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.373 Prec@1=66.504 Prec@5=87.064 rate=2.13 Hz, eta=0:06:16, total=0:13:17, wall=15:43 IST
=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.374 Prec@1=66.481 Prec@5=87.055 rate=2.13 Hz, eta=0:06:16, total=0:13:17, wall=15:43 IST
=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.374 Prec@1=66.481 Prec@5=87.055 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:43 IST
=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.374 Prec@1=66.481 Prec@5=87.055 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:44 IST
=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.278 Loss=1.376 Prec@1=66.456 Prec@5=87.023 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:44 IST
=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.278 Loss=1.376 Prec@1=66.456 Prec@5=87.023 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=15:44 IST
=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.278 Loss=1.376 Prec@1=66.456 Prec@5=87.023 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=15:45 IST
=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.377 Prec@1=66.421 Prec@5=86.999 rate=2.13 Hz, eta=0:04:42, total=0:14:52, wall=15:45 IST
=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.377 Prec@1=66.421 Prec@5=86.999 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=15:45 IST
=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.377 Prec@1=66.421 Prec@5=86.999 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=15:46 IST
=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.379 Prec@1=66.394 Prec@5=86.968 rate=2.13 Hz, eta=0:03:55, total=0:15:39, wall=15:46 IST
=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.379 Prec@1=66.394 Prec@5=86.968 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=15:46 IST
=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.379 Prec@1=66.394 Prec@5=86.968 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=15:47 IST
=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.380 Prec@1=66.363 Prec@5=86.942 rate=2.13 Hz, eta=0:03:08, total=0:16:26, wall=15:47 IST
=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.380 Prec@1=66.363 Prec@5=86.942 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=15:47 IST
=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.380 Prec@1=66.363 Prec@5=86.942 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=15:47 IST
=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.357 Prec@5=86.925 rate=2.13 Hz, eta=0:02:21, total=0:17:14, wall=15:47 IST
=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.357 Prec@5=86.925 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=15:47 IST
=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.357 Prec@5=86.925 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=15:48 IST
=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.347 Prec@5=86.925 rate=2.13 Hz, eta=0:01:34, total=0:18:00, wall=15:48 IST
=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.347 Prec@5=86.925 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=15:48 IST
=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.472 DataTime=0.277 Loss=1.381 Prec@1=66.347 Prec@5=86.925 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=15:49 IST
=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.471 DataTime=0.276 Loss=1.382 Prec@1=66.333 Prec@5=86.905 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.471 DataTime=0.276 Loss=1.382 Prec@1=66.333 Prec@5=86.905 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.471 DataTime=0.276 Loss=1.382 Prec@1=66.333 Prec@5=86.905 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.471 DataTime=0.276 Loss=1.382 Prec@1=66.332 Prec@5=86.905 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=15:49 IST
=> training   100.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.471 DataTime=0.276 Loss=1.382 Prec@1=66.332 Prec@5=86.905 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=15:49 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:49 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:49 IST
=> validation 0.00% of 1x98...Epoch=71/150 LR=0.05523 Time=6.891 Loss=0.834 Prec@1=77.344 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=15:49 IST
=> validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=6.891 Loss=0.834 Prec@1=77.344 Prec@5=94.727 rate=8316.77 Hz, eta=0:00:00, total=0:00:00, wall=15:49 IST
** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=6.891 Loss=0.834 Prec@1=77.344 Prec@5=94.727 rate=8316.77 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=0.557 Loss=1.441 Prec@1=64.808 Prec@5=86.392 rate=8316.77 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** validation 100.00% of 1x98...Epoch=71/150 LR=0.05523 Time=0.557 Loss=1.441 Prec@1=64.808 Prec@5=86.392 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=15:50 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=4.438 DataTime=4.192 Loss=1.364 Prec@1=68.359 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=4.438 DataTime=4.192 Loss=1.364 Prec@1=68.359 Prec@5=88.867 rate=6540.82 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=4.438 DataTime=4.192 Loss=1.364 Prec@1=68.359 Prec@5=88.867 rate=6540.82 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.499 DataTime=0.304 Loss=1.334 Prec@1=67.023 Prec@5=87.591 rate=6540.82 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.499 DataTime=0.304 Loss=1.334 Prec@1=67.023 Prec@5=87.591 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:51 IST
=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.499 DataTime=0.304 Loss=1.334 Prec@1=67.023 Prec@5=87.591 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:52 IST
=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.478 DataTime=0.283 Loss=1.342 Prec@1=66.969 Prec@5=87.444 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:52 IST
=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.478 DataTime=0.283 Loss=1.342 Prec@1=66.969 Prec@5=87.444 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=15:52 IST
=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.478 DataTime=0.283 Loss=1.342 Prec@1=66.969 Prec@5=87.444 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=15:52 IST
=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.477 DataTime=0.282 Loss=1.345 Prec@1=66.955 Prec@5=87.410 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=15:52 IST
=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.477 DataTime=0.282 Loss=1.345 Prec@1=66.955 Prec@5=87.410 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=15:52 IST
=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.477 DataTime=0.282 Loss=1.345 Prec@1=66.955 Prec@5=87.410 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=15:53 IST
=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.474 DataTime=0.279 Loss=1.346 Prec@1=67.000 Prec@5=87.374 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=15:53 IST
=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.474 DataTime=0.279 Loss=1.346 Prec@1=67.000 Prec@5=87.374 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=15:53 IST
=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.474 DataTime=0.279 Loss=1.346 Prec@1=67.000 Prec@5=87.374 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=15:54 IST
=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.277 Loss=1.349 Prec@1=66.978 Prec@5=87.350 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=15:54 IST
=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.277 Loss=1.349 Prec@1=66.978 Prec@5=87.350 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=15:54 IST
=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.277 Loss=1.349 Prec@1=66.978 Prec@5=87.350 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=15:55 IST
=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.352 Prec@1=66.911 Prec@5=87.283 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=15:55 IST
=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.352 Prec@1=66.911 Prec@5=87.283 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=15:55 IST
=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.352 Prec@1=66.911 Prec@5=87.283 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=15:55 IST
=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.806 Prec@5=87.228 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=15:55 IST
=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.806 Prec@5=87.228 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:55 IST
=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.806 Prec@5=87.228 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:56 IST
=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.276 Loss=1.358 Prec@1=66.775 Prec@5=87.227 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:56 IST
=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.276 Loss=1.358 Prec@1=66.775 Prec@5=87.227 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=15:56 IST
=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.472 DataTime=0.276 Loss=1.358 Prec@1=66.775 Prec@5=87.227 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=15:57 IST
=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.754 Prec@5=87.194 rate=2.15 Hz, eta=0:13:13, total=0:06:13, wall=15:57 IST
=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.754 Prec@5=87.194 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=15:57 IST
=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.754 Prec@5=87.194 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=15:58 IST
=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.362 Prec@1=66.726 Prec@5=87.165 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=15:58 IST
=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.362 Prec@1=66.726 Prec@5=87.165 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=15:58 IST
=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.362 Prec@1=66.726 Prec@5=87.165 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=15:59 IST
=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.702 Prec@5=87.128 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=15:59 IST
=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.702 Prec@5=87.128 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=15:59 IST
=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.702 Prec@5=87.128 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=15:59 IST
=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.686 Prec@5=87.131 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=15:59 IST
=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.686 Prec@5=87.131 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=15:59 IST
=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.364 Prec@1=66.686 Prec@5=87.131 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=16:00 IST
=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.365 Prec@1=66.661 Prec@5=87.115 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=16:00 IST
=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.365 Prec@1=66.661 Prec@5=87.115 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=16:00 IST
=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.365 Prec@1=66.661 Prec@5=87.115 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=16:01 IST
=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.274 Loss=1.366 Prec@1=66.636 Prec@5=87.096 rate=2.14 Hz, eta=0:09:20, total=0:10:06, wall=16:01 IST
=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.274 Loss=1.366 Prec@1=66.636 Prec@5=87.096 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=16:01 IST
=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.274 Loss=1.366 Prec@1=66.636 Prec@5=87.096 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=16:02 IST
=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.367 Prec@1=66.619 Prec@5=87.086 rate=2.14 Hz, eta=0:08:34, total=0:10:53, wall=16:02 IST
=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.367 Prec@1=66.619 Prec@5=87.086 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=16:02 IST
=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.367 Prec@1=66.619 Prec@5=87.086 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=16:02 IST
=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.368 Prec@1=66.597 Prec@5=87.066 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=16:02 IST
=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.368 Prec@1=66.597 Prec@5=87.066 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=16:02 IST
=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.273 Loss=1.368 Prec@1=66.597 Prec@5=87.066 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=16:03 IST
=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.370 Prec@1=66.566 Prec@5=87.046 rate=2.15 Hz, eta=0:07:00, total=0:12:25, wall=16:03 IST
=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.370 Prec@1=66.566 Prec@5=87.046 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=16:03 IST
=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.370 Prec@1=66.566 Prec@5=87.046 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=16:04 IST
=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.371 Prec@1=66.541 Prec@5=87.029 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=16:04 IST
=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.371 Prec@1=66.541 Prec@5=87.029 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=16:04 IST
=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.371 Prec@1=66.541 Prec@5=87.029 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=16:05 IST
=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.524 Prec@5=87.018 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=16:05 IST
=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.524 Prec@5=87.018 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=16:05 IST
=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.524 Prec@5=87.018 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=16:06 IST
=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.517 Prec@5=87.009 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=16:06 IST
=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.517 Prec@5=87.009 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=16:06 IST
=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.372 Prec@1=66.517 Prec@5=87.009 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=16:06 IST
=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.372 Prec@1=66.519 Prec@5=87.016 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=16:06 IST
=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.372 Prec@1=66.519 Prec@5=87.016 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=16:06 IST
=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.372 Prec@1=66.519 Prec@5=87.016 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=16:07 IST
=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.486 Prec@5=86.998 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=16:07 IST
=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.486 Prec@5=86.998 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=16:07 IST
=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.470 DataTime=0.275 Loss=1.373 Prec@1=66.486 Prec@5=86.998 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=16:08 IST
=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.374 Prec@1=66.466 Prec@5=86.981 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=16:08 IST
=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.374 Prec@1=66.466 Prec@5=86.981 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:08 IST
=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.374 Prec@1=66.466 Prec@5=86.981 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:09 IST
=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.375 Prec@1=66.459 Prec@5=86.963 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:09 IST
=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.375 Prec@1=66.459 Prec@5=86.963 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:09 IST
=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.375 Prec@1=66.459 Prec@5=86.963 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:09 IST
=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.376 Prec@1=66.437 Prec@5=86.949 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:09 IST
=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.376 Prec@1=66.437 Prec@5=86.949 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=16:09 IST
=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.376 Prec@1=66.437 Prec@5=86.949 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=16:09 IST
=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.376 Prec@1=66.437 Prec@5=86.949 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=16:09 IST
=> training   100.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.469 DataTime=0.274 Loss=1.376 Prec@1=66.437 Prec@5=86.949 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=16:09 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 0.00% of 1x98...Epoch=72/150 LR=0.05418 Time=6.810 Loss=0.906 Prec@1=77.734 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.810 Loss=0.906 Prec@1=77.734 Prec@5=93.555 rate=6463.83 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST
** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.810 Loss=0.906 Prec@1=77.734 Prec@5=93.555 rate=6463.83 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST
** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=0.552 Loss=1.489 Prec@1=63.900 Prec@5=85.968 rate=6463.83 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST
** validation 100.00% of 1x98...Epoch=72/150 LR=0.05418 Time=0.552 Loss=1.489 Prec@1=63.900 Prec@5=85.968 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=16:10 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> training   0.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=4.867 DataTime=4.603 Loss=1.306 Prec@1=68.359 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=4.867 DataTime=4.603 Loss=1.306 Prec@1=68.359 Prec@5=88.672 rate=7911.65 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST
=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=4.867 DataTime=4.603 Loss=1.306 Prec@1=68.359 Prec@5=88.672 rate=7911.65 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST
=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.497 DataTime=0.301 Loss=1.325 Prec@1=67.799 Prec@5=87.713 rate=7911.65 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST
=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.497 DataTime=0.301 Loss=1.325 Prec@1=67.799 Prec@5=87.713 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=16:11 IST
=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.497 DataTime=0.301 Loss=1.325 Prec@1=67.799 Prec@5=87.713 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=16:12 IST
=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.479 DataTime=0.283 Loss=1.323 Prec@1=67.660 Prec@5=87.649 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=16:12 IST
=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.479 DataTime=0.283 Loss=1.323 Prec@1=67.660 Prec@5=87.649 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:12 IST
=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.479 DataTime=0.283 Loss=1.323 Prec@1=67.660 Prec@5=87.649 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:13 IST
=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.281 Loss=1.330 Prec@1=67.542 Prec@5=87.542 rate=2.20 Hz, eta=0:17:27, total=0:01:31, wall=16:13 IST
=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.281 Loss=1.330 Prec@1=67.542 Prec@5=87.542 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:13 IST
=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.281 Loss=1.330 Prec@1=67.542 Prec@5=87.542 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:14 IST
=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.335 Prec@1=67.335 Prec@5=87.468 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:14 IST
=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.335 Prec@1=67.335 Prec@5=87.468 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=16:14 IST
=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.335 Prec@1=67.335 Prec@5=87.468 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=16:14 IST
=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.336 Prec@1=67.320 Prec@5=87.434 rate=2.16 Hz, eta=0:16:13, total=0:03:05, wall=16:14 IST
=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.336 Prec@1=67.320 Prec@5=87.434 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=16:14 IST
=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.475 DataTime=0.280 Loss=1.336 Prec@1=67.320 Prec@5=87.434 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=16:15 IST
=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.282 Loss=1.338 Prec@1=67.262 Prec@5=87.435 rate=2.15 Hz, eta=0:15:30, total=0:03:52, wall=16:15 IST
=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.282 Loss=1.338 Prec@1=67.262 Prec@5=87.435 rate=2.14 Hz, eta=0:14:49, total=0:04:41, wall=16:15 IST
=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.476 DataTime=0.282 Loss=1.338 Prec@1=67.262 Prec@5=87.435 rate=2.14 Hz, eta=0:14:49, total=0:04:41, wall=16:16 IST
=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.280 Loss=1.343 Prec@1=67.144 Prec@5=87.386 rate=2.14 Hz, eta=0:14:49, total=0:04:41, wall=16:16 IST
=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.280 Loss=1.343 Prec@1=67.144 Prec@5=87.386 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:16 IST
=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.280 Loss=1.343 Prec@1=67.144 Prec@5=87.386 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:17 IST
=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.279 Loss=1.347 Prec@1=67.065 Prec@5=87.344 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:17 IST
=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.279 Loss=1.347 Prec@1=67.065 Prec@5=87.344 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=16:17 IST
=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.474 DataTime=0.279 Loss=1.347 Prec@1=67.065 Prec@5=87.344 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=16:18 IST
=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.279 Loss=1.350 Prec@1=67.012 Prec@5=87.307 rate=2.14 Hz, eta=0:13:15, total=0:06:14, wall=16:18 IST
=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.279 Loss=1.350 Prec@1=67.012 Prec@5=87.307 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:18 IST
=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.279 Loss=1.350 Prec@1=67.012 Prec@5=87.307 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:18 IST
=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.278 Loss=1.353 Prec@1=66.936 Prec@5=87.280 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:18 IST
=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.278 Loss=1.353 Prec@1=66.936 Prec@5=87.280 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:18 IST
=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.473 DataTime=0.278 Loss=1.353 Prec@1=66.936 Prec@5=87.280 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:19 IST
=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.355 Prec@1=66.865 Prec@5=87.248 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:19 IST
=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.355 Prec@1=66.865 Prec@5=87.248 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:19 IST
=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.355 Prec@1=66.865 Prec@5=87.248 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:20 IST
=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.356 Prec@1=66.831 Prec@5=87.230 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:20 IST
=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.356 Prec@1=66.831 Prec@5=87.230 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=16:20 IST
=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.356 Prec@1=66.831 Prec@5=87.230 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=16:21 IST
=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.357 Prec@1=66.822 Prec@5=87.228 rate=2.14 Hz, eta=0:10:09, total=0:09:22, wall=16:21 IST
=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.357 Prec@1=66.822 Prec@5=87.228 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=16:21 IST
=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.278 Loss=1.357 Prec@1=66.822 Prec@5=87.228 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=16:21 IST
=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.357 Prec@1=66.809 Prec@5=87.222 rate=2.14 Hz, eta=0:09:22, total=0:10:08, wall=16:21 IST
=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.357 Prec@1=66.809 Prec@5=87.222 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=16:21 IST
=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.357 Prec@1=66.809 Prec@5=87.222 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=16:22 IST
=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.358 Prec@1=66.784 Prec@5=87.207 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=16:22 IST
=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.358 Prec@1=66.784 Prec@5=87.207 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=16:22 IST
=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.358 Prec@1=66.784 Prec@5=87.207 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=16:23 IST
=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.360 Prec@1=66.746 Prec@5=87.188 rate=2.14 Hz, eta=0:07:49, total=0:11:42, wall=16:23 IST
=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.360 Prec@1=66.746 Prec@5=87.188 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=16:23 IST
=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.360 Prec@1=66.746 Prec@5=87.188 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=16:24 IST
=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.276 Loss=1.360 Prec@1=66.730 Prec@5=87.182 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=16:24 IST
=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.276 Loss=1.360 Prec@1=66.730 Prec@5=87.182 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=16:24 IST
=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.276 Loss=1.360 Prec@1=66.730 Prec@5=87.182 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=16:25 IST
=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.361 Prec@1=66.718 Prec@5=87.172 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.361 Prec@1=66.718 Prec@5=87.172 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.361 Prec@1=66.718 Prec@5=87.172 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.277 Loss=1.362 Prec@1=66.698 Prec@5=87.161 rate=2.13 Hz, eta=0:05:28, total=0:14:03, wall=16:25 IST
=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.277 Loss=1.362 Prec@1=66.698 Prec@5=87.161 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=16:25 IST
=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.472 DataTime=0.277 Loss=1.362 Prec@1=66.698 Prec@5=87.161 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=16:26 IST
=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.363 Prec@1=66.698 Prec@5=87.148 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=16:26 IST
=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.363 Prec@1=66.698 Prec@5=87.148 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=16:26 IST
=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.363 Prec@1=66.698 Prec@5=87.148 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=16:27 IST
=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.365 Prec@1=66.662 Prec@5=87.125 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=16:27 IST
=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.365 Prec@1=66.662 Prec@5=87.125 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=16:27 IST
=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.365 Prec@1=66.662 Prec@5=87.125 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=16:28 IST
=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.366 Prec@1=66.649 Prec@5=87.107 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=16:28 IST
=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.366 Prec@1=66.649 Prec@5=87.107 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=16:28 IST
=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.366 Prec@1=66.649 Prec@5=87.107 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=16:28 IST
=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.367 Prec@1=66.629 Prec@5=87.095 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=16:28 IST
=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.367 Prec@1=66.629 Prec@5=87.095 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=16:28 IST
=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.367 Prec@1=66.629 Prec@5=87.095 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=16:29 IST
=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.368 Prec@1=66.602 Prec@5=87.083 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=16:29 IST
=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.368 Prec@1=66.602 Prec@5=87.083 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=16:29 IST
=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.368 Prec@1=66.602 Prec@5=87.083 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=16:30 IST
=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.369 Prec@1=66.583 Prec@5=87.068 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.369 Prec@1=66.583 Prec@5=87.068 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.369 Prec@1=66.583 Prec@5=87.068 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.369 Prec@1=66.581 Prec@5=87.067 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=16:30 IST
=> training   100.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.471 DataTime=0.277 Loss=1.369 Prec@1=66.581 Prec@5=87.067 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=16:30 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 0.00% of 1x98...Epoch=73/150 LR=0.05314 Time=6.924 Loss=0.918 Prec@1=75.391 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=6.924 Loss=0.918 Prec@1=75.391 Prec@5=92.969 rate=8305.23 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST
** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=6.924 Loss=0.918 Prec@1=75.391 Prec@5=92.969 rate=8305.23 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=0.544 Loss=1.458 Prec@1=64.702 Prec@5=86.250 rate=8305.23 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
** validation 100.00% of 1x98...Epoch=73/150 LR=0.05314 Time=0.544 Loss=1.458 Prec@1=64.702 Prec@5=86.250 rate=2.11 Hz, eta=0:00:00, total=0:00:46, wall=16:31 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=5.029 DataTime=4.839 Loss=1.236 Prec@1=68.750 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=5.029 DataTime=4.839 Loss=1.236 Prec@1=68.750 Prec@5=87.695 rate=5861.56 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=5.029 DataTime=4.839 Loss=1.236 Prec@1=68.750 Prec@5=87.695 rate=5861.56 Hz, eta=0:00:00, total=0:00:00, wall=16:32 IST
=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.509 DataTime=0.314 Loss=1.327 Prec@1=67.713 Prec@5=87.533 rate=5861.56 Hz, eta=0:00:00, total=0:00:00, wall=16:32 IST
=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.509 DataTime=0.314 Loss=1.327 Prec@1=67.713 Prec@5=87.533 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=16:32 IST
=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.509 DataTime=0.314 Loss=1.327 Prec@1=67.713 Prec@5=87.533 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=16:33 IST
=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.486 DataTime=0.291 Loss=1.323 Prec@1=67.675 Prec@5=87.647 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.486 DataTime=0.291 Loss=1.323 Prec@1=67.675 Prec@5=87.647 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.486 DataTime=0.291 Loss=1.323 Prec@1=67.675 Prec@5=87.647 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.477 DataTime=0.282 Loss=1.325 Prec@1=67.635 Prec@5=87.595 rate=2.17 Hz, eta=0:17:41, total=0:01:32, wall=16:33 IST
=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.477 DataTime=0.282 Loss=1.325 Prec@1=67.635 Prec@5=87.595 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:33 IST
=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.477 DataTime=0.282 Loss=1.325 Prec@1=67.635 Prec@5=87.595 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:34 IST
=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.476 DataTime=0.281 Loss=1.329 Prec@1=67.558 Prec@5=87.549 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:34 IST
=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.476 DataTime=0.281 Loss=1.329 Prec@1=67.558 Prec@5=87.549 rate=2.16 Hz, eta=0:16:14, total=0:03:06, wall=16:34 IST
=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.476 DataTime=0.281 Loss=1.329 Prec@1=67.558 Prec@5=87.549 rate=2.16 Hz, eta=0:16:14, total=0:03:06, wall=16:35 IST
=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.475 DataTime=0.281 Loss=1.331 Prec@1=67.470 Prec@5=87.551 rate=2.16 Hz, eta=0:16:14, total=0:03:06, wall=16:35 IST
=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.475 DataTime=0.281 Loss=1.331 Prec@1=67.470 Prec@5=87.551 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:35 IST
=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.475 DataTime=0.281 Loss=1.331 Prec@1=67.470 Prec@5=87.551 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:36 IST
=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.335 Prec@1=67.391 Prec@5=87.497 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:36 IST
=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.335 Prec@1=67.391 Prec@5=87.497 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=16:36 IST
=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.335 Prec@1=67.391 Prec@5=87.497 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=16:37 IST
=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.338 Prec@1=67.325 Prec@5=87.448 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.338 Prec@1=67.325 Prec@5=87.448 rate=2.15 Hz, eta=0:13:59, total=0:05:26, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.338 Prec@1=67.325 Prec@5=87.448 rate=2.15 Hz, eta=0:13:59, total=0:05:26, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.341 Prec@1=67.252 Prec@5=87.424 rate=2.15 Hz, eta=0:13:59, total=0:05:26, wall=16:37 IST
=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.341 Prec@1=67.252 Prec@5=87.424 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=16:37 IST
=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.341 Prec@1=67.252 Prec@5=87.424 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=16:38 IST
=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.342 Prec@1=67.198 Prec@5=87.409 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=16:38 IST
=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.342 Prec@1=67.198 Prec@5=87.409 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:38 IST
=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.279 Loss=1.342 Prec@1=67.198 Prec@5=87.409 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:39 IST
=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.344 Prec@1=67.153 Prec@5=87.389 rate=2.14 Hz, eta=0:12:29, total=0:07:01, wall=16:39 IST
=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.344 Prec@1=67.153 Prec@5=87.389 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:39 IST
=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.473 DataTime=0.278 Loss=1.344 Prec@1=67.153 Prec@5=87.389 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:40 IST
=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.346 Prec@1=67.113 Prec@5=87.361 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.346 Prec@1=67.113 Prec@5=87.361 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.278 Loss=1.346 Prec@1=67.113 Prec@5=87.361 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.277 Loss=1.348 Prec@1=67.077 Prec@5=87.329 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=16:40 IST
=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.277 Loss=1.348 Prec@1=67.077 Prec@5=87.329 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=16:40 IST
=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.472 DataTime=0.277 Loss=1.348 Prec@1=67.077 Prec@5=87.329 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=16:41 IST
=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.277 Loss=1.349 Prec@1=67.039 Prec@5=87.309 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=16:41 IST
=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.277 Loss=1.349 Prec@1=67.039 Prec@5=87.309 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=16:41 IST
=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.277 Loss=1.349 Prec@1=67.039 Prec@5=87.309 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=16:42 IST
=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.351 Prec@1=67.004 Prec@5=87.286 rate=2.14 Hz, eta=0:09:21, total=0:10:08, wall=16:42 IST
=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.351 Prec@1=67.004 Prec@5=87.286 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=16:42 IST
=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.351 Prec@1=67.004 Prec@5=87.286 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=16:43 IST
=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.353 Prec@1=66.959 Prec@5=87.258 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=16:43 IST
=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.353 Prec@1=66.959 Prec@5=87.258 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=16:43 IST
=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.353 Prec@1=66.959 Prec@5=87.258 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=16:44 IST
=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.276 Loss=1.354 Prec@1=66.940 Prec@5=87.244 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.276 Loss=1.354 Prec@1=66.940 Prec@5=87.244 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.276 Loss=1.354 Prec@1=66.940 Prec@5=87.244 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.355 Prec@1=66.927 Prec@5=87.225 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=16:44 IST
=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.355 Prec@1=66.927 Prec@5=87.225 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=16:44 IST
=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.355 Prec@1=66.927 Prec@5=87.225 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=16:45 IST
=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.356 Prec@1=66.894 Prec@5=87.211 rate=2.14 Hz, eta=0:06:14, total=0:13:14, wall=16:45 IST
=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.356 Prec@1=66.894 Prec@5=87.211 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=16:45 IST
=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.356 Prec@1=66.894 Prec@5=87.211 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=16:46 IST
=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.883 Prec@5=87.193 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=16:46 IST
=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.883 Prec@5=87.193 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=16:46 IST
=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.883 Prec@5=87.193 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=16:47 IST
=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.865 Prec@5=87.186 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.865 Prec@5=87.186 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.471 DataTime=0.276 Loss=1.357 Prec@1=66.865 Prec@5=87.186 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.359 Prec@1=66.831 Prec@5=87.165 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=16:47 IST
=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.359 Prec@1=66.831 Prec@5=87.165 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=16:47 IST
=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.359 Prec@1=66.831 Prec@5=87.165 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=16:48 IST
=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.808 Prec@5=87.148 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=16:48 IST
=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.808 Prec@5=87.148 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=16:48 IST
=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.808 Prec@5=87.148 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=16:49 IST
=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.805 Prec@5=87.142 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=16:49 IST
=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.805 Prec@5=87.142 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:49 IST
=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.470 DataTime=0.275 Loss=1.360 Prec@1=66.805 Prec@5=87.142 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:50 IST
=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.362 Prec@1=66.767 Prec@5=87.134 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=16:50 IST
=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.362 Prec@1=66.767 Prec@5=87.134 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:50 IST
=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.362 Prec@1=66.767 Prec@5=87.134 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:51 IST
=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.363 Prec@1=66.749 Prec@5=87.119 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.363 Prec@1=66.749 Prec@5=87.119 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.363 Prec@1=66.749 Prec@5=87.119 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.363 Prec@1=66.749 Prec@5=87.119 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=16:51 IST
=> training   100.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.469 DataTime=0.275 Loss=1.363 Prec@1=66.749 Prec@5=87.119 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=16:51 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 0.00% of 1x98...Epoch=74/150 LR=0.05209 Time=7.011 Loss=0.933 Prec@1=73.438 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=7.011 Loss=0.933 Prec@1=73.438 Prec@5=92.969 rate=6443.88 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=7.011 Loss=0.933 Prec@1=73.438 Prec@5=92.969 rate=6443.88 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=0.550 Loss=1.421 Prec@1=65.274 Prec@5=86.972 rate=6443.88 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 100.00% of 1x98...Epoch=74/150 LR=0.05209 Time=0.550 Loss=1.421 Prec@1=65.274 Prec@5=86.972 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=16:51 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.670 DataTime=4.439 Loss=1.183 Prec@1=72.070 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.670 DataTime=4.439 Loss=1.183 Prec@1=72.070 Prec@5=89.648 rate=9446.26 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.670 DataTime=4.439 Loss=1.183 Prec@1=72.070 Prec@5=89.648 rate=9446.26 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.498 DataTime=0.301 Loss=1.321 Prec@1=67.491 Prec@5=87.856 rate=9446.26 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.498 DataTime=0.301 Loss=1.321 Prec@1=67.491 Prec@5=87.856 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=16:52 IST
=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.498 DataTime=0.301 Loss=1.321 Prec@1=67.491 Prec@5=87.856 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=16:53 IST
=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.479 DataTime=0.283 Loss=1.321 Prec@1=67.617 Prec@5=87.792 rate=2.21 Hz, eta=0:18:05, total=0:00:45, wall=16:53 IST
=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.479 DataTime=0.283 Loss=1.321 Prec@1=67.617 Prec@5=87.792 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:53 IST
=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.479 DataTime=0.283 Loss=1.321 Prec@1=67.617 Prec@5=87.792 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:54 IST
=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.476 DataTime=0.280 Loss=1.326 Prec@1=67.490 Prec@5=87.680 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=16:54 IST
=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.476 DataTime=0.280 Loss=1.326 Prec@1=67.490 Prec@5=87.680 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:54 IST
=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.476 DataTime=0.280 Loss=1.326 Prec@1=67.490 Prec@5=87.680 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:55 IST
=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.471 DataTime=0.276 Loss=1.328 Prec@1=67.387 Prec@5=87.625 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.471 DataTime=0.276 Loss=1.328 Prec@1=67.387 Prec@5=87.625 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.471 DataTime=0.276 Loss=1.328 Prec@1=67.387 Prec@5=87.625 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.469 DataTime=0.274 Loss=1.333 Prec@1=67.317 Prec@5=87.550 rate=2.18 Hz, eta=0:16:05, total=0:03:04, wall=16:55 IST
=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.469 DataTime=0.274 Loss=1.333 Prec@1=67.317 Prec@5=87.550 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=16:55 IST
=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.469 DataTime=0.274 Loss=1.333 Prec@1=67.317 Prec@5=87.550 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=16:56 IST
=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.468 DataTime=0.273 Loss=1.335 Prec@1=67.281 Prec@5=87.556 rate=2.18 Hz, eta=0:15:20, total=0:03:50, wall=16:56 IST
=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.468 DataTime=0.273 Loss=1.335 Prec@1=67.281 Prec@5=87.556 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:56 IST
=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.468 DataTime=0.273 Loss=1.335 Prec@1=67.281 Prec@5=87.556 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:57 IST
=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.256 Prec@5=87.570 rate=2.17 Hz, eta=0:14:34, total=0:04:36, wall=16:57 IST
=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.256 Prec@5=87.570 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=16:57 IST
=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.256 Prec@5=87.570 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=16:58 IST
=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.337 Prec@1=67.231 Prec@5=87.523 rate=2.17 Hz, eta=0:13:50, total=0:05:22, wall=16:58 IST
=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.337 Prec@1=67.231 Prec@5=87.523 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=16:58 IST
=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.337 Prec@1=67.231 Prec@5=87.523 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=16:59 IST
=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.465 DataTime=0.270 Loss=1.340 Prec@1=67.178 Prec@5=87.505 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.465 DataTime=0.270 Loss=1.340 Prec@1=67.178 Prec@5=87.505 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.465 DataTime=0.270 Loss=1.340 Prec@1=67.178 Prec@5=87.505 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.342 Prec@1=67.111 Prec@5=87.478 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=16:59 IST
=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.342 Prec@1=67.111 Prec@5=87.478 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=16:59 IST
=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.342 Prec@1=67.111 Prec@5=87.478 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=17:00 IST
=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.343 Prec@1=67.097 Prec@5=87.451 rate=2.17 Hz, eta=0:11:32, total=0:07:41, wall=17:00 IST
=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.343 Prec@1=67.097 Prec@5=87.451 rate=2.17 Hz, eta=0:10:47, total=0:08:28, wall=17:00 IST
=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.343 Prec@1=67.097 Prec@5=87.451 rate=2.17 Hz, eta=0:10:47, total=0:08:28, wall=17:01 IST
=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.345 Prec@1=67.067 Prec@5=87.416 rate=2.17 Hz, eta=0:10:47, total=0:08:28, wall=17:01 IST
=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.345 Prec@1=67.067 Prec@5=87.416 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=17:01 IST
=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.345 Prec@1=67.067 Prec@5=87.416 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=17:02 IST
=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.347 Prec@1=67.032 Prec@5=87.396 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=17:02 IST
=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.347 Prec@1=67.032 Prec@5=87.396 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=17:02 IST
=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.347 Prec@1=67.032 Prec@5=87.396 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=17:02 IST
=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.347 Prec@1=67.014 Prec@5=87.389 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=17:02 IST
=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.347 Prec@1=67.014 Prec@5=87.389 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=17:02 IST
=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.270 Loss=1.347 Prec@1=67.014 Prec@5=87.389 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=17:03 IST
=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.349 Prec@1=66.989 Prec@5=87.379 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=17:03 IST
=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.349 Prec@1=66.989 Prec@5=87.379 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=17:03 IST
=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.349 Prec@1=66.989 Prec@5=87.379 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=17:04 IST
=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.271 Loss=1.350 Prec@1=66.975 Prec@5=87.350 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=17:04 IST
=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.271 Loss=1.350 Prec@1=66.975 Prec@5=87.350 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:04 IST
=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.271 Loss=1.350 Prec@1=66.975 Prec@5=87.350 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:05 IST
=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.350 Prec@1=66.976 Prec@5=87.342 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:05 IST
=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.350 Prec@1=66.976 Prec@5=87.342 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=17:05 IST
=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.350 Prec@1=66.976 Prec@5=87.342 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=17:06 IST
=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.351 Prec@1=66.957 Prec@5=87.339 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.351 Prec@1=66.957 Prec@5=87.339 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.351 Prec@1=66.957 Prec@5=87.339 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.352 Prec@1=66.942 Prec@5=87.315 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=17:06 IST
=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.352 Prec@1=66.942 Prec@5=87.315 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=17:06 IST
=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.352 Prec@1=66.942 Prec@5=87.315 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=17:07 IST
=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.920 Prec@5=87.300 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=17:07 IST
=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.920 Prec@5=87.300 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=17:07 IST
=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.920 Prec@5=87.300 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=17:08 IST
=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.927 Prec@5=87.297 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=17:08 IST
=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.927 Prec@5=87.297 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=17:08 IST
=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.353 Prec@1=66.927 Prec@5=87.297 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=17:09 IST
=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.354 Prec@1=66.912 Prec@5=87.285 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=17:09 IST
=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.354 Prec@1=66.912 Prec@5=87.285 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=17:09 IST
=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.354 Prec@1=66.912 Prec@5=87.285 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=17:09 IST
=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.355 Prec@1=66.896 Prec@5=87.269 rate=2.15 Hz, eta=0:02:20, total=0:17:01, wall=17:09 IST
=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.355 Prec@1=66.896 Prec@5=87.269 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=17:09 IST
=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.271 Loss=1.355 Prec@1=66.896 Prec@5=87.269 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=17:10 IST
=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.355 Prec@1=66.885 Prec@5=87.260 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=17:10 IST
=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.355 Prec@1=66.885 Prec@5=87.260 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:10 IST
=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.355 Prec@1=66.885 Prec@5=87.260 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:11 IST
=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.356 Prec@1=66.863 Prec@5=87.255 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.356 Prec@1=66.863 Prec@5=87.255 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.467 DataTime=0.272 Loss=1.356 Prec@1=66.863 Prec@5=87.255 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.272 Loss=1.356 Prec@1=66.863 Prec@5=87.255 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:11 IST
=> training   100.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.466 DataTime=0.272 Loss=1.356 Prec@1=66.863 Prec@5=87.255 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:11 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 0.00% of 1x98...Epoch=75/150 LR=0.05105 Time=6.099 Loss=0.842 Prec@1=78.711 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.099 Loss=0.842 Prec@1=78.711 Prec@5=94.336 rate=1330.49 Hz, eta=0:00:00, total=0:00:00, wall=17:11 IST
** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.099 Loss=0.842 Prec@1=78.711 Prec@5=94.336 rate=1330.49 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=0.539 Loss=1.415 Prec@1=65.494 Prec@5=86.936 rate=1330.49 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
** validation 100.00% of 1x98...Epoch=75/150 LR=0.05105 Time=0.539 Loss=1.415 Prec@1=65.494 Prec@5=86.936 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=17:12 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.116 DataTime=4.910 Loss=1.432 Prec@1=66.406 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.116 DataTime=4.910 Loss=1.432 Prec@1=66.406 Prec@5=85.938 rate=6262.64 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.116 DataTime=4.910 Loss=1.432 Prec@1=66.406 Prec@5=85.938 rate=6262.64 Hz, eta=0:00:00, total=0:00:00, wall=17:13 IST
=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.503 DataTime=0.309 Loss=1.316 Prec@1=67.605 Prec@5=87.806 rate=6262.64 Hz, eta=0:00:00, total=0:00:00, wall=17:13 IST
=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.503 DataTime=0.309 Loss=1.316 Prec@1=67.605 Prec@5=87.806 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=17:13 IST
=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.503 DataTime=0.309 Loss=1.316 Prec@1=67.605 Prec@5=87.806 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=17:14 IST
=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.485 DataTime=0.290 Loss=1.317 Prec@1=67.659 Prec@5=87.764 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.485 DataTime=0.290 Loss=1.317 Prec@1=67.659 Prec@5=87.764 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.485 DataTime=0.290 Loss=1.317 Prec@1=67.659 Prec@5=87.764 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.477 DataTime=0.282 Loss=1.316 Prec@1=67.700 Prec@5=87.721 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=17:14 IST
=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.477 DataTime=0.282 Loss=1.316 Prec@1=67.700 Prec@5=87.721 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=17:14 IST
=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.477 DataTime=0.282 Loss=1.316 Prec@1=67.700 Prec@5=87.721 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=17:15 IST
=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.475 DataTime=0.279 Loss=1.323 Prec@1=67.597 Prec@5=87.653 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=17:15 IST
=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.475 DataTime=0.279 Loss=1.323 Prec@1=67.597 Prec@5=87.653 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=17:15 IST
=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.475 DataTime=0.279 Loss=1.323 Prec@1=67.597 Prec@5=87.653 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=17:16 IST
=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.473 DataTime=0.277 Loss=1.325 Prec@1=67.554 Prec@5=87.629 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=17:16 IST
=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.473 DataTime=0.277 Loss=1.325 Prec@1=67.554 Prec@5=87.629 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=17:16 IST
=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.473 DataTime=0.277 Loss=1.325 Prec@1=67.554 Prec@5=87.629 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=17:17 IST
=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.472 DataTime=0.276 Loss=1.328 Prec@1=67.493 Prec@5=87.574 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.472 DataTime=0.276 Loss=1.328 Prec@1=67.493 Prec@5=87.574 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.472 DataTime=0.276 Loss=1.328 Prec@1=67.493 Prec@5=87.574 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.471 DataTime=0.276 Loss=1.329 Prec@1=67.462 Prec@5=87.558 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=17:17 IST
=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.471 DataTime=0.276 Loss=1.329 Prec@1=67.462 Prec@5=87.558 rate=2.16 Hz, eta=0:13:56, total=0:05:25, wall=17:17 IST
=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.471 DataTime=0.276 Loss=1.329 Prec@1=67.462 Prec@5=87.558 rate=2.16 Hz, eta=0:13:56, total=0:05:25, wall=17:18 IST
=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.470 DataTime=0.275 Loss=1.331 Prec@1=67.433 Prec@5=87.532 rate=2.16 Hz, eta=0:13:56, total=0:05:25, wall=17:18 IST
=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.470 DataTime=0.275 Loss=1.331 Prec@1=67.433 Prec@5=87.532 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=17:18 IST
=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.470 DataTime=0.275 Loss=1.331 Prec@1=67.433 Prec@5=87.532 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=17:19 IST
=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.273 Loss=1.333 Prec@1=67.383 Prec@5=87.515 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=17:19 IST
=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.273 Loss=1.333 Prec@1=67.383 Prec@5=87.515 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:19 IST
=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.273 Loss=1.333 Prec@1=67.383 Prec@5=87.515 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:20 IST
=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.334 Prec@1=67.372 Prec@5=87.503 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:20 IST
=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.334 Prec@1=67.372 Prec@5=87.503 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:20 IST
=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.334 Prec@1=67.372 Prec@5=87.503 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:20 IST
=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.345 Prec@5=87.490 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:20 IST
=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.345 Prec@5=87.490 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=17:20 IST
=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.467 DataTime=0.272 Loss=1.335 Prec@1=67.345 Prec@5=87.490 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=17:21 IST
=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.330 Prec@5=87.463 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=17:21 IST
=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.330 Prec@5=87.463 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=17:21 IST
=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.330 Prec@5=87.463 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=17:22 IST
=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.338 Prec@1=67.302 Prec@5=87.460 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=17:22 IST
=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.338 Prec@1=67.302 Prec@5=87.460 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:22 IST
=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.338 Prec@1=67.302 Prec@5=87.460 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:23 IST
=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.298 Prec@5=87.442 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:23 IST
=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.298 Prec@5=87.442 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:23 IST
=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.298 Prec@5=87.442 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:24 IST
=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.341 Prec@1=67.246 Prec@5=87.418 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:24 IST
=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.341 Prec@1=67.246 Prec@5=87.418 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=17:24 IST
=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.341 Prec@1=67.246 Prec@5=87.418 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=17:24 IST
=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.341 Prec@1=67.241 Prec@5=87.411 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=17:24 IST
=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.341 Prec@1=67.241 Prec@5=87.411 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=17:24 IST
=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.341 Prec@1=67.241 Prec@5=87.411 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=17:25 IST
=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.343 Prec@1=67.204 Prec@5=87.391 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=17:25 IST
=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.343 Prec@1=67.204 Prec@5=87.391 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:25 IST
=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.343 Prec@1=67.204 Prec@5=87.391 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:26 IST
=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.343 Prec@1=67.192 Prec@5=87.391 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:26 IST
=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.343 Prec@1=67.192 Prec@5=87.391 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=17:26 IST
=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.343 Prec@1=67.192 Prec@5=87.391 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=17:27 IST
=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.344 Prec@1=67.168 Prec@5=87.377 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=17:27 IST
=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.344 Prec@1=67.168 Prec@5=87.377 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:27 IST
=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.344 Prec@1=67.168 Prec@5=87.377 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:28 IST
=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.345 Prec@1=67.139 Prec@5=87.377 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.345 Prec@1=67.139 Prec@5=87.377 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.345 Prec@1=67.139 Prec@5=87.377 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.346 Prec@1=67.130 Prec@5=87.364 rate=2.15 Hz, eta=0:03:53, total=0:15:32, wall=17:28 IST
=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.346 Prec@1=67.130 Prec@5=87.364 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:28 IST
=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.346 Prec@1=67.130 Prec@5=87.364 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:29 IST
=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.347 Prec@1=67.095 Prec@5=87.348 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:29 IST
=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.347 Prec@1=67.095 Prec@5=87.348 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=17:29 IST
=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.273 Loss=1.347 Prec@1=67.095 Prec@5=87.348 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=17:30 IST
=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.348 Prec@1=67.069 Prec@5=87.337 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=17:30 IST
=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.348 Prec@1=67.069 Prec@5=87.337 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=17:30 IST
=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.348 Prec@1=67.069 Prec@5=87.337 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=17:31 IST
=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.349 Prec@1=67.038 Prec@5=87.328 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=17:31 IST
=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.349 Prec@1=67.038 Prec@5=87.328 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:31 IST
=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.469 DataTime=0.274 Loss=1.349 Prec@1=67.038 Prec@5=87.328 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:31 IST
=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.350 Prec@1=67.026 Prec@5=87.320 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=17:31 IST
=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.350 Prec@1=67.026 Prec@5=87.320 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=17:31 IST
=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.350 Prec@1=67.026 Prec@5=87.320 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=17:31 IST
=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.350 Prec@1=67.026 Prec@5=87.320 rate=2.14 Hz, eta=0:00:00, total=0:19:26, wall=17:31 IST
=> training   100.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.468 DataTime=0.274 Loss=1.350 Prec@1=67.026 Prec@5=87.320 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=17:31 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 0.00% of 1x98...Epoch=76/150 LR=0.05000 Time=6.531 Loss=0.941 Prec@1=75.000 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.531 Loss=0.941 Prec@1=75.000 Prec@5=92.969 rate=7080.50 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.531 Loss=0.941 Prec@1=75.000 Prec@5=92.969 rate=7080.50 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=0.550 Loss=1.409 Prec@1=65.576 Prec@5=87.112 rate=7080.50 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 100.00% of 1x98...Epoch=76/150 LR=0.05000 Time=0.550 Loss=1.409 Prec@1=65.576 Prec@5=87.112 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=17:32 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> training   0.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.506 DataTime=4.238 Loss=1.106 Prec@1=70.508 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.506 DataTime=4.238 Loss=1.106 Prec@1=70.508 Prec@5=90.625 rate=7502.66 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.506 DataTime=4.238 Loss=1.106 Prec@1=70.508 Prec@5=90.625 rate=7502.66 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST
=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.499 DataTime=0.303 Loss=1.309 Prec@1=68.042 Prec@5=87.809 rate=7502.66 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST
=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.499 DataTime=0.303 Loss=1.309 Prec@1=68.042 Prec@5=87.809 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=17:33 IST
=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.499 DataTime=0.303 Loss=1.309 Prec@1=68.042 Prec@5=87.809 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=17:34 IST
=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.479 DataTime=0.284 Loss=1.311 Prec@1=67.922 Prec@5=87.782 rate=2.20 Hz, eta=0:18:10, total=0:00:45, wall=17:34 IST
=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.479 DataTime=0.284 Loss=1.311 Prec@1=67.922 Prec@5=87.782 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=17:34 IST
=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.479 DataTime=0.284 Loss=1.311 Prec@1=67.922 Prec@5=87.782 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=17:35 IST
=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.473 DataTime=0.278 Loss=1.313 Prec@1=67.815 Prec@5=87.784 rate=2.19 Hz, eta=0:17:30, total=0:01:31, wall=17:35 IST
=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.473 DataTime=0.278 Loss=1.313 Prec@1=67.815 Prec@5=87.784 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=17:35 IST
=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.473 DataTime=0.278 Loss=1.313 Prec@1=67.815 Prec@5=87.784 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=17:36 IST
=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.472 DataTime=0.277 Loss=1.311 Prec@1=67.831 Prec@5=87.805 rate=2.18 Hz, eta=0:16:49, total=0:02:17, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.472 DataTime=0.277 Loss=1.311 Prec@1=67.831 Prec@5=87.805 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.472 DataTime=0.277 Loss=1.311 Prec@1=67.831 Prec@5=87.805 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.316 Prec@1=67.762 Prec@5=87.746 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=17:36 IST
=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.316 Prec@1=67.762 Prec@5=87.746 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=17:36 IST
=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.316 Prec@1=67.762 Prec@5=87.746 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=17:37 IST
=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.318 Prec@1=67.734 Prec@5=87.701 rate=2.17 Hz, eta=0:15:20, total=0:03:50, wall=17:37 IST
=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.318 Prec@1=67.734 Prec@5=87.701 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=17:37 IST
=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.469 DataTime=0.274 Loss=1.318 Prec@1=67.734 Prec@5=87.701 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=17:38 IST
=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.319 Prec@1=67.733 Prec@5=87.708 rate=2.17 Hz, eta=0:14:36, total=0:04:37, wall=17:38 IST
=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.319 Prec@1=67.733 Prec@5=87.708 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=17:38 IST
=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.319 Prec@1=67.733 Prec@5=87.708 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=17:39 IST
=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.321 Prec@1=67.688 Prec@5=87.664 rate=2.17 Hz, eta=0:13:51, total=0:05:23, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.321 Prec@1=67.688 Prec@5=87.664 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.321 Prec@1=67.688 Prec@5=87.664 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.323 Prec@1=67.656 Prec@5=87.641 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=17:39 IST
=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.323 Prec@1=67.656 Prec@5=87.641 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:39 IST
=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.323 Prec@1=67.656 Prec@5=87.641 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:40 IST
=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.324 Prec@1=67.612 Prec@5=87.634 rate=2.16 Hz, eta=0:12:21, total=0:06:57, wall=17:40 IST
=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.324 Prec@1=67.612 Prec@5=87.634 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:40 IST
=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.324 Prec@1=67.612 Prec@5=87.634 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:41 IST
=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.574 Prec@5=87.614 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=17:41 IST
=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.574 Prec@5=87.614 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=17:41 IST
=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.574 Prec@5=87.614 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=17:42 IST
=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.552 Prec@5=87.614 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=17:42 IST
=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.552 Prec@5=87.614 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=17:42 IST
=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.325 Prec@1=67.552 Prec@5=87.614 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=17:43 IST
=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.327 Prec@1=67.507 Prec@5=87.602 rate=2.16 Hz, eta=0:10:03, total=0:09:17, wall=17:43 IST
=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.327 Prec@1=67.507 Prec@5=87.602 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:43 IST
=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.327 Prec@1=67.507 Prec@5=87.602 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:43 IST
=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.328 Prec@1=67.482 Prec@5=87.584 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:43 IST
=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.328 Prec@1=67.482 Prec@5=87.584 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:43 IST
=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.328 Prec@1=67.482 Prec@5=87.584 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:44 IST
=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.331 Prec@1=67.424 Prec@5=87.557 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=17:44 IST
=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.331 Prec@1=67.424 Prec@5=87.557 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=17:44 IST
=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.331 Prec@1=67.424 Prec@5=87.557 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=17:45 IST
=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.397 Prec@5=87.546 rate=2.15 Hz, eta=0:07:44, total=0:11:36, wall=17:45 IST
=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.397 Prec@5=87.546 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=17:45 IST
=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.397 Prec@5=87.546 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=17:46 IST
=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.372 Prec@5=87.538 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.372 Prec@5=87.538 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.332 Prec@1=67.372 Prec@5=87.538 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.334 Prec@1=67.338 Prec@5=87.517 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=17:46 IST
=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.334 Prec@1=67.338 Prec@5=87.517 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=17:46 IST
=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.334 Prec@1=67.338 Prec@5=87.517 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=17:47 IST
=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.336 Prec@1=67.303 Prec@5=87.495 rate=2.15 Hz, eta=0:05:25, total=0:13:56, wall=17:47 IST
=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.336 Prec@1=67.303 Prec@5=87.495 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:47 IST
=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.336 Prec@1=67.303 Prec@5=87.495 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:48 IST
=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.281 Prec@5=87.482 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:48 IST
=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.281 Prec@5=87.482 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=17:48 IST
=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.281 Prec@5=87.482 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=17:49 IST
=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.273 Prec@5=87.475 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=17:49 IST
=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.273 Prec@5=87.475 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:49 IST
=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.337 Prec@1=67.273 Prec@5=87.475 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:50 IST
=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.239 Prec@5=87.458 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=17:50 IST
=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.239 Prec@5=87.458 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:50 IST
=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.468 DataTime=0.273 Loss=1.339 Prec@1=67.239 Prec@5=87.458 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:50 IST
=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.340 Prec@1=67.216 Prec@5=87.447 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:50 IST
=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.340 Prec@1=67.216 Prec@5=87.447 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=17:50 IST
=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.273 Loss=1.340 Prec@1=67.216 Prec@5=87.447 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=17:51 IST
=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.342 Prec@1=67.183 Prec@5=87.427 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=17:51 IST
=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.342 Prec@1=67.183 Prec@5=87.427 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=17:51 IST
=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.342 Prec@1=67.183 Prec@5=87.427 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=17:52 IST
=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.343 Prec@1=67.162 Prec@5=87.404 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.343 Prec@1=67.162 Prec@5=87.404 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.343 Prec@1=67.162 Prec@5=87.404 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.343 Prec@1=67.161 Prec@5=87.404 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:52 IST
=> training   100.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.467 DataTime=0.272 Loss=1.343 Prec@1=67.161 Prec@5=87.404 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=17:52 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 0.00% of 1x98...Epoch=77/150 LR=0.04895 Time=6.928 Loss=0.838 Prec@1=78.320 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.928 Loss=0.838 Prec@1=78.320 Prec@5=94.531 rate=8555.12 Hz, eta=0:00:00, total=0:00:00, wall=17:52 IST
** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.928 Loss=0.838 Prec@1=78.320 Prec@5=94.531 rate=8555.12 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=0.560 Loss=1.392 Prec@1=65.988 Prec@5=87.040 rate=8555.12 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
** validation 100.00% of 1x98...Epoch=77/150 LR=0.04895 Time=0.560 Loss=1.392 Prec@1=65.988 Prec@5=87.040 rate=2.04 Hz, eta=0:00:00, total=0:00:47, wall=17:53 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.635 DataTime=4.370 Loss=1.374 Prec@1=64.062 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.635 DataTime=4.370 Loss=1.374 Prec@1=64.062 Prec@5=86.523 rate=7142.35 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.635 DataTime=4.370 Loss=1.374 Prec@1=64.062 Prec@5=86.523 rate=7142.35 Hz, eta=0:00:00, total=0:00:00, wall=17:54 IST
=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.493 DataTime=0.299 Loss=1.304 Prec@1=67.874 Prec@5=88.022 rate=7142.35 Hz, eta=0:00:00, total=0:00:00, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.493 DataTime=0.299 Loss=1.304 Prec@1=67.874 Prec@5=88.022 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.493 DataTime=0.299 Loss=1.304 Prec@1=67.874 Prec@5=88.022 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.476 DataTime=0.280 Loss=1.306 Prec@1=67.895 Prec@5=87.975 rate=2.23 Hz, eta=0:17:54, total=0:00:45, wall=17:54 IST
=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.476 DataTime=0.280 Loss=1.306 Prec@1=67.895 Prec@5=87.975 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=17:54 IST
=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.476 DataTime=0.280 Loss=1.306 Prec@1=67.895 Prec@5=87.975 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=17:55 IST
=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.468 DataTime=0.273 Loss=1.305 Prec@1=67.826 Prec@5=87.955 rate=2.21 Hz, eta=0:17:22, total=0:01:31, wall=17:55 IST
=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.468 DataTime=0.273 Loss=1.305 Prec@1=67.826 Prec@5=87.955 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=17:55 IST
=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.468 DataTime=0.273 Loss=1.305 Prec@1=67.826 Prec@5=87.955 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=17:56 IST
=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.467 DataTime=0.272 Loss=1.308 Prec@1=67.822 Prec@5=87.876 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=17:56 IST
=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.467 DataTime=0.272 Loss=1.308 Prec@1=67.822 Prec@5=87.876 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=17:56 IST
=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.467 DataTime=0.272 Loss=1.308 Prec@1=67.822 Prec@5=87.876 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=17:57 IST
=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.271 Loss=1.311 Prec@1=67.727 Prec@5=87.822 rate=2.20 Hz, eta=0:15:56, total=0:03:02, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.271 Loss=1.311 Prec@1=67.727 Prec@5=87.822 rate=2.19 Hz, eta=0:15:13, total=0:03:48, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.271 Loss=1.311 Prec@1=67.727 Prec@5=87.822 rate=2.19 Hz, eta=0:15:13, total=0:03:48, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.270 Loss=1.312 Prec@1=67.731 Prec@5=87.791 rate=2.19 Hz, eta=0:15:13, total=0:03:48, wall=17:57 IST
=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.270 Loss=1.312 Prec@1=67.731 Prec@5=87.791 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:57 IST
=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.466 DataTime=0.270 Loss=1.312 Prec@1=67.731 Prec@5=87.791 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:58 IST
=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.712 Prec@5=87.813 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:58 IST
=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.712 Prec@5=87.813 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:58 IST
=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.712 Prec@5=87.813 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:59 IST
=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.313 Prec@1=67.729 Prec@5=87.811 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:59 IST
=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.313 Prec@1=67.729 Prec@5=87.811 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=17:59 IST
=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.313 Prec@1=67.729 Prec@5=87.811 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=18:00 IST
=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.315 Prec@1=67.679 Prec@5=87.784 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=18:00 IST
=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.315 Prec@1=67.679 Prec@5=87.784 rate=2.18 Hz, eta=0:12:14, total=0:06:53, wall=18:00 IST
=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.315 Prec@1=67.679 Prec@5=87.784 rate=2.18 Hz, eta=0:12:14, total=0:06:53, wall=18:01 IST
=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.316 Prec@1=67.651 Prec@5=87.768 rate=2.18 Hz, eta=0:12:14, total=0:06:53, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.316 Prec@1=67.651 Prec@5=87.768 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.316 Prec@1=67.651 Prec@5=87.768 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.318 Prec@1=67.598 Prec@5=87.736 rate=2.18 Hz, eta=0:11:29, total=0:07:39, wall=18:01 IST
=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.318 Prec@1=67.598 Prec@5=87.736 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=18:01 IST
=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.318 Prec@1=67.598 Prec@5=87.736 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=18:02 IST
=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.320 Prec@1=67.584 Prec@5=87.721 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=18:02 IST
=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.320 Prec@1=67.584 Prec@5=87.721 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=18:02 IST
=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.320 Prec@1=67.584 Prec@5=87.721 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=18:03 IST
=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.322 Prec@1=67.562 Prec@5=87.695 rate=2.18 Hz, eta=0:09:58, total=0:09:11, wall=18:03 IST
=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.322 Prec@1=67.562 Prec@5=87.695 rate=2.17 Hz, eta=0:09:14, total=0:09:59, wall=18:03 IST
=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.465 DataTime=0.269 Loss=1.322 Prec@1=67.562 Prec@5=87.695 rate=2.17 Hz, eta=0:09:14, total=0:09:59, wall=18:04 IST
=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.522 Prec@5=87.675 rate=2.17 Hz, eta=0:09:14, total=0:09:59, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.522 Prec@5=87.675 rate=2.17 Hz, eta=0:08:27, total=0:10:45, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.522 Prec@5=87.675 rate=2.17 Hz, eta=0:08:27, total=0:10:45, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.324 Prec@1=67.528 Prec@5=87.670 rate=2.17 Hz, eta=0:08:27, total=0:10:45, wall=18:04 IST
=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.324 Prec@1=67.528 Prec@5=87.670 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:04 IST
=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.324 Prec@1=67.528 Prec@5=87.670 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:05 IST
=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.325 Prec@1=67.495 Prec@5=87.649 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:05 IST
=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.325 Prec@1=67.495 Prec@5=87.649 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:05 IST
=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.325 Prec@1=67.495 Prec@5=87.649 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:06 IST
=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.326 Prec@1=67.471 Prec@5=87.635 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:06 IST
=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.326 Prec@1=67.471 Prec@5=87.635 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=18:06 IST
=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.326 Prec@1=67.471 Prec@5=87.635 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=18:07 IST
=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.449 Prec@5=87.615 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=18:07 IST
=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.449 Prec@5=87.615 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:07 IST
=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.449 Prec@5=87.615 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:08 IST
=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.329 Prec@1=67.413 Prec@5=87.590 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:08 IST
=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.329 Prec@1=67.413 Prec@5=87.590 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=18:08 IST
=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.267 Loss=1.329 Prec@1=67.413 Prec@5=87.590 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=18:08 IST
=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.330 Prec@1=67.381 Prec@5=87.573 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=18:08 IST
=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.330 Prec@1=67.381 Prec@5=87.573 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=18:08 IST
=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.330 Prec@1=67.381 Prec@5=87.573 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=18:09 IST
=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.331 Prec@1=67.362 Prec@5=87.554 rate=2.17 Hz, eta=0:03:51, total=0:15:22, wall=18:09 IST
=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.331 Prec@1=67.362 Prec@5=87.554 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:09 IST
=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.331 Prec@1=67.362 Prec@5=87.554 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:10 IST
=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.333 Prec@1=67.323 Prec@5=87.524 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:10 IST
=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.333 Prec@1=67.323 Prec@5=87.524 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:10 IST
=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.333 Prec@1=67.323 Prec@5=87.524 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:11 IST
=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.334 Prec@1=67.305 Prec@5=87.507 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.334 Prec@1=67.305 Prec@5=87.507 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.463 DataTime=0.268 Loss=1.334 Prec@1=67.305 Prec@5=87.507 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.335 Prec@1=67.297 Prec@5=87.506 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=18:11 IST
=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.335 Prec@1=67.297 Prec@5=87.506 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=18:11 IST
=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.335 Prec@1=67.297 Prec@5=87.506 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=18:12 IST
=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.336 Prec@1=67.276 Prec@5=87.489 rate=2.17 Hz, eta=0:00:47, total=0:18:28, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.336 Prec@1=67.276 Prec@5=87.489 rate=2.17 Hz, eta=0:00:00, total=0:19:14, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.336 Prec@1=67.276 Prec@5=87.489 rate=2.17 Hz, eta=0:00:00, total=0:19:14, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.336 Prec@1=67.276 Prec@5=87.488 rate=2.17 Hz, eta=0:00:00, total=0:19:14, wall=18:12 IST
=> training   100.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.464 DataTime=0.268 Loss=1.336 Prec@1=67.276 Prec@5=87.488 rate=2.17 Hz, eta=0:00:00, total=0:19:15, wall=18:12 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 0.00% of 1x98...Epoch=78/150 LR=0.04791 Time=7.162 Loss=0.918 Prec@1=77.148 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=7.162 Loss=0.918 Prec@1=77.148 Prec@5=92.578 rate=9064.45 Hz, eta=0:00:00, total=0:00:00, wall=18:12 IST
** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=7.162 Loss=0.918 Prec@1=77.148 Prec@5=92.578 rate=9064.45 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=0.556 Loss=1.406 Prec@1=65.552 Prec@5=87.002 rate=9064.45 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
** validation 100.00% of 1x98...Epoch=78/150 LR=0.04791 Time=0.556 Loss=1.406 Prec@1=65.552 Prec@5=87.002 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=18:13 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.159 DataTime=4.966 Loss=1.349 Prec@1=68.359 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.159 DataTime=4.966 Loss=1.349 Prec@1=68.359 Prec@5=87.500 rate=10107.65 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.159 DataTime=4.966 Loss=1.349 Prec@1=68.359 Prec@5=87.500 rate=10107.65 Hz, eta=0:00:00, total=0:00:00, wall=18:14 IST
=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.499 DataTime=0.307 Loss=1.290 Prec@1=68.073 Prec@5=88.256 rate=10107.65 Hz, eta=0:00:00, total=0:00:00, wall=18:14 IST
=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.499 DataTime=0.307 Loss=1.290 Prec@1=68.073 Prec@5=88.256 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=18:14 IST
=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.499 DataTime=0.307 Loss=1.290 Prec@1=68.073 Prec@5=88.256 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=18:15 IST
=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.478 DataTime=0.284 Loss=1.292 Prec@1=68.123 Prec@5=88.250 rate=2.23 Hz, eta=0:17:55, total=0:00:45, wall=18:15 IST
=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.478 DataTime=0.284 Loss=1.292 Prec@1=68.123 Prec@5=88.250 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:15 IST
=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.478 DataTime=0.284 Loss=1.292 Prec@1=68.123 Prec@5=88.250 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:15 IST
=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.471 DataTime=0.276 Loss=1.294 Prec@1=68.141 Prec@5=88.183 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:15 IST
=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.471 DataTime=0.276 Loss=1.294 Prec@1=68.141 Prec@5=88.183 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=18:15 IST
=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.471 DataTime=0.276 Loss=1.294 Prec@1=68.141 Prec@5=88.183 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=18:16 IST
=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.467 DataTime=0.272 Loss=1.299 Prec@1=68.072 Prec@5=88.080 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=18:16 IST
=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.467 DataTime=0.272 Loss=1.299 Prec@1=68.072 Prec@5=88.080 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:16 IST
=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.467 DataTime=0.272 Loss=1.299 Prec@1=68.072 Prec@5=88.080 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:17 IST
=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.301 Prec@1=68.086 Prec@5=88.037 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:17 IST
=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.301 Prec@1=68.086 Prec@5=88.037 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=18:17 IST
=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.301 Prec@1=68.086 Prec@5=88.037 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=18:18 IST
=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.304 Prec@1=68.021 Prec@5=87.977 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=18:18 IST
=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.304 Prec@1=68.021 Prec@5=87.977 rate=2.19 Hz, eta=0:14:27, total=0:04:34, wall=18:18 IST
=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.304 Prec@1=68.021 Prec@5=87.977 rate=2.19 Hz, eta=0:14:27, total=0:04:34, wall=18:19 IST
=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.305 Prec@1=67.993 Prec@5=87.950 rate=2.19 Hz, eta=0:14:27, total=0:04:34, wall=18:19 IST
=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.305 Prec@1=67.993 Prec@5=87.950 rate=2.18 Hz, eta=0:13:45, total=0:05:20, wall=18:19 IST
=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.305 Prec@1=67.993 Prec@5=87.950 rate=2.18 Hz, eta=0:13:45, total=0:05:20, wall=18:19 IST
=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.307 Prec@1=67.952 Prec@5=87.940 rate=2.18 Hz, eta=0:13:45, total=0:05:20, wall=18:19 IST
=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.307 Prec@1=67.952 Prec@5=87.940 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=18:19 IST
=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.271 Loss=1.307 Prec@1=67.952 Prec@5=87.940 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=18:20 IST
=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.309 Prec@1=67.915 Prec@5=87.902 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=18:20 IST
=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.309 Prec@1=67.915 Prec@5=87.902 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=18:20 IST
=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.309 Prec@1=67.915 Prec@5=87.902 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=18:21 IST
=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.312 Prec@1=67.883 Prec@5=87.868 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=18:21 IST
=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.312 Prec@1=67.883 Prec@5=87.868 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=18:21 IST
=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.270 Loss=1.312 Prec@1=67.883 Prec@5=87.868 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=18:22 IST
=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.862 Prec@5=87.847 rate=2.18 Hz, eta=0:11:30, total=0:07:40, wall=18:22 IST
=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.862 Prec@5=87.847 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=18:22 IST
=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.465 DataTime=0.269 Loss=1.313 Prec@1=67.862 Prec@5=87.847 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=18:22 IST
=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.315 Prec@1=67.817 Prec@5=87.823 rate=2.17 Hz, eta=0:10:45, total=0:08:26, wall=18:22 IST
=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.315 Prec@1=67.817 Prec@5=87.823 rate=2.18 Hz, eta=0:09:58, total=0:09:12, wall=18:22 IST
=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.315 Prec@1=67.817 Prec@5=87.823 rate=2.18 Hz, eta=0:09:58, total=0:09:12, wall=18:23 IST
=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.316 Prec@1=67.797 Prec@5=87.811 rate=2.18 Hz, eta=0:09:58, total=0:09:12, wall=18:23 IST
=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.316 Prec@1=67.797 Prec@5=87.811 rate=2.17 Hz, eta=0:09:13, total=0:09:58, wall=18:23 IST
=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.316 Prec@1=67.797 Prec@5=87.811 rate=2.17 Hz, eta=0:09:13, total=0:09:58, wall=18:24 IST
=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.317 Prec@1=67.775 Prec@5=87.781 rate=2.17 Hz, eta=0:09:13, total=0:09:58, wall=18:24 IST
=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.317 Prec@1=67.775 Prec@5=87.781 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=18:24 IST
=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.317 Prec@1=67.775 Prec@5=87.781 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=18:25 IST
=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.748 Prec@5=87.750 rate=2.17 Hz, eta=0:08:27, total=0:10:44, wall=18:25 IST
=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.748 Prec@5=87.750 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:25 IST
=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.748 Prec@5=87.750 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:25 IST
=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.728 Prec@5=87.749 rate=2.17 Hz, eta=0:07:41, total=0:11:31, wall=18:25 IST
=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.728 Prec@5=87.749 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:25 IST
=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.319 Prec@1=67.728 Prec@5=87.749 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:26 IST
=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.321 Prec@1=67.701 Prec@5=87.726 rate=2.17 Hz, eta=0:06:55, total=0:12:17, wall=18:26 IST
=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.321 Prec@1=67.701 Prec@5=87.726 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=18:26 IST
=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.321 Prec@1=67.701 Prec@5=87.726 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=18:27 IST
=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.321 Prec@1=67.689 Prec@5=87.708 rate=2.17 Hz, eta=0:06:09, total=0:13:03, wall=18:27 IST
=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.321 Prec@1=67.689 Prec@5=87.708 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:27 IST
=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.321 Prec@1=67.689 Prec@5=87.708 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:28 IST
=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.654 Prec@5=87.685 rate=2.17 Hz, eta=0:05:23, total=0:13:49, wall=18:28 IST
=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.654 Prec@5=87.685 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=18:28 IST
=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.323 Prec@1=67.654 Prec@5=87.685 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=18:29 IST
=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.324 Prec@1=67.628 Prec@5=87.668 rate=2.17 Hz, eta=0:04:37, total=0:14:36, wall=18:29 IST
=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.324 Prec@1=67.628 Prec@5=87.668 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=18:29 IST
=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.324 Prec@1=67.628 Prec@5=87.668 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=18:29 IST
=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.325 Prec@1=67.600 Prec@5=87.657 rate=2.17 Hz, eta=0:03:51, total=0:15:23, wall=18:29 IST
=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.325 Prec@1=67.600 Prec@5=87.657 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:29 IST
=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.269 Loss=1.325 Prec@1=67.600 Prec@5=87.657 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:30 IST
=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.326 Prec@1=67.576 Prec@5=87.637 rate=2.17 Hz, eta=0:03:05, total=0:16:09, wall=18:30 IST
=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.326 Prec@1=67.576 Prec@5=87.637 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:30 IST
=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.464 DataTime=0.268 Loss=1.326 Prec@1=67.576 Prec@5=87.637 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:31 IST
=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.556 Prec@5=87.624 rate=2.17 Hz, eta=0:02:19, total=0:16:55, wall=18:31 IST
=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.556 Prec@5=87.624 rate=2.17 Hz, eta=0:01:33, total=0:17:40, wall=18:31 IST
=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.327 Prec@1=67.556 Prec@5=87.624 rate=2.17 Hz, eta=0:01:33, total=0:17:40, wall=18:32 IST
=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.328 Prec@1=67.537 Prec@5=87.619 rate=2.17 Hz, eta=0:01:33, total=0:17:40, wall=18:32 IST
=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.328 Prec@1=67.537 Prec@5=87.619 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=18:32 IST
=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.328 Prec@1=67.537 Prec@5=87.619 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=18:32 IST
=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.329 Prec@1=67.526 Prec@5=87.605 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=18:32 IST
=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.329 Prec@1=67.526 Prec@5=87.605 rate=2.17 Hz, eta=0:00:00, total=0:19:12, wall=18:32 IST
=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.329 Prec@1=67.526 Prec@5=87.605 rate=2.17 Hz, eta=0:00:00, total=0:19:12, wall=18:32 IST
=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.329 Prec@1=67.522 Prec@5=87.604 rate=2.17 Hz, eta=0:00:00, total=0:19:12, wall=18:32 IST
=> training   100.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.463 DataTime=0.268 Loss=1.329 Prec@1=67.522 Prec@5=87.604 rate=2.17 Hz, eta=0:00:00, total=0:19:13, wall=18:32 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 0.00% of 1x98...Epoch=79/150 LR=0.04686 Time=6.999 Loss=0.906 Prec@1=77.148 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=6.999 Loss=0.906 Prec@1=77.148 Prec@5=93.164 rate=8311.31 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=6.999 Loss=0.906 Prec@1=77.148 Prec@5=93.164 rate=8311.31 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=0.557 Loss=1.399 Prec@1=66.058 Prec@5=87.148 rate=8311.31 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** validation 100.00% of 1x98...Epoch=79/150 LR=0.04686 Time=0.557 Loss=1.399 Prec@1=66.058 Prec@5=87.148 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=18:33 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> training   0.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=4.951 DataTime=4.709 Loss=1.341 Prec@1=66.406 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=4.951 DataTime=4.709 Loss=1.341 Prec@1=66.406 Prec@5=87.305 rate=9376.64 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=4.951 DataTime=4.709 Loss=1.341 Prec@1=66.406 Prec@5=87.305 rate=9376.64 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.509 DataTime=0.311 Loss=1.283 Prec@1=68.421 Prec@5=88.227 rate=9376.64 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.509 DataTime=0.311 Loss=1.283 Prec@1=68.421 Prec@5=88.227 rate=2.18 Hz, eta=0:18:24, total=0:00:46, wall=18:34 IST
=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.509 DataTime=0.311 Loss=1.283 Prec@1=68.421 Prec@5=88.227 rate=2.18 Hz, eta=0:18:24, total=0:00:46, wall=18:35 IST
=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.486 DataTime=0.290 Loss=1.286 Prec@1=68.452 Prec@5=88.118 rate=2.18 Hz, eta=0:18:24, total=0:00:46, wall=18:35 IST
=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.486 DataTime=0.290 Loss=1.286 Prec@1=68.452 Prec@5=88.118 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=18:35 IST
=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.486 DataTime=0.290 Loss=1.286 Prec@1=68.452 Prec@5=88.118 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=18:36 IST
=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.477 DataTime=0.281 Loss=1.288 Prec@1=68.391 Prec@5=88.097 rate=2.17 Hz, eta=0:17:42, total=0:01:32, wall=18:36 IST
=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.477 DataTime=0.281 Loss=1.288 Prec@1=68.391 Prec@5=88.097 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=18:36 IST
=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.477 DataTime=0.281 Loss=1.288 Prec@1=68.391 Prec@5=88.097 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=18:37 IST
=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.472 DataTime=0.276 Loss=1.293 Prec@1=68.258 Prec@5=88.048 rate=2.17 Hz, eta=0:16:53, total=0:02:18, wall=18:37 IST
=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.472 DataTime=0.276 Loss=1.293 Prec@1=68.258 Prec@5=88.048 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=18:37 IST
=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.472 DataTime=0.276 Loss=1.293 Prec@1=68.258 Prec@5=88.048 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=18:37 IST
=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.294 Prec@1=68.209 Prec@5=88.046 rate=2.17 Hz, eta=0:16:06, total=0:03:04, wall=18:37 IST
=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.294 Prec@1=68.209 Prec@5=88.046 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=18:37 IST
=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.294 Prec@1=68.209 Prec@5=88.046 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=18:38 IST
=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.470 DataTime=0.275 Loss=1.299 Prec@1=68.103 Prec@5=87.978 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=18:38 IST
=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.470 DataTime=0.275 Loss=1.299 Prec@1=68.103 Prec@5=87.978 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=18:38 IST
=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.470 DataTime=0.275 Loss=1.299 Prec@1=68.103 Prec@5=87.978 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=18:39 IST
=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.274 Loss=1.301 Prec@1=68.056 Prec@5=87.962 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=18:39 IST
=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.274 Loss=1.301 Prec@1=68.056 Prec@5=87.962 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:39 IST
=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.274 Loss=1.301 Prec@1=68.056 Prec@5=87.962 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:40 IST
=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.304 Prec@1=68.033 Prec@5=87.917 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=18:40 IST
=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.304 Prec@1=68.033 Prec@5=87.917 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:40 IST
=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.469 DataTime=0.273 Loss=1.304 Prec@1=68.033 Prec@5=87.917 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:40 IST
=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.272 Loss=1.306 Prec@1=67.953 Prec@5=87.891 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=18:40 IST
=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.272 Loss=1.306 Prec@1=67.953 Prec@5=87.891 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=18:40 IST
=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.272 Loss=1.306 Prec@1=67.953 Prec@5=87.891 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=18:41 IST
=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.307 Prec@1=67.933 Prec@5=87.891 rate=2.16 Hz, eta=0:12:20, total=0:06:56, wall=18:41 IST
=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.307 Prec@1=67.933 Prec@5=87.891 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=18:41 IST
=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.307 Prec@1=67.933 Prec@5=87.891 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=18:42 IST
=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.906 Prec@5=87.868 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=18:42 IST
=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.906 Prec@5=87.868 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=18:42 IST
=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.906 Prec@5=87.868 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=18:43 IST
=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.900 Prec@5=87.868 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=18:43 IST
=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.900 Prec@5=87.868 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=18:43 IST
=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.309 Prec@1=67.900 Prec@5=87.868 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=18:44 IST
=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.310 Prec@1=67.875 Prec@5=87.854 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=18:44 IST
=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.310 Prec@1=67.875 Prec@5=87.854 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=18:44 IST
=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.310 Prec@1=67.875 Prec@5=87.854 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=18:44 IST
=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.311 Prec@1=67.874 Prec@5=87.851 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=18:44 IST
=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.311 Prec@1=67.874 Prec@5=87.851 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=18:44 IST
=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.311 Prec@1=67.874 Prec@5=87.851 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=18:45 IST
=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.313 Prec@1=67.832 Prec@5=87.830 rate=2.16 Hz, eta=0:08:29, total=0:10:48, wall=18:45 IST
=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.313 Prec@1=67.832 Prec@5=87.830 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:45 IST
=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.313 Prec@1=67.832 Prec@5=87.830 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:46 IST
=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.314 Prec@1=67.801 Prec@5=87.808 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:46 IST
=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.314 Prec@1=67.801 Prec@5=87.808 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=18:46 IST
=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.466 DataTime=0.270 Loss=1.314 Prec@1=67.801 Prec@5=87.808 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=18:47 IST
=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.809 rate=2.16 Hz, eta=0:06:57, total=0:12:21, wall=18:47 IST
=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.809 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=18:47 IST
=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.809 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=18:47 IST
=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.808 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=18:47 IST
=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.808 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=18:47 IST
=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.795 Prec@5=87.808 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=18:48 IST
=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.780 Prec@5=87.791 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=18:48 IST
=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.780 Prec@5=87.791 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=18:48 IST
=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.780 Prec@5=87.791 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=18:49 IST
=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.747 Prec@5=87.779 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=18:49 IST
=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.747 Prec@5=87.779 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=18:49 IST
=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.747 Prec@5=87.779 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=18:50 IST
=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.317 Prec@1=67.722 Prec@5=87.766 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=18:50 IST
=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.317 Prec@1=67.722 Prec@5=87.766 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=18:50 IST
=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.317 Prec@1=67.722 Prec@5=87.766 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=18:51 IST
=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.319 Prec@1=67.691 Prec@5=87.746 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=18:51 IST
=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.319 Prec@1=67.691 Prec@5=87.746 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=18:51 IST
=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.319 Prec@1=67.691 Prec@5=87.746 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=18:51 IST
=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.664 Prec@5=87.731 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=18:51 IST
=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.664 Prec@5=87.731 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=18:51 IST
=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.664 Prec@5=87.731 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=18:52 IST
=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.654 Prec@5=87.726 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=18:52 IST
=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.654 Prec@5=87.726 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=18:52 IST
=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.320 Prec@1=67.654 Prec@5=87.726 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=18:53 IST
=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.321 Prec@1=67.641 Prec@5=87.715 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.321 Prec@1=67.641 Prec@5=87.715 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.321 Prec@1=67.641 Prec@5=87.715 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.321 Prec@1=67.641 Prec@5=87.715 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=18:53 IST
=> training   100.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.467 DataTime=0.271 Loss=1.321 Prec@1=67.641 Prec@5=87.715 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=18:53 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 0.00% of 1x98...Epoch=80/150 LR=0.04582 Time=6.981 Loss=0.974 Prec@1=75.391 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.981 Loss=0.974 Prec@1=75.391 Prec@5=92.383 rate=8049.13 Hz, eta=0:00:00, total=0:00:00, wall=18:53 IST
** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.981 Loss=0.974 Prec@1=75.391 Prec@5=92.383 rate=8049.13 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=0.551 Loss=1.436 Prec@1=65.152 Prec@5=86.572 rate=8049.13 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** validation 100.00% of 1x98...Epoch=80/150 LR=0.04582 Time=0.551 Loss=1.436 Prec@1=65.152 Prec@5=86.572 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=18:54 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.450 DataTime=4.211 Loss=1.366 Prec@1=69.141 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.450 DataTime=4.211 Loss=1.366 Prec@1=69.141 Prec@5=86.914 rate=4993.63 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.450 DataTime=4.211 Loss=1.366 Prec@1=69.141 Prec@5=86.914 rate=4993.63 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.502 DataTime=0.305 Loss=1.286 Prec@1=68.149 Prec@5=88.217 rate=4993.63 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.502 DataTime=0.305 Loss=1.286 Prec@1=68.149 Prec@5=88.217 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=18:55 IST
=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.502 DataTime=0.305 Loss=1.286 Prec@1=68.149 Prec@5=88.217 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=18:55 IST
=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.485 DataTime=0.288 Loss=1.281 Prec@1=68.259 Prec@5=88.208 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=18:55 IST
=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.485 DataTime=0.288 Loss=1.281 Prec@1=68.259 Prec@5=88.208 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=18:55 IST
=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.485 DataTime=0.288 Loss=1.281 Prec@1=68.259 Prec@5=88.208 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=18:56 IST
=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.477 DataTime=0.282 Loss=1.289 Prec@1=68.150 Prec@5=88.123 rate=2.16 Hz, eta=0:17:46, total=0:01:33, wall=18:56 IST
=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.477 DataTime=0.282 Loss=1.289 Prec@1=68.150 Prec@5=88.123 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=18:56 IST
=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.477 DataTime=0.282 Loss=1.289 Prec@1=68.150 Prec@5=88.123 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=18:57 IST
=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.472 DataTime=0.276 Loss=1.289 Prec@1=68.195 Prec@5=88.166 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=18:57 IST
=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.472 DataTime=0.276 Loss=1.289 Prec@1=68.195 Prec@5=88.166 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=18:57 IST
=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.472 DataTime=0.276 Loss=1.289 Prec@1=68.195 Prec@5=88.166 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=18:58 IST
=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.471 DataTime=0.275 Loss=1.291 Prec@1=68.147 Prec@5=88.142 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=18:58 IST
=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.471 DataTime=0.275 Loss=1.291 Prec@1=68.147 Prec@5=88.142 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:58 IST
=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.471 DataTime=0.275 Loss=1.291 Prec@1=68.147 Prec@5=88.142 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:59 IST
=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.292 Prec@1=68.141 Prec@5=88.138 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=18:59 IST
=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.292 Prec@1=68.141 Prec@5=88.138 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:59 IST
=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.292 Prec@1=68.141 Prec@5=88.138 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:59 IST
=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.295 Prec@1=68.105 Prec@5=88.072 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=18:59 IST
=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.295 Prec@1=68.105 Prec@5=88.072 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=18:59 IST
=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.295 Prec@1=68.105 Prec@5=88.072 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=19:00 IST
=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.298 Prec@1=68.068 Prec@5=88.024 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=19:00 IST
=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.298 Prec@1=68.068 Prec@5=88.024 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=19:00 IST
=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.298 Prec@1=68.068 Prec@5=88.024 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=19:01 IST
=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.275 Loss=1.300 Prec@1=68.016 Prec@5=87.995 rate=2.16 Hz, eta=0:13:08, total=0:06:11, wall=19:01 IST
=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.275 Loss=1.300 Prec@1=68.016 Prec@5=87.995 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:01 IST
=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.275 Loss=1.300 Prec@1=68.016 Prec@5=87.995 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:02 IST
=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.301 Prec@1=68.012 Prec@5=87.972 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:02 IST
=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.301 Prec@1=68.012 Prec@5=87.972 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=19:02 IST
=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.470 DataTime=0.274 Loss=1.301 Prec@1=68.012 Prec@5=87.972 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=19:02 IST
=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.302 Prec@1=67.985 Prec@5=87.942 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=19:02 IST
=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.302 Prec@1=67.985 Prec@5=87.942 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=19:02 IST
=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.469 DataTime=0.274 Loss=1.302 Prec@1=67.985 Prec@5=87.942 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=19:03 IST
=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.304 Prec@1=67.923 Prec@5=87.918 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=19:03 IST
=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.304 Prec@1=67.923 Prec@5=87.918 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:03 IST
=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.304 Prec@1=67.923 Prec@5=87.918 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:04 IST
=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.305 Prec@1=67.916 Prec@5=87.924 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:04 IST
=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.305 Prec@1=67.916 Prec@5=87.924 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=19:04 IST
=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.305 Prec@1=67.916 Prec@5=87.924 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=19:05 IST
=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.899 Prec@5=87.887 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=19:05 IST
=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.899 Prec@5=87.887 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=19:05 IST
=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.899 Prec@5=87.887 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=19:06 IST
=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.306 Prec@1=67.906 Prec@5=87.888 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=19:06 IST
=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.306 Prec@1=67.906 Prec@5=87.888 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=19:06 IST
=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.306 Prec@1=67.906 Prec@5=87.888 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=19:06 IST
=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.878 Prec@5=87.878 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=19:06 IST
=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.878 Prec@5=87.878 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=19:06 IST
=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.273 Loss=1.307 Prec@1=67.878 Prec@5=87.878 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=19:07 IST
=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.307 Prec@1=67.888 Prec@5=87.882 rate=2.15 Hz, eta=0:06:59, total=0:12:25, wall=19:07 IST
=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.307 Prec@1=67.888 Prec@5=87.882 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=19:07 IST
=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.307 Prec@1=67.888 Prec@5=87.882 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=19:08 IST
=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.309 Prec@1=67.854 Prec@5=87.862 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=19:08 IST
=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.309 Prec@1=67.854 Prec@5=87.862 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:08 IST
=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.468 DataTime=0.272 Loss=1.309 Prec@1=67.854 Prec@5=87.862 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:09 IST
=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.311 Prec@1=67.823 Prec@5=87.830 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:09 IST
=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.311 Prec@1=67.823 Prec@5=87.830 rate=2.15 Hz, eta=0:04:39, total=0:14:44, wall=19:09 IST
=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.311 Prec@1=67.823 Prec@5=87.830 rate=2.15 Hz, eta=0:04:39, total=0:14:44, wall=19:09 IST
=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.312 Prec@1=67.805 Prec@5=87.808 rate=2.15 Hz, eta=0:04:39, total=0:14:44, wall=19:09 IST
=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.312 Prec@1=67.805 Prec@5=87.808 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:09 IST
=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.312 Prec@1=67.805 Prec@5=87.808 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:10 IST
=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.313 Prec@1=67.777 Prec@5=87.801 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:10 IST
=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.313 Prec@1=67.777 Prec@5=87.801 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:10 IST
=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.272 Loss=1.313 Prec@1=67.777 Prec@5=87.801 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:11 IST
=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.753 Prec@5=87.782 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:11 IST
=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.753 Prec@5=87.782 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=19:11 IST
=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.314 Prec@1=67.753 Prec@5=87.782 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=19:12 IST
=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.735 Prec@5=87.765 rate=2.15 Hz, eta=0:02:20, total=0:17:03, wall=19:12 IST
=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.735 Prec@5=87.765 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=19:12 IST
=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.315 Prec@1=67.735 Prec@5=87.765 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=19:12 IST
=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.718 Prec@5=87.755 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=19:12 IST
=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.718 Prec@5=87.755 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=19:12 IST
=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.467 DataTime=0.271 Loss=1.316 Prec@1=67.718 Prec@5=87.755 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=19:13 IST
=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.466 DataTime=0.271 Loss=1.317 Prec@1=67.696 Prec@5=87.741 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=19:13 IST
=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.466 DataTime=0.271 Loss=1.317 Prec@1=67.696 Prec@5=87.741 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=19:13 IST
=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.466 DataTime=0.271 Loss=1.317 Prec@1=67.696 Prec@5=87.741 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=19:13 IST
=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.466 DataTime=0.271 Loss=1.317 Prec@1=67.696 Prec@5=87.741 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=19:13 IST
=> training   100.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.466 DataTime=0.271 Loss=1.317 Prec@1=67.696 Prec@5=87.741 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=19:13 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:13 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:13 IST
=> validation 0.00% of 1x98...Epoch=81/150 LR=0.04477 Time=6.828 Loss=0.832 Prec@1=78.125 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=19:13 IST
=> validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=6.828 Loss=0.832 Prec@1=78.125 Prec@5=93.945 rate=9851.53 Hz, eta=0:00:00, total=0:00:00, wall=19:13 IST
** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=6.828 Loss=0.832 Prec@1=78.125 Prec@5=93.945 rate=9851.53 Hz, eta=0:00:00, total=0:00:00, wall=19:14 IST
** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=0.552 Loss=1.391 Prec@1=66.164 Prec@5=87.196 rate=9851.53 Hz, eta=0:00:00, total=0:00:00, wall=19:14 IST
** validation 100.00% of 1x98...Epoch=81/150 LR=0.04477 Time=0.552 Loss=1.391 Prec@1=66.164 Prec@5=87.196 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=19:14 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> training   0.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=4.428 DataTime=4.179 Loss=1.237 Prec@1=69.531 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=4.428 DataTime=4.179 Loss=1.237 Prec@1=69.531 Prec@5=90.039 rate=4912.73 Hz, eta=0:00:00, total=0:00:00, wall=19:14 IST
=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=4.428 DataTime=4.179 Loss=1.237 Prec@1=69.531 Prec@5=90.039 rate=4912.73 Hz, eta=0:00:00, total=0:00:00, wall=19:15 IST
=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.499 DataTime=0.305 Loss=1.275 Prec@1=68.887 Prec@5=88.074 rate=4912.73 Hz, eta=0:00:00, total=0:00:00, wall=19:15 IST
=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.499 DataTime=0.305 Loss=1.275 Prec@1=68.887 Prec@5=88.074 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=19:15 IST
=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.499 DataTime=0.305 Loss=1.275 Prec@1=68.887 Prec@5=88.074 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=19:16 IST
=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.481 DataTime=0.285 Loss=1.271 Prec@1=68.836 Prec@5=88.240 rate=2.20 Hz, eta=0:18:13, total=0:00:45, wall=19:16 IST
=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.481 DataTime=0.285 Loss=1.271 Prec@1=68.836 Prec@5=88.240 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=19:16 IST
=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.481 DataTime=0.285 Loss=1.271 Prec@1=68.836 Prec@5=88.240 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=19:17 IST
=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.476 DataTime=0.280 Loss=1.276 Prec@1=68.668 Prec@5=88.208 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=19:17 IST
=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.476 DataTime=0.280 Loss=1.276 Prec@1=68.668 Prec@5=88.208 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=19:17 IST
=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.476 DataTime=0.280 Loss=1.276 Prec@1=68.668 Prec@5=88.208 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=19:17 IST
=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.475 DataTime=0.279 Loss=1.280 Prec@1=68.540 Prec@5=88.174 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=19:17 IST
=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.475 DataTime=0.279 Loss=1.280 Prec@1=68.540 Prec@5=88.174 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=19:17 IST
=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.475 DataTime=0.279 Loss=1.280 Prec@1=68.540 Prec@5=88.174 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=19:18 IST
=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.472 DataTime=0.275 Loss=1.282 Prec@1=68.536 Prec@5=88.167 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=19:18 IST
=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.472 DataTime=0.275 Loss=1.282 Prec@1=68.536 Prec@5=88.167 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:18 IST
=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.472 DataTime=0.275 Loss=1.282 Prec@1=68.536 Prec@5=88.167 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:19 IST
=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.274 Loss=1.282 Prec@1=68.484 Prec@5=88.174 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=19:19 IST
=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.274 Loss=1.282 Prec@1=68.484 Prec@5=88.174 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:19 IST
=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.274 Loss=1.282 Prec@1=68.484 Prec@5=88.174 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:20 IST
=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.275 Loss=1.283 Prec@1=68.438 Prec@5=88.154 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=19:20 IST
=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.275 Loss=1.283 Prec@1=68.438 Prec@5=88.154 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=19:20 IST
=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.471 DataTime=0.275 Loss=1.283 Prec@1=68.438 Prec@5=88.154 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=19:20 IST
=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.285 Prec@1=68.403 Prec@5=88.138 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=19:20 IST
=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.285 Prec@1=68.403 Prec@5=88.138 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:20 IST
=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.285 Prec@1=68.403 Prec@5=88.138 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:21 IST
=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.288 Prec@1=68.351 Prec@5=88.117 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:21 IST
=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.288 Prec@1=68.351 Prec@5=88.117 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:21 IST
=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.288 Prec@1=68.351 Prec@5=88.117 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:22 IST
=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.288 Prec@1=68.343 Prec@5=88.114 rate=2.15 Hz, eta=0:12:25, total=0:06:59, wall=19:22 IST
=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.288 Prec@1=68.343 Prec@5=88.114 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:22 IST
=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.288 Prec@1=68.343 Prec@5=88.114 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:23 IST
=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.468 DataTime=0.272 Loss=1.291 Prec@1=68.272 Prec@5=88.079 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:23 IST
=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.468 DataTime=0.272 Loss=1.291 Prec@1=68.272 Prec@5=88.079 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=19:23 IST
=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.468 DataTime=0.272 Loss=1.291 Prec@1=68.272 Prec@5=88.079 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=19:24 IST
=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.291 Prec@1=68.248 Prec@5=88.069 rate=2.15 Hz, eta=0:10:50, total=0:08:31, wall=19:24 IST
=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.291 Prec@1=68.248 Prec@5=88.069 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:24 IST
=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.291 Prec@1=68.248 Prec@5=88.069 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:24 IST
=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.294 Prec@1=68.229 Prec@5=88.036 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:24 IST
=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.294 Prec@1=68.229 Prec@5=88.036 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:24 IST
=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.294 Prec@1=68.229 Prec@5=88.036 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:25 IST
=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.296 Prec@1=68.196 Prec@5=88.016 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:25 IST
=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.296 Prec@1=68.196 Prec@5=88.016 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:25 IST
=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.296 Prec@1=68.196 Prec@5=88.016 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:26 IST
=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.297 Prec@1=68.171 Prec@5=88.003 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:26 IST
=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.297 Prec@1=68.171 Prec@5=88.003 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=19:26 IST
=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.297 Prec@1=68.171 Prec@5=88.003 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=19:27 IST
=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.167 Prec@5=87.991 rate=2.15 Hz, eta=0:07:46, total=0:11:39, wall=19:27 IST
=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.167 Prec@5=87.991 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=19:27 IST
=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.167 Prec@5=87.991 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=19:27 IST
=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.169 Prec@5=87.982 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=19:27 IST
=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.169 Prec@5=87.982 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=19:27 IST
=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.272 Loss=1.298 Prec@1=68.169 Prec@5=87.982 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=19:28 IST
=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.299 Prec@1=68.138 Prec@5=87.968 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=19:28 IST
=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.299 Prec@1=68.138 Prec@5=87.968 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=19:28 IST
=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.299 Prec@1=68.138 Prec@5=87.968 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=19:29 IST
=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.300 Prec@1=68.121 Prec@5=87.959 rate=2.14 Hz, eta=0:05:27, total=0:14:00, wall=19:29 IST
=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.300 Prec@1=68.121 Prec@5=87.959 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=19:29 IST
=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.470 DataTime=0.274 Loss=1.300 Prec@1=68.121 Prec@5=87.959 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=19:30 IST
=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.301 Prec@1=68.100 Prec@5=87.947 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=19:30 IST
=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.301 Prec@1=68.100 Prec@5=87.947 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=19:30 IST
=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.301 Prec@1=68.100 Prec@5=87.947 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=19:31 IST
=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.302 Prec@1=68.069 Prec@5=87.933 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=19:31 IST
=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.302 Prec@1=68.069 Prec@5=87.933 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=19:31 IST
=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.273 Loss=1.302 Prec@1=68.069 Prec@5=87.933 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=19:31 IST
=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.303 Prec@1=68.034 Prec@5=87.928 rate=2.14 Hz, eta=0:03:07, total=0:16:21, wall=19:31 IST
=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.303 Prec@1=68.034 Prec@5=87.928 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=19:31 IST
=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.303 Prec@1=68.034 Prec@5=87.928 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=19:32 IST
=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.305 Prec@1=68.009 Prec@5=87.912 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=19:32 IST
=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.305 Prec@1=68.009 Prec@5=87.912 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=19:32 IST
=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.305 Prec@1=68.009 Prec@5=87.912 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=19:33 IST
=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.307 Prec@1=67.977 Prec@5=87.879 rate=2.14 Hz, eta=0:01:34, total=0:17:55, wall=19:33 IST
=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.307 Prec@1=67.977 Prec@5=87.879 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=19:33 IST
=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.307 Prec@1=67.977 Prec@5=87.879 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=19:34 IST
=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.308 Prec@1=67.954 Prec@5=87.858 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=19:34 IST
=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.308 Prec@1=67.954 Prec@5=87.858 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=19:34 IST
=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.308 Prec@1=67.954 Prec@5=87.858 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=19:34 IST
=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.308 Prec@1=67.953 Prec@5=87.857 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=19:34 IST
=> training   100.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.469 DataTime=0.274 Loss=1.308 Prec@1=67.953 Prec@5=87.857 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=19:34 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:34 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:34 IST
=> validation 0.00% of 1x98...Epoch=82/150 LR=0.04373 Time=6.582 Loss=0.819 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=19:34 IST
=> validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=6.582 Loss=0.819 Prec@1=81.055 Prec@5=95.312 rate=7940.86 Hz, eta=0:00:00, total=0:00:00, wall=19:34 IST
** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=6.582 Loss=0.819 Prec@1=81.055 Prec@5=95.312 rate=7940.86 Hz, eta=0:00:00, total=0:00:00, wall=19:35 IST
** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=0.543 Loss=1.396 Prec@1=65.958 Prec@5=87.122 rate=7940.86 Hz, eta=0:00:00, total=0:00:00, wall=19:35 IST
** validation 100.00% of 1x98...Epoch=82/150 LR=0.04373 Time=0.543 Loss=1.396 Prec@1=65.958 Prec@5=87.122 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=19:35 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:35 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:35 IST
=> training   0.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.020 DataTime=4.768 Loss=1.251 Prec@1=68.359 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=19:35 IST
=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.020 DataTime=4.768 Loss=1.251 Prec@1=68.359 Prec@5=87.695 rate=7739.46 Hz, eta=0:00:00, total=0:00:00, wall=19:35 IST
=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.020 DataTime=4.768 Loss=1.251 Prec@1=68.359 Prec@5=87.695 rate=7739.46 Hz, eta=0:00:00, total=0:00:00, wall=19:36 IST
=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.499 DataTime=0.302 Loss=1.272 Prec@1=68.644 Prec@5=88.403 rate=7739.46 Hz, eta=0:00:00, total=0:00:00, wall=19:36 IST
=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.499 DataTime=0.302 Loss=1.272 Prec@1=68.644 Prec@5=88.403 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=19:36 IST
=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.499 DataTime=0.302 Loss=1.272 Prec@1=68.644 Prec@5=88.403 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=19:36 IST
=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.479 DataTime=0.282 Loss=1.267 Prec@1=68.842 Prec@5=88.538 rate=2.23 Hz, eta=0:17:59, total=0:00:45, wall=19:36 IST
=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.479 DataTime=0.282 Loss=1.267 Prec@1=68.842 Prec@5=88.538 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=19:36 IST
=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.479 DataTime=0.282 Loss=1.267 Prec@1=68.842 Prec@5=88.538 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=19:37 IST
=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.473 DataTime=0.277 Loss=1.271 Prec@1=68.699 Prec@5=88.450 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=19:37 IST
=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.473 DataTime=0.277 Loss=1.271 Prec@1=68.699 Prec@5=88.450 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=19:37 IST
=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.473 DataTime=0.277 Loss=1.271 Prec@1=68.699 Prec@5=88.450 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=19:38 IST
=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.274 Loss=1.273 Prec@1=68.693 Prec@5=88.365 rate=2.19 Hz, eta=0:16:44, total=0:02:17, wall=19:38 IST
=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.274 Loss=1.273 Prec@1=68.693 Prec@5=88.365 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:38 IST
=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.274 Loss=1.273 Prec@1=68.693 Prec@5=88.365 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:39 IST
=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.273 Loss=1.276 Prec@1=68.623 Prec@5=88.324 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=19:39 IST
=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.273 Loss=1.276 Prec@1=68.623 Prec@5=88.324 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=19:39 IST
=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.273 Loss=1.276 Prec@1=68.623 Prec@5=88.324 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=19:39 IST
=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.278 Prec@1=68.587 Prec@5=88.317 rate=2.18 Hz, eta=0:15:18, total=0:03:49, wall=19:39 IST
=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.278 Prec@1=68.587 Prec@5=88.317 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=19:39 IST
=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.278 Prec@1=68.587 Prec@5=88.317 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=19:40 IST
=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.281 Prec@1=68.523 Prec@5=88.283 rate=2.17 Hz, eta=0:14:37, total=0:04:37, wall=19:40 IST
=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.281 Prec@1=68.523 Prec@5=88.283 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=19:40 IST
=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.281 Prec@1=68.523 Prec@5=88.283 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=19:41 IST
=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.471 DataTime=0.275 Loss=1.280 Prec@1=68.537 Prec@5=88.269 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=19:41 IST
=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.471 DataTime=0.275 Loss=1.280 Prec@1=68.537 Prec@5=88.269 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=19:41 IST
=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.471 DataTime=0.275 Loss=1.280 Prec@1=68.537 Prec@5=88.269 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=19:42 IST
=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.469 Prec@5=88.232 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=19:42 IST
=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.469 Prec@5=88.232 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=19:42 IST
=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.469 Prec@5=88.232 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=19:43 IST
=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.283 Prec@1=68.437 Prec@5=88.224 rate=2.15 Hz, eta=0:12:23, total=0:06:58, wall=19:43 IST
=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.283 Prec@1=68.437 Prec@5=88.224 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=19:43 IST
=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.469 DataTime=0.274 Loss=1.283 Prec@1=68.437 Prec@5=88.224 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=19:43 IST
=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.284 Prec@1=68.424 Prec@5=88.221 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=19:43 IST
=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.284 Prec@1=68.424 Prec@5=88.221 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=19:43 IST
=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.284 Prec@1=68.424 Prec@5=88.221 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=19:44 IST
=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.285 Prec@1=68.397 Prec@5=88.193 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=19:44 IST
=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.285 Prec@1=68.397 Prec@5=88.193 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=19:44 IST
=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.285 Prec@1=68.397 Prec@5=88.193 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=19:45 IST
=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.288 Prec@1=68.351 Prec@5=88.158 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=19:45 IST
=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.288 Prec@1=68.351 Prec@5=88.158 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=19:45 IST
=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.288 Prec@1=68.351 Prec@5=88.158 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=19:46 IST
=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.317 Prec@5=88.141 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=19:46 IST
=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.317 Prec@5=88.141 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=19:46 IST
=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.317 Prec@5=88.141 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=19:46 IST
=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.260 Prec@5=88.101 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=19:46 IST
=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.260 Prec@5=88.101 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=19:46 IST
=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.260 Prec@5=88.101 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=19:47 IST
=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.253 Prec@5=88.100 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=19:47 IST
=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.253 Prec@5=88.100 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=19:47 IST
=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.253 Prec@5=88.100 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=19:48 IST
=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.236 Prec@5=88.077 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=19:48 IST
=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.236 Prec@5=88.077 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=19:48 IST
=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.236 Prec@5=88.077 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=19:49 IST
=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.295 Prec@1=68.212 Prec@5=88.053 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=19:49 IST
=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.295 Prec@1=68.212 Prec@5=88.053 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:49 IST
=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.295 Prec@1=68.212 Prec@5=88.053 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:50 IST
=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.297 Prec@1=68.180 Prec@5=88.044 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=19:50 IST
=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.297 Prec@1=68.180 Prec@5=88.044 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=19:50 IST
=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.297 Prec@1=68.180 Prec@5=88.044 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=19:50 IST
=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.297 Prec@1=68.166 Prec@5=88.029 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=19:50 IST
=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.297 Prec@1=68.166 Prec@5=88.029 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:50 IST
=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.297 Prec@1=68.166 Prec@5=88.029 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:51 IST
=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.298 Prec@1=68.155 Prec@5=88.018 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=19:51 IST
=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.298 Prec@1=68.155 Prec@5=88.018 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:51 IST
=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.298 Prec@1=68.155 Prec@5=88.018 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:52 IST
=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.299 Prec@1=68.132 Prec@5=88.003 rate=2.15 Hz, eta=0:03:06, total=0:16:17, wall=19:52 IST
=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.299 Prec@1=68.132 Prec@5=88.003 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=19:52 IST
=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.299 Prec@1=68.132 Prec@5=88.003 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=19:53 IST
=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.300 Prec@1=68.108 Prec@5=87.980 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=19:53 IST
=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.300 Prec@1=68.108 Prec@5=87.980 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=19:53 IST
=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.300 Prec@1=68.108 Prec@5=87.980 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=19:53 IST
=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.301 Prec@1=68.093 Prec@5=87.971 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=19:53 IST
=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.301 Prec@1=68.093 Prec@5=87.971 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=19:53 IST
=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.468 DataTime=0.273 Loss=1.301 Prec@1=68.093 Prec@5=87.971 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=19:54 IST
=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.301 Prec@1=68.075 Prec@5=87.956 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=19:54 IST
=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.301 Prec@1=68.075 Prec@5=87.956 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=19:54 IST
=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.273 Loss=1.301 Prec@1=68.075 Prec@5=87.956 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=19:54 IST
=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.272 Loss=1.301 Prec@1=68.074 Prec@5=87.957 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=19:54 IST
=> training   100.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.467 DataTime=0.272 Loss=1.301 Prec@1=68.074 Prec@5=87.957 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=19:54 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:54 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:54 IST
=> validation 0.00% of 1x98...Epoch=83/150 LR=0.04270 Time=7.011 Loss=0.941 Prec@1=75.781 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=19:54 IST
=> validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=7.011 Loss=0.941 Prec@1=75.781 Prec@5=93.945 rate=5661.49 Hz, eta=0:00:00, total=0:00:00, wall=19:54 IST
** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=7.011 Loss=0.941 Prec@1=75.781 Prec@5=93.945 rate=5661.49 Hz, eta=0:00:00, total=0:00:00, wall=19:55 IST
** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=0.554 Loss=1.398 Prec@1=66.104 Prec@5=87.146 rate=5661.49 Hz, eta=0:00:00, total=0:00:00, wall=19:55 IST
** validation 100.00% of 1x98...Epoch=83/150 LR=0.04270 Time=0.554 Loss=1.398 Prec@1=66.104 Prec@5=87.146 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=19:55 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:55 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:55 IST
=> training   0.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=4.635 DataTime=4.380 Loss=1.205 Prec@1=71.680 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=19:55 IST
=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=4.635 DataTime=4.380 Loss=1.205 Prec@1=71.680 Prec@5=90.234 rate=5371.64 Hz, eta=0:00:00, total=0:00:00, wall=19:55 IST
=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=4.635 DataTime=4.380 Loss=1.205 Prec@1=71.680 Prec@5=90.234 rate=5371.64 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST
=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.502 DataTime=0.310 Loss=1.262 Prec@1=68.970 Prec@5=88.533 rate=5371.64 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST
=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.502 DataTime=0.310 Loss=1.262 Prec@1=68.970 Prec@5=88.533 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=19:56 IST
=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.502 DataTime=0.310 Loss=1.262 Prec@1=68.970 Prec@5=88.533 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=19:57 IST
=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.483 DataTime=0.289 Loss=1.262 Prec@1=68.912 Prec@5=88.514 rate=2.19 Hz, eta=0:18:15, total=0:00:46, wall=19:57 IST
=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.483 DataTime=0.289 Loss=1.262 Prec@1=68.912 Prec@5=88.514 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=19:57 IST
=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.483 DataTime=0.289 Loss=1.262 Prec@1=68.912 Prec@5=88.514 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=19:58 IST
=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.478 DataTime=0.283 Loss=1.261 Prec@1=68.953 Prec@5=88.515 rate=2.18 Hz, eta=0:17:38, total=0:01:32, wall=19:58 IST
=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.478 DataTime=0.283 Loss=1.261 Prec@1=68.953 Prec@5=88.515 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:58 IST
=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.478 DataTime=0.283 Loss=1.261 Prec@1=68.953 Prec@5=88.515 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:58 IST
=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.474 DataTime=0.278 Loss=1.264 Prec@1=68.851 Prec@5=88.466 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=19:58 IST
=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.474 DataTime=0.278 Loss=1.264 Prec@1=68.851 Prec@5=88.466 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=19:58 IST
=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.474 DataTime=0.278 Loss=1.264 Prec@1=68.851 Prec@5=88.466 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=19:59 IST
=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.277 Loss=1.270 Prec@1=68.711 Prec@5=88.383 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=19:59 IST
=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.277 Loss=1.270 Prec@1=68.711 Prec@5=88.383 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=19:59 IST
=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.277 Loss=1.270 Prec@1=68.711 Prec@5=88.383 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=20:00 IST
=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.471 DataTime=0.275 Loss=1.272 Prec@1=68.680 Prec@5=88.374 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=20:00 IST
=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.471 DataTime=0.275 Loss=1.272 Prec@1=68.680 Prec@5=88.374 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=20:00 IST
=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.471 DataTime=0.275 Loss=1.272 Prec@1=68.680 Prec@5=88.374 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=20:01 IST
=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.276 Loss=1.273 Prec@1=68.659 Prec@5=88.347 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=20:01 IST
=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.276 Loss=1.273 Prec@1=68.659 Prec@5=88.347 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=20:01 IST
=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.472 DataTime=0.276 Loss=1.273 Prec@1=68.659 Prec@5=88.347 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=20:01 IST
=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.686 Prec@5=88.355 rate=2.15 Hz, eta=0:13:58, total=0:05:26, wall=20:01 IST
=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.686 Prec@5=88.355 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=20:01 IST
=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.686 Prec@5=88.355 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=20:02 IST
=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.273 Prec@1=68.659 Prec@5=88.339 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=20:02 IST
=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.273 Prec@1=68.659 Prec@5=88.339 rate=2.16 Hz, eta=0:12:23, total=0:06:57, wall=20:02 IST
=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.273 Prec@1=68.659 Prec@5=88.339 rate=2.16 Hz, eta=0:12:23, total=0:06:57, wall=20:03 IST
=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.275 Prec@1=68.622 Prec@5=88.296 rate=2.16 Hz, eta=0:12:23, total=0:06:57, wall=20:03 IST
=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.275 Prec@1=68.622 Prec@5=88.296 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=20:03 IST
=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.275 Prec@1=68.622 Prec@5=88.296 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=20:04 IST
=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.277 Prec@1=68.577 Prec@5=88.267 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=20:04 IST
=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.277 Prec@1=68.577 Prec@5=88.267 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=20:04 IST
=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.277 Prec@1=68.577 Prec@5=88.267 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=20:05 IST
=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.278 Prec@1=68.550 Prec@5=88.258 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=20:05 IST
=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.278 Prec@1=68.550 Prec@5=88.258 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=20:05 IST
=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.278 Prec@1=68.550 Prec@5=88.258 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=20:05 IST
=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.278 Prec@1=68.540 Prec@5=88.254 rate=2.15 Hz, eta=0:10:04, total=0:09:17, wall=20:05 IST
=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.278 Prec@1=68.540 Prec@5=88.254 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=20:05 IST
=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.273 Loss=1.278 Prec@1=68.540 Prec@5=88.254 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=20:06 IST
=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.280 Prec@1=68.520 Prec@5=88.241 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=20:06 IST
=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.280 Prec@1=68.520 Prec@5=88.241 rate=2.15 Hz, eta=0:08:32, total=0:10:52, wall=20:06 IST
=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.469 DataTime=0.274 Loss=1.280 Prec@1=68.520 Prec@5=88.241 rate=2.15 Hz, eta=0:08:32, total=0:10:52, wall=20:07 IST
=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.282 Prec@1=68.483 Prec@5=88.215 rate=2.15 Hz, eta=0:08:32, total=0:10:52, wall=20:07 IST
=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.282 Prec@1=68.483 Prec@5=88.215 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=20:07 IST
=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.282 Prec@1=68.483 Prec@5=88.215 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=20:08 IST
=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.283 Prec@1=68.454 Prec@5=88.206 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=20:08 IST
=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.283 Prec@1=68.454 Prec@5=88.206 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:08 IST
=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.283 Prec@1=68.454 Prec@5=88.206 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:08 IST
=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.467 DataTime=0.273 Loss=1.284 Prec@1=68.422 Prec@5=88.192 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:08 IST
=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.467 DataTime=0.273 Loss=1.284 Prec@1=68.422 Prec@5=88.192 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=20:08 IST
=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.467 DataTime=0.273 Loss=1.284 Prec@1=68.422 Prec@5=88.192 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=20:09 IST
=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.286 Prec@1=68.396 Prec@5=88.168 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=20:09 IST
=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.286 Prec@1=68.396 Prec@5=88.168 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=20:09 IST
=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.286 Prec@1=68.396 Prec@5=88.168 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=20:10 IST
=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.287 Prec@1=68.368 Prec@5=88.161 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=20:10 IST
=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.287 Prec@1=68.368 Prec@5=88.161 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=20:10 IST
=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.287 Prec@1=68.368 Prec@5=88.161 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=20:11 IST
=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.326 Prec@5=88.147 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=20:11 IST
=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.326 Prec@5=88.147 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=20:11 IST
=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.289 Prec@1=68.326 Prec@5=88.147 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=20:12 IST
=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.286 Prec@5=88.114 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=20:12 IST
=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.286 Prec@5=88.114 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=20:12 IST
=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.291 Prec@1=68.286 Prec@5=88.114 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=20:12 IST
=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.261 Prec@5=88.101 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=20:12 IST
=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.261 Prec@5=88.101 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=20:12 IST
=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.292 Prec@1=68.261 Prec@5=88.101 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=20:13 IST
=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.293 Prec@1=68.246 Prec@5=88.087 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=20:13 IST
=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.293 Prec@1=68.246 Prec@5=88.087 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=20:13 IST
=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.293 Prec@1=68.246 Prec@5=88.087 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=20:14 IST
=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.274 Loss=1.294 Prec@1=68.223 Prec@5=88.068 rate=2.15 Hz, eta=0:01:34, total=0:17:52, wall=20:14 IST
=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.274 Loss=1.294 Prec@1=68.223 Prec@5=88.068 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=20:14 IST
=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.274 Loss=1.294 Prec@1=68.223 Prec@5=88.068 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=20:15 IST
=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.220 Prec@5=88.057 rate=2.14 Hz, eta=0:00:47, total=0:18:39, wall=20:15 IST
=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.220 Prec@5=88.057 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=20:15 IST
=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.220 Prec@5=88.057 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=20:15 IST
=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.220 Prec@5=88.057 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=20:15 IST
=> training   100.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.468 DataTime=0.273 Loss=1.294 Prec@1=68.220 Prec@5=88.057 rate=2.15 Hz, eta=0:00:00, total=0:19:26, wall=20:15 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:15 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:15 IST
=> validation 0.00% of 1x98...Epoch=84/150 LR=0.04166 Time=7.143 Loss=0.807 Prec@1=79.883 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=20:15 IST
=> validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=7.143 Loss=0.807 Prec@1=79.883 Prec@5=94.141 rate=8264.74 Hz, eta=0:00:00, total=0:00:00, wall=20:15 IST
** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=7.143 Loss=0.807 Prec@1=79.883 Prec@5=94.141 rate=8264.74 Hz, eta=0:00:00, total=0:00:00, wall=20:16 IST
** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=0.555 Loss=1.375 Prec@1=66.562 Prec@5=87.430 rate=8264.74 Hz, eta=0:00:00, total=0:00:00, wall=20:16 IST
** validation 100.00% of 1x98...Epoch=84/150 LR=0.04166 Time=0.555 Loss=1.375 Prec@1=66.562 Prec@5=87.430 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=20:16 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:16 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:16 IST
=> training   0.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=4.990 DataTime=4.738 Loss=1.211 Prec@1=69.922 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=20:16 IST
=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=4.990 DataTime=4.738 Loss=1.211 Prec@1=69.922 Prec@5=87.891 rate=8537.16 Hz, eta=0:00:00, total=0:00:00, wall=20:16 IST
=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=4.990 DataTime=4.738 Loss=1.211 Prec@1=69.922 Prec@5=87.891 rate=8537.16 Hz, eta=0:00:00, total=0:00:00, wall=20:16 IST
=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.498 DataTime=0.302 Loss=1.250 Prec@1=69.299 Prec@5=88.598 rate=8537.16 Hz, eta=0:00:00, total=0:00:00, wall=20:16 IST
=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.498 DataTime=0.302 Loss=1.250 Prec@1=69.299 Prec@5=88.598 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=20:16 IST
=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.498 DataTime=0.302 Loss=1.250 Prec@1=69.299 Prec@5=88.598 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=20:17 IST
=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.479 DataTime=0.281 Loss=1.253 Prec@1=69.196 Prec@5=88.647 rate=2.23 Hz, eta=0:17:56, total=0:00:45, wall=20:17 IST
=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.479 DataTime=0.281 Loss=1.253 Prec@1=69.196 Prec@5=88.647 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=20:17 IST
=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.479 DataTime=0.281 Loss=1.253 Prec@1=69.196 Prec@5=88.647 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=20:18 IST
=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.472 DataTime=0.275 Loss=1.261 Prec@1=69.059 Prec@5=88.506 rate=2.20 Hz, eta=0:17:24, total=0:01:31, wall=20:18 IST
=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.472 DataTime=0.275 Loss=1.261 Prec@1=69.059 Prec@5=88.506 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=20:18 IST
=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.472 DataTime=0.275 Loss=1.261 Prec@1=69.059 Prec@5=88.506 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=20:19 IST
=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.270 Loss=1.259 Prec@1=69.103 Prec@5=88.527 rate=2.20 Hz, eta=0:16:42, total=0:02:17, wall=20:19 IST
=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.270 Loss=1.259 Prec@1=69.103 Prec@5=88.527 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=20:19 IST
=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.270 Loss=1.259 Prec@1=69.103 Prec@5=88.527 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=20:19 IST
=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.267 Loss=1.262 Prec@1=69.061 Prec@5=88.485 rate=2.20 Hz, eta=0:15:57, total=0:03:02, wall=20:19 IST
=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.267 Loss=1.262 Prec@1=69.061 Prec@5=88.485 rate=2.20 Hz, eta=0:15:11, total=0:03:47, wall=20:19 IST
=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.267 Loss=1.262 Prec@1=69.061 Prec@5=88.485 rate=2.20 Hz, eta=0:15:11, total=0:03:47, wall=20:20 IST
=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.265 Loss=1.265 Prec@1=68.999 Prec@5=88.430 rate=2.20 Hz, eta=0:15:11, total=0:03:47, wall=20:20 IST
=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.265 Loss=1.265 Prec@1=68.999 Prec@5=88.430 rate=2.20 Hz, eta=0:14:23, total=0:04:32, wall=20:20 IST
=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.265 Loss=1.265 Prec@1=68.999 Prec@5=88.430 rate=2.20 Hz, eta=0:14:23, total=0:04:32, wall=20:21 IST
=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.268 Prec@1=68.916 Prec@5=88.400 rate=2.20 Hz, eta=0:14:23, total=0:04:32, wall=20:21 IST
=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.268 Prec@1=68.916 Prec@5=88.400 rate=2.19 Hz, eta=0:13:40, total=0:05:19, wall=20:21 IST
=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.268 Prec@1=68.916 Prec@5=88.400 rate=2.19 Hz, eta=0:13:40, total=0:05:19, wall=20:22 IST
=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.267 Loss=1.270 Prec@1=68.871 Prec@5=88.369 rate=2.19 Hz, eta=0:13:40, total=0:05:19, wall=20:22 IST
=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.267 Loss=1.270 Prec@1=68.871 Prec@5=88.369 rate=2.19 Hz, eta=0:12:57, total=0:06:05, wall=20:22 IST
=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.267 Loss=1.270 Prec@1=68.871 Prec@5=88.369 rate=2.19 Hz, eta=0:12:57, total=0:06:05, wall=20:23 IST
=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.266 Loss=1.271 Prec@1=68.807 Prec@5=88.351 rate=2.19 Hz, eta=0:12:57, total=0:06:05, wall=20:23 IST
=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.266 Loss=1.271 Prec@1=68.807 Prec@5=88.351 rate=2.19 Hz, eta=0:12:11, total=0:06:51, wall=20:23 IST
=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.462 DataTime=0.266 Loss=1.271 Prec@1=68.807 Prec@5=88.351 rate=2.19 Hz, eta=0:12:11, total=0:06:51, wall=20:23 IST
=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.272 Prec@1=68.782 Prec@5=88.350 rate=2.19 Hz, eta=0:12:11, total=0:06:51, wall=20:23 IST
=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.272 Prec@1=68.782 Prec@5=88.350 rate=2.19 Hz, eta=0:11:27, total=0:07:37, wall=20:23 IST
=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.463 DataTime=0.266 Loss=1.272 Prec@1=68.782 Prec@5=88.350 rate=2.19 Hz, eta=0:11:27, total=0:07:37, wall=20:24 IST
=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.269 Loss=1.273 Prec@1=68.739 Prec@5=88.341 rate=2.19 Hz, eta=0:11:27, total=0:07:37, wall=20:24 IST
=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.269 Loss=1.273 Prec@1=68.739 Prec@5=88.341 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=20:24 IST
=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.269 Loss=1.273 Prec@1=68.739 Prec@5=88.341 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=20:25 IST
=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.268 Loss=1.274 Prec@1=68.703 Prec@5=88.307 rate=2.17 Hz, eta=0:10:44, total=0:08:26, wall=20:25 IST
=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.268 Loss=1.274 Prec@1=68.703 Prec@5=88.307 rate=2.17 Hz, eta=0:09:58, total=0:09:12, wall=20:25 IST
=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.464 DataTime=0.268 Loss=1.274 Prec@1=68.703 Prec@5=88.307 rate=2.17 Hz, eta=0:09:58, total=0:09:12, wall=20:26 IST
=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.269 Loss=1.275 Prec@1=68.681 Prec@5=88.304 rate=2.17 Hz, eta=0:09:58, total=0:09:12, wall=20:26 IST
=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.269 Loss=1.275 Prec@1=68.681 Prec@5=88.304 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:26 IST
=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.465 DataTime=0.269 Loss=1.275 Prec@1=68.681 Prec@5=88.304 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:27 IST
=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.276 Prec@1=68.658 Prec@5=88.279 rate=2.17 Hz, eta=0:09:14, total=0:10:00, wall=20:27 IST
=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.276 Prec@1=68.658 Prec@5=88.279 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:27 IST
=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.276 Prec@1=68.658 Prec@5=88.279 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:27 IST
=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.277 Prec@1=68.630 Prec@5=88.263 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=20:27 IST
=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.277 Prec@1=68.630 Prec@5=88.263 rate=2.16 Hz, eta=0:07:44, total=0:11:36, wall=20:27 IST
=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.467 DataTime=0.271 Loss=1.277 Prec@1=68.630 Prec@5=88.263 rate=2.16 Hz, eta=0:07:44, total=0:11:36, wall=20:28 IST
=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.272 Loss=1.279 Prec@1=68.572 Prec@5=88.234 rate=2.16 Hz, eta=0:07:44, total=0:11:36, wall=20:28 IST
=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.272 Loss=1.279 Prec@1=68.572 Prec@5=88.234 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:28 IST
=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.468 DataTime=0.272 Loss=1.279 Prec@1=68.572 Prec@5=88.234 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:29 IST
=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.280 Prec@1=68.548 Prec@5=88.218 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=20:29 IST
=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.280 Prec@1=68.548 Prec@5=88.218 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=20:29 IST
=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.280 Prec@1=68.548 Prec@5=88.218 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=20:30 IST
=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.281 Prec@1=68.527 Prec@5=88.211 rate=2.15 Hz, eta=0:06:13, total=0:13:12, wall=20:30 IST
=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.281 Prec@1=68.527 Prec@5=88.211 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=20:30 IST
=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.469 DataTime=0.273 Loss=1.281 Prec@1=68.527 Prec@5=88.211 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=20:30 IST
=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=20:30 IST
=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=20:30 IST
=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=20:31 IST
=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.283 Prec@1=68.481 Prec@5=88.187 rate=2.14 Hz, eta=0:04:41, total=0:14:47, wall=20:31 IST
=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.283 Prec@1=68.481 Prec@5=88.187 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=20:31 IST
=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.274 Loss=1.283 Prec@1=68.481 Prec@5=88.187 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=20:32 IST
=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.464 Prec@5=88.177 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=20:32 IST
=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.464 Prec@5=88.177 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=20:32 IST
=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.470 DataTime=0.275 Loss=1.283 Prec@1=68.464 Prec@5=88.177 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=20:33 IST
=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.275 Loss=1.284 Prec@1=68.442 Prec@5=88.170 rate=2.14 Hz, eta=0:03:08, total=0:16:23, wall=20:33 IST
=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.275 Loss=1.284 Prec@1=68.442 Prec@5=88.170 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=20:33 IST
=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.275 Loss=1.284 Prec@1=68.442 Prec@5=88.170 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=20:34 IST
=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.285 Prec@1=68.420 Prec@5=88.151 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=20:34 IST
=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.285 Prec@1=68.420 Prec@5=88.151 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=20:34 IST
=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.285 Prec@1=68.420 Prec@5=88.151 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=20:34 IST
=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.286 Prec@1=68.413 Prec@5=88.137 rate=2.13 Hz, eta=0:01:34, total=0:17:58, wall=20:34 IST
=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.286 Prec@1=68.413 Prec@5=88.137 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=20:34 IST
=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.286 Prec@1=68.413 Prec@5=88.137 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=20:35 IST
=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.287 Prec@1=68.381 Prec@5=88.125 rate=2.13 Hz, eta=0:00:47, total=0:18:46, wall=20:35 IST
=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.287 Prec@1=68.381 Prec@5=88.125 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=20:35 IST
=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.287 Prec@1=68.381 Prec@5=88.125 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=20:35 IST
=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.287 Prec@1=68.381 Prec@5=88.125 rate=2.13 Hz, eta=0:00:00, total=0:19:33, wall=20:35 IST
=> training   100.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.471 DataTime=0.276 Loss=1.287 Prec@1=68.381 Prec@5=88.125 rate=2.13 Hz, eta=0:00:00, total=0:19:34, wall=20:35 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> validation 0.00% of 1x98...Epoch=85/150 LR=0.04063 Time=6.352 Loss=0.854 Prec@1=78.125 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.352 Loss=0.854 Prec@1=78.125 Prec@5=93.359 rate=5469.29 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST
** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.352 Loss=0.854 Prec@1=78.125 Prec@5=93.359 rate=5469.29 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST
** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=0.557 Loss=1.390 Prec@1=66.206 Prec@5=87.168 rate=5469.29 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST
** validation 100.00% of 1x98...Epoch=85/150 LR=0.04063 Time=0.557 Loss=1.390 Prec@1=66.206 Prec@5=87.168 rate=2.03 Hz, eta=0:00:00, total=0:00:48, wall=20:36 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST
=> training   0.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.063 DataTime=4.839 Loss=1.342 Prec@1=69.141 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST
=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.063 DataTime=4.839 Loss=1.342 Prec@1=69.141 Prec@5=86.523 rate=8207.95 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST
=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.063 DataTime=4.839 Loss=1.342 Prec@1=69.141 Prec@5=86.523 rate=8207.95 Hz, eta=0:00:00, total=0:00:00, wall=20:37 IST
=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.512 DataTime=0.317 Loss=1.233 Prec@1=69.678 Prec@5=88.850 rate=8207.95 Hz, eta=0:00:00, total=0:00:00, wall=20:37 IST
=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.512 DataTime=0.317 Loss=1.233 Prec@1=69.678 Prec@5=88.850 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=20:37 IST
=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.512 DataTime=0.317 Loss=1.233 Prec@1=69.678 Prec@5=88.850 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=20:38 IST
=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.495 DataTime=0.299 Loss=1.242 Prec@1=69.370 Prec@5=88.750 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=20:38 IST
=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.495 DataTime=0.299 Loss=1.242 Prec@1=69.370 Prec@5=88.750 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=20:38 IST
=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.495 DataTime=0.299 Loss=1.242 Prec@1=69.370 Prec@5=88.750 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=20:39 IST
=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.486 DataTime=0.291 Loss=1.245 Prec@1=69.357 Prec@5=88.691 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=20:39 IST
=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.486 DataTime=0.291 Loss=1.245 Prec@1=69.357 Prec@5=88.691 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=20:39 IST
=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.486 DataTime=0.291 Loss=1.245 Prec@1=69.357 Prec@5=88.691 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=20:39 IST
=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.483 DataTime=0.287 Loss=1.248 Prec@1=69.279 Prec@5=88.671 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=20:39 IST
=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.483 DataTime=0.287 Loss=1.248 Prec@1=69.279 Prec@5=88.671 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=20:39 IST
=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.483 DataTime=0.287 Loss=1.248 Prec@1=69.279 Prec@5=88.671 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=20:40 IST
=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.285 Loss=1.250 Prec@1=69.231 Prec@5=88.645 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=20:40 IST
=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.285 Loss=1.250 Prec@1=69.231 Prec@5=88.645 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=20:40 IST
=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.285 Loss=1.250 Prec@1=69.231 Prec@5=88.645 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=20:41 IST
=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.252 Prec@1=69.168 Prec@5=88.637 rate=2.13 Hz, eta=0:15:40, total=0:03:55, wall=20:41 IST
=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.252 Prec@1=69.168 Prec@5=88.637 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:41 IST
=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.252 Prec@1=69.168 Prec@5=88.637 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:42 IST
=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.255 Prec@1=69.094 Prec@5=88.599 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=20:42 IST
=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.255 Prec@1=69.094 Prec@5=88.599 rate=2.11 Hz, eta=0:14:13, total=0:05:32, wall=20:42 IST
=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.255 Prec@1=69.094 Prec@5=88.599 rate=2.11 Hz, eta=0:14:13, total=0:05:32, wall=20:43 IST
=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.259 Prec@1=69.021 Prec@5=88.545 rate=2.11 Hz, eta=0:14:13, total=0:05:32, wall=20:43 IST
=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.259 Prec@1=69.021 Prec@5=88.545 rate=2.11 Hz, eta=0:13:28, total=0:06:20, wall=20:43 IST
=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.259 Prec@1=69.021 Prec@5=88.545 rate=2.11 Hz, eta=0:13:28, total=0:06:20, wall=20:43 IST
=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.263 Prec@1=68.957 Prec@5=88.486 rate=2.11 Hz, eta=0:13:28, total=0:06:20, wall=20:43 IST
=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.263 Prec@1=68.957 Prec@5=88.486 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=20:43 IST
=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.263 Prec@1=68.957 Prec@5=88.486 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=20:44 IST
=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.286 Loss=1.265 Prec@1=68.905 Prec@5=88.451 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=20:44 IST
=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.286 Loss=1.265 Prec@1=68.905 Prec@5=88.451 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=20:44 IST
=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.286 Loss=1.265 Prec@1=68.905 Prec@5=88.451 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=20:45 IST
=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.267 Prec@1=68.829 Prec@5=88.425 rate=2.10 Hz, eta=0:11:54, total=0:07:56, wall=20:45 IST
=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.267 Prec@1=68.829 Prec@5=88.425 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=20:45 IST
=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.267 Prec@1=68.829 Prec@5=88.425 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=20:46 IST
=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.268 Prec@1=68.808 Prec@5=88.403 rate=2.10 Hz, eta=0:11:08, total=0:08:44, wall=20:46 IST
=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.268 Prec@1=68.808 Prec@5=88.403 rate=2.10 Hz, eta=0:10:21, total=0:09:33, wall=20:46 IST
=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.268 Prec@1=68.808 Prec@5=88.403 rate=2.10 Hz, eta=0:10:21, total=0:09:33, wall=20:47 IST
=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.777 Prec@5=88.372 rate=2.10 Hz, eta=0:10:21, total=0:09:33, wall=20:47 IST
=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.777 Prec@5=88.372 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=20:47 IST
=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.777 Prec@5=88.372 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=20:47 IST
=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.773 Prec@5=88.365 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=20:47 IST
=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.773 Prec@5=88.365 rate=2.09 Hz, eta=0:08:46, total=0:11:08, wall=20:47 IST
=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.481 DataTime=0.287 Loss=1.270 Prec@1=68.773 Prec@5=88.365 rate=2.09 Hz, eta=0:08:46, total=0:11:08, wall=20:48 IST
=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.271 Prec@1=68.748 Prec@5=88.339 rate=2.09 Hz, eta=0:08:46, total=0:11:08, wall=20:48 IST
=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.271 Prec@1=68.748 Prec@5=88.339 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=20:48 IST
=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.271 Prec@1=68.748 Prec@5=88.339 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=20:49 IST
=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.273 Prec@1=68.728 Prec@5=88.321 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=20:49 IST
=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.273 Prec@1=68.728 Prec@5=88.321 rate=2.10 Hz, eta=0:07:10, total=0:12:44, wall=20:49 IST
=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.273 Prec@1=68.728 Prec@5=88.321 rate=2.10 Hz, eta=0:07:10, total=0:12:44, wall=20:50 IST
=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.274 Prec@1=68.700 Prec@5=88.302 rate=2.10 Hz, eta=0:07:10, total=0:12:44, wall=20:50 IST
=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.274 Prec@1=68.700 Prec@5=88.302 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=20:50 IST
=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.480 DataTime=0.286 Loss=1.274 Prec@1=68.700 Prec@5=88.302 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=20:51 IST
=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.276 Prec@1=68.672 Prec@5=88.282 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=20:51 IST
=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.276 Prec@1=68.672 Prec@5=88.282 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=20:51 IST
=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.276 Prec@1=68.672 Prec@5=88.282 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=20:51 IST
=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.277 Prec@1=68.639 Prec@5=88.270 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=20:51 IST
=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.277 Prec@1=68.639 Prec@5=88.270 rate=2.10 Hz, eta=0:04:46, total=0:15:06, wall=20:51 IST
=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.277 Prec@1=68.639 Prec@5=88.270 rate=2.10 Hz, eta=0:04:46, total=0:15:06, wall=20:52 IST
=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.278 Prec@1=68.602 Prec@5=88.248 rate=2.10 Hz, eta=0:04:46, total=0:15:06, wall=20:52 IST
=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.278 Prec@1=68.602 Prec@5=88.248 rate=2.10 Hz, eta=0:03:59, total=0:15:54, wall=20:52 IST
=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.278 Prec@1=68.602 Prec@5=88.248 rate=2.10 Hz, eta=0:03:59, total=0:15:54, wall=20:53 IST
=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.279 Prec@1=68.578 Prec@5=88.242 rate=2.10 Hz, eta=0:03:59, total=0:15:54, wall=20:53 IST
=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.279 Prec@1=68.578 Prec@5=88.242 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=20:53 IST
=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.285 Loss=1.279 Prec@1=68.578 Prec@5=88.242 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=20:54 IST
=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.284 Loss=1.280 Prec@1=68.561 Prec@5=88.225 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=20:54 IST
=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.284 Loss=1.280 Prec@1=68.561 Prec@5=88.225 rate=2.10 Hz, eta=0:02:23, total=0:17:29, wall=20:54 IST
=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.479 DataTime=0.284 Loss=1.280 Prec@1=68.561 Prec@5=88.225 rate=2.10 Hz, eta=0:02:23, total=0:17:29, wall=20:55 IST
=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.280 Prec@1=68.550 Prec@5=88.219 rate=2.10 Hz, eta=0:02:23, total=0:17:29, wall=20:55 IST
=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.280 Prec@1=68.550 Prec@5=88.219 rate=2.10 Hz, eta=0:01:36, total=0:18:15, wall=20:55 IST
=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.280 Prec@1=68.550 Prec@5=88.219 rate=2.10 Hz, eta=0:01:36, total=0:18:15, wall=20:55 IST
=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.281 Prec@1=68.525 Prec@5=88.206 rate=2.10 Hz, eta=0:01:36, total=0:18:15, wall=20:55 IST
=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.281 Prec@1=68.525 Prec@5=88.206 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=20:55 IST
=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.284 Loss=1.281 Prec@1=68.525 Prec@5=88.206 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=20:56 IST
=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.283 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=20:56 IST
=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.283 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=20:56 IST
=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.283 Loss=1.282 Prec@1=68.504 Prec@5=88.200 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=20:56 IST
=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.283 Loss=1.282 Prec@1=68.503 Prec@5=88.199 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=20:56 IST
=> training   100.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.478 DataTime=0.283 Loss=1.282 Prec@1=68.503 Prec@5=88.199 rate=2.10 Hz, eta=0:00:00, total=0:19:50, wall=20:56 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> validation 0.00% of 1x98...Epoch=86/150 LR=0.03960 Time=7.091 Loss=0.921 Prec@1=76.367 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=7.091 Loss=0.921 Prec@1=76.367 Prec@5=93.164 rate=7776.78 Hz, eta=0:00:00, total=0:00:00, wall=20:56 IST
** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=7.091 Loss=0.921 Prec@1=76.367 Prec@5=93.164 rate=7776.78 Hz, eta=0:00:00, total=0:00:00, wall=20:57 IST
** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=0.561 Loss=1.396 Prec@1=65.966 Prec@5=87.080 rate=7776.78 Hz, eta=0:00:00, total=0:00:00, wall=20:57 IST
** validation 100.00% of 1x98...Epoch=86/150 LR=0.03960 Time=0.561 Loss=1.396 Prec@1=65.966 Prec@5=87.080 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=20:57 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:57 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:57 IST
=> training   0.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.014 DataTime=4.770 Loss=1.262 Prec@1=68.945 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=20:57 IST
=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.014 DataTime=4.770 Loss=1.262 Prec@1=68.945 Prec@5=88.086 rate=7937.01 Hz, eta=0:00:00, total=0:00:00, wall=20:57 IST
=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.014 DataTime=4.770 Loss=1.262 Prec@1=68.945 Prec@5=88.086 rate=7937.01 Hz, eta=0:00:00, total=0:00:00, wall=20:58 IST
=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.499 DataTime=0.304 Loss=1.253 Prec@1=69.127 Prec@5=88.682 rate=7937.01 Hz, eta=0:00:00, total=0:00:00, wall=20:58 IST
=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.499 DataTime=0.304 Loss=1.253 Prec@1=69.127 Prec@5=88.682 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=20:58 IST
=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.499 DataTime=0.304 Loss=1.253 Prec@1=69.127 Prec@5=88.682 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=20:59 IST
=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.479 DataTime=0.284 Loss=1.245 Prec@1=69.296 Prec@5=88.709 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=20:59 IST
=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.479 DataTime=0.284 Loss=1.245 Prec@1=69.296 Prec@5=88.709 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=20:59 IST
=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.479 DataTime=0.284 Loss=1.245 Prec@1=69.296 Prec@5=88.709 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=20:59 IST
=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.474 DataTime=0.280 Loss=1.246 Prec@1=69.311 Prec@5=88.689 rate=2.20 Hz, eta=0:17:25, total=0:01:31, wall=20:59 IST
=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.474 DataTime=0.280 Loss=1.246 Prec@1=69.311 Prec@5=88.689 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=20:59 IST
=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.474 DataTime=0.280 Loss=1.246 Prec@1=69.311 Prec@5=88.689 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=21:00 IST
=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.248 Prec@1=69.307 Prec@5=88.666 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=21:00 IST
=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.248 Prec@1=69.307 Prec@5=88.666 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:00 IST
=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.248 Prec@1=69.307 Prec@5=88.666 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:01 IST
=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.201 Prec@5=88.617 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=21:01 IST
=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.201 Prec@5=88.617 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=21:01 IST
=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.201 Prec@5=88.617 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=21:02 IST
=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.251 Prec@1=69.196 Prec@5=88.629 rate=2.16 Hz, eta=0:15:26, total=0:03:51, wall=21:02 IST
=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.251 Prec@1=69.196 Prec@5=88.629 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=21:02 IST
=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.251 Prec@1=69.196 Prec@5=88.629 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=21:03 IST
=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.252 Prec@1=69.174 Prec@5=88.604 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=21:03 IST
=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.252 Prec@1=69.174 Prec@5=88.604 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=21:03 IST
=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.252 Prec@1=69.174 Prec@5=88.604 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=21:03 IST
=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.140 Prec@5=88.560 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=21:03 IST
=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.140 Prec@5=88.560 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=21:03 IST
=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.140 Prec@5=88.560 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=21:04 IST
=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.255 Prec@1=69.113 Prec@5=88.565 rate=2.14 Hz, eta=0:13:13, total=0:06:13, wall=21:04 IST
=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.255 Prec@1=69.113 Prec@5=88.565 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=21:04 IST
=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.255 Prec@1=69.113 Prec@5=88.565 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=21:05 IST
=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.064 Prec@5=88.531 rate=2.15 Hz, eta=0:12:26, total=0:07:00, wall=21:05 IST
=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.064 Prec@5=88.531 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=21:05 IST
=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.064 Prec@5=88.531 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=21:06 IST
=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.259 Prec@1=69.013 Prec@5=88.513 rate=2.14 Hz, eta=0:11:41, total=0:07:47, wall=21:06 IST
=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.259 Prec@1=69.013 Prec@5=88.513 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=21:06 IST
=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.472 DataTime=0.277 Loss=1.259 Prec@1=69.013 Prec@5=88.513 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=21:07 IST
=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.260 Prec@1=68.989 Prec@5=88.498 rate=2.14 Hz, eta=0:10:55, total=0:08:34, wall=21:07 IST
=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.260 Prec@1=68.989 Prec@5=88.498 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=21:07 IST
=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.260 Prec@1=68.989 Prec@5=88.498 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=21:07 IST
=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.262 Prec@1=68.938 Prec@5=88.468 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=21:07 IST
=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.262 Prec@1=68.938 Prec@5=88.468 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=21:07 IST
=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.262 Prec@1=68.938 Prec@5=88.468 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=21:08 IST
=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.263 Prec@1=68.923 Prec@5=88.465 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=21:08 IST
=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.263 Prec@1=68.923 Prec@5=88.465 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=21:08 IST
=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.263 Prec@1=68.923 Prec@5=88.465 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=21:09 IST
=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.264 Prec@1=68.906 Prec@5=88.459 rate=2.14 Hz, eta=0:08:35, total=0:10:55, wall=21:09 IST
=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.264 Prec@1=68.906 Prec@5=88.459 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=21:09 IST
=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.277 Loss=1.264 Prec@1=68.906 Prec@5=88.459 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=21:10 IST
=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.265 Prec@1=68.893 Prec@5=88.438 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=21:10 IST
=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.265 Prec@1=68.893 Prec@5=88.438 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=21:10 IST
=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.471 DataTime=0.276 Loss=1.265 Prec@1=68.893 Prec@5=88.438 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=21:10 IST
=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.850 Prec@5=88.419 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=21:10 IST
=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.850 Prec@5=88.419 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=21:10 IST
=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.850 Prec@5=88.419 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=21:11 IST
=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.833 Prec@5=88.410 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=21:11 IST
=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.833 Prec@5=88.410 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:11 IST
=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.267 Prec@1=68.833 Prec@5=88.410 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:12 IST
=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.268 Prec@1=68.808 Prec@5=88.392 rate=2.14 Hz, eta=0:05:27, total=0:14:01, wall=21:12 IST
=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.268 Prec@1=68.808 Prec@5=88.392 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=21:12 IST
=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.268 Prec@1=68.808 Prec@5=88.392 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=21:13 IST
=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.796 Prec@5=88.386 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=21:13 IST
=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.796 Prec@5=88.386 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=21:13 IST
=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.796 Prec@5=88.386 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=21:14 IST
=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.768 Prec@5=88.374 rate=2.14 Hz, eta=0:03:54, total=0:15:35, wall=21:14 IST
=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.768 Prec@5=88.374 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=21:14 IST
=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.269 Prec@1=68.768 Prec@5=88.374 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=21:14 IST
=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.270 Prec@1=68.761 Prec@5=88.370 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=21:14 IST
=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.270 Prec@1=68.761 Prec@5=88.370 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=21:14 IST
=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.276 Loss=1.270 Prec@1=68.761 Prec@5=88.370 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=21:15 IST
=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.271 Prec@1=68.736 Prec@5=88.353 rate=2.14 Hz, eta=0:02:21, total=0:17:09, wall=21:15 IST
=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.271 Prec@1=68.736 Prec@5=88.353 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=21:15 IST
=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.271 Prec@1=68.736 Prec@5=88.353 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=21:16 IST
=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.717 Prec@5=88.331 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=21:16 IST
=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.717 Prec@5=88.331 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=21:16 IST
=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.470 DataTime=0.275 Loss=1.272 Prec@1=68.717 Prec@5=88.331 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=21:17 IST
=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.273 Prec@1=68.706 Prec@5=88.323 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=21:17 IST
=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.273 Prec@1=68.706 Prec@5=88.323 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=21:17 IST
=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.273 Prec@1=68.706 Prec@5=88.323 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=21:17 IST
=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.273 Prec@1=68.706 Prec@5=88.323 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=21:17 IST
=> training   100.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.469 DataTime=0.275 Loss=1.273 Prec@1=68.706 Prec@5=88.323 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=21:17 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:17 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:17 IST
=> validation 0.00% of 1x98...Epoch=87/150 LR=0.03858 Time=6.253 Loss=0.935 Prec@1=77.148 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=21:17 IST
=> validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=6.253 Loss=0.935 Prec@1=77.148 Prec@5=92.188 rate=5067.68 Hz, eta=0:00:00, total=0:00:00, wall=21:17 IST
** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=6.253 Loss=0.935 Prec@1=77.148 Prec@5=92.188 rate=5067.68 Hz, eta=0:00:00, total=0:00:00, wall=21:18 IST
** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=0.545 Loss=1.362 Prec@1=66.840 Prec@5=87.520 rate=5067.68 Hz, eta=0:00:00, total=0:00:00, wall=21:18 IST
** validation 100.00% of 1x98...Epoch=87/150 LR=0.03858 Time=0.545 Loss=1.362 Prec@1=66.840 Prec@5=87.520 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=21:18 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:18 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:18 IST
=> training   0.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.683 DataTime=4.274 Loss=1.300 Prec@1=68.945 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=21:18 IST
=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.683 DataTime=4.274 Loss=1.300 Prec@1=68.945 Prec@5=89.453 rate=6150.21 Hz, eta=0:00:00, total=0:00:00, wall=21:18 IST
=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.683 DataTime=4.274 Loss=1.300 Prec@1=68.945 Prec@5=89.453 rate=6150.21 Hz, eta=0:00:00, total=0:00:00, wall=21:18 IST
=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.504 DataTime=0.307 Loss=1.239 Prec@1=69.346 Prec@5=88.927 rate=6150.21 Hz, eta=0:00:00, total=0:00:00, wall=21:18 IST
=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.504 DataTime=0.307 Loss=1.239 Prec@1=69.346 Prec@5=88.927 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=21:18 IST
=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.504 DataTime=0.307 Loss=1.239 Prec@1=69.346 Prec@5=88.927 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=21:19 IST
=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.480 DataTime=0.283 Loss=1.238 Prec@1=69.415 Prec@5=88.867 rate=2.18 Hz, eta=0:18:21, total=0:00:46, wall=21:19 IST
=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.480 DataTime=0.283 Loss=1.238 Prec@1=69.415 Prec@5=88.867 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=21:19 IST
=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.480 DataTime=0.283 Loss=1.238 Prec@1=69.415 Prec@5=88.867 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=21:20 IST
=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.474 DataTime=0.277 Loss=1.238 Prec@1=69.394 Prec@5=88.855 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=21:20 IST
=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.474 DataTime=0.277 Loss=1.238 Prec@1=69.394 Prec@5=88.855 rate=2.18 Hz, eta=0:16:49, total=0:02:18, wall=21:20 IST
=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.474 DataTime=0.277 Loss=1.238 Prec@1=69.394 Prec@5=88.855 rate=2.18 Hz, eta=0:16:49, total=0:02:18, wall=21:21 IST
=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.473 DataTime=0.276 Loss=1.240 Prec@1=69.414 Prec@5=88.792 rate=2.18 Hz, eta=0:16:49, total=0:02:18, wall=21:21 IST
=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.473 DataTime=0.276 Loss=1.240 Prec@1=69.414 Prec@5=88.792 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:21 IST
=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.473 DataTime=0.276 Loss=1.240 Prec@1=69.414 Prec@5=88.792 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:22 IST
=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.241 Prec@1=69.390 Prec@5=88.781 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:22 IST
=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.241 Prec@1=69.390 Prec@5=88.781 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=21:22 IST
=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.241 Prec@1=69.390 Prec@5=88.781 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=21:22 IST
=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.243 Prec@1=69.376 Prec@5=88.757 rate=2.17 Hz, eta=0:15:22, total=0:03:50, wall=21:22 IST
=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.243 Prec@1=69.376 Prec@5=88.757 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=21:22 IST
=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.470 DataTime=0.274 Loss=1.243 Prec@1=69.376 Prec@5=88.757 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=21:23 IST
=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.244 Prec@1=69.334 Prec@5=88.759 rate=2.16 Hz, eta=0:14:39, total=0:04:37, wall=21:23 IST
=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.244 Prec@1=69.334 Prec@5=88.759 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:23 IST
=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.244 Prec@1=69.334 Prec@5=88.759 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:24 IST
=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.247 Prec@1=69.237 Prec@5=88.728 rate=2.16 Hz, eta=0:13:53, total=0:05:24, wall=21:24 IST
=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.247 Prec@1=69.237 Prec@5=88.728 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=21:24 IST
=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.247 Prec@1=69.237 Prec@5=88.728 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=21:25 IST
=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.248 Prec@1=69.213 Prec@5=88.698 rate=2.16 Hz, eta=0:13:08, total=0:06:10, wall=21:25 IST
=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.248 Prec@1=69.213 Prec@5=88.698 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:25 IST
=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.469 DataTime=0.273 Loss=1.248 Prec@1=69.213 Prec@5=88.698 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:25 IST
=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.468 DataTime=0.272 Loss=1.250 Prec@1=69.185 Prec@5=88.666 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=21:25 IST
=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.468 DataTime=0.272 Loss=1.250 Prec@1=69.185 Prec@5=88.666 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:25 IST
=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.468 DataTime=0.272 Loss=1.250 Prec@1=69.185 Prec@5=88.666 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:26 IST
=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.251 Prec@1=69.161 Prec@5=88.649 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=21:26 IST
=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.251 Prec@1=69.161 Prec@5=88.649 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=21:26 IST
=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.251 Prec@1=69.161 Prec@5=88.649 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=21:27 IST
=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.465 DataTime=0.270 Loss=1.252 Prec@1=69.154 Prec@5=88.632 rate=2.16 Hz, eta=0:10:48, total=0:08:29, wall=21:27 IST
=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.465 DataTime=0.270 Loss=1.252 Prec@1=69.154 Prec@5=88.632 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=21:27 IST
=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.465 DataTime=0.270 Loss=1.252 Prec@1=69.154 Prec@5=88.632 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=21:28 IST
=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.253 Prec@1=69.111 Prec@5=88.610 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=21:28 IST
=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.253 Prec@1=69.111 Prec@5=88.610 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=21:28 IST
=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.253 Prec@1=69.111 Prec@5=88.610 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=21:29 IST
=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.254 Prec@1=69.091 Prec@5=88.601 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=21:29 IST
=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.254 Prec@1=69.091 Prec@5=88.601 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=21:29 IST
=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.254 Prec@1=69.091 Prec@5=88.601 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=21:29 IST
=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.255 Prec@1=69.060 Prec@5=88.584 rate=2.16 Hz, eta=0:08:31, total=0:10:49, wall=21:29 IST
=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.255 Prec@1=69.060 Prec@5=88.584 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=21:29 IST
=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.255 Prec@1=69.060 Prec@5=88.584 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=21:30 IST
=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.257 Prec@1=69.035 Prec@5=88.565 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=21:30 IST
=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.257 Prec@1=69.035 Prec@5=88.565 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=21:30 IST
=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.257 Prec@1=69.035 Prec@5=88.565 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=21:31 IST
=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.258 Prec@1=69.006 Prec@5=88.544 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=21:31 IST
=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.258 Prec@1=69.006 Prec@5=88.544 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=21:31 IST
=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.258 Prec@1=69.006 Prec@5=88.544 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=21:32 IST
=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.260 Prec@1=68.971 Prec@5=88.521 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=21:32 IST
=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.260 Prec@1=68.971 Prec@5=88.521 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=21:32 IST
=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.260 Prec@1=68.971 Prec@5=88.521 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=21:32 IST
=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.260 Prec@1=68.951 Prec@5=88.511 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=21:32 IST
=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.260 Prec@1=68.951 Prec@5=88.511 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:32 IST
=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.260 Prec@1=68.951 Prec@5=88.511 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:33 IST
=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.261 Prec@1=68.939 Prec@5=88.503 rate=2.15 Hz, eta=0:04:39, total=0:14:42, wall=21:33 IST
=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.261 Prec@1=68.939 Prec@5=88.503 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=21:33 IST
=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.261 Prec@1=68.939 Prec@5=88.503 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=21:34 IST
=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.262 Prec@1=68.913 Prec@5=88.488 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=21:34 IST
=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.262 Prec@1=68.913 Prec@5=88.488 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=21:34 IST
=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.273 Loss=1.262 Prec@1=68.913 Prec@5=88.488 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=21:35 IST
=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.905 Prec@5=88.472 rate=2.15 Hz, eta=0:03:06, total=0:16:16, wall=21:35 IST
=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.905 Prec@5=88.472 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=21:35 IST
=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.905 Prec@5=88.472 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=21:36 IST
=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.890 Prec@5=88.464 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=21:36 IST
=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.890 Prec@5=88.464 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=21:36 IST
=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.263 Prec@1=68.890 Prec@5=88.464 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=21:36 IST
=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.264 Prec@1=68.866 Prec@5=88.454 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=21:36 IST
=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.264 Prec@1=68.866 Prec@5=88.454 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:36 IST
=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.264 Prec@1=68.866 Prec@5=88.454 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:37 IST
=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.265 Prec@1=68.844 Prec@5=88.443 rate=2.15 Hz, eta=0:00:47, total=0:18:35, wall=21:37 IST
=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.265 Prec@1=68.844 Prec@5=88.443 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=21:37 IST
=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.467 DataTime=0.272 Loss=1.265 Prec@1=68.844 Prec@5=88.443 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=21:37 IST
=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.466 DataTime=0.272 Loss=1.265 Prec@1=68.844 Prec@5=88.443 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=21:37 IST
=> training   100.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.466 DataTime=0.272 Loss=1.265 Prec@1=68.844 Prec@5=88.443 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=21:37 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:37 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:37 IST
=> validation 0.00% of 1x98...Epoch=88/150 LR=0.03757 Time=6.349 Loss=0.887 Prec@1=77.734 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=21:37 IST
=> validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=6.349 Loss=0.887 Prec@1=77.734 Prec@5=94.336 rate=8554.02 Hz, eta=0:00:00, total=0:00:00, wall=21:37 IST
** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=6.349 Loss=0.887 Prec@1=77.734 Prec@5=94.336 rate=8554.02 Hz, eta=0:00:00, total=0:00:00, wall=21:38 IST
** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=0.544 Loss=1.363 Prec@1=66.720 Prec@5=87.726 rate=8554.02 Hz, eta=0:00:00, total=0:00:00, wall=21:38 IST
** validation 100.00% of 1x98...Epoch=88/150 LR=0.03757 Time=0.544 Loss=1.363 Prec@1=66.720 Prec@5=87.726 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=21:38 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:38 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:38 IST
=> training   0.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=4.827 DataTime=4.612 Loss=1.278 Prec@1=69.922 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=21:38 IST
=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=4.827 DataTime=4.612 Loss=1.278 Prec@1=69.922 Prec@5=88.281 rate=6721.87 Hz, eta=0:00:00, total=0:00:00, wall=21:38 IST
=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=4.827 DataTime=4.612 Loss=1.278 Prec@1=69.922 Prec@5=88.281 rate=6721.87 Hz, eta=0:00:00, total=0:00:00, wall=21:39 IST
=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.500 DataTime=0.305 Loss=1.213 Prec@1=69.858 Prec@5=89.209 rate=6721.87 Hz, eta=0:00:00, total=0:00:00, wall=21:39 IST
=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.500 DataTime=0.305 Loss=1.213 Prec@1=69.858 Prec@5=89.209 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=21:39 IST
=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.500 DataTime=0.305 Loss=1.213 Prec@1=69.858 Prec@5=89.209 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=21:40 IST
=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.481 DataTime=0.285 Loss=1.221 Prec@1=69.654 Prec@5=89.107 rate=2.21 Hz, eta=0:18:06, total=0:00:45, wall=21:40 IST
=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.481 DataTime=0.285 Loss=1.221 Prec@1=69.654 Prec@5=89.107 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=21:40 IST
=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.481 DataTime=0.285 Loss=1.221 Prec@1=69.654 Prec@5=89.107 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=21:40 IST
=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.278 Loss=1.224 Prec@1=69.679 Prec@5=89.072 rate=2.19 Hz, eta=0:17:31, total=0:01:31, wall=21:40 IST
=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.278 Loss=1.224 Prec@1=69.679 Prec@5=89.072 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=21:40 IST
=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.278 Loss=1.224 Prec@1=69.679 Prec@5=89.072 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=21:41 IST
=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.277 Loss=1.227 Prec@1=69.688 Prec@5=88.954 rate=2.18 Hz, eta=0:16:51, total=0:02:18, wall=21:41 IST
=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.277 Loss=1.227 Prec@1=69.688 Prec@5=88.954 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:41 IST
=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.277 Loss=1.227 Prec@1=69.688 Prec@5=88.954 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:42 IST
=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.279 Loss=1.229 Prec@1=69.601 Prec@5=88.949 rate=2.17 Hz, eta=0:16:09, total=0:03:04, wall=21:42 IST
=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.279 Loss=1.229 Prec@1=69.601 Prec@5=88.949 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:42 IST
=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.279 Loss=1.229 Prec@1=69.601 Prec@5=88.949 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:43 IST
=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.477 DataTime=0.281 Loss=1.233 Prec@1=69.543 Prec@5=88.900 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=21:43 IST
=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.477 DataTime=0.281 Loss=1.233 Prec@1=69.543 Prec@5=88.900 rate=2.13 Hz, eta=0:14:51, total=0:04:41, wall=21:43 IST
=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.477 DataTime=0.281 Loss=1.233 Prec@1=69.543 Prec@5=88.900 rate=2.13 Hz, eta=0:14:51, total=0:04:41, wall=21:44 IST
=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.476 DataTime=0.281 Loss=1.235 Prec@1=69.473 Prec@5=88.860 rate=2.13 Hz, eta=0:14:51, total=0:04:41, wall=21:44 IST
=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.476 DataTime=0.281 Loss=1.235 Prec@1=69.473 Prec@5=88.860 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=21:44 IST
=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.476 DataTime=0.281 Loss=1.235 Prec@1=69.473 Prec@5=88.860 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=21:44 IST
=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.237 Prec@1=69.425 Prec@5=88.836 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=21:44 IST
=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.237 Prec@1=69.425 Prec@5=88.836 rate=2.13 Hz, eta=0:13:18, total=0:06:16, wall=21:44 IST
=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.237 Prec@1=69.425 Prec@5=88.836 rate=2.13 Hz, eta=0:13:18, total=0:06:16, wall=21:45 IST
=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.240 Prec@1=69.359 Prec@5=88.778 rate=2.13 Hz, eta=0:13:18, total=0:06:16, wall=21:45 IST
=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.240 Prec@1=69.359 Prec@5=88.778 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=21:45 IST
=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.475 DataTime=0.280 Loss=1.240 Prec@1=69.359 Prec@5=88.778 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=21:46 IST
=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.242 Prec@1=69.313 Prec@5=88.770 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=21:46 IST
=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.242 Prec@1=69.313 Prec@5=88.770 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=21:46 IST
=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.242 Prec@1=69.313 Prec@5=88.770 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=21:47 IST
=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.243 Prec@1=69.292 Prec@5=88.734 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=21:47 IST
=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.243 Prec@1=69.292 Prec@5=88.734 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=21:47 IST
=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.243 Prec@1=69.292 Prec@5=88.734 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=21:47 IST
=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.244 Prec@1=69.277 Prec@5=88.730 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=21:47 IST
=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.244 Prec@1=69.277 Prec@5=88.730 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=21:47 IST
=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.244 Prec@1=69.277 Prec@5=88.730 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=21:48 IST
=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.245 Prec@1=69.270 Prec@5=88.719 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=21:48 IST
=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.245 Prec@1=69.270 Prec@5=88.719 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=21:48 IST
=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.245 Prec@1=69.270 Prec@5=88.719 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=21:49 IST
=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.246 Prec@1=69.242 Prec@5=88.692 rate=2.13 Hz, eta=0:09:24, total=0:10:10, wall=21:49 IST
=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.246 Prec@1=69.242 Prec@5=88.692 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=21:49 IST
=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.278 Loss=1.246 Prec@1=69.242 Prec@5=88.692 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=21:50 IST
=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.248 Prec@1=69.206 Prec@5=88.675 rate=2.13 Hz, eta=0:08:38, total=0:10:58, wall=21:50 IST
=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.248 Prec@1=69.206 Prec@5=88.675 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=21:50 IST
=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.248 Prec@1=69.206 Prec@5=88.675 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=21:51 IST
=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.249 Prec@1=69.185 Prec@5=88.669 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=21:51 IST
=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.249 Prec@1=69.185 Prec@5=88.669 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=21:51 IST
=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.249 Prec@1=69.185 Prec@5=88.669 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=21:51 IST
=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.250 Prec@1=69.152 Prec@5=88.633 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=21:51 IST
=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.250 Prec@1=69.152 Prec@5=88.633 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=21:51 IST
=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.474 DataTime=0.279 Loss=1.250 Prec@1=69.152 Prec@5=88.633 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=21:52 IST
=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.132 Prec@5=88.612 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=21:52 IST
=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.132 Prec@5=88.612 rate=2.13 Hz, eta=0:05:30, total=0:14:07, wall=21:52 IST
=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.251 Prec@1=69.132 Prec@5=88.612 rate=2.13 Hz, eta=0:05:30, total=0:14:07, wall=21:53 IST
=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.252 Prec@1=69.113 Prec@5=88.599 rate=2.13 Hz, eta=0:05:30, total=0:14:07, wall=21:53 IST
=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.252 Prec@1=69.113 Prec@5=88.599 rate=2.13 Hz, eta=0:04:43, total=0:14:54, wall=21:53 IST
=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.252 Prec@1=69.113 Prec@5=88.599 rate=2.13 Hz, eta=0:04:43, total=0:14:54, wall=21:54 IST
=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.253 Prec@1=69.101 Prec@5=88.588 rate=2.13 Hz, eta=0:04:43, total=0:14:54, wall=21:54 IST
=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.253 Prec@1=69.101 Prec@5=88.588 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=21:54 IST
=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.253 Prec@1=69.101 Prec@5=88.588 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=21:55 IST
=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.088 Prec@5=88.571 rate=2.12 Hz, eta=0:03:56, total=0:15:41, wall=21:55 IST
=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.088 Prec@5=88.571 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=21:55 IST
=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.473 DataTime=0.278 Loss=1.254 Prec@1=69.088 Prec@5=88.571 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=21:55 IST
=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.255 Prec@1=69.062 Prec@5=88.556 rate=2.13 Hz, eta=0:03:09, total=0:16:28, wall=21:55 IST
=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.255 Prec@1=69.062 Prec@5=88.556 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=21:55 IST
=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.255 Prec@1=69.062 Prec@5=88.556 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=21:56 IST
=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.256 Prec@1=69.038 Prec@5=88.539 rate=2.13 Hz, eta=0:02:22, total=0:17:15, wall=21:56 IST
=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.256 Prec@1=69.038 Prec@5=88.539 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=21:56 IST
=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.278 Loss=1.256 Prec@1=69.038 Prec@5=88.539 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=21:57 IST
=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.017 Prec@5=88.529 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=21:57 IST
=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.017 Prec@5=88.529 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=21:57 IST
=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.257 Prec@1=69.017 Prec@5=88.529 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=21:58 IST
=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.258 Prec@1=68.992 Prec@5=88.511 rate=2.13 Hz, eta=0:00:47, total=0:18:48, wall=21:58 IST
=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.258 Prec@1=68.992 Prec@5=88.511 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=21:58 IST
=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.258 Prec@1=68.992 Prec@5=88.511 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=21:58 IST
=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.258 Prec@1=68.990 Prec@5=88.510 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=21:58 IST
=> training   100.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.472 DataTime=0.277 Loss=1.258 Prec@1=68.990 Prec@5=88.510 rate=2.13 Hz, eta=0:00:00, total=0:19:35, wall=21:58 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> validation 0.00% of 1x98...Epoch=89/150 LR=0.03655 Time=6.831 Loss=0.888 Prec@1=75.781 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=6.831 Loss=0.888 Prec@1=75.781 Prec@5=93.750 rate=3057.34 Hz, eta=0:00:00, total=0:00:00, wall=21:58 IST
** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=6.831 Loss=0.888 Prec@1=75.781 Prec@5=93.750 rate=3057.34 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=0.555 Loss=1.361 Prec@1=66.810 Prec@5=87.622 rate=3057.34 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
** validation 100.00% of 1x98...Epoch=89/150 LR=0.03655 Time=0.555 Loss=1.361 Prec@1=66.810 Prec@5=87.622 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=21:59 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:59 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:59 IST
=> training   0.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=4.346 DataTime=4.086 Loss=1.206 Prec@1=71.094 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=21:59 IST
=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=4.346 DataTime=4.086 Loss=1.206 Prec@1=71.094 Prec@5=87.695 rate=9125.00 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=4.346 DataTime=4.086 Loss=1.206 Prec@1=71.094 Prec@5=87.695 rate=9125.00 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.511 DataTime=0.318 Loss=1.219 Prec@1=69.870 Prec@5=89.022 rate=9125.00 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.511 DataTime=0.318 Loss=1.219 Prec@1=69.870 Prec@5=89.022 rate=2.14 Hz, eta=0:18:44, total=0:00:47, wall=21:59 IST
=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.511 DataTime=0.318 Loss=1.219 Prec@1=69.870 Prec@5=89.022 rate=2.14 Hz, eta=0:18:44, total=0:00:47, wall=22:00 IST
=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.491 DataTime=0.296 Loss=1.216 Prec@1=69.832 Prec@5=89.115 rate=2.14 Hz, eta=0:18:44, total=0:00:47, wall=22:00 IST
=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.491 DataTime=0.296 Loss=1.216 Prec@1=69.832 Prec@5=89.115 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=22:00 IST
=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.491 DataTime=0.296 Loss=1.216 Prec@1=69.832 Prec@5=89.115 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=22:01 IST
=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.480 DataTime=0.285 Loss=1.217 Prec@1=69.815 Prec@5=89.094 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=22:01 IST
=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.480 DataTime=0.285 Loss=1.217 Prec@1=69.815 Prec@5=89.094 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=22:01 IST
=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.480 DataTime=0.285 Loss=1.217 Prec@1=69.815 Prec@5=89.094 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=22:02 IST
=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.474 DataTime=0.279 Loss=1.220 Prec@1=69.798 Prec@5=89.049 rate=2.15 Hz, eta=0:17:05, total=0:02:20, wall=22:02 IST
=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.474 DataTime=0.279 Loss=1.220 Prec@1=69.798 Prec@5=89.049 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=22:02 IST
=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.474 DataTime=0.279 Loss=1.220 Prec@1=69.798 Prec@5=89.049 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=22:03 IST
=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.472 DataTime=0.277 Loss=1.226 Prec@1=69.666 Prec@5=88.967 rate=2.16 Hz, eta=0:16:14, total=0:03:05, wall=22:03 IST
=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.472 DataTime=0.277 Loss=1.226 Prec@1=69.666 Prec@5=88.967 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:03 IST
=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.472 DataTime=0.277 Loss=1.226 Prec@1=69.666 Prec@5=88.967 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:03 IST
=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.275 Loss=1.228 Prec@1=69.573 Prec@5=88.937 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:03 IST
=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.275 Loss=1.228 Prec@1=69.573 Prec@5=88.937 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=22:03 IST
=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.275 Loss=1.228 Prec@1=69.573 Prec@5=88.937 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=22:04 IST
=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.471 DataTime=0.275 Loss=1.231 Prec@1=69.485 Prec@5=88.874 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=22:04 IST
=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.471 DataTime=0.275 Loss=1.231 Prec@1=69.485 Prec@5=88.874 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=22:04 IST
=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.471 DataTime=0.275 Loss=1.231 Prec@1=69.485 Prec@5=88.874 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=22:05 IST
=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.233 Prec@1=69.459 Prec@5=88.858 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=22:05 IST
=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.233 Prec@1=69.459 Prec@5=88.858 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:05 IST
=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.233 Prec@1=69.459 Prec@5=88.858 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:06 IST
=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.235 Prec@1=69.438 Prec@5=88.824 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:06 IST
=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.235 Prec@1=69.438 Prec@5=88.824 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:06 IST
=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.470 DataTime=0.274 Loss=1.235 Prec@1=69.438 Prec@5=88.824 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:06 IST
=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.417 Prec@5=88.796 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:06 IST
=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.417 Prec@5=88.796 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=22:06 IST
=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.417 Prec@5=88.796 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=22:07 IST
=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.237 Prec@1=69.397 Prec@5=88.795 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=22:07 IST
=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.237 Prec@1=69.397 Prec@5=88.795 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=22:07 IST
=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.237 Prec@1=69.397 Prec@5=88.795 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=22:08 IST
=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.238 Prec@1=69.383 Prec@5=88.767 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=22:08 IST
=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.238 Prec@1=69.383 Prec@5=88.767 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=22:08 IST
=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.238 Prec@1=69.383 Prec@5=88.767 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=22:09 IST
=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.240 Prec@1=69.369 Prec@5=88.726 rate=2.16 Hz, eta=0:10:04, total=0:09:17, wall=22:09 IST
=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.240 Prec@1=69.369 Prec@5=88.726 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=22:09 IST
=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.240 Prec@1=69.369 Prec@5=88.726 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=22:10 IST
=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.241 Prec@1=69.348 Prec@5=88.716 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=22:10 IST
=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.241 Prec@1=69.348 Prec@5=88.716 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=22:10 IST
=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.241 Prec@1=69.348 Prec@5=88.716 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=22:10 IST
=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.242 Prec@1=69.344 Prec@5=88.721 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=22:10 IST
=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.242 Prec@1=69.344 Prec@5=88.721 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=22:10 IST
=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.468 DataTime=0.272 Loss=1.242 Prec@1=69.344 Prec@5=88.721 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=22:11 IST
=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.242 Prec@1=69.327 Prec@5=88.716 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=22:11 IST
=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.242 Prec@1=69.327 Prec@5=88.716 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=22:11 IST
=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.242 Prec@1=69.327 Prec@5=88.716 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=22:12 IST
=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.242 Prec@1=69.330 Prec@5=88.726 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=22:12 IST
=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.242 Prec@1=69.330 Prec@5=88.726 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=22:12 IST
=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.272 Loss=1.242 Prec@1=69.330 Prec@5=88.726 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=22:13 IST
=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.243 Prec@1=69.313 Prec@5=88.716 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=22:13 IST
=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.243 Prec@1=69.313 Prec@5=88.716 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=22:13 IST
=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.243 Prec@1=69.313 Prec@5=88.716 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=22:13 IST
=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.244 Prec@1=69.305 Prec@5=88.704 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=22:13 IST
=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.244 Prec@1=69.305 Prec@5=88.704 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=22:13 IST
=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.244 Prec@1=69.305 Prec@5=88.704 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=22:14 IST
=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.246 Prec@1=69.270 Prec@5=88.684 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=22:14 IST
=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.246 Prec@1=69.270 Prec@5=88.684 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=22:14 IST
=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.246 Prec@1=69.270 Prec@5=88.684 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=22:15 IST
=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.247 Prec@1=69.258 Prec@5=88.668 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=22:15 IST
=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.247 Prec@1=69.258 Prec@5=88.668 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=22:15 IST
=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.247 Prec@1=69.258 Prec@5=88.668 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=22:16 IST
=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.248 Prec@1=69.235 Prec@5=88.657 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=22:16 IST
=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.248 Prec@1=69.235 Prec@5=88.657 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=22:16 IST
=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.467 DataTime=0.271 Loss=1.248 Prec@1=69.235 Prec@5=88.657 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=22:16 IST
=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.210 Prec@5=88.638 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=22:16 IST
=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.210 Prec@5=88.638 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=22:16 IST
=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.210 Prec@5=88.638 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=22:17 IST
=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.197 Prec@5=88.626 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=22:17 IST
=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.197 Prec@5=88.626 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=22:17 IST
=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.249 Prec@1=69.197 Prec@5=88.626 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=22:18 IST
=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.250 Prec@1=69.180 Prec@5=88.615 rate=2.15 Hz, eta=0:00:47, total=0:18:34, wall=22:18 IST
=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.250 Prec@1=69.180 Prec@5=88.615 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=22:18 IST
=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.250 Prec@1=69.180 Prec@5=88.615 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=22:18 IST
=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.250 Prec@1=69.180 Prec@5=88.614 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=22:18 IST
=> training   100.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.466 DataTime=0.270 Loss=1.250 Prec@1=69.180 Prec@5=88.614 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=22:18 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:18 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:18 IST
=> validation 0.00% of 1x98...Epoch=90/150 LR=0.03555 Time=7.151 Loss=0.852 Prec@1=77.734 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=22:18 IST
=> validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=7.151 Loss=0.852 Prec@1=77.734 Prec@5=94.727 rate=7746.71 Hz, eta=0:00:00, total=0:00:00, wall=22:18 IST
** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=7.151 Loss=0.852 Prec@1=77.734 Prec@5=94.727 rate=7746.71 Hz, eta=0:00:00, total=0:00:00, wall=22:19 IST
** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=0.554 Loss=1.351 Prec@1=66.950 Prec@5=87.782 rate=7746.71 Hz, eta=0:00:00, total=0:00:00, wall=22:19 IST
** validation 100.00% of 1x98...Epoch=90/150 LR=0.03555 Time=0.554 Loss=1.351 Prec@1=66.950 Prec@5=87.782 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=22:19 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:19 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:19 IST
=> training   0.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=4.868 DataTime=4.636 Loss=1.177 Prec@1=71.094 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=22:19 IST
=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=4.868 DataTime=4.636 Loss=1.177 Prec@1=71.094 Prec@5=90.039 rate=8094.67 Hz, eta=0:00:00, total=0:00:00, wall=22:19 IST
=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=4.868 DataTime=4.636 Loss=1.177 Prec@1=71.094 Prec@5=90.039 rate=8094.67 Hz, eta=0:00:00, total=0:00:00, wall=22:20 IST
=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.510 DataTime=0.315 Loss=1.213 Prec@1=69.922 Prec@5=88.995 rate=8094.67 Hz, eta=0:00:00, total=0:00:00, wall=22:20 IST
=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.510 DataTime=0.315 Loss=1.213 Prec@1=69.922 Prec@5=88.995 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=22:20 IST
=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.510 DataTime=0.315 Loss=1.213 Prec@1=69.922 Prec@5=88.995 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=22:21 IST
=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.490 DataTime=0.293 Loss=1.218 Prec@1=69.815 Prec@5=89.044 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=22:21 IST
=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.490 DataTime=0.293 Loss=1.218 Prec@1=69.815 Prec@5=89.044 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=22:21 IST
=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.490 DataTime=0.293 Loss=1.218 Prec@1=69.815 Prec@5=89.044 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=22:21 IST
=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.482 DataTime=0.286 Loss=1.214 Prec@1=69.869 Prec@5=89.084 rate=2.15 Hz, eta=0:17:51, total=0:01:33, wall=22:21 IST
=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.482 DataTime=0.286 Loss=1.214 Prec@1=69.869 Prec@5=89.084 rate=2.14 Hz, eta=0:17:06, total=0:02:20, wall=22:21 IST
=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.482 DataTime=0.286 Loss=1.214 Prec@1=69.869 Prec@5=89.084 rate=2.14 Hz, eta=0:17:06, total=0:02:20, wall=22:22 IST
=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.478 DataTime=0.281 Loss=1.222 Prec@1=69.712 Prec@5=89.007 rate=2.14 Hz, eta=0:17:06, total=0:02:20, wall=22:22 IST
=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.478 DataTime=0.281 Loss=1.222 Prec@1=69.712 Prec@5=89.007 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=22:22 IST
=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.478 DataTime=0.281 Loss=1.222 Prec@1=69.712 Prec@5=89.007 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=22:23 IST
=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.223 Prec@1=69.672 Prec@5=88.990 rate=2.15 Hz, eta=0:16:19, total=0:03:06, wall=22:23 IST
=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.223 Prec@1=69.672 Prec@5=88.990 rate=2.14 Hz, eta=0:15:33, total=0:03:53, wall=22:23 IST
=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.223 Prec@1=69.672 Prec@5=88.990 rate=2.14 Hz, eta=0:15:33, total=0:03:53, wall=22:24 IST
=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.224 Prec@1=69.677 Prec@5=88.999 rate=2.14 Hz, eta=0:15:33, total=0:03:53, wall=22:24 IST
=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.224 Prec@1=69.677 Prec@5=88.999 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=22:24 IST
=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.224 Prec@1=69.677 Prec@5=88.999 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=22:25 IST
=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.477 DataTime=0.280 Loss=1.222 Prec@1=69.732 Prec@5=89.027 rate=2.14 Hz, eta=0:14:48, total=0:04:40, wall=22:25 IST
=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.477 DataTime=0.280 Loss=1.222 Prec@1=69.732 Prec@5=89.027 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=22:25 IST
=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.477 DataTime=0.280 Loss=1.222 Prec@1=69.732 Prec@5=89.027 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=22:25 IST
=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.222 Prec@1=69.734 Prec@5=89.028 rate=2.13 Hz, eta=0:14:06, total=0:05:29, wall=22:25 IST
=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.222 Prec@1=69.734 Prec@5=89.028 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=22:25 IST
=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.476 DataTime=0.280 Loss=1.222 Prec@1=69.734 Prec@5=89.028 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=22:26 IST
=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.223 Prec@1=69.716 Prec@5=89.002 rate=2.13 Hz, eta=0:13:20, total=0:06:16, wall=22:26 IST
=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.223 Prec@1=69.716 Prec@5=89.002 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=22:26 IST
=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.475 DataTime=0.279 Loss=1.223 Prec@1=69.716 Prec@5=89.002 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=22:27 IST
=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.224 Prec@1=69.708 Prec@5=88.986 rate=2.13 Hz, eta=0:12:31, total=0:07:02, wall=22:27 IST
=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.224 Prec@1=69.708 Prec@5=88.986 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=22:27 IST
=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.224 Prec@1=69.708 Prec@5=88.986 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=22:28 IST
=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.225 Prec@1=69.670 Prec@5=88.982 rate=2.13 Hz, eta=0:11:44, total=0:07:49, wall=22:28 IST
=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.225 Prec@1=69.670 Prec@5=88.982 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=22:28 IST
=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.225 Prec@1=69.670 Prec@5=88.982 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=22:29 IST
=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.225 Prec@1=69.669 Prec@5=88.978 rate=2.13 Hz, eta=0:10:57, total=0:08:36, wall=22:29 IST
=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.225 Prec@1=69.669 Prec@5=88.978 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=22:29 IST
=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.225 Prec@1=69.669 Prec@5=88.978 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=22:29 IST
=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.228 Prec@1=69.639 Prec@5=88.951 rate=2.13 Hz, eta=0:10:12, total=0:09:24, wall=22:29 IST
=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.228 Prec@1=69.639 Prec@5=88.951 rate=2.13 Hz, eta=0:09:25, total=0:10:12, wall=22:29 IST
=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.228 Prec@1=69.639 Prec@5=88.951 rate=2.13 Hz, eta=0:09:25, total=0:10:12, wall=22:30 IST
=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.229 Prec@1=69.619 Prec@5=88.936 rate=2.13 Hz, eta=0:09:25, total=0:10:12, wall=22:30 IST
=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.229 Prec@1=69.619 Prec@5=88.936 rate=2.13 Hz, eta=0:08:38, total=0:10:59, wall=22:30 IST
=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.229 Prec@1=69.619 Prec@5=88.936 rate=2.13 Hz, eta=0:08:38, total=0:10:59, wall=22:31 IST
=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.231 Prec@1=69.577 Prec@5=88.922 rate=2.13 Hz, eta=0:08:38, total=0:10:59, wall=22:31 IST
=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.231 Prec@1=69.577 Prec@5=88.922 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=22:31 IST
=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.279 Loss=1.231 Prec@1=69.577 Prec@5=88.922 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=22:32 IST
=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.232 Prec@1=69.558 Prec@5=88.896 rate=2.12 Hz, eta=0:07:51, total=0:11:46, wall=22:32 IST
=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.232 Prec@1=69.558 Prec@5=88.896 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=22:32 IST
=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.232 Prec@1=69.558 Prec@5=88.896 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=22:32 IST
=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.233 Prec@1=69.552 Prec@5=88.882 rate=2.12 Hz, eta=0:07:04, total=0:12:33, wall=22:32 IST
=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.233 Prec@1=69.552 Prec@5=88.882 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=22:32 IST
=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.233 Prec@1=69.552 Prec@5=88.882 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=22:33 IST
=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.235 Prec@1=69.515 Prec@5=88.853 rate=2.12 Hz, eta=0:06:17, total=0:13:20, wall=22:33 IST
=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.235 Prec@1=69.515 Prec@5=88.853 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=22:33 IST
=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.235 Prec@1=69.515 Prec@5=88.853 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=22:34 IST
=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.236 Prec@1=69.483 Prec@5=88.836 rate=2.12 Hz, eta=0:05:30, total=0:14:07, wall=22:34 IST
=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.236 Prec@1=69.483 Prec@5=88.836 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=22:34 IST
=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.236 Prec@1=69.483 Prec@5=88.836 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=22:35 IST
=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.451 Prec@5=88.817 rate=2.12 Hz, eta=0:04:43, total=0:14:55, wall=22:35 IST
=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.451 Prec@5=88.817 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=22:35 IST
=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.451 Prec@5=88.817 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=22:36 IST
=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.449 Prec@5=88.814 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=22:36 IST
=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.449 Prec@5=88.814 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:36 IST
=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.474 DataTime=0.278 Loss=1.238 Prec@1=69.449 Prec@5=88.814 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:36 IST
=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.239 Prec@1=69.423 Prec@5=88.801 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=22:36 IST
=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.239 Prec@1=69.423 Prec@5=88.801 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:36 IST
=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.239 Prec@1=69.423 Prec@5=88.801 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:37 IST
=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.240 Prec@1=69.384 Prec@5=88.784 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=22:37 IST
=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.240 Prec@1=69.384 Prec@5=88.784 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:37 IST
=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.240 Prec@1=69.384 Prec@5=88.784 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:38 IST
=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.241 Prec@1=69.361 Prec@5=88.776 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=22:38 IST
=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.241 Prec@1=69.361 Prec@5=88.776 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:38 IST
=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.241 Prec@1=69.361 Prec@5=88.776 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:39 IST
=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.243 Prec@1=69.334 Prec@5=88.758 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=22:39 IST
=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.243 Prec@1=69.334 Prec@5=88.758 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=22:39 IST
=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.243 Prec@1=69.334 Prec@5=88.758 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=22:39 IST
=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.243 Prec@1=69.334 Prec@5=88.757 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=22:39 IST
=> training   100.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.473 DataTime=0.278 Loss=1.243 Prec@1=69.334 Prec@5=88.757 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=22:39 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST
=> validation 0.00% of 1x98...Epoch=91/150 LR=0.03455 Time=7.021 Loss=0.884 Prec@1=77.734 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST
=> validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=7.021 Loss=0.884 Prec@1=77.734 Prec@5=93.945 rate=3272.07 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST
** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=7.021 Loss=0.884 Prec@1=77.734 Prec@5=93.945 rate=3272.07 Hz, eta=0:00:00, total=0:00:00, wall=22:40 IST
** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=0.553 Loss=1.339 Prec@1=67.222 Prec@5=87.854 rate=3272.07 Hz, eta=0:00:00, total=0:00:00, wall=22:40 IST
** validation 100.00% of 1x98...Epoch=91/150 LR=0.03455 Time=0.553 Loss=1.339 Prec@1=67.222 Prec@5=87.854 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=22:40 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:40 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:40 IST
=> training   0.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.995 DataTime=4.789 Loss=1.234 Prec@1=67.773 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=22:40 IST
=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.995 DataTime=4.789 Loss=1.234 Prec@1=67.773 Prec@5=88.672 rate=5940.89 Hz, eta=0:00:00, total=0:00:00, wall=22:40 IST
=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.995 DataTime=4.789 Loss=1.234 Prec@1=67.773 Prec@5=88.672 rate=5940.89 Hz, eta=0:00:00, total=0:00:00, wall=22:41 IST
=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.500 DataTime=0.304 Loss=1.206 Prec@1=70.003 Prec@5=89.337 rate=5940.89 Hz, eta=0:00:00, total=0:00:00, wall=22:41 IST
=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.500 DataTime=0.304 Loss=1.206 Prec@1=70.003 Prec@5=89.337 rate=2.22 Hz, eta=0:18:03, total=0:00:45, wall=22:41 IST
=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.500 DataTime=0.304 Loss=1.206 Prec@1=70.003 Prec@5=89.337 rate=2.22 Hz, eta=0:18:03, total=0:00:45, wall=22:41 IST
=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.481 DataTime=0.285 Loss=1.204 Prec@1=70.148 Prec@5=89.341 rate=2.22 Hz, eta=0:18:03, total=0:00:45, wall=22:41 IST
=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.481 DataTime=0.285 Loss=1.204 Prec@1=70.148 Prec@5=89.341 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=22:41 IST
=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.481 DataTime=0.285 Loss=1.204 Prec@1=70.148 Prec@5=89.341 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=22:42 IST
=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.479 DataTime=0.283 Loss=1.207 Prec@1=70.106 Prec@5=89.247 rate=2.19 Hz, eta=0:17:29, total=0:01:31, wall=22:42 IST
=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.479 DataTime=0.283 Loss=1.207 Prec@1=70.106 Prec@5=89.247 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=22:42 IST
=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.479 DataTime=0.283 Loss=1.207 Prec@1=70.106 Prec@5=89.247 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=22:43 IST
=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.474 DataTime=0.278 Loss=1.210 Prec@1=70.072 Prec@5=89.190 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=22:43 IST
=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.474 DataTime=0.278 Loss=1.210 Prec@1=70.072 Prec@5=89.190 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=22:43 IST
=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.474 DataTime=0.278 Loss=1.210 Prec@1=70.072 Prec@5=89.190 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=22:44 IST
=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.473 DataTime=0.277 Loss=1.210 Prec@1=70.076 Prec@5=89.165 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=22:44 IST
=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.473 DataTime=0.277 Loss=1.210 Prec@1=70.076 Prec@5=89.165 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:44 IST
=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.473 DataTime=0.277 Loss=1.210 Prec@1=70.076 Prec@5=89.165 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:44 IST
=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.211 Prec@1=70.043 Prec@5=89.144 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=22:44 IST
=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.211 Prec@1=70.043 Prec@5=89.144 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=22:44 IST
=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.211 Prec@1=70.043 Prec@5=89.144 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=22:45 IST
=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.215 Prec@1=69.942 Prec@5=89.085 rate=2.16 Hz, eta=0:14:42, total=0:04:38, wall=22:45 IST
=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.215 Prec@1=69.942 Prec@5=89.085 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=22:45 IST
=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.472 DataTime=0.276 Loss=1.215 Prec@1=69.942 Prec@5=89.085 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=22:46 IST
=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.471 DataTime=0.275 Loss=1.216 Prec@1=69.907 Prec@5=89.048 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=22:46 IST
=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.471 DataTime=0.275 Loss=1.216 Prec@1=69.907 Prec@5=89.048 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:46 IST
=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.471 DataTime=0.275 Loss=1.216 Prec@1=69.907 Prec@5=89.048 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:47 IST
=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.855 Prec@5=89.020 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=22:47 IST
=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.855 Prec@5=89.020 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:47 IST
=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.855 Prec@5=89.020 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:48 IST
=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.858 Prec@5=89.002 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=22:48 IST
=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.858 Prec@5=89.002 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=22:48 IST
=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.219 Prec@1=69.858 Prec@5=89.002 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=22:48 IST
=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.821 Prec@5=88.988 rate=2.15 Hz, eta=0:11:39, total=0:07:45, wall=22:48 IST
=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.821 Prec@5=88.988 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=22:48 IST
=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.821 Prec@5=88.988 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=22:49 IST
=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.803 Prec@5=88.989 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=22:49 IST
=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.803 Prec@5=88.989 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=22:49 IST
=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.274 Loss=1.221 Prec@1=69.803 Prec@5=88.989 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=22:50 IST
=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.222 Prec@1=69.775 Prec@5=88.975 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=22:50 IST
=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.222 Prec@1=69.775 Prec@5=88.975 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=22:50 IST
=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.470 DataTime=0.275 Loss=1.222 Prec@1=69.775 Prec@5=88.975 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=22:51 IST
=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.223 Prec@1=69.758 Prec@5=88.963 rate=2.14 Hz, eta=0:09:20, total=0:10:07, wall=22:51 IST
=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.223 Prec@1=69.758 Prec@5=88.963 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=22:51 IST
=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.223 Prec@1=69.758 Prec@5=88.963 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=22:51 IST
=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.224 Prec@1=69.733 Prec@5=88.951 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=22:51 IST
=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.224 Prec@1=69.733 Prec@5=88.951 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:51 IST
=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.224 Prec@1=69.733 Prec@5=88.951 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:52 IST
=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.226 Prec@1=69.690 Prec@5=88.933 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=22:52 IST
=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.226 Prec@1=69.690 Prec@5=88.933 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=22:52 IST
=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.226 Prec@1=69.690 Prec@5=88.933 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=22:53 IST
=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.227 Prec@1=69.674 Prec@5=88.921 rate=2.15 Hz, eta=0:07:00, total=0:12:26, wall=22:53 IST
=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.227 Prec@1=69.674 Prec@5=88.921 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=22:53 IST
=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.274 Loss=1.227 Prec@1=69.674 Prec@5=88.921 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=22:54 IST
=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.228 Prec@1=69.645 Prec@5=88.906 rate=2.14 Hz, eta=0:06:14, total=0:13:13, wall=22:54 IST
=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.228 Prec@1=69.645 Prec@5=88.906 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=22:54 IST
=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.228 Prec@1=69.645 Prec@5=88.906 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=22:55 IST
=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.468 DataTime=0.273 Loss=1.228 Prec@1=69.629 Prec@5=88.892 rate=2.14 Hz, eta=0:05:27, total=0:13:59, wall=22:55 IST
=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.468 DataTime=0.273 Loss=1.228 Prec@1=69.629 Prec@5=88.892 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:55 IST
=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.468 DataTime=0.273 Loss=1.228 Prec@1=69.629 Prec@5=88.892 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:55 IST
=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.229 Prec@1=69.610 Prec@5=88.876 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=22:55 IST
=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.229 Prec@1=69.610 Prec@5=88.876 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=22:55 IST
=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.229 Prec@1=69.610 Prec@5=88.876 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=22:56 IST
=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.230 Prec@1=69.603 Prec@5=88.867 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=22:56 IST
=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.230 Prec@1=69.603 Prec@5=88.867 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=22:56 IST
=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.230 Prec@1=69.603 Prec@5=88.867 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=22:57 IST
=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.232 Prec@1=69.575 Prec@5=88.849 rate=2.14 Hz, eta=0:03:07, total=0:16:19, wall=22:57 IST
=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.232 Prec@1=69.575 Prec@5=88.849 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=22:57 IST
=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.232 Prec@1=69.575 Prec@5=88.849 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=22:58 IST
=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.233 Prec@1=69.540 Prec@5=88.836 rate=2.14 Hz, eta=0:02:20, total=0:17:07, wall=22:58 IST
=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.233 Prec@1=69.540 Prec@5=88.836 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=22:58 IST
=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.233 Prec@1=69.540 Prec@5=88.836 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=22:58 IST
=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.235 Prec@1=69.520 Prec@5=88.813 rate=2.14 Hz, eta=0:01:34, total=0:17:53, wall=22:58 IST
=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.235 Prec@1=69.520 Prec@5=88.813 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=22:58 IST
=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.235 Prec@1=69.520 Prec@5=88.813 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=22:59 IST
=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.493 Prec@5=88.801 rate=2.14 Hz, eta=0:00:47, total=0:18:40, wall=22:59 IST
=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.493 Prec@5=88.801 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=22:59 IST
=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.493 Prec@5=88.801 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=22:59 IST
=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.491 Prec@5=88.799 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=22:59 IST
=> training   100.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.469 DataTime=0.273 Loss=1.236 Prec@1=69.491 Prec@5=88.799 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=22:59 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> validation 0.00% of 1x98...Epoch=92/150 LR=0.03356 Time=6.310 Loss=0.769 Prec@1=79.688 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=6.310 Loss=0.769 Prec@1=79.688 Prec@5=95.117 rate=4390.55 Hz, eta=0:00:00, total=0:00:00, wall=22:59 IST
** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=6.310 Loss=0.769 Prec@1=79.688 Prec@5=95.117 rate=4390.55 Hz, eta=0:00:00, total=0:00:00, wall=23:00 IST
** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=0.549 Loss=1.325 Prec@1=67.512 Prec@5=88.082 rate=4390.55 Hz, eta=0:00:00, total=0:00:00, wall=23:00 IST
** validation 100.00% of 1x98...Epoch=92/150 LR=0.03356 Time=0.549 Loss=1.325 Prec@1=67.512 Prec@5=88.082 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=23:00 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST
=> training   0.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.212 DataTime=5.005 Loss=1.248 Prec@1=68.359 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST
=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.212 DataTime=5.005 Loss=1.248 Prec@1=68.359 Prec@5=88.672 rate=10275.69 Hz, eta=0:00:00, total=0:00:00, wall=23:00 IST
=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.212 DataTime=5.005 Loss=1.248 Prec@1=68.359 Prec@5=88.672 rate=10275.69 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST
=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.517 DataTime=0.322 Loss=1.197 Prec@1=70.200 Prec@5=89.213 rate=10275.69 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST
=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.517 DataTime=0.322 Loss=1.197 Prec@1=70.200 Prec@5=89.213 rate=2.15 Hz, eta=0:18:37, total=0:00:46, wall=23:01 IST
=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.517 DataTime=0.322 Loss=1.197 Prec@1=70.200 Prec@5=89.213 rate=2.15 Hz, eta=0:18:37, total=0:00:46, wall=23:02 IST
=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.501 DataTime=0.304 Loss=1.199 Prec@1=70.095 Prec@5=89.244 rate=2.15 Hz, eta=0:18:37, total=0:00:46, wall=23:02 IST
=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.501 DataTime=0.304 Loss=1.199 Prec@1=70.095 Prec@5=89.244 rate=2.11 Hz, eta=0:18:12, total=0:01:35, wall=23:02 IST
=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.501 DataTime=0.304 Loss=1.199 Prec@1=70.095 Prec@5=89.244 rate=2.11 Hz, eta=0:18:12, total=0:01:35, wall=23:03 IST
=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.494 DataTime=0.298 Loss=1.198 Prec@1=70.200 Prec@5=89.279 rate=2.11 Hz, eta=0:18:12, total=0:01:35, wall=23:03 IST
=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.494 DataTime=0.298 Loss=1.198 Prec@1=70.200 Prec@5=89.279 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=23:03 IST
=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.494 DataTime=0.298 Loss=1.198 Prec@1=70.200 Prec@5=89.279 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=23:03 IST
=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.292 Loss=1.200 Prec@1=70.224 Prec@5=89.243 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=23:03 IST
=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.292 Loss=1.200 Prec@1=70.224 Prec@5=89.243 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=23:03 IST
=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.292 Loss=1.200 Prec@1=70.224 Prec@5=89.243 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=23:04 IST
=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.290 Loss=1.203 Prec@1=70.189 Prec@5=89.197 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=23:04 IST
=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.290 Loss=1.203 Prec@1=70.189 Prec@5=89.197 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:04 IST
=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.290 Loss=1.203 Prec@1=70.189 Prec@5=89.197 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:05 IST
=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.482 DataTime=0.286 Loss=1.202 Prec@1=70.207 Prec@5=89.200 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:05 IST
=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.482 DataTime=0.286 Loss=1.202 Prec@1=70.207 Prec@5=89.200 rate=2.11 Hz, eta=0:15:00, total=0:04:44, wall=23:05 IST
=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.482 DataTime=0.286 Loss=1.202 Prec@1=70.207 Prec@5=89.200 rate=2.11 Hz, eta=0:15:00, total=0:04:44, wall=23:06 IST
=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.483 DataTime=0.286 Loss=1.204 Prec@1=70.157 Prec@5=89.174 rate=2.11 Hz, eta=0:15:00, total=0:04:44, wall=23:06 IST
=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.483 DataTime=0.286 Loss=1.204 Prec@1=70.157 Prec@5=89.174 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=23:06 IST
=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.483 DataTime=0.286 Loss=1.204 Prec@1=70.157 Prec@5=89.174 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=23:07 IST
=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.206 Prec@1=70.106 Prec@5=89.153 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=23:07 IST
=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.206 Prec@1=70.106 Prec@5=89.153 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=23:07 IST
=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.206 Prec@1=70.106 Prec@5=89.153 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=23:08 IST
=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.209 Prec@1=70.055 Prec@5=89.104 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=23:08 IST
=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.209 Prec@1=70.055 Prec@5=89.104 rate=2.07 Hz, eta=0:12:52, total=0:07:14, wall=23:08 IST
=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.209 Prec@1=70.055 Prec@5=89.104 rate=2.07 Hz, eta=0:12:52, total=0:07:14, wall=23:08 IST
=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.212 Prec@1=70.000 Prec@5=89.069 rate=2.07 Hz, eta=0:12:52, total=0:07:14, wall=23:08 IST
=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.212 Prec@1=70.000 Prec@5=89.069 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=23:08 IST
=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.212 Prec@1=70.000 Prec@5=89.069 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=23:09 IST
=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.293 Loss=1.213 Prec@1=69.996 Prec@5=89.053 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=23:09 IST
=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.293 Loss=1.213 Prec@1=69.996 Prec@5=89.053 rate=2.07 Hz, eta=0:11:18, total=0:08:52, wall=23:09 IST
=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.489 DataTime=0.293 Loss=1.213 Prec@1=69.996 Prec@5=89.053 rate=2.07 Hz, eta=0:11:18, total=0:08:52, wall=23:10 IST
=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.950 Prec@5=89.040 rate=2.07 Hz, eta=0:11:18, total=0:08:52, wall=23:10 IST
=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.950 Prec@5=89.040 rate=2.07 Hz, eta=0:10:29, total=0:09:40, wall=23:10 IST
=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.950 Prec@5=89.040 rate=2.07 Hz, eta=0:10:29, total=0:09:40, wall=23:11 IST
=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.916 Prec@5=89.037 rate=2.07 Hz, eta=0:10:29, total=0:09:40, wall=23:11 IST
=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.916 Prec@5=89.037 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=23:11 IST
=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.215 Prec@1=69.916 Prec@5=89.037 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=23:12 IST
=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.216 Prec@1=69.901 Prec@5=89.022 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=23:12 IST
=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.216 Prec@1=69.901 Prec@5=89.022 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=23:12 IST
=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.292 Loss=1.216 Prec@1=69.901 Prec@5=89.022 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=23:12 IST
=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.218 Prec@1=69.879 Prec@5=89.007 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=23:12 IST
=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.218 Prec@1=69.879 Prec@5=89.007 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=23:12 IST
=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.218 Prec@1=69.879 Prec@5=89.007 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=23:13 IST
=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.219 Prec@1=69.850 Prec@5=88.991 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=23:13 IST
=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.219 Prec@1=69.850 Prec@5=88.991 rate=2.06 Hz, eta=0:07:17, total=0:12:56, wall=23:13 IST
=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.219 Prec@1=69.850 Prec@5=88.991 rate=2.06 Hz, eta=0:07:17, total=0:12:56, wall=23:14 IST
=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.220 Prec@1=69.822 Prec@5=88.975 rate=2.06 Hz, eta=0:07:17, total=0:12:56, wall=23:14 IST
=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.220 Prec@1=69.822 Prec@5=88.975 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=23:14 IST
=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.220 Prec@1=69.822 Prec@5=88.975 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=23:15 IST
=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.221 Prec@1=69.797 Prec@5=88.959 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=23:15 IST
=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.221 Prec@1=69.797 Prec@5=88.959 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=23:15 IST
=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.488 DataTime=0.293 Loss=1.221 Prec@1=69.797 Prec@5=88.959 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=23:16 IST
=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.292 Loss=1.222 Prec@1=69.792 Prec@5=88.957 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=23:16 IST
=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.292 Loss=1.222 Prec@1=69.792 Prec@5=88.957 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=23:16 IST
=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.292 Loss=1.222 Prec@1=69.792 Prec@5=88.957 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=23:16 IST
=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.223 Prec@1=69.766 Prec@5=88.941 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=23:16 IST
=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.223 Prec@1=69.766 Prec@5=88.941 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=23:16 IST
=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.223 Prec@1=69.766 Prec@5=88.941 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=23:17 IST
=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.225 Prec@1=69.729 Prec@5=88.920 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=23:17 IST
=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.225 Prec@1=69.729 Prec@5=88.920 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=23:17 IST
=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.487 DataTime=0.291 Loss=1.225 Prec@1=69.729 Prec@5=88.920 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=23:18 IST
=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.225 Prec@1=69.714 Prec@5=88.919 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=23:18 IST
=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.225 Prec@1=69.714 Prec@5=88.919 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=23:18 IST
=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.225 Prec@1=69.714 Prec@5=88.919 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=23:19 IST
=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.226 Prec@1=69.692 Prec@5=88.904 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=23:19 IST
=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.226 Prec@1=69.692 Prec@5=88.904 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=23:19 IST
=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.291 Loss=1.226 Prec@1=69.692 Prec@5=88.904 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=23:20 IST
=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.290 Loss=1.227 Prec@1=69.682 Prec@5=88.894 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=23:20 IST
=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.290 Loss=1.227 Prec@1=69.682 Prec@5=88.894 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=23:20 IST
=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.486 DataTime=0.290 Loss=1.227 Prec@1=69.682 Prec@5=88.894 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=23:20 IST
=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.228 Prec@1=69.661 Prec@5=88.881 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=23:20 IST
=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.228 Prec@1=69.661 Prec@5=88.881 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=23:20 IST
=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.228 Prec@1=69.661 Prec@5=88.881 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=23:20 IST
=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.228 Prec@1=69.661 Prec@5=88.881 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=23:20 IST
=> training   100.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.485 DataTime=0.290 Loss=1.228 Prec@1=69.661 Prec@5=88.881 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=23:20 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> validation 0.00% of 1x98...Epoch=93/150 LR=0.03257 Time=6.851 Loss=0.837 Prec@1=78.906 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=6.851 Loss=0.837 Prec@1=78.906 Prec@5=93.750 rate=7902.39 Hz, eta=0:00:00, total=0:00:00, wall=23:21 IST
** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=6.851 Loss=0.837 Prec@1=78.906 Prec@5=93.750 rate=7902.39 Hz, eta=0:00:00, total=0:00:00, wall=23:21 IST
** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=0.553 Loss=1.328 Prec@1=67.572 Prec@5=87.986 rate=7902.39 Hz, eta=0:00:00, total=0:00:00, wall=23:21 IST
** validation 100.00% of 1x98...Epoch=93/150 LR=0.03257 Time=0.553 Loss=1.328 Prec@1=67.572 Prec@5=87.986 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=23:21 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> training   0.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.136 DataTime=4.880 Loss=1.122 Prec@1=71.875 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=23:21 IST
=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.136 DataTime=4.880 Loss=1.122 Prec@1=71.875 Prec@5=90.430 rate=9012.01 Hz, eta=0:00:00, total=0:00:00, wall=23:21 IST
=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.136 DataTime=4.880 Loss=1.122 Prec@1=71.875 Prec@5=90.430 rate=9012.01 Hz, eta=0:00:00, total=0:00:00, wall=23:22 IST
=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.512 DataTime=0.318 Loss=1.196 Prec@1=70.541 Prec@5=89.271 rate=9012.01 Hz, eta=0:00:00, total=0:00:00, wall=23:22 IST
=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.512 DataTime=0.318 Loss=1.196 Prec@1=70.541 Prec@5=89.271 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=23:22 IST
=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.512 DataTime=0.318 Loss=1.196 Prec@1=70.541 Prec@5=89.271 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=23:23 IST
=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.496 DataTime=0.300 Loss=1.198 Prec@1=70.448 Prec@5=89.235 rate=2.17 Hz, eta=0:18:28, total=0:00:46, wall=23:23 IST
=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.496 DataTime=0.300 Loss=1.198 Prec@1=70.448 Prec@5=89.235 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=23:23 IST
=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.496 DataTime=0.300 Loss=1.198 Prec@1=70.448 Prec@5=89.235 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=23:24 IST
=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.493 DataTime=0.297 Loss=1.197 Prec@1=70.453 Prec@5=89.233 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=23:24 IST
=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.493 DataTime=0.297 Loss=1.197 Prec@1=70.453 Prec@5=89.233 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=23:24 IST
=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.493 DataTime=0.297 Loss=1.197 Prec@1=70.453 Prec@5=89.233 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=23:25 IST
=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.490 DataTime=0.295 Loss=1.198 Prec@1=70.408 Prec@5=89.254 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=23:25 IST
=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.490 DataTime=0.295 Loss=1.198 Prec@1=70.408 Prec@5=89.254 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=23:25 IST
=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.490 DataTime=0.295 Loss=1.198 Prec@1=70.408 Prec@5=89.254 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=23:25 IST
=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.487 DataTime=0.292 Loss=1.198 Prec@1=70.350 Prec@5=89.270 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=23:25 IST
=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.487 DataTime=0.292 Loss=1.198 Prec@1=70.350 Prec@5=89.270 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:25 IST
=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.487 DataTime=0.292 Loss=1.198 Prec@1=70.350 Prec@5=89.270 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:26 IST
=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.201 Prec@1=70.278 Prec@5=89.264 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=23:26 IST
=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.201 Prec@1=70.278 Prec@5=89.264 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=23:26 IST
=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.201 Prec@1=70.278 Prec@5=89.264 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=23:27 IST
=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.204 Prec@1=70.204 Prec@5=89.218 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=23:27 IST
=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.204 Prec@1=70.204 Prec@5=89.218 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=23:27 IST
=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.485 DataTime=0.290 Loss=1.204 Prec@1=70.204 Prec@5=89.218 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=23:28 IST
=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.483 DataTime=0.288 Loss=1.205 Prec@1=70.143 Prec@5=89.222 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=23:28 IST
=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.483 DataTime=0.288 Loss=1.205 Prec@1=70.143 Prec@5=89.222 rate=2.10 Hz, eta=0:13:30, total=0:06:21, wall=23:28 IST
=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.483 DataTime=0.288 Loss=1.205 Prec@1=70.143 Prec@5=89.222 rate=2.10 Hz, eta=0:13:30, total=0:06:21, wall=23:29 IST
=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.481 DataTime=0.286 Loss=1.205 Prec@1=70.114 Prec@5=89.213 rate=2.10 Hz, eta=0:13:30, total=0:06:21, wall=23:29 IST
=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.481 DataTime=0.286 Loss=1.205 Prec@1=70.114 Prec@5=89.213 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=23:29 IST
=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.481 DataTime=0.286 Loss=1.205 Prec@1=70.114 Prec@5=89.213 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=23:29 IST
=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.480 DataTime=0.285 Loss=1.206 Prec@1=70.094 Prec@5=89.211 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=23:29 IST
=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.480 DataTime=0.285 Loss=1.206 Prec@1=70.094 Prec@5=89.211 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=23:29 IST
=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.480 DataTime=0.285 Loss=1.206 Prec@1=70.094 Prec@5=89.211 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=23:30 IST
=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.479 DataTime=0.284 Loss=1.206 Prec@1=70.093 Prec@5=89.220 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=23:30 IST
=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.479 DataTime=0.284 Loss=1.206 Prec@1=70.093 Prec@5=89.220 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=23:30 IST
=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.479 DataTime=0.284 Loss=1.206 Prec@1=70.093 Prec@5=89.220 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=23:31 IST
=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.478 DataTime=0.283 Loss=1.208 Prec@1=70.093 Prec@5=89.202 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=23:31 IST
=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.478 DataTime=0.283 Loss=1.208 Prec@1=70.093 Prec@5=89.202 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=23:31 IST
=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.478 DataTime=0.283 Loss=1.208 Prec@1=70.093 Prec@5=89.202 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=23:32 IST
=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.282 Loss=1.209 Prec@1=70.074 Prec@5=89.181 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=23:32 IST
=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.282 Loss=1.209 Prec@1=70.074 Prec@5=89.181 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=23:32 IST
=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.282 Loss=1.209 Prec@1=70.074 Prec@5=89.181 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=23:33 IST
=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.281 Loss=1.210 Prec@1=70.032 Prec@5=89.149 rate=2.11 Hz, eta=0:09:28, total=0:10:15, wall=23:33 IST
=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.281 Loss=1.210 Prec@1=70.032 Prec@5=89.149 rate=2.11 Hz, eta=0:08:41, total=0:11:02, wall=23:33 IST
=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.477 DataTime=0.281 Loss=1.210 Prec@1=70.032 Prec@5=89.149 rate=2.11 Hz, eta=0:08:41, total=0:11:02, wall=23:33 IST
=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.281 Loss=1.212 Prec@1=69.991 Prec@5=89.136 rate=2.11 Hz, eta=0:08:41, total=0:11:02, wall=23:33 IST
=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.281 Loss=1.212 Prec@1=69.991 Prec@5=89.136 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=23:33 IST
=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.281 Loss=1.212 Prec@1=69.991 Prec@5=89.136 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=23:34 IST
=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.280 Loss=1.214 Prec@1=69.964 Prec@5=89.115 rate=2.12 Hz, eta=0:07:53, total=0:11:49, wall=23:34 IST
=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.280 Loss=1.214 Prec@1=69.964 Prec@5=89.115 rate=2.12 Hz, eta=0:07:06, total=0:12:36, wall=23:34 IST
=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.476 DataTime=0.280 Loss=1.214 Prec@1=69.964 Prec@5=89.115 rate=2.12 Hz, eta=0:07:06, total=0:12:36, wall=23:35 IST
=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.280 Loss=1.214 Prec@1=69.947 Prec@5=89.098 rate=2.12 Hz, eta=0:07:06, total=0:12:36, wall=23:35 IST
=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.280 Loss=1.214 Prec@1=69.947 Prec@5=89.098 rate=2.12 Hz, eta=0:06:18, total=0:13:23, wall=23:35 IST
=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.280 Loss=1.214 Prec@1=69.947 Prec@5=89.098 rate=2.12 Hz, eta=0:06:18, total=0:13:23, wall=23:36 IST
=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.216 Prec@1=69.928 Prec@5=89.077 rate=2.12 Hz, eta=0:06:18, total=0:13:23, wall=23:36 IST
=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.216 Prec@1=69.928 Prec@5=89.077 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:36 IST
=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.216 Prec@1=69.928 Prec@5=89.077 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:36 IST
=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.217 Prec@1=69.926 Prec@5=89.063 rate=2.12 Hz, eta=0:05:31, total=0:14:09, wall=23:36 IST
=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.217 Prec@1=69.926 Prec@5=89.063 rate=2.12 Hz, eta=0:04:44, total=0:14:57, wall=23:36 IST
=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.217 Prec@1=69.926 Prec@5=89.063 rate=2.12 Hz, eta=0:04:44, total=0:14:57, wall=23:37 IST
=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.218 Prec@1=69.906 Prec@5=89.044 rate=2.12 Hz, eta=0:04:44, total=0:14:57, wall=23:37 IST
=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.218 Prec@1=69.906 Prec@5=89.044 rate=2.12 Hz, eta=0:03:56, total=0:15:44, wall=23:37 IST
=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.475 DataTime=0.279 Loss=1.218 Prec@1=69.906 Prec@5=89.044 rate=2.12 Hz, eta=0:03:56, total=0:15:44, wall=23:38 IST
=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.279 Loss=1.219 Prec@1=69.880 Prec@5=89.037 rate=2.12 Hz, eta=0:03:56, total=0:15:44, wall=23:38 IST
=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.279 Loss=1.219 Prec@1=69.880 Prec@5=89.037 rate=2.12 Hz, eta=0:03:09, total=0:16:31, wall=23:38 IST
=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.279 Loss=1.219 Prec@1=69.880 Prec@5=89.037 rate=2.12 Hz, eta=0:03:09, total=0:16:31, wall=23:39 IST
=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.278 Loss=1.220 Prec@1=69.862 Prec@5=89.024 rate=2.12 Hz, eta=0:03:09, total=0:16:31, wall=23:39 IST
=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.278 Loss=1.220 Prec@1=69.862 Prec@5=89.024 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=23:39 IST
=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.474 DataTime=0.278 Loss=1.220 Prec@1=69.862 Prec@5=89.024 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=23:40 IST
=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.278 Loss=1.220 Prec@1=69.847 Prec@5=89.023 rate=2.12 Hz, eta=0:02:22, total=0:17:17, wall=23:40 IST
=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.278 Loss=1.220 Prec@1=69.847 Prec@5=89.023 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=23:40 IST
=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.278 Loss=1.220 Prec@1=69.847 Prec@5=89.023 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=23:40 IST
=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.837 Prec@5=89.014 rate=2.12 Hz, eta=0:01:35, total=0:18:03, wall=23:40 IST
=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.837 Prec@5=89.014 rate=2.12 Hz, eta=0:00:48, total=0:18:50, wall=23:40 IST
=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.837 Prec@5=89.014 rate=2.12 Hz, eta=0:00:48, total=0:18:50, wall=23:41 IST
=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.822 Prec@5=89.016 rate=2.12 Hz, eta=0:00:48, total=0:18:50, wall=23:41 IST
=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.822 Prec@5=89.016 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=23:41 IST
=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.822 Prec@5=89.016 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=23:41 IST
=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.822 Prec@5=89.015 rate=2.12 Hz, eta=0:00:00, total=0:19:37, wall=23:41 IST
=> training   100.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.473 DataTime=0.277 Loss=1.221 Prec@1=69.822 Prec@5=89.015 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=23:41 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST
=> validation 0.00% of 1x98...Epoch=94/150 LR=0.03159 Time=6.580 Loss=0.800 Prec@1=81.055 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST
=> validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=6.580 Loss=0.800 Prec@1=81.055 Prec@5=94.531 rate=4614.25 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST
** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=6.580 Loss=0.800 Prec@1=81.055 Prec@5=94.531 rate=4614.25 Hz, eta=0:00:00, total=0:00:00, wall=23:42 IST
** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=0.550 Loss=1.333 Prec@1=67.452 Prec@5=87.898 rate=4614.25 Hz, eta=0:00:00, total=0:00:00, wall=23:42 IST
** validation 100.00% of 1x98...Epoch=94/150 LR=0.03159 Time=0.550 Loss=1.333 Prec@1=67.452 Prec@5=87.898 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=23:42 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST
=> training   0.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=4.635 DataTime=4.364 Loss=1.127 Prec@1=72.852 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST
=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=4.635 DataTime=4.364 Loss=1.127 Prec@1=72.852 Prec@5=91.406 rate=3813.47 Hz, eta=0:00:00, total=0:00:00, wall=23:42 IST
=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=4.635 DataTime=4.364 Loss=1.127 Prec@1=72.852 Prec@5=91.406 rate=3813.47 Hz, eta=0:00:00, total=0:00:00, wall=23:43 IST
=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.525 DataTime=0.331 Loss=1.179 Prec@1=70.912 Prec@5=89.668 rate=3813.47 Hz, eta=0:00:00, total=0:00:00, wall=23:43 IST
=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.525 DataTime=0.331 Loss=1.179 Prec@1=70.912 Prec@5=89.668 rate=2.09 Hz, eta=0:19:11, total=0:00:48, wall=23:43 IST
=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.525 DataTime=0.331 Loss=1.179 Prec@1=70.912 Prec@5=89.668 rate=2.09 Hz, eta=0:19:11, total=0:00:48, wall=23:44 IST
=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.503 DataTime=0.308 Loss=1.181 Prec@1=70.815 Prec@5=89.623 rate=2.09 Hz, eta=0:19:11, total=0:00:48, wall=23:44 IST
=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.503 DataTime=0.308 Loss=1.181 Prec@1=70.815 Prec@5=89.623 rate=2.08 Hz, eta=0:18:24, total=0:01:36, wall=23:44 IST
=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.503 DataTime=0.308 Loss=1.181 Prec@1=70.815 Prec@5=89.623 rate=2.08 Hz, eta=0:18:24, total=0:01:36, wall=23:45 IST
=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.496 DataTime=0.300 Loss=1.185 Prec@1=70.734 Prec@5=89.552 rate=2.08 Hz, eta=0:18:24, total=0:01:36, wall=23:45 IST
=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.496 DataTime=0.300 Loss=1.185 Prec@1=70.734 Prec@5=89.552 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=23:45 IST
=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.496 DataTime=0.300 Loss=1.185 Prec@1=70.734 Prec@5=89.552 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=23:45 IST
=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.495 DataTime=0.299 Loss=1.185 Prec@1=70.740 Prec@5=89.572 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=23:45 IST
=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.495 DataTime=0.299 Loss=1.185 Prec@1=70.740 Prec@5=89.572 rate=2.07 Hz, eta=0:16:56, total=0:03:13, wall=23:45 IST
=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.495 DataTime=0.299 Loss=1.185 Prec@1=70.740 Prec@5=89.572 rate=2.07 Hz, eta=0:16:56, total=0:03:13, wall=23:46 IST
=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.187 Prec@1=70.696 Prec@5=89.534 rate=2.07 Hz, eta=0:16:56, total=0:03:13, wall=23:46 IST
=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.187 Prec@1=70.696 Prec@5=89.534 rate=2.07 Hz, eta=0:16:08, total=0:04:02, wall=23:46 IST
=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.187 Prec@1=70.696 Prec@5=89.534 rate=2.07 Hz, eta=0:16:08, total=0:04:02, wall=23:47 IST
=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.189 Prec@1=70.634 Prec@5=89.499 rate=2.07 Hz, eta=0:16:08, total=0:04:02, wall=23:47 IST
=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.189 Prec@1=70.634 Prec@5=89.499 rate=2.06 Hz, eta=0:15:23, total=0:04:51, wall=23:47 IST
=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.493 DataTime=0.298 Loss=1.189 Prec@1=70.634 Prec@5=89.499 rate=2.06 Hz, eta=0:15:23, total=0:04:51, wall=23:48 IST
=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.491 DataTime=0.296 Loss=1.191 Prec@1=70.567 Prec@5=89.469 rate=2.06 Hz, eta=0:15:23, total=0:04:51, wall=23:48 IST
=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.491 DataTime=0.296 Loss=1.191 Prec@1=70.567 Prec@5=89.469 rate=2.06 Hz, eta=0:14:32, total=0:05:39, wall=23:48 IST
=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.491 DataTime=0.296 Loss=1.191 Prec@1=70.567 Prec@5=89.469 rate=2.06 Hz, eta=0:14:32, total=0:05:39, wall=23:49 IST
=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.490 DataTime=0.295 Loss=1.194 Prec@1=70.480 Prec@5=89.420 rate=2.06 Hz, eta=0:14:32, total=0:05:39, wall=23:49 IST
=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.490 DataTime=0.295 Loss=1.194 Prec@1=70.480 Prec@5=89.420 rate=2.07 Hz, eta=0:13:43, total=0:06:27, wall=23:49 IST
=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.490 DataTime=0.295 Loss=1.194 Prec@1=70.480 Prec@5=89.420 rate=2.07 Hz, eta=0:13:43, total=0:06:27, wall=23:49 IST
=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.420 Prec@5=89.399 rate=2.07 Hz, eta=0:13:43, total=0:06:27, wall=23:49 IST
=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.420 Prec@5=89.399 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=23:49 IST
=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.420 Prec@5=89.399 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=23:50 IST
=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.197 Prec@1=70.357 Prec@5=89.389 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=23:50 IST
=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.197 Prec@1=70.357 Prec@5=89.389 rate=2.06 Hz, eta=0:12:07, total=0:08:05, wall=23:50 IST
=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.197 Prec@1=70.357 Prec@5=89.389 rate=2.06 Hz, eta=0:12:07, total=0:08:05, wall=23:51 IST
=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.199 Prec@1=70.300 Prec@5=89.357 rate=2.06 Hz, eta=0:12:07, total=0:08:05, wall=23:51 IST
=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.199 Prec@1=70.300 Prec@5=89.357 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=23:51 IST
=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.199 Prec@1=70.300 Prec@5=89.357 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=23:52 IST
=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.263 Prec@5=89.337 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=23:52 IST
=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.263 Prec@5=89.337 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=23:52 IST
=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.263 Prec@5=89.337 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=23:53 IST
=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.268 Prec@5=89.337 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=23:53 IST
=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.268 Prec@5=89.337 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=23:53 IST
=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.268 Prec@5=89.337 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=23:53 IST
=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.272 Prec@5=89.326 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=23:53 IST
=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.272 Prec@5=89.326 rate=2.06 Hz, eta=0:08:54, total=0:11:20, wall=23:53 IST
=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.201 Prec@1=70.272 Prec@5=89.326 rate=2.06 Hz, eta=0:08:54, total=0:11:20, wall=23:54 IST
=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.203 Prec@1=70.229 Prec@5=89.301 rate=2.06 Hz, eta=0:08:54, total=0:11:20, wall=23:54 IST
=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.203 Prec@1=70.229 Prec@5=89.301 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=23:54 IST
=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.203 Prec@1=70.229 Prec@5=89.301 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=23:55 IST
=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.295 Loss=1.203 Prec@1=70.208 Prec@5=89.286 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=23:55 IST
=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.295 Loss=1.203 Prec@1=70.208 Prec@5=89.286 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=23:55 IST
=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.295 Loss=1.203 Prec@1=70.208 Prec@5=89.286 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=23:56 IST
=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.196 Prec@5=89.266 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=23:56 IST
=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.196 Prec@5=89.266 rate=2.06 Hz, eta=0:06:29, total=0:13:47, wall=23:56 IST
=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.196 Prec@5=89.266 rate=2.06 Hz, eta=0:06:29, total=0:13:47, wall=23:57 IST
=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.186 Prec@5=89.255 rate=2.06 Hz, eta=0:06:29, total=0:13:47, wall=23:57 IST
=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.186 Prec@5=89.255 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=23:57 IST
=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.205 Prec@1=70.186 Prec@5=89.255 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=23:58 IST
=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.206 Prec@1=70.178 Prec@5=89.243 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=23:58 IST
=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.206 Prec@1=70.178 Prec@5=89.243 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=23:58 IST
=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.206 Prec@1=70.178 Prec@5=89.243 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=23:58 IST
=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.207 Prec@1=70.150 Prec@5=89.224 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=23:58 IST
=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.207 Prec@1=70.150 Prec@5=89.224 rate=2.06 Hz, eta=0:04:04, total=0:16:13, wall=23:58 IST
=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.207 Prec@1=70.150 Prec@5=89.224 rate=2.06 Hz, eta=0:04:04, total=0:16:13, wall=23:59 IST
=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.208 Prec@1=70.129 Prec@5=89.211 rate=2.06 Hz, eta=0:04:04, total=0:16:13, wall=23:59 IST
=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.208 Prec@1=70.129 Prec@5=89.211 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=23:59 IST
=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.208 Prec@1=70.129 Prec@5=89.211 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:00 IST
=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.208 Prec@1=70.134 Prec@5=89.219 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:00 IST
=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.208 Prec@1=70.134 Prec@5=89.219 rate=2.06 Hz, eta=0:02:26, total=0:17:51, wall=00:00 IST
=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.489 DataTime=0.294 Loss=1.208 Prec@1=70.134 Prec@5=89.219 rate=2.06 Hz, eta=0:02:26, total=0:17:51, wall=00:01 IST
=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.209 Prec@1=70.111 Prec@5=89.198 rate=2.06 Hz, eta=0:02:26, total=0:17:51, wall=00:01 IST
=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.209 Prec@1=70.111 Prec@5=89.198 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:01 IST
=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.209 Prec@1=70.111 Prec@5=89.198 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:02 IST
=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.210 Prec@1=70.090 Prec@5=89.182 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:02 IST
=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.210 Prec@1=70.090 Prec@5=89.182 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:02 IST
=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.210 Prec@1=70.090 Prec@5=89.182 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:02 IST
=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.211 Prec@1=70.079 Prec@5=89.164 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:02 IST
=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.211 Prec@1=70.079 Prec@5=89.164 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:02 IST
=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.211 Prec@1=70.079 Prec@5=89.164 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:02 IST
=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.211 Prec@1=70.079 Prec@5=89.164 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:02 IST
=> training   100.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.488 DataTime=0.293 Loss=1.211 Prec@1=70.079 Prec@5=89.164 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:02 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> validation 0.00% of 1x98...Epoch=95/150 LR=0.03062 Time=6.891 Loss=0.802 Prec@1=77.930 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.891 Loss=0.802 Prec@1=77.930 Prec@5=94.727 rate=7551.96 Hz, eta=0:00:00, total=0:00:00, wall=00:03 IST
** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.891 Loss=0.802 Prec@1=77.930 Prec@5=94.727 rate=7551.96 Hz, eta=0:00:00, total=0:00:00, wall=00:03 IST
** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=0.560 Loss=1.344 Prec@1=67.188 Prec@5=87.762 rate=7551.96 Hz, eta=0:00:00, total=0:00:00, wall=00:03 IST
** validation 100.00% of 1x98...Epoch=95/150 LR=0.03062 Time=0.560 Loss=1.344 Prec@1=67.188 Prec@5=87.762 rate=2.04 Hz, eta=0:00:00, total=0:00:48, wall=00:03 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> training   0.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=4.699 DataTime=4.437 Loss=1.171 Prec@1=69.727 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=00:03 IST
=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=4.699 DataTime=4.437 Loss=1.171 Prec@1=69.727 Prec@5=88.867 rate=6595.74 Hz, eta=0:00:00, total=0:00:00, wall=00:03 IST
=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=4.699 DataTime=4.437 Loss=1.171 Prec@1=69.727 Prec@5=88.867 rate=6595.74 Hz, eta=0:00:00, total=0:00:00, wall=00:04 IST
=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.508 DataTime=0.312 Loss=1.180 Prec@1=71.011 Prec@5=89.573 rate=6595.74 Hz, eta=0:00:00, total=0:00:00, wall=00:04 IST
=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.508 DataTime=0.312 Loss=1.180 Prec@1=71.011 Prec@5=89.573 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=00:04 IST
=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.508 DataTime=0.312 Loss=1.180 Prec@1=71.011 Prec@5=89.573 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=00:05 IST
=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.499 DataTime=0.301 Loss=1.184 Prec@1=70.778 Prec@5=89.487 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=00:05 IST
=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.499 DataTime=0.301 Loss=1.184 Prec@1=70.778 Prec@5=89.487 rate=2.10 Hz, eta=0:18:14, total=0:01:35, wall=00:05 IST
=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.499 DataTime=0.301 Loss=1.184 Prec@1=70.778 Prec@5=89.487 rate=2.10 Hz, eta=0:18:14, total=0:01:35, wall=00:06 IST
=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.496 DataTime=0.299 Loss=1.179 Prec@1=70.836 Prec@5=89.564 rate=2.10 Hz, eta=0:18:14, total=0:01:35, wall=00:06 IST
=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.496 DataTime=0.299 Loss=1.179 Prec@1=70.836 Prec@5=89.564 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=00:06 IST
=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.496 DataTime=0.299 Loss=1.179 Prec@1=70.836 Prec@5=89.564 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=00:07 IST
=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.494 DataTime=0.297 Loss=1.182 Prec@1=70.753 Prec@5=89.517 rate=2.08 Hz, eta=0:17:38, total=0:02:24, wall=00:07 IST
=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.494 DataTime=0.297 Loss=1.182 Prec@1=70.753 Prec@5=89.517 rate=2.07 Hz, eta=0:16:53, total=0:03:13, wall=00:07 IST
=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.494 DataTime=0.297 Loss=1.182 Prec@1=70.753 Prec@5=89.517 rate=2.07 Hz, eta=0:16:53, total=0:03:13, wall=00:07 IST
=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.492 DataTime=0.295 Loss=1.183 Prec@1=70.707 Prec@5=89.522 rate=2.07 Hz, eta=0:16:53, total=0:03:13, wall=00:07 IST
=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.492 DataTime=0.295 Loss=1.183 Prec@1=70.707 Prec@5=89.522 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:07 IST
=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.492 DataTime=0.295 Loss=1.183 Prec@1=70.707 Prec@5=89.522 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:08 IST
=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.185 Prec@1=70.670 Prec@5=89.487 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:08 IST
=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.185 Prec@1=70.670 Prec@5=89.487 rate=2.07 Hz, eta=0:15:17, total=0:04:49, wall=00:08 IST
=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.185 Prec@1=70.670 Prec@5=89.487 rate=2.07 Hz, eta=0:15:17, total=0:04:49, wall=00:09 IST
=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.293 Loss=1.186 Prec@1=70.613 Prec@5=89.472 rate=2.07 Hz, eta=0:15:17, total=0:04:49, wall=00:09 IST
=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.293 Loss=1.186 Prec@1=70.613 Prec@5=89.472 rate=2.08 Hz, eta=0:14:28, total=0:05:37, wall=00:09 IST
=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.293 Loss=1.186 Prec@1=70.613 Prec@5=89.472 rate=2.08 Hz, eta=0:14:28, total=0:05:37, wall=00:10 IST
=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.487 DataTime=0.291 Loss=1.186 Prec@1=70.629 Prec@5=89.457 rate=2.08 Hz, eta=0:14:28, total=0:05:37, wall=00:10 IST
=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.487 DataTime=0.291 Loss=1.186 Prec@1=70.629 Prec@5=89.457 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=00:10 IST
=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.487 DataTime=0.291 Loss=1.186 Prec@1=70.629 Prec@5=89.457 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=00:11 IST
=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.187 Prec@1=70.591 Prec@5=89.450 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=00:11 IST
=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.187 Prec@1=70.591 Prec@5=89.450 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:11 IST
=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.187 Prec@1=70.591 Prec@5=89.450 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:12 IST
=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.560 Prec@5=89.466 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:12 IST
=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.560 Prec@5=89.466 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:12 IST
=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.560 Prec@5=89.466 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:12 IST
=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.189 Prec@1=70.536 Prec@5=89.434 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:12 IST
=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.189 Prec@1=70.536 Prec@5=89.434 rate=2.06 Hz, eta=0:11:20, total=0:08:54, wall=00:12 IST
=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.189 Prec@1=70.536 Prec@5=89.434 rate=2.06 Hz, eta=0:11:20, total=0:08:54, wall=00:13 IST
=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.191 Prec@1=70.489 Prec@5=89.409 rate=2.06 Hz, eta=0:11:20, total=0:08:54, wall=00:13 IST
=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.191 Prec@1=70.489 Prec@5=89.409 rate=2.06 Hz, eta=0:10:32, total=0:09:43, wall=00:13 IST
=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.191 Prec@1=70.489 Prec@5=89.409 rate=2.06 Hz, eta=0:10:32, total=0:09:43, wall=00:14 IST
=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.192 Prec@1=70.454 Prec@5=89.400 rate=2.06 Hz, eta=0:10:32, total=0:09:43, wall=00:14 IST
=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.192 Prec@1=70.454 Prec@5=89.400 rate=2.06 Hz, eta=0:09:44, total=0:10:32, wall=00:14 IST
=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.490 DataTime=0.294 Loss=1.192 Prec@1=70.454 Prec@5=89.400 rate=2.06 Hz, eta=0:09:44, total=0:10:32, wall=00:15 IST
=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.194 Prec@1=70.409 Prec@5=89.362 rate=2.06 Hz, eta=0:09:44, total=0:10:32, wall=00:15 IST
=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.194 Prec@1=70.409 Prec@5=89.362 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:15 IST
=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.194 Prec@1=70.409 Prec@5=89.362 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:16 IST
=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.390 Prec@5=89.337 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:16 IST
=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.390 Prec@5=89.337 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=00:16 IST
=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.294 Loss=1.195 Prec@1=70.390 Prec@5=89.337 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=00:16 IST
=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.196 Prec@1=70.358 Prec@5=89.311 rate=2.06 Hz, eta=0:08:07, total=0:12:09, wall=00:16 IST
=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.196 Prec@1=70.358 Prec@5=89.311 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:16 IST
=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.196 Prec@1=70.358 Prec@5=89.311 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:17 IST
=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.198 Prec@1=70.324 Prec@5=89.288 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:17 IST
=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.198 Prec@1=70.324 Prec@5=89.288 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:17 IST
=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.198 Prec@1=70.324 Prec@5=89.288 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:18 IST
=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.199 Prec@1=70.308 Prec@5=89.281 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:18 IST
=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.199 Prec@1=70.308 Prec@5=89.281 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:18 IST
=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.199 Prec@1=70.308 Prec@5=89.281 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:19 IST
=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.200 Prec@1=70.284 Prec@5=89.254 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:19 IST
=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.200 Prec@1=70.284 Prec@5=89.254 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=00:19 IST
=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.200 Prec@1=70.284 Prec@5=89.254 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=00:20 IST
=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.201 Prec@1=70.267 Prec@5=89.248 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=00:20 IST
=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.201 Prec@1=70.267 Prec@5=89.248 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=00:20 IST
=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.201 Prec@1=70.267 Prec@5=89.248 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=00:20 IST
=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.202 Prec@1=70.239 Prec@5=89.237 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=00:20 IST
=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.202 Prec@1=70.239 Prec@5=89.237 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:20 IST
=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.202 Prec@1=70.239 Prec@5=89.237 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:21 IST
=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.203 Prec@1=70.212 Prec@5=89.226 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:21 IST
=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.203 Prec@1=70.212 Prec@5=89.226 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:21 IST
=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.489 DataTime=0.293 Loss=1.203 Prec@1=70.212 Prec@5=89.226 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:22 IST
=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.204 Prec@1=70.207 Prec@5=89.215 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:22 IST
=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.204 Prec@1=70.207 Prec@5=89.215 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:22 IST
=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.204 Prec@1=70.207 Prec@5=89.215 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:23 IST
=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.181 Prec@5=89.202 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=00:23 IST
=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.181 Prec@5=89.202 rate=2.06 Hz, eta=0:00:49, total=0:19:27, wall=00:23 IST
=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.181 Prec@5=89.202 rate=2.06 Hz, eta=0:00:49, total=0:19:27, wall=00:24 IST
=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.162 Prec@5=89.189 rate=2.06 Hz, eta=0:00:49, total=0:19:27, wall=00:24 IST
=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.162 Prec@5=89.189 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:24 IST
=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.162 Prec@5=89.189 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:24 IST
=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.163 Prec@5=89.189 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=00:24 IST
=> training   100.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.488 DataTime=0.292 Loss=1.205 Prec@1=70.163 Prec@5=89.189 rate=2.06 Hz, eta=0:00:00, total=0:20:16, wall=00:24 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> validation 0.00% of 1x98...Epoch=96/150 LR=0.02966 Time=7.203 Loss=0.809 Prec@1=79.883 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=7.203 Loss=0.809 Prec@1=79.883 Prec@5=94.141 rate=6845.98 Hz, eta=0:00:00, total=0:00:00, wall=00:24 IST
** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=7.203 Loss=0.809 Prec@1=79.883 Prec@5=94.141 rate=6845.98 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=0.562 Loss=1.316 Prec@1=67.960 Prec@5=88.090 rate=6845.98 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
** validation 100.00% of 1x98...Epoch=96/150 LR=0.02966 Time=0.562 Loss=1.316 Prec@1=67.960 Prec@5=88.090 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=00:25 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> training   0.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=4.432 DataTime=4.147 Loss=1.242 Prec@1=67.969 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=4.432 DataTime=4.147 Loss=1.242 Prec@1=67.969 Prec@5=89.062 rate=5200.26 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=4.432 DataTime=4.147 Loss=1.242 Prec@1=67.969 Prec@5=89.062 rate=5200.26 Hz, eta=0:00:00, total=0:00:00, wall=00:26 IST
=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.513 DataTime=0.318 Loss=1.165 Prec@1=71.011 Prec@5=89.687 rate=5200.26 Hz, eta=0:00:00, total=0:00:00, wall=00:26 IST
=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.513 DataTime=0.318 Loss=1.165 Prec@1=71.011 Prec@5=89.687 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=00:26 IST
=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.513 DataTime=0.318 Loss=1.165 Prec@1=71.011 Prec@5=89.687 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=00:26 IST
=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.496 DataTime=0.300 Loss=1.167 Prec@1=70.969 Prec@5=89.702 rate=2.13 Hz, eta=0:18:46, total=0:00:47, wall=00:26 IST
=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.496 DataTime=0.300 Loss=1.167 Prec@1=70.969 Prec@5=89.702 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=00:26 IST
=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.496 DataTime=0.300 Loss=1.167 Prec@1=70.969 Prec@5=89.702 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=00:27 IST
=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.492 DataTime=0.296 Loss=1.169 Prec@1=70.980 Prec@5=89.645 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=00:27 IST
=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.492 DataTime=0.296 Loss=1.169 Prec@1=70.980 Prec@5=89.645 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=00:27 IST
=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.492 DataTime=0.296 Loss=1.169 Prec@1=70.980 Prec@5=89.645 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=00:28 IST
=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.295 Loss=1.173 Prec@1=70.907 Prec@5=89.591 rate=2.10 Hz, eta=0:17:30, total=0:02:23, wall=00:28 IST
=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.295 Loss=1.173 Prec@1=70.907 Prec@5=89.591 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:28 IST
=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.295 Loss=1.173 Prec@1=70.907 Prec@5=89.591 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:29 IST
=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.296 Loss=1.174 Prec@1=70.896 Prec@5=89.597 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:29 IST
=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.296 Loss=1.174 Prec@1=70.896 Prec@5=89.597 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:29 IST
=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.296 Loss=1.174 Prec@1=70.896 Prec@5=89.597 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:30 IST
=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.297 Loss=1.175 Prec@1=70.859 Prec@5=89.591 rate=2.07 Hz, eta=0:16:05, total=0:04:01, wall=00:30 IST
=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.297 Loss=1.175 Prec@1=70.859 Prec@5=89.591 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=00:30 IST
=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.491 DataTime=0.297 Loss=1.175 Prec@1=70.859 Prec@5=89.591 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=00:30 IST
=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.296 Loss=1.176 Prec@1=70.854 Prec@5=89.607 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=00:30 IST
=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.296 Loss=1.176 Prec@1=70.854 Prec@5=89.607 rate=2.07 Hz, eta=0:14:32, total=0:05:39, wall=00:30 IST
=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.296 Loss=1.176 Prec@1=70.854 Prec@5=89.607 rate=2.07 Hz, eta=0:14:32, total=0:05:39, wall=00:31 IST
=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.295 Loss=1.176 Prec@1=70.865 Prec@5=89.596 rate=2.07 Hz, eta=0:14:32, total=0:05:39, wall=00:31 IST
=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.295 Loss=1.176 Prec@1=70.865 Prec@5=89.596 rate=2.07 Hz, eta=0:13:44, total=0:06:27, wall=00:31 IST
=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.490 DataTime=0.295 Loss=1.176 Prec@1=70.865 Prec@5=89.596 rate=2.07 Hz, eta=0:13:44, total=0:06:27, wall=00:32 IST
=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.179 Prec@1=70.795 Prec@5=89.551 rate=2.07 Hz, eta=0:13:44, total=0:06:27, wall=00:32 IST
=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.179 Prec@1=70.795 Prec@5=89.551 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:32 IST
=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.179 Prec@1=70.795 Prec@5=89.551 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:33 IST
=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.180 Prec@1=70.750 Prec@5=89.542 rate=2.07 Hz, eta=0:12:55, total=0:07:16, wall=00:33 IST
=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.180 Prec@1=70.750 Prec@5=89.542 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:33 IST
=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.180 Prec@1=70.750 Prec@5=89.542 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:34 IST
=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.182 Prec@1=70.711 Prec@5=89.517 rate=2.06 Hz, eta=0:12:07, total=0:08:04, wall=00:34 IST
=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.182 Prec@1=70.711 Prec@5=89.517 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=00:34 IST
=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.182 Prec@1=70.711 Prec@5=89.517 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=00:34 IST
=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.184 Prec@1=70.652 Prec@5=89.493 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=00:34 IST
=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.184 Prec@1=70.652 Prec@5=89.493 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=00:34 IST
=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.184 Prec@1=70.652 Prec@5=89.493 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=00:35 IST
=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.186 Prec@1=70.601 Prec@5=89.473 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=00:35 IST
=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.186 Prec@1=70.601 Prec@5=89.473 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=00:35 IST
=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.186 Prec@1=70.601 Prec@5=89.473 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=00:36 IST
=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.582 Prec@5=89.470 rate=2.06 Hz, eta=0:09:43, total=0:10:31, wall=00:36 IST
=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.582 Prec@5=89.470 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:36 IST
=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.294 Loss=1.187 Prec@1=70.582 Prec@5=89.470 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:37 IST
=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.293 Loss=1.188 Prec@1=70.566 Prec@5=89.457 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=00:37 IST
=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.293 Loss=1.188 Prec@1=70.566 Prec@5=89.457 rate=2.06 Hz, eta=0:08:06, total=0:12:08, wall=00:37 IST
=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.489 DataTime=0.293 Loss=1.188 Prec@1=70.566 Prec@5=89.457 rate=2.06 Hz, eta=0:08:06, total=0:12:08, wall=00:38 IST
=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.189 Prec@1=70.527 Prec@5=89.436 rate=2.06 Hz, eta=0:08:06, total=0:12:08, wall=00:38 IST
=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.189 Prec@1=70.527 Prec@5=89.436 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:38 IST
=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.189 Prec@1=70.527 Prec@5=89.436 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:38 IST
=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.506 Prec@5=89.425 rate=2.06 Hz, eta=0:07:18, total=0:12:57, wall=00:38 IST
=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.506 Prec@5=89.425 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:38 IST
=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.506 Prec@5=89.425 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:39 IST
=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.497 Prec@5=89.422 rate=2.06 Hz, eta=0:06:29, total=0:13:45, wall=00:39 IST
=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.497 Prec@5=89.422 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:39 IST
=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.190 Prec@1=70.497 Prec@5=89.422 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:40 IST
=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.191 Prec@1=70.485 Prec@5=89.420 rate=2.06 Hz, eta=0:05:40, total=0:14:34, wall=00:40 IST
=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.191 Prec@1=70.485 Prec@5=89.420 rate=2.06 Hz, eta=0:04:52, total=0:15:23, wall=00:40 IST
=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.191 Prec@1=70.485 Prec@5=89.420 rate=2.06 Hz, eta=0:04:52, total=0:15:23, wall=00:41 IST
=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.192 Prec@1=70.462 Prec@5=89.398 rate=2.06 Hz, eta=0:04:52, total=0:15:23, wall=00:41 IST
=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.192 Prec@1=70.462 Prec@5=89.398 rate=2.06 Hz, eta=0:04:04, total=0:16:12, wall=00:41 IST
=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.192 Prec@1=70.462 Prec@5=89.398 rate=2.06 Hz, eta=0:04:04, total=0:16:12, wall=00:42 IST
=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.193 Prec@1=70.439 Prec@5=89.389 rate=2.06 Hz, eta=0:04:04, total=0:16:12, wall=00:42 IST
=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.193 Prec@1=70.439 Prec@5=89.389 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:42 IST
=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.193 Prec@1=70.439 Prec@5=89.389 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:43 IST
=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.194 Prec@1=70.430 Prec@5=89.373 rate=2.06 Hz, eta=0:03:15, total=0:17:01, wall=00:43 IST
=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.194 Prec@1=70.430 Prec@5=89.373 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:43 IST
=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.488 DataTime=0.293 Loss=1.194 Prec@1=70.430 Prec@5=89.373 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:43 IST
=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.195 Prec@1=70.417 Prec@5=89.357 rate=2.06 Hz, eta=0:02:26, total=0:17:50, wall=00:43 IST
=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.195 Prec@1=70.417 Prec@5=89.357 rate=2.06 Hz, eta=0:01:38, total=0:18:37, wall=00:43 IST
=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.195 Prec@1=70.417 Prec@5=89.357 rate=2.06 Hz, eta=0:01:38, total=0:18:37, wall=00:44 IST
=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.196 Prec@1=70.386 Prec@5=89.341 rate=2.06 Hz, eta=0:01:38, total=0:18:37, wall=00:44 IST
=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.196 Prec@1=70.386 Prec@5=89.341 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:44 IST
=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.196 Prec@1=70.386 Prec@5=89.341 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:45 IST
=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.197 Prec@1=70.366 Prec@5=89.317 rate=2.06 Hz, eta=0:00:49, total=0:19:26, wall=00:45 IST
=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.197 Prec@1=70.366 Prec@5=89.317 rate=2.06 Hz, eta=0:00:00, total=0:20:13, wall=00:45 IST
=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.197 Prec@1=70.366 Prec@5=89.317 rate=2.06 Hz, eta=0:00:00, total=0:20:13, wall=00:45 IST
=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.197 Prec@1=70.366 Prec@5=89.317 rate=2.06 Hz, eta=0:00:00, total=0:20:13, wall=00:45 IST
=> training   100.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.487 DataTime=0.292 Loss=1.197 Prec@1=70.366 Prec@5=89.317 rate=2.06 Hz, eta=0:00:00, total=0:20:14, wall=00:45 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:45 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:45 IST
=> validation 0.00% of 1x98...Epoch=97/150 LR=0.02871 Time=6.019 Loss=0.817 Prec@1=79.297 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=00:45 IST
=> validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=6.019 Loss=0.817 Prec@1=79.297 Prec@5=95.117 rate=6976.66 Hz, eta=0:00:00, total=0:00:00, wall=00:45 IST
** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=6.019 Loss=0.817 Prec@1=79.297 Prec@5=95.117 rate=6976.66 Hz, eta=0:00:00, total=0:00:00, wall=00:46 IST
** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=0.545 Loss=1.308 Prec@1=67.970 Prec@5=88.326 rate=6976.66 Hz, eta=0:00:00, total=0:00:00, wall=00:46 IST
** validation 100.00% of 1x98...Epoch=97/150 LR=0.02871 Time=0.545 Loss=1.308 Prec@1=67.970 Prec@5=88.326 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=00:46 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST
=> training   0.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=4.284 DataTime=4.013 Loss=1.146 Prec@1=71.484 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST
=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=4.284 DataTime=4.013 Loss=1.146 Prec@1=71.484 Prec@5=90.039 rate=5645.16 Hz, eta=0:00:00, total=0:00:00, wall=00:46 IST
=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=4.284 DataTime=4.013 Loss=1.146 Prec@1=71.484 Prec@5=90.039 rate=5645.16 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST
=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.513 DataTime=0.317 Loss=1.168 Prec@1=71.119 Prec@5=89.515 rate=5645.16 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST
=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.513 DataTime=0.317 Loss=1.168 Prec@1=71.119 Prec@5=89.515 rate=2.12 Hz, eta=0:18:50, total=0:00:47, wall=00:47 IST
=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.513 DataTime=0.317 Loss=1.168 Prec@1=71.119 Prec@5=89.515 rate=2.12 Hz, eta=0:18:50, total=0:00:47, wall=00:48 IST
=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.498 DataTime=0.301 Loss=1.166 Prec@1=71.209 Prec@5=89.565 rate=2.12 Hz, eta=0:18:50, total=0:00:47, wall=00:48 IST
=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.498 DataTime=0.301 Loss=1.166 Prec@1=71.209 Prec@5=89.565 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=00:48 IST
=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.498 DataTime=0.301 Loss=1.166 Prec@1=71.209 Prec@5=89.565 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=00:48 IST
=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.492 DataTime=0.296 Loss=1.166 Prec@1=71.180 Prec@5=89.603 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=00:48 IST
=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.492 DataTime=0.296 Loss=1.166 Prec@1=71.180 Prec@5=89.603 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=00:48 IST
=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.492 DataTime=0.296 Loss=1.166 Prec@1=71.180 Prec@5=89.603 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=00:49 IST
=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.490 DataTime=0.294 Loss=1.166 Prec@1=71.112 Prec@5=89.628 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=00:49 IST
=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.490 DataTime=0.294 Loss=1.166 Prec@1=71.112 Prec@5=89.628 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:49 IST
=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.490 DataTime=0.294 Loss=1.166 Prec@1=71.112 Prec@5=89.628 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:50 IST
=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.488 DataTime=0.292 Loss=1.165 Prec@1=71.096 Prec@5=89.658 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=00:50 IST
=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.488 DataTime=0.292 Loss=1.165 Prec@1=71.096 Prec@5=89.658 rate=2.08 Hz, eta=0:16:00, total=0:04:00, wall=00:50 IST
=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.488 DataTime=0.292 Loss=1.165 Prec@1=71.096 Prec@5=89.658 rate=2.08 Hz, eta=0:16:00, total=0:04:00, wall=00:51 IST
=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.487 DataTime=0.291 Loss=1.166 Prec@1=71.048 Prec@5=89.628 rate=2.08 Hz, eta=0:16:00, total=0:04:00, wall=00:51 IST
=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.487 DataTime=0.291 Loss=1.166 Prec@1=71.048 Prec@5=89.628 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=00:51 IST
=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.487 DataTime=0.291 Loss=1.166 Prec@1=71.048 Prec@5=89.628 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=00:52 IST
=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.024 Prec@5=89.640 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=00:52 IST
=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.024 Prec@5=89.640 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=00:52 IST
=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.024 Prec@5=89.640 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=00:52 IST
=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.169 Prec@1=70.986 Prec@5=89.623 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=00:52 IST
=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.169 Prec@1=70.986 Prec@5=89.623 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=00:52 IST
=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.169 Prec@1=70.986 Prec@5=89.623 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=00:53 IST
=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.170 Prec@1=70.989 Prec@5=89.624 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=00:53 IST
=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.170 Prec@1=70.989 Prec@5=89.624 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=00:53 IST
=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.170 Prec@1=70.989 Prec@5=89.624 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=00:54 IST
=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.172 Prec@1=70.930 Prec@5=89.605 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=00:54 IST
=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.172 Prec@1=70.930 Prec@5=89.605 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=00:54 IST
=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.172 Prec@1=70.930 Prec@5=89.605 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=00:55 IST
=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.172 Prec@1=70.910 Prec@5=89.583 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=00:55 IST
=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.172 Prec@1=70.910 Prec@5=89.583 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=00:55 IST
=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.172 Prec@1=70.910 Prec@5=89.583 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=00:56 IST
=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.174 Prec@1=70.861 Prec@5=89.561 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=00:56 IST
=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.174 Prec@1=70.861 Prec@5=89.561 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=00:56 IST
=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.174 Prec@1=70.861 Prec@5=89.561 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=00:56 IST
=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.176 Prec@1=70.812 Prec@5=89.534 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=00:56 IST
=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.176 Prec@1=70.812 Prec@5=89.534 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=00:56 IST
=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.176 Prec@1=70.812 Prec@5=89.534 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=00:57 IST
=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.177 Prec@1=70.782 Prec@5=89.539 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=00:57 IST
=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.177 Prec@1=70.782 Prec@5=89.539 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=00:57 IST
=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.177 Prec@1=70.782 Prec@5=89.539 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=00:58 IST
=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.484 DataTime=0.289 Loss=1.178 Prec@1=70.760 Prec@5=89.525 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=00:58 IST
=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.484 DataTime=0.289 Loss=1.178 Prec@1=70.760 Prec@5=89.525 rate=2.08 Hz, eta=0:08:02, total=0:12:02, wall=00:58 IST
=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.484 DataTime=0.289 Loss=1.178 Prec@1=70.760 Prec@5=89.525 rate=2.08 Hz, eta=0:08:02, total=0:12:02, wall=00:59 IST
=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.179 Prec@1=70.749 Prec@5=89.524 rate=2.08 Hz, eta=0:08:02, total=0:12:02, wall=00:59 IST
=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.179 Prec@1=70.749 Prec@5=89.524 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=00:59 IST
=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.179 Prec@1=70.749 Prec@5=89.524 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=01:00 IST
=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.180 Prec@1=70.724 Prec@5=89.503 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=01:00 IST
=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.180 Prec@1=70.724 Prec@5=89.503 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=01:00 IST
=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.486 DataTime=0.290 Loss=1.180 Prec@1=70.724 Prec@5=89.503 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=01:00 IST
=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.181 Prec@1=70.686 Prec@5=89.499 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=01:00 IST
=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.181 Prec@1=70.686 Prec@5=89.499 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=01:00 IST
=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.181 Prec@1=70.686 Prec@5=89.499 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=01:01 IST
=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.182 Prec@1=70.675 Prec@5=89.495 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=01:01 IST
=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.182 Prec@1=70.675 Prec@5=89.495 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=01:01 IST
=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.182 Prec@1=70.675 Prec@5=89.495 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=01:02 IST
=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.183 Prec@1=70.659 Prec@5=89.483 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=01:02 IST
=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.183 Prec@1=70.659 Prec@5=89.483 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=01:02 IST
=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.183 Prec@1=70.659 Prec@5=89.483 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=01:03 IST
=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.184 Prec@1=70.625 Prec@5=89.461 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=01:03 IST
=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.184 Prec@1=70.625 Prec@5=89.461 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=01:03 IST
=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.184 Prec@1=70.625 Prec@5=89.461 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=01:04 IST
=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.185 Prec@1=70.611 Prec@5=89.452 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=01:04 IST
=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.185 Prec@1=70.611 Prec@5=89.452 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=01:04 IST
=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.185 Prec@1=70.611 Prec@5=89.452 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=01:05 IST
=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.186 Prec@1=70.602 Prec@5=89.445 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=01:05 IST
=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.186 Prec@1=70.602 Prec@5=89.445 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=01:05 IST
=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.289 Loss=1.186 Prec@1=70.602 Prec@5=89.445 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=01:05 IST
=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.186 Prec@1=70.594 Prec@5=89.438 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=01:05 IST
=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.186 Prec@1=70.594 Prec@5=89.438 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=01:05 IST
=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.186 Prec@1=70.594 Prec@5=89.438 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=01:06 IST
=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.188 Prec@1=70.568 Prec@5=89.421 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=01:06 IST
=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.188 Prec@1=70.568 Prec@5=89.421 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=01:06 IST
=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.188 Prec@1=70.568 Prec@5=89.421 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=01:06 IST
=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.188 Prec@1=70.567 Prec@5=89.421 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=01:06 IST
=> training   100.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.485 DataTime=0.290 Loss=1.188 Prec@1=70.567 Prec@5=89.421 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=01:06 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST
=> validation 0.00% of 1x98...Epoch=98/150 LR=0.02777 Time=6.961 Loss=0.764 Prec@1=79.688 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST
=> validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=6.961 Loss=0.764 Prec@1=79.688 Prec@5=93.555 rate=7647.30 Hz, eta=0:00:00, total=0:00:00, wall=01:06 IST
** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=6.961 Loss=0.764 Prec@1=79.688 Prec@5=93.555 rate=7647.30 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST
** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=0.558 Loss=1.317 Prec@1=67.844 Prec@5=88.282 rate=7647.30 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST
** validation 100.00% of 1x98...Epoch=98/150 LR=0.02777 Time=0.558 Loss=1.317 Prec@1=67.844 Prec@5=88.282 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=01:07 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> training   0.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.534 DataTime=4.279 Loss=1.085 Prec@1=73.242 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST
=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.534 DataTime=4.279 Loss=1.085 Prec@1=73.242 Prec@5=90.430 rate=7297.35 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST
=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.534 DataTime=4.279 Loss=1.085 Prec@1=73.242 Prec@5=90.430 rate=7297.35 Hz, eta=0:00:00, total=0:00:00, wall=01:08 IST
=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.504 DataTime=0.309 Loss=1.159 Prec@1=71.218 Prec@5=89.923 rate=7297.35 Hz, eta=0:00:00, total=0:00:00, wall=01:08 IST
=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.504 DataTime=0.309 Loss=1.159 Prec@1=71.218 Prec@5=89.923 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=01:08 IST
=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.504 DataTime=0.309 Loss=1.159 Prec@1=71.218 Prec@5=89.923 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=01:09 IST
=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.488 DataTime=0.292 Loss=1.159 Prec@1=71.056 Prec@5=89.808 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=01:09 IST
=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.488 DataTime=0.292 Loss=1.159 Prec@1=71.056 Prec@5=89.808 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=01:09 IST
=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.488 DataTime=0.292 Loss=1.159 Prec@1=71.056 Prec@5=89.808 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=01:10 IST
=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.482 DataTime=0.287 Loss=1.151 Prec@1=71.299 Prec@5=89.914 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=01:10 IST
=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.482 DataTime=0.287 Loss=1.151 Prec@1=71.299 Prec@5=89.914 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=01:10 IST
=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.482 DataTime=0.287 Loss=1.151 Prec@1=71.299 Prec@5=89.914 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=01:10 IST
=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.156 Prec@1=71.208 Prec@5=89.874 rate=2.14 Hz, eta=0:17:09, total=0:02:20, wall=01:10 IST
=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.156 Prec@1=71.208 Prec@5=89.874 rate=2.14 Hz, eta=0:16:23, total=0:03:07, wall=01:10 IST
=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.156 Prec@1=71.208 Prec@5=89.874 rate=2.14 Hz, eta=0:16:23, total=0:03:07, wall=01:11 IST
=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.477 DataTime=0.282 Loss=1.154 Prec@1=71.273 Prec@5=89.899 rate=2.14 Hz, eta=0:16:23, total=0:03:07, wall=01:11 IST
=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.477 DataTime=0.282 Loss=1.154 Prec@1=71.273 Prec@5=89.899 rate=2.14 Hz, eta=0:15:37, total=0:03:54, wall=01:11 IST
=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.477 DataTime=0.282 Loss=1.154 Prec@1=71.273 Prec@5=89.899 rate=2.14 Hz, eta=0:15:37, total=0:03:54, wall=01:12 IST
=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.156 Prec@1=71.219 Prec@5=89.856 rate=2.14 Hz, eta=0:15:37, total=0:03:54, wall=01:12 IST
=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.156 Prec@1=71.219 Prec@5=89.856 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=01:12 IST
=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.156 Prec@1=71.219 Prec@5=89.856 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=01:13 IST
=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.273 Prec@5=89.874 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=01:13 IST
=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.273 Prec@5=89.874 rate=2.11 Hz, eta=0:14:12, total=0:05:31, wall=01:13 IST
=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.273 Prec@5=89.874 rate=2.11 Hz, eta=0:14:12, total=0:05:31, wall=01:13 IST
=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.157 Prec@1=71.238 Prec@5=89.834 rate=2.11 Hz, eta=0:14:12, total=0:05:31, wall=01:13 IST
=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.157 Prec@1=71.238 Prec@5=89.834 rate=2.11 Hz, eta=0:13:24, total=0:06:18, wall=01:13 IST
=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.283 Loss=1.157 Prec@1=71.238 Prec@5=89.834 rate=2.11 Hz, eta=0:13:24, total=0:06:18, wall=01:14 IST
=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.159 Prec@1=71.184 Prec@5=89.806 rate=2.11 Hz, eta=0:13:24, total=0:06:18, wall=01:14 IST
=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.159 Prec@1=71.184 Prec@5=89.806 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=01:14 IST
=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.159 Prec@1=71.184 Prec@5=89.806 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=01:15 IST
=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.162 Prec@1=71.128 Prec@5=89.768 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=01:15 IST
=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.162 Prec@1=71.128 Prec@5=89.768 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=01:15 IST
=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.162 Prec@1=71.128 Prec@5=89.768 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=01:16 IST
=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.163 Prec@1=71.075 Prec@5=89.748 rate=2.11 Hz, eta=0:11:53, total=0:07:55, wall=01:16 IST
=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.163 Prec@1=71.075 Prec@5=89.748 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=01:16 IST
=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.163 Prec@1=71.075 Prec@5=89.748 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=01:17 IST
=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.166 Prec@1=71.036 Prec@5=89.709 rate=2.10 Hz, eta=0:11:06, total=0:08:43, wall=01:17 IST
=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.166 Prec@1=71.036 Prec@5=89.709 rate=2.10 Hz, eta=0:10:19, total=0:09:31, wall=01:17 IST
=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.166 Prec@1=71.036 Prec@5=89.709 rate=2.10 Hz, eta=0:10:19, total=0:09:31, wall=01:17 IST
=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.167 Prec@1=71.013 Prec@5=89.696 rate=2.10 Hz, eta=0:10:19, total=0:09:31, wall=01:17 IST
=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.167 Prec@1=71.013 Prec@5=89.696 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=01:17 IST
=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.167 Prec@1=71.013 Prec@5=89.696 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=01:18 IST
=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.168 Prec@1=70.987 Prec@5=89.688 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=01:18 IST
=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.168 Prec@1=70.987 Prec@5=89.688 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=01:18 IST
=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.168 Prec@1=70.987 Prec@5=89.688 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=01:19 IST
=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.169 Prec@1=70.971 Prec@5=89.679 rate=2.10 Hz, eta=0:08:45, total=0:11:07, wall=01:19 IST
=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.169 Prec@1=70.971 Prec@5=89.679 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=01:19 IST
=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.169 Prec@1=70.971 Prec@5=89.679 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=01:20 IST
=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.170 Prec@1=70.959 Prec@5=89.655 rate=2.10 Hz, eta=0:07:57, total=0:11:55, wall=01:20 IST
=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.170 Prec@1=70.959 Prec@5=89.655 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=01:20 IST
=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.170 Prec@1=70.959 Prec@5=89.655 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=01:21 IST
=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.172 Prec@1=70.925 Prec@5=89.630 rate=2.10 Hz, eta=0:07:10, total=0:12:43, wall=01:21 IST
=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.172 Prec@1=70.925 Prec@5=89.630 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=01:21 IST
=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.172 Prec@1=70.925 Prec@5=89.630 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=01:21 IST
=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.173 Prec@1=70.903 Prec@5=89.618 rate=2.09 Hz, eta=0:06:22, total=0:13:32, wall=01:21 IST
=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.173 Prec@1=70.903 Prec@5=89.618 rate=2.10 Hz, eta=0:05:35, total=0:14:19, wall=01:21 IST
=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.173 Prec@1=70.903 Prec@5=89.618 rate=2.10 Hz, eta=0:05:35, total=0:14:19, wall=01:22 IST
=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.174 Prec@1=70.875 Prec@5=89.593 rate=2.10 Hz, eta=0:05:35, total=0:14:19, wall=01:22 IST
=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.174 Prec@1=70.875 Prec@5=89.593 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=01:22 IST
=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.174 Prec@1=70.875 Prec@5=89.593 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=01:23 IST
=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.175 Prec@1=70.855 Prec@5=89.584 rate=2.09 Hz, eta=0:04:47, total=0:15:07, wall=01:23 IST
=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.175 Prec@1=70.855 Prec@5=89.584 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=01:23 IST
=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.285 Loss=1.175 Prec@1=70.855 Prec@5=89.584 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=01:24 IST
=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.177 Prec@1=70.806 Prec@5=89.557 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=01:24 IST
=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.177 Prec@1=70.806 Prec@5=89.557 rate=2.10 Hz, eta=0:03:11, total=0:16:42, wall=01:24 IST
=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.177 Prec@1=70.806 Prec@5=89.557 rate=2.10 Hz, eta=0:03:11, total=0:16:42, wall=01:25 IST
=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.794 Prec@5=89.547 rate=2.10 Hz, eta=0:03:11, total=0:16:42, wall=01:25 IST
=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.794 Prec@5=89.547 rate=2.09 Hz, eta=0:02:24, total=0:17:30, wall=01:25 IST
=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.794 Prec@5=89.547 rate=2.09 Hz, eta=0:02:24, total=0:17:30, wall=01:25 IST
=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.796 Prec@5=89.552 rate=2.09 Hz, eta=0:02:24, total=0:17:30, wall=01:25 IST
=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.796 Prec@5=89.552 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=01:25 IST
=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.178 Prec@1=70.796 Prec@5=89.552 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=01:26 IST
=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.179 Prec@1=70.773 Prec@5=89.539 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=01:26 IST
=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.179 Prec@1=70.773 Prec@5=89.539 rate=2.09 Hz, eta=0:00:48, total=0:19:06, wall=01:26 IST
=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.480 DataTime=0.284 Loss=1.179 Prec@1=70.773 Prec@5=89.539 rate=2.09 Hz, eta=0:00:48, total=0:19:06, wall=01:27 IST
=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.180 Prec@1=70.752 Prec@5=89.530 rate=2.09 Hz, eta=0:00:48, total=0:19:06, wall=01:27 IST
=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.180 Prec@1=70.752 Prec@5=89.530 rate=2.09 Hz, eta=0:00:00, total=0:19:54, wall=01:27 IST
=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.180 Prec@1=70.752 Prec@5=89.530 rate=2.09 Hz, eta=0:00:00, total=0:19:54, wall=01:27 IST
=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.180 Prec@1=70.751 Prec@5=89.530 rate=2.09 Hz, eta=0:00:00, total=0:19:54, wall=01:27 IST
=> training   100.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.479 DataTime=0.284 Loss=1.180 Prec@1=70.751 Prec@5=89.530 rate=2.09 Hz, eta=0:00:00, total=0:19:55, wall=01:27 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST
=> validation 0.00% of 1x98...Epoch=99/150 LR=0.02684 Time=6.954 Loss=0.807 Prec@1=78.906 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST
=> validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=6.954 Loss=0.807 Prec@1=78.906 Prec@5=95.312 rate=8156.34 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST
** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=6.954 Loss=0.807 Prec@1=78.906 Prec@5=95.312 rate=8156.34 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST
** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=0.556 Loss=1.306 Prec@1=67.990 Prec@5=88.380 rate=8156.34 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST
** validation 100.00% of 1x98...Epoch=99/150 LR=0.02684 Time=0.556 Loss=1.306 Prec@1=67.990 Prec@5=88.380 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=01:28 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> training   0.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=4.408 DataTime=4.098 Loss=1.268 Prec@1=69.531 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=4.408 DataTime=4.098 Loss=1.268 Prec@1=69.531 Prec@5=88.672 rate=4410.77 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST
=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=4.408 DataTime=4.098 Loss=1.268 Prec@1=69.531 Prec@5=88.672 rate=4410.77 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.519 DataTime=0.323 Loss=1.134 Prec@1=71.740 Prec@5=90.192 rate=4410.77 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.519 DataTime=0.323 Loss=1.134 Prec@1=71.740 Prec@5=90.192 rate=2.11 Hz, eta=0:19:00, total=0:00:47, wall=01:29 IST
=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.519 DataTime=0.323 Loss=1.134 Prec@1=71.740 Prec@5=90.192 rate=2.11 Hz, eta=0:19:00, total=0:00:47, wall=01:30 IST
=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.498 DataTime=0.303 Loss=1.141 Prec@1=71.545 Prec@5=90.070 rate=2.11 Hz, eta=0:19:00, total=0:00:47, wall=01:30 IST
=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.498 DataTime=0.303 Loss=1.141 Prec@1=71.545 Prec@5=90.070 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=01:30 IST
=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.498 DataTime=0.303 Loss=1.141 Prec@1=71.545 Prec@5=90.070 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=01:30 IST
=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.293 Loss=1.140 Prec@1=71.564 Prec@5=90.029 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=01:30 IST
=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.293 Loss=1.140 Prec@1=71.564 Prec@5=90.029 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=01:30 IST
=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.293 Loss=1.140 Prec@1=71.564 Prec@5=90.029 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=01:31 IST
=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.142 Prec@1=71.573 Prec@5=90.012 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=01:31 IST
=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.142 Prec@1=71.573 Prec@5=90.012 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=01:31 IST
=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.142 Prec@1=71.573 Prec@5=90.012 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=01:32 IST
=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.141 Prec@1=71.581 Prec@5=90.023 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=01:32 IST
=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.141 Prec@1=71.581 Prec@5=90.023 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=01:32 IST
=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.141 Prec@1=71.581 Prec@5=90.023 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=01:33 IST
=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.146 Prec@1=71.486 Prec@5=89.967 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=01:33 IST
=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.146 Prec@1=71.486 Prec@5=89.967 rate=2.08 Hz, eta=0:15:15, total=0:04:49, wall=01:33 IST
=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.146 Prec@1=71.486 Prec@5=89.967 rate=2.08 Hz, eta=0:15:15, total=0:04:49, wall=01:34 IST
=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.149 Prec@1=71.446 Prec@5=89.952 rate=2.08 Hz, eta=0:15:15, total=0:04:49, wall=01:34 IST
=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.149 Prec@1=71.446 Prec@5=89.952 rate=2.07 Hz, eta=0:14:28, total=0:05:37, wall=01:34 IST
=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.149 Prec@1=71.446 Prec@5=89.952 rate=2.07 Hz, eta=0:14:28, total=0:05:37, wall=01:35 IST
=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.151 Prec@1=71.397 Prec@5=89.933 rate=2.07 Hz, eta=0:14:28, total=0:05:37, wall=01:35 IST
=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.151 Prec@1=71.397 Prec@5=89.933 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=01:35 IST
=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.489 DataTime=0.295 Loss=1.151 Prec@1=71.397 Prec@5=89.933 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=01:35 IST
=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.152 Prec@1=71.379 Prec@5=89.929 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=01:35 IST
=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.152 Prec@1=71.379 Prec@5=89.929 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=01:35 IST
=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.488 DataTime=0.294 Loss=1.152 Prec@1=71.379 Prec@5=89.929 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=01:36 IST
=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.153 Prec@1=71.349 Prec@5=89.920 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=01:36 IST
=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.153 Prec@1=71.349 Prec@5=89.920 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=01:36 IST
=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.153 Prec@1=71.349 Prec@5=89.920 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=01:37 IST
=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.155 Prec@1=71.313 Prec@5=89.896 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=01:37 IST
=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.155 Prec@1=71.313 Prec@5=89.896 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=01:37 IST
=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.155 Prec@1=71.313 Prec@5=89.896 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=01:38 IST
=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.157 Prec@1=71.263 Prec@5=89.872 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=01:38 IST
=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.157 Prec@1=71.263 Prec@5=89.872 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=01:38 IST
=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.157 Prec@1=71.263 Prec@5=89.872 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=01:39 IST
=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.158 Prec@1=71.245 Prec@5=89.846 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=01:39 IST
=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.158 Prec@1=71.245 Prec@5=89.846 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=01:39 IST
=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.487 DataTime=0.292 Loss=1.158 Prec@1=71.245 Prec@5=89.846 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=01:39 IST
=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.159 Prec@1=71.249 Prec@5=89.841 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=01:39 IST
=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.159 Prec@1=71.249 Prec@5=89.841 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=01:39 IST
=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.292 Loss=1.159 Prec@1=71.249 Prec@5=89.841 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=01:40 IST
=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.213 Prec@5=89.803 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=01:40 IST
=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.213 Prec@5=89.803 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=01:40 IST
=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.213 Prec@5=89.803 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=01:41 IST
=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.178 Prec@5=89.793 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=01:41 IST
=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.178 Prec@5=89.793 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=01:41 IST
=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.161 Prec@1=71.178 Prec@5=89.793 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=01:42 IST
=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.162 Prec@1=71.152 Prec@5=89.788 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=01:42 IST
=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.162 Prec@1=71.152 Prec@5=89.788 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=01:42 IST
=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.162 Prec@1=71.152 Prec@5=89.788 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=01:43 IST
=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.164 Prec@1=71.106 Prec@5=89.758 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=01:43 IST
=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.164 Prec@1=71.106 Prec@5=89.758 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=01:43 IST
=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.164 Prec@1=71.106 Prec@5=89.758 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=01:43 IST
=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.166 Prec@1=71.076 Prec@5=89.742 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=01:43 IST
=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.166 Prec@1=71.076 Prec@5=89.742 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=01:43 IST
=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.290 Loss=1.166 Prec@1=71.076 Prec@5=89.742 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=01:44 IST
=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.044 Prec@5=89.728 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=01:44 IST
=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.044 Prec@5=89.728 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=01:44 IST
=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.167 Prec@1=71.044 Prec@5=89.728 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=01:45 IST
=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.168 Prec@1=71.025 Prec@5=89.709 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=01:45 IST
=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.168 Prec@1=71.025 Prec@5=89.709 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=01:45 IST
=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.168 Prec@1=71.025 Prec@5=89.709 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=01:46 IST
=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.169 Prec@1=71.002 Prec@5=89.692 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=01:46 IST
=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.169 Prec@1=71.002 Prec@5=89.692 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=01:46 IST
=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.169 Prec@1=71.002 Prec@5=89.692 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=01:47 IST
=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.170 Prec@1=70.968 Prec@5=89.671 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=01:47 IST
=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.170 Prec@1=70.968 Prec@5=89.671 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=01:47 IST
=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.170 Prec@1=70.968 Prec@5=89.671 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=01:47 IST
=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.171 Prec@1=70.948 Prec@5=89.651 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=01:47 IST
=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.171 Prec@1=70.948 Prec@5=89.651 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=01:47 IST
=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.486 DataTime=0.291 Loss=1.171 Prec@1=70.948 Prec@5=89.651 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=01:48 IST
=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.291 Loss=1.172 Prec@1=70.934 Prec@5=89.639 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.291 Loss=1.172 Prec@1=70.934 Prec@5=89.639 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.291 Loss=1.172 Prec@1=70.934 Prec@5=89.639 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=01:48 IST
=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.291 Loss=1.172 Prec@1=70.935 Prec@5=89.639 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=01:48 IST
=> training   100.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.485 DataTime=0.291 Loss=1.172 Prec@1=70.935 Prec@5=89.639 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=01:48 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> validation 0.00% of 1x98...Epoch=100/150 LR=0.02591 Time=6.111 Loss=0.811 Prec@1=78.711 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=6.111 Loss=0.811 Prec@1=78.711 Prec@5=94.141 rate=7403.30 Hz, eta=0:00:00, total=0:00:00, wall=01:48 IST
** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=6.111 Loss=0.811 Prec@1=78.711 Prec@5=94.141 rate=7403.30 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=0.550 Loss=1.286 Prec@1=68.462 Prec@5=88.560 rate=7403.30 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** validation 100.00% of 1x98...Epoch=100/150 LR=0.02591 Time=0.550 Loss=1.286 Prec@1=68.462 Prec@5=88.560 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=01:49 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=4.497 DataTime=4.230 Loss=1.183 Prec@1=71.094 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=4.497 DataTime=4.230 Loss=1.183 Prec@1=71.094 Prec@5=89.648 rate=7603.87 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=4.497 DataTime=4.230 Loss=1.183 Prec@1=71.094 Prec@5=89.648 rate=7603.87 Hz, eta=0:00:00, total=0:00:00, wall=01:50 IST
=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.516 DataTime=0.320 Loss=1.140 Prec@1=71.786 Prec@5=90.027 rate=7603.87 Hz, eta=0:00:00, total=0:00:00, wall=01:50 IST
=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.516 DataTime=0.320 Loss=1.140 Prec@1=71.786 Prec@5=90.027 rate=2.12 Hz, eta=0:18:52, total=0:00:47, wall=01:50 IST
=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.516 DataTime=0.320 Loss=1.140 Prec@1=71.786 Prec@5=90.027 rate=2.12 Hz, eta=0:18:52, total=0:00:47, wall=01:51 IST
=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.496 DataTime=0.300 Loss=1.141 Prec@1=71.682 Prec@5=90.009 rate=2.12 Hz, eta=0:18:52, total=0:00:47, wall=01:51 IST
=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.496 DataTime=0.300 Loss=1.141 Prec@1=71.682 Prec@5=90.009 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=01:51 IST
=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.496 DataTime=0.300 Loss=1.141 Prec@1=71.682 Prec@5=90.009 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=01:52 IST
=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.487 DataTime=0.291 Loss=1.143 Prec@1=71.560 Prec@5=89.969 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=01:52 IST
=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.487 DataTime=0.291 Loss=1.143 Prec@1=71.560 Prec@5=89.969 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=01:52 IST
=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.487 DataTime=0.291 Loss=1.143 Prec@1=71.560 Prec@5=89.969 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=01:52 IST
=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.485 DataTime=0.288 Loss=1.146 Prec@1=71.519 Prec@5=89.939 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=01:52 IST
=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.485 DataTime=0.288 Loss=1.146 Prec@1=71.519 Prec@5=89.939 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=01:52 IST
=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.485 DataTime=0.288 Loss=1.146 Prec@1=71.519 Prec@5=89.939 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=01:53 IST
=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.147 Prec@1=71.512 Prec@5=89.935 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=01:53 IST
=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.147 Prec@1=71.512 Prec@5=89.935 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=01:53 IST
=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.147 Prec@1=71.512 Prec@5=89.935 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=01:54 IST
=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.483 DataTime=0.286 Loss=1.147 Prec@1=71.526 Prec@5=89.916 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=01:54 IST
=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.483 DataTime=0.286 Loss=1.147 Prec@1=71.526 Prec@5=89.916 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=01:54 IST
=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.483 DataTime=0.286 Loss=1.147 Prec@1=71.526 Prec@5=89.916 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=01:55 IST
=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.148 Prec@1=71.491 Prec@5=89.895 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=01:55 IST
=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.148 Prec@1=71.491 Prec@5=89.895 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=01:55 IST
=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.148 Prec@1=71.491 Prec@5=89.895 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=01:56 IST
=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.150 Prec@1=71.459 Prec@5=89.865 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.150 Prec@1=71.459 Prec@5=89.865 rate=2.10 Hz, eta=0:13:29, total=0:06:20, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.150 Prec@1=71.459 Prec@5=89.865 rate=2.10 Hz, eta=0:13:29, total=0:06:20, wall=01:56 IST
=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.407 Prec@5=89.844 rate=2.10 Hz, eta=0:13:29, total=0:06:20, wall=01:56 IST
=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.407 Prec@5=89.844 rate=2.10 Hz, eta=0:12:42, total=0:07:08, wall=01:56 IST
=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.407 Prec@5=89.844 rate=2.10 Hz, eta=0:12:42, total=0:07:08, wall=01:57 IST
=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.410 Prec@5=89.860 rate=2.10 Hz, eta=0:12:42, total=0:07:08, wall=01:57 IST
=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.410 Prec@5=89.860 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=01:57 IST
=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.152 Prec@1=71.410 Prec@5=89.860 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=01:58 IST
=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.154 Prec@1=71.371 Prec@5=89.843 rate=2.10 Hz, eta=0:11:55, total=0:07:56, wall=01:58 IST
=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.154 Prec@1=71.371 Prec@5=89.843 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=01:58 IST
=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.154 Prec@1=71.371 Prec@5=89.843 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=01:59 IST
=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.341 Prec@5=89.845 rate=2.10 Hz, eta=0:11:07, total=0:08:44, wall=01:59 IST
=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.341 Prec@5=89.845 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=01:59 IST
=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.155 Prec@1=71.341 Prec@5=89.845 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=02:00 IST
=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.333 Prec@5=89.829 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.333 Prec@5=89.829 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.333 Prec@5=89.829 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=02:00 IST
=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.156 Prec@1=71.326 Prec@5=89.836 rate=2.10 Hz, eta=0:09:33, total=0:10:20, wall=02:00 IST
=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.156 Prec@1=71.326 Prec@5=89.836 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=02:00 IST
=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.156 Prec@1=71.326 Prec@5=89.836 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=02:01 IST
=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.312 Prec@5=89.832 rate=2.10 Hz, eta=0:08:45, total=0:11:08, wall=02:01 IST
=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.312 Prec@5=89.832 rate=2.09 Hz, eta=0:07:58, total=0:11:56, wall=02:01 IST
=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.156 Prec@1=71.312 Prec@5=89.832 rate=2.09 Hz, eta=0:07:58, total=0:11:56, wall=02:02 IST
=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.157 Prec@1=71.296 Prec@5=89.832 rate=2.09 Hz, eta=0:07:58, total=0:11:56, wall=02:02 IST
=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.157 Prec@1=71.296 Prec@5=89.832 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=02:02 IST
=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.157 Prec@1=71.296 Prec@5=89.832 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=02:03 IST
=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.158 Prec@1=71.263 Prec@5=89.811 rate=2.09 Hz, eta=0:07:10, total=0:12:44, wall=02:03 IST
=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.158 Prec@1=71.263 Prec@5=89.811 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=02:03 IST
=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.158 Prec@1=71.263 Prec@5=89.811 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=02:04 IST
=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.160 Prec@1=71.227 Prec@5=89.787 rate=2.09 Hz, eta=0:06:23, total=0:13:32, wall=02:04 IST
=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.160 Prec@1=71.227 Prec@5=89.787 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=02:04 IST
=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.284 Loss=1.160 Prec@1=71.227 Prec@5=89.787 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=02:04 IST
=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.160 Prec@1=71.208 Prec@5=89.795 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=02:04 IST
=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.160 Prec@1=71.208 Prec@5=89.795 rate=2.09 Hz, eta=0:04:47, total=0:15:09, wall=02:04 IST
=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.160 Prec@1=71.208 Prec@5=89.795 rate=2.09 Hz, eta=0:04:47, total=0:15:09, wall=02:05 IST
=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.161 Prec@1=71.183 Prec@5=89.778 rate=2.09 Hz, eta=0:04:47, total=0:15:09, wall=02:05 IST
=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.161 Prec@1=71.183 Prec@5=89.778 rate=2.09 Hz, eta=0:04:00, total=0:15:56, wall=02:05 IST
=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.480 DataTime=0.285 Loss=1.161 Prec@1=71.183 Prec@5=89.778 rate=2.09 Hz, eta=0:04:00, total=0:15:56, wall=02:06 IST
=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.156 Prec@5=89.754 rate=2.09 Hz, eta=0:04:00, total=0:15:56, wall=02:06 IST
=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.156 Prec@5=89.754 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=02:06 IST
=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.156 Prec@5=89.754 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=02:07 IST
=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.141 Prec@5=89.751 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=02:07 IST
=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.141 Prec@5=89.751 rate=2.09 Hz, eta=0:02:24, total=0:17:34, wall=02:07 IST
=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.285 Loss=1.163 Prec@1=71.141 Prec@5=89.751 rate=2.09 Hz, eta=0:02:24, total=0:17:34, wall=02:08 IST
=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.286 Loss=1.164 Prec@1=71.138 Prec@5=89.747 rate=2.09 Hz, eta=0:02:24, total=0:17:34, wall=02:08 IST
=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.286 Loss=1.164 Prec@1=71.138 Prec@5=89.747 rate=2.09 Hz, eta=0:01:36, total=0:18:23, wall=02:08 IST
=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.481 DataTime=0.286 Loss=1.164 Prec@1=71.138 Prec@5=89.747 rate=2.09 Hz, eta=0:01:36, total=0:18:23, wall=02:09 IST
=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.164 Prec@1=71.133 Prec@5=89.738 rate=2.09 Hz, eta=0:01:36, total=0:18:23, wall=02:09 IST
=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.164 Prec@1=71.133 Prec@5=89.738 rate=2.08 Hz, eta=0:00:48, total=0:19:12, wall=02:09 IST
=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.164 Prec@1=71.133 Prec@5=89.738 rate=2.08 Hz, eta=0:00:48, total=0:19:12, wall=02:09 IST
=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.165 Prec@1=71.112 Prec@5=89.724 rate=2.08 Hz, eta=0:00:48, total=0:19:12, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.165 Prec@1=71.112 Prec@5=89.724 rate=2.08 Hz, eta=0:00:00, total=0:20:00, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.165 Prec@1=71.112 Prec@5=89.724 rate=2.08 Hz, eta=0:00:00, total=0:20:00, wall=02:09 IST
=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.165 Prec@1=71.110 Prec@5=89.723 rate=2.08 Hz, eta=0:00:00, total=0:20:00, wall=02:09 IST
=> training   100.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.482 DataTime=0.286 Loss=1.165 Prec@1=71.110 Prec@5=89.723 rate=2.08 Hz, eta=0:00:00, total=0:20:00, wall=02:09 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 0.00% of 1x98...Epoch=101/150 LR=0.02500 Time=7.071 Loss=0.788 Prec@1=79.102 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=02:09 IST
=> validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=7.071 Loss=0.788 Prec@1=79.102 Prec@5=94.922 rate=5702.91 Hz, eta=0:00:00, total=0:00:00, wall=02:09 IST
** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=7.071 Loss=0.788 Prec@1=79.102 Prec@5=94.922 rate=5702.91 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=0.551 Loss=1.320 Prec@1=67.952 Prec@5=87.976 rate=5702.91 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
** validation 100.00% of 1x98...Epoch=101/150 LR=0.02500 Time=0.551 Loss=1.320 Prec@1=67.952 Prec@5=87.976 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=02:10 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.468 DataTime=4.207 Loss=0.966 Prec@1=73.828 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=02:10 IST
=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.468 DataTime=4.207 Loss=0.966 Prec@1=73.828 Prec@5=93.359 rate=4985.81 Hz, eta=0:00:00, total=0:00:00, wall=02:10 IST
=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.468 DataTime=4.207 Loss=0.966 Prec@1=73.828 Prec@5=93.359 rate=4985.81 Hz, eta=0:00:00, total=0:00:00, wall=02:11 IST
=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.521 DataTime=0.324 Loss=1.117 Prec@1=72.163 Prec@5=90.298 rate=4985.81 Hz, eta=0:00:00, total=0:00:00, wall=02:11 IST
=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.521 DataTime=0.324 Loss=1.117 Prec@1=72.163 Prec@5=90.298 rate=2.10 Hz, eta=0:19:05, total=0:00:48, wall=02:11 IST
=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.521 DataTime=0.324 Loss=1.117 Prec@1=72.163 Prec@5=90.298 rate=2.10 Hz, eta=0:19:05, total=0:00:48, wall=02:12 IST
=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.497 DataTime=0.302 Loss=1.123 Prec@1=72.129 Prec@5=90.289 rate=2.10 Hz, eta=0:19:05, total=0:00:48, wall=02:12 IST
=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.497 DataTime=0.302 Loss=1.123 Prec@1=72.129 Prec@5=90.289 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=02:12 IST
=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.497 DataTime=0.302 Loss=1.123 Prec@1=72.129 Prec@5=90.289 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=02:13 IST
=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.493 DataTime=0.297 Loss=1.124 Prec@1=72.048 Prec@5=90.235 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=02:13 IST
=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.493 DataTime=0.297 Loss=1.124 Prec@1=72.048 Prec@5=90.235 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=02:13 IST
=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.493 DataTime=0.297 Loss=1.124 Prec@1=72.048 Prec@5=90.235 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=02:14 IST
=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.293 Loss=1.127 Prec@1=71.956 Prec@5=90.182 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=02:14 IST
=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.293 Loss=1.127 Prec@1=71.956 Prec@5=90.182 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=02:14 IST
=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.293 Loss=1.127 Prec@1=71.956 Prec@5=90.182 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=02:14 IST
=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.127 Prec@1=71.932 Prec@5=90.180 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=02:14 IST
=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.127 Prec@1=71.932 Prec@5=90.180 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=02:14 IST
=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.127 Prec@1=71.932 Prec@5=90.180 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=02:15 IST
=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.291 Loss=1.130 Prec@1=71.905 Prec@5=90.116 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=02:15 IST
=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.291 Loss=1.130 Prec@1=71.905 Prec@5=90.116 rate=2.08 Hz, eta=0:15:14, total=0:04:48, wall=02:15 IST
=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.291 Loss=1.130 Prec@1=71.905 Prec@5=90.116 rate=2.08 Hz, eta=0:15:14, total=0:04:48, wall=02:16 IST
=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.292 Loss=1.131 Prec@1=71.875 Prec@5=90.130 rate=2.08 Hz, eta=0:15:14, total=0:04:48, wall=02:16 IST
=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.292 Loss=1.131 Prec@1=71.875 Prec@5=90.130 rate=2.07 Hz, eta=0:14:28, total=0:05:38, wall=02:16 IST
=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.489 DataTime=0.292 Loss=1.131 Prec@1=71.875 Prec@5=90.130 rate=2.07 Hz, eta=0:14:28, total=0:05:38, wall=02:17 IST
=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.132 Prec@1=71.860 Prec@5=90.110 rate=2.07 Hz, eta=0:14:28, total=0:05:38, wall=02:17 IST
=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.132 Prec@1=71.860 Prec@5=90.110 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=02:17 IST
=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.132 Prec@1=71.860 Prec@5=90.110 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=02:18 IST
=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.135 Prec@1=71.806 Prec@5=90.085 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=02:18 IST
=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.135 Prec@1=71.806 Prec@5=90.085 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=02:18 IST
=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.488 DataTime=0.292 Loss=1.135 Prec@1=71.806 Prec@5=90.085 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=02:18 IST
=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.291 Loss=1.138 Prec@1=71.711 Prec@5=90.051 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=02:18 IST
=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.291 Loss=1.138 Prec@1=71.711 Prec@5=90.051 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=02:18 IST
=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.291 Loss=1.138 Prec@1=71.711 Prec@5=90.051 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=02:19 IST
=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.290 Loss=1.138 Prec@1=71.681 Prec@5=90.065 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=02:19 IST
=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.290 Loss=1.138 Prec@1=71.681 Prec@5=90.065 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=02:19 IST
=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.487 DataTime=0.290 Loss=1.138 Prec@1=71.681 Prec@5=90.065 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=02:20 IST
=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.140 Prec@1=71.639 Prec@5=90.056 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=02:20 IST
=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.140 Prec@1=71.639 Prec@5=90.056 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=02:20 IST
=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.140 Prec@1=71.639 Prec@5=90.056 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=02:21 IST
=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.142 Prec@1=71.574 Prec@5=90.016 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=02:21 IST
=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.142 Prec@1=71.574 Prec@5=90.016 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=02:21 IST
=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.142 Prec@1=71.574 Prec@5=90.016 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=02:22 IST
=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.143 Prec@1=71.572 Prec@5=90.010 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=02:22 IST
=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.143 Prec@1=71.572 Prec@5=90.010 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=02:22 IST
=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.143 Prec@1=71.572 Prec@5=90.010 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=02:22 IST
=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.533 Prec@5=89.971 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=02:22 IST
=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.533 Prec@5=89.971 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=02:22 IST
=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.533 Prec@5=89.971 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=02:23 IST
=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.509 Prec@5=89.951 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=02:23 IST
=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.509 Prec@5=89.951 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=02:23 IST
=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.509 Prec@5=89.951 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=02:24 IST
=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.148 Prec@1=71.461 Prec@5=89.936 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=02:24 IST
=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.148 Prec@1=71.461 Prec@5=89.936 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=02:24 IST
=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.148 Prec@1=71.461 Prec@5=89.936 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=02:25 IST
=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.149 Prec@1=71.427 Prec@5=89.913 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=02:25 IST
=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.149 Prec@1=71.427 Prec@5=89.913 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=02:25 IST
=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.149 Prec@1=71.427 Prec@5=89.913 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=02:26 IST
=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.150 Prec@1=71.400 Prec@5=89.893 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=02:26 IST
=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.150 Prec@1=71.400 Prec@5=89.893 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=02:26 IST
=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.150 Prec@1=71.400 Prec@5=89.893 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=02:26 IST
=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.151 Prec@1=71.377 Prec@5=89.884 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=02:26 IST
=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.151 Prec@1=71.377 Prec@5=89.884 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:26 IST
=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.486 DataTime=0.290 Loss=1.151 Prec@1=71.377 Prec@5=89.884 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:27 IST
=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.152 Prec@1=71.361 Prec@5=89.866 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:27 IST
=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.152 Prec@1=71.361 Prec@5=89.866 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=02:27 IST
=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.152 Prec@1=71.361 Prec@5=89.866 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=02:28 IST
=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.153 Prec@1=71.355 Prec@5=89.856 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=02:28 IST
=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.153 Prec@1=71.355 Prec@5=89.856 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=02:28 IST
=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.153 Prec@1=71.355 Prec@5=89.856 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=02:29 IST
=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.323 Prec@5=89.833 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=02:29 IST
=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.323 Prec@5=89.833 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=02:29 IST
=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.323 Prec@5=89.833 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=02:30 IST
=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.303 Prec@5=89.822 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=02:30 IST
=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.303 Prec@5=89.822 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=02:30 IST
=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.155 Prec@1=71.303 Prec@5=89.822 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=02:30 IST
=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.156 Prec@1=71.296 Prec@5=89.819 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.156 Prec@1=71.296 Prec@5=89.819 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.156 Prec@1=71.296 Prec@5=89.819 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=02:30 IST
=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.156 Prec@1=71.296 Prec@5=89.817 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=02:30 IST
=> training   100.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.485 DataTime=0.289 Loss=1.156 Prec@1=71.296 Prec@5=89.817 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=02:30 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> validation 0.00% of 1x98...Epoch=102/150 LR=0.02410 Time=7.082 Loss=0.881 Prec@1=78.125 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=02:31 IST
=> validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=7.082 Loss=0.881 Prec@1=78.125 Prec@5=93.359 rate=9159.19 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=7.082 Loss=0.881 Prec@1=78.125 Prec@5=93.359 rate=9159.19 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=0.558 Loss=1.312 Prec@1=67.894 Prec@5=88.254 rate=9159.19 Hz, eta=0:00:00, total=0:00:00, wall=02:31 IST
** validation 100.00% of 1x98...Epoch=102/150 LR=0.02410 Time=0.558 Loss=1.312 Prec@1=67.894 Prec@5=88.254 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=02:31 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST
=> training   0.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.405 DataTime=4.149 Loss=0.996 Prec@1=74.805 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST
=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.405 DataTime=4.149 Loss=0.996 Prec@1=74.805 Prec@5=92.773 rate=5521.02 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST
=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.405 DataTime=4.149 Loss=0.996 Prec@1=74.805 Prec@5=92.773 rate=5521.02 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST
=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.509 DataTime=0.315 Loss=1.117 Prec@1=72.268 Prec@5=90.406 rate=5521.02 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST
=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.509 DataTime=0.315 Loss=1.117 Prec@1=72.268 Prec@5=90.406 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=02:32 IST
=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.509 DataTime=0.315 Loss=1.117 Prec@1=72.268 Prec@5=90.406 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=02:33 IST
=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.496 DataTime=0.301 Loss=1.123 Prec@1=71.982 Prec@5=90.346 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=02:33 IST
=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.496 DataTime=0.301 Loss=1.123 Prec@1=71.982 Prec@5=90.346 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=02:33 IST
=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.496 DataTime=0.301 Loss=1.123 Prec@1=71.982 Prec@5=90.346 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=02:34 IST
=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.490 DataTime=0.296 Loss=1.124 Prec@1=72.008 Prec@5=90.247 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=02:34 IST
=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.490 DataTime=0.296 Loss=1.124 Prec@1=72.008 Prec@5=90.247 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=02:34 IST
=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.490 DataTime=0.296 Loss=1.124 Prec@1=72.008 Prec@5=90.247 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=02:35 IST
=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.488 DataTime=0.294 Loss=1.123 Prec@1=72.055 Prec@5=90.256 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=02:35 IST
=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.488 DataTime=0.294 Loss=1.123 Prec@1=72.055 Prec@5=90.256 rate=2.10 Hz, eta=0:16:43, total=0:03:11, wall=02:35 IST
=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.488 DataTime=0.294 Loss=1.123 Prec@1=72.055 Prec@5=90.256 rate=2.10 Hz, eta=0:16:43, total=0:03:11, wall=02:35 IST
=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.487 DataTime=0.293 Loss=1.126 Prec@1=71.962 Prec@5=90.239 rate=2.10 Hz, eta=0:16:43, total=0:03:11, wall=02:35 IST
=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.487 DataTime=0.293 Loss=1.126 Prec@1=71.962 Prec@5=90.239 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=02:35 IST
=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.487 DataTime=0.293 Loss=1.126 Prec@1=71.962 Prec@5=90.239 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=02:36 IST
=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.292 Loss=1.126 Prec@1=71.989 Prec@5=90.224 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=02:36 IST
=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.292 Loss=1.126 Prec@1=71.989 Prec@5=90.224 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=02:36 IST
=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.292 Loss=1.126 Prec@1=71.989 Prec@5=90.224 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=02:37 IST
=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.975 Prec@5=90.164 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=02:37 IST
=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.975 Prec@5=90.164 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=02:37 IST
=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.975 Prec@5=90.164 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=02:38 IST
=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.955 Prec@5=90.172 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=02:38 IST
=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.955 Prec@5=90.172 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=02:38 IST
=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.129 Prec@1=71.955 Prec@5=90.172 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=02:39 IST
=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.131 Prec@1=71.908 Prec@5=90.152 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=02:39 IST
=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.131 Prec@1=71.908 Prec@5=90.152 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=02:39 IST
=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.131 Prec@1=71.908 Prec@5=90.152 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=02:40 IST
=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.853 Prec@5=90.140 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=02:40 IST
=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.853 Prec@5=90.140 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=02:40 IST
=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.853 Prec@5=90.140 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=02:40 IST
=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.134 Prec@1=71.842 Prec@5=90.126 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=02:40 IST
=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.134 Prec@1=71.842 Prec@5=90.126 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=02:40 IST
=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.134 Prec@1=71.842 Prec@5=90.126 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=02:41 IST
=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.135 Prec@1=71.787 Prec@5=90.106 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=02:41 IST
=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.135 Prec@1=71.787 Prec@5=90.106 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=02:41 IST
=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.135 Prec@1=71.787 Prec@5=90.106 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=02:42 IST
=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.136 Prec@1=71.765 Prec@5=90.089 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=02:42 IST
=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.136 Prec@1=71.765 Prec@5=90.089 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=02:42 IST
=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.136 Prec@1=71.765 Prec@5=90.089 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=02:43 IST
=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.734 Prec@5=90.068 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=02:43 IST
=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.734 Prec@5=90.068 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=02:43 IST
=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.734 Prec@5=90.068 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=02:44 IST
=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.705 Prec@5=90.057 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=02:44 IST
=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.705 Prec@5=90.057 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=02:44 IST
=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.705 Prec@5=90.057 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=02:44 IST
=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.140 Prec@1=71.663 Prec@5=90.035 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=02:44 IST
=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.140 Prec@1=71.663 Prec@5=90.035 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=02:44 IST
=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.140 Prec@1=71.663 Prec@5=90.035 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=02:45 IST
=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.141 Prec@1=71.645 Prec@5=90.028 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=02:45 IST
=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.141 Prec@1=71.645 Prec@5=90.028 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=02:45 IST
=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.141 Prec@1=71.645 Prec@5=90.028 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=02:46 IST
=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.142 Prec@1=71.620 Prec@5=90.006 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=02:46 IST
=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.142 Prec@1=71.620 Prec@5=90.006 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=02:46 IST
=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.289 Loss=1.142 Prec@1=71.620 Prec@5=90.006 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=02:47 IST
=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.144 Prec@1=71.594 Prec@5=89.987 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=02:47 IST
=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.144 Prec@1=71.594 Prec@5=89.987 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=02:47 IST
=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.144 Prec@1=71.594 Prec@5=89.987 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=02:48 IST
=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.573 Prec@5=89.972 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=02:48 IST
=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.573 Prec@5=89.972 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:48 IST
=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.573 Prec@5=89.972 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:48 IST
=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.556 Prec@5=89.961 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=02:48 IST
=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.556 Prec@5=89.961 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=02:48 IST
=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.145 Prec@1=71.556 Prec@5=89.961 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=02:49 IST
=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.146 Prec@1=71.535 Prec@5=89.956 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=02:49 IST
=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.146 Prec@1=71.535 Prec@5=89.956 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=02:49 IST
=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.146 Prec@1=71.535 Prec@5=89.956 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=02:50 IST
=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.514 Prec@5=89.947 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=02:50 IST
=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.514 Prec@5=89.947 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=02:50 IST
=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.290 Loss=1.146 Prec@1=71.514 Prec@5=89.947 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=02:51 IST
=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.147 Prec@1=71.496 Prec@5=89.928 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=02:51 IST
=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.147 Prec@1=71.496 Prec@5=89.928 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=02:51 IST
=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.485 DataTime=0.290 Loss=1.147 Prec@1=71.496 Prec@5=89.928 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=02:52 IST
=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.149 Prec@1=71.469 Prec@5=89.903 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=02:52 IST
=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.149 Prec@1=71.469 Prec@5=89.903 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=02:52 IST
=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.149 Prec@1=71.469 Prec@5=89.903 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=02:52 IST
=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.149 Prec@1=71.469 Prec@5=89.903 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=02:52 IST
=> training   100.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.486 DataTime=0.291 Loss=1.149 Prec@1=71.469 Prec@5=89.903 rate=2.07 Hz, eta=0:00:00, total=0:20:11, wall=02:52 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> validation 0.00% of 1x98...Epoch=103/150 LR=0.02321 Time=7.020 Loss=0.746 Prec@1=79.492 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.020 Loss=0.746 Prec@1=79.492 Prec@5=95.508 rate=7584.09 Hz, eta=0:00:00, total=0:00:00, wall=02:52 IST
** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.020 Loss=0.746 Prec@1=79.492 Prec@5=95.508 rate=7584.09 Hz, eta=0:00:00, total=0:00:00, wall=02:53 IST
** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=0.567 Loss=1.282 Prec@1=68.680 Prec@5=88.576 rate=7584.09 Hz, eta=0:00:00, total=0:00:00, wall=02:53 IST
** validation 100.00% of 1x98...Epoch=103/150 LR=0.02321 Time=0.567 Loss=1.282 Prec@1=68.680 Prec@5=88.576 rate=2.02 Hz, eta=0:00:00, total=0:00:48, wall=02:53 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:53 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:53 IST
=> training   0.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.200 DataTime=4.901 Loss=1.120 Prec@1=70.508 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=02:53 IST
=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.200 DataTime=4.901 Loss=1.120 Prec@1=70.508 Prec@5=89.258 rate=6891.13 Hz, eta=0:00:00, total=0:00:00, wall=02:53 IST
=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.200 DataTime=4.901 Loss=1.120 Prec@1=70.508 Prec@5=89.258 rate=6891.13 Hz, eta=0:00:00, total=0:00:00, wall=02:54 IST
=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.514 DataTime=0.320 Loss=1.106 Prec@1=72.565 Prec@5=90.424 rate=6891.13 Hz, eta=0:00:00, total=0:00:00, wall=02:54 IST
=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.514 DataTime=0.320 Loss=1.106 Prec@1=72.565 Prec@5=90.424 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:54 IST
=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.514 DataTime=0.320 Loss=1.106 Prec@1=72.565 Prec@5=90.424 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:54 IST
=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.491 DataTime=0.294 Loss=1.105 Prec@1=72.543 Prec@5=90.516 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=02:54 IST
=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.491 DataTime=0.294 Loss=1.105 Prec@1=72.543 Prec@5=90.516 rate=2.15 Hz, eta=0:17:50, total=0:01:33, wall=02:54 IST
=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.491 DataTime=0.294 Loss=1.105 Prec@1=72.543 Prec@5=90.516 rate=2.15 Hz, eta=0:17:50, total=0:01:33, wall=02:55 IST
=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.288 Loss=1.107 Prec@1=72.387 Prec@5=90.485 rate=2.15 Hz, eta=0:17:50, total=0:01:33, wall=02:55 IST
=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.288 Loss=1.107 Prec@1=72.387 Prec@5=90.485 rate=2.14 Hz, eta=0:17:10, total=0:02:20, wall=02:55 IST
=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.288 Loss=1.107 Prec@1=72.387 Prec@5=90.485 rate=2.14 Hz, eta=0:17:10, total=0:02:20, wall=02:56 IST
=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.483 DataTime=0.287 Loss=1.110 Prec@1=72.288 Prec@5=90.424 rate=2.14 Hz, eta=0:17:10, total=0:02:20, wall=02:56 IST
=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.483 DataTime=0.287 Loss=1.110 Prec@1=72.288 Prec@5=90.424 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=02:56 IST
=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.483 DataTime=0.287 Loss=1.110 Prec@1=72.288 Prec@5=90.424 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=02:57 IST
=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.112 Prec@1=72.258 Prec@5=90.400 rate=2.13 Hz, eta=0:16:28, total=0:03:08, wall=02:57 IST
=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.112 Prec@1=72.258 Prec@5=90.400 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=02:57 IST
=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.112 Prec@1=72.258 Prec@5=90.400 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=02:58 IST
=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.115 Prec@1=72.176 Prec@5=90.369 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=02:58 IST
=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.115 Prec@1=72.176 Prec@5=90.369 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=02:58 IST
=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.115 Prec@1=72.176 Prec@5=90.369 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=02:58 IST
=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.116 Prec@1=72.130 Prec@5=90.347 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=02:58 IST
=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.116 Prec@1=72.130 Prec@5=90.347 rate=2.09 Hz, eta=0:14:24, total=0:05:36, wall=02:58 IST
=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.116 Prec@1=72.130 Prec@5=90.347 rate=2.09 Hz, eta=0:14:24, total=0:05:36, wall=02:59 IST
=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.117 Prec@1=72.122 Prec@5=90.311 rate=2.09 Hz, eta=0:14:24, total=0:05:36, wall=02:59 IST
=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.117 Prec@1=72.122 Prec@5=90.311 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=02:59 IST
=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.117 Prec@1=72.122 Prec@5=90.311 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=03:00 IST
=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.121 Prec@1=72.065 Prec@5=90.277 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=03:00 IST
=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.121 Prec@1=72.065 Prec@5=90.277 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=03:00 IST
=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.121 Prec@1=72.065 Prec@5=90.277 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=03:01 IST
=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.488 DataTime=0.292 Loss=1.123 Prec@1=72.019 Prec@5=90.239 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=03:01 IST
=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.488 DataTime=0.292 Loss=1.123 Prec@1=72.019 Prec@5=90.239 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=03:01 IST
=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.488 DataTime=0.292 Loss=1.123 Prec@1=72.019 Prec@5=90.239 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=03:02 IST
=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.292 Loss=1.125 Prec@1=71.953 Prec@5=90.214 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=03:02 IST
=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.292 Loss=1.125 Prec@1=71.953 Prec@5=90.214 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=03:02 IST
=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.292 Loss=1.125 Prec@1=71.953 Prec@5=90.214 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=03:02 IST
=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.127 Prec@1=71.906 Prec@5=90.174 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=03:02 IST
=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.127 Prec@1=71.906 Prec@5=90.174 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=03:02 IST
=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.127 Prec@1=71.906 Prec@5=90.174 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=03:03 IST
=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.128 Prec@1=71.883 Prec@5=90.156 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=03:03 IST
=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.128 Prec@1=71.883 Prec@5=90.156 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=03:03 IST
=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.128 Prec@1=71.883 Prec@5=90.156 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=03:04 IST
=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.130 Prec@1=71.860 Prec@5=90.133 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=03:04 IST
=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.130 Prec@1=71.860 Prec@5=90.133 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=03:04 IST
=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.487 DataTime=0.291 Loss=1.130 Prec@1=71.860 Prec@5=90.133 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=03:05 IST
=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.833 Prec@5=90.113 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=03:05 IST
=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.833 Prec@5=90.113 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=03:05 IST
=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.833 Prec@5=90.113 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=03:06 IST
=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.820 Prec@5=90.106 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=03:06 IST
=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.820 Prec@5=90.106 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=03:06 IST
=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.486 DataTime=0.290 Loss=1.131 Prec@1=71.820 Prec@5=90.106 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=03:06 IST
=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.804 Prec@5=90.088 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=03:06 IST
=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.804 Prec@5=90.088 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=03:06 IST
=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.290 Loss=1.133 Prec@1=71.804 Prec@5=90.088 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=03:07 IST
=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.133 Prec@1=71.788 Prec@5=90.085 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=03:07 IST
=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.133 Prec@1=71.788 Prec@5=90.085 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=03:07 IST
=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.133 Prec@1=71.788 Prec@5=90.085 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=03:08 IST
=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.134 Prec@1=71.759 Prec@5=90.070 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=03:08 IST
=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.134 Prec@1=71.759 Prec@5=90.070 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=03:08 IST
=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.134 Prec@1=71.759 Prec@5=90.070 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=03:09 IST
=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.057 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=03:09 IST
=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.057 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:09 IST
=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.057 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:10 IST
=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.720 Prec@5=90.050 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:10 IST
=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.720 Prec@5=90.050 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=03:10 IST
=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.720 Prec@5=90.050 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=03:10 IST
=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.054 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=03:10 IST
=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.054 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=03:10 IST
=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.136 Prec@1=71.736 Prec@5=90.054 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=03:11 IST
=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.137 Prec@1=71.704 Prec@5=90.041 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=03:11 IST
=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.137 Prec@1=71.704 Prec@5=90.041 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=03:11 IST
=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.137 Prec@1=71.704 Prec@5=90.041 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=03:12 IST
=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.677 Prec@5=90.033 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=03:12 IST
=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.677 Prec@5=90.033 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=03:12 IST
=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.138 Prec@1=71.677 Prec@5=90.033 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=03:13 IST
=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.649 Prec@5=90.019 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=03:13 IST
=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.649 Prec@5=90.019 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=03:13 IST
=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.649 Prec@5=90.019 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=03:13 IST
=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.648 Prec@5=90.019 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=03:13 IST
=> training   100.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.485 DataTime=0.289 Loss=1.139 Prec@1=71.648 Prec@5=90.019 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=03:13 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST
=> validation 0.00% of 1x98...Epoch=104/150 LR=0.02233 Time=6.872 Loss=0.810 Prec@1=78.516 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST
=> validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=6.872 Loss=0.810 Prec@1=78.516 Prec@5=95.312 rate=8199.88 Hz, eta=0:00:00, total=0:00:00, wall=03:13 IST
** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=6.872 Loss=0.810 Prec@1=78.516 Prec@5=95.312 rate=8199.88 Hz, eta=0:00:00, total=0:00:00, wall=03:14 IST
** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=0.555 Loss=1.288 Prec@1=68.576 Prec@5=88.528 rate=8199.88 Hz, eta=0:00:00, total=0:00:00, wall=03:14 IST
** validation 100.00% of 1x98...Epoch=104/150 LR=0.02233 Time=0.555 Loss=1.288 Prec@1=68.576 Prec@5=88.528 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=03:14 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:14 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:14 IST
=> training   0.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.833 DataTime=4.607 Loss=1.061 Prec@1=73.828 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=03:14 IST
=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.833 DataTime=4.607 Loss=1.061 Prec@1=73.828 Prec@5=91.992 rate=5242.54 Hz, eta=0:00:00, total=0:00:00, wall=03:14 IST
=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.833 DataTime=4.607 Loss=1.061 Prec@1=73.828 Prec@5=91.992 rate=5242.54 Hz, eta=0:00:00, total=0:00:00, wall=03:15 IST
=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.512 DataTime=0.315 Loss=1.115 Prec@1=72.148 Prec@5=90.354 rate=5242.54 Hz, eta=0:00:00, total=0:00:00, wall=03:15 IST
=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.512 DataTime=0.315 Loss=1.115 Prec@1=72.148 Prec@5=90.354 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=03:15 IST
=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.512 DataTime=0.315 Loss=1.115 Prec@1=72.148 Prec@5=90.354 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=03:15 IST
=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.493 DataTime=0.297 Loss=1.112 Prec@1=72.266 Prec@5=90.437 rate=2.16 Hz, eta=0:18:33, total=0:00:46, wall=03:15 IST
=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.493 DataTime=0.297 Loss=1.112 Prec@1=72.266 Prec@5=90.437 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=03:15 IST
=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.493 DataTime=0.297 Loss=1.112 Prec@1=72.266 Prec@5=90.437 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=03:16 IST
=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.486 DataTime=0.289 Loss=1.110 Prec@1=72.314 Prec@5=90.412 rate=2.13 Hz, eta=0:17:59, total=0:01:34, wall=03:16 IST
=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.486 DataTime=0.289 Loss=1.110 Prec@1=72.314 Prec@5=90.412 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=03:16 IST
=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.486 DataTime=0.289 Loss=1.110 Prec@1=72.314 Prec@5=90.412 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=03:17 IST
=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.266 Prec@5=90.405 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=03:17 IST
=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.266 Prec@5=90.405 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=03:17 IST
=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.266 Prec@5=90.405 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=03:18 IST
=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.113 Prec@1=72.216 Prec@5=90.413 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=03:18 IST
=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.113 Prec@1=72.216 Prec@5=90.413 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=03:18 IST
=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.113 Prec@1=72.216 Prec@5=90.413 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=03:19 IST
=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.248 Prec@5=90.437 rate=2.10 Hz, eta=0:15:51, total=0:03:58, wall=03:19 IST
=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.248 Prec@5=90.437 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=03:19 IST
=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.248 Prec@5=90.437 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=03:19 IST
=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.246 Prec@5=90.424 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=03:19 IST
=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.246 Prec@5=90.424 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=03:19 IST
=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.112 Prec@1=72.246 Prec@5=90.424 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=03:20 IST
=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.212 Prec@5=90.384 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=03:20 IST
=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.212 Prec@5=90.384 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:20 IST
=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.212 Prec@5=90.384 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:21 IST
=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.183 Prec@5=90.374 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:21 IST
=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.183 Prec@5=90.374 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=03:21 IST
=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.114 Prec@1=72.183 Prec@5=90.374 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=03:22 IST
=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.115 Prec@1=72.151 Prec@5=90.357 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=03:22 IST
=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.115 Prec@1=72.151 Prec@5=90.357 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=03:22 IST
=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.115 Prec@1=72.151 Prec@5=90.357 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=03:23 IST
=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.117 Prec@1=72.122 Prec@5=90.345 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=03:23 IST
=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.117 Prec@1=72.122 Prec@5=90.345 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=03:23 IST
=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.117 Prec@1=72.122 Prec@5=90.345 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=03:24 IST
=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.287 Loss=1.118 Prec@1=72.070 Prec@5=90.329 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=03:24 IST
=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.287 Loss=1.118 Prec@1=72.070 Prec@5=90.329 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:24 IST
=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.287 Loss=1.118 Prec@1=72.070 Prec@5=90.329 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:24 IST
=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.119 Prec@1=72.081 Prec@5=90.310 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:24 IST
=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.119 Prec@1=72.081 Prec@5=90.310 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=03:24 IST
=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.119 Prec@1=72.081 Prec@5=90.310 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=03:25 IST
=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.121 Prec@1=72.044 Prec@5=90.290 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=03:25 IST
=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.121 Prec@1=72.044 Prec@5=90.290 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=03:25 IST
=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.121 Prec@1=72.044 Prec@5=90.290 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=03:26 IST
=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.030 Prec@5=90.289 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=03:26 IST
=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.030 Prec@5=90.289 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=03:26 IST
=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.030 Prec@5=90.289 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=03:27 IST
=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.025 Prec@5=90.290 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=03:27 IST
=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.025 Prec@5=90.290 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=03:27 IST
=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.121 Prec@1=72.025 Prec@5=90.290 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=03:28 IST
=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.122 Prec@1=71.994 Prec@5=90.285 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=03:28 IST
=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.122 Prec@1=71.994 Prec@5=90.285 rate=2.07 Hz, eta=0:06:26, total=0:13:39, wall=03:28 IST
=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.122 Prec@1=71.994 Prec@5=90.285 rate=2.07 Hz, eta=0:06:26, total=0:13:39, wall=03:28 IST
=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.122 Prec@1=71.989 Prec@5=90.270 rate=2.07 Hz, eta=0:06:26, total=0:13:39, wall=03:28 IST
=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.122 Prec@1=71.989 Prec@5=90.270 rate=2.08 Hz, eta=0:05:38, total=0:14:27, wall=03:28 IST
=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.288 Loss=1.122 Prec@1=71.989 Prec@5=90.270 rate=2.08 Hz, eta=0:05:38, total=0:14:27, wall=03:29 IST
=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.124 Prec@1=71.964 Prec@5=90.243 rate=2.08 Hz, eta=0:05:38, total=0:14:27, wall=03:29 IST
=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.124 Prec@1=71.964 Prec@5=90.243 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=03:29 IST
=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.124 Prec@1=71.964 Prec@5=90.243 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=03:30 IST
=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.125 Prec@1=71.956 Prec@5=90.226 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=03:30 IST
=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.125 Prec@1=71.956 Prec@5=90.226 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:30 IST
=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.485 DataTime=0.289 Loss=1.125 Prec@1=71.956 Prec@5=90.226 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:31 IST
=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.126 Prec@1=71.932 Prec@5=90.213 rate=2.07 Hz, eta=0:04:02, total=0:16:04, wall=03:31 IST
=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.126 Prec@1=71.932 Prec@5=90.213 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=03:31 IST
=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.126 Prec@1=71.932 Prec@5=90.213 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=03:32 IST
=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.909 Prec@5=90.205 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=03:32 IST
=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.909 Prec@5=90.205 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:32 IST
=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.909 Prec@5=90.205 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:32 IST
=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.892 Prec@5=90.198 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:32 IST
=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.892 Prec@5=90.198 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:32 IST
=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.484 DataTime=0.288 Loss=1.127 Prec@1=71.892 Prec@5=90.198 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:33 IST
=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.128 Prec@1=71.879 Prec@5=90.182 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:33 IST
=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.128 Prec@1=71.879 Prec@5=90.182 rate=2.08 Hz, eta=0:00:49, total=0:19:15, wall=03:33 IST
=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.128 Prec@1=71.879 Prec@5=90.182 rate=2.08 Hz, eta=0:00:49, total=0:19:15, wall=03:34 IST
=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.129 Prec@1=71.867 Prec@5=90.173 rate=2.08 Hz, eta=0:00:49, total=0:19:15, wall=03:34 IST
=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.129 Prec@1=71.867 Prec@5=90.173 rate=2.08 Hz, eta=0:00:00, total=0:20:04, wall=03:34 IST
=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.129 Prec@1=71.867 Prec@5=90.173 rate=2.08 Hz, eta=0:00:00, total=0:20:04, wall=03:34 IST
=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.129 Prec@1=71.867 Prec@5=90.172 rate=2.08 Hz, eta=0:00:00, total=0:20:04, wall=03:34 IST
=> training   100.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.483 DataTime=0.287 Loss=1.129 Prec@1=71.867 Prec@5=90.172 rate=2.08 Hz, eta=0:00:00, total=0:20:04, wall=03:34 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST
=> validation 0.00% of 1x98...Epoch=105/150 LR=0.02146 Time=6.960 Loss=0.765 Prec@1=79.883 Prec@5=95.703 rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST
=> validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=6.960 Loss=0.765 Prec@1=79.883 Prec@5=95.703 rate=7131.75 Hz, eta=0:00:00, total=0:00:00, wall=03:34 IST
** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=6.960 Loss=0.765 Prec@1=79.883 Prec@5=95.703 rate=7131.75 Hz, eta=0:00:00, total=0:00:00, wall=03:35 IST
** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=0.554 Loss=1.279 Prec@1=68.552 Prec@5=88.648 rate=7131.75 Hz, eta=0:00:00, total=0:00:00, wall=03:35 IST
** validation 100.00% of 1x98...Epoch=105/150 LR=0.02146 Time=0.554 Loss=1.279 Prec@1=68.552 Prec@5=88.648 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=03:35 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:35 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:35 IST
=> training   0.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=4.495 DataTime=4.250 Loss=1.115 Prec@1=70.508 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=03:35 IST
=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=4.495 DataTime=4.250 Loss=1.115 Prec@1=70.508 Prec@5=90.234 rate=5089.29 Hz, eta=0:00:00, total=0:00:00, wall=03:35 IST
=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=4.495 DataTime=4.250 Loss=1.115 Prec@1=70.508 Prec@5=90.234 rate=5089.29 Hz, eta=0:00:00, total=0:00:00, wall=03:36 IST
=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.514 DataTime=0.318 Loss=1.109 Prec@1=72.598 Prec@5=90.312 rate=5089.29 Hz, eta=0:00:00, total=0:00:00, wall=03:36 IST
=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.514 DataTime=0.318 Loss=1.109 Prec@1=72.598 Prec@5=90.312 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=03:36 IST
=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.514 DataTime=0.318 Loss=1.109 Prec@1=72.598 Prec@5=90.312 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=03:37 IST
=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.497 DataTime=0.301 Loss=1.099 Prec@1=72.759 Prec@5=90.481 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=03:37 IST
=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.497 DataTime=0.301 Loss=1.099 Prec@1=72.759 Prec@5=90.481 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=03:37 IST
=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.497 DataTime=0.301 Loss=1.099 Prec@1=72.759 Prec@5=90.481 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=03:37 IST
=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.491 DataTime=0.294 Loss=1.098 Prec@1=72.777 Prec@5=90.510 rate=2.11 Hz, eta=0:18:11, total=0:01:35, wall=03:37 IST
=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.491 DataTime=0.294 Loss=1.098 Prec@1=72.777 Prec@5=90.510 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:37 IST
=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.491 DataTime=0.294 Loss=1.098 Prec@1=72.777 Prec@5=90.510 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:38 IST
=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.489 DataTime=0.293 Loss=1.099 Prec@1=72.734 Prec@5=90.516 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:38 IST
=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.489 DataTime=0.293 Loss=1.099 Prec@1=72.734 Prec@5=90.516 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=03:38 IST
=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.489 DataTime=0.293 Loss=1.099 Prec@1=72.734 Prec@5=90.516 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=03:39 IST
=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.100 Prec@1=72.700 Prec@5=90.494 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=03:39 IST
=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.100 Prec@1=72.700 Prec@5=90.494 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=03:39 IST
=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.100 Prec@1=72.700 Prec@5=90.494 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=03:40 IST
=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.102 Prec@1=72.635 Prec@5=90.500 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=03:40 IST
=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.102 Prec@1=72.635 Prec@5=90.500 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=03:40 IST
=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.102 Prec@1=72.635 Prec@5=90.500 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=03:41 IST
=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.104 Prec@1=72.604 Prec@5=90.458 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=03:41 IST
=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.104 Prec@1=72.604 Prec@5=90.458 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=03:41 IST
=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.289 Loss=1.104 Prec@1=72.604 Prec@5=90.458 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=03:41 IST
=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.106 Prec@1=72.542 Prec@5=90.452 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=03:41 IST
=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.106 Prec@1=72.542 Prec@5=90.452 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:41 IST
=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.485 DataTime=0.288 Loss=1.106 Prec@1=72.542 Prec@5=90.452 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:42 IST
=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.106 Prec@1=72.506 Prec@5=90.448 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=03:42 IST
=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.106 Prec@1=72.506 Prec@5=90.448 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=03:42 IST
=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.106 Prec@1=72.506 Prec@5=90.448 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=03:43 IST
=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.108 Prec@1=72.461 Prec@5=90.422 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=03:43 IST
=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.108 Prec@1=72.461 Prec@5=90.422 rate=2.09 Hz, eta=0:11:59, total=0:07:59, wall=03:43 IST
=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.108 Prec@1=72.461 Prec@5=90.422 rate=2.09 Hz, eta=0:11:59, total=0:07:59, wall=03:44 IST
=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.109 Prec@1=72.408 Prec@5=90.397 rate=2.09 Hz, eta=0:11:59, total=0:07:59, wall=03:44 IST
=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.109 Prec@1=72.408 Prec@5=90.397 rate=2.09 Hz, eta=0:11:11, total=0:08:47, wall=03:44 IST
=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.109 Prec@1=72.408 Prec@5=90.397 rate=2.09 Hz, eta=0:11:11, total=0:08:47, wall=03:45 IST
=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.110 Prec@1=72.353 Prec@5=90.378 rate=2.09 Hz, eta=0:11:11, total=0:08:47, wall=03:45 IST
=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.110 Prec@1=72.353 Prec@5=90.378 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:45 IST
=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.110 Prec@1=72.353 Prec@5=90.378 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:45 IST
=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.286 Loss=1.111 Prec@1=72.336 Prec@5=90.374 rate=2.08 Hz, eta=0:10:24, total=0:09:36, wall=03:45 IST
=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.286 Loss=1.111 Prec@1=72.336 Prec@5=90.374 rate=2.09 Hz, eta=0:09:36, total=0:10:23, wall=03:45 IST
=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.286 Loss=1.111 Prec@1=72.336 Prec@5=90.374 rate=2.09 Hz, eta=0:09:36, total=0:10:23, wall=03:46 IST
=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.112 Prec@1=72.308 Prec@5=90.362 rate=2.09 Hz, eta=0:09:36, total=0:10:23, wall=03:46 IST
=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.112 Prec@1=72.308 Prec@5=90.362 rate=2.08 Hz, eta=0:08:49, total=0:11:13, wall=03:46 IST
=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.112 Prec@1=72.308 Prec@5=90.362 rate=2.08 Hz, eta=0:08:49, total=0:11:13, wall=03:47 IST
=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.113 Prec@1=72.281 Prec@5=90.344 rate=2.08 Hz, eta=0:08:49, total=0:11:13, wall=03:47 IST
=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.113 Prec@1=72.281 Prec@5=90.344 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=03:47 IST
=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.113 Prec@1=72.281 Prec@5=90.344 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=03:48 IST
=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.114 Prec@1=72.262 Prec@5=90.338 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=03:48 IST
=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.114 Prec@1=72.262 Prec@5=90.338 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=03:48 IST
=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.114 Prec@1=72.262 Prec@5=90.338 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=03:49 IST
=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.115 Prec@1=72.238 Prec@5=90.327 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=03:49 IST
=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.115 Prec@1=72.238 Prec@5=90.327 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=03:49 IST
=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.115 Prec@1=72.238 Prec@5=90.327 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=03:49 IST
=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.116 Prec@1=72.195 Prec@5=90.306 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=03:49 IST
=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.116 Prec@1=72.195 Prec@5=90.306 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=03:49 IST
=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.116 Prec@1=72.195 Prec@5=90.306 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=03:50 IST
=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.117 Prec@1=72.174 Prec@5=90.298 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=03:50 IST
=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.117 Prec@1=72.174 Prec@5=90.298 rate=2.08 Hz, eta=0:04:49, total=0:15:14, wall=03:50 IST
=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.117 Prec@1=72.174 Prec@5=90.298 rate=2.08 Hz, eta=0:04:49, total=0:15:14, wall=03:51 IST
=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.118 Prec@1=72.159 Prec@5=90.286 rate=2.08 Hz, eta=0:04:49, total=0:15:14, wall=03:51 IST
=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.118 Prec@1=72.159 Prec@5=90.286 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=03:51 IST
=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.483 DataTime=0.287 Loss=1.118 Prec@1=72.159 Prec@5=90.286 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=03:52 IST
=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.119 Prec@1=72.136 Prec@5=90.274 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=03:52 IST
=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.119 Prec@1=72.136 Prec@5=90.274 rate=2.08 Hz, eta=0:03:13, total=0:16:51, wall=03:52 IST
=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.287 Loss=1.119 Prec@1=72.136 Prec@5=90.274 rate=2.08 Hz, eta=0:03:13, total=0:16:51, wall=03:53 IST
=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.115 Prec@5=90.276 rate=2.08 Hz, eta=0:03:13, total=0:16:51, wall=03:53 IST
=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.115 Prec@5=90.276 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:53 IST
=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.115 Prec@5=90.276 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:53 IST
=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.103 Prec@5=90.270 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=03:53 IST
=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.103 Prec@5=90.270 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:53 IST
=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.120 Prec@1=72.103 Prec@5=90.270 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:54 IST
=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.121 Prec@1=72.085 Prec@5=90.261 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=03:54 IST
=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.121 Prec@1=72.085 Prec@5=90.261 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=03:54 IST
=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.121 Prec@1=72.085 Prec@5=90.261 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=03:55 IST
=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.122 Prec@1=72.060 Prec@5=90.248 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=03:55 IST
=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.122 Prec@1=72.060 Prec@5=90.248 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=03:55 IST
=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.122 Prec@1=72.060 Prec@5=90.248 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=03:55 IST
=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.122 Prec@1=72.058 Prec@5=90.248 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=03:55 IST
=> training   100.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.484 DataTime=0.288 Loss=1.122 Prec@1=72.058 Prec@5=90.248 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=03:55 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> validation 0.00% of 1x98...Epoch=106/150 LR=0.02061 Time=7.191 Loss=0.792 Prec@1=78.906 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=7.191 Loss=0.792 Prec@1=78.906 Prec@5=95.312 rate=8417.51 Hz, eta=0:00:00, total=0:00:00, wall=03:55 IST
** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=7.191 Loss=0.792 Prec@1=78.906 Prec@5=95.312 rate=8417.51 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST
** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=0.561 Loss=1.274 Prec@1=68.852 Prec@5=88.854 rate=8417.51 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST
** validation 100.00% of 1x98...Epoch=106/150 LR=0.02061 Time=0.561 Loss=1.274 Prec@1=68.852 Prec@5=88.854 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=03:56 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> training   0.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.316 DataTime=5.126 Loss=1.173 Prec@1=70.898 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.316 DataTime=5.126 Loss=1.173 Prec@1=70.898 Prec@5=89.844 rate=7670.95 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST
=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.316 DataTime=5.126 Loss=1.173 Prec@1=70.898 Prec@5=89.844 rate=7670.95 Hz, eta=0:00:00, total=0:00:00, wall=03:57 IST
=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.515 DataTime=0.322 Loss=1.091 Prec@1=72.685 Prec@5=90.658 rate=7670.95 Hz, eta=0:00:00, total=0:00:00, wall=03:57 IST
=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.515 DataTime=0.322 Loss=1.091 Prec@1=72.685 Prec@5=90.658 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=03:57 IST
=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.515 DataTime=0.322 Loss=1.091 Prec@1=72.685 Prec@5=90.658 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=03:58 IST
=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.496 DataTime=0.301 Loss=1.084 Prec@1=72.784 Prec@5=90.747 rate=2.16 Hz, eta=0:18:30, total=0:00:46, wall=03:58 IST
=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.496 DataTime=0.301 Loss=1.084 Prec@1=72.784 Prec@5=90.747 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=03:58 IST
=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.496 DataTime=0.301 Loss=1.084 Prec@1=72.784 Prec@5=90.747 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=03:59 IST
=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.493 DataTime=0.298 Loss=1.085 Prec@1=72.784 Prec@5=90.790 rate=2.13 Hz, eta=0:18:01, total=0:01:34, wall=03:59 IST
=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.493 DataTime=0.298 Loss=1.085 Prec@1=72.784 Prec@5=90.790 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:59 IST
=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.493 DataTime=0.298 Loss=1.085 Prec@1=72.784 Prec@5=90.790 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:59 IST
=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.492 DataTime=0.298 Loss=1.085 Prec@1=72.822 Prec@5=90.742 rate=2.10 Hz, eta=0:17:27, total=0:02:23, wall=03:59 IST
=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.492 DataTime=0.298 Loss=1.085 Prec@1=72.822 Prec@5=90.742 rate=2.09 Hz, eta=0:16:47, total=0:03:12, wall=03:59 IST
=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.492 DataTime=0.298 Loss=1.085 Prec@1=72.822 Prec@5=90.742 rate=2.09 Hz, eta=0:16:47, total=0:03:12, wall=04:00 IST
=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.489 DataTime=0.295 Loss=1.086 Prec@1=72.811 Prec@5=90.749 rate=2.09 Hz, eta=0:16:47, total=0:03:12, wall=04:00 IST
=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.489 DataTime=0.295 Loss=1.086 Prec@1=72.811 Prec@5=90.749 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=04:00 IST
=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.489 DataTime=0.295 Loss=1.086 Prec@1=72.811 Prec@5=90.749 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=04:01 IST
=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.088 Prec@1=72.772 Prec@5=90.717 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=04:01 IST
=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.088 Prec@1=72.772 Prec@5=90.717 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=04:01 IST
=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.088 Prec@1=72.772 Prec@5=90.717 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=04:02 IST
=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.090 Prec@1=72.748 Prec@5=90.699 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=04:02 IST
=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.090 Prec@1=72.748 Prec@5=90.699 rate=2.09 Hz, eta=0:14:23, total=0:05:36, wall=04:02 IST
=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.090 Prec@1=72.748 Prec@5=90.699 rate=2.09 Hz, eta=0:14:23, total=0:05:36, wall=04:03 IST
=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.093 Prec@1=72.704 Prec@5=90.648 rate=2.09 Hz, eta=0:14:23, total=0:05:36, wall=04:03 IST
=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.093 Prec@1=72.704 Prec@5=90.648 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=04:03 IST
=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.093 Prec@1=72.704 Prec@5=90.648 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=04:03 IST
=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.095 Prec@1=72.672 Prec@5=90.631 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=04:03 IST
=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.095 Prec@1=72.672 Prec@5=90.631 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=04:03 IST
=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.095 Prec@1=72.672 Prec@5=90.631 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=04:04 IST
=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.097 Prec@1=72.626 Prec@5=90.591 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=04:04 IST
=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.097 Prec@1=72.626 Prec@5=90.591 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=04:04 IST
=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.097 Prec@1=72.626 Prec@5=90.591 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=04:05 IST
=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.099 Prec@1=72.565 Prec@5=90.556 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=04:05 IST
=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.099 Prec@1=72.565 Prec@5=90.556 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=04:05 IST
=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.099 Prec@1=72.565 Prec@5=90.556 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=04:06 IST
=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.101 Prec@1=72.506 Prec@5=90.530 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=04:06 IST
=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.101 Prec@1=72.506 Prec@5=90.530 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=04:06 IST
=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.101 Prec@1=72.506 Prec@5=90.530 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=04:07 IST
=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.102 Prec@1=72.494 Prec@5=90.517 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=04:07 IST
=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.102 Prec@1=72.494 Prec@5=90.517 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=04:07 IST
=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.102 Prec@1=72.494 Prec@5=90.517 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=04:07 IST
=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.103 Prec@1=72.480 Prec@5=90.512 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=04:07 IST
=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.103 Prec@1=72.480 Prec@5=90.512 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=04:07 IST
=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.488 DataTime=0.293 Loss=1.103 Prec@1=72.480 Prec@5=90.512 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=04:08 IST
=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.103 Prec@1=72.463 Prec@5=90.516 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=04:08 IST
=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.103 Prec@1=72.463 Prec@5=90.516 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:08 IST
=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.103 Prec@1=72.463 Prec@5=90.516 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:09 IST
=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.449 Prec@5=90.510 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:09 IST
=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.449 Prec@5=90.510 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=04:09 IST
=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.449 Prec@5=90.510 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=04:10 IST
=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.439 Prec@5=90.495 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=04:10 IST
=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.439 Prec@5=90.495 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=04:10 IST
=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.104 Prec@1=72.439 Prec@5=90.495 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=04:11 IST
=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.107 Prec@1=72.391 Prec@5=90.467 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=04:11 IST
=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.107 Prec@1=72.391 Prec@5=90.467 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=04:11 IST
=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.107 Prec@1=72.391 Prec@5=90.467 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=04:11 IST
=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.108 Prec@1=72.363 Prec@5=90.446 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=04:11 IST
=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.108 Prec@1=72.363 Prec@5=90.446 rate=2.07 Hz, eta=0:04:51, total=0:15:20, wall=04:11 IST
=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.108 Prec@1=72.363 Prec@5=90.446 rate=2.07 Hz, eta=0:04:51, total=0:15:20, wall=04:12 IST
=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.109 Prec@1=72.333 Prec@5=90.428 rate=2.07 Hz, eta=0:04:51, total=0:15:20, wall=04:12 IST
=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.109 Prec@1=72.333 Prec@5=90.428 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=04:12 IST
=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.487 DataTime=0.292 Loss=1.109 Prec@1=72.333 Prec@5=90.428 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=04:13 IST
=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.291 Loss=1.110 Prec@1=72.313 Prec@5=90.415 rate=2.07 Hz, eta=0:04:03, total=0:16:08, wall=04:13 IST
=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.291 Loss=1.110 Prec@1=72.313 Prec@5=90.415 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:13 IST
=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.291 Loss=1.110 Prec@1=72.313 Prec@5=90.415 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:14 IST
=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.111 Prec@1=72.301 Prec@5=90.402 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:14 IST
=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.111 Prec@1=72.301 Prec@5=90.402 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:14 IST
=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.111 Prec@1=72.301 Prec@5=90.402 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:15 IST
=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.112 Prec@1=72.290 Prec@5=90.394 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:15 IST
=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.112 Prec@1=72.290 Prec@5=90.394 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:15 IST
=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.486 DataTime=0.290 Loss=1.112 Prec@1=72.290 Prec@5=90.394 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:15 IST
=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.262 Prec@5=90.382 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:15 IST
=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.262 Prec@5=90.382 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=04:15 IST
=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.262 Prec@5=90.382 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=04:16 IST
=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.247 Prec@5=90.377 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=04:16 IST
=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.247 Prec@5=90.377 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=04:16 IST
=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.247 Prec@5=90.377 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=04:16 IST
=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.246 Prec@5=90.377 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=04:16 IST
=> training   100.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.485 DataTime=0.290 Loss=1.113 Prec@1=72.246 Prec@5=90.377 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=04:16 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> validation 0.00% of 1x98...Epoch=107/150 LR=0.01977 Time=6.811 Loss=0.729 Prec@1=82.227 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=6.811 Loss=0.729 Prec@1=82.227 Prec@5=94.922 rate=8084.73 Hz, eta=0:00:00, total=0:00:00, wall=04:16 IST
** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=6.811 Loss=0.729 Prec@1=82.227 Prec@5=94.922 rate=8084.73 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST
** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=0.557 Loss=1.265 Prec@1=69.178 Prec@5=88.844 rate=8084.73 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST
** validation 100.00% of 1x98...Epoch=107/150 LR=0.01977 Time=0.557 Loss=1.265 Prec@1=69.178 Prec@5=88.844 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=04:17 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST
=> training   0.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.103 DataTime=4.897 Loss=1.077 Prec@1=70.117 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST
=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.103 DataTime=4.897 Loss=1.077 Prec@1=70.117 Prec@5=91.211 rate=6317.44 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST
=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.103 DataTime=4.897 Loss=1.077 Prec@1=70.117 Prec@5=91.211 rate=6317.44 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST
=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.520 DataTime=0.325 Loss=1.075 Prec@1=72.971 Prec@5=90.903 rate=6317.44 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST
=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.520 DataTime=0.325 Loss=1.075 Prec@1=72.971 Prec@5=90.903 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=04:18 IST
=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.520 DataTime=0.325 Loss=1.075 Prec@1=72.971 Prec@5=90.903 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=04:19 IST
=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.495 DataTime=0.299 Loss=1.078 Prec@1=72.962 Prec@5=90.769 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=04:19 IST
=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.495 DataTime=0.299 Loss=1.078 Prec@1=72.962 Prec@5=90.769 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=04:19 IST
=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.495 DataTime=0.299 Loss=1.078 Prec@1=72.962 Prec@5=90.769 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=04:20 IST
=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.080 Prec@1=72.926 Prec@5=90.687 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=04:20 IST
=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.080 Prec@1=72.926 Prec@5=90.687 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=04:20 IST
=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.080 Prec@1=72.926 Prec@5=90.687 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=04:21 IST
=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.085 Prec@1=72.818 Prec@5=90.641 rate=2.13 Hz, eta=0:17:14, total=0:02:21, wall=04:21 IST
=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.085 Prec@1=72.818 Prec@5=90.641 rate=2.11 Hz, eta=0:16:33, total=0:03:09, wall=04:21 IST
=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.085 Prec@1=72.818 Prec@5=90.641 rate=2.11 Hz, eta=0:16:33, total=0:03:09, wall=04:21 IST
=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.085 Prec@1=72.807 Prec@5=90.648 rate=2.11 Hz, eta=0:16:33, total=0:03:09, wall=04:21 IST
=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.085 Prec@1=72.807 Prec@5=90.648 rate=2.12 Hz, eta=0:15:45, total=0:03:56, wall=04:21 IST
=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.085 Prec@1=72.807 Prec@5=90.648 rate=2.12 Hz, eta=0:15:45, total=0:03:56, wall=04:22 IST
=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.088 Prec@1=72.771 Prec@5=90.608 rate=2.12 Hz, eta=0:15:45, total=0:03:56, wall=04:22 IST
=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.088 Prec@1=72.771 Prec@5=90.608 rate=2.11 Hz, eta=0:15:02, total=0:04:45, wall=04:22 IST
=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.088 Prec@1=72.771 Prec@5=90.608 rate=2.11 Hz, eta=0:15:02, total=0:04:45, wall=04:23 IST
=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.089 Prec@1=72.761 Prec@5=90.616 rate=2.11 Hz, eta=0:15:02, total=0:04:45, wall=04:23 IST
=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.089 Prec@1=72.761 Prec@5=90.616 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=04:23 IST
=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.483 DataTime=0.288 Loss=1.089 Prec@1=72.761 Prec@5=90.616 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=04:24 IST
=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.288 Loss=1.090 Prec@1=72.753 Prec@5=90.616 rate=2.10 Hz, eta=0:14:16, total=0:05:33, wall=04:24 IST
=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.288 Loss=1.090 Prec@1=72.753 Prec@5=90.616 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=04:24 IST
=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.288 Loss=1.090 Prec@1=72.753 Prec@5=90.616 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=04:25 IST
=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.092 Prec@1=72.705 Prec@5=90.596 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=04:25 IST
=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.092 Prec@1=72.705 Prec@5=90.596 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=04:25 IST
=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.092 Prec@1=72.705 Prec@5=90.596 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=04:25 IST
=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.093 Prec@1=72.678 Prec@5=90.578 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=04:25 IST
=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.093 Prec@1=72.678 Prec@5=90.578 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=04:25 IST
=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.093 Prec@1=72.678 Prec@5=90.578 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=04:26 IST
=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.093 Prec@1=72.662 Prec@5=90.568 rate=2.08 Hz, eta=0:12:01, total=0:08:00, wall=04:26 IST
=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.093 Prec@1=72.662 Prec@5=90.568 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=04:26 IST
=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.093 Prec@1=72.662 Prec@5=90.568 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=04:27 IST
=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.095 Prec@1=72.632 Prec@5=90.572 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=04:27 IST
=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.095 Prec@1=72.632 Prec@5=90.572 rate=2.08 Hz, eta=0:10:25, total=0:09:36, wall=04:27 IST
=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.484 DataTime=0.289 Loss=1.095 Prec@1=72.632 Prec@5=90.572 rate=2.08 Hz, eta=0:10:25, total=0:09:36, wall=04:28 IST
=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.096 Prec@1=72.604 Prec@5=90.563 rate=2.08 Hz, eta=0:10:25, total=0:09:36, wall=04:28 IST
=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.096 Prec@1=72.604 Prec@5=90.563 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:28 IST
=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.096 Prec@1=72.604 Prec@5=90.563 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:29 IST
=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.097 Prec@1=72.572 Prec@5=90.544 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:29 IST
=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.097 Prec@1=72.572 Prec@5=90.544 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=04:29 IST
=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.097 Prec@1=72.572 Prec@5=90.544 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=04:29 IST
=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.098 Prec@1=72.556 Prec@5=90.527 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=04:29 IST
=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.098 Prec@1=72.556 Prec@5=90.527 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:29 IST
=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.487 DataTime=0.291 Loss=1.098 Prec@1=72.556 Prec@5=90.527 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:30 IST
=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.099 Prec@1=72.543 Prec@5=90.509 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=04:30 IST
=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.099 Prec@1=72.543 Prec@5=90.509 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=04:30 IST
=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.099 Prec@1=72.543 Prec@5=90.509 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=04:31 IST
=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.099 Prec@1=72.541 Prec@5=90.503 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=04:31 IST
=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.099 Prec@1=72.541 Prec@5=90.503 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=04:31 IST
=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.099 Prec@1=72.541 Prec@5=90.503 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=04:32 IST
=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.100 Prec@1=72.524 Prec@5=90.490 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=04:32 IST
=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.100 Prec@1=72.524 Prec@5=90.490 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=04:32 IST
=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.100 Prec@1=72.524 Prec@5=90.490 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=04:33 IST
=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.101 Prec@1=72.502 Prec@5=90.485 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=04:33 IST
=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.101 Prec@1=72.502 Prec@5=90.485 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=04:33 IST
=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.101 Prec@1=72.502 Prec@5=90.485 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=04:33 IST
=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.491 Prec@5=90.484 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=04:33 IST
=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.491 Prec@5=90.484 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=04:33 IST
=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.491 Prec@5=90.484 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=04:34 IST
=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.477 Prec@5=90.476 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=04:34 IST
=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.477 Prec@5=90.476 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:34 IST
=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.102 Prec@1=72.477 Prec@5=90.476 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:35 IST
=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.470 Prec@5=90.473 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=04:35 IST
=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.470 Prec@5=90.473 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:35 IST
=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.470 Prec@5=90.473 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:36 IST
=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.452 Prec@5=90.463 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=04:36 IST
=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.452 Prec@5=90.463 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:36 IST
=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.290 Loss=1.103 Prec@1=72.452 Prec@5=90.463 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:37 IST
=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.104 Prec@1=72.424 Prec@5=90.446 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=04:37 IST
=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.104 Prec@1=72.424 Prec@5=90.446 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=04:37 IST
=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.486 DataTime=0.291 Loss=1.104 Prec@1=72.424 Prec@5=90.446 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=04:38 IST
=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.106 Prec@1=72.402 Prec@5=90.433 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=04:38 IST
=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.106 Prec@1=72.402 Prec@5=90.433 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=04:38 IST
=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.106 Prec@1=72.402 Prec@5=90.433 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=04:38 IST
=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.106 Prec@1=72.402 Prec@5=90.433 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=04:38 IST
=> training   100.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.485 DataTime=0.290 Loss=1.106 Prec@1=72.402 Prec@5=90.433 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=04:38 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:38 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:38 IST
=> validation 0.00% of 1x98...Epoch=108/150 LR=0.01894 Time=6.251 Loss=0.774 Prec@1=80.469 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=04:38 IST
=> validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.251 Loss=0.774 Prec@1=80.469 Prec@5=95.508 rate=8300.06 Hz, eta=0:00:00, total=0:00:00, wall=04:38 IST
** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.251 Loss=0.774 Prec@1=80.469 Prec@5=95.508 rate=8300.06 Hz, eta=0:00:00, total=0:00:00, wall=04:38 IST
** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=0.550 Loss=1.270 Prec@1=69.048 Prec@5=88.768 rate=8300.06 Hz, eta=0:00:00, total=0:00:00, wall=04:38 IST
** validation 100.00% of 1x98...Epoch=108/150 LR=0.01894 Time=0.550 Loss=1.270 Prec@1=69.048 Prec@5=88.768 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=04:38 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> training   0.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.284 DataTime=5.094 Loss=1.074 Prec@1=71.680 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.284 DataTime=5.094 Loss=1.074 Prec@1=71.680 Prec@5=92.383 rate=9968.50 Hz, eta=0:00:00, total=0:00:00, wall=04:39 IST
=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.284 DataTime=5.094 Loss=1.074 Prec@1=71.680 Prec@5=92.383 rate=9968.50 Hz, eta=0:00:00, total=0:00:00, wall=04:39 IST
=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.523 DataTime=0.327 Loss=1.070 Prec@1=73.134 Prec@5=90.890 rate=9968.50 Hz, eta=0:00:00, total=0:00:00, wall=04:39 IST
=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.523 DataTime=0.327 Loss=1.070 Prec@1=73.134 Prec@5=90.890 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=04:39 IST
=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.523 DataTime=0.327 Loss=1.070 Prec@1=73.134 Prec@5=90.890 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=04:40 IST
=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.504 DataTime=0.307 Loss=1.077 Prec@1=73.187 Prec@5=90.749 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=04:40 IST
=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.504 DataTime=0.307 Loss=1.077 Prec@1=73.187 Prec@5=90.749 rate=2.09 Hz, eta=0:18:20, total=0:01:36, wall=04:40 IST
=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.504 DataTime=0.307 Loss=1.077 Prec@1=73.187 Prec@5=90.749 rate=2.09 Hz, eta=0:18:20, total=0:01:36, wall=04:41 IST
=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.492 DataTime=0.294 Loss=1.077 Prec@1=73.121 Prec@5=90.768 rate=2.09 Hz, eta=0:18:20, total=0:01:36, wall=04:41 IST
=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.492 DataTime=0.294 Loss=1.077 Prec@1=73.121 Prec@5=90.768 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=04:41 IST
=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.492 DataTime=0.294 Loss=1.077 Prec@1=73.121 Prec@5=90.768 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=04:42 IST
=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.489 DataTime=0.292 Loss=1.077 Prec@1=73.021 Prec@5=90.830 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=04:42 IST
=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.489 DataTime=0.292 Loss=1.077 Prec@1=73.021 Prec@5=90.830 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=04:42 IST
=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.489 DataTime=0.292 Loss=1.077 Prec@1=73.021 Prec@5=90.830 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=04:43 IST
=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.487 DataTime=0.291 Loss=1.077 Prec@1=73.040 Prec@5=90.842 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=04:43 IST
=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.487 DataTime=0.291 Loss=1.077 Prec@1=73.040 Prec@5=90.842 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=04:43 IST
=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.487 DataTime=0.291 Loss=1.077 Prec@1=73.040 Prec@5=90.842 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=04:43 IST
=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.078 Prec@1=73.016 Prec@5=90.835 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=04:43 IST
=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.078 Prec@1=73.016 Prec@5=90.835 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=04:43 IST
=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.078 Prec@1=73.016 Prec@5=90.835 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=04:44 IST
=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.290 Loss=1.079 Prec@1=73.039 Prec@5=90.811 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=04:44 IST
=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.290 Loss=1.079 Prec@1=73.039 Prec@5=90.811 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=04:44 IST
=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.290 Loss=1.079 Prec@1=73.039 Prec@5=90.811 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=04:45 IST
=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.080 Prec@1=73.028 Prec@5=90.801 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=04:45 IST
=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.080 Prec@1=73.028 Prec@5=90.801 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=04:45 IST
=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.080 Prec@1=73.028 Prec@5=90.801 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=04:46 IST
=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.082 Prec@1=72.994 Prec@5=90.766 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=04:46 IST
=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.082 Prec@1=72.994 Prec@5=90.766 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=04:46 IST
=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.082 Prec@1=72.994 Prec@5=90.766 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=04:47 IST
=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.083 Prec@1=72.955 Prec@5=90.749 rate=2.08 Hz, eta=0:12:48, total=0:07:12, wall=04:47 IST
=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.083 Prec@1=72.955 Prec@5=90.749 rate=2.09 Hz, eta=0:12:00, total=0:08:00, wall=04:47 IST
=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.083 Prec@1=72.955 Prec@5=90.749 rate=2.09 Hz, eta=0:12:00, total=0:08:00, wall=04:47 IST
=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.086 Prec@1=72.913 Prec@5=90.707 rate=2.09 Hz, eta=0:12:00, total=0:08:00, wall=04:47 IST
=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.086 Prec@1=72.913 Prec@5=90.707 rate=2.08 Hz, eta=0:11:13, total=0:08:48, wall=04:47 IST
=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.288 Loss=1.086 Prec@1=72.913 Prec@5=90.707 rate=2.08 Hz, eta=0:11:13, total=0:08:48, wall=04:48 IST
=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.088 Prec@1=72.878 Prec@5=90.685 rate=2.08 Hz, eta=0:11:13, total=0:08:48, wall=04:48 IST
=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.088 Prec@1=72.878 Prec@5=90.685 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=04:48 IST
=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.486 DataTime=0.289 Loss=1.088 Prec@1=72.878 Prec@5=90.685 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=04:49 IST
=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.838 Prec@5=90.663 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=04:49 IST
=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.838 Prec@5=90.663 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:49 IST
=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.838 Prec@5=90.663 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:50 IST
=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.825 Prec@5=90.654 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=04:50 IST
=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.825 Prec@5=90.654 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=04:50 IST
=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.089 Prec@1=72.825 Prec@5=90.654 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=04:51 IST
=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.090 Prec@1=72.806 Prec@5=90.649 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=04:51 IST
=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.090 Prec@1=72.806 Prec@5=90.649 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=04:51 IST
=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.090 Prec@1=72.806 Prec@5=90.649 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=04:51 IST
=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.773 Prec@5=90.637 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=04:51 IST
=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.773 Prec@5=90.637 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=04:51 IST
=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.773 Prec@5=90.637 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=04:52 IST
=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.766 Prec@5=90.633 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=04:52 IST
=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.766 Prec@5=90.633 rate=2.08 Hz, eta=0:06:26, total=0:13:39, wall=04:52 IST
=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.091 Prec@1=72.766 Prec@5=90.633 rate=2.08 Hz, eta=0:06:26, total=0:13:39, wall=04:53 IST
=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.092 Prec@1=72.748 Prec@5=90.620 rate=2.08 Hz, eta=0:06:26, total=0:13:39, wall=04:53 IST
=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.092 Prec@1=72.748 Prec@5=90.620 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=04:53 IST
=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.092 Prec@1=72.748 Prec@5=90.620 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=04:54 IST
=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.093 Prec@1=72.733 Prec@5=90.609 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=04:54 IST
=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.093 Prec@1=72.733 Prec@5=90.609 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=04:54 IST
=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.093 Prec@1=72.733 Prec@5=90.609 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=04:55 IST
=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.094 Prec@1=72.700 Prec@5=90.590 rate=2.07 Hz, eta=0:04:50, total=0:15:16, wall=04:55 IST
=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.094 Prec@1=72.700 Prec@5=90.590 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=04:55 IST
=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.094 Prec@1=72.700 Prec@5=90.590 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=04:55 IST
=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.095 Prec@1=72.673 Prec@5=90.585 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=04:55 IST
=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.095 Prec@1=72.673 Prec@5=90.585 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=04:55 IST
=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.095 Prec@1=72.673 Prec@5=90.585 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=04:56 IST
=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.656 Prec@5=90.573 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=04:56 IST
=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.656 Prec@5=90.573 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=04:56 IST
=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.656 Prec@5=90.573 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=04:57 IST
=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.288 Loss=1.096 Prec@1=72.644 Prec@5=90.578 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=04:57 IST
=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.288 Loss=1.096 Prec@1=72.644 Prec@5=90.578 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=04:57 IST
=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.288 Loss=1.096 Prec@1=72.644 Prec@5=90.578 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=04:58 IST
=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.096 Prec@1=72.635 Prec@5=90.578 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=04:58 IST
=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.096 Prec@1=72.635 Prec@5=90.578 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=04:58 IST
=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.485 DataTime=0.289 Loss=1.096 Prec@1=72.635 Prec@5=90.578 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=04:59 IST
=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.636 Prec@5=90.572 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=04:59 IST
=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.636 Prec@5=90.572 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=04:59 IST
=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.636 Prec@5=90.572 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=04:59 IST
=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.635 Prec@5=90.572 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=04:59 IST
=> training   100.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.484 DataTime=0.289 Loss=1.096 Prec@1=72.635 Prec@5=90.572 rate=2.08 Hz, eta=0:00:00, total=0:20:06, wall=04:59 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST
=> validation 0.00% of 1x98...Epoch=109/150 LR=0.01813 Time=6.972 Loss=0.721 Prec@1=80.664 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST
=> validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.972 Loss=0.721 Prec@1=80.664 Prec@5=94.531 rate=6112.95 Hz, eta=0:00:00, total=0:00:00, wall=04:59 IST
** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.972 Loss=0.721 Prec@1=80.664 Prec@5=94.531 rate=6112.95 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST
** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=0.557 Loss=1.255 Prec@1=69.202 Prec@5=88.896 rate=6112.95 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST
** validation 100.00% of 1x98...Epoch=109/150 LR=0.01813 Time=0.557 Loss=1.255 Prec@1=69.202 Prec@5=88.896 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=05:00 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:00 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:00 IST
=> training   0.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=4.469 DataTime=4.213 Loss=0.961 Prec@1=75.977 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=05:00 IST
=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=4.469 DataTime=4.213 Loss=0.961 Prec@1=75.977 Prec@5=92.383 rate=7311.60 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST
=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=4.469 DataTime=4.213 Loss=0.961 Prec@1=75.977 Prec@5=92.383 rate=7311.60 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST
=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.526 DataTime=0.333 Loss=1.053 Prec@1=73.517 Prec@5=91.190 rate=7311.60 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST
=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.526 DataTime=0.333 Loss=1.053 Prec@1=73.517 Prec@5=91.190 rate=2.08 Hz, eta=0:19:16, total=0:00:48, wall=05:00 IST
=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.526 DataTime=0.333 Loss=1.053 Prec@1=73.517 Prec@5=91.190 rate=2.08 Hz, eta=0:19:16, total=0:00:48, wall=05:01 IST
=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.503 DataTime=0.307 Loss=1.058 Prec@1=73.404 Prec@5=91.037 rate=2.08 Hz, eta=0:19:16, total=0:00:48, wall=05:01 IST
=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.503 DataTime=0.307 Loss=1.058 Prec@1=73.404 Prec@5=91.037 rate=2.08 Hz, eta=0:18:26, total=0:01:36, wall=05:01 IST
=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.503 DataTime=0.307 Loss=1.058 Prec@1=73.404 Prec@5=91.037 rate=2.08 Hz, eta=0:18:26, total=0:01:36, wall=05:02 IST
=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.494 DataTime=0.300 Loss=1.059 Prec@1=73.443 Prec@5=91.034 rate=2.08 Hz, eta=0:18:26, total=0:01:36, wall=05:02 IST
=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.494 DataTime=0.300 Loss=1.059 Prec@1=73.443 Prec@5=91.034 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=05:02 IST
=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.494 DataTime=0.300 Loss=1.059 Prec@1=73.443 Prec@5=91.034 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=05:03 IST
=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.298 Loss=1.057 Prec@1=73.499 Prec@5=91.074 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=05:03 IST
=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.298 Loss=1.057 Prec@1=73.499 Prec@5=91.074 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=05:03 IST
=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.298 Loss=1.057 Prec@1=73.499 Prec@5=91.074 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=05:04 IST
=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.060 Prec@1=73.436 Prec@5=91.030 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=05:04 IST
=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.060 Prec@1=73.436 Prec@5=91.030 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=05:04 IST
=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.060 Prec@1=73.436 Prec@5=91.030 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=05:05 IST
=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.297 Loss=1.061 Prec@1=73.379 Prec@5=91.018 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=05:05 IST
=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.297 Loss=1.061 Prec@1=73.379 Prec@5=91.018 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=05:05 IST
=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.492 DataTime=0.297 Loss=1.061 Prec@1=73.379 Prec@5=91.018 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=05:05 IST
=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.064 Prec@1=73.327 Prec@5=90.983 rate=2.07 Hz, eta=0:15:20, total=0:04:50, wall=05:05 IST
=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.064 Prec@1=73.327 Prec@5=90.983 rate=2.07 Hz, eta=0:14:31, total=0:05:39, wall=05:05 IST
=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.064 Prec@1=73.327 Prec@5=90.983 rate=2.07 Hz, eta=0:14:31, total=0:05:39, wall=05:06 IST
=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.065 Prec@1=73.270 Prec@5=90.961 rate=2.07 Hz, eta=0:14:31, total=0:05:39, wall=05:06 IST
=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.065 Prec@1=73.270 Prec@5=90.961 rate=2.06 Hz, eta=0:13:44, total=0:06:28, wall=05:06 IST
=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.065 Prec@1=73.270 Prec@5=90.961 rate=2.06 Hz, eta=0:13:44, total=0:06:28, wall=05:07 IST
=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.066 Prec@1=73.254 Prec@5=90.976 rate=2.06 Hz, eta=0:13:44, total=0:06:28, wall=05:07 IST
=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.066 Prec@1=73.254 Prec@5=90.976 rate=2.06 Hz, eta=0:12:57, total=0:07:17, wall=05:07 IST
=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.490 DataTime=0.296 Loss=1.066 Prec@1=73.254 Prec@5=90.976 rate=2.06 Hz, eta=0:12:57, total=0:07:17, wall=05:08 IST
=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.491 DataTime=0.296 Loss=1.068 Prec@1=73.218 Prec@5=90.948 rate=2.06 Hz, eta=0:12:57, total=0:07:17, wall=05:08 IST
=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.491 DataTime=0.296 Loss=1.068 Prec@1=73.218 Prec@5=90.948 rate=2.06 Hz, eta=0:12:10, total=0:08:06, wall=05:08 IST
=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.491 DataTime=0.296 Loss=1.068 Prec@1=73.218 Prec@5=90.948 rate=2.06 Hz, eta=0:12:10, total=0:08:06, wall=05:09 IST
=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.069 Prec@1=73.196 Prec@5=90.926 rate=2.06 Hz, eta=0:12:10, total=0:08:06, wall=05:09 IST
=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.069 Prec@1=73.196 Prec@5=90.926 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=05:09 IST
=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.069 Prec@1=73.196 Prec@5=90.926 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=05:09 IST
=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.071 Prec@1=73.153 Prec@5=90.890 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=05:09 IST
=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.071 Prec@1=73.153 Prec@5=90.890 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=05:09 IST
=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.071 Prec@1=73.153 Prec@5=90.890 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=05:10 IST
=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.073 Prec@1=73.111 Prec@5=90.862 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=05:10 IST
=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.073 Prec@1=73.111 Prec@5=90.862 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=05:10 IST
=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.073 Prec@1=73.111 Prec@5=90.862 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=05:11 IST
=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.073 Prec@1=73.096 Prec@5=90.851 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=05:11 IST
=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.073 Prec@1=73.096 Prec@5=90.851 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=05:11 IST
=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.073 Prec@1=73.096 Prec@5=90.851 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=05:12 IST
=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.075 Prec@1=73.059 Prec@5=90.833 rate=2.06 Hz, eta=0:08:55, total=0:11:20, wall=05:12 IST
=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.075 Prec@1=73.059 Prec@5=90.833 rate=2.06 Hz, eta=0:08:06, total=0:12:09, wall=05:12 IST
=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.075 Prec@1=73.059 Prec@5=90.833 rate=2.06 Hz, eta=0:08:06, total=0:12:09, wall=05:13 IST
=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.076 Prec@1=73.038 Prec@5=90.823 rate=2.06 Hz, eta=0:08:06, total=0:12:09, wall=05:13 IST
=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.076 Prec@1=73.038 Prec@5=90.823 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=05:13 IST
=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.076 Prec@1=73.038 Prec@5=90.823 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=05:13 IST
=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.077 Prec@1=73.007 Prec@5=90.814 rate=2.06 Hz, eta=0:07:18, total=0:12:58, wall=05:13 IST
=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.077 Prec@1=73.007 Prec@5=90.814 rate=2.06 Hz, eta=0:06:30, total=0:13:47, wall=05:13 IST
=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.077 Prec@1=73.007 Prec@5=90.814 rate=2.06 Hz, eta=0:06:30, total=0:13:47, wall=05:14 IST
=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.985 Prec@5=90.801 rate=2.06 Hz, eta=0:06:30, total=0:13:47, wall=05:14 IST
=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.985 Prec@5=90.801 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=05:14 IST
=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.985 Prec@5=90.801 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=05:15 IST
=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.962 Prec@5=90.791 rate=2.06 Hz, eta=0:05:41, total=0:14:35, wall=05:15 IST
=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.962 Prec@5=90.791 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=05:15 IST
=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.078 Prec@1=72.962 Prec@5=90.791 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=05:16 IST
=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.080 Prec@1=72.946 Prec@5=90.760 rate=2.06 Hz, eta=0:04:52, total=0:15:24, wall=05:16 IST
=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.080 Prec@1=72.946 Prec@5=90.760 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=05:16 IST
=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.080 Prec@1=72.946 Prec@5=90.760 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=05:17 IST
=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.081 Prec@1=72.934 Prec@5=90.757 rate=2.05 Hz, eta=0:04:04, total=0:16:13, wall=05:17 IST
=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.081 Prec@1=72.934 Prec@5=90.757 rate=2.05 Hz, eta=0:03:15, total=0:17:03, wall=05:17 IST
=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.295 Loss=1.081 Prec@1=72.934 Prec@5=90.757 rate=2.05 Hz, eta=0:03:15, total=0:17:03, wall=05:18 IST
=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.082 Prec@1=72.913 Prec@5=90.735 rate=2.05 Hz, eta=0:03:15, total=0:17:03, wall=05:18 IST
=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.082 Prec@1=72.913 Prec@5=90.735 rate=2.05 Hz, eta=0:02:27, total=0:17:51, wall=05:18 IST
=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.489 DataTime=0.294 Loss=1.082 Prec@1=72.913 Prec@5=90.735 rate=2.05 Hz, eta=0:02:27, total=0:17:51, wall=05:18 IST
=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.083 Prec@1=72.893 Prec@5=90.718 rate=2.05 Hz, eta=0:02:27, total=0:17:51, wall=05:18 IST
=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.083 Prec@1=72.893 Prec@5=90.718 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=05:18 IST
=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.083 Prec@1=72.893 Prec@5=90.718 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=05:19 IST
=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.084 Prec@1=72.884 Prec@5=90.711 rate=2.06 Hz, eta=0:01:38, total=0:18:39, wall=05:19 IST
=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.084 Prec@1=72.884 Prec@5=90.711 rate=2.06 Hz, eta=0:00:49, total=0:19:28, wall=05:19 IST
=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.294 Loss=1.084 Prec@1=72.884 Prec@5=90.711 rate=2.06 Hz, eta=0:00:49, total=0:19:28, wall=05:20 IST
=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.085 Prec@1=72.862 Prec@5=90.695 rate=2.06 Hz, eta=0:00:49, total=0:19:28, wall=05:20 IST
=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.085 Prec@1=72.862 Prec@5=90.695 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=05:20 IST
=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.085 Prec@1=72.862 Prec@5=90.695 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=05:20 IST
=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.085 Prec@1=72.862 Prec@5=90.695 rate=2.06 Hz, eta=0:00:00, total=0:20:15, wall=05:20 IST
=> training   100.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.488 DataTime=0.293 Loss=1.085 Prec@1=72.862 Prec@5=90.695 rate=2.06 Hz, eta=0:00:00, total=0:20:16, wall=05:20 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:20 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:20 IST
=> validation 0.00% of 1x98...Epoch=110/150 LR=0.01733 Time=6.710 Loss=0.765 Prec@1=80.273 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=05:20 IST
=> validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=6.710 Loss=0.765 Prec@1=80.273 Prec@5=95.117 rate=8562.67 Hz, eta=0:00:00, total=0:00:00, wall=05:20 IST
** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=6.710 Loss=0.765 Prec@1=80.273 Prec@5=95.117 rate=8562.67 Hz, eta=0:00:00, total=0:00:00, wall=05:21 IST
** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=0.557 Loss=1.242 Prec@1=69.474 Prec@5=89.150 rate=8562.67 Hz, eta=0:00:00, total=0:00:00, wall=05:21 IST
** validation 100.00% of 1x98...Epoch=110/150 LR=0.01733 Time=0.557 Loss=1.242 Prec@1=69.474 Prec@5=89.150 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=05:21 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:21 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:21 IST
=> training   0.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.667 DataTime=4.392 Loss=1.047 Prec@1=71.875 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=05:21 IST
=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.667 DataTime=4.392 Loss=1.047 Prec@1=71.875 Prec@5=90.625 rate=6671.25 Hz, eta=0:00:00, total=0:00:00, wall=05:21 IST
=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.667 DataTime=4.392 Loss=1.047 Prec@1=71.875 Prec@5=90.625 rate=6671.25 Hz, eta=0:00:00, total=0:00:00, wall=05:22 IST
=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.508 DataTime=0.314 Loss=1.056 Prec@1=73.641 Prec@5=91.114 rate=6671.25 Hz, eta=0:00:00, total=0:00:00, wall=05:22 IST
=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.508 DataTime=0.314 Loss=1.056 Prec@1=73.641 Prec@5=91.114 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=05:22 IST
=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.508 DataTime=0.314 Loss=1.056 Prec@1=73.641 Prec@5=91.114 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=05:23 IST
=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.496 DataTime=0.301 Loss=1.057 Prec@1=73.513 Prec@5=91.091 rate=2.17 Hz, eta=0:18:29, total=0:00:46, wall=05:23 IST
=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.496 DataTime=0.301 Loss=1.057 Prec@1=73.513 Prec@5=91.091 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=05:23 IST
=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.496 DataTime=0.301 Loss=1.057 Prec@1=73.513 Prec@5=91.091 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=05:23 IST
=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.488 DataTime=0.293 Loss=1.058 Prec@1=73.494 Prec@5=91.047 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=05:23 IST
=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.488 DataTime=0.293 Loss=1.058 Prec@1=73.494 Prec@5=91.047 rate=2.11 Hz, eta=0:17:21, total=0:02:22, wall=05:23 IST
=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.488 DataTime=0.293 Loss=1.058 Prec@1=73.494 Prec@5=91.047 rate=2.11 Hz, eta=0:17:21, total=0:02:22, wall=05:24 IST
=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.058 Prec@1=73.504 Prec@5=91.049 rate=2.11 Hz, eta=0:17:21, total=0:02:22, wall=05:24 IST
=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.058 Prec@1=73.504 Prec@5=91.049 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=05:24 IST
=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.058 Prec@1=73.504 Prec@5=91.049 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=05:25 IST
=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.059 Prec@1=73.516 Prec@5=91.031 rate=2.10 Hz, eta=0:16:40, total=0:03:10, wall=05:25 IST
=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.059 Prec@1=73.516 Prec@5=91.031 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=05:25 IST
=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.059 Prec@1=73.516 Prec@5=91.031 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=05:26 IST
=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.484 DataTime=0.289 Loss=1.059 Prec@1=73.540 Prec@5=91.036 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=05:26 IST
=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.484 DataTime=0.289 Loss=1.059 Prec@1=73.540 Prec@5=91.036 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=05:26 IST
=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.484 DataTime=0.289 Loss=1.059 Prec@1=73.540 Prec@5=91.036 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=05:27 IST
=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.485 DataTime=0.289 Loss=1.059 Prec@1=73.537 Prec@5=91.044 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=05:27 IST
=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.485 DataTime=0.289 Loss=1.059 Prec@1=73.537 Prec@5=91.044 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=05:27 IST
=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.485 DataTime=0.289 Loss=1.059 Prec@1=73.537 Prec@5=91.044 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=05:27 IST
=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.062 Prec@1=73.462 Prec@5=91.027 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=05:27 IST
=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.062 Prec@1=73.462 Prec@5=91.027 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=05:27 IST
=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.062 Prec@1=73.462 Prec@5=91.027 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=05:28 IST
=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.063 Prec@1=73.441 Prec@5=91.002 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=05:28 IST
=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.063 Prec@1=73.441 Prec@5=91.002 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=05:28 IST
=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.063 Prec@1=73.441 Prec@5=91.002 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=05:29 IST
=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.064 Prec@1=73.400 Prec@5=90.979 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=05:29 IST
=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.064 Prec@1=73.400 Prec@5=90.979 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:29 IST
=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.064 Prec@1=73.400 Prec@5=90.979 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:30 IST
=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.066 Prec@1=73.358 Prec@5=90.959 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:30 IST
=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.066 Prec@1=73.358 Prec@5=90.959 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=05:30 IST
=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.066 Prec@1=73.358 Prec@5=90.959 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=05:31 IST
=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.065 Prec@1=73.366 Prec@5=90.941 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=05:31 IST
=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.065 Prec@1=73.366 Prec@5=90.941 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=05:31 IST
=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.065 Prec@1=73.366 Prec@5=90.941 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=05:31 IST
=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.067 Prec@1=73.325 Prec@5=90.935 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=05:31 IST
=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.067 Prec@1=73.325 Prec@5=90.935 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=05:31 IST
=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.067 Prec@1=73.325 Prec@5=90.935 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=05:32 IST
=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.068 Prec@1=73.301 Prec@5=90.922 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=05:32 IST
=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.068 Prec@1=73.301 Prec@5=90.922 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=05:32 IST
=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.290 Loss=1.068 Prec@1=73.301 Prec@5=90.922 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=05:33 IST
=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.069 Prec@1=73.282 Prec@5=90.916 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=05:33 IST
=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.069 Prec@1=73.282 Prec@5=90.916 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=05:33 IST
=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.069 Prec@1=73.282 Prec@5=90.916 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=05:34 IST
=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.070 Prec@1=73.242 Prec@5=90.899 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=05:34 IST
=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.070 Prec@1=73.242 Prec@5=90.899 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=05:34 IST
=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.070 Prec@1=73.242 Prec@5=90.899 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=05:35 IST
=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.071 Prec@1=73.238 Prec@5=90.886 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=05:35 IST
=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.071 Prec@1=73.238 Prec@5=90.886 rate=2.06 Hz, eta=0:06:28, total=0:13:43, wall=05:35 IST
=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.071 Prec@1=73.238 Prec@5=90.886 rate=2.06 Hz, eta=0:06:28, total=0:13:43, wall=05:36 IST
=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.072 Prec@1=73.189 Prec@5=90.871 rate=2.06 Hz, eta=0:06:28, total=0:13:43, wall=05:36 IST
=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.072 Prec@1=73.189 Prec@5=90.871 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=05:36 IST
=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.072 Prec@1=73.189 Prec@5=90.871 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=05:36 IST
=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.073 Prec@1=73.167 Prec@5=90.852 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=05:36 IST
=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.073 Prec@1=73.167 Prec@5=90.852 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=05:36 IST
=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.073 Prec@1=73.167 Prec@5=90.852 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=05:37 IST
=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.153 Prec@5=90.849 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=05:37 IST
=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.153 Prec@5=90.849 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=05:37 IST
=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.153 Prec@5=90.849 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=05:38 IST
=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.149 Prec@5=90.839 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=05:38 IST
=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.149 Prec@5=90.839 rate=2.06 Hz, eta=0:03:14, total=0:16:58, wall=05:38 IST
=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.074 Prec@1=73.149 Prec@5=90.839 rate=2.06 Hz, eta=0:03:14, total=0:16:58, wall=05:39 IST
=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.115 Prec@5=90.825 rate=2.06 Hz, eta=0:03:14, total=0:16:58, wall=05:39 IST
=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.115 Prec@5=90.825 rate=2.06 Hz, eta=0:02:26, total=0:17:46, wall=05:39 IST
=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.115 Prec@5=90.825 rate=2.06 Hz, eta=0:02:26, total=0:17:46, wall=05:40 IST
=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.096 Prec@5=90.815 rate=2.06 Hz, eta=0:02:26, total=0:17:46, wall=05:40 IST
=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.096 Prec@5=90.815 rate=2.06 Hz, eta=0:01:37, total=0:18:35, wall=05:40 IST
=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.487 DataTime=0.291 Loss=1.076 Prec@1=73.096 Prec@5=90.815 rate=2.06 Hz, eta=0:01:37, total=0:18:35, wall=05:40 IST
=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.076 Prec@5=90.809 rate=2.06 Hz, eta=0:01:37, total=0:18:35, wall=05:40 IST
=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.076 Prec@5=90.809 rate=2.06 Hz, eta=0:00:49, total=0:19:22, wall=05:40 IST
=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.076 Prec@5=90.809 rate=2.06 Hz, eta=0:00:49, total=0:19:22, wall=05:41 IST
=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.071 Prec@5=90.805 rate=2.06 Hz, eta=0:00:49, total=0:19:22, wall=05:41 IST
=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.071 Prec@5=90.805 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=05:41 IST
=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.071 Prec@5=90.805 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=05:41 IST
=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.071 Prec@5=90.805 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=05:41 IST
=> training   100.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.486 DataTime=0.291 Loss=1.077 Prec@1=73.071 Prec@5=90.805 rate=2.07 Hz, eta=0:00:00, total=0:20:11, wall=05:41 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:41 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:41 IST
=> validation 0.00% of 1x98...Epoch=111/150 LR=0.01654 Time=6.774 Loss=0.757 Prec@1=80.859 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=05:41 IST
=> validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=6.774 Loss=0.757 Prec@1=80.859 Prec@5=94.922 rate=9537.16 Hz, eta=0:00:00, total=0:00:00, wall=05:41 IST
** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=6.774 Loss=0.757 Prec@1=80.859 Prec@5=94.922 rate=9537.16 Hz, eta=0:00:00, total=0:00:00, wall=05:42 IST
** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=0.563 Loss=1.253 Prec@1=69.370 Prec@5=89.094 rate=9537.16 Hz, eta=0:00:00, total=0:00:00, wall=05:42 IST
** validation 100.00% of 1x98...Epoch=111/150 LR=0.01654 Time=0.563 Loss=1.253 Prec@1=69.370 Prec@5=89.094 rate=2.02 Hz, eta=0:00:00, total=0:00:48, wall=05:42 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> training   0.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=4.991 DataTime=4.661 Loss=1.065 Prec@1=73.633 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=4.991 DataTime=4.661 Loss=1.065 Prec@1=73.633 Prec@5=90.039 rate=12713.44 Hz, eta=0:00:00, total=0:00:00, wall=05:42 IST
=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=4.991 DataTime=4.661 Loss=1.065 Prec@1=73.633 Prec@5=90.039 rate=12713.44 Hz, eta=0:00:00, total=0:00:00, wall=05:43 IST
=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.525 DataTime=0.329 Loss=1.042 Prec@1=73.900 Prec@5=91.313 rate=12713.44 Hz, eta=0:00:00, total=0:00:00, wall=05:43 IST
=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.525 DataTime=0.329 Loss=1.042 Prec@1=73.900 Prec@5=91.313 rate=2.10 Hz, eta=0:19:04, total=0:00:48, wall=05:43 IST
=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.525 DataTime=0.329 Loss=1.042 Prec@1=73.900 Prec@5=91.313 rate=2.10 Hz, eta=0:19:04, total=0:00:48, wall=05:44 IST
=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.495 DataTime=0.298 Loss=1.036 Prec@1=74.059 Prec@5=91.395 rate=2.10 Hz, eta=0:19:04, total=0:00:48, wall=05:44 IST
=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.495 DataTime=0.298 Loss=1.036 Prec@1=74.059 Prec@5=91.395 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=05:44 IST
=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.495 DataTime=0.298 Loss=1.036 Prec@1=74.059 Prec@5=91.395 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=05:45 IST
=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.489 DataTime=0.292 Loss=1.039 Prec@1=73.975 Prec@5=91.306 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=05:45 IST
=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.489 DataTime=0.292 Loss=1.039 Prec@1=73.975 Prec@5=91.306 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=05:45 IST
=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.489 DataTime=0.292 Loss=1.039 Prec@1=73.975 Prec@5=91.306 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=05:45 IST
=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.039 Prec@1=73.975 Prec@5=91.318 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=05:45 IST
=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.039 Prec@1=73.975 Prec@5=91.318 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=05:45 IST
=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.039 Prec@1=73.975 Prec@5=91.318 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=05:46 IST
=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.488 DataTime=0.291 Loss=1.041 Prec@1=73.932 Prec@5=91.287 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=05:46 IST
=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.488 DataTime=0.291 Loss=1.041 Prec@1=73.932 Prec@5=91.287 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=05:46 IST
=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.488 DataTime=0.291 Loss=1.041 Prec@1=73.932 Prec@5=91.287 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=05:47 IST
=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.045 Prec@1=73.863 Prec@5=91.234 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=05:47 IST
=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.045 Prec@1=73.863 Prec@5=91.234 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=05:47 IST
=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.045 Prec@1=73.863 Prec@5=91.234 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=05:48 IST
=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.290 Loss=1.047 Prec@1=73.806 Prec@5=91.181 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=05:48 IST
=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.290 Loss=1.047 Prec@1=73.806 Prec@5=91.181 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=05:48 IST
=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.290 Loss=1.047 Prec@1=73.806 Prec@5=91.181 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=05:49 IST
=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.048 Prec@1=73.768 Prec@5=91.178 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=05:49 IST
=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.048 Prec@1=73.768 Prec@5=91.178 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=05:49 IST
=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.048 Prec@1=73.768 Prec@5=91.178 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=05:49 IST
=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.051 Prec@1=73.710 Prec@5=91.131 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=05:49 IST
=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.051 Prec@1=73.710 Prec@5=91.131 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=05:49 IST
=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.051 Prec@1=73.710 Prec@5=91.131 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=05:50 IST
=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.487 DataTime=0.290 Loss=1.052 Prec@1=73.669 Prec@5=91.117 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=05:50 IST
=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.487 DataTime=0.290 Loss=1.052 Prec@1=73.669 Prec@5=91.117 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:50 IST
=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.487 DataTime=0.290 Loss=1.052 Prec@1=73.669 Prec@5=91.117 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:51 IST
=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.054 Prec@1=73.645 Prec@5=91.115 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=05:51 IST
=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.054 Prec@1=73.645 Prec@5=91.115 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=05:51 IST
=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.486 DataTime=0.289 Loss=1.054 Prec@1=73.645 Prec@5=91.115 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=05:52 IST
=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.055 Prec@1=73.624 Prec@5=91.098 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=05:52 IST
=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.055 Prec@1=73.624 Prec@5=91.098 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=05:52 IST
=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.055 Prec@1=73.624 Prec@5=91.098 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=05:53 IST
=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.056 Prec@1=73.593 Prec@5=91.070 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=05:53 IST
=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.056 Prec@1=73.593 Prec@5=91.070 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=05:53 IST
=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.056 Prec@1=73.593 Prec@5=91.070 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=05:53 IST
=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.056 Prec@1=73.581 Prec@5=91.063 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=05:53 IST
=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.056 Prec@1=73.581 Prec@5=91.063 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=05:53 IST
=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.056 Prec@1=73.581 Prec@5=91.063 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=05:54 IST
=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.057 Prec@1=73.557 Prec@5=91.057 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=05:54 IST
=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.057 Prec@1=73.557 Prec@5=91.057 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=05:54 IST
=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.057 Prec@1=73.557 Prec@5=91.057 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=05:55 IST
=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.058 Prec@1=73.537 Prec@5=91.037 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=05:55 IST
=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.058 Prec@1=73.537 Prec@5=91.037 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=05:55 IST
=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.058 Prec@1=73.537 Prec@5=91.037 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=05:56 IST
=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.060 Prec@1=73.504 Prec@5=91.023 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=05:56 IST
=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.060 Prec@1=73.504 Prec@5=91.023 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=05:56 IST
=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.060 Prec@1=73.504 Prec@5=91.023 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=05:57 IST
=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.061 Prec@1=73.472 Prec@5=91.008 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=05:57 IST
=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.061 Prec@1=73.472 Prec@5=91.008 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=05:57 IST
=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.061 Prec@1=73.472 Prec@5=91.008 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=05:58 IST
=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.062 Prec@1=73.444 Prec@5=90.993 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=05:58 IST
=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.062 Prec@1=73.444 Prec@5=90.993 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=05:58 IST
=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.062 Prec@1=73.444 Prec@5=90.993 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=05:58 IST
=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.063 Prec@1=73.418 Prec@5=90.979 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=05:58 IST
=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.063 Prec@1=73.418 Prec@5=90.979 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=05:58 IST
=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.063 Prec@1=73.418 Prec@5=90.979 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=05:59 IST
=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.064 Prec@1=73.403 Prec@5=90.982 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=05:59 IST
=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.064 Prec@1=73.403 Prec@5=90.982 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=05:59 IST
=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.288 Loss=1.064 Prec@1=73.403 Prec@5=90.982 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=06:00 IST
=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.065 Prec@1=73.377 Prec@5=90.968 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=06:00 IST
=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.065 Prec@1=73.377 Prec@5=90.968 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=06:00 IST
=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.065 Prec@1=73.377 Prec@5=90.968 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=06:01 IST
=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.066 Prec@1=73.356 Prec@5=90.959 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=06:01 IST
=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.066 Prec@1=73.356 Prec@5=90.959 rate=2.07 Hz, eta=0:01:37, total=0:18:29, wall=06:01 IST
=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.066 Prec@1=73.356 Prec@5=90.959 rate=2.07 Hz, eta=0:01:37, total=0:18:29, wall=06:02 IST
=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.067 Prec@1=73.328 Prec@5=90.942 rate=2.07 Hz, eta=0:01:37, total=0:18:29, wall=06:02 IST
=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.067 Prec@1=73.328 Prec@5=90.942 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=06:02 IST
=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.485 DataTime=0.289 Loss=1.067 Prec@1=73.328 Prec@5=90.942 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=06:02 IST
=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.068 Prec@1=73.310 Prec@5=90.928 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=06:02 IST
=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.068 Prec@1=73.310 Prec@5=90.928 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=06:02 IST
=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.068 Prec@1=73.310 Prec@5=90.928 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=06:02 IST
=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.068 Prec@1=73.310 Prec@5=90.929 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=06:02 IST
=> training   100.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.484 DataTime=0.288 Loss=1.068 Prec@1=73.310 Prec@5=90.929 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=06:02 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST
=> validation 0.00% of 1x98...Epoch=112/150 LR=0.01577 Time=6.901 Loss=0.774 Prec@1=80.469 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST
=> validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=6.901 Loss=0.774 Prec@1=80.469 Prec@5=94.531 rate=7489.18 Hz, eta=0:00:00, total=0:00:00, wall=06:02 IST
** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=6.901 Loss=0.774 Prec@1=80.469 Prec@5=94.531 rate=7489.18 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST
** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=0.556 Loss=1.243 Prec@1=69.546 Prec@5=89.188 rate=7489.18 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST
** validation 100.00% of 1x98...Epoch=112/150 LR=0.01577 Time=0.556 Loss=1.243 Prec@1=69.546 Prec@5=89.188 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=06:03 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST
=> training   0.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=4.868 DataTime=4.635 Loss=1.077 Prec@1=74.609 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST
=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=4.868 DataTime=4.635 Loss=1.077 Prec@1=74.609 Prec@5=90.039 rate=7578.00 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST
=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=4.868 DataTime=4.635 Loss=1.077 Prec@1=74.609 Prec@5=90.039 rate=7578.00 Hz, eta=0:00:00, total=0:00:00, wall=06:04 IST
=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.518 DataTime=0.326 Loss=1.050 Prec@1=73.876 Prec@5=91.165 rate=7578.00 Hz, eta=0:00:00, total=0:00:00, wall=06:04 IST
=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.518 DataTime=0.326 Loss=1.050 Prec@1=73.876 Prec@5=91.165 rate=2.13 Hz, eta=0:18:49, total=0:00:47, wall=06:04 IST
=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.518 DataTime=0.326 Loss=1.050 Prec@1=73.876 Prec@5=91.165 rate=2.13 Hz, eta=0:18:49, total=0:00:47, wall=06:05 IST
=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.498 DataTime=0.302 Loss=1.050 Prec@1=73.863 Prec@5=91.135 rate=2.13 Hz, eta=0:18:49, total=0:00:47, wall=06:05 IST
=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.498 DataTime=0.302 Loss=1.050 Prec@1=73.863 Prec@5=91.135 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=06:05 IST
=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.498 DataTime=0.302 Loss=1.050 Prec@1=73.863 Prec@5=91.135 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=06:06 IST
=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.494 DataTime=0.299 Loss=1.052 Prec@1=73.837 Prec@5=91.155 rate=2.11 Hz, eta=0:18:10, total=0:01:35, wall=06:06 IST
=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.494 DataTime=0.299 Loss=1.052 Prec@1=73.837 Prec@5=91.155 rate=2.09 Hz, eta=0:17:31, total=0:02:23, wall=06:06 IST
=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.494 DataTime=0.299 Loss=1.052 Prec@1=73.837 Prec@5=91.155 rate=2.09 Hz, eta=0:17:31, total=0:02:23, wall=06:07 IST
=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.491 DataTime=0.296 Loss=1.047 Prec@1=73.916 Prec@5=91.211 rate=2.09 Hz, eta=0:17:31, total=0:02:23, wall=06:07 IST
=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.491 DataTime=0.296 Loss=1.047 Prec@1=73.916 Prec@5=91.211 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=06:07 IST
=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.491 DataTime=0.296 Loss=1.047 Prec@1=73.916 Prec@5=91.211 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=06:07 IST
=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.049 Prec@1=73.881 Prec@5=91.189 rate=2.09 Hz, eta=0:16:46, total=0:03:12, wall=06:07 IST
=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.049 Prec@1=73.881 Prec@5=91.189 rate=2.08 Hz, eta=0:16:02, total=0:04:00, wall=06:07 IST
=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.049 Prec@1=73.881 Prec@5=91.189 rate=2.08 Hz, eta=0:16:02, total=0:04:00, wall=06:08 IST
=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.891 Prec@5=91.189 rate=2.08 Hz, eta=0:16:02, total=0:04:00, wall=06:08 IST
=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.891 Prec@5=91.189 rate=2.07 Hz, eta=0:15:16, total=0:04:49, wall=06:08 IST
=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.891 Prec@5=91.189 rate=2.07 Hz, eta=0:15:16, total=0:04:49, wall=06:09 IST
=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.871 Prec@5=91.164 rate=2.07 Hz, eta=0:15:16, total=0:04:49, wall=06:09 IST
=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.871 Prec@5=91.164 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=06:09 IST
=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.490 DataTime=0.295 Loss=1.048 Prec@1=73.871 Prec@5=91.164 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=06:10 IST
=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.489 DataTime=0.293 Loss=1.050 Prec@1=73.820 Prec@5=91.144 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=06:10 IST
=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.489 DataTime=0.293 Loss=1.050 Prec@1=73.820 Prec@5=91.144 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=06:10 IST
=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.489 DataTime=0.293 Loss=1.050 Prec@1=73.820 Prec@5=91.144 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=06:11 IST
=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.293 Loss=1.051 Prec@1=73.778 Prec@5=91.138 rate=2.07 Hz, eta=0:13:41, total=0:06:26, wall=06:11 IST
=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.293 Loss=1.051 Prec@1=73.778 Prec@5=91.138 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=06:11 IST
=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.293 Loss=1.051 Prec@1=73.778 Prec@5=91.138 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=06:11 IST
=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.761 Prec@5=91.133 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=06:11 IST
=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.761 Prec@5=91.133 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=06:11 IST
=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.761 Prec@5=91.133 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=06:12 IST
=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.741 Prec@5=91.132 rate=2.07 Hz, eta=0:12:04, total=0:08:03, wall=06:12 IST
=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.741 Prec@5=91.132 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=06:12 IST
=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.051 Prec@1=73.741 Prec@5=91.132 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=06:13 IST
=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.717 Prec@5=91.137 rate=2.07 Hz, eta=0:11:17, total=0:08:52, wall=06:13 IST
=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.717 Prec@5=91.137 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=06:13 IST
=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.717 Prec@5=91.137 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=06:14 IST
=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.714 Prec@5=91.129 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=06:14 IST
=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.714 Prec@5=91.129 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:14 IST
=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.052 Prec@1=73.714 Prec@5=91.129 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:15 IST
=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.053 Prec@1=73.681 Prec@5=91.132 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:15 IST
=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.053 Prec@1=73.681 Prec@5=91.132 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=06:15 IST
=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.053 Prec@1=73.681 Prec@5=91.132 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=06:15 IST
=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.054 Prec@1=73.667 Prec@5=91.109 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=06:15 IST
=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.054 Prec@1=73.667 Prec@5=91.109 rate=2.07 Hz, eta=0:08:05, total=0:12:06, wall=06:15 IST
=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.054 Prec@1=73.667 Prec@5=91.109 rate=2.07 Hz, eta=0:08:05, total=0:12:06, wall=06:16 IST
=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.055 Prec@1=73.653 Prec@5=91.103 rate=2.07 Hz, eta=0:08:05, total=0:12:06, wall=06:16 IST
=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.055 Prec@1=73.653 Prec@5=91.103 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=06:16 IST
=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.055 Prec@1=73.653 Prec@5=91.103 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=06:17 IST
=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.055 Prec@1=73.638 Prec@5=91.095 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=06:17 IST
=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.055 Prec@1=73.638 Prec@5=91.095 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=06:17 IST
=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.488 DataTime=0.292 Loss=1.055 Prec@1=73.638 Prec@5=91.095 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=06:18 IST
=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.055 Prec@1=73.628 Prec@5=91.087 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=06:18 IST
=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.055 Prec@1=73.628 Prec@5=91.087 rate=2.07 Hz, eta=0:05:39, total=0:14:32, wall=06:18 IST
=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.055 Prec@1=73.628 Prec@5=91.087 rate=2.07 Hz, eta=0:05:39, total=0:14:32, wall=06:19 IST
=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.056 Prec@1=73.606 Prec@5=91.067 rate=2.07 Hz, eta=0:05:39, total=0:14:32, wall=06:19 IST
=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.056 Prec@1=73.606 Prec@5=91.067 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=06:19 IST
=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.292 Loss=1.056 Prec@1=73.606 Prec@5=91.067 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=06:20 IST
=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.057 Prec@1=73.576 Prec@5=91.056 rate=2.06 Hz, eta=0:04:51, total=0:15:21, wall=06:20 IST
=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.057 Prec@1=73.576 Prec@5=91.056 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=06:20 IST
=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.057 Prec@1=73.576 Prec@5=91.056 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=06:20 IST
=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.058 Prec@1=73.561 Prec@5=91.044 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=06:20 IST
=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.058 Prec@1=73.561 Prec@5=91.044 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=06:20 IST
=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.058 Prec@1=73.561 Prec@5=91.044 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=06:21 IST
=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.059 Prec@1=73.533 Prec@5=91.032 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=06:21 IST
=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.059 Prec@1=73.533 Prec@5=91.032 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=06:21 IST
=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.487 DataTime=0.291 Loss=1.059 Prec@1=73.533 Prec@5=91.032 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=06:22 IST
=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.060 Prec@1=73.510 Prec@5=91.025 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=06:22 IST
=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.060 Prec@1=73.510 Prec@5=91.025 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=06:22 IST
=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.060 Prec@1=73.510 Prec@5=91.025 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=06:23 IST
=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.290 Loss=1.060 Prec@1=73.488 Prec@5=91.016 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=06:23 IST
=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.290 Loss=1.060 Prec@1=73.488 Prec@5=91.016 rate=2.07 Hz, eta=0:00:49, total=0:19:22, wall=06:23 IST
=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.290 Loss=1.060 Prec@1=73.488 Prec@5=91.016 rate=2.07 Hz, eta=0:00:49, total=0:19:22, wall=06:24 IST
=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.061 Prec@1=73.487 Prec@5=91.004 rate=2.07 Hz, eta=0:00:49, total=0:19:22, wall=06:24 IST
=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.061 Prec@1=73.487 Prec@5=91.004 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=06:24 IST
=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.061 Prec@1=73.487 Prec@5=91.004 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=06:24 IST
=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.061 Prec@1=73.484 Prec@5=91.004 rate=2.06 Hz, eta=0:00:00, total=0:20:11, wall=06:24 IST
=> training   100.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.486 DataTime=0.291 Loss=1.061 Prec@1=73.484 Prec@5=91.004 rate=2.06 Hz, eta=0:00:00, total=0:20:12, wall=06:24 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST
=> validation 0.00% of 1x98...Epoch=113/150 LR=0.01502 Time=7.003 Loss=0.719 Prec@1=82.031 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST
=> validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=7.003 Loss=0.719 Prec@1=82.031 Prec@5=94.727 rate=8697.92 Hz, eta=0:00:00, total=0:00:00, wall=06:24 IST
** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=7.003 Loss=0.719 Prec@1=82.031 Prec@5=94.727 rate=8697.92 Hz, eta=0:00:00, total=0:00:00, wall=06:25 IST
** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=0.559 Loss=1.238 Prec@1=69.870 Prec@5=89.288 rate=8697.92 Hz, eta=0:00:00, total=0:00:00, wall=06:25 IST
** validation 100.00% of 1x98...Epoch=113/150 LR=0.01502 Time=0.559 Loss=1.238 Prec@1=69.870 Prec@5=89.288 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=06:25 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:25 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:25 IST
=> training   0.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.614 DataTime=4.362 Loss=1.062 Prec@1=73.047 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=06:25 IST
=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.614 DataTime=4.362 Loss=1.062 Prec@1=73.047 Prec@5=91.406 rate=5912.12 Hz, eta=0:00:00, total=0:00:00, wall=06:25 IST
=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.614 DataTime=4.362 Loss=1.062 Prec@1=73.047 Prec@5=91.406 rate=5912.12 Hz, eta=0:00:00, total=0:00:00, wall=06:25 IST
=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.511 DataTime=0.313 Loss=1.027 Prec@1=74.219 Prec@5=91.426 rate=5912.12 Hz, eta=0:00:00, total=0:00:00, wall=06:25 IST
=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.511 DataTime=0.313 Loss=1.027 Prec@1=74.219 Prec@5=91.426 rate=2.15 Hz, eta=0:18:36, total=0:00:46, wall=06:25 IST
=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.511 DataTime=0.313 Loss=1.027 Prec@1=74.219 Prec@5=91.426 rate=2.15 Hz, eta=0:18:36, total=0:00:46, wall=06:26 IST
=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.498 DataTime=0.300 Loss=1.023 Prec@1=74.334 Prec@5=91.461 rate=2.15 Hz, eta=0:18:36, total=0:00:46, wall=06:26 IST
=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.498 DataTime=0.300 Loss=1.023 Prec@1=74.334 Prec@5=91.461 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=06:26 IST
=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.498 DataTime=0.300 Loss=1.023 Prec@1=74.334 Prec@5=91.461 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=06:27 IST
=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.494 DataTime=0.296 Loss=1.025 Prec@1=74.289 Prec@5=91.474 rate=2.11 Hz, eta=0:18:13, total=0:01:35, wall=06:27 IST
=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.494 DataTime=0.296 Loss=1.025 Prec@1=74.289 Prec@5=91.474 rate=2.09 Hz, eta=0:17:33, total=0:02:23, wall=06:27 IST
=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.494 DataTime=0.296 Loss=1.025 Prec@1=74.289 Prec@5=91.474 rate=2.09 Hz, eta=0:17:33, total=0:02:23, wall=06:28 IST
=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.489 DataTime=0.291 Loss=1.028 Prec@1=74.225 Prec@5=91.429 rate=2.09 Hz, eta=0:17:33, total=0:02:23, wall=06:28 IST
=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.489 DataTime=0.291 Loss=1.028 Prec@1=74.225 Prec@5=91.429 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:28 IST
=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.489 DataTime=0.291 Loss=1.028 Prec@1=74.225 Prec@5=91.429 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:29 IST
=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.490 DataTime=0.293 Loss=1.031 Prec@1=74.156 Prec@5=91.389 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:29 IST
=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.490 DataTime=0.293 Loss=1.031 Prec@1=74.156 Prec@5=91.389 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=06:29 IST
=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.490 DataTime=0.293 Loss=1.031 Prec@1=74.156 Prec@5=91.389 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=06:29 IST
=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.032 Prec@1=74.117 Prec@5=91.379 rate=2.08 Hz, eta=0:16:03, total=0:04:01, wall=06:29 IST
=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.032 Prec@1=74.117 Prec@5=91.379 rate=2.09 Hz, eta=0:15:11, total=0:04:47, wall=06:29 IST
=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.032 Prec@1=74.117 Prec@5=91.379 rate=2.09 Hz, eta=0:15:11, total=0:04:47, wall=06:30 IST
=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.488 DataTime=0.291 Loss=1.034 Prec@1=74.117 Prec@5=91.364 rate=2.09 Hz, eta=0:15:11, total=0:04:47, wall=06:30 IST
=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.488 DataTime=0.291 Loss=1.034 Prec@1=74.117 Prec@5=91.364 rate=2.08 Hz, eta=0:14:26, total=0:05:37, wall=06:30 IST
=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.488 DataTime=0.291 Loss=1.034 Prec@1=74.117 Prec@5=91.364 rate=2.08 Hz, eta=0:14:26, total=0:05:37, wall=06:31 IST
=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.034 Prec@1=74.102 Prec@5=91.367 rate=2.08 Hz, eta=0:14:26, total=0:05:37, wall=06:31 IST
=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.034 Prec@1=74.102 Prec@5=91.367 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=06:31 IST
=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.487 DataTime=0.290 Loss=1.034 Prec@1=74.102 Prec@5=91.367 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=06:32 IST
=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.486 DataTime=0.289 Loss=1.035 Prec@1=74.082 Prec@5=91.351 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=06:32 IST
=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.486 DataTime=0.289 Loss=1.035 Prec@1=74.082 Prec@5=91.351 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=06:32 IST
=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.486 DataTime=0.289 Loss=1.035 Prec@1=74.082 Prec@5=91.351 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=06:33 IST
=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.036 Prec@1=74.043 Prec@5=91.326 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=06:33 IST
=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.036 Prec@1=74.043 Prec@5=91.326 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=06:33 IST
=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.036 Prec@1=74.043 Prec@5=91.326 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=06:33 IST
=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.038 Prec@1=73.983 Prec@5=91.296 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=06:33 IST
=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.038 Prec@1=73.983 Prec@5=91.296 rate=2.08 Hz, eta=0:11:13, total=0:08:49, wall=06:33 IST
=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.038 Prec@1=73.983 Prec@5=91.296 rate=2.08 Hz, eta=0:11:13, total=0:08:49, wall=06:34 IST
=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.040 Prec@1=73.931 Prec@5=91.273 rate=2.08 Hz, eta=0:11:13, total=0:08:49, wall=06:34 IST
=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.040 Prec@1=73.931 Prec@5=91.273 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=06:34 IST
=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.289 Loss=1.040 Prec@1=73.931 Prec@5=91.273 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=06:35 IST
=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.041 Prec@1=73.908 Prec@5=91.256 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=06:35 IST
=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.041 Prec@1=73.908 Prec@5=91.256 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=06:35 IST
=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.485 DataTime=0.288 Loss=1.041 Prec@1=73.908 Prec@5=91.256 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=06:36 IST
=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.287 Loss=1.043 Prec@1=73.867 Prec@5=91.250 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=06:36 IST
=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.287 Loss=1.043 Prec@1=73.867 Prec@5=91.250 rate=2.08 Hz, eta=0:08:48, total=0:11:12, wall=06:36 IST
=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.287 Loss=1.043 Prec@1=73.867 Prec@5=91.250 rate=2.08 Hz, eta=0:08:48, total=0:11:12, wall=06:37 IST
=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.286 Loss=1.043 Prec@1=73.843 Prec@5=91.234 rate=2.08 Hz, eta=0:08:48, total=0:11:12, wall=06:37 IST
=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.286 Loss=1.043 Prec@1=73.843 Prec@5=91.234 rate=2.08 Hz, eta=0:08:00, total=0:12:00, wall=06:37 IST
=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.483 DataTime=0.286 Loss=1.043 Prec@1=73.843 Prec@5=91.234 rate=2.08 Hz, eta=0:08:00, total=0:12:00, wall=06:37 IST
=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.287 Loss=1.045 Prec@1=73.818 Prec@5=91.208 rate=2.08 Hz, eta=0:08:00, total=0:12:00, wall=06:37 IST
=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.287 Loss=1.045 Prec@1=73.818 Prec@5=91.208 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=06:37 IST
=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.287 Loss=1.045 Prec@1=73.818 Prec@5=91.208 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=06:38 IST
=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.046 Prec@1=73.811 Prec@5=91.201 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=06:38 IST
=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.046 Prec@1=73.811 Prec@5=91.201 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=06:38 IST
=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.046 Prec@1=73.811 Prec@5=91.201 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=06:39 IST
=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.045 Prec@1=73.811 Prec@5=91.200 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=06:39 IST
=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.045 Prec@1=73.811 Prec@5=91.200 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=06:39 IST
=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.045 Prec@1=73.811 Prec@5=91.200 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=06:40 IST
=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.775 Prec@5=91.189 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=06:40 IST
=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.775 Prec@5=91.189 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=06:40 IST
=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.775 Prec@5=91.189 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=06:41 IST
=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.779 Prec@5=91.178 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=06:41 IST
=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.779 Prec@5=91.178 rate=2.08 Hz, eta=0:04:01, total=0:16:04, wall=06:41 IST
=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.047 Prec@1=73.779 Prec@5=91.178 rate=2.08 Hz, eta=0:04:01, total=0:16:04, wall=06:42 IST
=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.762 Prec@5=91.174 rate=2.08 Hz, eta=0:04:01, total=0:16:04, wall=06:42 IST
=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.762 Prec@5=91.174 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=06:42 IST
=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.762 Prec@5=91.174 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=06:42 IST
=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.749 Prec@5=91.174 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=06:42 IST
=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.749 Prec@5=91.174 rate=2.07 Hz, eta=0:02:25, total=0:17:41, wall=06:42 IST
=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.048 Prec@1=73.749 Prec@5=91.174 rate=2.07 Hz, eta=0:02:25, total=0:17:41, wall=06:43 IST
=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.049 Prec@1=73.723 Prec@5=91.160 rate=2.07 Hz, eta=0:02:25, total=0:17:41, wall=06:43 IST
=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.049 Prec@1=73.723 Prec@5=91.160 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=06:43 IST
=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.049 Prec@1=73.723 Prec@5=91.160 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=06:44 IST
=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.050 Prec@1=73.705 Prec@5=91.149 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=06:44 IST
=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.050 Prec@1=73.705 Prec@5=91.149 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=06:44 IST
=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.050 Prec@1=73.705 Prec@5=91.149 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=06:45 IST
=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.051 Prec@1=73.683 Prec@5=91.131 rate=2.07 Hz, eta=0:00:49, total=0:19:17, wall=06:45 IST
=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.051 Prec@1=73.683 Prec@5=91.131 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=06:45 IST
=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.051 Prec@1=73.683 Prec@5=91.131 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=06:45 IST
=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.051 Prec@1=73.682 Prec@5=91.132 rate=2.07 Hz, eta=0:00:00, total=0:20:05, wall=06:45 IST
=> training   100.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.484 DataTime=0.288 Loss=1.051 Prec@1=73.682 Prec@5=91.132 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=06:45 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:45 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:45 IST
=> validation 0.00% of 1x98...Epoch=114/150 LR=0.01428 Time=7.041 Loss=0.737 Prec@1=82.422 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=06:45 IST
=> validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=7.041 Loss=0.737 Prec@1=82.422 Prec@5=94.531 rate=7986.20 Hz, eta=0:00:00, total=0:00:00, wall=06:45 IST
** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=7.041 Loss=0.737 Prec@1=82.422 Prec@5=94.531 rate=7986.20 Hz, eta=0:00:00, total=0:00:00, wall=06:46 IST
** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=0.559 Loss=1.227 Prec@1=69.786 Prec@5=89.294 rate=7986.20 Hz, eta=0:00:00, total=0:00:00, wall=06:46 IST
** validation 100.00% of 1x98...Epoch=114/150 LR=0.01428 Time=0.559 Loss=1.227 Prec@1=69.786 Prec@5=89.294 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=06:46 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> training   0.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=4.881 DataTime=4.612 Loss=1.069 Prec@1=72.656 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=4.881 DataTime=4.612 Loss=1.069 Prec@1=72.656 Prec@5=90.625 rate=7707.25 Hz, eta=0:00:00, total=0:00:00, wall=06:46 IST
=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=4.881 DataTime=4.612 Loss=1.069 Prec@1=72.656 Prec@5=90.625 rate=7707.25 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST
=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.512 DataTime=0.315 Loss=1.006 Prec@1=74.698 Prec@5=91.718 rate=7707.25 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST
=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.512 DataTime=0.315 Loss=1.006 Prec@1=74.698 Prec@5=91.718 rate=2.15 Hz, eta=0:18:34, total=0:00:46, wall=06:47 IST
=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.512 DataTime=0.315 Loss=1.006 Prec@1=74.698 Prec@5=91.718 rate=2.15 Hz, eta=0:18:34, total=0:00:46, wall=06:47 IST
=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.497 DataTime=0.299 Loss=1.010 Prec@1=74.652 Prec@5=91.685 rate=2.15 Hz, eta=0:18:34, total=0:00:46, wall=06:47 IST
=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.497 DataTime=0.299 Loss=1.010 Prec@1=74.652 Prec@5=91.685 rate=2.11 Hz, eta=0:18:08, total=0:01:35, wall=06:47 IST
=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.497 DataTime=0.299 Loss=1.010 Prec@1=74.652 Prec@5=91.685 rate=2.11 Hz, eta=0:18:08, total=0:01:35, wall=06:48 IST
=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.493 DataTime=0.295 Loss=1.013 Prec@1=74.542 Prec@5=91.618 rate=2.11 Hz, eta=0:18:08, total=0:01:35, wall=06:48 IST
=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.493 DataTime=0.295 Loss=1.013 Prec@1=74.542 Prec@5=91.618 rate=2.10 Hz, eta=0:17:29, total=0:02:23, wall=06:48 IST
=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.493 DataTime=0.295 Loss=1.013 Prec@1=74.542 Prec@5=91.618 rate=2.10 Hz, eta=0:17:29, total=0:02:23, wall=06:49 IST
=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.489 DataTime=0.291 Loss=1.017 Prec@1=74.484 Prec@5=91.554 rate=2.10 Hz, eta=0:17:29, total=0:02:23, wall=06:49 IST
=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.489 DataTime=0.291 Loss=1.017 Prec@1=74.484 Prec@5=91.554 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:49 IST
=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.489 DataTime=0.291 Loss=1.017 Prec@1=74.484 Prec@5=91.554 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:50 IST
=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.019 Prec@1=74.427 Prec@5=91.538 rate=2.10 Hz, eta=0:16:42, total=0:03:11, wall=06:50 IST
=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.019 Prec@1=74.427 Prec@5=91.538 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=06:50 IST
=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.019 Prec@1=74.427 Prec@5=91.538 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=06:51 IST
=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.021 Prec@1=74.383 Prec@5=91.538 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=06:51 IST
=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.021 Prec@1=74.383 Prec@5=91.538 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=06:51 IST
=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.021 Prec@1=74.383 Prec@5=91.538 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=06:51 IST
=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.289 Loss=1.026 Prec@1=74.275 Prec@5=91.452 rate=2.09 Hz, eta=0:15:10, total=0:04:47, wall=06:51 IST
=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.289 Loss=1.026 Prec@1=74.275 Prec@5=91.452 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=06:51 IST
=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.289 Loss=1.026 Prec@1=74.275 Prec@5=91.452 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=06:52 IST
=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.026 Prec@1=74.271 Prec@5=91.457 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=06:52 IST
=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.026 Prec@1=74.271 Prec@5=91.457 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=06:52 IST
=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.026 Prec@1=74.271 Prec@5=91.457 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=06:53 IST
=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.028 Prec@1=74.256 Prec@5=91.434 rate=2.08 Hz, eta=0:13:37, total=0:06:24, wall=06:53 IST
=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.028 Prec@1=74.256 Prec@5=91.434 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=06:53 IST
=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.028 Prec@1=74.256 Prec@5=91.434 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=06:54 IST
=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.029 Prec@1=74.247 Prec@5=91.410 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=06:54 IST
=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.029 Prec@1=74.247 Prec@5=91.410 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=06:54 IST
=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.290 Loss=1.029 Prec@1=74.247 Prec@5=91.410 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=06:55 IST
=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.207 Prec@5=91.402 rate=2.08 Hz, eta=0:12:03, total=0:08:02, wall=06:55 IST
=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.207 Prec@5=91.402 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=06:55 IST
=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.207 Prec@5=91.402 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=06:55 IST
=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.197 Prec@5=91.400 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=06:55 IST
=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.197 Prec@5=91.400 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=06:55 IST
=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.030 Prec@1=74.197 Prec@5=91.400 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=06:56 IST
=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.291 Loss=1.031 Prec@1=74.166 Prec@5=91.380 rate=2.08 Hz, eta=0:10:27, total=0:09:38, wall=06:56 IST
=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.291 Loss=1.031 Prec@1=74.166 Prec@5=91.380 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:56 IST
=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.487 DataTime=0.291 Loss=1.031 Prec@1=74.166 Prec@5=91.380 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:57 IST
=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.134 Prec@5=91.369 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=06:57 IST
=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.134 Prec@5=91.369 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=06:57 IST
=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.134 Prec@5=91.369 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=06:58 IST
=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.128 Prec@5=91.370 rate=2.07 Hz, eta=0:08:51, total=0:11:16, wall=06:58 IST
=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.128 Prec@5=91.370 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=06:58 IST
=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.128 Prec@5=91.370 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=06:59 IST
=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.093 Prec@5=91.366 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=06:59 IST
=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.093 Prec@5=91.366 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=06:59 IST
=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.033 Prec@1=74.093 Prec@5=91.366 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=06:59 IST
=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.291 Loss=1.034 Prec@1=74.073 Prec@5=91.357 rate=2.07 Hz, eta=0:07:15, total=0:12:53, wall=06:59 IST
=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.291 Loss=1.034 Prec@1=74.073 Prec@5=91.357 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=06:59 IST
=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.291 Loss=1.034 Prec@1=74.073 Prec@5=91.357 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=07:00 IST
=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.035 Prec@1=74.047 Prec@5=91.348 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=07:00 IST
=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.035 Prec@1=74.047 Prec@5=91.348 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=07:00 IST
=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.035 Prec@1=74.047 Prec@5=91.348 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=07:01 IST
=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.035 Prec@5=91.338 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=07:01 IST
=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.035 Prec@5=91.338 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:01 IST
=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.035 Prec@5=91.338 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:02 IST
=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.031 Prec@5=91.337 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:02 IST
=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.031 Prec@5=91.337 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=07:02 IST
=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.036 Prec@1=74.031 Prec@5=91.337 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=07:03 IST
=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.037 Prec@1=74.003 Prec@5=91.319 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=07:03 IST
=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.037 Prec@1=74.003 Prec@5=91.319 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=07:03 IST
=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.037 Prec@1=74.003 Prec@5=91.319 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=07:04 IST
=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.038 Prec@1=73.970 Prec@5=91.306 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=07:04 IST
=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.038 Prec@1=73.970 Prec@5=91.306 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=07:04 IST
=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.038 Prec@1=73.970 Prec@5=91.306 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=07:04 IST
=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.039 Prec@1=73.956 Prec@5=91.290 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=07:04 IST
=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.039 Prec@1=73.956 Prec@5=91.290 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:04 IST
=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.039 Prec@1=73.956 Prec@5=91.290 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:05 IST
=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.040 Prec@1=73.924 Prec@5=91.275 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:05 IST
=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.040 Prec@1=73.924 Prec@5=91.275 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=07:05 IST
=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.486 DataTime=0.290 Loss=1.040 Prec@1=73.924 Prec@5=91.275 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=07:06 IST
=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.485 DataTime=0.290 Loss=1.041 Prec@1=73.899 Prec@5=91.254 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=07:06 IST
=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.485 DataTime=0.290 Loss=1.041 Prec@1=73.899 Prec@5=91.254 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=07:06 IST
=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.485 DataTime=0.290 Loss=1.041 Prec@1=73.899 Prec@5=91.254 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=07:06 IST
=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.485 DataTime=0.290 Loss=1.041 Prec@1=73.898 Prec@5=91.253 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=07:06 IST
=> training   100.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.485 DataTime=0.290 Loss=1.041 Prec@1=73.898 Prec@5=91.253 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=07:06 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> validation 0.00% of 1x98...Epoch=115/150 LR=0.01355 Time=6.041 Loss=0.719 Prec@1=81.641 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.041 Loss=0.719 Prec@1=81.641 Prec@5=94.922 rate=7835.27 Hz, eta=0:00:00, total=0:00:00, wall=07:06 IST
** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.041 Loss=0.719 Prec@1=81.641 Prec@5=94.922 rate=7835.27 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST
** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=0.546 Loss=1.222 Prec@1=70.074 Prec@5=89.298 rate=7835.27 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST
** validation 100.00% of 1x98...Epoch=115/150 LR=0.01355 Time=0.546 Loss=1.222 Prec@1=70.074 Prec@5=89.298 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=07:07 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> training   0.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.025 DataTime=4.784 Loss=1.066 Prec@1=73.633 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.025 DataTime=4.784 Loss=1.066 Prec@1=73.633 Prec@5=91.406 rate=8953.51 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST
=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.025 DataTime=4.784 Loss=1.066 Prec@1=73.633 Prec@5=91.406 rate=8953.51 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST
=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.515 DataTime=0.320 Loss=1.010 Prec@1=74.629 Prec@5=91.563 rate=8953.51 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST
=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.515 DataTime=0.320 Loss=1.010 Prec@1=74.629 Prec@5=91.563 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:08 IST
=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.515 DataTime=0.320 Loss=1.010 Prec@1=74.629 Prec@5=91.563 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:09 IST
=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.501 DataTime=0.305 Loss=1.012 Prec@1=74.688 Prec@5=91.524 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:09 IST
=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.501 DataTime=0.305 Loss=1.012 Prec@1=74.688 Prec@5=91.524 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=07:09 IST
=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.501 DataTime=0.305 Loss=1.012 Prec@1=74.688 Prec@5=91.524 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=07:09 IST
=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.489 DataTime=0.293 Loss=1.013 Prec@1=74.639 Prec@5=91.531 rate=2.10 Hz, eta=0:18:15, total=0:01:35, wall=07:09 IST
=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.489 DataTime=0.293 Loss=1.013 Prec@1=74.639 Prec@5=91.531 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=07:09 IST
=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.489 DataTime=0.293 Loss=1.013 Prec@1=74.639 Prec@5=91.531 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=07:10 IST
=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.491 DataTime=0.295 Loss=1.019 Prec@1=74.525 Prec@5=91.473 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=07:10 IST
=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.491 DataTime=0.295 Loss=1.019 Prec@1=74.525 Prec@5=91.473 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:10 IST
=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.491 DataTime=0.295 Loss=1.019 Prec@1=74.525 Prec@5=91.473 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:11 IST
=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.290 Loss=1.018 Prec@1=74.485 Prec@5=91.514 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:11 IST
=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.290 Loss=1.018 Prec@1=74.485 Prec@5=91.514 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=07:11 IST
=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.290 Loss=1.018 Prec@1=74.485 Prec@5=91.514 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=07:12 IST
=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.017 Prec@1=74.512 Prec@5=91.531 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=07:12 IST
=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.017 Prec@1=74.512 Prec@5=91.531 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=07:12 IST
=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.017 Prec@1=74.512 Prec@5=91.531 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=07:13 IST
=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.018 Prec@1=74.459 Prec@5=91.529 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=07:13 IST
=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.018 Prec@1=74.459 Prec@5=91.529 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=07:13 IST
=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.018 Prec@1=74.459 Prec@5=91.529 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=07:13 IST
=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.018 Prec@1=74.471 Prec@5=91.525 rate=2.09 Hz, eta=0:14:22, total=0:05:35, wall=07:13 IST
=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.018 Prec@1=74.471 Prec@5=91.525 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=07:13 IST
=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.018 Prec@1=74.471 Prec@5=91.525 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=07:14 IST
=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.020 Prec@1=74.399 Prec@5=91.492 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=07:14 IST
=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.020 Prec@1=74.399 Prec@5=91.492 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=07:14 IST
=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.020 Prec@1=74.399 Prec@5=91.492 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=07:15 IST
=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.292 Loss=1.023 Prec@1=74.331 Prec@5=91.467 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=07:15 IST
=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.292 Loss=1.023 Prec@1=74.331 Prec@5=91.467 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:15 IST
=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.292 Loss=1.023 Prec@1=74.331 Prec@5=91.467 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:16 IST
=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.024 Prec@1=74.310 Prec@5=91.446 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:16 IST
=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.024 Prec@1=74.310 Prec@5=91.446 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=07:16 IST
=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.024 Prec@1=74.310 Prec@5=91.446 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=07:17 IST
=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.024 Prec@1=74.316 Prec@5=91.437 rate=2.07 Hz, eta=0:11:15, total=0:08:50, wall=07:17 IST
=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.024 Prec@1=74.316 Prec@5=91.437 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=07:17 IST
=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.024 Prec@1=74.316 Prec@5=91.437 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=07:17 IST
=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.293 Loss=1.025 Prec@1=74.312 Prec@5=91.422 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=07:17 IST
=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.293 Loss=1.025 Prec@1=74.312 Prec@5=91.422 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=07:17 IST
=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.488 DataTime=0.293 Loss=1.025 Prec@1=74.312 Prec@5=91.422 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=07:18 IST
=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.026 Prec@1=74.278 Prec@5=91.408 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=07:18 IST
=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.026 Prec@1=74.278 Prec@5=91.408 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=07:18 IST
=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.292 Loss=1.026 Prec@1=74.278 Prec@5=91.408 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=07:19 IST
=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.026 Prec@1=74.278 Prec@5=91.400 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=07:19 IST
=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.026 Prec@1=74.278 Prec@5=91.400 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=07:19 IST
=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.026 Prec@1=74.278 Prec@5=91.400 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=07:20 IST
=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.027 Prec@1=74.258 Prec@5=91.386 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=07:20 IST
=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.027 Prec@1=74.258 Prec@5=91.386 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=07:20 IST
=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.487 DataTime=0.291 Loss=1.027 Prec@1=74.258 Prec@5=91.386 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=07:21 IST
=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.028 Prec@1=74.246 Prec@5=91.383 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=07:21 IST
=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.028 Prec@1=74.246 Prec@5=91.383 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=07:21 IST
=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.028 Prec@1=74.246 Prec@5=91.383 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=07:21 IST
=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.227 Prec@5=91.375 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=07:21 IST
=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.227 Prec@5=91.375 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=07:21 IST
=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.227 Prec@5=91.375 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=07:22 IST
=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.213 Prec@5=91.373 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=07:22 IST
=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.213 Prec@5=91.373 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:22 IST
=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.213 Prec@5=91.373 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:23 IST
=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.217 Prec@5=91.371 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=07:23 IST
=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.217 Prec@5=91.371 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=07:23 IST
=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.029 Prec@1=74.217 Prec@5=91.371 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=07:24 IST
=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.030 Prec@1=74.201 Prec@5=91.360 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=07:24 IST
=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.030 Prec@1=74.201 Prec@5=91.360 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=07:24 IST
=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.291 Loss=1.030 Prec@1=74.201 Prec@5=91.360 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=07:25 IST
=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.031 Prec@1=74.185 Prec@5=91.346 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=07:25 IST
=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.031 Prec@1=74.185 Prec@5=91.346 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:25 IST
=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.031 Prec@1=74.185 Prec@5=91.346 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:26 IST
=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.168 Prec@5=91.333 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:26 IST
=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.168 Prec@5=91.333 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:26 IST
=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.486 DataTime=0.290 Loss=1.032 Prec@1=74.168 Prec@5=91.333 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:26 IST
=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.032 Prec@1=74.147 Prec@5=91.331 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=07:26 IST
=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.032 Prec@1=74.147 Prec@5=91.331 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=07:26 IST
=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.032 Prec@1=74.147 Prec@5=91.331 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=07:27 IST
=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.033 Prec@1=74.141 Prec@5=91.326 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=07:27 IST
=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.033 Prec@1=74.141 Prec@5=91.326 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=07:27 IST
=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.033 Prec@1=74.141 Prec@5=91.326 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=07:27 IST
=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.033 Prec@1=74.141 Prec@5=91.326 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=07:27 IST
=> training   100.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.485 DataTime=0.290 Loss=1.033 Prec@1=74.141 Prec@5=91.326 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=07:27 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:27 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:27 IST
=> validation 0.00% of 1x98...Epoch=116/150 LR=0.01284 Time=7.001 Loss=0.728 Prec@1=81.836 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=07:27 IST
=> validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=7.001 Loss=0.728 Prec@1=81.836 Prec@5=95.117 rate=8117.14 Hz, eta=0:00:00, total=0:00:00, wall=07:27 IST
** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=7.001 Loss=0.728 Prec@1=81.836 Prec@5=95.117 rate=8117.14 Hz, eta=0:00:00, total=0:00:00, wall=07:28 IST
** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=0.563 Loss=1.225 Prec@1=70.118 Prec@5=89.372 rate=8117.14 Hz, eta=0:00:00, total=0:00:00, wall=07:28 IST
** validation 100.00% of 1x98...Epoch=116/150 LR=0.01284 Time=0.563 Loss=1.225 Prec@1=70.118 Prec@5=89.372 rate=2.04 Hz, eta=0:00:00, total=0:00:48, wall=07:28 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:28 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:28 IST
=> training   0.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.205 DataTime=5.016 Loss=1.019 Prec@1=74.609 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=07:28 IST
=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.205 DataTime=5.016 Loss=1.019 Prec@1=74.609 Prec@5=92.188 rate=7799.40 Hz, eta=0:00:00, total=0:00:00, wall=07:28 IST
=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.205 DataTime=5.016 Loss=1.019 Prec@1=74.609 Prec@5=92.188 rate=7799.40 Hz, eta=0:00:00, total=0:00:00, wall=07:29 IST
=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.501 DataTime=0.304 Loss=1.008 Prec@1=74.923 Prec@5=91.691 rate=7799.40 Hz, eta=0:00:00, total=0:00:00, wall=07:29 IST
=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.501 DataTime=0.304 Loss=1.008 Prec@1=74.923 Prec@5=91.691 rate=2.22 Hz, eta=0:18:00, total=0:00:45, wall=07:29 IST
=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.501 DataTime=0.304 Loss=1.008 Prec@1=74.923 Prec@5=91.691 rate=2.22 Hz, eta=0:18:00, total=0:00:45, wall=07:30 IST
=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.494 DataTime=0.297 Loss=1.002 Prec@1=74.914 Prec@5=91.753 rate=2.22 Hz, eta=0:18:00, total=0:00:45, wall=07:30 IST
=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.494 DataTime=0.297 Loss=1.002 Prec@1=74.914 Prec@5=91.753 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=07:30 IST
=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.494 DataTime=0.297 Loss=1.002 Prec@1=74.914 Prec@5=91.753 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=07:31 IST
=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.491 DataTime=0.296 Loss=1.007 Prec@1=74.777 Prec@5=91.665 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=07:31 IST
=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.491 DataTime=0.296 Loss=1.007 Prec@1=74.777 Prec@5=91.665 rate=2.11 Hz, eta=0:17:23, total=0:02:22, wall=07:31 IST
=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.491 DataTime=0.296 Loss=1.007 Prec@1=74.777 Prec@5=91.665 rate=2.11 Hz, eta=0:17:23, total=0:02:22, wall=07:31 IST
=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.488 DataTime=0.294 Loss=1.005 Prec@1=74.773 Prec@5=91.701 rate=2.11 Hz, eta=0:17:23, total=0:02:22, wall=07:31 IST
=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.488 DataTime=0.294 Loss=1.005 Prec@1=74.773 Prec@5=91.701 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=07:31 IST
=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.488 DataTime=0.294 Loss=1.005 Prec@1=74.773 Prec@5=91.701 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=07:32 IST
=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.733 Prec@5=91.674 rate=2.10 Hz, eta=0:16:39, total=0:03:10, wall=07:32 IST
=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.733 Prec@5=91.674 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=07:32 IST
=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.733 Prec@5=91.674 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=07:33 IST
=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.719 Prec@5=91.670 rate=2.10 Hz, eta=0:15:52, total=0:03:58, wall=07:33 IST
=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.719 Prec@5=91.670 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=07:33 IST
=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.008 Prec@1=74.719 Prec@5=91.670 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=07:34 IST
=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.663 Prec@5=91.661 rate=2.10 Hz, eta=0:15:07, total=0:04:46, wall=07:34 IST
=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.663 Prec@5=91.661 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=07:34 IST
=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.663 Prec@5=91.661 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=07:35 IST
=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.011 Prec@1=74.650 Prec@5=91.637 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=07:35 IST
=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.011 Prec@1=74.650 Prec@5=91.637 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=07:35 IST
=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.011 Prec@1=74.650 Prec@5=91.637 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=07:35 IST
=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.012 Prec@1=74.616 Prec@5=91.610 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=07:35 IST
=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.012 Prec@1=74.616 Prec@5=91.610 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=07:35 IST
=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.012 Prec@1=74.616 Prec@5=91.610 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=07:36 IST
=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.014 Prec@1=74.584 Prec@5=91.579 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=07:36 IST
=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.014 Prec@1=74.584 Prec@5=91.579 rate=2.08 Hz, eta=0:12:03, total=0:08:01, wall=07:36 IST
=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.014 Prec@1=74.584 Prec@5=91.579 rate=2.08 Hz, eta=0:12:03, total=0:08:01, wall=07:37 IST
=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.015 Prec@1=74.550 Prec@5=91.549 rate=2.08 Hz, eta=0:12:03, total=0:08:01, wall=07:37 IST
=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.015 Prec@1=74.550 Prec@5=91.549 rate=2.07 Hz, eta=0:11:16, total=0:08:50, wall=07:37 IST
=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.292 Loss=1.015 Prec@1=74.550 Prec@5=91.549 rate=2.07 Hz, eta=0:11:16, total=0:08:50, wall=07:38 IST
=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.016 Prec@1=74.552 Prec@5=91.534 rate=2.07 Hz, eta=0:11:16, total=0:08:50, wall=07:38 IST
=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.016 Prec@1=74.552 Prec@5=91.534 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=07:38 IST
=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.487 DataTime=0.291 Loss=1.016 Prec@1=74.552 Prec@5=91.534 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=07:39 IST
=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.016 Prec@1=74.540 Prec@5=91.528 rate=2.07 Hz, eta=0:10:27, total=0:09:39, wall=07:39 IST
=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.016 Prec@1=74.540 Prec@5=91.528 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=07:39 IST
=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.016 Prec@1=74.540 Prec@5=91.528 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=07:39 IST
=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.016 Prec@1=74.526 Prec@5=91.526 rate=2.07 Hz, eta=0:09:39, total=0:10:27, wall=07:39 IST
=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.016 Prec@1=74.526 Prec@5=91.526 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=07:39 IST
=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.016 Prec@1=74.526 Prec@5=91.526 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=07:40 IST
=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.018 Prec@1=74.482 Prec@5=91.496 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=07:40 IST
=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.018 Prec@1=74.482 Prec@5=91.496 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=07:40 IST
=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.018 Prec@1=74.482 Prec@5=91.496 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=07:41 IST
=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.019 Prec@1=74.473 Prec@5=91.486 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=07:41 IST
=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.019 Prec@1=74.473 Prec@5=91.486 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=07:41 IST
=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.019 Prec@1=74.473 Prec@5=91.486 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=07:42 IST
=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.453 Prec@5=91.482 rate=2.08 Hz, eta=0:07:14, total=0:12:51, wall=07:42 IST
=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.453 Prec@5=91.482 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=07:42 IST
=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.453 Prec@5=91.482 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=07:43 IST
=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.020 Prec@1=74.434 Prec@5=91.477 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=07:43 IST
=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.020 Prec@1=74.434 Prec@5=91.477 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=07:43 IST
=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.020 Prec@1=74.434 Prec@5=91.477 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=07:43 IST
=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.409 Prec@5=91.475 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=07:43 IST
=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.409 Prec@5=91.475 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=07:43 IST
=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.020 Prec@1=74.409 Prec@5=91.475 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=07:44 IST
=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.021 Prec@1=74.400 Prec@5=91.475 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=07:44 IST
=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.021 Prec@1=74.400 Prec@5=91.475 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=07:44 IST
=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.021 Prec@1=74.400 Prec@5=91.475 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=07:45 IST
=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.022 Prec@1=74.372 Prec@5=91.461 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=07:45 IST
=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.022 Prec@1=74.372 Prec@5=91.461 rate=2.07 Hz, eta=0:03:14, total=0:16:53, wall=07:45 IST
=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.022 Prec@1=74.372 Prec@5=91.461 rate=2.07 Hz, eta=0:03:14, total=0:16:53, wall=07:46 IST
=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.022 Prec@1=74.370 Prec@5=91.458 rate=2.07 Hz, eta=0:03:14, total=0:16:53, wall=07:46 IST
=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.022 Prec@1=74.370 Prec@5=91.458 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:46 IST
=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.486 DataTime=0.291 Loss=1.022 Prec@1=74.370 Prec@5=91.458 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:47 IST
=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.357 Prec@5=91.449 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=07:47 IST
=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.357 Prec@5=91.449 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=07:47 IST
=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.357 Prec@5=91.449 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=07:48 IST
=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.344 Prec@5=91.442 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=07:48 IST
=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.344 Prec@5=91.442 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=07:48 IST
=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.023 Prec@1=74.344 Prec@5=91.442 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=07:48 IST
=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.024 Prec@1=74.329 Prec@5=91.440 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=07:48 IST
=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.024 Prec@1=74.329 Prec@5=91.440 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=07:48 IST
=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.024 Prec@1=74.329 Prec@5=91.440 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=07:48 IST
=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.024 Prec@1=74.328 Prec@5=91.438 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=07:48 IST
=> training   100.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.485 DataTime=0.290 Loss=1.024 Prec@1=74.328 Prec@5=91.438 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=07:48 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:48 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:48 IST
=> validation 0.00% of 1x98...Epoch=117/150 LR=0.01215 Time=6.844 Loss=0.732 Prec@1=81.641 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=07:48 IST
=> validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=6.844 Loss=0.732 Prec@1=81.641 Prec@5=94.727 rate=8520.58 Hz, eta=0:00:00, total=0:00:00, wall=07:48 IST
** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=6.844 Loss=0.732 Prec@1=81.641 Prec@5=94.727 rate=8520.58 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST
** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=0.562 Loss=1.210 Prec@1=70.340 Prec@5=89.586 rate=8520.58 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST
** validation 100.00% of 1x98...Epoch=117/150 LR=0.01215 Time=0.562 Loss=1.210 Prec@1=70.340 Prec@5=89.586 rate=2.03 Hz, eta=0:00:00, total=0:00:48, wall=07:49 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST
=> training   0.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.596 DataTime=4.301 Loss=0.884 Prec@1=77.930 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST
=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.596 DataTime=4.301 Loss=0.884 Prec@1=77.930 Prec@5=92.773 rate=4226.11 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST
=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.596 DataTime=4.301 Loss=0.884 Prec@1=77.930 Prec@5=92.773 rate=4226.11 Hz, eta=0:00:00, total=0:00:00, wall=07:50 IST
=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.511 DataTime=0.316 Loss=0.993 Prec@1=75.166 Prec@5=91.866 rate=4226.11 Hz, eta=0:00:00, total=0:00:00, wall=07:50 IST
=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.511 DataTime=0.316 Loss=0.993 Prec@1=75.166 Prec@5=91.866 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:50 IST
=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.511 DataTime=0.316 Loss=0.993 Prec@1=75.166 Prec@5=91.866 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:51 IST
=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.493 DataTime=0.297 Loss=0.993 Prec@1=75.163 Prec@5=91.815 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=07:51 IST
=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.493 DataTime=0.297 Loss=0.993 Prec@1=75.163 Prec@5=91.815 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=07:51 IST
=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.493 DataTime=0.297 Loss=0.993 Prec@1=75.163 Prec@5=91.815 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=07:52 IST
=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.494 DataTime=0.299 Loss=0.990 Prec@1=75.160 Prec@5=91.912 rate=2.13 Hz, eta=0:18:02, total=0:01:34, wall=07:52 IST
=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.494 DataTime=0.299 Loss=0.990 Prec@1=75.160 Prec@5=91.912 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=07:52 IST
=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.494 DataTime=0.299 Loss=0.990 Prec@1=75.160 Prec@5=91.912 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=07:53 IST
=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.490 DataTime=0.295 Loss=0.993 Prec@1=75.131 Prec@5=91.852 rate=2.09 Hz, eta=0:17:33, total=0:02:24, wall=07:53 IST
=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.490 DataTime=0.295 Loss=0.993 Prec@1=75.131 Prec@5=91.852 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:53 IST
=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.490 DataTime=0.295 Loss=0.993 Prec@1=75.131 Prec@5=91.852 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:53 IST
=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=0.998 Prec@1=74.988 Prec@5=91.784 rate=2.09 Hz, eta=0:16:45, total=0:03:11, wall=07:53 IST
=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=0.998 Prec@1=74.988 Prec@5=91.784 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=07:53 IST
=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=0.998 Prec@1=74.988 Prec@5=91.784 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=07:54 IST
=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=1.001 Prec@1=74.928 Prec@5=91.733 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=07:54 IST
=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=1.001 Prec@1=74.928 Prec@5=91.733 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=07:54 IST
=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.488 DataTime=0.293 Loss=1.001 Prec@1=74.928 Prec@5=91.733 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=07:55 IST
=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.001 Prec@1=74.939 Prec@5=91.729 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=07:55 IST
=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.001 Prec@1=74.939 Prec@5=91.729 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=07:55 IST
=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.001 Prec@1=74.939 Prec@5=91.729 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=07:56 IST
=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.000 Prec@1=74.937 Prec@5=91.738 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=07:56 IST
=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.000 Prec@1=74.937 Prec@5=91.738 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=07:56 IST
=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.000 Prec@1=74.937 Prec@5=91.738 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=07:57 IST
=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.884 Prec@5=91.712 rate=2.08 Hz, eta=0:13:39, total=0:06:25, wall=07:57 IST
=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.884 Prec@5=91.712 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=07:57 IST
=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.884 Prec@5=91.712 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=07:57 IST
=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.003 Prec@1=74.871 Prec@5=91.691 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=07:57 IST
=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.003 Prec@1=74.871 Prec@5=91.691 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:57 IST
=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.003 Prec@1=74.871 Prec@5=91.691 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:58 IST
=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.004 Prec@1=74.831 Prec@5=91.673 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=07:58 IST
=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.004 Prec@1=74.831 Prec@5=91.673 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=07:58 IST
=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.004 Prec@1=74.831 Prec@5=91.673 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=07:59 IST
=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.005 Prec@1=74.806 Prec@5=91.649 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=07:59 IST
=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.005 Prec@1=74.806 Prec@5=91.649 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=07:59 IST
=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.005 Prec@1=74.806 Prec@5=91.649 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=08:00 IST
=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.783 Prec@5=91.637 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=08:00 IST
=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.783 Prec@5=91.637 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=08:00 IST
=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.783 Prec@5=91.637 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=08:01 IST
=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.781 Prec@5=91.649 rate=2.07 Hz, eta=0:09:41, total=0:10:29, wall=08:01 IST
=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.781 Prec@5=91.649 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=08:01 IST
=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.006 Prec@1=74.781 Prec@5=91.649 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=08:01 IST
=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.785 Prec@5=91.635 rate=2.07 Hz, eta=0:08:53, total=0:11:17, wall=08:01 IST
=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.785 Prec@5=91.635 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=08:01 IST
=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.785 Prec@5=91.635 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=08:02 IST
=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.008 Prec@1=74.750 Prec@5=91.614 rate=2.07 Hz, eta=0:08:03, total=0:12:04, wall=08:02 IST
=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.008 Prec@1=74.750 Prec@5=91.614 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:02 IST
=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.291 Loss=1.008 Prec@1=74.750 Prec@5=91.614 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:03 IST
=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.008 Prec@1=74.728 Prec@5=91.610 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:03 IST
=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.008 Prec@1=74.728 Prec@5=91.610 rate=2.07 Hz, eta=0:06:28, total=0:13:43, wall=08:03 IST
=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.487 DataTime=0.292 Loss=1.008 Prec@1=74.728 Prec@5=91.610 rate=2.07 Hz, eta=0:06:28, total=0:13:43, wall=08:04 IST
=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.690 Prec@5=91.587 rate=2.07 Hz, eta=0:06:28, total=0:13:43, wall=08:04 IST
=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.690 Prec@5=91.587 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=08:04 IST
=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.010 Prec@1=74.690 Prec@5=91.587 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=08:05 IST
=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.011 Prec@1=74.659 Prec@5=91.565 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=08:05 IST
=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.011 Prec@1=74.659 Prec@5=91.565 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=08:05 IST
=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.011 Prec@1=74.659 Prec@5=91.565 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=08:05 IST
=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.012 Prec@1=74.645 Prec@5=91.559 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=08:05 IST
=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.012 Prec@1=74.645 Prec@5=91.559 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=08:05 IST
=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.012 Prec@1=74.645 Prec@5=91.559 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=08:06 IST
=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.013 Prec@1=74.617 Prec@5=91.541 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=08:06 IST
=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.013 Prec@1=74.617 Prec@5=91.541 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:06 IST
=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.013 Prec@1=74.617 Prec@5=91.541 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:07 IST
=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.014 Prec@1=74.606 Prec@5=91.535 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:07 IST
=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.014 Prec@1=74.606 Prec@5=91.535 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:07 IST
=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.486 DataTime=0.291 Loss=1.014 Prec@1=74.606 Prec@5=91.535 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:08 IST
=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.597 Prec@5=91.533 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:08 IST
=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.597 Prec@5=91.533 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:08 IST
=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.597 Prec@5=91.533 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:09 IST
=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.584 Prec@5=91.529 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:09 IST
=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.584 Prec@5=91.529 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:09 IST
=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.290 Loss=1.015 Prec@1=74.584 Prec@5=91.529 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:10 IST
=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.291 Loss=1.015 Prec@1=74.580 Prec@5=91.527 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:10 IST
=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.291 Loss=1.015 Prec@1=74.580 Prec@5=91.527 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=08:10 IST
=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.291 Loss=1.015 Prec@1=74.580 Prec@5=91.527 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=08:10 IST
=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.291 Loss=1.015 Prec@1=74.577 Prec@5=91.526 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=08:10 IST
=> training   100.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.485 DataTime=0.291 Loss=1.015 Prec@1=74.577 Prec@5=91.526 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=08:10 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> validation 0.00% of 1x98...Epoch=118/150 LR=0.01147 Time=6.891 Loss=0.697 Prec@1=80.273 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.891 Loss=0.697 Prec@1=80.273 Prec@5=94.727 rate=8430.99 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.891 Loss=0.697 Prec@1=80.273 Prec@5=94.727 rate=8430.99 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=0.563 Loss=1.215 Prec@1=70.110 Prec@5=89.574 rate=8430.99 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
** validation 100.00% of 1x98...Epoch=118/150 LR=0.01147 Time=0.563 Loss=1.215 Prec@1=70.110 Prec@5=89.574 rate=2.03 Hz, eta=0:00:00, total=0:00:48, wall=08:10 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:11 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:11 IST
=> training   0.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.947 DataTime=4.674 Loss=0.907 Prec@1=77.930 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=08:11 IST
=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.947 DataTime=4.674 Loss=0.907 Prec@1=77.930 Prec@5=93.750 rate=10055.30 Hz, eta=0:00:00, total=0:00:00, wall=08:11 IST
=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.947 DataTime=4.674 Loss=0.907 Prec@1=77.930 Prec@5=93.750 rate=10055.30 Hz, eta=0:00:00, total=0:00:00, wall=08:11 IST
=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.527 DataTime=0.333 Loss=0.981 Prec@1=75.365 Prec@5=91.841 rate=10055.30 Hz, eta=0:00:00, total=0:00:00, wall=08:11 IST
=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.527 DataTime=0.333 Loss=0.981 Prec@1=75.365 Prec@5=91.841 rate=2.09 Hz, eta=0:19:07, total=0:00:48, wall=08:11 IST
=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.527 DataTime=0.333 Loss=0.981 Prec@1=75.365 Prec@5=91.841 rate=2.09 Hz, eta=0:19:07, total=0:00:48, wall=08:12 IST
=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.508 DataTime=0.314 Loss=0.976 Prec@1=75.534 Prec@5=91.918 rate=2.09 Hz, eta=0:19:07, total=0:00:48, wall=08:12 IST
=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.508 DataTime=0.314 Loss=0.976 Prec@1=75.534 Prec@5=91.918 rate=2.07 Hz, eta=0:18:33, total=0:01:37, wall=08:12 IST
=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.508 DataTime=0.314 Loss=0.976 Prec@1=75.534 Prec@5=91.918 rate=2.07 Hz, eta=0:18:33, total=0:01:37, wall=08:13 IST
=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.498 DataTime=0.304 Loss=0.981 Prec@1=75.386 Prec@5=91.929 rate=2.07 Hz, eta=0:18:33, total=0:01:37, wall=08:13 IST
=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.498 DataTime=0.304 Loss=0.981 Prec@1=75.386 Prec@5=91.929 rate=2.08 Hz, eta=0:17:39, total=0:02:24, wall=08:13 IST
=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.498 DataTime=0.304 Loss=0.981 Prec@1=75.386 Prec@5=91.929 rate=2.08 Hz, eta=0:17:39, total=0:02:24, wall=08:14 IST
=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.492 DataTime=0.298 Loss=0.985 Prec@1=75.265 Prec@5=91.859 rate=2.08 Hz, eta=0:17:39, total=0:02:24, wall=08:14 IST
=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.492 DataTime=0.298 Loss=0.985 Prec@1=75.265 Prec@5=91.859 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=08:14 IST
=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.492 DataTime=0.298 Loss=0.985 Prec@1=75.265 Prec@5=91.859 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=08:15 IST
=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.294 Loss=0.988 Prec@1=75.211 Prec@5=91.818 rate=2.08 Hz, eta=0:16:48, total=0:03:12, wall=08:15 IST
=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.294 Loss=0.988 Prec@1=75.211 Prec@5=91.818 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=08:15 IST
=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.294 Loss=0.988 Prec@1=75.211 Prec@5=91.818 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=08:15 IST
=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.989 Prec@1=75.185 Prec@5=91.797 rate=2.09 Hz, eta=0:15:57, total=0:03:59, wall=08:15 IST
=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.989 Prec@1=75.185 Prec@5=91.797 rate=2.08 Hz, eta=0:15:12, total=0:04:48, wall=08:15 IST
=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.989 Prec@1=75.185 Prec@5=91.797 rate=2.08 Hz, eta=0:15:12, total=0:04:48, wall=08:16 IST
=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.989 Prec@1=75.189 Prec@5=91.796 rate=2.08 Hz, eta=0:15:12, total=0:04:48, wall=08:16 IST
=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.989 Prec@1=75.189 Prec@5=91.796 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=08:16 IST
=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.989 Prec@1=75.189 Prec@5=91.796 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=08:17 IST
=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.992 Prec@1=75.113 Prec@5=91.766 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=08:17 IST
=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.992 Prec@1=75.113 Prec@5=91.766 rate=2.08 Hz, eta=0:13:40, total=0:06:25, wall=08:17 IST
=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.488 DataTime=0.293 Loss=0.992 Prec@1=75.113 Prec@5=91.766 rate=2.08 Hz, eta=0:13:40, total=0:06:25, wall=08:18 IST
=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.992 Prec@1=75.105 Prec@5=91.772 rate=2.08 Hz, eta=0:13:40, total=0:06:25, wall=08:18 IST
=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.992 Prec@1=75.105 Prec@5=91.772 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=08:18 IST
=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.992 Prec@1=75.105 Prec@5=91.772 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=08:19 IST
=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.993 Prec@1=75.084 Prec@5=91.764 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=08:19 IST
=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.993 Prec@1=75.084 Prec@5=91.764 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=08:19 IST
=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.993 Prec@1=75.084 Prec@5=91.764 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=08:19 IST
=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.995 Prec@1=75.048 Prec@5=91.729 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=08:19 IST
=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.995 Prec@1=75.048 Prec@5=91.729 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=08:19 IST
=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.995 Prec@1=75.048 Prec@5=91.729 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=08:20 IST
=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.996 Prec@1=75.016 Prec@5=91.711 rate=2.07 Hz, eta=0:11:16, total=0:08:51, wall=08:20 IST
=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.996 Prec@1=75.016 Prec@5=91.711 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=08:20 IST
=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.292 Loss=0.996 Prec@1=75.016 Prec@5=91.711 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=08:21 IST
=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.997 Prec@1=75.007 Prec@5=91.708 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=08:21 IST
=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.997 Prec@1=75.007 Prec@5=91.708 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=08:21 IST
=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.997 Prec@1=75.007 Prec@5=91.708 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=08:22 IST
=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.981 Prec@5=91.701 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=08:22 IST
=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.981 Prec@5=91.701 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=08:22 IST
=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.981 Prec@5=91.701 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=08:23 IST
=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.998 Prec@5=91.714 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=08:23 IST
=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.998 Prec@5=91.714 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=08:23 IST
=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.997 Prec@1=74.998 Prec@5=91.714 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=08:23 IST
=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.998 Prec@1=74.950 Prec@5=91.702 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=08:23 IST
=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.998 Prec@1=74.950 Prec@5=91.702 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:23 IST
=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.487 DataTime=0.291 Loss=0.998 Prec@1=74.950 Prec@5=91.702 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:24 IST
=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.999 Prec@1=74.917 Prec@5=91.688 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=08:24 IST
=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.999 Prec@1=74.917 Prec@5=91.688 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=08:24 IST
=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=0.999 Prec@1=74.917 Prec@5=91.688 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=08:25 IST
=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.485 DataTime=0.290 Loss=1.001 Prec@1=74.889 Prec@5=91.674 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=08:25 IST
=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.485 DataTime=0.290 Loss=1.001 Prec@1=74.889 Prec@5=91.674 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=08:25 IST
=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.485 DataTime=0.290 Loss=1.001 Prec@1=74.889 Prec@5=91.674 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=08:26 IST
=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.852 Prec@5=91.661 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=08:26 IST
=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.852 Prec@5=91.661 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=08:26 IST
=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.002 Prec@1=74.852 Prec@5=91.661 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=08:27 IST
=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.003 Prec@1=74.809 Prec@5=91.641 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=08:27 IST
=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.003 Prec@1=74.809 Prec@5=91.641 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=08:27 IST
=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.003 Prec@1=74.809 Prec@5=91.641 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=08:28 IST
=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.004 Prec@1=74.805 Prec@5=91.635 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=08:28 IST
=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.004 Prec@1=74.805 Prec@5=91.635 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=08:28 IST
=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.004 Prec@1=74.805 Prec@5=91.635 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=08:28 IST
=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.005 Prec@1=74.781 Prec@5=91.621 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=08:28 IST
=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.005 Prec@1=74.781 Prec@5=91.621 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:28 IST
=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.005 Prec@1=74.781 Prec@5=91.621 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:29 IST
=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.767 Prec@5=91.613 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=08:29 IST
=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.767 Prec@5=91.613 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=08:29 IST
=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.767 Prec@5=91.613 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=08:30 IST
=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.760 Prec@5=91.613 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=08:30 IST
=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.760 Prec@5=91.613 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=08:30 IST
=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.006 Prec@1=74.760 Prec@5=91.613 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=08:31 IST
=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.754 Prec@5=91.610 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=08:31 IST
=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.754 Prec@5=91.610 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=08:31 IST
=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.754 Prec@5=91.610 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=08:31 IST
=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.754 Prec@5=91.610 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=08:31 IST
=> training   100.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.486 DataTime=0.291 Loss=1.007 Prec@1=74.754 Prec@5=91.610 rate=2.07 Hz, eta=0:00:00, total=0:20:11, wall=08:31 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST
=> validation 0.00% of 1x98...Epoch=119/150 LR=0.01082 Time=6.992 Loss=0.710 Prec@1=80.859 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST
=> validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=6.992 Loss=0.710 Prec@1=80.859 Prec@5=95.312 rate=7486.71 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST
** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=6.992 Loss=0.710 Prec@1=80.859 Prec@5=95.312 rate=7486.71 Hz, eta=0:00:00, total=0:00:00, wall=08:32 IST
** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=0.560 Loss=1.204 Prec@1=70.458 Prec@5=89.708 rate=7486.71 Hz, eta=0:00:00, total=0:00:00, wall=08:32 IST
** validation 100.00% of 1x98...Epoch=119/150 LR=0.01082 Time=0.560 Loss=1.204 Prec@1=70.458 Prec@5=89.708 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=08:32 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:32 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:32 IST
=> training   0.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=5.090 DataTime=4.734 Loss=1.096 Prec@1=73.438 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=08:32 IST
=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=5.090 DataTime=4.734 Loss=1.096 Prec@1=73.438 Prec@5=89.453 rate=9989.21 Hz, eta=0:00:00, total=0:00:00, wall=08:32 IST
=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=5.090 DataTime=4.734 Loss=1.096 Prec@1=73.438 Prec@5=89.453 rate=9989.21 Hz, eta=0:00:00, total=0:00:00, wall=08:33 IST
=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.515 DataTime=0.318 Loss=0.990 Prec@1=75.300 Prec@5=91.799 rate=9989.21 Hz, eta=0:00:00, total=0:00:00, wall=08:33 IST
=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.515 DataTime=0.318 Loss=0.990 Prec@1=75.300 Prec@5=91.799 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=08:33 IST
=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.515 DataTime=0.318 Loss=0.990 Prec@1=75.300 Prec@5=91.799 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=08:33 IST
=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.492 DataTime=0.295 Loss=0.984 Prec@1=75.396 Prec@5=91.906 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=08:33 IST
=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.492 DataTime=0.295 Loss=0.984 Prec@1=75.396 Prec@5=91.906 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=08:33 IST
=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.492 DataTime=0.295 Loss=0.984 Prec@1=75.396 Prec@5=91.906 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=08:34 IST
=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.984 Prec@1=75.337 Prec@5=91.943 rate=2.14 Hz, eta=0:17:54, total=0:01:33, wall=08:34 IST
=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.984 Prec@1=75.337 Prec@5=91.943 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=08:34 IST
=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.984 Prec@1=75.337 Prec@5=91.943 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=08:35 IST
=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.289 Loss=0.984 Prec@1=75.304 Prec@5=91.936 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=08:35 IST
=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.289 Loss=0.984 Prec@1=75.304 Prec@5=91.936 rate=2.12 Hz, eta=0:16:32, total=0:03:09, wall=08:35 IST
=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.289 Loss=0.984 Prec@1=75.304 Prec@5=91.936 rate=2.12 Hz, eta=0:16:32, total=0:03:09, wall=08:36 IST
=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.253 Prec@5=91.881 rate=2.12 Hz, eta=0:16:32, total=0:03:09, wall=08:36 IST
=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.253 Prec@5=91.881 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=08:36 IST
=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.253 Prec@5=91.881 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=08:37 IST
=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.484 DataTime=0.288 Loss=0.987 Prec@1=75.254 Prec@5=91.879 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=08:37 IST
=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.484 DataTime=0.288 Loss=0.987 Prec@1=75.254 Prec@5=91.879 rate=2.10 Hz, eta=0:15:04, total=0:04:45, wall=08:37 IST
=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.484 DataTime=0.288 Loss=0.987 Prec@1=75.254 Prec@5=91.879 rate=2.10 Hz, eta=0:15:04, total=0:04:45, wall=08:37 IST
=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.987 Prec@1=75.227 Prec@5=91.903 rate=2.10 Hz, eta=0:15:04, total=0:04:45, wall=08:37 IST
=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.987 Prec@1=75.227 Prec@5=91.903 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=08:37 IST
=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.987 Prec@1=75.227 Prec@5=91.903 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=08:38 IST
=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.988 Prec@1=75.219 Prec@5=91.885 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=08:38 IST
=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.988 Prec@1=75.219 Prec@5=91.885 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=08:38 IST
=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.988 Prec@1=75.219 Prec@5=91.885 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=08:39 IST
=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.182 Prec@5=91.869 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=08:39 IST
=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.182 Prec@5=91.869 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=08:39 IST
=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.182 Prec@5=91.869 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=08:40 IST
=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.186 Prec@5=91.868 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=08:40 IST
=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.186 Prec@5=91.868 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=08:40 IST
=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.186 Prec@5=91.868 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=08:41 IST
=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.167 Prec@5=91.852 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=08:41 IST
=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.167 Prec@5=91.852 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=08:41 IST
=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.989 Prec@1=75.167 Prec@5=91.852 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=08:41 IST
=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.989 Prec@1=75.161 Prec@5=91.858 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=08:41 IST
=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.989 Prec@1=75.161 Prec@5=91.858 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=08:41 IST
=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.989 Prec@1=75.161 Prec@5=91.858 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=08:42 IST
=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.990 Prec@1=75.117 Prec@5=91.847 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=08:42 IST
=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.990 Prec@1=75.117 Prec@5=91.847 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=08:42 IST
=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.990 Prec@1=75.117 Prec@5=91.847 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=08:43 IST
=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.990 Prec@1=75.115 Prec@5=91.849 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=08:43 IST
=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.990 Prec@1=75.115 Prec@5=91.849 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=08:43 IST
=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.291 Loss=0.990 Prec@1=75.115 Prec@5=91.849 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=08:44 IST
=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.991 Prec@1=75.104 Prec@5=91.845 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=08:44 IST
=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.991 Prec@1=75.104 Prec@5=91.845 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=08:44 IST
=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.991 Prec@1=75.104 Prec@5=91.845 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=08:45 IST
=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.069 Prec@5=91.835 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=08:45 IST
=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.069 Prec@5=91.835 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=08:45 IST
=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.069 Prec@5=91.835 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=08:45 IST
=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.992 Prec@1=75.054 Prec@5=91.832 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=08:45 IST
=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.992 Prec@1=75.054 Prec@5=91.832 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=08:45 IST
=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.992 Prec@1=75.054 Prec@5=91.832 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=08:46 IST
=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.051 Prec@5=91.830 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=08:46 IST
=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.051 Prec@5=91.830 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=08:46 IST
=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.992 Prec@1=75.051 Prec@5=91.830 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=08:47 IST
=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.993 Prec@1=75.023 Prec@5=91.825 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=08:47 IST
=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.993 Prec@1=75.023 Prec@5=91.825 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=08:47 IST
=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.993 Prec@1=75.023 Prec@5=91.825 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=08:48 IST
=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.993 Prec@1=75.024 Prec@5=91.823 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=08:48 IST
=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.993 Prec@1=75.024 Prec@5=91.823 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=08:48 IST
=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.993 Prec@1=75.024 Prec@5=91.823 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=08:49 IST
=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.994 Prec@1=75.002 Prec@5=91.808 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=08:49 IST
=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.994 Prec@1=75.002 Prec@5=91.808 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:49 IST
=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.486 DataTime=0.290 Loss=0.994 Prec@1=75.002 Prec@5=91.808 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:50 IST
=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=75.009 Prec@5=91.803 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=08:50 IST
=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=75.009 Prec@5=91.803 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=08:50 IST
=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=75.009 Prec@5=91.803 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=08:50 IST
=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.983 Prec@5=91.791 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=08:50 IST
=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.983 Prec@5=91.791 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:50 IST
=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.983 Prec@5=91.791 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:51 IST
=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.982 Prec@5=91.792 rate=2.07 Hz, eta=0:01:37, total=0:18:31, wall=08:51 IST
=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.982 Prec@5=91.792 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:51 IST
=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.995 Prec@1=74.982 Prec@5=91.792 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:52 IST
=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.996 Prec@1=74.970 Prec@5=91.785 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=08:52 IST
=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.996 Prec@1=74.970 Prec@5=91.785 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=08:52 IST
=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.996 Prec@1=74.970 Prec@5=91.785 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=08:52 IST
=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.996 Prec@1=74.970 Prec@5=91.786 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=08:52 IST
=> training   100.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.485 DataTime=0.290 Loss=0.996 Prec@1=74.970 Prec@5=91.786 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=08:52 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:52 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:52 IST
=> validation 0.00% of 1x98...Epoch=120/150 LR=0.01017 Time=6.923 Loss=0.721 Prec@1=80.273 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=08:52 IST
=> validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=6.923 Loss=0.721 Prec@1=80.273 Prec@5=94.922 rate=10117.26 Hz, eta=0:00:00, total=0:00:00, wall=08:52 IST
** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=6.923 Loss=0.721 Prec@1=80.273 Prec@5=94.922 rate=10117.26 Hz, eta=0:00:00, total=0:00:00, wall=08:53 IST
** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=0.562 Loss=1.210 Prec@1=70.370 Prec@5=89.652 rate=10117.26 Hz, eta=0:00:00, total=0:00:00, wall=08:53 IST
** validation 100.00% of 1x98...Epoch=120/150 LR=0.01017 Time=0.562 Loss=1.210 Prec@1=70.370 Prec@5=89.652 rate=2.03 Hz, eta=0:00:00, total=0:00:48, wall=08:53 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:53 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:53 IST
=> training   0.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=4.273 DataTime=4.035 Loss=0.942 Prec@1=76.562 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=08:53 IST
=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=4.273 DataTime=4.035 Loss=0.942 Prec@1=76.562 Prec@5=92.188 rate=4834.37 Hz, eta=0:00:00, total=0:00:00, wall=08:53 IST
=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=4.273 DataTime=4.035 Loss=0.942 Prec@1=76.562 Prec@5=92.188 rate=4834.37 Hz, eta=0:00:00, total=0:00:00, wall=08:54 IST
=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.509 DataTime=0.313 Loss=0.966 Prec@1=75.516 Prec@5=92.050 rate=4834.37 Hz, eta=0:00:00, total=0:00:00, wall=08:54 IST
=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.509 DataTime=0.313 Loss=0.966 Prec@1=75.516 Prec@5=92.050 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=08:54 IST
=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.509 DataTime=0.313 Loss=0.966 Prec@1=75.516 Prec@5=92.050 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=08:55 IST
=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.496 DataTime=0.300 Loss=0.970 Prec@1=75.527 Prec@5=92.086 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=08:55 IST
=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.496 DataTime=0.300 Loss=0.970 Prec@1=75.527 Prec@5=92.086 rate=2.10 Hz, eta=0:18:13, total=0:01:35, wall=08:55 IST
=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.496 DataTime=0.300 Loss=0.970 Prec@1=75.527 Prec@5=92.086 rate=2.10 Hz, eta=0:18:13, total=0:01:35, wall=08:55 IST
=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.971 Prec@1=75.498 Prec@5=92.099 rate=2.10 Hz, eta=0:18:13, total=0:01:35, wall=08:55 IST
=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.971 Prec@1=75.498 Prec@5=92.099 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=08:55 IST
=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.971 Prec@1=75.498 Prec@5=92.099 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=08:56 IST
=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.968 Prec@1=75.597 Prec@5=92.111 rate=2.10 Hz, eta=0:17:26, total=0:02:23, wall=08:56 IST
=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.968 Prec@1=75.597 Prec@5=92.111 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=08:56 IST
=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.292 Loss=0.968 Prec@1=75.597 Prec@5=92.111 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=08:57 IST
=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.290 Loss=0.971 Prec@1=75.539 Prec@5=92.057 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=08:57 IST
=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.290 Loss=0.971 Prec@1=75.539 Prec@5=92.057 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=08:57 IST
=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.290 Loss=0.971 Prec@1=75.539 Prec@5=92.057 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=08:58 IST
=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.291 Loss=0.974 Prec@1=75.516 Prec@5=92.026 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=08:58 IST
=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.291 Loss=0.974 Prec@1=75.516 Prec@5=92.026 rate=2.08 Hz, eta=0:15:14, total=0:04:49, wall=08:58 IST
=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.291 Loss=0.974 Prec@1=75.516 Prec@5=92.026 rate=2.08 Hz, eta=0:15:14, total=0:04:49, wall=08:59 IST
=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.492 Prec@5=92.014 rate=2.08 Hz, eta=0:15:14, total=0:04:49, wall=08:59 IST
=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.492 Prec@5=92.014 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=08:59 IST
=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.492 Prec@5=92.014 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=08:59 IST
=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.491 Prec@5=92.012 rate=2.07 Hz, eta=0:14:29, total=0:05:38, wall=08:59 IST
=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.491 Prec@5=92.012 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=08:59 IST
=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.489 DataTime=0.293 Loss=0.975 Prec@1=75.491 Prec@5=92.012 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=09:00 IST
=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.424 Prec@5=91.968 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=09:00 IST
=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.424 Prec@5=91.968 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=09:00 IST
=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.424 Prec@5=91.968 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=09:01 IST
=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.383 Prec@5=91.970 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=09:01 IST
=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.383 Prec@5=91.970 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=09:01 IST
=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.978 Prec@1=75.383 Prec@5=91.970 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=09:02 IST
=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.979 Prec@1=75.393 Prec@5=91.964 rate=2.07 Hz, eta=0:12:05, total=0:08:03, wall=09:02 IST
=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.979 Prec@1=75.393 Prec@5=91.964 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:02 IST
=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.979 Prec@1=75.393 Prec@5=91.964 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:03 IST
=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.980 Prec@1=75.376 Prec@5=91.950 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:03 IST
=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.980 Prec@1=75.376 Prec@5=91.950 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=09:03 IST
=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.980 Prec@1=75.376 Prec@5=91.950 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=09:03 IST
=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.358 Prec@5=91.949 rate=2.06 Hz, eta=0:10:31, total=0:09:42, wall=09:03 IST
=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.358 Prec@5=91.949 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=09:03 IST
=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.358 Prec@5=91.949 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=09:04 IST
=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.330 Prec@5=91.942 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=09:04 IST
=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.330 Prec@5=91.942 rate=2.06 Hz, eta=0:08:54, total=0:11:19, wall=09:04 IST
=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.293 Loss=0.981 Prec@1=75.330 Prec@5=91.942 rate=2.06 Hz, eta=0:08:54, total=0:11:19, wall=09:05 IST
=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.981 Prec@1=75.332 Prec@5=91.947 rate=2.06 Hz, eta=0:08:54, total=0:11:19, wall=09:05 IST
=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.981 Prec@1=75.332 Prec@5=91.947 rate=2.06 Hz, eta=0:08:05, total=0:12:07, wall=09:05 IST
=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.488 DataTime=0.292 Loss=0.981 Prec@1=75.332 Prec@5=91.947 rate=2.06 Hz, eta=0:08:05, total=0:12:07, wall=09:06 IST
=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.307 Prec@5=91.934 rate=2.06 Hz, eta=0:08:05, total=0:12:07, wall=09:06 IST
=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.307 Prec@5=91.934 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=09:06 IST
=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.307 Prec@5=91.934 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=09:07 IST
=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.312 Prec@5=91.943 rate=2.07 Hz, eta=0:07:16, total=0:12:55, wall=09:07 IST
=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.312 Prec@5=91.943 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=09:07 IST
=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.982 Prec@1=75.312 Prec@5=91.943 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=09:08 IST
=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.292 Loss=0.983 Prec@1=75.288 Prec@5=91.932 rate=2.06 Hz, eta=0:06:28, total=0:13:44, wall=09:08 IST
=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.292 Loss=0.983 Prec@1=75.288 Prec@5=91.932 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=09:08 IST
=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.292 Loss=0.983 Prec@1=75.288 Prec@5=91.932 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=09:08 IST
=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.984 Prec@1=75.276 Prec@5=91.925 rate=2.06 Hz, eta=0:05:40, total=0:14:33, wall=09:08 IST
=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.984 Prec@1=75.276 Prec@5=91.925 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:08 IST
=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.984 Prec@1=75.276 Prec@5=91.925 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:09 IST
=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.985 Prec@1=75.245 Prec@5=91.911 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:09 IST
=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.985 Prec@1=75.245 Prec@5=91.911 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=09:09 IST
=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.487 DataTime=0.291 Loss=0.985 Prec@1=75.245 Prec@5=91.911 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=09:10 IST
=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.985 Prec@1=75.224 Prec@5=91.904 rate=2.06 Hz, eta=0:04:03, total=0:16:09, wall=09:10 IST
=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.985 Prec@1=75.224 Prec@5=91.904 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=09:10 IST
=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.985 Prec@1=75.224 Prec@5=91.904 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=09:11 IST
=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.986 Prec@1=75.196 Prec@5=91.895 rate=2.06 Hz, eta=0:03:14, total=0:16:57, wall=09:11 IST
=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.986 Prec@1=75.196 Prec@5=91.895 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=09:11 IST
=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.986 Prec@1=75.196 Prec@5=91.895 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=09:12 IST
=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.177 Prec@5=91.886 rate=2.06 Hz, eta=0:02:26, total=0:17:45, wall=09:12 IST
=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.177 Prec@5=91.886 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:12 IST
=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.177 Prec@5=91.886 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:12 IST
=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.171 Prec@5=91.880 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:12 IST
=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.171 Prec@5=91.880 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=09:12 IST
=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.171 Prec@5=91.880 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=09:13 IST
=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.151 Prec@5=91.879 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=09:13 IST
=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.151 Prec@5=91.879 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:13 IST
=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.151 Prec@5=91.879 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:13 IST
=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.150 Prec@5=91.879 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:13 IST
=> training   100.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.486 DataTime=0.291 Loss=0.987 Prec@1=75.150 Prec@5=91.879 rate=2.07 Hz, eta=0:00:00, total=0:20:11, wall=09:13 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> validation 0.00% of 1x98...Epoch=121/150 LR=0.00955 Time=6.569 Loss=0.685 Prec@1=80.859 Prec@5=96.484 rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=6.569 Loss=0.685 Prec@1=80.859 Prec@5=96.484 rate=7368.43 Hz, eta=0:00:00, total=0:00:00, wall=09:13 IST
** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=6.569 Loss=0.685 Prec@1=80.859 Prec@5=96.484 rate=7368.43 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST
** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=0.564 Loss=1.195 Prec@1=70.660 Prec@5=89.772 rate=7368.43 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST
** validation 100.00% of 1x98...Epoch=121/150 LR=0.00955 Time=0.564 Loss=1.195 Prec@1=70.660 Prec@5=89.772 rate=2.01 Hz, eta=0:00:00, total=0:00:48, wall=09:14 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> training   0.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=4.573 DataTime=4.333 Loss=0.904 Prec@1=75.195 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=4.573 DataTime=4.333 Loss=0.904 Prec@1=75.195 Prec@5=93.359 rate=7791.02 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST
=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=4.573 DataTime=4.333 Loss=0.904 Prec@1=75.195 Prec@5=93.359 rate=7791.02 Hz, eta=0:00:00, total=0:00:00, wall=09:15 IST
=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.510 DataTime=0.316 Loss=0.956 Prec@1=76.054 Prec@5=92.263 rate=7791.02 Hz, eta=0:00:00, total=0:00:00, wall=09:15 IST
=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.510 DataTime=0.316 Loss=0.956 Prec@1=76.054 Prec@5=92.263 rate=2.15 Hz, eta=0:18:37, total=0:00:47, wall=09:15 IST
=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.510 DataTime=0.316 Loss=0.956 Prec@1=76.054 Prec@5=92.263 rate=2.15 Hz, eta=0:18:37, total=0:00:47, wall=09:16 IST
=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.494 DataTime=0.299 Loss=0.956 Prec@1=76.003 Prec@5=92.247 rate=2.15 Hz, eta=0:18:37, total=0:00:47, wall=09:16 IST
=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.494 DataTime=0.299 Loss=0.956 Prec@1=76.003 Prec@5=92.247 rate=2.12 Hz, eta=0:18:04, total=0:01:34, wall=09:16 IST
=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.494 DataTime=0.299 Loss=0.956 Prec@1=76.003 Prec@5=92.247 rate=2.12 Hz, eta=0:18:04, total=0:01:34, wall=09:17 IST
=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.493 DataTime=0.298 Loss=0.957 Prec@1=75.958 Prec@5=92.262 rate=2.12 Hz, eta=0:18:04, total=0:01:34, wall=09:17 IST
=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.493 DataTime=0.298 Loss=0.957 Prec@1=75.958 Prec@5=92.262 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=09:17 IST
=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.493 DataTime=0.298 Loss=0.957 Prec@1=75.958 Prec@5=92.262 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=09:17 IST
=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.489 DataTime=0.294 Loss=0.958 Prec@1=75.907 Prec@5=92.232 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=09:17 IST
=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.489 DataTime=0.294 Loss=0.958 Prec@1=75.907 Prec@5=92.232 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=09:17 IST
=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.489 DataTime=0.294 Loss=0.958 Prec@1=75.907 Prec@5=92.232 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=09:18 IST
=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.962 Prec@1=75.833 Prec@5=92.202 rate=2.09 Hz, eta=0:16:44, total=0:03:11, wall=09:18 IST
=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.962 Prec@1=75.833 Prec@5=92.202 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=09:18 IST
=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.962 Prec@1=75.833 Prec@5=92.202 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=09:19 IST
=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.293 Loss=0.962 Prec@1=75.835 Prec@5=92.181 rate=2.09 Hz, eta=0:15:56, total=0:03:59, wall=09:19 IST
=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.293 Loss=0.962 Prec@1=75.835 Prec@5=92.181 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=09:19 IST
=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.293 Loss=0.962 Prec@1=75.835 Prec@5=92.181 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=09:20 IST
=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.963 Prec@1=75.799 Prec@5=92.177 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=09:20 IST
=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.963 Prec@1=75.799 Prec@5=92.177 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:20 IST
=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.963 Prec@1=75.799 Prec@5=92.177 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:21 IST
=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.965 Prec@1=75.733 Prec@5=92.128 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:21 IST
=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.965 Prec@1=75.733 Prec@5=92.128 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=09:21 IST
=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.965 Prec@1=75.733 Prec@5=92.128 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=09:21 IST
=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.968 Prec@1=75.675 Prec@5=92.089 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=09:21 IST
=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.968 Prec@1=75.675 Prec@5=92.089 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=09:21 IST
=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.968 Prec@1=75.675 Prec@5=92.089 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=09:22 IST
=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.969 Prec@1=75.650 Prec@5=92.082 rate=2.07 Hz, eta=0:12:53, total=0:07:15, wall=09:22 IST
=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.969 Prec@1=75.650 Prec@5=92.082 rate=2.07 Hz, eta=0:12:06, total=0:08:03, wall=09:22 IST
=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.488 DataTime=0.294 Loss=0.969 Prec@1=75.650 Prec@5=92.082 rate=2.07 Hz, eta=0:12:06, total=0:08:03, wall=09:23 IST
=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.293 Loss=0.971 Prec@1=75.602 Prec@5=92.059 rate=2.07 Hz, eta=0:12:06, total=0:08:03, wall=09:23 IST
=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.293 Loss=0.971 Prec@1=75.602 Prec@5=92.059 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:23 IST
=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.293 Loss=0.971 Prec@1=75.602 Prec@5=92.059 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:24 IST
=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.971 Prec@1=75.590 Prec@5=92.060 rate=2.07 Hz, eta=0:11:17, total=0:08:51, wall=09:24 IST
=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.971 Prec@1=75.590 Prec@5=92.060 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=09:24 IST
=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.971 Prec@1=75.590 Prec@5=92.060 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=09:25 IST
=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.971 Prec@1=75.587 Prec@5=92.059 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=09:25 IST
=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.971 Prec@1=75.587 Prec@5=92.059 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=09:25 IST
=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.971 Prec@1=75.587 Prec@5=92.059 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=09:25 IST
=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.972 Prec@1=75.552 Prec@5=92.049 rate=2.07 Hz, eta=0:09:40, total=0:10:28, wall=09:25 IST
=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.972 Prec@1=75.552 Prec@5=92.049 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=09:25 IST
=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.972 Prec@1=75.552 Prec@5=92.049 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=09:26 IST
=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.974 Prec@1=75.527 Prec@5=92.031 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=09:26 IST
=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.974 Prec@1=75.527 Prec@5=92.031 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=09:26 IST
=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.974 Prec@1=75.527 Prec@5=92.031 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=09:27 IST
=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.513 Prec@5=92.017 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=09:27 IST
=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.513 Prec@5=92.017 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=09:27 IST
=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.513 Prec@5=92.017 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=09:28 IST
=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.511 Prec@5=92.006 rate=2.07 Hz, eta=0:07:16, total=0:12:54, wall=09:28 IST
=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.511 Prec@5=92.006 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=09:28 IST
=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.975 Prec@1=75.511 Prec@5=92.006 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=09:29 IST
=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.976 Prec@1=75.498 Prec@5=91.998 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=09:29 IST
=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.976 Prec@1=75.498 Prec@5=91.998 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=09:29 IST
=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.292 Loss=0.976 Prec@1=75.498 Prec@5=91.998 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=09:30 IST
=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.976 Prec@1=75.481 Prec@5=91.988 rate=2.07 Hz, eta=0:05:39, total=0:14:31, wall=09:30 IST
=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.976 Prec@1=75.481 Prec@5=91.988 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:30 IST
=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.487 DataTime=0.292 Loss=0.976 Prec@1=75.481 Prec@5=91.988 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:30 IST
=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.474 Prec@5=91.979 rate=2.06 Hz, eta=0:04:51, total=0:15:20, wall=09:30 IST
=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.474 Prec@5=91.979 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=09:30 IST
=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.474 Prec@5=91.979 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=09:31 IST
=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.458 Prec@5=91.985 rate=2.07 Hz, eta=0:04:02, total=0:16:07, wall=09:31 IST
=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.458 Prec@5=91.985 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=09:31 IST
=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.458 Prec@5=91.985 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=09:32 IST
=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.451 Prec@5=91.982 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=09:32 IST
=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.451 Prec@5=91.982 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=09:32 IST
=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.451 Prec@5=91.982 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=09:33 IST
=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.443 Prec@5=91.980 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=09:33 IST
=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.443 Prec@5=91.980 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:33 IST
=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.977 Prec@1=75.443 Prec@5=91.980 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:34 IST
=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.978 Prec@1=75.427 Prec@5=91.976 rate=2.07 Hz, eta=0:01:37, total=0:18:33, wall=09:34 IST
=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.978 Prec@1=75.427 Prec@5=91.976 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=09:34 IST
=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.978 Prec@1=75.427 Prec@5=91.976 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=09:34 IST
=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.979 Prec@1=75.407 Prec@5=91.965 rate=2.07 Hz, eta=0:00:49, total=0:19:21, wall=09:34 IST
=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.979 Prec@1=75.407 Prec@5=91.965 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:34 IST
=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.979 Prec@1=75.407 Prec@5=91.965 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:34 IST
=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.979 Prec@1=75.407 Prec@5=91.965 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:34 IST
=> training   100.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.486 DataTime=0.291 Loss=0.979 Prec@1=75.407 Prec@5=91.965 rate=2.07 Hz, eta=0:00:00, total=0:20:10, wall=09:34 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> validation 0.00% of 1x98...Epoch=122/150 LR=0.00894 Time=6.961 Loss=0.720 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=6.961 Loss=0.720 Prec@1=81.055 Prec@5=95.312 rate=8378.64 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST
** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=6.961 Loss=0.720 Prec@1=81.055 Prec@5=95.312 rate=8378.64 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST
** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=0.558 Loss=1.205 Prec@1=70.508 Prec@5=89.814 rate=8378.64 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST
** validation 100.00% of 1x98...Epoch=122/150 LR=0.00894 Time=0.558 Loss=1.205 Prec@1=70.508 Prec@5=89.814 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=09:35 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> training   0.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=4.713 DataTime=4.478 Loss=0.845 Prec@1=79.102 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST
=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=4.713 DataTime=4.478 Loss=0.845 Prec@1=79.102 Prec@5=93.945 rate=7367.84 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST
=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=4.713 DataTime=4.478 Loss=0.845 Prec@1=79.102 Prec@5=93.945 rate=7367.84 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST
=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.516 DataTime=0.322 Loss=0.957 Prec@1=75.977 Prec@5=92.238 rate=7367.84 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST
=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.516 DataTime=0.322 Loss=0.957 Prec@1=75.977 Prec@5=92.238 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=09:36 IST
=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.516 DataTime=0.322 Loss=0.957 Prec@1=75.977 Prec@5=92.238 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=09:37 IST
=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.497 DataTime=0.301 Loss=0.952 Prec@1=76.115 Prec@5=92.365 rate=2.13 Hz, eta=0:18:47, total=0:00:47, wall=09:37 IST
=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.497 DataTime=0.301 Loss=0.952 Prec@1=76.115 Prec@5=92.365 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=09:37 IST
=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.497 DataTime=0.301 Loss=0.952 Prec@1=76.115 Prec@5=92.365 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=09:38 IST
=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.490 DataTime=0.293 Loss=0.952 Prec@1=76.074 Prec@5=92.321 rate=2.11 Hz, eta=0:18:09, total=0:01:35, wall=09:38 IST
=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.490 DataTime=0.293 Loss=0.952 Prec@1=76.074 Prec@5=92.321 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=09:38 IST
=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.490 DataTime=0.293 Loss=0.952 Prec@1=76.074 Prec@5=92.321 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=09:39 IST
=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.952 Prec@1=76.008 Prec@5=92.317 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=09:39 IST
=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.952 Prec@1=76.008 Prec@5=92.317 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=09:39 IST
=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.952 Prec@1=76.008 Prec@5=92.317 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=09:39 IST
=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.953 Prec@1=75.991 Prec@5=92.309 rate=2.11 Hz, eta=0:16:37, total=0:03:10, wall=09:39 IST
=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.953 Prec@1=75.991 Prec@5=92.309 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=09:39 IST
=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.953 Prec@1=75.991 Prec@5=92.309 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=09:40 IST
=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.937 Prec@5=92.253 rate=2.10 Hz, eta=0:15:54, total=0:03:58, wall=09:40 IST
=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.937 Prec@5=92.253 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=09:40 IST
=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.937 Prec@5=92.253 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=09:41 IST
=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.958 Prec@1=75.865 Prec@5=92.225 rate=2.09 Hz, eta=0:15:08, total=0:04:47, wall=09:41 IST
=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.958 Prec@1=75.865 Prec@5=92.225 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:41 IST
=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.958 Prec@1=75.865 Prec@5=92.225 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:42 IST
=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.789 Prec@5=92.211 rate=2.08 Hz, eta=0:14:25, total=0:05:36, wall=09:42 IST
=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.789 Prec@5=92.211 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=09:42 IST
=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.789 Prec@5=92.211 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=09:43 IST
=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.292 Loss=0.960 Prec@1=75.779 Prec@5=92.209 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=09:43 IST
=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.292 Loss=0.960 Prec@1=75.779 Prec@5=92.209 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=09:43 IST
=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.292 Loss=0.960 Prec@1=75.779 Prec@5=92.209 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=09:43 IST
=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.961 Prec@1=75.801 Prec@5=92.195 rate=2.08 Hz, eta=0:12:50, total=0:07:13, wall=09:43 IST
=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.961 Prec@1=75.801 Prec@5=92.195 rate=2.07 Hz, eta=0:12:03, total=0:08:02, wall=09:43 IST
=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.487 DataTime=0.292 Loss=0.961 Prec@1=75.801 Prec@5=92.195 rate=2.07 Hz, eta=0:12:03, total=0:08:02, wall=09:44 IST
=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.291 Loss=0.960 Prec@1=75.811 Prec@5=92.215 rate=2.07 Hz, eta=0:12:03, total=0:08:02, wall=09:44 IST
=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.291 Loss=0.960 Prec@1=75.811 Prec@5=92.215 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=09:44 IST
=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.291 Loss=0.960 Prec@1=75.811 Prec@5=92.215 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=09:45 IST
=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.805 Prec@5=92.214 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=09:45 IST
=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.805 Prec@5=92.214 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=09:45 IST
=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.960 Prec@1=75.805 Prec@5=92.214 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=09:46 IST
=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.961 Prec@1=75.792 Prec@5=92.193 rate=2.08 Hz, eta=0:10:26, total=0:09:38, wall=09:46 IST
=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.961 Prec@1=75.792 Prec@5=92.193 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=09:46 IST
=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.961 Prec@1=75.792 Prec@5=92.193 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=09:47 IST
=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.962 Prec@1=75.772 Prec@5=92.182 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=09:47 IST
=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.962 Prec@1=75.772 Prec@5=92.182 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=09:47 IST
=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.962 Prec@1=75.772 Prec@5=92.182 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=09:47 IST
=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.750 Prec@5=92.166 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=09:47 IST
=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.750 Prec@5=92.166 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=09:47 IST
=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.750 Prec@5=92.166 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=09:48 IST
=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.740 Prec@5=92.172 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=09:48 IST
=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.740 Prec@5=92.172 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=09:48 IST
=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.963 Prec@1=75.740 Prec@5=92.172 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=09:49 IST
=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.964 Prec@1=75.715 Prec@5=92.155 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=09:49 IST
=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.964 Prec@1=75.715 Prec@5=92.155 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=09:49 IST
=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.486 DataTime=0.290 Loss=0.964 Prec@1=75.715 Prec@5=92.155 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=09:50 IST
=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.964 Prec@1=75.707 Prec@5=92.160 rate=2.07 Hz, eta=0:06:27, total=0:13:41, wall=09:50 IST
=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.964 Prec@1=75.707 Prec@5=92.160 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=09:50 IST
=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.964 Prec@1=75.707 Prec@5=92.160 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=09:51 IST
=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.965 Prec@1=75.694 Prec@5=92.140 rate=2.07 Hz, eta=0:05:38, total=0:14:28, wall=09:51 IST
=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.965 Prec@1=75.694 Prec@5=92.140 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=09:51 IST
=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.965 Prec@1=75.694 Prec@5=92.140 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=09:52 IST
=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.676 Prec@5=92.127 rate=2.07 Hz, eta=0:04:50, total=0:15:17, wall=09:52 IST
=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.676 Prec@5=92.127 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=09:52 IST
=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.676 Prec@5=92.127 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=09:52 IST
=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.660 Prec@5=92.124 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=09:52 IST
=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.660 Prec@5=92.124 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=09:52 IST
=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.966 Prec@1=75.660 Prec@5=92.124 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=09:53 IST
=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.484 DataTime=0.289 Loss=0.967 Prec@1=75.653 Prec@5=92.110 rate=2.07 Hz, eta=0:03:14, total=0:16:54, wall=09:53 IST
=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.484 DataTime=0.289 Loss=0.967 Prec@1=75.653 Prec@5=92.110 rate=2.07 Hz, eta=0:02:25, total=0:17:40, wall=09:53 IST
=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.484 DataTime=0.289 Loss=0.967 Prec@1=75.653 Prec@5=92.110 rate=2.07 Hz, eta=0:02:25, total=0:17:40, wall=09:54 IST
=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.289 Loss=0.968 Prec@1=75.646 Prec@5=92.099 rate=2.07 Hz, eta=0:02:25, total=0:17:40, wall=09:54 IST
=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.289 Loss=0.968 Prec@1=75.646 Prec@5=92.099 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=09:54 IST
=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.289 Loss=0.968 Prec@1=75.646 Prec@5=92.099 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=09:55 IST
=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.968 Prec@1=75.641 Prec@5=92.090 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=09:55 IST
=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.968 Prec@1=75.641 Prec@5=92.090 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=09:55 IST
=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.968 Prec@1=75.641 Prec@5=92.090 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=09:56 IST
=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.969 Prec@1=75.613 Prec@5=92.078 rate=2.07 Hz, eta=0:00:49, total=0:19:19, wall=09:56 IST
=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.969 Prec@1=75.613 Prec@5=92.078 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=09:56 IST
=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.969 Prec@1=75.613 Prec@5=92.078 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=09:56 IST
=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.969 Prec@1=75.613 Prec@5=92.078 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=09:56 IST
=> training   100.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.485 DataTime=0.290 Loss=0.969 Prec@1=75.613 Prec@5=92.078 rate=2.07 Hz, eta=0:00:00, total=0:20:08, wall=09:56 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:56 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:56 IST
=> validation 0.00% of 1x98...Epoch=123/150 LR=0.00835 Time=5.962 Loss=0.735 Prec@1=80.078 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=09:56 IST
=> validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=5.962 Loss=0.735 Prec@1=80.078 Prec@5=94.336 rate=3485.52 Hz, eta=0:00:00, total=0:00:00, wall=09:56 IST
** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=5.962 Loss=0.735 Prec@1=80.078 Prec@5=94.336 rate=3485.52 Hz, eta=0:00:00, total=0:00:00, wall=09:56 IST
** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=0.550 Loss=1.191 Prec@1=70.740 Prec@5=89.772 rate=3485.52 Hz, eta=0:00:00, total=0:00:00, wall=09:56 IST
** validation 100.00% of 1x98...Epoch=123/150 LR=0.00835 Time=0.550 Loss=1.191 Prec@1=70.740 Prec@5=89.772 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=09:56 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> training   0.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=4.879 DataTime=4.608 Loss=0.964 Prec@1=75.781 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=4.879 DataTime=4.608 Loss=0.964 Prec@1=75.781 Prec@5=92.773 rate=7995.91 Hz, eta=0:00:00, total=0:00:00, wall=09:57 IST
=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=4.879 DataTime=4.608 Loss=0.964 Prec@1=75.781 Prec@5=92.773 rate=7995.91 Hz, eta=0:00:00, total=0:00:00, wall=09:57 IST
=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.523 DataTime=0.324 Loss=0.936 Prec@1=76.501 Prec@5=92.445 rate=7995.91 Hz, eta=0:00:00, total=0:00:00, wall=09:57 IST
=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.523 DataTime=0.324 Loss=0.936 Prec@1=76.501 Prec@5=92.445 rate=2.11 Hz, eta=0:18:59, total=0:00:47, wall=09:57 IST
=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.523 DataTime=0.324 Loss=0.936 Prec@1=76.501 Prec@5=92.445 rate=2.11 Hz, eta=0:18:59, total=0:00:47, wall=09:58 IST
=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.506 DataTime=0.308 Loss=0.928 Prec@1=76.622 Prec@5=92.563 rate=2.11 Hz, eta=0:18:59, total=0:00:47, wall=09:58 IST
=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.506 DataTime=0.308 Loss=0.928 Prec@1=76.622 Prec@5=92.563 rate=2.08 Hz, eta=0:18:28, total=0:01:36, wall=09:58 IST
=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.506 DataTime=0.308 Loss=0.928 Prec@1=76.622 Prec@5=92.563 rate=2.08 Hz, eta=0:18:28, total=0:01:36, wall=09:59 IST
=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.497 DataTime=0.300 Loss=0.941 Prec@1=76.374 Prec@5=92.374 rate=2.08 Hz, eta=0:18:28, total=0:01:36, wall=09:59 IST
=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.497 DataTime=0.300 Loss=0.941 Prec@1=76.374 Prec@5=92.374 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=09:59 IST
=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.497 DataTime=0.300 Loss=0.941 Prec@1=76.374 Prec@5=92.374 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=10:00 IST
=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.493 DataTime=0.296 Loss=0.945 Prec@1=76.288 Prec@5=92.324 rate=2.08 Hz, eta=0:17:37, total=0:02:24, wall=10:00 IST
=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.493 DataTime=0.296 Loss=0.945 Prec@1=76.288 Prec@5=92.324 rate=2.08 Hz, eta=0:16:49, total=0:03:12, wall=10:00 IST
=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.493 DataTime=0.296 Loss=0.945 Prec@1=76.288 Prec@5=92.324 rate=2.08 Hz, eta=0:16:49, total=0:03:12, wall=10:01 IST
=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.489 DataTime=0.292 Loss=0.944 Prec@1=76.291 Prec@5=92.338 rate=2.08 Hz, eta=0:16:49, total=0:03:12, wall=10:01 IST
=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.489 DataTime=0.292 Loss=0.944 Prec@1=76.291 Prec@5=92.338 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:01 IST
=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.489 DataTime=0.292 Loss=0.944 Prec@1=76.291 Prec@5=92.338 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:01 IST
=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.945 Prec@1=76.248 Prec@5=92.319 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:01 IST
=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.945 Prec@1=76.248 Prec@5=92.319 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=10:01 IST
=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.945 Prec@1=76.248 Prec@5=92.319 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=10:02 IST
=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.181 Prec@5=92.281 rate=2.09 Hz, eta=0:15:11, total=0:04:48, wall=10:02 IST
=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.181 Prec@5=92.281 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=10:02 IST
=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.181 Prec@5=92.281 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=10:03 IST
=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.948 Prec@1=76.170 Prec@5=92.264 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=10:03 IST
=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.948 Prec@1=76.170 Prec@5=92.264 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=10:03 IST
=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.948 Prec@1=76.170 Prec@5=92.264 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=10:04 IST
=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.949 Prec@1=76.148 Prec@5=92.265 rate=2.08 Hz, eta=0:13:38, total=0:06:25, wall=10:04 IST
=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.949 Prec@1=76.148 Prec@5=92.265 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=10:04 IST
=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.949 Prec@1=76.148 Prec@5=92.265 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=10:05 IST
=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.950 Prec@1=76.119 Prec@5=92.246 rate=2.08 Hz, eta=0:12:51, total=0:07:14, wall=10:05 IST
=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.950 Prec@1=76.119 Prec@5=92.246 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=10:05 IST
=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.487 DataTime=0.291 Loss=0.950 Prec@1=76.119 Prec@5=92.246 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=10:05 IST
=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.059 Prec@5=92.228 rate=2.07 Hz, eta=0:12:04, total=0:08:02, wall=10:05 IST
=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.059 Prec@5=92.228 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:05 IST
=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.059 Prec@5=92.228 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:06 IST
=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.052 Prec@5=92.232 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:06 IST
=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.052 Prec@5=92.232 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=10:06 IST
=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.052 Prec@5=92.232 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=10:07 IST
=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.049 Prec@5=92.236 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=10:07 IST
=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.049 Prec@5=92.236 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=10:07 IST
=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.049 Prec@5=92.236 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=10:08 IST
=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.952 Prec@1=76.047 Prec@5=92.234 rate=2.08 Hz, eta=0:09:38, total=0:10:26, wall=10:08 IST
=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.952 Prec@1=76.047 Prec@5=92.234 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=10:08 IST
=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.952 Prec@1=76.047 Prec@5=92.234 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=10:09 IST
=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.028 Prec@5=92.231 rate=2.08 Hz, eta=0:08:50, total=0:11:14, wall=10:09 IST
=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.028 Prec@5=92.231 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=10:09 IST
=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.953 Prec@1=76.028 Prec@5=92.231 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=10:09 IST
=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=76.001 Prec@5=92.224 rate=2.07 Hz, eta=0:08:02, total=0:12:03, wall=10:09 IST
=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=76.001 Prec@5=92.224 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:09 IST
=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=76.001 Prec@5=92.224 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:10 IST
=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=75.994 Prec@5=92.223 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:10 IST
=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=75.994 Prec@5=92.223 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=10:10 IST
=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.954 Prec@1=75.994 Prec@5=92.223 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=10:11 IST
=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.966 Prec@5=92.216 rate=2.07 Hz, eta=0:06:27, total=0:13:40, wall=10:11 IST
=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.966 Prec@5=92.216 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=10:11 IST
=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.290 Loss=0.955 Prec@1=75.966 Prec@5=92.216 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=10:12 IST
=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.956 Prec@1=75.944 Prec@5=92.192 rate=2.07 Hz, eta=0:05:38, total=0:14:29, wall=10:12 IST
=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.956 Prec@1=75.944 Prec@5=92.192 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:12 IST
=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.486 DataTime=0.290 Loss=0.956 Prec@1=75.944 Prec@5=92.192 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:13 IST
=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.957 Prec@1=75.934 Prec@5=92.186 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:13 IST
=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.957 Prec@1=75.934 Prec@5=92.186 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=10:13 IST
=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.957 Prec@1=75.934 Prec@5=92.186 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=10:13 IST
=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.915 Prec@5=92.172 rate=2.07 Hz, eta=0:04:02, total=0:16:05, wall=10:13 IST
=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.915 Prec@5=92.172 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=10:13 IST
=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.915 Prec@5=92.172 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=10:14 IST
=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.913 Prec@5=92.172 rate=2.07 Hz, eta=0:03:13, total=0:16:53, wall=10:14 IST
=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.913 Prec@5=92.172 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=10:14 IST
=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.913 Prec@5=92.172 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=10:15 IST
=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.902 Prec@5=92.167 rate=2.07 Hz, eta=0:02:25, total=0:17:42, wall=10:15 IST
=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.902 Prec@5=92.167 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=10:15 IST
=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.485 DataTime=0.289 Loss=0.958 Prec@1=75.902 Prec@5=92.167 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=10:16 IST
=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.959 Prec@1=75.882 Prec@5=92.157 rate=2.07 Hz, eta=0:01:37, total=0:18:30, wall=10:16 IST
=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.959 Prec@1=75.882 Prec@5=92.157 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=10:16 IST
=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.959 Prec@1=75.882 Prec@5=92.157 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=10:17 IST
=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.960 Prec@1=75.865 Prec@5=92.149 rate=2.07 Hz, eta=0:00:49, total=0:19:18, wall=10:17 IST
=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.960 Prec@1=75.865 Prec@5=92.149 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=10:17 IST
=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.960 Prec@1=75.865 Prec@5=92.149 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=10:17 IST
=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.960 Prec@1=75.865 Prec@5=92.150 rate=2.07 Hz, eta=0:00:00, total=0:20:06, wall=10:17 IST
=> training   100.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.484 DataTime=0.289 Loss=0.960 Prec@1=75.865 Prec@5=92.150 rate=2.07 Hz, eta=0:00:00, total=0:20:07, wall=10:17 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:17 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:17 IST
=> validation 0.00% of 1x98...Epoch=124/150 LR=0.00778 Time=6.482 Loss=0.714 Prec@1=81.641 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=10:17 IST
=> validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=6.482 Loss=0.714 Prec@1=81.641 Prec@5=95.312 rate=7947.67 Hz, eta=0:00:00, total=0:00:00, wall=10:17 IST
** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=6.482 Loss=0.714 Prec@1=81.641 Prec@5=95.312 rate=7947.67 Hz, eta=0:00:00, total=0:00:00, wall=10:18 IST
** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=0.552 Loss=1.189 Prec@1=70.970 Prec@5=89.890 rate=7947.67 Hz, eta=0:00:00, total=0:00:00, wall=10:18 IST
** validation 100.00% of 1x98...Epoch=124/150 LR=0.00778 Time=0.552 Loss=1.189 Prec@1=70.970 Prec@5=89.890 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=10:18 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:18 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:18 IST
=> training   0.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=4.640 DataTime=4.372 Loss=1.044 Prec@1=75.391 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=10:18 IST
=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=4.640 DataTime=4.372 Loss=1.044 Prec@1=75.391 Prec@5=91.602 rate=6712.35 Hz, eta=0:00:00, total=0:00:00, wall=10:18 IST
=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=4.640 DataTime=4.372 Loss=1.044 Prec@1=75.391 Prec@5=91.602 rate=6712.35 Hz, eta=0:00:00, total=0:00:00, wall=10:19 IST
=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.516 DataTime=0.321 Loss=0.939 Prec@1=76.342 Prec@5=92.344 rate=6712.35 Hz, eta=0:00:00, total=0:00:00, wall=10:19 IST
=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.516 DataTime=0.321 Loss=0.939 Prec@1=76.342 Prec@5=92.344 rate=2.13 Hz, eta=0:18:48, total=0:00:47, wall=10:19 IST
=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.516 DataTime=0.321 Loss=0.939 Prec@1=76.342 Prec@5=92.344 rate=2.13 Hz, eta=0:18:48, total=0:00:47, wall=10:19 IST
=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.491 DataTime=0.295 Loss=0.941 Prec@1=76.252 Prec@5=92.355 rate=2.13 Hz, eta=0:18:48, total=0:00:47, wall=10:19 IST
=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.491 DataTime=0.295 Loss=0.941 Prec@1=76.252 Prec@5=92.355 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=10:19 IST
=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.491 DataTime=0.295 Loss=0.941 Prec@1=76.252 Prec@5=92.355 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=10:20 IST
=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.489 DataTime=0.292 Loss=0.942 Prec@1=76.253 Prec@5=92.345 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=10:20 IST
=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.489 DataTime=0.292 Loss=0.942 Prec@1=76.253 Prec@5=92.345 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=10:20 IST
=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.489 DataTime=0.292 Loss=0.942 Prec@1=76.253 Prec@5=92.345 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=10:21 IST
=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.940 Prec@1=76.233 Prec@5=92.364 rate=2.11 Hz, eta=0:17:22, total=0:02:22, wall=10:21 IST
=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.940 Prec@1=76.233 Prec@5=92.364 rate=2.11 Hz, eta=0:16:36, total=0:03:10, wall=10:21 IST
=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.940 Prec@1=76.233 Prec@5=92.364 rate=2.11 Hz, eta=0:16:36, total=0:03:10, wall=10:22 IST
=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.488 DataTime=0.293 Loss=0.941 Prec@1=76.250 Prec@5=92.368 rate=2.11 Hz, eta=0:16:36, total=0:03:10, wall=10:22 IST
=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.488 DataTime=0.293 Loss=0.941 Prec@1=76.250 Prec@5=92.368 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:22 IST
=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.488 DataTime=0.293 Loss=0.941 Prec@1=76.250 Prec@5=92.368 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:23 IST
=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.942 Prec@1=76.273 Prec@5=92.356 rate=2.09 Hz, eta=0:15:59, total=0:04:00, wall=10:23 IST
=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.942 Prec@1=76.273 Prec@5=92.356 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=10:23 IST
=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.942 Prec@1=76.273 Prec@5=92.356 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=10:23 IST
=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.484 DataTime=0.288 Loss=0.942 Prec@1=76.262 Prec@5=92.367 rate=2.09 Hz, eta=0:15:09, total=0:04:47, wall=10:23 IST
=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.484 DataTime=0.288 Loss=0.942 Prec@1=76.262 Prec@5=92.367 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=10:23 IST
=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.484 DataTime=0.288 Loss=0.942 Prec@1=76.262 Prec@5=92.367 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=10:24 IST
=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.942 Prec@1=76.290 Prec@5=92.374 rate=2.09 Hz, eta=0:14:20, total=0:05:34, wall=10:24 IST
=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.942 Prec@1=76.290 Prec@5=92.374 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=10:24 IST
=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.942 Prec@1=76.290 Prec@5=92.374 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=10:25 IST
=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.487 DataTime=0.291 Loss=0.943 Prec@1=76.249 Prec@5=92.382 rate=2.08 Hz, eta=0:13:36, total=0:06:24, wall=10:25 IST
=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.487 DataTime=0.291 Loss=0.943 Prec@1=76.249 Prec@5=92.382 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=10:25 IST
=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.487 DataTime=0.291 Loss=0.943 Prec@1=76.249 Prec@5=92.382 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=10:26 IST
=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.289 Loss=0.944 Prec@1=76.233 Prec@5=92.369 rate=2.08 Hz, eta=0:12:51, total=0:07:13, wall=10:26 IST
=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.289 Loss=0.944 Prec@1=76.233 Prec@5=92.369 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=10:26 IST
=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.289 Loss=0.944 Prec@1=76.233 Prec@5=92.369 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=10:27 IST
=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.944 Prec@1=76.227 Prec@5=92.366 rate=2.08 Hz, eta=0:12:01, total=0:08:01, wall=10:27 IST
=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.944 Prec@1=76.227 Prec@5=92.366 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:27 IST
=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.944 Prec@1=76.227 Prec@5=92.366 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:27 IST
=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.945 Prec@1=76.183 Prec@5=92.352 rate=2.08 Hz, eta=0:11:14, total=0:08:49, wall=10:27 IST
=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.945 Prec@1=76.183 Prec@5=92.352 rate=2.07 Hz, eta=0:10:27, total=0:09:38, wall=10:27 IST
=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.945 Prec@1=76.183 Prec@5=92.352 rate=2.07 Hz, eta=0:10:27, total=0:09:38, wall=10:28 IST
=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.946 Prec@1=76.177 Prec@5=92.337 rate=2.07 Hz, eta=0:10:27, total=0:09:38, wall=10:28 IST
=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.946 Prec@1=76.177 Prec@5=92.337 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=10:28 IST
=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.946 Prec@1=76.177 Prec@5=92.337 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=10:29 IST
=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.947 Prec@1=76.158 Prec@5=92.326 rate=2.08 Hz, eta=0:09:39, total=0:10:26, wall=10:29 IST
=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.947 Prec@1=76.158 Prec@5=92.326 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=10:29 IST
=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.947 Prec@1=76.158 Prec@5=92.326 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=10:30 IST
=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.132 Prec@5=92.311 rate=2.07 Hz, eta=0:08:51, total=0:11:15, wall=10:30 IST
=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.132 Prec@5=92.311 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=10:30 IST
=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.132 Prec@5=92.311 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=10:31 IST
=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.121 Prec@5=92.306 rate=2.08 Hz, eta=0:08:02, total=0:12:03, wall=10:31 IST
=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.121 Prec@5=92.306 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=10:31 IST
=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.121 Prec@5=92.306 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=10:31 IST
=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.127 Prec@5=92.319 rate=2.07 Hz, eta=0:07:15, total=0:12:52, wall=10:31 IST
=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.127 Prec@5=92.319 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=10:31 IST
=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.948 Prec@1=76.127 Prec@5=92.319 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=10:32 IST
=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.108 Prec@5=92.321 rate=2.07 Hz, eta=0:06:26, total=0:13:40, wall=10:32 IST
=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.108 Prec@5=92.321 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=10:32 IST
=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.486 DataTime=0.290 Loss=0.948 Prec@1=76.108 Prec@5=92.321 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=10:33 IST
=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.102 Prec@5=92.309 rate=2.07 Hz, eta=0:05:39, total=0:14:29, wall=10:33 IST
=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.102 Prec@5=92.309 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:33 IST
=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.102 Prec@5=92.309 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:34 IST
=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.084 Prec@5=92.307 rate=2.07 Hz, eta=0:04:50, total=0:15:18, wall=10:34 IST
=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.084 Prec@5=92.307 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=10:34 IST
=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.949 Prec@1=76.084 Prec@5=92.307 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=10:35 IST
=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.950 Prec@1=76.053 Prec@5=92.295 rate=2.07 Hz, eta=0:04:02, total=0:16:06, wall=10:35 IST
=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.950 Prec@1=76.053 Prec@5=92.295 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=10:35 IST
=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.950 Prec@1=76.053 Prec@5=92.295 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=10:35 IST
=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.951 Prec@1=76.036 Prec@5=92.287 rate=2.07 Hz, eta=0:03:14, total=0:16:55, wall=10:35 IST
=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.951 Prec@1=76.036 Prec@5=92.287 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=10:35 IST
=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.951 Prec@1=76.036 Prec@5=92.287 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=10:36 IST
=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.028 Prec@5=92.281 rate=2.07 Hz, eta=0:02:25, total=0:17:43, wall=10:36 IST
=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.028 Prec@5=92.281 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=10:36 IST
=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.028 Prec@5=92.281 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=10:37 IST
=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.032 Prec@5=92.281 rate=2.07 Hz, eta=0:01:37, total=0:18:32, wall=10:37 IST
=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.032 Prec@5=92.281 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=10:37 IST
=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.032 Prec@5=92.281 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=10:38 IST
=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.027 Prec@5=92.279 rate=2.07 Hz, eta=0:00:49, total=0:19:20, wall=10:38 IST
=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.027 Prec@5=92.279 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=10:38 IST
=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.027 Prec@5=92.279 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=10:38 IST
=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.026 Prec@5=92.280 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=10:38 IST
=> training   100.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.485 DataTime=0.290 Loss=0.952 Prec@1=76.026 Prec@5=92.280 rate=2.07 Hz, eta=0:00:00, total=0:20:09, wall=10:38 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:38 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:38 IST
=> validation 0.00% of 1x98...Epoch=125/150 LR=0.00723 Time=6.949 Loss=0.668 Prec@1=82.031 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=10:38 IST
=> validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.949 Loss=0.668 Prec@1=82.031 Prec@5=95.117 rate=8158.07 Hz, eta=0:00:00, total=0:00:00, wall=10:38 IST
** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.949 Loss=0.668 Prec@1=82.031 Prec@5=95.117 rate=8158.07 Hz, eta=0:00:00, total=0:00:00, wall=10:39 IST
** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=0.556 Loss=1.179 Prec@1=70.972 Prec@5=89.966 rate=8158.07 Hz, eta=0:00:00, total=0:00:00, wall=10:39 IST
** validation 100.00% of 1x98...Epoch=125/150 LR=0.00723 Time=0.556 Loss=1.179 Prec@1=70.972 Prec@5=89.966 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=10:39 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:39 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:39 IST
=> training   0.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=4.891 DataTime=4.603 Loss=1.002 Prec@1=77.930 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=10:39 IST
=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=4.891 DataTime=4.603 Loss=1.002 Prec@1=77.930 Prec@5=91.406 rate=8166.86 Hz, eta=0:00:00, total=0:00:00, wall=10:39 IST
=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=4.891 DataTime=4.603 Loss=1.002 Prec@1=77.930 Prec@5=91.406 rate=8166.86 Hz, eta=0:00:00, total=0:00:00, wall=10:40 IST
=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.521 DataTime=0.326 Loss=0.920 Prec@1=76.800 Prec@5=92.623 rate=8166.86 Hz, eta=0:00:00, total=0:00:00, wall=10:40 IST
=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.521 DataTime=0.326 Loss=0.920 Prec@1=76.800 Prec@5=92.623 rate=2.12 Hz, eta=0:18:55, total=0:00:47, wall=10:40 IST
=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.521 DataTime=0.326 Loss=0.920 Prec@1=76.800 Prec@5=92.623 rate=2.12 Hz, eta=0:18:55, total=0:00:47, wall=10:41 IST
=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.501 DataTime=0.304 Loss=0.925 Prec@1=76.609 Prec@5=92.590 rate=2.12 Hz, eta=0:18:55, total=0:00:47, wall=10:41 IST
=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.501 DataTime=0.304 Loss=0.925 Prec@1=76.609 Prec@5=92.590 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=10:41 IST
=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.501 DataTime=0.304 Loss=0.925 Prec@1=76.609 Prec@5=92.590 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=10:41 IST
=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.496 DataTime=0.299 Loss=0.926 Prec@1=76.632 Prec@5=92.575 rate=2.10 Hz, eta=0:18:18, total=0:01:35, wall=10:41 IST
=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.496 DataTime=0.299 Loss=0.926 Prec@1=76.632 Prec@5=92.575 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=10:41 IST
=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.496 DataTime=0.299 Loss=0.926 Prec@1=76.632 Prec@5=92.575 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=10:42 IST
=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.493 DataTime=0.297 Loss=0.928 Prec@1=76.637 Prec@5=92.589 rate=2.08 Hz, eta=0:17:36, total=0:02:24, wall=10:42 IST
=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.493 DataTime=0.297 Loss=0.928 Prec@1=76.637 Prec@5=92.589 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=10:42 IST
=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.493 DataTime=0.297 Loss=0.928 Prec@1=76.637 Prec@5=92.589 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=10:43 IST
=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.294 Loss=0.928 Prec@1=76.603 Prec@5=92.564 rate=2.08 Hz, eta=0:16:51, total=0:03:12, wall=10:43 IST
=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.294 Loss=0.928 Prec@1=76.603 Prec@5=92.564 rate=2.08 Hz, eta=0:16:03, total=0:04:00, wall=10:43 IST
=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.294 Loss=0.928 Prec@1=76.603 Prec@5=92.564 rate=2.08 Hz, eta=0:16:03, total=0:04:00, wall=10:44 IST
=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.295 Loss=0.931 Prec@1=76.525 Prec@5=92.528 rate=2.08 Hz, eta=0:16:03, total=0:04:00, wall=10:44 IST
=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.295 Loss=0.931 Prec@1=76.525 Prec@5=92.528 rate=2.07 Hz, eta=0:15:19, total=0:04:50, wall=10:44 IST
=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.491 DataTime=0.295 Loss=0.931 Prec@1=76.525 Prec@5=92.528 rate=2.07 Hz, eta=0:15:19, total=0:04:50, wall=10:45 IST
=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.293 Loss=0.932 Prec@1=76.500 Prec@5=92.528 rate=2.07 Hz, eta=0:15:19, total=0:04:50, wall=10:45 IST
=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.293 Loss=0.932 Prec@1=76.500 Prec@5=92.528 rate=2.07 Hz, eta=0:14:30, total=0:05:38, wall=10:45 IST
=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.293 Loss=0.932 Prec@1=76.500 Prec@5=92.528 rate=2.07 Hz, eta=0:14:30, total=0:05:38, wall=10:45 IST
=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.292 Loss=0.932 Prec@1=76.482 Prec@5=92.518 rate=2.07 Hz, eta=0:14:30, total=0:05:38, wall=10:45 IST
=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.292 Loss=0.932 Prec@1=76.482 Prec@5=92.518 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=10:45 IST
=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.490 DataTime=0.292 Loss=0.932 Prec@1=76.482 Prec@5=92.518 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=10:46 IST
=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.934 Prec@1=76.455 Prec@5=92.491 rate=2.07 Hz, eta=0:13:42, total=0:06:27, wall=10:46 IST
=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.934 Prec@1=76.455 Prec@5=92.491 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=10:46 IST
=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.934 Prec@1=76.455 Prec@5=92.491 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=10:47 IST
=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.291 Loss=0.935 Prec@1=76.433 Prec@5=92.469 rate=2.07 Hz, eta=0:12:54, total=0:07:15, wall=10:47 IST
=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.291 Loss=0.935 Prec@1=76.433 Prec@5=92.469 rate=2.07 Hz, eta=0:12:06, total=0:08:04, wall=10:47 IST
=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.291 Loss=0.935 Prec@1=76.433 Prec@5=92.469 rate=2.07 Hz, eta=0:12:06, total=0:08:04, wall=10:48 IST
=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.936 Prec@1=76.419 Prec@5=92.460 rate=2.07 Hz, eta=0:12:06, total=0:08:04, wall=10:48 IST
=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.936 Prec@1=76.419 Prec@5=92.460 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=10:48 IST
=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.489 DataTime=0.292 Loss=0.936 Prec@1=76.419 Prec@5=92.460 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=10:49 IST
=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.292 Loss=0.935 Prec@1=76.417 Prec@5=92.462 rate=2.06 Hz, eta=0:11:19, total=0:08:53, wall=10:49 IST
=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.292 Loss=0.935 Prec@1=76.417 Prec@5=92.462 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=10:49 IST
=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.292 Loss=0.935 Prec@1=76.417 Prec@5=92.462 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=10:49 IST
=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.291 Loss=0.936 Prec@1=76.408 Prec@5=92.454 rate=2.06 Hz, eta=0:10:30, total=0:09:41, wall=10:49 IST
=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.291 Loss=0.936 Prec@1=76.408 Prec@5=92.454 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=10:49 IST
=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.488 DataTime=0.291 Loss=0.936 Prec@1=76.408 Prec@5=92.454 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=10:50 IST
=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.486 DataTime=0.290 Loss=0.937 Prec@1=76.384 Prec@5=92.444 rate=2.06 Hz, eta=0:09:42, total=0:10:30, wall=10:50 IST
=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.486 DataTime=0.290 Loss=0.937 Prec@1=76.384 Prec@5=92.444 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=10:50 IST
=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.486 DataTime=0.290 Loss=0.937 Prec@1=76.384 Prec@5=92.444 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=10:51 IST
=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.937 Prec@1=76.383 Prec@5=92.436 rate=2.07 Hz, eta=0:08:52, total=0:11:16, wall=10:51 IST
=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.937 Prec@1=76.383 Prec@5=92.436 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=10:51 IST
=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.937 Prec@1=76.383 Prec@5=92.436 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=10:52 IST
=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.938 Prec@1=76.362 Prec@5=92.431 rate=2.07 Hz, eta=0:08:03, total=0:12:03, wall=10:52 IST
=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.938 Prec@1=76.362 Prec@5=92.431 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:52 IST
=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.485 DataTime=0.289 Loss=0.938 Prec@1=76.362 Prec@5=92.431 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:53 IST
=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.484 DataTime=0.287 Loss=0.939 Prec@1=76.338 Prec@5=92.419 rate=2.07 Hz, eta=0:07:14, total=0:12:51, wall=10:53 IST
=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.484 DataTime=0.287 Loss=0.939 Prec@1=76.338 Prec@5=92.419 rate=2.08 Hz, eta=0:06:25, total=0:13:37, wall=10:53 IST
=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.484 DataTime=0.287 Loss=0.939 Prec@1=76.338 Prec@5=92.419 rate=2.08 Hz, eta=0:06:25, total=0:13:37, wall=10:53 IST
=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.482 DataTime=0.285 Loss=0.940 Prec@1=76.313 Prec@5=92.411 rate=2.08 Hz, eta=0:06:25, total=0:13:37, wall=10:53 IST
=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.482 DataTime=0.285 Loss=0.940 Prec@1=76.313 Prec@5=92.411 rate=2.09 Hz, eta=0:05:36, total=0:14:23, wall=10:53 IST
=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.482 DataTime=0.285 Loss=0.940 Prec@1=76.313 Prec@5=92.411 rate=2.09 Hz, eta=0:05:36, total=0:14:23, wall=10:54 IST
=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.481 DataTime=0.285 Loss=0.940 Prec@1=76.299 Prec@5=92.412 rate=2.09 Hz, eta=0:05:36, total=0:14:23, wall=10:54 IST
=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.481 DataTime=0.285 Loss=0.940 Prec@1=76.299 Prec@5=92.412 rate=2.09 Hz, eta=0:04:48, total=0:15:10, wall=10:54 IST
=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.481 DataTime=0.285 Loss=0.940 Prec@1=76.299 Prec@5=92.412 rate=2.09 Hz, eta=0:04:48, total=0:15:10, wall=10:55 IST
=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.291 Prec@5=92.407 rate=2.09 Hz, eta=0:04:48, total=0:15:10, wall=10:55 IST
=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.291 Prec@5=92.407 rate=2.09 Hz, eta=0:03:59, total=0:15:56, wall=10:55 IST
=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.291 Prec@5=92.407 rate=2.09 Hz, eta=0:03:59, total=0:15:56, wall=10:56 IST
=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.272 Prec@5=92.393 rate=2.09 Hz, eta=0:03:59, total=0:15:56, wall=10:56 IST
=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.272 Prec@5=92.393 rate=2.09 Hz, eta=0:03:12, total=0:16:44, wall=10:56 IST
=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.284 Loss=0.941 Prec@1=76.272 Prec@5=92.393 rate=2.09 Hz, eta=0:03:12, total=0:16:44, wall=10:56 IST
=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.255 Prec@5=92.382 rate=2.09 Hz, eta=0:03:12, total=0:16:44, wall=10:56 IST
=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.255 Prec@5=92.382 rate=2.09 Hz, eta=0:02:24, total=0:17:31, wall=10:56 IST
=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.255 Prec@5=92.382 rate=2.09 Hz, eta=0:02:24, total=0:17:31, wall=10:57 IST
=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.269 Prec@5=92.380 rate=2.09 Hz, eta=0:02:24, total=0:17:31, wall=10:57 IST
=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.269 Prec@5=92.380 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=10:57 IST
=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.480 DataTime=0.283 Loss=0.942 Prec@1=76.269 Prec@5=92.380 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=10:58 IST
=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.479 DataTime=0.283 Loss=0.942 Prec@1=76.263 Prec@5=92.380 rate=2.09 Hz, eta=0:01:36, total=0:18:18, wall=10:58 IST
=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.479 DataTime=0.283 Loss=0.942 Prec@1=76.263 Prec@5=92.380 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=10:58 IST
=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.479 DataTime=0.283 Loss=0.942 Prec@1=76.263 Prec@5=92.380 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=10:59 IST
=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.478 DataTime=0.282 Loss=0.943 Prec@1=76.246 Prec@5=92.371 rate=2.10 Hz, eta=0:00:48, total=0:19:05, wall=10:59 IST
=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.478 DataTime=0.282 Loss=0.943 Prec@1=76.246 Prec@5=92.371 rate=2.10 Hz, eta=0:00:00, total=0:19:51, wall=10:59 IST
=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.478 DataTime=0.282 Loss=0.943 Prec@1=76.246 Prec@5=92.371 rate=2.10 Hz, eta=0:00:00, total=0:19:51, wall=10:59 IST
=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.478 DataTime=0.282 Loss=0.943 Prec@1=76.244 Prec@5=92.370 rate=2.10 Hz, eta=0:00:00, total=0:19:51, wall=10:59 IST
=> training   100.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.478 DataTime=0.282 Loss=0.943 Prec@1=76.244 Prec@5=92.370 rate=2.10 Hz, eta=0:00:00, total=0:19:51, wall=10:59 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:59 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:59 IST
=> validation 0.00% of 1x98...Epoch=126/150 LR=0.00670 Time=6.263 Loss=0.679 Prec@1=81.836 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=10:59 IST
=> validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=6.263 Loss=0.679 Prec@1=81.836 Prec@5=95.508 rate=9123.68 Hz, eta=0:00:00, total=0:00:00, wall=10:59 IST
** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=6.263 Loss=0.679 Prec@1=81.836 Prec@5=95.508 rate=9123.68 Hz, eta=0:00:00, total=0:00:00, wall=11:00 IST
** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=0.547 Loss=1.179 Prec@1=71.038 Prec@5=89.958 rate=9123.68 Hz, eta=0:00:00, total=0:00:00, wall=11:00 IST
** validation 100.00% of 1x98...Epoch=126/150 LR=0.00670 Time=0.547 Loss=1.179 Prec@1=71.038 Prec@5=89.958 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=11:00 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:00 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:00 IST
=> training   0.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.941 DataTime=4.685 Loss=0.954 Prec@1=74.414 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=11:00 IST
=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.941 DataTime=4.685 Loss=0.954 Prec@1=74.414 Prec@5=92.383 rate=8996.04 Hz, eta=0:00:00, total=0:00:00, wall=11:00 IST
=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.941 DataTime=4.685 Loss=0.954 Prec@1=74.414 Prec@5=92.383 rate=8996.04 Hz, eta=0:00:00, total=0:00:00, wall=11:01 IST
=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.498 DataTime=0.301 Loss=0.927 Prec@1=76.578 Prec@5=92.582 rate=8996.04 Hz, eta=0:00:00, total=0:00:00, wall=11:01 IST
=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.498 DataTime=0.301 Loss=0.927 Prec@1=76.578 Prec@5=92.582 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=11:01 IST
=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.498 DataTime=0.301 Loss=0.927 Prec@1=76.578 Prec@5=92.582 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=11:01 IST
=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.480 DataTime=0.283 Loss=0.922 Prec@1=76.728 Prec@5=92.598 rate=2.23 Hz, eta=0:17:58, total=0:00:45, wall=11:01 IST
=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.480 DataTime=0.283 Loss=0.922 Prec@1=76.728 Prec@5=92.598 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=11:01 IST
=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.480 DataTime=0.283 Loss=0.922 Prec@1=76.728 Prec@5=92.598 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=11:02 IST
=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.473 DataTime=0.276 Loss=0.924 Prec@1=76.735 Prec@5=92.546 rate=2.20 Hz, eta=0:17:28, total=0:01:31, wall=11:02 IST
=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.473 DataTime=0.276 Loss=0.924 Prec@1=76.735 Prec@5=92.546 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=11:02 IST
=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.473 DataTime=0.276 Loss=0.924 Prec@1=76.735 Prec@5=92.546 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=11:03 IST
=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.470 DataTime=0.273 Loss=0.921 Prec@1=76.803 Prec@5=92.597 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=11:03 IST
=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.470 DataTime=0.273 Loss=0.921 Prec@1=76.803 Prec@5=92.597 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=11:03 IST
=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.470 DataTime=0.273 Loss=0.921 Prec@1=76.803 Prec@5=92.597 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=11:04 IST
=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.921 Prec@1=76.766 Prec@5=92.586 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=11:04 IST
=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.921 Prec@1=76.766 Prec@5=92.586 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=11:04 IST
=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.921 Prec@1=76.766 Prec@5=92.586 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=11:04 IST
=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.269 Loss=0.922 Prec@1=76.756 Prec@5=92.597 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=11:04 IST
=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.269 Loss=0.922 Prec@1=76.756 Prec@5=92.597 rate=2.18 Hz, eta=0:14:31, total=0:04:35, wall=11:04 IST
=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.269 Loss=0.922 Prec@1=76.756 Prec@5=92.597 rate=2.18 Hz, eta=0:14:31, total=0:04:35, wall=11:05 IST
=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.922 Prec@1=76.776 Prec@5=92.591 rate=2.18 Hz, eta=0:14:31, total=0:04:35, wall=11:05 IST
=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.922 Prec@1=76.776 Prec@5=92.591 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=11:05 IST
=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.922 Prec@1=76.776 Prec@5=92.591 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=11:06 IST
=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.923 Prec@1=76.769 Prec@5=92.587 rate=2.18 Hz, eta=0:13:45, total=0:05:21, wall=11:06 IST
=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.923 Prec@1=76.769 Prec@5=92.587 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=11:06 IST
=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.923 Prec@1=76.769 Prec@5=92.587 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=11:07 IST
=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.924 Prec@1=76.735 Prec@5=92.564 rate=2.18 Hz, eta=0:13:00, total=0:06:07, wall=11:07 IST
=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.924 Prec@1=76.735 Prec@5=92.564 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=11:07 IST
=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.268 Loss=0.924 Prec@1=76.735 Prec@5=92.564 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=11:08 IST
=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.925 Prec@1=76.705 Prec@5=92.564 rate=2.18 Hz, eta=0:12:15, total=0:06:53, wall=11:08 IST
=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.925 Prec@1=76.705 Prec@5=92.564 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=11:08 IST
=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.925 Prec@1=76.705 Prec@5=92.564 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=11:08 IST
=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.926 Prec@1=76.699 Prec@5=92.557 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=11:08 IST
=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.926 Prec@1=76.699 Prec@5=92.557 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=11:08 IST
=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.465 DataTime=0.269 Loss=0.926 Prec@1=76.699 Prec@5=92.557 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=11:09 IST
=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.269 Loss=0.926 Prec@1=76.697 Prec@5=92.553 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=11:09 IST
=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.269 Loss=0.926 Prec@1=76.697 Prec@5=92.553 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=11:09 IST
=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.269 Loss=0.926 Prec@1=76.697 Prec@5=92.553 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=11:10 IST
=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.270 Loss=0.928 Prec@1=76.660 Prec@5=92.538 rate=2.17 Hz, eta=0:10:01, total=0:09:14, wall=11:10 IST
=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.270 Loss=0.928 Prec@1=76.660 Prec@5=92.538 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=11:10 IST
=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.466 DataTime=0.270 Loss=0.928 Prec@1=76.660 Prec@5=92.538 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=11:11 IST
=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.928 Prec@1=76.654 Prec@5=92.533 rate=2.16 Hz, eta=0:09:16, total=0:10:01, wall=11:11 IST
=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.928 Prec@1=76.654 Prec@5=92.533 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=11:11 IST
=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.928 Prec@1=76.654 Prec@5=92.533 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=11:11 IST
=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.270 Loss=0.928 Prec@1=76.646 Prec@5=92.530 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=11:11 IST
=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.270 Loss=0.928 Prec@1=76.646 Prec@5=92.530 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=11:11 IST
=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.270 Loss=0.928 Prec@1=76.646 Prec@5=92.530 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=11:12 IST
=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.930 Prec@1=76.609 Prec@5=92.516 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=11:12 IST
=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.930 Prec@1=76.609 Prec@5=92.516 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:12 IST
=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.930 Prec@1=76.609 Prec@5=92.516 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:13 IST
=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.931 Prec@1=76.605 Prec@5=92.516 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=11:13 IST
=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.931 Prec@1=76.605 Prec@5=92.516 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=11:13 IST
=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.931 Prec@1=76.605 Prec@5=92.516 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=11:14 IST
=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.604 Prec@5=92.517 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=11:14 IST
=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.604 Prec@5=92.517 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=11:14 IST
=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.604 Prec@5=92.517 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=11:15 IST
=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.589 Prec@5=92.514 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=11:15 IST
=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.589 Prec@5=92.514 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=11:15 IST
=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.931 Prec@1=76.589 Prec@5=92.514 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=11:15 IST
=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.555 Prec@5=92.497 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=11:15 IST
=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.555 Prec@5=92.497 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:15 IST
=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.555 Prec@5=92.497 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:16 IST
=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.552 Prec@5=92.498 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=11:16 IST
=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.552 Prec@5=92.498 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=11:16 IST
=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.932 Prec@1=76.552 Prec@5=92.498 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=11:17 IST
=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.933 Prec@1=76.532 Prec@5=92.492 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=11:17 IST
=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.933 Prec@1=76.532 Prec@5=92.492 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:17 IST
=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.933 Prec@1=76.532 Prec@5=92.492 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:18 IST
=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.934 Prec@1=76.500 Prec@5=92.478 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=11:18 IST
=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.934 Prec@1=76.500 Prec@5=92.478 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=11:18 IST
=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.934 Prec@1=76.500 Prec@5=92.478 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=11:18 IST
=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.935 Prec@1=76.487 Prec@5=92.467 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=11:18 IST
=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.935 Prec@1=76.487 Prec@5=92.467 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=11:18 IST
=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.468 DataTime=0.271 Loss=0.935 Prec@1=76.487 Prec@5=92.467 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=11:19 IST
=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.935 Prec@1=76.473 Prec@5=92.466 rate=2.15 Hz, eta=0:00:47, total=0:18:37, wall=11:19 IST
=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.935 Prec@1=76.473 Prec@5=92.466 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=11:19 IST
=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.935 Prec@1=76.473 Prec@5=92.466 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=11:19 IST
=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.935 Prec@1=76.472 Prec@5=92.465 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=11:19 IST
=> training   100.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.467 DataTime=0.271 Loss=0.935 Prec@1=76.472 Prec@5=92.465 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=11:19 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST
=> validation 0.00% of 1x98...Epoch=127/150 LR=0.00618 Time=6.932 Loss=0.716 Prec@1=81.641 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST
=> validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.932 Loss=0.716 Prec@1=81.641 Prec@5=95.312 rate=5576.72 Hz, eta=0:00:00, total=0:00:00, wall=11:19 IST
** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.932 Loss=0.716 Prec@1=81.641 Prec@5=95.312 rate=5576.72 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST
** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=0.561 Loss=1.177 Prec@1=71.090 Prec@5=90.072 rate=5576.72 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST
** validation 100.00% of 1x98...Epoch=127/150 LR=0.00618 Time=0.561 Loss=1.177 Prec@1=71.090 Prec@5=90.072 rate=2.04 Hz, eta=0:00:00, total=0:00:48, wall=11:20 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST
=> training   0.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=4.637 DataTime=4.253 Loss=0.825 Prec@1=77.734 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST
=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=4.637 DataTime=4.253 Loss=0.825 Prec@1=77.734 Prec@5=93.555 rate=4857.25 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST
=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=4.637 DataTime=4.253 Loss=0.825 Prec@1=77.734 Prec@5=93.555 rate=4857.25 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST
=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.506 DataTime=0.311 Loss=0.903 Prec@1=77.166 Prec@5=92.903 rate=4857.25 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST
=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.506 DataTime=0.311 Loss=0.903 Prec@1=77.166 Prec@5=92.903 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=11:21 IST
=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.506 DataTime=0.311 Loss=0.903 Prec@1=77.166 Prec@5=92.903 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=11:22 IST
=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.904 Prec@1=77.111 Prec@5=92.768 rate=2.17 Hz, eta=0:18:27, total=0:00:46, wall=11:22 IST
=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.904 Prec@1=77.111 Prec@5=92.768 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=11:22 IST
=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.904 Prec@1=77.111 Prec@5=92.768 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=11:23 IST
=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.286 Loss=0.909 Prec@1=77.020 Prec@5=92.718 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=11:23 IST
=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.286 Loss=0.909 Prec@1=77.020 Prec@5=92.718 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=11:23 IST
=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.286 Loss=0.909 Prec@1=77.020 Prec@5=92.718 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=11:23 IST
=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.287 Loss=0.912 Prec@1=76.950 Prec@5=92.697 rate=2.14 Hz, eta=0:17:08, total=0:02:20, wall=11:23 IST
=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.287 Loss=0.912 Prec@1=76.950 Prec@5=92.697 rate=2.12 Hz, eta=0:16:30, total=0:03:08, wall=11:23 IST
=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.287 Loss=0.912 Prec@1=76.950 Prec@5=92.697 rate=2.12 Hz, eta=0:16:30, total=0:03:08, wall=11:24 IST
=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.913 Prec@1=76.934 Prec@5=92.699 rate=2.12 Hz, eta=0:16:30, total=0:03:08, wall=11:24 IST
=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.913 Prec@1=76.934 Prec@5=92.699 rate=2.11 Hz, eta=0:15:47, total=0:03:56, wall=11:24 IST
=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.913 Prec@1=76.934 Prec@5=92.699 rate=2.11 Hz, eta=0:15:47, total=0:03:56, wall=11:25 IST
=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.917 Prec@1=76.890 Prec@5=92.635 rate=2.11 Hz, eta=0:15:47, total=0:03:56, wall=11:25 IST
=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.917 Prec@1=76.890 Prec@5=92.635 rate=2.11 Hz, eta=0:15:03, total=0:04:45, wall=11:25 IST
=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.917 Prec@1=76.890 Prec@5=92.635 rate=2.11 Hz, eta=0:15:03, total=0:04:45, wall=11:26 IST
=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.289 Loss=0.919 Prec@1=76.854 Prec@5=92.601 rate=2.11 Hz, eta=0:15:03, total=0:04:45, wall=11:26 IST
=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.289 Loss=0.919 Prec@1=76.854 Prec@5=92.601 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=11:26 IST
=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.289 Loss=0.919 Prec@1=76.854 Prec@5=92.601 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=11:27 IST
=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.919 Prec@1=76.859 Prec@5=92.604 rate=2.09 Hz, eta=0:14:21, total=0:05:35, wall=11:27 IST
=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.919 Prec@1=76.859 Prec@5=92.604 rate=2.09 Hz, eta=0:13:36, total=0:06:24, wall=11:27 IST
=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.485 DataTime=0.290 Loss=0.919 Prec@1=76.859 Prec@5=92.604 rate=2.09 Hz, eta=0:13:36, total=0:06:24, wall=11:27 IST
=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.484 DataTime=0.289 Loss=0.919 Prec@1=76.842 Prec@5=92.589 rate=2.09 Hz, eta=0:13:36, total=0:06:24, wall=11:27 IST
=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.484 DataTime=0.289 Loss=0.919 Prec@1=76.842 Prec@5=92.589 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=11:27 IST
=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.484 DataTime=0.289 Loss=0.919 Prec@1=76.842 Prec@5=92.589 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=11:28 IST
=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.288 Loss=0.919 Prec@1=76.840 Prec@5=92.610 rate=2.09 Hz, eta=0:12:47, total=0:07:11, wall=11:28 IST
=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.288 Loss=0.919 Prec@1=76.840 Prec@5=92.610 rate=2.09 Hz, eta=0:11:58, total=0:07:59, wall=11:28 IST
=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.483 DataTime=0.288 Loss=0.919 Prec@1=76.840 Prec@5=92.610 rate=2.09 Hz, eta=0:11:58, total=0:07:59, wall=11:29 IST
=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.920 Prec@1=76.813 Prec@5=92.603 rate=2.09 Hz, eta=0:11:58, total=0:07:59, wall=11:29 IST
=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.920 Prec@1=76.813 Prec@5=92.603 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=11:29 IST
=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.482 DataTime=0.287 Loss=0.920 Prec@1=76.813 Prec@5=92.603 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=11:30 IST
=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.481 DataTime=0.285 Loss=0.920 Prec@1=76.793 Prec@5=92.608 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=11:30 IST
=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.481 DataTime=0.285 Loss=0.920 Prec@1=76.793 Prec@5=92.608 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=11:30 IST
=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.481 DataTime=0.285 Loss=0.920 Prec@1=76.793 Prec@5=92.608 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=11:31 IST
=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.480 DataTime=0.284 Loss=0.920 Prec@1=76.782 Prec@5=92.614 rate=2.10 Hz, eta=0:10:20, total=0:09:32, wall=11:31 IST
=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.480 DataTime=0.284 Loss=0.920 Prec@1=76.782 Prec@5=92.614 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=11:31 IST
=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.480 DataTime=0.284 Loss=0.920 Prec@1=76.782 Prec@5=92.614 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=11:31 IST
=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.479 DataTime=0.284 Loss=0.921 Prec@1=76.762 Prec@5=92.610 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=11:31 IST
=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.479 DataTime=0.284 Loss=0.921 Prec@1=76.762 Prec@5=92.610 rate=2.10 Hz, eta=0:08:44, total=0:11:06, wall=11:31 IST
=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.479 DataTime=0.284 Loss=0.921 Prec@1=76.762 Prec@5=92.610 rate=2.10 Hz, eta=0:08:44, total=0:11:06, wall=11:32 IST
=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.765 Prec@5=92.601 rate=2.10 Hz, eta=0:08:44, total=0:11:06, wall=11:32 IST
=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.765 Prec@5=92.601 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=11:32 IST
=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.765 Prec@5=92.601 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=11:33 IST
=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.761 Prec@5=92.599 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=11:33 IST
=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.761 Prec@5=92.599 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=11:33 IST
=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.761 Prec@5=92.599 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=11:34 IST
=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.752 Prec@5=92.597 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=11:34 IST
=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.752 Prec@5=92.597 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=11:34 IST
=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.478 DataTime=0.283 Loss=0.921 Prec@1=76.752 Prec@5=92.597 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=11:35 IST
=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.282 Loss=0.922 Prec@1=76.744 Prec@5=92.593 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=11:35 IST
=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.282 Loss=0.922 Prec@1=76.744 Prec@5=92.593 rate=2.11 Hz, eta=0:05:33, total=0:14:14, wall=11:35 IST
=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.282 Loss=0.922 Prec@1=76.744 Prec@5=92.593 rate=2.11 Hz, eta=0:05:33, total=0:14:14, wall=11:35 IST
=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.281 Loss=0.922 Prec@1=76.735 Prec@5=92.586 rate=2.11 Hz, eta=0:05:33, total=0:14:14, wall=11:35 IST
=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.281 Loss=0.922 Prec@1=76.735 Prec@5=92.586 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=11:35 IST
=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.477 DataTime=0.281 Loss=0.922 Prec@1=76.735 Prec@5=92.586 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=11:36 IST
=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.710 Prec@5=92.588 rate=2.11 Hz, eta=0:04:45, total=0:15:01, wall=11:36 IST
=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.710 Prec@5=92.588 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=11:36 IST
=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.710 Prec@5=92.588 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=11:37 IST
=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.694 Prec@5=92.578 rate=2.11 Hz, eta=0:03:57, total=0:15:47, wall=11:37 IST
=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.694 Prec@5=92.578 rate=2.11 Hz, eta=0:03:10, total=0:16:35, wall=11:37 IST
=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.476 DataTime=0.281 Loss=0.923 Prec@1=76.694 Prec@5=92.578 rate=2.11 Hz, eta=0:03:10, total=0:16:35, wall=11:38 IST
=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.281 Loss=0.924 Prec@1=76.685 Prec@5=92.571 rate=2.11 Hz, eta=0:03:10, total=0:16:35, wall=11:38 IST
=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.281 Loss=0.924 Prec@1=76.685 Prec@5=92.571 rate=2.11 Hz, eta=0:02:22, total=0:17:22, wall=11:38 IST
=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.281 Loss=0.924 Prec@1=76.685 Prec@5=92.571 rate=2.11 Hz, eta=0:02:22, total=0:17:22, wall=11:38 IST
=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.924 Prec@1=76.684 Prec@5=92.576 rate=2.11 Hz, eta=0:02:22, total=0:17:22, wall=11:38 IST
=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.924 Prec@1=76.684 Prec@5=92.576 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=11:38 IST
=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.924 Prec@1=76.684 Prec@5=92.576 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=11:39 IST
=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.684 Prec@5=92.569 rate=2.11 Hz, eta=0:01:35, total=0:18:08, wall=11:39 IST
=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.684 Prec@5=92.569 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=11:39 IST
=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.684 Prec@5=92.569 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=11:40 IST
=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.670 Prec@5=92.564 rate=2.11 Hz, eta=0:00:48, total=0:18:56, wall=11:40 IST
=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.670 Prec@5=92.564 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=11:40 IST
=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.670 Prec@5=92.564 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=11:40 IST
=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.670 Prec@5=92.564 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=11:40 IST
=> training   100.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.475 DataTime=0.280 Loss=0.925 Prec@1=76.670 Prec@5=92.564 rate=2.11 Hz, eta=0:00:00, total=0:19:43, wall=11:40 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> validation 0.00% of 1x98...Epoch=128/150 LR=0.00569 Time=6.857 Loss=0.701 Prec@1=81.250 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.857 Loss=0.701 Prec@1=81.250 Prec@5=95.312 rate=8378.79 Hz, eta=0:00:00, total=0:00:00, wall=11:40 IST
** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.857 Loss=0.701 Prec@1=81.250 Prec@5=95.312 rate=8378.79 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=0.556 Loss=1.171 Prec@1=71.270 Prec@5=90.080 rate=8378.79 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
** validation 100.00% of 1x98...Epoch=128/150 LR=0.00569 Time=0.556 Loss=1.171 Prec@1=71.270 Prec@5=90.080 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=11:41 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> training   0.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=4.653 DataTime=4.418 Loss=0.875 Prec@1=78.516 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=4.653 DataTime=4.418 Loss=0.875 Prec@1=78.516 Prec@5=93.359 rate=4482.20 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=4.653 DataTime=4.418 Loss=0.875 Prec@1=78.516 Prec@5=93.359 rate=4482.20 Hz, eta=0:00:00, total=0:00:00, wall=11:42 IST
=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.515 DataTime=0.323 Loss=0.911 Prec@1=77.058 Prec@5=92.640 rate=4482.20 Hz, eta=0:00:00, total=0:00:00, wall=11:42 IST
=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.515 DataTime=0.323 Loss=0.911 Prec@1=77.058 Prec@5=92.640 rate=2.13 Hz, eta=0:18:45, total=0:00:47, wall=11:42 IST
=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.515 DataTime=0.323 Loss=0.911 Prec@1=77.058 Prec@5=92.640 rate=2.13 Hz, eta=0:18:45, total=0:00:47, wall=11:43 IST
=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.501 DataTime=0.308 Loss=0.906 Prec@1=77.157 Prec@5=92.782 rate=2.13 Hz, eta=0:18:45, total=0:00:47, wall=11:43 IST
=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.501 DataTime=0.308 Loss=0.906 Prec@1=77.157 Prec@5=92.782 rate=2.09 Hz, eta=0:18:19, total=0:01:35, wall=11:43 IST
=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.501 DataTime=0.308 Loss=0.906 Prec@1=77.157 Prec@5=92.782 rate=2.09 Hz, eta=0:18:19, total=0:01:35, wall=11:43 IST
=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.494 DataTime=0.301 Loss=0.905 Prec@1=77.171 Prec@5=92.802 rate=2.09 Hz, eta=0:18:19, total=0:01:35, wall=11:43 IST
=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.494 DataTime=0.301 Loss=0.905 Prec@1=77.171 Prec@5=92.802 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=11:43 IST
=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.494 DataTime=0.301 Loss=0.905 Prec@1=77.171 Prec@5=92.802 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=11:44 IST
=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.489 DataTime=0.295 Loss=0.908 Prec@1=77.127 Prec@5=92.754 rate=2.09 Hz, eta=0:17:32, total=0:02:23, wall=11:44 IST
=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.489 DataTime=0.295 Loss=0.908 Prec@1=77.127 Prec@5=92.754 rate=2.09 Hz, eta=0:16:43, total=0:03:11, wall=11:44 IST
=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.489 DataTime=0.295 Loss=0.908 Prec@1=77.127 Prec@5=92.754 rate=2.09 Hz, eta=0:16:43, total=0:03:11, wall=11:45 IST
=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.910 Prec@1=77.065 Prec@5=92.751 rate=2.09 Hz, eta=0:16:43, total=0:03:11, wall=11:45 IST
=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.910 Prec@1=77.065 Prec@5=92.751 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=11:45 IST
=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.910 Prec@1=77.065 Prec@5=92.751 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=11:46 IST
=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.909 Prec@1=77.054 Prec@5=92.761 rate=2.09 Hz, eta=0:15:58, total=0:03:59, wall=11:46 IST
=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.909 Prec@1=77.054 Prec@5=92.761 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=11:46 IST
=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.488 DataTime=0.294 Loss=0.909 Prec@1=77.054 Prec@5=92.761 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=11:47 IST
=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.910 Prec@1=77.039 Prec@5=92.768 rate=2.08 Hz, eta=0:15:13, total=0:04:48, wall=11:47 IST
=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.910 Prec@1=77.039 Prec@5=92.768 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=11:47 IST
=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.910 Prec@1=77.039 Prec@5=92.768 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=11:47 IST
=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.910 Prec@1=77.041 Prec@5=92.766 rate=2.09 Hz, eta=0:14:23, total=0:05:35, wall=11:47 IST
=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.910 Prec@1=77.041 Prec@5=92.766 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=11:47 IST
=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.910 Prec@1=77.041 Prec@5=92.766 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=11:48 IST
=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.909 Prec@1=77.044 Prec@5=92.766 rate=2.09 Hz, eta=0:13:35, total=0:06:23, wall=11:48 IST
=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.909 Prec@1=77.044 Prec@5=92.766 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=11:48 IST
=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.485 DataTime=0.291 Loss=0.909 Prec@1=77.044 Prec@5=92.766 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=11:49 IST
=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.908 Prec@1=77.064 Prec@5=92.771 rate=2.08 Hz, eta=0:12:49, total=0:07:12, wall=11:49 IST
=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.908 Prec@1=77.064 Prec@5=92.771 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=11:49 IST
=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.908 Prec@1=77.064 Prec@5=92.771 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=11:50 IST
=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.291 Loss=0.909 Prec@1=77.053 Prec@5=92.763 rate=2.08 Hz, eta=0:12:02, total=0:08:01, wall=11:50 IST
=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.291 Loss=0.909 Prec@1=77.053 Prec@5=92.763 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=11:50 IST
=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.291 Loss=0.909 Prec@1=77.053 Prec@5=92.763 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=11:51 IST
=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.293 Loss=0.911 Prec@1=77.024 Prec@5=92.734 rate=2.08 Hz, eta=0:11:15, total=0:08:50, wall=11:51 IST
=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.293 Loss=0.911 Prec@1=77.024 Prec@5=92.734 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=11:51 IST
=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.293 Loss=0.911 Prec@1=77.024 Prec@5=92.734 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=11:52 IST
=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.912 Prec@1=77.009 Prec@5=92.735 rate=2.07 Hz, eta=0:10:28, total=0:09:39, wall=11:52 IST
=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.912 Prec@1=77.009 Prec@5=92.735 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=11:52 IST
=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.912 Prec@1=77.009 Prec@5=92.735 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=11:52 IST
=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.911 Prec@1=77.002 Prec@5=92.736 rate=2.07 Hz, eta=0:09:40, total=0:10:27, wall=11:52 IST
=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.911 Prec@1=77.002 Prec@5=92.736 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=11:52 IST
=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.911 Prec@1=77.002 Prec@5=92.736 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=11:53 IST
=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.912 Prec@1=76.991 Prec@5=92.719 rate=2.07 Hz, eta=0:08:52, total=0:11:17, wall=11:53 IST
=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.912 Prec@1=76.991 Prec@5=92.719 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=11:53 IST
=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.487 DataTime=0.292 Loss=0.912 Prec@1=76.991 Prec@5=92.719 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=11:54 IST
=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.913 Prec@1=76.981 Prec@5=92.713 rate=2.07 Hz, eta=0:08:04, total=0:12:05, wall=11:54 IST
=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.913 Prec@1=76.981 Prec@5=92.713 rate=2.07 Hz, eta=0:07:16, total=0:12:53, wall=11:54 IST
=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.913 Prec@1=76.981 Prec@5=92.713 rate=2.07 Hz, eta=0:07:16, total=0:12:53, wall=11:55 IST
=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.960 Prec@5=92.704 rate=2.07 Hz, eta=0:07:16, total=0:12:53, wall=11:55 IST
=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.960 Prec@5=92.704 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=11:55 IST
=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.960 Prec@5=92.704 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=11:56 IST
=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.956 Prec@5=92.698 rate=2.07 Hz, eta=0:06:27, total=0:13:42, wall=11:56 IST
=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.956 Prec@5=92.698 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=11:56 IST
=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.914 Prec@1=76.956 Prec@5=92.698 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=11:56 IST
=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.937 Prec@5=92.694 rate=2.07 Hz, eta=0:05:39, total=0:14:30, wall=11:56 IST
=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.937 Prec@5=92.694 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=11:56 IST
=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.937 Prec@5=92.694 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=11:57 IST
=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.926 Prec@5=92.694 rate=2.07 Hz, eta=0:04:51, total=0:15:19, wall=11:57 IST
=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.926 Prec@5=92.694 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=11:57 IST
=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.926 Prec@5=92.694 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=11:58 IST
=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.914 Prec@5=92.682 rate=2.07 Hz, eta=0:04:02, total=0:16:08, wall=11:58 IST
=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.914 Prec@5=92.682 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=11:58 IST
=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.915 Prec@1=76.914 Prec@5=92.682 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=11:59 IST
=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.916 Prec@1=76.893 Prec@5=92.670 rate=2.07 Hz, eta=0:03:14, total=0:16:56, wall=11:59 IST
=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.916 Prec@1=76.893 Prec@5=92.670 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=11:59 IST
=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.916 Prec@1=76.893 Prec@5=92.670 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=12:00 IST
=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.883 Prec@5=92.664 rate=2.07 Hz, eta=0:02:26, total=0:17:44, wall=12:00 IST
=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.883 Prec@5=92.664 rate=2.06 Hz, eta=0:01:37, total=0:18:34, wall=12:00 IST
=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.883 Prec@5=92.664 rate=2.06 Hz, eta=0:01:37, total=0:18:34, wall=12:00 IST
=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.865 Prec@5=92.657 rate=2.06 Hz, eta=0:01:37, total=0:18:34, wall=12:00 IST
=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.865 Prec@5=92.657 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=12:00 IST
=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.865 Prec@5=92.657 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=12:01 IST
=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.864 Prec@5=92.662 rate=2.06 Hz, eta=0:00:49, total=0:19:23, wall=12:01 IST
=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.864 Prec@5=92.662 rate=2.06 Hz, eta=0:00:00, total=0:20:12, wall=12:01 IST
=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.864 Prec@5=92.662 rate=2.06 Hz, eta=0:00:00, total=0:20:12, wall=12:01 IST
=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.863 Prec@5=92.662 rate=2.06 Hz, eta=0:00:00, total=0:20:12, wall=12:01 IST
=> training   100.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.486 DataTime=0.292 Loss=0.917 Prec@1=76.863 Prec@5=92.662 rate=2.06 Hz, eta=0:00:00, total=0:20:12, wall=12:01 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> validation 0.00% of 1x98...Epoch=129/150 LR=0.00521 Time=7.039 Loss=0.656 Prec@1=81.055 Prec@5=96.289 rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=7.039 Loss=0.656 Prec@1=81.055 Prec@5=96.289 rate=8470.49 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST
** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=7.039 Loss=0.656 Prec@1=81.055 Prec@5=96.289 rate=8470.49 Hz, eta=0:00:00, total=0:00:00, wall=12:02 IST
** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=0.564 Loss=1.168 Prec@1=71.224 Prec@5=90.122 rate=8470.49 Hz, eta=0:00:00, total=0:00:00, wall=12:02 IST
** validation 100.00% of 1x98...Epoch=129/150 LR=0.00521 Time=0.564 Loss=1.168 Prec@1=71.224 Prec@5=90.122 rate=2.03 Hz, eta=0:00:00, total=0:00:48, wall=12:02 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:02 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:02 IST
=> training   0.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.012 DataTime=4.761 Loss=0.940 Prec@1=76.562 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=12:02 IST
=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.012 DataTime=4.761 Loss=0.940 Prec@1=76.562 Prec@5=92.383 rate=7473.11 Hz, eta=0:00:00, total=0:00:00, wall=12:02 IST
=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.012 DataTime=4.761 Loss=0.940 Prec@1=76.562 Prec@5=92.383 rate=7473.11 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST
=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.516 DataTime=0.319 Loss=0.903 Prec@1=77.340 Prec@5=92.880 rate=7473.11 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST
=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.516 DataTime=0.319 Loss=0.903 Prec@1=77.340 Prec@5=92.880 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=12:03 IST
=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.516 DataTime=0.319 Loss=0.903 Prec@1=77.340 Prec@5=92.880 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=12:04 IST
=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.493 DataTime=0.296 Loss=0.903 Prec@1=77.271 Prec@5=92.926 rate=2.14 Hz, eta=0:18:40, total=0:00:47, wall=12:04 IST
=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.493 DataTime=0.296 Loss=0.903 Prec@1=77.271 Prec@5=92.926 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=12:04 IST
=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.493 DataTime=0.296 Loss=0.903 Prec@1=77.271 Prec@5=92.926 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=12:05 IST
=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.489 DataTime=0.293 Loss=0.899 Prec@1=77.393 Prec@5=92.958 rate=2.14 Hz, eta=0:17:57, total=0:01:34, wall=12:05 IST
=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.489 DataTime=0.293 Loss=0.899 Prec@1=77.393 Prec@5=92.958 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=12:05 IST
=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.489 DataTime=0.293 Loss=0.899 Prec@1=77.393 Prec@5=92.958 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=12:05 IST
=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.484 DataTime=0.289 Loss=0.897 Prec@1=77.438 Prec@5=92.975 rate=2.12 Hz, eta=0:17:20, total=0:02:22, wall=12:05 IST
=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.484 DataTime=0.289 Loss=0.897 Prec@1=77.438 Prec@5=92.975 rate=2.12 Hz, eta=0:16:31, total=0:03:09, wall=12:05 IST
=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.484 DataTime=0.289 Loss=0.897 Prec@1=77.438 Prec@5=92.975 rate=2.12 Hz, eta=0:16:31, total=0:03:09, wall=12:06 IST
=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.286 Loss=0.897 Prec@1=77.440 Prec@5=92.948 rate=2.12 Hz, eta=0:16:31, total=0:03:09, wall=12:06 IST
=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.286 Loss=0.897 Prec@1=77.440 Prec@5=92.948 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=12:06 IST
=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.286 Loss=0.897 Prec@1=77.440 Prec@5=92.948 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=12:07 IST
=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.896 Prec@1=77.444 Prec@5=92.949 rate=2.12 Hz, eta=0:15:43, total=0:03:56, wall=12:07 IST
=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.896 Prec@1=77.444 Prec@5=92.949 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=12:07 IST
=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.896 Prec@1=77.444 Prec@5=92.949 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=12:08 IST
=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.898 Prec@1=77.382 Prec@5=92.920 rate=2.12 Hz, eta=0:14:57, total=0:04:43, wall=12:08 IST
=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.898 Prec@1=77.382 Prec@5=92.920 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=12:08 IST
=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.898 Prec@1=77.382 Prec@5=92.920 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=12:09 IST
=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.899 Prec@1=77.361 Prec@5=92.897 rate=2.12 Hz, eta=0:14:11, total=0:05:31, wall=12:09 IST
=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.899 Prec@1=77.361 Prec@5=92.897 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=12:09 IST
=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.285 Loss=0.899 Prec@1=77.361 Prec@5=92.897 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=12:09 IST
=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.287 Loss=0.900 Prec@1=77.349 Prec@5=92.886 rate=2.11 Hz, eta=0:13:26, total=0:06:19, wall=12:09 IST
=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.287 Loss=0.900 Prec@1=77.349 Prec@5=92.886 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=12:09 IST
=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.481 DataTime=0.287 Loss=0.900 Prec@1=77.349 Prec@5=92.886 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=12:10 IST
=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.900 Prec@1=77.330 Prec@5=92.886 rate=2.10 Hz, eta=0:12:41, total=0:07:08, wall=12:10 IST
=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.900 Prec@1=77.330 Prec@5=92.886 rate=2.10 Hz, eta=0:11:53, total=0:07:55, wall=12:10 IST
=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.900 Prec@1=77.330 Prec@5=92.886 rate=2.10 Hz, eta=0:11:53, total=0:07:55, wall=12:11 IST
=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.901 Prec@1=77.290 Prec@5=92.877 rate=2.10 Hz, eta=0:11:53, total=0:07:55, wall=12:11 IST
=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.901 Prec@1=77.290 Prec@5=92.877 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=12:11 IST
=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.901 Prec@1=77.290 Prec@5=92.877 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=12:12 IST
=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.266 Prec@5=92.852 rate=2.10 Hz, eta=0:11:07, total=0:08:43, wall=12:12 IST
=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.266 Prec@5=92.852 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=12:12 IST
=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.266 Prec@5=92.852 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=12:13 IST
=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.252 Prec@5=92.853 rate=2.10 Hz, eta=0:10:20, total=0:09:31, wall=12:13 IST
=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.252 Prec@5=92.853 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=12:13 IST
=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.480 DataTime=0.286 Loss=0.903 Prec@1=77.252 Prec@5=92.853 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=12:13 IST
=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.905 Prec@1=77.209 Prec@5=92.822 rate=2.10 Hz, eta=0:09:32, total=0:10:19, wall=12:13 IST
=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.905 Prec@1=77.209 Prec@5=92.822 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=12:13 IST
=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.905 Prec@1=77.209 Prec@5=92.822 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=12:14 IST
=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.906 Prec@1=77.181 Prec@5=92.802 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=12:14 IST
=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.906 Prec@1=77.181 Prec@5=92.802 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=12:14 IST
=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.479 DataTime=0.285 Loss=0.906 Prec@1=77.181 Prec@5=92.802 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=12:15 IST
=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.478 DataTime=0.284 Loss=0.906 Prec@1=77.158 Prec@5=92.800 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=12:15 IST
=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.478 DataTime=0.284 Loss=0.906 Prec@1=77.158 Prec@5=92.800 rate=2.10 Hz, eta=0:07:08, total=0:12:40, wall=12:15 IST
=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.478 DataTime=0.284 Loss=0.906 Prec@1=77.158 Prec@5=92.800 rate=2.10 Hz, eta=0:07:08, total=0:12:40, wall=12:16 IST
=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.477 DataTime=0.283 Loss=0.907 Prec@1=77.152 Prec@5=92.796 rate=2.10 Hz, eta=0:07:08, total=0:12:40, wall=12:16 IST
=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.477 DataTime=0.283 Loss=0.907 Prec@1=77.152 Prec@5=92.796 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=12:16 IST
=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.477 DataTime=0.283 Loss=0.907 Prec@1=77.152 Prec@5=92.796 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=12:17 IST
=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.907 Prec@1=77.136 Prec@5=92.784 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=12:17 IST
=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.907 Prec@1=77.136 Prec@5=92.784 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=12:17 IST
=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.907 Prec@1=77.136 Prec@5=92.784 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=12:17 IST
=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.908 Prec@1=77.117 Prec@5=92.778 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=12:17 IST
=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.908 Prec@1=77.117 Prec@5=92.778 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=12:17 IST
=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.476 DataTime=0.282 Loss=0.908 Prec@1=77.117 Prec@5=92.778 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=12:18 IST
=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.908 Prec@1=77.100 Prec@5=92.771 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=12:18 IST
=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.908 Prec@1=77.100 Prec@5=92.771 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=12:18 IST
=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.908 Prec@1=77.100 Prec@5=92.771 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=12:19 IST
=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.909 Prec@1=77.074 Prec@5=92.757 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=12:19 IST
=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.909 Prec@1=77.074 Prec@5=92.757 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=12:19 IST
=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.281 Loss=0.909 Prec@1=77.074 Prec@5=92.757 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=12:20 IST
=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.280 Loss=0.910 Prec@1=77.070 Prec@5=92.754 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=12:20 IST
=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.280 Loss=0.910 Prec@1=77.070 Prec@5=92.754 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=12:20 IST
=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.475 DataTime=0.280 Loss=0.910 Prec@1=77.070 Prec@5=92.754 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=12:20 IST
=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.067 Prec@5=92.756 rate=2.12 Hz, eta=0:02:22, total=0:17:19, wall=12:20 IST
=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.067 Prec@5=92.756 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=12:20 IST
=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.067 Prec@5=92.756 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=12:21 IST
=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.048 Prec@5=92.752 rate=2.12 Hz, eta=0:01:35, total=0:18:04, wall=12:21 IST
=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.048 Prec@5=92.752 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=12:21 IST
=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.474 DataTime=0.279 Loss=0.910 Prec@1=77.048 Prec@5=92.752 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=12:22 IST
=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.473 DataTime=0.279 Loss=0.910 Prec@1=77.039 Prec@5=92.750 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=12:22 IST
=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.473 DataTime=0.279 Loss=0.910 Prec@1=77.039 Prec@5=92.750 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:22 IST
=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.473 DataTime=0.279 Loss=0.910 Prec@1=77.039 Prec@5=92.750 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:22 IST
=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.473 DataTime=0.279 Loss=0.911 Prec@1=77.038 Prec@5=92.750 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:22 IST
=> training   100.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.473 DataTime=0.279 Loss=0.911 Prec@1=77.038 Prec@5=92.750 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:22 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:22 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:22 IST
=> validation 0.00% of 1x98...Epoch=130/150 LR=0.00476 Time=7.039 Loss=0.658 Prec@1=82.422 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=12:22 IST
=> validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=7.039 Loss=0.658 Prec@1=82.422 Prec@5=95.898 rate=6925.69 Hz, eta=0:00:00, total=0:00:00, wall=12:22 IST
** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=7.039 Loss=0.658 Prec@1=82.422 Prec@5=95.898 rate=6925.69 Hz, eta=0:00:00, total=0:00:00, wall=12:23 IST
** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=0.548 Loss=1.164 Prec@1=71.358 Prec@5=90.134 rate=6925.69 Hz, eta=0:00:00, total=0:00:00, wall=12:23 IST
** validation 100.00% of 1x98...Epoch=130/150 LR=0.00476 Time=0.548 Loss=1.164 Prec@1=71.358 Prec@5=90.134 rate=2.10 Hz, eta=0:00:00, total=0:00:46, wall=12:23 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:23 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:23 IST
=> training   0.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=4.565 DataTime=4.325 Loss=0.987 Prec@1=76.953 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=12:23 IST
=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=4.565 DataTime=4.325 Loss=0.987 Prec@1=76.953 Prec@5=91.406 rate=6637.11 Hz, eta=0:00:00, total=0:00:00, wall=12:23 IST
=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=4.565 DataTime=4.325 Loss=0.987 Prec@1=76.953 Prec@5=91.406 rate=6637.11 Hz, eta=0:00:00, total=0:00:00, wall=12:24 IST
=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.496 DataTime=0.304 Loss=0.900 Prec@1=77.400 Prec@5=92.949 rate=6637.11 Hz, eta=0:00:00, total=0:00:00, wall=12:24 IST
=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.496 DataTime=0.304 Loss=0.900 Prec@1=77.400 Prec@5=92.949 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=12:24 IST
=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.496 DataTime=0.304 Loss=0.900 Prec@1=77.400 Prec@5=92.949 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=12:25 IST
=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.481 DataTime=0.288 Loss=0.897 Prec@1=77.533 Prec@5=92.942 rate=2.22 Hz, eta=0:18:02, total=0:00:45, wall=12:25 IST
=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.481 DataTime=0.288 Loss=0.897 Prec@1=77.533 Prec@5=92.942 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=12:25 IST
=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.481 DataTime=0.288 Loss=0.897 Prec@1=77.533 Prec@5=92.942 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=12:25 IST
=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.477 DataTime=0.283 Loss=0.891 Prec@1=77.540 Prec@5=92.992 rate=2.18 Hz, eta=0:17:36, total=0:01:32, wall=12:25 IST
=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.477 DataTime=0.283 Loss=0.891 Prec@1=77.540 Prec@5=92.992 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=12:25 IST
=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.477 DataTime=0.283 Loss=0.891 Prec@1=77.540 Prec@5=92.992 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=12:26 IST
=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.280 Loss=0.893 Prec@1=77.490 Prec@5=92.993 rate=2.16 Hz, eta=0:16:57, total=0:02:19, wall=12:26 IST
=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.280 Loss=0.893 Prec@1=77.490 Prec@5=92.993 rate=2.16 Hz, eta=0:16:10, total=0:03:05, wall=12:26 IST
=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.280 Loss=0.893 Prec@1=77.490 Prec@5=92.993 rate=2.16 Hz, eta=0:16:10, total=0:03:05, wall=12:27 IST
=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.278 Loss=0.892 Prec@1=77.501 Prec@5=92.990 rate=2.16 Hz, eta=0:16:10, total=0:03:05, wall=12:27 IST
=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.278 Loss=0.892 Prec@1=77.501 Prec@5=92.990 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=12:27 IST
=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.278 Loss=0.892 Prec@1=77.501 Prec@5=92.990 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=12:28 IST
=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.893 Prec@1=77.494 Prec@5=92.965 rate=2.16 Hz, eta=0:15:25, total=0:03:51, wall=12:28 IST
=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.893 Prec@1=77.494 Prec@5=92.965 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=12:28 IST
=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.893 Prec@1=77.494 Prec@5=92.965 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=12:28 IST
=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.895 Prec@1=77.451 Prec@5=92.936 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=12:28 IST
=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.895 Prec@1=77.451 Prec@5=92.936 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=12:28 IST
=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.895 Prec@1=77.451 Prec@5=92.936 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=12:29 IST
=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.895 Prec@1=77.448 Prec@5=92.922 rate=2.15 Hz, eta=0:13:56, total=0:05:25, wall=12:29 IST
=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.895 Prec@1=77.448 Prec@5=92.922 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=12:29 IST
=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.895 Prec@1=77.448 Prec@5=92.922 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=12:30 IST
=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.426 Prec@5=92.923 rate=2.15 Hz, eta=0:13:10, total=0:06:11, wall=12:30 IST
=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.426 Prec@5=92.923 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:30 IST
=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.426 Prec@5=92.923 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:31 IST
=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.423 Prec@5=92.941 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=12:31 IST
=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.423 Prec@5=92.941 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:31 IST
=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.423 Prec@5=92.941 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:32 IST
=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.402 Prec@5=92.949 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=12:32 IST
=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.402 Prec@5=92.949 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=12:32 IST
=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.895 Prec@1=77.402 Prec@5=92.949 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=12:32 IST
=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.388 Prec@5=92.945 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=12:32 IST
=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.388 Prec@5=92.945 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=12:32 IST
=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.388 Prec@5=92.945 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=12:33 IST
=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.381 Prec@5=92.924 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=12:33 IST
=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.381 Prec@5=92.924 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:33 IST
=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.381 Prec@5=92.924 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:34 IST
=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.369 Prec@5=92.917 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:34 IST
=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.369 Prec@5=92.917 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:34 IST
=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.469 DataTime=0.275 Loss=0.896 Prec@1=77.369 Prec@5=92.917 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:35 IST
=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.382 Prec@5=92.919 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:35 IST
=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.382 Prec@5=92.919 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=12:35 IST
=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.470 DataTime=0.276 Loss=0.896 Prec@1=77.382 Prec@5=92.919 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=12:35 IST
=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.897 Prec@1=77.363 Prec@5=92.908 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=12:35 IST
=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.897 Prec@1=77.363 Prec@5=92.908 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=12:35 IST
=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.471 DataTime=0.277 Loss=0.897 Prec@1=77.363 Prec@5=92.908 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=12:36 IST
=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.898 Prec@1=77.331 Prec@5=92.899 rate=2.14 Hz, eta=0:07:02, total=0:12:29, wall=12:36 IST
=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.898 Prec@1=77.331 Prec@5=92.899 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:36 IST
=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.898 Prec@1=77.331 Prec@5=92.899 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:37 IST
=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.313 Prec@5=92.892 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=12:37 IST
=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.313 Prec@5=92.892 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=12:37 IST
=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.313 Prec@5=92.892 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=12:38 IST
=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=12:38 IST
=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.13 Hz, eta=0:04:42, total=0:14:53, wall=12:38 IST
=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.13 Hz, eta=0:04:42, total=0:14:53, wall=12:39 IST
=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.13 Hz, eta=0:04:42, total=0:14:53, wall=12:39 IST
=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=12:39 IST
=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.899 Prec@1=77.307 Prec@5=92.885 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=12:39 IST
=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.474 DataTime=0.280 Loss=0.899 Prec@1=77.314 Prec@5=92.885 rate=2.12 Hz, eta=0:03:56, total=0:15:42, wall=12:39 IST
=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.474 DataTime=0.280 Loss=0.899 Prec@1=77.314 Prec@5=92.885 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=12:39 IST
=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.474 DataTime=0.280 Loss=0.899 Prec@1=77.314 Prec@5=92.885 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=12:40 IST
=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.311 Prec@5=92.879 rate=2.12 Hz, eta=0:03:09, total=0:16:30, wall=12:40 IST
=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.311 Prec@5=92.879 rate=2.12 Hz, eta=0:02:22, total=0:17:16, wall=12:40 IST
=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.311 Prec@5=92.879 rate=2.12 Hz, eta=0:02:22, total=0:17:16, wall=12:41 IST
=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.900 Prec@1=77.311 Prec@5=92.875 rate=2.12 Hz, eta=0:02:22, total=0:17:16, wall=12:41 IST
=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.900 Prec@1=77.311 Prec@5=92.875 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=12:41 IST
=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.472 DataTime=0.278 Loss=0.900 Prec@1=77.311 Prec@5=92.875 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=12:42 IST
=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.292 Prec@5=92.873 rate=2.13 Hz, eta=0:01:35, total=0:18:02, wall=12:42 IST
=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.292 Prec@5=92.873 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=12:42 IST
=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.292 Prec@5=92.873 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=12:43 IST
=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.286 Prec@5=92.863 rate=2.12 Hz, eta=0:00:48, total=0:18:51, wall=12:43 IST
=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.286 Prec@5=92.863 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:43 IST
=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.286 Prec@5=92.863 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:43 IST
=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.285 Prec@5=92.862 rate=2.12 Hz, eta=0:00:00, total=0:19:38, wall=12:43 IST
=> training   100.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.473 DataTime=0.279 Loss=0.900 Prec@1=77.285 Prec@5=92.862 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=12:43 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:43 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:43 IST
=> validation 0.00% of 1x98...Epoch=131/150 LR=0.00432 Time=6.859 Loss=0.680 Prec@1=82.031 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=12:43 IST
=> validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=6.859 Loss=0.680 Prec@1=82.031 Prec@5=95.117 rate=7236.10 Hz, eta=0:00:00, total=0:00:00, wall=12:43 IST
** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=6.859 Loss=0.680 Prec@1=82.031 Prec@5=95.117 rate=7236.10 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=0.554 Loss=1.162 Prec@1=71.376 Prec@5=90.110 rate=7236.10 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
** validation 100.00% of 1x98...Epoch=131/150 LR=0.00432 Time=0.554 Loss=1.162 Prec@1=71.376 Prec@5=90.110 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=12:44 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> training   0.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=5.106 DataTime=4.900 Loss=0.828 Prec@1=79.297 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=12:44 IST
=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=5.106 DataTime=4.900 Loss=0.828 Prec@1=79.297 Prec@5=93.555 rate=8118.07 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=5.106 DataTime=4.900 Loss=0.828 Prec@1=79.297 Prec@5=93.555 rate=8118.07 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.503 DataTime=0.309 Loss=0.882 Prec@1=77.742 Prec@5=93.023 rate=8118.07 Hz, eta=0:00:00, total=0:00:00, wall=12:44 IST
=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.503 DataTime=0.309 Loss=0.882 Prec@1=77.742 Prec@5=93.023 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=12:44 IST
=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.503 DataTime=0.309 Loss=0.882 Prec@1=77.742 Prec@5=93.023 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=12:45 IST
=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.484 DataTime=0.288 Loss=0.886 Prec@1=77.753 Prec@5=93.009 rate=2.21 Hz, eta=0:18:07, total=0:00:45, wall=12:45 IST
=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.484 DataTime=0.288 Loss=0.886 Prec@1=77.753 Prec@5=93.009 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=12:45 IST
=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.484 DataTime=0.288 Loss=0.886 Prec@1=77.753 Prec@5=93.009 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=12:46 IST
=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.474 DataTime=0.280 Loss=0.884 Prec@1=77.736 Prec@5=93.003 rate=2.18 Hz, eta=0:17:35, total=0:01:32, wall=12:46 IST
=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.474 DataTime=0.280 Loss=0.884 Prec@1=77.736 Prec@5=93.003 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=12:46 IST
=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.474 DataTime=0.280 Loss=0.884 Prec@1=77.736 Prec@5=93.003 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=12:47 IST
=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.882 Prec@1=77.785 Prec@5=93.013 rate=2.19 Hz, eta=0:16:46, total=0:02:17, wall=12:47 IST
=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.882 Prec@1=77.785 Prec@5=93.013 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=12:47 IST
=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.882 Prec@1=77.785 Prec@5=93.013 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=12:48 IST
=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.885 Prec@1=77.721 Prec@5=93.032 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=12:48 IST
=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.885 Prec@1=77.721 Prec@5=93.032 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:48 IST
=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.885 Prec@1=77.721 Prec@5=93.032 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:48 IST
=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.886 Prec@1=77.703 Prec@5=93.020 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=12:48 IST
=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.886 Prec@1=77.703 Prec@5=93.020 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=12:48 IST
=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.886 Prec@1=77.703 Prec@5=93.020 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=12:49 IST
=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.886 Prec@1=77.698 Prec@5=93.008 rate=2.17 Hz, eta=0:14:35, total=0:04:36, wall=12:49 IST
=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.886 Prec@1=77.698 Prec@5=93.008 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=12:49 IST
=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.886 Prec@1=77.698 Prec@5=93.008 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=12:50 IST
=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.887 Prec@1=77.662 Prec@5=93.000 rate=2.16 Hz, eta=0:13:52, total=0:05:23, wall=12:50 IST
=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.887 Prec@1=77.662 Prec@5=93.000 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=12:50 IST
=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.887 Prec@1=77.662 Prec@5=93.000 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=12:51 IST
=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.888 Prec@1=77.631 Prec@5=92.988 rate=2.16 Hz, eta=0:13:07, total=0:06:10, wall=12:51 IST
=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.888 Prec@1=77.631 Prec@5=92.988 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=12:51 IST
=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.888 Prec@1=77.631 Prec@5=92.988 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=12:51 IST
=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.889 Prec@1=77.635 Prec@5=92.975 rate=2.16 Hz, eta=0:12:22, total=0:06:57, wall=12:51 IST
=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.889 Prec@1=77.635 Prec@5=92.975 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:51 IST
=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.889 Prec@1=77.635 Prec@5=92.975 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:52 IST
=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.889 Prec@1=77.624 Prec@5=92.968 rate=2.16 Hz, eta=0:11:36, total=0:07:44, wall=12:52 IST
=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.889 Prec@1=77.624 Prec@5=92.968 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:52 IST
=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.889 Prec@1=77.624 Prec@5=92.968 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:53 IST
=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.604 Prec@5=92.967 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=12:53 IST
=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.604 Prec@5=92.967 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:53 IST
=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.604 Prec@5=92.967 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:54 IST
=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.595 Prec@5=92.962 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=12:54 IST
=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.595 Prec@5=92.962 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:54 IST
=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.595 Prec@5=92.962 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:55 IST
=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.585 Prec@5=92.971 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=12:55 IST
=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.585 Prec@5=92.971 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:55 IST
=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.890 Prec@1=77.585 Prec@5=92.971 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:55 IST
=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.891 Prec@1=77.568 Prec@5=92.960 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=12:55 IST
=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.891 Prec@1=77.568 Prec@5=92.960 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=12:55 IST
=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.891 Prec@1=77.568 Prec@5=92.960 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=12:56 IST
=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.890 Prec@1=77.573 Prec@5=92.962 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=12:56 IST
=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.890 Prec@1=77.573 Prec@5=92.962 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=12:56 IST
=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.890 Prec@1=77.573 Prec@5=92.962 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=12:57 IST
=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.891 Prec@1=77.538 Prec@5=92.960 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=12:57 IST
=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.891 Prec@1=77.538 Prec@5=92.960 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=12:57 IST
=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.891 Prec@1=77.538 Prec@5=92.960 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=12:58 IST
=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.892 Prec@1=77.524 Prec@5=92.959 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=12:58 IST
=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.892 Prec@1=77.524 Prec@5=92.959 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=12:58 IST
=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.468 DataTime=0.274 Loss=0.892 Prec@1=77.524 Prec@5=92.959 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=12:58 IST
=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.892 Prec@1=77.519 Prec@5=92.956 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=12:58 IST
=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.892 Prec@1=77.519 Prec@5=92.956 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=12:58 IST
=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.274 Loss=0.892 Prec@1=77.519 Prec@5=92.956 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=12:59 IST
=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.507 Prec@5=92.957 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=12:59 IST
=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.507 Prec@5=92.957 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=12:59 IST
=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.507 Prec@5=92.957 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=13:00 IST
=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.509 Prec@5=92.964 rate=2.14 Hz, eta=0:03:54, total=0:15:33, wall=13:00 IST
=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.509 Prec@5=92.964 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=13:00 IST
=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.892 Prec@1=77.509 Prec@5=92.964 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=13:01 IST
=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.893 Prec@1=77.492 Prec@5=92.955 rate=2.14 Hz, eta=0:03:07, total=0:16:20, wall=13:01 IST
=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.893 Prec@1=77.492 Prec@5=92.955 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=13:01 IST
=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.469 DataTime=0.275 Loss=0.893 Prec@1=77.492 Prec@5=92.955 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=13:02 IST
=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.470 DataTime=0.276 Loss=0.893 Prec@1=77.485 Prec@5=92.956 rate=2.14 Hz, eta=0:02:21, total=0:17:07, wall=13:02 IST
=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.470 DataTime=0.276 Loss=0.893 Prec@1=77.485 Prec@5=92.956 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=13:02 IST
=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.470 DataTime=0.276 Loss=0.893 Prec@1=77.485 Prec@5=92.956 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=13:02 IST
=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.276 Loss=0.893 Prec@1=77.477 Prec@5=92.952 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=13:02 IST
=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.276 Loss=0.893 Prec@1=77.477 Prec@5=92.952 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=13:02 IST
=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.276 Loss=0.893 Prec@1=77.477 Prec@5=92.952 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=13:03 IST
=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.894 Prec@1=77.470 Prec@5=92.949 rate=2.14 Hz, eta=0:00:47, total=0:18:44, wall=13:03 IST
=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.894 Prec@1=77.470 Prec@5=92.949 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=13:03 IST
=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.471 DataTime=0.277 Loss=0.894 Prec@1=77.470 Prec@5=92.949 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=13:03 IST
=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.470 DataTime=0.277 Loss=0.894 Prec@1=77.471 Prec@5=92.949 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=13:03 IST
=> training   100.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.470 DataTime=0.277 Loss=0.894 Prec@1=77.471 Prec@5=92.949 rate=2.14 Hz, eta=0:00:00, total=0:19:32, wall=13:03 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST
=> validation 0.00% of 1x98...Epoch=132/150 LR=0.00391 Time=7.107 Loss=0.670 Prec@1=82.812 Prec@5=96.094 rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST
=> validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=7.107 Loss=0.670 Prec@1=82.812 Prec@5=96.094 rate=8080.09 Hz, eta=0:00:00, total=0:00:00, wall=13:03 IST
** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=7.107 Loss=0.670 Prec@1=82.812 Prec@5=96.094 rate=8080.09 Hz, eta=0:00:00, total=0:00:00, wall=13:04 IST
** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=0.555 Loss=1.164 Prec@1=71.380 Prec@5=90.236 rate=8080.09 Hz, eta=0:00:00, total=0:00:00, wall=13:04 IST
** validation 100.00% of 1x98...Epoch=132/150 LR=0.00391 Time=0.555 Loss=1.164 Prec@1=71.380 Prec@5=90.236 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=13:04 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:04 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:04 IST
=> training   0.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=5.079 DataTime=4.846 Loss=0.932 Prec@1=75.195 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=13:04 IST
=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=5.079 DataTime=4.846 Loss=0.932 Prec@1=75.195 Prec@5=92.578 rate=8802.89 Hz, eta=0:00:00, total=0:00:00, wall=13:04 IST
=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=5.079 DataTime=4.846 Loss=0.932 Prec@1=75.195 Prec@5=92.578 rate=8802.89 Hz, eta=0:00:00, total=0:00:00, wall=13:05 IST
=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.513 DataTime=0.318 Loss=0.888 Prec@1=77.493 Prec@5=93.104 rate=8802.89 Hz, eta=0:00:00, total=0:00:00, wall=13:05 IST
=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.513 DataTime=0.318 Loss=0.888 Prec@1=77.493 Prec@5=93.104 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=13:05 IST
=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.513 DataTime=0.318 Loss=0.888 Prec@1=77.493 Prec@5=93.104 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=13:06 IST
=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.492 DataTime=0.298 Loss=0.884 Prec@1=77.593 Prec@5=93.119 rate=2.16 Hz, eta=0:18:32, total=0:00:46, wall=13:06 IST
=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.492 DataTime=0.298 Loss=0.884 Prec@1=77.593 Prec@5=93.119 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:06 IST
=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.492 DataTime=0.298 Loss=0.884 Prec@1=77.593 Prec@5=93.119 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:07 IST
=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.486 DataTime=0.291 Loss=0.885 Prec@1=77.627 Prec@5=93.053 rate=2.14 Hz, eta=0:17:53, total=0:01:33, wall=13:07 IST
=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.486 DataTime=0.291 Loss=0.885 Prec@1=77.627 Prec@5=93.053 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=13:07 IST
=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.486 DataTime=0.291 Loss=0.885 Prec@1=77.627 Prec@5=93.053 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=13:07 IST
=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.484 DataTime=0.289 Loss=0.884 Prec@1=77.642 Prec@5=93.036 rate=2.13 Hz, eta=0:17:12, total=0:02:21, wall=13:07 IST
=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.484 DataTime=0.289 Loss=0.884 Prec@1=77.642 Prec@5=93.036 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=13:07 IST
=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.484 DataTime=0.289 Loss=0.884 Prec@1=77.642 Prec@5=93.036 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=13:08 IST
=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.483 DataTime=0.288 Loss=0.883 Prec@1=77.690 Prec@5=93.041 rate=2.12 Hz, eta=0:16:29, total=0:03:08, wall=13:08 IST
=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.483 DataTime=0.288 Loss=0.883 Prec@1=77.690 Prec@5=93.041 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=13:08 IST
=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.483 DataTime=0.288 Loss=0.883 Prec@1=77.690 Prec@5=93.041 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=13:09 IST
=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.482 DataTime=0.287 Loss=0.885 Prec@1=77.650 Prec@5=93.015 rate=2.11 Hz, eta=0:15:47, total=0:03:57, wall=13:09 IST
=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.482 DataTime=0.287 Loss=0.885 Prec@1=77.650 Prec@5=93.015 rate=2.11 Hz, eta=0:15:01, total=0:04:44, wall=13:09 IST
=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.482 DataTime=0.287 Loss=0.885 Prec@1=77.650 Prec@5=93.015 rate=2.11 Hz, eta=0:15:01, total=0:04:44, wall=13:10 IST
=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.481 DataTime=0.287 Loss=0.884 Prec@1=77.664 Prec@5=93.015 rate=2.11 Hz, eta=0:15:01, total=0:04:44, wall=13:10 IST
=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.481 DataTime=0.287 Loss=0.884 Prec@1=77.664 Prec@5=93.015 rate=2.11 Hz, eta=0:14:14, total=0:05:32, wall=13:10 IST
=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.481 DataTime=0.287 Loss=0.884 Prec@1=77.664 Prec@5=93.015 rate=2.11 Hz, eta=0:14:14, total=0:05:32, wall=13:11 IST
=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.693 Prec@5=93.037 rate=2.11 Hz, eta=0:14:14, total=0:05:32, wall=13:11 IST
=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.693 Prec@5=93.037 rate=2.11 Hz, eta=0:13:25, total=0:06:19, wall=13:11 IST
=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.693 Prec@5=93.037 rate=2.11 Hz, eta=0:13:25, total=0:06:19, wall=13:11 IST
=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.018 rate=2.11 Hz, eta=0:13:25, total=0:06:19, wall=13:11 IST
=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.018 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=13:11 IST
=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.018 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=13:12 IST
=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.11 Hz, eta=0:12:39, total=0:07:07, wall=13:12 IST
=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=13:12 IST
=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=13:13 IST
=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.644 Prec@5=93.017 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=13:13 IST
=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.644 Prec@5=93.017 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=13:13 IST
=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.884 Prec@1=77.644 Prec@5=93.017 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=13:14 IST
=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.632 Prec@5=93.008 rate=2.11 Hz, eta=0:11:05, total=0:08:42, wall=13:14 IST
=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.632 Prec@5=93.008 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:14 IST
=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.632 Prec@5=93.008 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:15 IST
=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.031 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:15 IST
=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.031 rate=2.10 Hz, eta=0:09:31, total=0:10:18, wall=13:15 IST
=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.657 Prec@5=93.031 rate=2.10 Hz, eta=0:09:31, total=0:10:18, wall=13:15 IST
=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.10 Hz, eta=0:09:31, total=0:10:18, wall=13:15 IST
=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=13:15 IST
=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.640 Prec@5=93.019 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=13:16 IST
=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.669 Prec@5=93.028 rate=2.10 Hz, eta=0:08:43, total=0:11:06, wall=13:16 IST
=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.669 Prec@5=93.028 rate=2.10 Hz, eta=0:07:57, total=0:11:54, wall=13:16 IST
=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.669 Prec@5=93.028 rate=2.10 Hz, eta=0:07:57, total=0:11:54, wall=13:17 IST
=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.662 Prec@5=93.025 rate=2.10 Hz, eta=0:07:57, total=0:11:54, wall=13:17 IST
=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.662 Prec@5=93.025 rate=2.10 Hz, eta=0:07:09, total=0:12:42, wall=13:17 IST
=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.480 DataTime=0.285 Loss=0.883 Prec@1=77.662 Prec@5=93.025 rate=2.10 Hz, eta=0:07:09, total=0:12:42, wall=13:18 IST
=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.658 Prec@5=93.022 rate=2.10 Hz, eta=0:07:09, total=0:12:42, wall=13:18 IST
=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.658 Prec@5=93.022 rate=2.10 Hz, eta=0:06:21, total=0:13:30, wall=13:18 IST
=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.658 Prec@5=93.022 rate=2.10 Hz, eta=0:06:21, total=0:13:30, wall=13:19 IST
=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.664 Prec@5=93.024 rate=2.10 Hz, eta=0:06:21, total=0:13:30, wall=13:19 IST
=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.664 Prec@5=93.024 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=13:19 IST
=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.884 Prec@1=77.664 Prec@5=93.024 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=13:19 IST
=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.656 Prec@5=93.019 rate=2.10 Hz, eta=0:05:34, total=0:14:18, wall=13:19 IST
=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.656 Prec@5=93.019 rate=2.10 Hz, eta=0:04:46, total=0:15:05, wall=13:19 IST
=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.656 Prec@5=93.019 rate=2.10 Hz, eta=0:04:46, total=0:15:05, wall=13:20 IST
=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.650 Prec@5=93.019 rate=2.10 Hz, eta=0:04:46, total=0:15:05, wall=13:20 IST
=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.650 Prec@5=93.019 rate=2.10 Hz, eta=0:03:59, total=0:15:52, wall=13:20 IST
=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.650 Prec@5=93.019 rate=2.10 Hz, eta=0:03:59, total=0:15:52, wall=13:21 IST
=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:03:59, total=0:15:52, wall=13:21 IST
=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=13:21 IST
=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.285 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=13:22 IST
=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.638 Prec@5=93.010 rate=2.10 Hz, eta=0:03:11, total=0:16:41, wall=13:22 IST
=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.638 Prec@5=93.010 rate=2.10 Hz, eta=0:02:23, total=0:17:27, wall=13:22 IST
=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.638 Prec@5=93.010 rate=2.10 Hz, eta=0:02:23, total=0:17:27, wall=13:23 IST
=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:02:23, total=0:17:27, wall=13:23 IST
=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:23 IST
=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.479 DataTime=0.284 Loss=0.885 Prec@1=77.644 Prec@5=93.015 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:23 IST
=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.634 Prec@5=93.009 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:23 IST
=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.634 Prec@5=93.009 rate=2.10 Hz, eta=0:00:48, total=0:19:02, wall=13:23 IST
=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.284 Loss=0.885 Prec@1=77.634 Prec@5=93.009 rate=2.10 Hz, eta=0:00:48, total=0:19:02, wall=13:24 IST
=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.283 Loss=0.886 Prec@1=77.632 Prec@5=93.004 rate=2.10 Hz, eta=0:00:48, total=0:19:02, wall=13:24 IST
=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.283 Loss=0.886 Prec@1=77.632 Prec@5=93.004 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=13:24 IST
=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.283 Loss=0.886 Prec@1=77.632 Prec@5=93.004 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=13:24 IST
=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.283 Loss=0.886 Prec@1=77.631 Prec@5=93.003 rate=2.10 Hz, eta=0:00:00, total=0:19:49, wall=13:24 IST
=> training   100.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.478 DataTime=0.283 Loss=0.886 Prec@1=77.631 Prec@5=93.003 rate=2.10 Hz, eta=0:00:00, total=0:19:50, wall=13:24 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:24 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:24 IST
=> validation 0.00% of 1x98...Epoch=133/150 LR=0.00351 Time=6.951 Loss=0.669 Prec@1=82.422 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=13:24 IST
=> validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=6.951 Loss=0.669 Prec@1=82.422 Prec@5=94.922 rate=8588.19 Hz, eta=0:00:00, total=0:00:00, wall=13:24 IST
** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=6.951 Loss=0.669 Prec@1=82.422 Prec@5=94.922 rate=8588.19 Hz, eta=0:00:00, total=0:00:00, wall=13:25 IST
** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=0.552 Loss=1.154 Prec@1=71.596 Prec@5=90.272 rate=8588.19 Hz, eta=0:00:00, total=0:00:00, wall=13:25 IST
** validation 100.00% of 1x98...Epoch=133/150 LR=0.00351 Time=0.552 Loss=1.154 Prec@1=71.596 Prec@5=90.272 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=13:25 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:25 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:25 IST
=> training   0.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=4.969 DataTime=4.730 Loss=0.878 Prec@1=78.516 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=13:25 IST
=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=4.969 DataTime=4.730 Loss=0.878 Prec@1=78.516 Prec@5=93.555 rate=8497.99 Hz, eta=0:00:00, total=0:00:00, wall=13:25 IST
=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=4.969 DataTime=4.730 Loss=0.878 Prec@1=78.516 Prec@5=93.555 rate=8497.99 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.508 DataTime=0.314 Loss=0.862 Prec@1=78.191 Prec@5=93.220 rate=8497.99 Hz, eta=0:00:00, total=0:00:00, wall=13:26 IST
=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.508 DataTime=0.314 Loss=0.862 Prec@1=78.191 Prec@5=93.220 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=13:26 IST
=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.508 DataTime=0.314 Loss=0.862 Prec@1=78.191 Prec@5=93.220 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=13:27 IST
=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.492 DataTime=0.298 Loss=0.869 Prec@1=78.046 Prec@5=93.241 rate=2.18 Hz, eta=0:18:22, total=0:00:46, wall=13:27 IST
=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.492 DataTime=0.298 Loss=0.869 Prec@1=78.046 Prec@5=93.241 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=13:27 IST
=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.492 DataTime=0.298 Loss=0.869 Prec@1=78.046 Prec@5=93.241 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=13:27 IST
=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.489 DataTime=0.294 Loss=0.873 Prec@1=77.966 Prec@5=93.176 rate=2.14 Hz, eta=0:17:56, total=0:01:34, wall=13:27 IST
=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.489 DataTime=0.294 Loss=0.873 Prec@1=77.966 Prec@5=93.176 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=13:27 IST
=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.489 DataTime=0.294 Loss=0.873 Prec@1=77.966 Prec@5=93.176 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=13:28 IST
=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.481 DataTime=0.285 Loss=0.872 Prec@1=77.954 Prec@5=93.214 rate=2.12 Hz, eta=0:17:19, total=0:02:22, wall=13:28 IST
=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.481 DataTime=0.285 Loss=0.872 Prec@1=77.954 Prec@5=93.214 rate=2.13 Hz, eta=0:16:24, total=0:03:07, wall=13:28 IST
=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.481 DataTime=0.285 Loss=0.872 Prec@1=77.954 Prec@5=93.214 rate=2.13 Hz, eta=0:16:24, total=0:03:07, wall=13:29 IST
=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.281 Loss=0.874 Prec@1=77.906 Prec@5=93.195 rate=2.13 Hz, eta=0:16:24, total=0:03:07, wall=13:29 IST
=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.281 Loss=0.874 Prec@1=77.906 Prec@5=93.195 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=13:29 IST
=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.281 Loss=0.874 Prec@1=77.906 Prec@5=93.195 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=13:30 IST
=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.872 Prec@1=77.953 Prec@5=93.216 rate=2.14 Hz, eta=0:15:36, total=0:03:54, wall=13:30 IST
=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.872 Prec@1=77.953 Prec@5=93.216 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=13:30 IST
=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.872 Prec@1=77.953 Prec@5=93.216 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=13:31 IST
=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.873 Prec@1=77.906 Prec@5=93.224 rate=2.13 Hz, eta=0:14:53, total=0:04:42, wall=13:31 IST
=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.873 Prec@1=77.906 Prec@5=93.224 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=13:31 IST
=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.282 Loss=0.873 Prec@1=77.906 Prec@5=93.224 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=13:31 IST
=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.873 Prec@1=77.900 Prec@5=93.213 rate=2.12 Hz, eta=0:14:08, total=0:05:29, wall=13:31 IST
=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.873 Prec@1=77.900 Prec@5=93.213 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=13:31 IST
=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.873 Prec@1=77.900 Prec@5=93.213 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=13:32 IST
=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.283 Loss=0.875 Prec@1=77.879 Prec@5=93.177 rate=2.12 Hz, eta=0:13:21, total=0:06:17, wall=13:32 IST
=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.283 Loss=0.875 Prec@1=77.879 Prec@5=93.177 rate=2.12 Hz, eta=0:12:35, total=0:07:05, wall=13:32 IST
=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.283 Loss=0.875 Prec@1=77.879 Prec@5=93.177 rate=2.12 Hz, eta=0:12:35, total=0:07:05, wall=13:33 IST
=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.859 Prec@5=93.160 rate=2.12 Hz, eta=0:12:35, total=0:07:05, wall=13:33 IST
=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.859 Prec@5=93.160 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=13:33 IST
=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.859 Prec@5=93.160 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=13:34 IST
=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.870 Prec@5=93.156 rate=2.11 Hz, eta=0:11:50, total=0:07:53, wall=13:34 IST
=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.870 Prec@5=93.156 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:34 IST
=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.870 Prec@5=93.156 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:35 IST
=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.876 Prec@1=77.872 Prec@5=93.147 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:35 IST
=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.876 Prec@1=77.872 Prec@5=93.147 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=13:35 IST
=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.477 DataTime=0.282 Loss=0.876 Prec@1=77.872 Prec@5=93.147 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=13:35 IST
=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.882 Prec@5=93.143 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=13:35 IST
=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.882 Prec@5=93.143 rate=2.11 Hz, eta=0:09:30, total=0:10:16, wall=13:35 IST
=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.882 Prec@5=93.143 rate=2.11 Hz, eta=0:09:30, total=0:10:16, wall=13:36 IST
=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.869 Prec@5=93.141 rate=2.11 Hz, eta=0:09:30, total=0:10:16, wall=13:36 IST
=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.869 Prec@5=93.141 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:36 IST
=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.876 Prec@1=77.869 Prec@5=93.141 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:37 IST
=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.877 Prec@1=77.871 Prec@5=93.133 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:37 IST
=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.877 Prec@1=77.871 Prec@5=93.133 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=13:37 IST
=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.283 Loss=0.877 Prec@1=77.871 Prec@5=93.133 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=13:38 IST
=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.122 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=13:38 IST
=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.122 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=13:38 IST
=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.122 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=13:39 IST
=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.126 rate=2.10 Hz, eta=0:07:08, total=0:12:41, wall=13:39 IST
=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.126 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=13:39 IST
=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.877 Prec@1=77.868 Prec@5=93.126 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=13:39 IST
=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.860 Prec@5=93.114 rate=2.10 Hz, eta=0:06:21, total=0:13:28, wall=13:39 IST
=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.860 Prec@5=93.114 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=13:39 IST
=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.877 Prec@1=77.860 Prec@5=93.114 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=13:40 IST
=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.851 Prec@5=93.102 rate=2.10 Hz, eta=0:05:34, total=0:14:17, wall=13:40 IST
=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.851 Prec@5=93.102 rate=2.10 Hz, eta=0:04:46, total=0:15:04, wall=13:40 IST
=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.851 Prec@5=93.102 rate=2.10 Hz, eta=0:04:46, total=0:15:04, wall=13:41 IST
=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.878 Prec@1=77.847 Prec@5=93.102 rate=2.10 Hz, eta=0:04:46, total=0:15:04, wall=13:41 IST
=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.878 Prec@1=77.847 Prec@5=93.102 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=13:41 IST
=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.878 Prec@1=77.847 Prec@5=93.102 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=13:42 IST
=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.839 Prec@5=93.106 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=13:42 IST
=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.839 Prec@5=93.106 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=13:42 IST
=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.839 Prec@5=93.106 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=13:43 IST
=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.827 Prec@5=93.103 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=13:43 IST
=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.827 Prec@5=93.103 rate=2.10 Hz, eta=0:02:23, total=0:17:28, wall=13:43 IST
=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.284 Loss=0.878 Prec@1=77.827 Prec@5=93.103 rate=2.10 Hz, eta=0:02:23, total=0:17:28, wall=13:43 IST
=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.879 Prec@1=77.809 Prec@5=93.089 rate=2.10 Hz, eta=0:02:23, total=0:17:28, wall=13:43 IST
=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.879 Prec@1=77.809 Prec@5=93.089 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:43 IST
=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.879 Prec@1=77.809 Prec@5=93.089 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:44 IST
=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.879 Prec@1=77.806 Prec@5=93.086 rate=2.10 Hz, eta=0:01:36, total=0:18:16, wall=13:44 IST
=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.879 Prec@1=77.806 Prec@5=93.086 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=13:44 IST
=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.478 DataTime=0.284 Loss=0.879 Prec@1=77.806 Prec@5=93.086 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=13:45 IST
=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.880 Prec@1=77.789 Prec@5=93.072 rate=2.10 Hz, eta=0:00:48, total=0:19:03, wall=13:45 IST
=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.880 Prec@1=77.789 Prec@5=93.072 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=13:45 IST
=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.880 Prec@1=77.789 Prec@5=93.072 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=13:45 IST
=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.880 Prec@1=77.789 Prec@5=93.072 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=13:45 IST
=> training   100.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.479 DataTime=0.285 Loss=0.880 Prec@1=77.789 Prec@5=93.072 rate=2.10 Hz, eta=0:00:00, total=0:19:52, wall=13:45 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:45 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:45 IST
=> validation 0.00% of 1x98...Epoch=134/150 LR=0.00314 Time=6.978 Loss=0.659 Prec@1=83.398 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=13:45 IST
=> validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.978 Loss=0.659 Prec@1=83.398 Prec@5=95.312 rate=8561.49 Hz, eta=0:00:00, total=0:00:00, wall=13:45 IST
** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.978 Loss=0.659 Prec@1=83.398 Prec@5=95.312 rate=8561.49 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST
** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=0.557 Loss=1.152 Prec@1=71.662 Prec@5=90.306 rate=8561.49 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST
** validation 100.00% of 1x98...Epoch=134/150 LR=0.00314 Time=0.557 Loss=1.152 Prec@1=71.662 Prec@5=90.306 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=13:46 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> training   0.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=5.137 DataTime=4.897 Loss=0.926 Prec@1=75.391 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST
=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=5.137 DataTime=4.897 Loss=0.926 Prec@1=75.391 Prec@5=93.164 rate=7776.89 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST
=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=5.137 DataTime=4.897 Loss=0.926 Prec@1=75.391 Prec@5=93.164 rate=7776.89 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST
=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.495 DataTime=0.301 Loss=0.864 Prec@1=78.311 Prec@5=93.216 rate=7776.89 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST
=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.495 DataTime=0.301 Loss=0.864 Prec@1=78.311 Prec@5=93.216 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=13:47 IST
=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.495 DataTime=0.301 Loss=0.864 Prec@1=78.311 Prec@5=93.216 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=13:48 IST
=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.867 Prec@1=78.125 Prec@5=93.219 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=13:48 IST
=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.867 Prec@1=78.125 Prec@5=93.219 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=13:48 IST
=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.867 Prec@1=78.125 Prec@5=93.219 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=13:48 IST
=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.865 Prec@1=78.157 Prec@5=93.275 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=13:48 IST
=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.865 Prec@1=78.157 Prec@5=93.275 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=13:48 IST
=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.281 Loss=0.865 Prec@1=78.157 Prec@5=93.275 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=13:49 IST
=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.473 DataTime=0.277 Loss=0.864 Prec@1=78.189 Prec@5=93.249 rate=2.18 Hz, eta=0:16:52, total=0:02:18, wall=13:49 IST
=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.473 DataTime=0.277 Loss=0.864 Prec@1=78.189 Prec@5=93.249 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=13:49 IST
=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.473 DataTime=0.277 Loss=0.864 Prec@1=78.189 Prec@5=93.249 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=13:50 IST
=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.471 DataTime=0.276 Loss=0.865 Prec@1=78.183 Prec@5=93.262 rate=2.18 Hz, eta=0:16:06, total=0:03:04, wall=13:50 IST
=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.471 DataTime=0.276 Loss=0.865 Prec@1=78.183 Prec@5=93.262 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=13:50 IST
=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.471 DataTime=0.276 Loss=0.865 Prec@1=78.183 Prec@5=93.262 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=13:51 IST
=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.472 DataTime=0.278 Loss=0.864 Prec@1=78.189 Prec@5=93.269 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=13:51 IST
=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.472 DataTime=0.278 Loss=0.864 Prec@1=78.189 Prec@5=93.269 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:51 IST
=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.472 DataTime=0.278 Loss=0.864 Prec@1=78.189 Prec@5=93.269 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:52 IST
=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.281 Loss=0.864 Prec@1=78.159 Prec@5=93.263 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=13:52 IST
=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.281 Loss=0.864 Prec@1=78.159 Prec@5=93.263 rate=2.14 Hz, eta=0:14:03, total=0:05:28, wall=13:52 IST
=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.281 Loss=0.864 Prec@1=78.159 Prec@5=93.263 rate=2.14 Hz, eta=0:14:03, total=0:05:28, wall=13:52 IST
=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.280 Loss=0.863 Prec@1=78.177 Prec@5=93.284 rate=2.14 Hz, eta=0:14:03, total=0:05:28, wall=13:52 IST
=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.280 Loss=0.863 Prec@1=78.177 Prec@5=93.284 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=13:52 IST
=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.475 DataTime=0.280 Loss=0.863 Prec@1=78.177 Prec@5=93.284 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=13:53 IST
=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.476 DataTime=0.281 Loss=0.864 Prec@1=78.158 Prec@5=93.267 rate=2.14 Hz, eta=0:13:16, total=0:06:15, wall=13:53 IST
=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.476 DataTime=0.281 Loss=0.864 Prec@1=78.158 Prec@5=93.267 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=13:53 IST
=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.476 DataTime=0.281 Loss=0.864 Prec@1=78.158 Prec@5=93.267 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=13:54 IST
=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.282 Loss=0.865 Prec@1=78.139 Prec@5=93.251 rate=2.13 Hz, eta=0:12:33, total=0:07:03, wall=13:54 IST
=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.282 Loss=0.865 Prec@1=78.139 Prec@5=93.251 rate=2.12 Hz, eta=0:11:48, total=0:07:52, wall=13:54 IST
=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.477 DataTime=0.282 Loss=0.865 Prec@1=78.139 Prec@5=93.251 rate=2.12 Hz, eta=0:11:48, total=0:07:52, wall=13:55 IST
=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.283 Loss=0.866 Prec@1=78.096 Prec@5=93.240 rate=2.12 Hz, eta=0:11:48, total=0:07:52, wall=13:55 IST
=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.283 Loss=0.866 Prec@1=78.096 Prec@5=93.240 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:55 IST
=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.283 Loss=0.866 Prec@1=78.096 Prec@5=93.240 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:56 IST
=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.866 Prec@1=78.090 Prec@5=93.249 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=13:56 IST
=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.866 Prec@1=78.090 Prec@5=93.249 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:56 IST
=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.866 Prec@1=78.090 Prec@5=93.249 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:56 IST
=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.866 Prec@1=78.085 Prec@5=93.247 rate=2.11 Hz, eta=0:10:17, total=0:09:29, wall=13:56 IST
=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.866 Prec@1=78.085 Prec@5=93.247 rate=2.11 Hz, eta=0:09:30, total=0:10:17, wall=13:56 IST
=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.866 Prec@1=78.085 Prec@5=93.247 rate=2.11 Hz, eta=0:09:30, total=0:10:17, wall=13:57 IST
=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.867 Prec@1=78.090 Prec@5=93.237 rate=2.11 Hz, eta=0:09:30, total=0:10:17, wall=13:57 IST
=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.867 Prec@1=78.090 Prec@5=93.237 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:57 IST
=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.478 DataTime=0.284 Loss=0.867 Prec@1=78.090 Prec@5=93.237 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:58 IST
=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.867 Prec@1=78.069 Prec@5=93.227 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=13:58 IST
=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.867 Prec@1=78.069 Prec@5=93.227 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=13:58 IST
=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.479 DataTime=0.284 Loss=0.867 Prec@1=78.069 Prec@5=93.227 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=13:59 IST
=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.868 Prec@1=78.060 Prec@5=93.214 rate=2.10 Hz, eta=0:07:56, total=0:11:53, wall=13:59 IST
=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.868 Prec@1=78.060 Prec@5=93.214 rate=2.10 Hz, eta=0:07:09, total=0:12:43, wall=13:59 IST
=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.868 Prec@1=78.060 Prec@5=93.214 rate=2.10 Hz, eta=0:07:09, total=0:12:43, wall=14:00 IST
=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.869 Prec@1=78.053 Prec@5=93.203 rate=2.10 Hz, eta=0:07:09, total=0:12:43, wall=14:00 IST
=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.869 Prec@1=78.053 Prec@5=93.203 rate=2.10 Hz, eta=0:06:22, total=0:13:31, wall=14:00 IST
=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.285 Loss=0.869 Prec@1=78.053 Prec@5=93.203 rate=2.10 Hz, eta=0:06:22, total=0:13:31, wall=14:00 IST
=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.042 Prec@5=93.199 rate=2.10 Hz, eta=0:06:22, total=0:13:31, wall=14:00 IST
=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.042 Prec@5=93.199 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=14:00 IST
=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.042 Prec@5=93.199 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=14:01 IST
=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.037 Prec@5=93.202 rate=2.09 Hz, eta=0:05:35, total=0:14:19, wall=14:01 IST
=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.037 Prec@5=93.202 rate=2.09 Hz, eta=0:04:47, total=0:15:08, wall=14:01 IST
=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.037 Prec@5=93.202 rate=2.09 Hz, eta=0:04:47, total=0:15:08, wall=14:02 IST
=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.027 Prec@5=93.194 rate=2.09 Hz, eta=0:04:47, total=0:15:08, wall=14:02 IST
=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.027 Prec@5=93.194 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=14:02 IST
=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.480 DataTime=0.286 Loss=0.869 Prec@1=78.027 Prec@5=93.194 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=14:03 IST
=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.869 Prec@1=78.022 Prec@5=93.197 rate=2.09 Hz, eta=0:03:59, total=0:15:55, wall=14:03 IST
=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.869 Prec@1=78.022 Prec@5=93.197 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=14:03 IST
=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.869 Prec@1=78.022 Prec@5=93.197 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=14:04 IST
=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.030 Prec@5=93.198 rate=2.09 Hz, eta=0:03:12, total=0:16:45, wall=14:04 IST
=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.030 Prec@5=93.198 rate=2.09 Hz, eta=0:02:24, total=0:17:33, wall=14:04 IST
=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.030 Prec@5=93.198 rate=2.09 Hz, eta=0:02:24, total=0:17:33, wall=14:04 IST
=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.026 Prec@5=93.192 rate=2.09 Hz, eta=0:02:24, total=0:17:33, wall=14:04 IST
=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.026 Prec@5=93.192 rate=2.09 Hz, eta=0:01:36, total=0:18:21, wall=14:04 IST
=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.026 Prec@5=93.192 rate=2.09 Hz, eta=0:01:36, total=0:18:21, wall=14:05 IST
=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.016 Prec@5=93.189 rate=2.09 Hz, eta=0:01:36, total=0:18:21, wall=14:05 IST
=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.016 Prec@5=93.189 rate=2.09 Hz, eta=0:00:48, total=0:19:09, wall=14:05 IST
=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.870 Prec@1=78.016 Prec@5=93.189 rate=2.09 Hz, eta=0:00:48, total=0:19:09, wall=14:06 IST
=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.871 Prec@1=78.006 Prec@5=93.186 rate=2.09 Hz, eta=0:00:48, total=0:19:09, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.871 Prec@1=78.006 Prec@5=93.186 rate=2.09 Hz, eta=0:00:00, total=0:19:57, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.871 Prec@1=78.006 Prec@5=93.186 rate=2.09 Hz, eta=0:00:00, total=0:19:57, wall=14:06 IST
=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.871 Prec@1=78.006 Prec@5=93.186 rate=2.09 Hz, eta=0:00:00, total=0:19:57, wall=14:06 IST
=> training   100.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.481 DataTime=0.286 Loss=0.871 Prec@1=78.006 Prec@5=93.186 rate=2.09 Hz, eta=0:00:00, total=0:19:58, wall=14:06 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 0.00% of 1x98...Epoch=135/150 LR=0.00278 Time=7.161 Loss=0.669 Prec@1=83.008 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=14:06 IST
=> validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=7.161 Loss=0.669 Prec@1=83.008 Prec@5=95.312 rate=7805.12 Hz, eta=0:00:00, total=0:00:00, wall=14:06 IST
** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=7.161 Loss=0.669 Prec@1=83.008 Prec@5=95.312 rate=7805.12 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=0.556 Loss=1.151 Prec@1=71.790 Prec@5=90.268 rate=7805.12 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
** validation 100.00% of 1x98...Epoch=135/150 LR=0.00278 Time=0.556 Loss=1.151 Prec@1=71.790 Prec@5=90.268 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=14:07 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=4.868 DataTime=4.629 Loss=0.939 Prec@1=75.391 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=14:07 IST
=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=4.868 DataTime=4.629 Loss=0.939 Prec@1=75.391 Prec@5=91.797 rate=10397.60 Hz, eta=0:00:00, total=0:00:00, wall=14:07 IST
=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=4.868 DataTime=4.629 Loss=0.939 Prec@1=75.391 Prec@5=91.797 rate=10397.60 Hz, eta=0:00:00, total=0:00:00, wall=14:08 IST
=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.519 DataTime=0.327 Loss=0.855 Prec@1=78.338 Prec@5=93.423 rate=10397.60 Hz, eta=0:00:00, total=0:00:00, wall=14:08 IST
=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.519 DataTime=0.327 Loss=0.855 Prec@1=78.338 Prec@5=93.423 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=14:08 IST
=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.519 DataTime=0.327 Loss=0.855 Prec@1=78.338 Prec@5=93.423 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=14:09 IST
=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.496 DataTime=0.302 Loss=0.859 Prec@1=78.162 Prec@5=93.353 rate=2.12 Hz, eta=0:18:51, total=0:00:47, wall=14:09 IST
=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.496 DataTime=0.302 Loss=0.859 Prec@1=78.162 Prec@5=93.353 rate=2.12 Hz, eta=0:18:06, total=0:01:34, wall=14:09 IST
=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.496 DataTime=0.302 Loss=0.859 Prec@1=78.162 Prec@5=93.353 rate=2.12 Hz, eta=0:18:06, total=0:01:34, wall=14:09 IST
=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.490 DataTime=0.296 Loss=0.861 Prec@1=78.172 Prec@5=93.352 rate=2.12 Hz, eta=0:18:06, total=0:01:34, wall=14:09 IST
=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.490 DataTime=0.296 Loss=0.861 Prec@1=78.172 Prec@5=93.352 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=14:09 IST
=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.490 DataTime=0.296 Loss=0.861 Prec@1=78.172 Prec@5=93.352 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=14:10 IST
=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.487 DataTime=0.292 Loss=0.861 Prec@1=78.184 Prec@5=93.327 rate=2.11 Hz, eta=0:17:24, total=0:02:22, wall=14:10 IST
=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.487 DataTime=0.292 Loss=0.861 Prec@1=78.184 Prec@5=93.327 rate=2.11 Hz, eta=0:16:38, total=0:03:10, wall=14:10 IST
=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.487 DataTime=0.292 Loss=0.861 Prec@1=78.184 Prec@5=93.327 rate=2.11 Hz, eta=0:16:38, total=0:03:10, wall=14:11 IST
=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.486 DataTime=0.291 Loss=0.861 Prec@1=78.200 Prec@5=93.325 rate=2.11 Hz, eta=0:16:38, total=0:03:10, wall=14:11 IST
=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.486 DataTime=0.291 Loss=0.861 Prec@1=78.200 Prec@5=93.325 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=14:11 IST
=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.486 DataTime=0.291 Loss=0.861 Prec@1=78.200 Prec@5=93.325 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=14:12 IST
=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.290 Loss=0.862 Prec@1=78.199 Prec@5=93.299 rate=2.10 Hz, eta=0:15:53, total=0:03:58, wall=14:12 IST
=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.290 Loss=0.862 Prec@1=78.199 Prec@5=93.299 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=14:12 IST
=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.290 Loss=0.862 Prec@1=78.199 Prec@5=93.299 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=14:13 IST
=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.862 Prec@1=78.180 Prec@5=93.309 rate=2.10 Hz, eta=0:15:06, total=0:04:46, wall=14:13 IST
=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.862 Prec@1=78.180 Prec@5=93.309 rate=2.10 Hz, eta=0:14:20, total=0:05:34, wall=14:13 IST
=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.862 Prec@1=78.180 Prec@5=93.309 rate=2.10 Hz, eta=0:14:20, total=0:05:34, wall=14:13 IST
=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.142 Prec@5=93.301 rate=2.10 Hz, eta=0:14:20, total=0:05:34, wall=14:13 IST
=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.142 Prec@5=93.301 rate=2.09 Hz, eta=0:13:33, total=0:06:22, wall=14:13 IST
=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.142 Prec@5=93.301 rate=2.09 Hz, eta=0:13:33, total=0:06:22, wall=14:14 IST
=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.159 Prec@5=93.313 rate=2.09 Hz, eta=0:13:33, total=0:06:22, wall=14:14 IST
=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.159 Prec@5=93.313 rate=2.09 Hz, eta=0:12:44, total=0:07:10, wall=14:14 IST
=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.159 Prec@5=93.313 rate=2.09 Hz, eta=0:12:44, total=0:07:10, wall=14:15 IST
=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.145 Prec@5=93.302 rate=2.09 Hz, eta=0:12:44, total=0:07:10, wall=14:15 IST
=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.145 Prec@5=93.302 rate=2.09 Hz, eta=0:11:58, total=0:07:58, wall=14:15 IST
=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.287 Loss=0.863 Prec@1=78.145 Prec@5=93.302 rate=2.09 Hz, eta=0:11:58, total=0:07:58, wall=14:16 IST
=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.160 Prec@5=93.305 rate=2.09 Hz, eta=0:11:58, total=0:07:58, wall=14:16 IST
=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.160 Prec@5=93.305 rate=2.09 Hz, eta=0:11:12, total=0:08:48, wall=14:16 IST
=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.160 Prec@5=93.305 rate=2.09 Hz, eta=0:11:12, total=0:08:48, wall=14:17 IST
=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.173 Prec@5=93.297 rate=2.09 Hz, eta=0:11:12, total=0:08:48, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.173 Prec@5=93.297 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.173 Prec@5=93.297 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=14:17 IST
=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.152 Prec@5=93.288 rate=2.08 Hz, eta=0:10:26, total=0:09:37, wall=14:17 IST
=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.152 Prec@5=93.288 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=14:17 IST
=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.152 Prec@5=93.288 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=14:18 IST
=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.153 Prec@5=93.298 rate=2.08 Hz, eta=0:09:38, total=0:10:25, wall=14:18 IST
=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.153 Prec@5=93.298 rate=2.08 Hz, eta=0:08:50, total=0:11:13, wall=14:18 IST
=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.485 DataTime=0.289 Loss=0.863 Prec@1=78.153 Prec@5=93.298 rate=2.08 Hz, eta=0:08:50, total=0:11:13, wall=14:19 IST
=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.175 Prec@5=93.303 rate=2.08 Hz, eta=0:08:50, total=0:11:13, wall=14:19 IST
=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.175 Prec@5=93.303 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=14:19 IST
=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.863 Prec@1=78.175 Prec@5=93.303 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=14:20 IST
=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.166 Prec@5=93.291 rate=2.08 Hz, eta=0:08:01, total=0:12:01, wall=14:20 IST
=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.166 Prec@5=93.291 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=14:20 IST
=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.166 Prec@5=93.291 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=14:21 IST
=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.170 Prec@5=93.279 rate=2.08 Hz, eta=0:07:13, total=0:12:50, wall=14:21 IST
=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.170 Prec@5=93.279 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=14:21 IST
=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.170 Prec@5=93.279 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=14:22 IST
=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.164 Prec@5=93.274 rate=2.08 Hz, eta=0:06:25, total=0:13:38, wall=14:22 IST
=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.164 Prec@5=93.274 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=14:22 IST
=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.164 Prec@5=93.274 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=14:22 IST
=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.864 Prec@1=78.149 Prec@5=93.267 rate=2.08 Hz, eta=0:05:37, total=0:14:26, wall=14:22 IST
=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.864 Prec@1=78.149 Prec@5=93.267 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=14:22 IST
=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.289 Loss=0.864 Prec@1=78.149 Prec@5=93.267 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=14:23 IST
=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.158 Prec@5=93.277 rate=2.08 Hz, eta=0:04:49, total=0:15:15, wall=14:23 IST
=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.158 Prec@5=93.277 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=14:23 IST
=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.158 Prec@5=93.277 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=14:24 IST
=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.169 Prec@5=93.283 rate=2.08 Hz, eta=0:04:01, total=0:16:02, wall=14:24 IST
=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.169 Prec@5=93.283 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=14:24 IST
=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.169 Prec@5=93.283 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=14:25 IST
=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.160 Prec@5=93.278 rate=2.08 Hz, eta=0:03:13, total=0:16:52, wall=14:25 IST
=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.160 Prec@5=93.278 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=14:25 IST
=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.864 Prec@1=78.160 Prec@5=93.278 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=14:26 IST
=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.154 Prec@5=93.270 rate=2.08 Hz, eta=0:02:25, total=0:17:40, wall=14:26 IST
=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.154 Prec@5=93.270 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=14:26 IST
=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.154 Prec@5=93.270 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=14:26 IST
=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.135 Prec@5=93.270 rate=2.08 Hz, eta=0:01:37, total=0:18:28, wall=14:26 IST
=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.135 Prec@5=93.270 rate=2.08 Hz, eta=0:00:49, total=0:19:16, wall=14:26 IST
=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.484 DataTime=0.288 Loss=0.865 Prec@1=78.135 Prec@5=93.270 rate=2.08 Hz, eta=0:00:49, total=0:19:16, wall=14:27 IST
=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.288 Loss=0.865 Prec@1=78.139 Prec@5=93.271 rate=2.08 Hz, eta=0:00:49, total=0:19:16, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.288 Loss=0.865 Prec@1=78.139 Prec@5=93.271 rate=2.08 Hz, eta=0:00:00, total=0:20:03, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.288 Loss=0.865 Prec@1=78.139 Prec@5=93.271 rate=2.08 Hz, eta=0:00:00, total=0:20:03, wall=14:27 IST
=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.288 Loss=0.865 Prec@1=78.138 Prec@5=93.270 rate=2.08 Hz, eta=0:00:00, total=0:20:03, wall=14:27 IST
=> training   100.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.483 DataTime=0.288 Loss=0.865 Prec@1=78.138 Prec@5=93.270 rate=2.08 Hz, eta=0:00:00, total=0:20:04, wall=14:27 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 0.00% of 1x98...Epoch=136/150 LR=0.00245 Time=6.696 Loss=0.690 Prec@1=81.250 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=6.696 Loss=0.690 Prec@1=81.250 Prec@5=94.922 rate=4897.33 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST
** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=6.696 Loss=0.690 Prec@1=81.250 Prec@5=94.922 rate=4897.33 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=0.559 Loss=1.150 Prec@1=71.910 Prec@5=90.392 rate=4897.33 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** validation 100.00% of 1x98...Epoch=136/150 LR=0.00245 Time=0.559 Loss=1.150 Prec@1=71.910 Prec@5=90.392 rate=2.04 Hz, eta=0:00:00, total=0:00:48, wall=14:28 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.728 DataTime=4.459 Loss=0.858 Prec@1=78.906 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.728 DataTime=4.459 Loss=0.858 Prec@1=78.906 Prec@5=93.555 rate=9705.35 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.728 DataTime=4.459 Loss=0.858 Prec@1=78.906 Prec@5=93.555 rate=9705.35 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.513 DataTime=0.321 Loss=0.857 Prec@1=78.345 Prec@5=93.328 rate=9705.35 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.513 DataTime=0.321 Loss=0.857 Prec@1=78.345 Prec@5=93.328 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=14:29 IST
=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.513 DataTime=0.321 Loss=0.857 Prec@1=78.345 Prec@5=93.328 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=14:30 IST
=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.493 DataTime=0.299 Loss=0.858 Prec@1=78.397 Prec@5=93.340 rate=2.14 Hz, eta=0:18:41, total=0:00:47, wall=14:30 IST
=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.493 DataTime=0.299 Loss=0.858 Prec@1=78.397 Prec@5=93.340 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=14:30 IST
=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.493 DataTime=0.299 Loss=0.858 Prec@1=78.397 Prec@5=93.340 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=14:31 IST
=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.291 Loss=0.854 Prec@1=78.473 Prec@5=93.394 rate=2.13 Hz, eta=0:18:00, total=0:01:34, wall=14:31 IST
=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.291 Loss=0.854 Prec@1=78.473 Prec@5=93.394 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=14:31 IST
=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.291 Loss=0.854 Prec@1=78.473 Prec@5=93.394 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=14:31 IST
=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.290 Loss=0.855 Prec@1=78.401 Prec@5=93.387 rate=2.13 Hz, eta=0:17:13, total=0:02:21, wall=14:31 IST
=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.290 Loss=0.855 Prec@1=78.401 Prec@5=93.387 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=14:31 IST
=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.485 DataTime=0.290 Loss=0.855 Prec@1=78.401 Prec@5=93.387 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=14:32 IST
=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.358 rate=2.11 Hz, eta=0:16:34, total=0:03:09, wall=14:32 IST
=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.358 rate=2.12 Hz, eta=0:15:46, total=0:03:56, wall=14:32 IST
=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.358 rate=2.12 Hz, eta=0:15:46, total=0:03:56, wall=14:33 IST
=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.360 Prec@5=93.353 rate=2.12 Hz, eta=0:15:46, total=0:03:56, wall=14:33 IST
=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.360 Prec@5=93.353 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=14:33 IST
=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.360 Prec@5=93.353 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=14:34 IST
=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.365 Prec@5=93.376 rate=2.10 Hz, eta=0:15:03, total=0:04:45, wall=14:34 IST
=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.365 Prec@5=93.376 rate=2.10 Hz, eta=0:14:18, total=0:05:34, wall=14:34 IST
=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.365 Prec@5=93.376 rate=2.10 Hz, eta=0:14:18, total=0:05:34, wall=14:35 IST
=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.338 Prec@5=93.379 rate=2.10 Hz, eta=0:14:18, total=0:05:34, wall=14:35 IST
=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.338 Prec@5=93.379 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=14:35 IST
=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.857 Prec@1=78.338 Prec@5=93.379 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=14:35 IST
=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.351 Prec@5=93.380 rate=2.10 Hz, eta=0:13:32, total=0:06:22, wall=14:35 IST
=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.351 Prec@5=93.380 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=14:35 IST
=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.289 Loss=0.856 Prec@1=78.351 Prec@5=93.380 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=14:36 IST
=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.376 Prec@5=93.383 rate=2.09 Hz, eta=0:12:45, total=0:07:10, wall=14:36 IST
=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.376 Prec@5=93.383 rate=2.09 Hz, eta=0:11:57, total=0:07:57, wall=14:36 IST
=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.376 Prec@5=93.383 rate=2.09 Hz, eta=0:11:57, total=0:07:57, wall=14:37 IST
=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.855 Prec@1=78.370 Prec@5=93.378 rate=2.09 Hz, eta=0:11:57, total=0:07:57, wall=14:37 IST
=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.855 Prec@1=78.370 Prec@5=93.378 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=14:37 IST
=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.855 Prec@1=78.370 Prec@5=93.378 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=14:38 IST
=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.368 rate=2.09 Hz, eta=0:11:10, total=0:08:46, wall=14:38 IST
=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.368 rate=2.09 Hz, eta=0:10:22, total=0:09:34, wall=14:38 IST
=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.856 Prec@1=78.365 Prec@5=93.368 rate=2.09 Hz, eta=0:10:22, total=0:09:34, wall=14:39 IST
=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.392 Prec@5=93.372 rate=2.09 Hz, eta=0:10:22, total=0:09:34, wall=14:39 IST
=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.392 Prec@5=93.372 rate=2.09 Hz, eta=0:09:35, total=0:10:22, wall=14:39 IST
=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.392 Prec@5=93.372 rate=2.09 Hz, eta=0:09:35, total=0:10:22, wall=14:39 IST
=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.399 Prec@5=93.377 rate=2.09 Hz, eta=0:09:35, total=0:10:22, wall=14:39 IST
=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.399 Prec@5=93.377 rate=2.09 Hz, eta=0:08:47, total=0:11:10, wall=14:39 IST
=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.288 Loss=0.855 Prec@1=78.399 Prec@5=93.377 rate=2.09 Hz, eta=0:08:47, total=0:11:10, wall=14:40 IST
=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.856 Prec@1=78.382 Prec@5=93.368 rate=2.09 Hz, eta=0:08:47, total=0:11:10, wall=14:40 IST
=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.856 Prec@1=78.382 Prec@5=93.368 rate=2.09 Hz, eta=0:08:00, total=0:11:59, wall=14:40 IST
=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.483 DataTime=0.288 Loss=0.856 Prec@1=78.382 Prec@5=93.368 rate=2.09 Hz, eta=0:08:00, total=0:11:59, wall=14:41 IST
=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.287 Loss=0.856 Prec@1=78.366 Prec@5=93.367 rate=2.09 Hz, eta=0:08:00, total=0:11:59, wall=14:41 IST
=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.287 Loss=0.856 Prec@1=78.366 Prec@5=93.367 rate=2.09 Hz, eta=0:07:11, total=0:12:46, wall=14:41 IST
=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.482 DataTime=0.287 Loss=0.856 Prec@1=78.366 Prec@5=93.367 rate=2.09 Hz, eta=0:07:11, total=0:12:46, wall=14:42 IST
=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.481 DataTime=0.286 Loss=0.856 Prec@1=78.345 Prec@5=93.366 rate=2.09 Hz, eta=0:07:11, total=0:12:46, wall=14:42 IST
=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.481 DataTime=0.286 Loss=0.856 Prec@1=78.345 Prec@5=93.366 rate=2.09 Hz, eta=0:06:23, total=0:13:33, wall=14:42 IST
=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.481 DataTime=0.286 Loss=0.856 Prec@1=78.345 Prec@5=93.366 rate=2.09 Hz, eta=0:06:23, total=0:13:33, wall=14:43 IST
=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.480 DataTime=0.285 Loss=0.856 Prec@1=78.350 Prec@5=93.369 rate=2.09 Hz, eta=0:06:23, total=0:13:33, wall=14:43 IST
=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.480 DataTime=0.285 Loss=0.856 Prec@1=78.350 Prec@5=93.369 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=14:43 IST
=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.480 DataTime=0.285 Loss=0.856 Prec@1=78.350 Prec@5=93.369 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=14:43 IST
=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.343 Prec@5=93.367 rate=2.09 Hz, eta=0:05:35, total=0:14:20, wall=14:43 IST
=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.343 Prec@5=93.367 rate=2.10 Hz, eta=0:04:47, total=0:15:06, wall=14:43 IST
=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.343 Prec@5=93.367 rate=2.10 Hz, eta=0:04:47, total=0:15:06, wall=14:44 IST
=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.344 Prec@5=93.358 rate=2.10 Hz, eta=0:04:47, total=0:15:06, wall=14:44 IST
=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.344 Prec@5=93.358 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=14:44 IST
=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.479 DataTime=0.284 Loss=0.857 Prec@1=78.344 Prec@5=93.358 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=14:45 IST
=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.319 Prec@5=93.345 rate=2.10 Hz, eta=0:03:59, total=0:15:53, wall=14:45 IST
=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.319 Prec@5=93.345 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=14:45 IST
=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.319 Prec@5=93.345 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=14:46 IST
=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.325 Prec@5=93.353 rate=2.10 Hz, eta=0:03:11, total=0:16:40, wall=14:46 IST
=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.325 Prec@5=93.353 rate=2.10 Hz, eta=0:02:23, total=0:17:26, wall=14:46 IST
=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.478 DataTime=0.283 Loss=0.858 Prec@1=78.325 Prec@5=93.353 rate=2.10 Hz, eta=0:02:23, total=0:17:26, wall=14:46 IST
=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.324 Prec@5=93.352 rate=2.10 Hz, eta=0:02:23, total=0:17:26, wall=14:46 IST
=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.324 Prec@5=93.352 rate=2.10 Hz, eta=0:01:35, total=0:18:13, wall=14:46 IST
=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.324 Prec@5=93.352 rate=2.10 Hz, eta=0:01:35, total=0:18:13, wall=14:47 IST
=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.320 Prec@5=93.349 rate=2.10 Hz, eta=0:01:35, total=0:18:13, wall=14:47 IST
=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.320 Prec@5=93.349 rate=2.11 Hz, eta=0:00:48, total=0:19:00, wall=14:47 IST
=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.477 DataTime=0.282 Loss=0.858 Prec@1=78.320 Prec@5=93.349 rate=2.11 Hz, eta=0:00:48, total=0:19:00, wall=14:48 IST
=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.476 DataTime=0.281 Loss=0.858 Prec@1=78.307 Prec@5=93.346 rate=2.11 Hz, eta=0:00:48, total=0:19:00, wall=14:48 IST
=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.476 DataTime=0.281 Loss=0.858 Prec@1=78.307 Prec@5=93.346 rate=2.11 Hz, eta=0:00:00, total=0:19:46, wall=14:48 IST
=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.476 DataTime=0.281 Loss=0.858 Prec@1=78.307 Prec@5=93.346 rate=2.11 Hz, eta=0:00:00, total=0:19:46, wall=14:48 IST
=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.476 DataTime=0.281 Loss=0.858 Prec@1=78.304 Prec@5=93.345 rate=2.11 Hz, eta=0:00:00, total=0:19:46, wall=14:48 IST
=> training   100.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.476 DataTime=0.281 Loss=0.858 Prec@1=78.304 Prec@5=93.345 rate=2.11 Hz, eta=0:00:00, total=0:19:47, wall=14:48 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> validation 0.00% of 1x98...Epoch=137/150 LR=0.00213 Time=6.872 Loss=0.662 Prec@1=83.203 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=14:48 IST
=> validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=6.872 Loss=0.662 Prec@1=83.203 Prec@5=95.898 rate=6898.31 Hz, eta=0:00:00, total=0:00:00, wall=14:48 IST
** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=6.872 Loss=0.662 Prec@1=83.203 Prec@5=95.898 rate=6898.31 Hz, eta=0:00:00, total=0:00:00, wall=14:49 IST
** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=0.553 Loss=1.144 Prec@1=71.738 Prec@5=90.452 rate=6898.31 Hz, eta=0:00:00, total=0:00:00, wall=14:49 IST
** validation 100.00% of 1x98...Epoch=137/150 LR=0.00213 Time=0.553 Loss=1.144 Prec@1=71.738 Prec@5=90.452 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=14:49 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:49 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:49 IST
=> training   0.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=4.946 DataTime=4.710 Loss=0.839 Prec@1=78.516 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=14:49 IST
=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=4.946 DataTime=4.710 Loss=0.839 Prec@1=78.516 Prec@5=93.164 rate=7287.73 Hz, eta=0:00:00, total=0:00:00, wall=14:49 IST
=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=4.946 DataTime=4.710 Loss=0.839 Prec@1=78.516 Prec@5=93.164 rate=7287.73 Hz, eta=0:00:00, total=0:00:00, wall=14:50 IST
=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.502 DataTime=0.306 Loss=0.831 Prec@1=78.957 Prec@5=93.649 rate=7287.73 Hz, eta=0:00:00, total=0:00:00, wall=14:50 IST
=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.502 DataTime=0.306 Loss=0.831 Prec@1=78.957 Prec@5=93.649 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=14:50 IST
=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.502 DataTime=0.306 Loss=0.831 Prec@1=78.957 Prec@5=93.649 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=14:51 IST
=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.482 DataTime=0.285 Loss=0.841 Prec@1=78.747 Prec@5=93.509 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=14:51 IST
=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.482 DataTime=0.285 Loss=0.841 Prec@1=78.747 Prec@5=93.509 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=14:51 IST
=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.482 DataTime=0.285 Loss=0.841 Prec@1=78.747 Prec@5=93.509 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=14:51 IST
=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.644 Prec@5=93.477 rate=2.19 Hz, eta=0:17:32, total=0:01:31, wall=14:51 IST
=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.644 Prec@5=93.477 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=14:51 IST
=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.644 Prec@5=93.477 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=14:52 IST
=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.623 Prec@5=93.475 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=14:52 IST
=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.623 Prec@5=93.475 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=14:52 IST
=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.474 DataTime=0.278 Loss=0.844 Prec@1=78.623 Prec@5=93.475 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=14:53 IST
=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.844 Prec@1=78.686 Prec@5=93.467 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=14:53 IST
=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.844 Prec@1=78.686 Prec@5=93.467 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=14:53 IST
=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.844 Prec@1=78.686 Prec@5=93.467 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=14:54 IST
=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.473 DataTime=0.278 Loss=0.845 Prec@1=78.650 Prec@5=93.441 rate=2.17 Hz, eta=0:15:23, total=0:03:51, wall=14:54 IST
=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.473 DataTime=0.278 Loss=0.845 Prec@1=78.650 Prec@5=93.441 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=14:54 IST
=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.473 DataTime=0.278 Loss=0.845 Prec@1=78.650 Prec@5=93.441 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=14:54 IST
=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.637 Prec@5=93.433 rate=2.15 Hz, eta=0:14:43, total=0:04:39, wall=14:54 IST
=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.637 Prec@5=93.433 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:54 IST
=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.637 Prec@5=93.433 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:55 IST
=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.848 Prec@1=78.610 Prec@5=93.417 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=14:55 IST
=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.848 Prec@1=78.610 Prec@5=93.417 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=14:55 IST
=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.472 DataTime=0.276 Loss=0.848 Prec@1=78.610 Prec@5=93.417 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=14:56 IST
=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.616 Prec@5=93.416 rate=2.15 Hz, eta=0:13:12, total=0:06:12, wall=14:56 IST
=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.616 Prec@5=93.416 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:56 IST
=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.616 Prec@5=93.416 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:57 IST
=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.612 Prec@5=93.417 rate=2.15 Hz, eta=0:12:26, total=0:06:59, wall=14:57 IST
=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.612 Prec@5=93.417 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=14:57 IST
=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.276 Loss=0.848 Prec@1=78.612 Prec@5=93.417 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=14:58 IST
=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.849 Prec@1=78.601 Prec@5=93.410 rate=2.14 Hz, eta=0:11:40, total=0:07:46, wall=14:58 IST
=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.849 Prec@1=78.601 Prec@5=93.410 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:58 IST
=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.849 Prec@1=78.601 Prec@5=93.410 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:58 IST
=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.850 Prec@1=78.566 Prec@5=93.392 rate=2.15 Hz, eta=0:10:53, total=0:08:33, wall=14:58 IST
=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.850 Prec@1=78.566 Prec@5=93.392 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:58 IST
=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.850 Prec@1=78.566 Prec@5=93.392 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:59 IST
=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.521 Prec@5=93.388 rate=2.14 Hz, eta=0:10:07, total=0:09:20, wall=14:59 IST
=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.521 Prec@5=93.388 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=14:59 IST
=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.521 Prec@5=93.388 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=15:00 IST
=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.512 Prec@5=93.392 rate=2.14 Hz, eta=0:09:21, total=0:10:07, wall=15:00 IST
=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.512 Prec@5=93.392 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=15:00 IST
=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.851 Prec@1=78.512 Prec@5=93.392 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=15:01 IST
=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.851 Prec@1=78.507 Prec@5=93.391 rate=2.14 Hz, eta=0:08:34, total=0:10:54, wall=15:01 IST
=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.851 Prec@1=78.507 Prec@5=93.391 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=15:01 IST
=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.851 Prec@1=78.507 Prec@5=93.391 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=15:01 IST
=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.852 Prec@1=78.497 Prec@5=93.383 rate=2.14 Hz, eta=0:07:48, total=0:11:41, wall=15:01 IST
=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.852 Prec@1=78.497 Prec@5=93.383 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=15:01 IST
=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.471 DataTime=0.275 Loss=0.852 Prec@1=78.497 Prec@5=93.383 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=15:02 IST
=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.502 Prec@5=93.385 rate=2.14 Hz, eta=0:07:01, total=0:12:28, wall=15:02 IST
=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.502 Prec@5=93.385 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=15:02 IST
=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.502 Prec@5=93.385 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=15:03 IST
=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.491 Prec@5=93.380 rate=2.14 Hz, eta=0:06:14, total=0:13:15, wall=15:03 IST
=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.491 Prec@5=93.380 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:03 IST
=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.275 Loss=0.852 Prec@1=78.491 Prec@5=93.380 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:04 IST
=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:05:28, total=0:14:01, wall=15:04 IST
=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=15:04 IST
=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=15:05 IST
=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:04:41, total=0:14:48, wall=15:05 IST
=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=15:05 IST
=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.468 Prec@5=93.373 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=15:05 IST
=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.453 Prec@5=93.368 rate=2.14 Hz, eta=0:03:54, total=0:15:34, wall=15:05 IST
=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.453 Prec@5=93.368 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:05 IST
=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.470 DataTime=0.274 Loss=0.853 Prec@1=78.453 Prec@5=93.368 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:06 IST
=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.449 Prec@5=93.375 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=15:06 IST
=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.449 Prec@5=93.375 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:06 IST
=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.449 Prec@5=93.375 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:07 IST
=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.454 Prec@5=93.379 rate=2.14 Hz, eta=0:02:21, total=0:17:08, wall=15:07 IST
=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.454 Prec@5=93.379 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=15:07 IST
=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.454 Prec@5=93.379 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=15:08 IST
=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.451 Prec@5=93.381 rate=2.14 Hz, eta=0:01:34, total=0:17:54, wall=15:08 IST
=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.451 Prec@5=93.381 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=15:08 IST
=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.451 Prec@5=93.381 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=15:08 IST
=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.445 Prec@5=93.382 rate=2.14 Hz, eta=0:00:47, total=0:18:41, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.445 Prec@5=93.382 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.445 Prec@5=93.382 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.446 Prec@5=93.382 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=15:08 IST
=> training   100.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.469 DataTime=0.274 Loss=0.853 Prec@1=78.446 Prec@5=93.382 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=15:08 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> validation 0.00% of 1x98...Epoch=138/150 LR=0.00184 Time=6.839 Loss=0.644 Prec@1=82.617 Prec@5=96.094 rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=6.839 Loss=0.644 Prec@1=82.617 Prec@5=96.094 rate=7607.00 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=6.839 Loss=0.644 Prec@1=82.617 Prec@5=96.094 rate=7607.00 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=0.548 Loss=1.143 Prec@1=71.794 Prec@5=90.414 rate=7607.00 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
** validation 100.00% of 1x98...Epoch=138/150 LR=0.00184 Time=0.548 Loss=1.143 Prec@1=71.794 Prec@5=90.414 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=15:09 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=4.835 DataTime=4.586 Loss=0.910 Prec@1=77.344 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=15:09 IST
=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=4.835 DataTime=4.586 Loss=0.910 Prec@1=77.344 Prec@5=93.164 rate=7536.25 Hz, eta=0:00:00, total=0:00:00, wall=15:09 IST
=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=4.835 DataTime=4.586 Loss=0.910 Prec@1=77.344 Prec@5=93.164 rate=7536.25 Hz, eta=0:00:00, total=0:00:00, wall=15:10 IST
=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.502 DataTime=0.306 Loss=0.846 Prec@1=78.651 Prec@5=93.458 rate=7536.25 Hz, eta=0:00:00, total=0:00:00, wall=15:10 IST
=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.502 DataTime=0.306 Loss=0.846 Prec@1=78.651 Prec@5=93.458 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:10 IST
=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.502 DataTime=0.306 Loss=0.846 Prec@1=78.651 Prec@5=93.458 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:11 IST
=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.487 DataTime=0.290 Loss=0.840 Prec@1=78.766 Prec@5=93.556 rate=2.20 Hz, eta=0:18:11, total=0:00:45, wall=15:11 IST
=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.487 DataTime=0.290 Loss=0.840 Prec@1=78.766 Prec@5=93.556 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=15:11 IST
=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.487 DataTime=0.290 Loss=0.840 Prec@1=78.766 Prec@5=93.556 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=15:12 IST
=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.479 DataTime=0.281 Loss=0.840 Prec@1=78.776 Prec@5=93.535 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=15:12 IST
=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.479 DataTime=0.281 Loss=0.840 Prec@1=78.776 Prec@5=93.535 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=15:12 IST
=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.479 DataTime=0.281 Loss=0.840 Prec@1=78.776 Prec@5=93.535 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=15:13 IST
=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.477 DataTime=0.279 Loss=0.841 Prec@1=78.777 Prec@5=93.547 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=15:13 IST
=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.477 DataTime=0.279 Loss=0.841 Prec@1=78.777 Prec@5=93.547 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:13 IST
=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.477 DataTime=0.279 Loss=0.841 Prec@1=78.777 Prec@5=93.547 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:13 IST
=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.476 DataTime=0.278 Loss=0.842 Prec@1=78.745 Prec@5=93.523 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:13 IST
=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.476 DataTime=0.278 Loss=0.842 Prec@1=78.745 Prec@5=93.523 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=15:13 IST
=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.476 DataTime=0.278 Loss=0.842 Prec@1=78.745 Prec@5=93.523 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=15:14 IST
=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.845 Prec@1=78.686 Prec@5=93.473 rate=2.15 Hz, eta=0:15:32, total=0:03:53, wall=15:14 IST
=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.845 Prec@1=78.686 Prec@5=93.473 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=15:14 IST
=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.845 Prec@1=78.686 Prec@5=93.473 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=15:15 IST
=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.276 Loss=0.846 Prec@1=78.682 Prec@5=93.461 rate=2.14 Hz, eta=0:14:47, total=0:04:40, wall=15:15 IST
=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.276 Loss=0.846 Prec@1=78.682 Prec@5=93.461 rate=2.14 Hz, eta=0:14:00, total=0:05:27, wall=15:15 IST
=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.276 Loss=0.846 Prec@1=78.682 Prec@5=93.461 rate=2.14 Hz, eta=0:14:00, total=0:05:27, wall=15:16 IST
=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.677 Prec@5=93.444 rate=2.14 Hz, eta=0:14:00, total=0:05:27, wall=15:16 IST
=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.677 Prec@5=93.444 rate=2.14 Hz, eta=0:13:14, total=0:06:14, wall=15:16 IST
=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.677 Prec@5=93.444 rate=2.14 Hz, eta=0:13:14, total=0:06:14, wall=15:17 IST
=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.846 Prec@1=78.654 Prec@5=93.444 rate=2.14 Hz, eta=0:13:14, total=0:06:14, wall=15:17 IST
=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.846 Prec@1=78.654 Prec@5=93.444 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=15:17 IST
=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.474 DataTime=0.277 Loss=0.846 Prec@1=78.654 Prec@5=93.444 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=15:17 IST
=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.847 Prec@1=78.638 Prec@5=93.443 rate=2.14 Hz, eta=0:12:30, total=0:07:01, wall=15:17 IST
=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.847 Prec@1=78.638 Prec@5=93.443 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:17 IST
=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.847 Prec@1=78.638 Prec@5=93.443 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:18 IST
=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.646 Prec@5=93.438 rate=2.14 Hz, eta=0:11:42, total=0:07:48, wall=15:18 IST
=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.646 Prec@5=93.438 rate=2.14 Hz, eta=0:10:54, total=0:08:34, wall=15:18 IST
=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.646 Prec@5=93.438 rate=2.14 Hz, eta=0:10:54, total=0:08:34, wall=15:19 IST
=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.846 Prec@1=78.658 Prec@5=93.446 rate=2.14 Hz, eta=0:10:54, total=0:08:34, wall=15:19 IST
=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.846 Prec@1=78.658 Prec@5=93.446 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=15:19 IST
=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.846 Prec@1=78.658 Prec@5=93.446 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=15:20 IST
=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.846 Prec@1=78.634 Prec@5=93.455 rate=2.14 Hz, eta=0:10:08, total=0:09:21, wall=15:20 IST
=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.846 Prec@1=78.634 Prec@5=93.455 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:20 IST
=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.846 Prec@1=78.634 Prec@5=93.455 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:20 IST
=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.643 Prec@5=93.451 rate=2.13 Hz, eta=0:09:23, total=0:10:09, wall=15:20 IST
=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.643 Prec@5=93.451 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=15:20 IST
=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.276 Loss=0.846 Prec@1=78.643 Prec@5=93.451 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=15:21 IST
=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.638 Prec@5=93.445 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=15:21 IST
=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.638 Prec@5=93.445 rate=2.13 Hz, eta=0:07:51, total=0:11:45, wall=15:21 IST
=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.638 Prec@5=93.445 rate=2.13 Hz, eta=0:07:51, total=0:11:45, wall=15:22 IST
=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.633 Prec@5=93.451 rate=2.13 Hz, eta=0:07:51, total=0:11:45, wall=15:22 IST
=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.633 Prec@5=93.451 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=15:22 IST
=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.473 DataTime=0.277 Loss=0.847 Prec@1=78.633 Prec@5=93.451 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=15:23 IST
=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.620 Prec@5=93.451 rate=2.13 Hz, eta=0:07:03, total=0:12:32, wall=15:23 IST
=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.620 Prec@5=93.451 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=15:23 IST
=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.620 Prec@5=93.451 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=15:24 IST
=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.624 Prec@5=93.449 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=15:24 IST
=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.624 Prec@5=93.449 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:24 IST
=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.472 DataTime=0.276 Loss=0.847 Prec@1=78.624 Prec@5=93.449 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:24 IST
=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.617 Prec@5=93.444 rate=2.13 Hz, eta=0:05:29, total=0:14:05, wall=15:24 IST
=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.617 Prec@5=93.444 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=15:24 IST
=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.617 Prec@5=93.444 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=15:25 IST
=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.452 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=15:25 IST
=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.452 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=15:25 IST
=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.452 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=15:26 IST
=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.456 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=15:26 IST
=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.456 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=15:26 IST
=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.622 Prec@5=93.456 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=15:27 IST
=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.616 Prec@5=93.448 rate=2.13 Hz, eta=0:03:08, total=0:16:25, wall=15:27 IST
=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.616 Prec@5=93.448 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=15:27 IST
=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.847 Prec@1=78.616 Prec@5=93.448 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=15:27 IST
=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.470 DataTime=0.275 Loss=0.848 Prec@1=78.610 Prec@5=93.444 rate=2.13 Hz, eta=0:02:21, total=0:17:11, wall=15:27 IST
=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.470 DataTime=0.275 Loss=0.848 Prec@1=78.610 Prec@5=93.444 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=15:27 IST
=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.470 DataTime=0.275 Loss=0.848 Prec@1=78.610 Prec@5=93.444 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=15:28 IST
=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.604 Prec@5=93.449 rate=2.14 Hz, eta=0:01:34, total=0:17:57, wall=15:28 IST
=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.604 Prec@5=93.449 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=15:28 IST
=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.604 Prec@5=93.449 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=15:29 IST
=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.590 Prec@5=93.449 rate=2.13 Hz, eta=0:00:47, total=0:18:44, wall=15:29 IST
=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.590 Prec@5=93.449 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=15:29 IST
=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.275 Loss=0.848 Prec@1=78.590 Prec@5=93.449 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=15:29 IST
=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.274 Loss=0.848 Prec@1=78.590 Prec@5=93.449 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=15:29 IST
=> training   100.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.471 DataTime=0.274 Loss=0.848 Prec@1=78.590 Prec@5=93.449 rate=2.13 Hz, eta=0:00:00, total=0:19:32, wall=15:29 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> validation 0.00% of 1x98...Epoch=139/150 LR=0.00157 Time=7.123 Loss=0.661 Prec@1=84.375 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=15:29 IST
=> validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=7.123 Loss=0.661 Prec@1=84.375 Prec@5=95.508 rate=3867.17 Hz, eta=0:00:00, total=0:00:00, wall=15:29 IST
** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=7.123 Loss=0.661 Prec@1=84.375 Prec@5=95.508 rate=3867.17 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST
** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=0.555 Loss=1.145 Prec@1=71.836 Prec@5=90.412 rate=3867.17 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST
** validation 100.00% of 1x98...Epoch=139/150 LR=0.00157 Time=0.555 Loss=1.145 Prec@1=71.836 Prec@5=90.412 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=15:30 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST
=> training   0.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.358 DataTime=4.087 Loss=0.907 Prec@1=78.906 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST
=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.358 DataTime=4.087 Loss=0.907 Prec@1=78.906 Prec@5=91.992 rate=7151.29 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST
=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.358 DataTime=4.087 Loss=0.907 Prec@1=78.906 Prec@5=91.992 rate=7151.29 Hz, eta=0:00:00, total=0:00:00, wall=15:31 IST
=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.509 DataTime=0.310 Loss=0.841 Prec@1=78.893 Prec@5=93.379 rate=7151.29 Hz, eta=0:00:00, total=0:00:00, wall=15:31 IST
=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.509 DataTime=0.310 Loss=0.841 Prec@1=78.893 Prec@5=93.379 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=15:31 IST
=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.509 DataTime=0.310 Loss=0.841 Prec@1=78.893 Prec@5=93.379 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=15:32 IST
=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.488 DataTime=0.291 Loss=0.842 Prec@1=78.836 Prec@5=93.418 rate=2.15 Hz, eta=0:18:38, total=0:00:47, wall=15:32 IST
=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.488 DataTime=0.291 Loss=0.842 Prec@1=78.836 Prec@5=93.418 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=15:32 IST
=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.488 DataTime=0.291 Loss=0.842 Prec@1=78.836 Prec@5=93.418 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=15:32 IST
=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.478 DataTime=0.282 Loss=0.846 Prec@1=78.682 Prec@5=93.420 rate=2.15 Hz, eta=0:17:52, total=0:01:33, wall=15:32 IST
=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.478 DataTime=0.282 Loss=0.846 Prec@1=78.682 Prec@5=93.420 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=15:32 IST
=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.478 DataTime=0.282 Loss=0.846 Prec@1=78.682 Prec@5=93.420 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=15:33 IST
=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.475 DataTime=0.279 Loss=0.847 Prec@1=78.675 Prec@5=93.396 rate=2.16 Hz, eta=0:17:01, total=0:02:19, wall=15:33 IST
=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.475 DataTime=0.279 Loss=0.847 Prec@1=78.675 Prec@5=93.396 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:33 IST
=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.475 DataTime=0.279 Loss=0.847 Prec@1=78.675 Prec@5=93.396 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:34 IST
=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.472 DataTime=0.275 Loss=0.848 Prec@1=78.615 Prec@5=93.370 rate=2.15 Hz, eta=0:16:16, total=0:03:06, wall=15:34 IST
=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.472 DataTime=0.275 Loss=0.848 Prec@1=78.615 Prec@5=93.370 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=15:34 IST
=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.472 DataTime=0.275 Loss=0.848 Prec@1=78.615 Prec@5=93.370 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=15:35 IST
=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.845 Prec@1=78.675 Prec@5=93.418 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=15:35 IST
=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.845 Prec@1=78.675 Prec@5=93.418 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=15:35 IST
=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.845 Prec@1=78.675 Prec@5=93.418 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=15:35 IST
=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.846 Prec@1=78.692 Prec@5=93.415 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=15:35 IST
=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.846 Prec@1=78.692 Prec@5=93.415 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:35 IST
=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.471 DataTime=0.274 Loss=0.846 Prec@1=78.692 Prec@5=93.415 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:36 IST
=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.470 DataTime=0.273 Loss=0.844 Prec@1=78.750 Prec@5=93.419 rate=2.15 Hz, eta=0:13:57, total=0:05:25, wall=15:36 IST
=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.470 DataTime=0.273 Loss=0.844 Prec@1=78.750 Prec@5=93.419 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=15:36 IST
=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.470 DataTime=0.273 Loss=0.844 Prec@1=78.750 Prec@5=93.419 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=15:37 IST
=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.273 Loss=0.844 Prec@1=78.738 Prec@5=93.430 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=15:37 IST
=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.273 Loss=0.844 Prec@1=78.738 Prec@5=93.430 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=15:37 IST
=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.273 Loss=0.844 Prec@1=78.738 Prec@5=93.430 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=15:38 IST
=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.845 Prec@1=78.708 Prec@5=93.418 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=15:38 IST
=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.845 Prec@1=78.708 Prec@5=93.418 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=15:38 IST
=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.845 Prec@1=78.708 Prec@5=93.418 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=15:39 IST
=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.733 Prec@5=93.438 rate=2.15 Hz, eta=0:11:37, total=0:07:44, wall=15:39 IST
=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.733 Prec@5=93.438 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=15:39 IST
=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.733 Prec@5=93.438 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=15:39 IST
=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.730 Prec@5=93.445 rate=2.15 Hz, eta=0:10:51, total=0:08:31, wall=15:39 IST
=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.730 Prec@5=93.445 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=15:39 IST
=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.469 DataTime=0.272 Loss=0.844 Prec@1=78.730 Prec@5=93.445 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=15:40 IST
=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.735 Prec@5=93.451 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=15:40 IST
=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.735 Prec@5=93.451 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=15:40 IST
=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.735 Prec@5=93.451 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=15:41 IST
=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.843 Prec@1=78.742 Prec@5=93.469 rate=2.15 Hz, eta=0:09:18, total=0:10:04, wall=15:41 IST
=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.843 Prec@1=78.742 Prec@5=93.469 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=15:41 IST
=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.843 Prec@1=78.742 Prec@5=93.469 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=15:42 IST
=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.742 Prec@5=93.465 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=15:42 IST
=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.742 Prec@5=93.465 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=15:42 IST
=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.742 Prec@5=93.465 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=15:42 IST
=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.744 Prec@5=93.477 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=15:42 IST
=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.744 Prec@5=93.477 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=15:42 IST
=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.744 Prec@5=93.477 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=15:43 IST
=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.727 Prec@5=93.476 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=15:43 IST
=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.727 Prec@5=93.476 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=15:43 IST
=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.727 Prec@5=93.476 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=15:44 IST
=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.721 Prec@5=93.471 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=15:44 IST
=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.721 Prec@5=93.471 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=15:44 IST
=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.721 Prec@5=93.471 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=15:45 IST
=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.710 Prec@5=93.474 rate=2.15 Hz, eta=0:05:26, total=0:13:57, wall=15:45 IST
=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.710 Prec@5=93.474 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=15:45 IST
=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.710 Prec@5=93.474 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=15:46 IST
=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.467 DataTime=0.271 Loss=0.844 Prec@1=78.703 Prec@5=93.476 rate=2.15 Hz, eta=0:04:40, total=0:14:44, wall=15:46 IST
=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.467 DataTime=0.271 Loss=0.844 Prec@1=78.703 Prec@5=93.476 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=15:46 IST
=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.467 DataTime=0.271 Loss=0.844 Prec@1=78.703 Prec@5=93.476 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=15:46 IST
=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.698 Prec@5=93.475 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=15:46 IST
=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.698 Prec@5=93.475 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=15:46 IST
=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.698 Prec@5=93.475 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=15:47 IST
=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.701 Prec@5=93.475 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=15:47 IST
=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.701 Prec@5=93.475 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=15:47 IST
=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.701 Prec@5=93.475 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=15:48 IST
=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.691 Prec@5=93.472 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=15:48 IST
=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.691 Prec@5=93.472 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=15:48 IST
=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.691 Prec@5=93.472 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=15:49 IST
=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.694 Prec@5=93.479 rate=2.15 Hz, eta=0:01:34, total=0:17:51, wall=15:49 IST
=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.694 Prec@5=93.479 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=15:49 IST
=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.694 Prec@5=93.479 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=15:49 IST
=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.701 Prec@5=93.480 rate=2.15 Hz, eta=0:00:47, total=0:18:38, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.701 Prec@5=93.480 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.272 Loss=0.844 Prec@1=78.701 Prec@5=93.480 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=15:49 IST
=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.699 Prec@5=93.480 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=15:49 IST
=> training   100.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.468 DataTime=0.271 Loss=0.844 Prec@1=78.699 Prec@5=93.480 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=15:49 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> validation 0.00% of 1x98...Epoch=140/150 LR=0.00132 Time=6.249 Loss=0.671 Prec@1=83.203 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.249 Loss=0.671 Prec@1=83.203 Prec@5=95.117 rate=9333.76 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.249 Loss=0.671 Prec@1=83.203 Prec@5=95.117 rate=9333.76 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=0.543 Loss=1.142 Prec@1=72.050 Prec@5=90.426 rate=9333.76 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** validation 100.00% of 1x98...Epoch=140/150 LR=0.00132 Time=0.543 Loss=1.142 Prec@1=72.050 Prec@5=90.426 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=15:50 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.667 DataTime=4.395 Loss=0.836 Prec@1=77.930 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.667 DataTime=4.395 Loss=0.836 Prec@1=77.930 Prec@5=94.336 rate=5211.16 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.667 DataTime=4.395 Loss=0.836 Prec@1=77.930 Prec@5=94.336 rate=5211.16 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.502 DataTime=0.307 Loss=0.837 Prec@1=78.535 Prec@5=93.704 rate=5211.16 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.502 DataTime=0.307 Loss=0.837 Prec@1=78.535 Prec@5=93.704 rate=2.19 Hz, eta=0:18:16, total=0:00:46, wall=15:51 IST
=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.502 DataTime=0.307 Loss=0.837 Prec@1=78.535 Prec@5=93.704 rate=2.19 Hz, eta=0:18:16, total=0:00:46, wall=15:52 IST
=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.481 DataTime=0.284 Loss=0.830 Prec@1=78.859 Prec@5=93.697 rate=2.19 Hz, eta=0:18:16, total=0:00:46, wall=15:52 IST
=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.481 DataTime=0.284 Loss=0.830 Prec@1=78.859 Prec@5=93.697 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=15:52 IST
=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.481 DataTime=0.284 Loss=0.830 Prec@1=78.859 Prec@5=93.697 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=15:53 IST
=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.473 DataTime=0.277 Loss=0.831 Prec@1=78.917 Prec@5=93.662 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=15:53 IST
=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.473 DataTime=0.277 Loss=0.831 Prec@1=78.917 Prec@5=93.662 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=15:53 IST
=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.473 DataTime=0.277 Loss=0.831 Prec@1=78.917 Prec@5=93.662 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=15:54 IST
=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.472 DataTime=0.275 Loss=0.830 Prec@1=78.965 Prec@5=93.653 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=15:54 IST
=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.472 DataTime=0.275 Loss=0.830 Prec@1=78.965 Prec@5=93.653 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=15:54 IST
=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.472 DataTime=0.275 Loss=0.830 Prec@1=78.965 Prec@5=93.653 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=15:54 IST
=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.469 DataTime=0.272 Loss=0.831 Prec@1=78.990 Prec@5=93.652 rate=2.17 Hz, eta=0:16:08, total=0:03:04, wall=15:54 IST
=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.469 DataTime=0.272 Loss=0.831 Prec@1=78.990 Prec@5=93.652 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=15:54 IST
=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.469 DataTime=0.272 Loss=0.831 Prec@1=78.990 Prec@5=93.652 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=15:55 IST
=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.270 Loss=0.832 Prec@1=78.984 Prec@5=93.633 rate=2.18 Hz, eta=0:15:19, total=0:03:50, wall=15:55 IST
=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.270 Loss=0.832 Prec@1=78.984 Prec@5=93.633 rate=2.18 Hz, eta=0:14:33, total=0:04:35, wall=15:55 IST
=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.270 Loss=0.832 Prec@1=78.984 Prec@5=93.633 rate=2.18 Hz, eta=0:14:33, total=0:04:35, wall=15:56 IST
=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.833 Prec@1=78.950 Prec@5=93.614 rate=2.18 Hz, eta=0:14:33, total=0:04:35, wall=15:56 IST
=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.833 Prec@1=78.950 Prec@5=93.614 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=15:56 IST
=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.833 Prec@1=78.950 Prec@5=93.614 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=15:57 IST
=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.832 Prec@1=78.960 Prec@5=93.617 rate=2.17 Hz, eta=0:13:50, total=0:05:23, wall=15:57 IST
=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.832 Prec@1=78.960 Prec@5=93.617 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=15:57 IST
=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.832 Prec@1=78.960 Prec@5=93.617 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=15:57 IST
=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.834 Prec@1=78.943 Prec@5=93.592 rate=2.16 Hz, eta=0:13:06, total=0:06:10, wall=15:57 IST
=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.834 Prec@1=78.943 Prec@5=93.592 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=15:57 IST
=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.271 Loss=0.834 Prec@1=78.943 Prec@5=93.592 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=15:58 IST
=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.923 Prec@5=93.587 rate=2.16 Hz, eta=0:12:21, total=0:06:56, wall=15:58 IST
=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.923 Prec@5=93.587 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=15:58 IST
=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.923 Prec@5=93.587 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=15:59 IST
=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.940 Prec@5=93.578 rate=2.16 Hz, eta=0:11:35, total=0:07:43, wall=15:59 IST
=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.940 Prec@5=93.578 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=15:59 IST
=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.940 Prec@5=93.578 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=16:00 IST
=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.967 Prec@5=93.588 rate=2.16 Hz, eta=0:10:50, total=0:08:30, wall=16:00 IST
=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.967 Prec@5=93.588 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:00 IST
=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.967 Prec@5=93.588 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:01 IST
=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.945 Prec@5=93.587 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:01 IST
=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.945 Prec@5=93.587 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=16:01 IST
=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.834 Prec@1=78.945 Prec@5=93.587 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=16:01 IST
=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.835 Prec@1=78.918 Prec@5=93.580 rate=2.15 Hz, eta=0:09:17, total=0:10:03, wall=16:01 IST
=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.835 Prec@1=78.918 Prec@5=93.580 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:01 IST
=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.835 Prec@1=78.918 Prec@5=93.580 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:02 IST
=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.836 Prec@1=78.894 Prec@5=93.582 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:02 IST
=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.836 Prec@1=78.894 Prec@5=93.582 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=16:02 IST
=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.468 DataTime=0.272 Loss=0.836 Prec@1=78.894 Prec@5=93.582 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=16:03 IST
=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.884 Prec@5=93.569 rate=2.15 Hz, eta=0:07:45, total=0:11:37, wall=16:03 IST
=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.884 Prec@5=93.569 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=16:03 IST
=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.884 Prec@5=93.569 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=16:04 IST
=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.570 rate=2.15 Hz, eta=0:06:59, total=0:12:23, wall=16:04 IST
=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.570 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=16:04 IST
=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.570 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=16:04 IST
=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.837 Prec@1=78.864 Prec@5=93.569 rate=2.15 Hz, eta=0:06:12, total=0:13:10, wall=16:04 IST
=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.837 Prec@1=78.864 Prec@5=93.569 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=16:04 IST
=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.837 Prec@1=78.864 Prec@5=93.569 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=16:05 IST
=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.577 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=16:05 IST
=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.577 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=16:05 IST
=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.577 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=16:06 IST
=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.572 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=16:06 IST
=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.572 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=16:06 IST
=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.272 Loss=0.836 Prec@1=78.877 Prec@5=93.572 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=16:07 IST
=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.836 Prec@1=78.865 Prec@5=93.566 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=16:07 IST
=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.836 Prec@1=78.865 Prec@5=93.566 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=16:07 IST
=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.836 Prec@1=78.865 Prec@5=93.566 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=16:08 IST
=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.844 Prec@5=93.564 rate=2.15 Hz, eta=0:03:06, total=0:16:15, wall=16:08 IST
=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.844 Prec@5=93.564 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:08 IST
=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.844 Prec@5=93.564 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:08 IST
=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.842 Prec@5=93.559 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:08 IST
=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.842 Prec@5=93.559 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=16:08 IST
=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.842 Prec@5=93.559 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=16:09 IST
=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.852 Prec@5=93.562 rate=2.15 Hz, eta=0:01:33, total=0:17:49, wall=16:09 IST
=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.852 Prec@5=93.562 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:09 IST
=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.467 DataTime=0.271 Loss=0.837 Prec@1=78.852 Prec@5=93.562 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:10 IST
=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.466 DataTime=0.271 Loss=0.837 Prec@1=78.851 Prec@5=93.560 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:10 IST
=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.466 DataTime=0.271 Loss=0.837 Prec@1=78.851 Prec@5=93.560 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=16:10 IST
=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.466 DataTime=0.271 Loss=0.837 Prec@1=78.851 Prec@5=93.560 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=16:10 IST
=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.466 DataTime=0.271 Loss=0.837 Prec@1=78.849 Prec@5=93.560 rate=2.15 Hz, eta=0:00:00, total=0:19:21, wall=16:10 IST
=> training   100.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.466 DataTime=0.271 Loss=0.837 Prec@1=78.849 Prec@5=93.560 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=16:10 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 0.00% of 1x98...Epoch=141/150 LR=0.00109 Time=6.201 Loss=0.634 Prec@1=84.180 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST
=> validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=6.201 Loss=0.634 Prec@1=84.180 Prec@5=95.508 rate=11542.68 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST
** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=6.201 Loss=0.634 Prec@1=84.180 Prec@5=95.508 rate=11542.68 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST
** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=0.543 Loss=1.138 Prec@1=72.070 Prec@5=90.490 rate=11542.68 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST
** validation 100.00% of 1x98...Epoch=141/150 LR=0.00109 Time=0.543 Loss=1.138 Prec@1=72.070 Prec@5=90.490 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=16:11 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST
=> training   0.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=4.775 DataTime=4.491 Loss=0.798 Prec@1=80.664 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST
=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=4.775 DataTime=4.491 Loss=0.798 Prec@1=80.664 Prec@5=93.555 rate=7265.33 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST
=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=4.775 DataTime=4.491 Loss=0.798 Prec@1=80.664 Prec@5=93.555 rate=7265.33 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST
=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.500 DataTime=0.303 Loss=0.832 Prec@1=79.103 Prec@5=93.611 rate=7265.33 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST
=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.500 DataTime=0.303 Loss=0.832 Prec@1=79.103 Prec@5=93.611 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:12 IST
=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.500 DataTime=0.303 Loss=0.832 Prec@1=79.103 Prec@5=93.611 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:12 IST
=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.478 DataTime=0.281 Loss=0.828 Prec@1=79.165 Prec@5=93.682 rate=2.21 Hz, eta=0:18:08, total=0:00:45, wall=16:12 IST
=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.478 DataTime=0.281 Loss=0.828 Prec@1=79.165 Prec@5=93.682 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=16:12 IST
=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.478 DataTime=0.281 Loss=0.828 Prec@1=79.165 Prec@5=93.682 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=16:13 IST
=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.469 DataTime=0.274 Loss=0.828 Prec@1=79.148 Prec@5=93.662 rate=2.20 Hz, eta=0:17:26, total=0:01:31, wall=16:13 IST
=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.469 DataTime=0.274 Loss=0.828 Prec@1=79.148 Prec@5=93.662 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=16:13 IST
=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.469 DataTime=0.274 Loss=0.828 Prec@1=79.148 Prec@5=93.662 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=16:14 IST
=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.470 DataTime=0.275 Loss=0.832 Prec@1=79.008 Prec@5=93.639 rate=2.21 Hz, eta=0:16:37, total=0:02:16, wall=16:14 IST
=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.470 DataTime=0.275 Loss=0.832 Prec@1=79.008 Prec@5=93.639 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=16:14 IST
=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.470 DataTime=0.275 Loss=0.832 Prec@1=79.008 Prec@5=93.639 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=16:15 IST
=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.467 DataTime=0.272 Loss=0.833 Prec@1=78.991 Prec@5=93.637 rate=2.18 Hz, eta=0:16:02, total=0:03:03, wall=16:15 IST
=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.467 DataTime=0.272 Loss=0.833 Prec@1=78.991 Prec@5=93.637 rate=2.19 Hz, eta=0:15:16, total=0:03:49, wall=16:15 IST
=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.467 DataTime=0.272 Loss=0.833 Prec@1=78.991 Prec@5=93.637 rate=2.19 Hz, eta=0:15:16, total=0:03:49, wall=16:15 IST
=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.465 DataTime=0.270 Loss=0.833 Prec@1=78.937 Prec@5=93.611 rate=2.19 Hz, eta=0:15:16, total=0:03:49, wall=16:15 IST
=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.465 DataTime=0.270 Loss=0.833 Prec@1=78.937 Prec@5=93.611 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=16:15 IST
=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.465 DataTime=0.270 Loss=0.833 Prec@1=78.937 Prec@5=93.611 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=16:16 IST
=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.464 DataTime=0.269 Loss=0.833 Prec@1=78.928 Prec@5=93.603 rate=2.19 Hz, eta=0:14:28, total=0:04:34, wall=16:16 IST
=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.464 DataTime=0.269 Loss=0.833 Prec@1=78.928 Prec@5=93.603 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=16:16 IST
=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.464 DataTime=0.269 Loss=0.833 Prec@1=78.928 Prec@5=93.603 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=16:17 IST
=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.833 Prec@1=78.906 Prec@5=93.616 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=16:17 IST
=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.833 Prec@1=78.906 Prec@5=93.616 rate=2.19 Hz, eta=0:12:58, total=0:06:06, wall=16:17 IST
=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.833 Prec@1=78.906 Prec@5=93.616 rate=2.19 Hz, eta=0:12:58, total=0:06:06, wall=16:18 IST
=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.833 Prec@1=78.883 Prec@5=93.624 rate=2.19 Hz, eta=0:12:58, total=0:06:06, wall=16:18 IST
=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.833 Prec@1=78.883 Prec@5=93.624 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=16:18 IST
=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.833 Prec@1=78.883 Prec@5=93.624 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=16:19 IST
=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.834 Prec@1=78.900 Prec@5=93.610 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=16:19 IST
=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.834 Prec@1=78.900 Prec@5=93.610 rate=2.18 Hz, eta=0:11:28, total=0:07:39, wall=16:19 IST
=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.834 Prec@1=78.900 Prec@5=93.610 rate=2.18 Hz, eta=0:11:28, total=0:07:39, wall=16:19 IST
=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.909 Prec@5=93.615 rate=2.18 Hz, eta=0:11:28, total=0:07:39, wall=16:19 IST
=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.909 Prec@5=93.615 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=16:19 IST
=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.909 Prec@5=93.615 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=16:20 IST
=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.916 Prec@5=93.613 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=16:20 IST
=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.916 Prec@5=93.613 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=16:20 IST
=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.916 Prec@5=93.613 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=16:21 IST
=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.266 Loss=0.834 Prec@1=78.922 Prec@5=93.609 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=16:21 IST
=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.266 Loss=0.834 Prec@1=78.922 Prec@5=93.609 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=16:21 IST
=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.266 Loss=0.834 Prec@1=78.922 Prec@5=93.609 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=16:22 IST
=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.927 Prec@5=93.607 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=16:22 IST
=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.927 Prec@5=93.607 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=16:22 IST
=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.927 Prec@5=93.607 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=16:22 IST
=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.910 Prec@5=93.603 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=16:22 IST
=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.910 Prec@5=93.603 rate=2.18 Hz, eta=0:07:39, total=0:11:29, wall=16:22 IST
=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.834 Prec@1=78.910 Prec@5=93.603 rate=2.18 Hz, eta=0:07:39, total=0:11:29, wall=16:23 IST
=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.899 Prec@5=93.605 rate=2.18 Hz, eta=0:07:39, total=0:11:29, wall=16:23 IST
=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.899 Prec@5=93.605 rate=2.17 Hz, eta=0:06:54, total=0:12:16, wall=16:23 IST
=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.267 Loss=0.834 Prec@1=78.899 Prec@5=93.605 rate=2.17 Hz, eta=0:06:54, total=0:12:16, wall=16:24 IST
=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.605 rate=2.17 Hz, eta=0:06:54, total=0:12:16, wall=16:24 IST
=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.605 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=16:24 IST
=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.605 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=16:25 IST
=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.835 Prec@1=78.895 Prec@5=93.598 rate=2.17 Hz, eta=0:06:09, total=0:13:02, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.835 Prec@1=78.895 Prec@5=93.598 rate=2.17 Hz, eta=0:05:22, total=0:13:48, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.462 DataTime=0.267 Loss=0.835 Prec@1=78.895 Prec@5=93.598 rate=2.17 Hz, eta=0:05:22, total=0:13:48, wall=16:25 IST
=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.892 Prec@5=93.590 rate=2.17 Hz, eta=0:05:22, total=0:13:48, wall=16:25 IST
=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.892 Prec@5=93.590 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=16:25 IST
=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.892 Prec@5=93.590 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=16:26 IST
=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.585 rate=2.17 Hz, eta=0:04:37, total=0:14:35, wall=16:26 IST
=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.585 rate=2.17 Hz, eta=0:03:51, total=0:15:21, wall=16:26 IST
=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.585 rate=2.17 Hz, eta=0:03:51, total=0:15:21, wall=16:27 IST
=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.902 Prec@5=93.580 rate=2.17 Hz, eta=0:03:51, total=0:15:21, wall=16:27 IST
=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.902 Prec@5=93.580 rate=2.17 Hz, eta=0:03:05, total=0:16:08, wall=16:27 IST
=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.902 Prec@5=93.580 rate=2.17 Hz, eta=0:03:05, total=0:16:08, wall=16:28 IST
=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.583 rate=2.17 Hz, eta=0:03:05, total=0:16:08, wall=16:28 IST
=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.583 rate=2.17 Hz, eta=0:02:19, total=0:16:54, wall=16:28 IST
=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.897 Prec@5=93.583 rate=2.17 Hz, eta=0:02:19, total=0:16:54, wall=16:29 IST
=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.579 rate=2.17 Hz, eta=0:02:19, total=0:16:54, wall=16:29 IST
=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.579 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=16:29 IST
=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.895 Prec@5=93.579 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=16:29 IST
=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.894 Prec@5=93.569 rate=2.17 Hz, eta=0:01:33, total=0:17:41, wall=16:29 IST
=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.894 Prec@5=93.569 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=16:29 IST
=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.835 Prec@1=78.894 Prec@5=93.569 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=16:30 IST
=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.836 Prec@1=78.894 Prec@5=93.563 rate=2.17 Hz, eta=0:00:47, total=0:18:27, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.836 Prec@1=78.894 Prec@5=93.563 rate=2.17 Hz, eta=0:00:00, total=0:19:13, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.836 Prec@1=78.894 Prec@5=93.563 rate=2.17 Hz, eta=0:00:00, total=0:19:13, wall=16:30 IST
=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.836 Prec@1=78.894 Prec@5=93.562 rate=2.17 Hz, eta=0:00:00, total=0:19:13, wall=16:30 IST
=> training   100.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.463 DataTime=0.268 Loss=0.836 Prec@1=78.894 Prec@5=93.562 rate=2.17 Hz, eta=0:00:00, total=0:19:14, wall=16:30 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 0.00% of 1x98...Epoch=142/150 LR=0.00089 Time=6.522 Loss=0.645 Prec@1=83.594 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=6.522 Loss=0.645 Prec@1=83.594 Prec@5=95.508 rate=3231.61 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST
** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=6.522 Loss=0.645 Prec@1=83.594 Prec@5=95.508 rate=3231.61 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=0.553 Loss=1.139 Prec@1=72.042 Prec@5=90.528 rate=3231.61 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
** validation 100.00% of 1x98...Epoch=142/150 LR=0.00089 Time=0.553 Loss=1.139 Prec@1=72.042 Prec@5=90.528 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=16:31 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.565 DataTime=4.349 Loss=0.887 Prec@1=78.711 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=16:31 IST
=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.565 DataTime=4.349 Loss=0.887 Prec@1=78.711 Prec@5=92.188 rate=6736.00 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.565 DataTime=4.349 Loss=0.887 Prec@1=78.711 Prec@5=92.188 rate=6736.00 Hz, eta=0:00:00, total=0:00:00, wall=16:32 IST
=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.501 DataTime=0.305 Loss=0.838 Prec@1=79.132 Prec@5=93.402 rate=6736.00 Hz, eta=0:00:00, total=0:00:00, wall=16:32 IST
=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.501 DataTime=0.305 Loss=0.838 Prec@1=79.132 Prec@5=93.402 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=16:32 IST
=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.501 DataTime=0.305 Loss=0.838 Prec@1=79.132 Prec@5=93.402 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=16:33 IST
=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.482 DataTime=0.287 Loss=0.828 Prec@1=79.213 Prec@5=93.574 rate=2.19 Hz, eta=0:18:14, total=0:00:46, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.482 DataTime=0.287 Loss=0.828 Prec@1=79.213 Prec@5=93.574 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.482 DataTime=0.287 Loss=0.828 Prec@1=79.213 Prec@5=93.574 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=16:33 IST
=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.476 DataTime=0.282 Loss=0.832 Prec@1=79.085 Prec@5=93.583 rate=2.18 Hz, eta=0:17:37, total=0:01:32, wall=16:33 IST
=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.476 DataTime=0.282 Loss=0.832 Prec@1=79.085 Prec@5=93.583 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=16:33 IST
=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.476 DataTime=0.282 Loss=0.832 Prec@1=79.085 Prec@5=93.583 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=16:34 IST
=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.279 Loss=0.831 Prec@1=79.125 Prec@5=93.587 rate=2.17 Hz, eta=0:16:55, total=0:02:18, wall=16:34 IST
=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.279 Loss=0.831 Prec@1=79.125 Prec@5=93.587 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=16:34 IST
=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.279 Loss=0.831 Prec@1=79.125 Prec@5=93.587 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=16:35 IST
=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.278 Loss=0.829 Prec@1=79.144 Prec@5=93.614 rate=2.16 Hz, eta=0:16:11, total=0:03:05, wall=16:35 IST
=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.278 Loss=0.829 Prec@1=79.144 Prec@5=93.614 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=16:35 IST
=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.473 DataTime=0.278 Loss=0.829 Prec@1=79.144 Prec@5=93.614 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=16:36 IST
=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.470 DataTime=0.275 Loss=0.830 Prec@1=79.102 Prec@5=93.601 rate=2.16 Hz, eta=0:15:28, total=0:03:52, wall=16:36 IST
=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.470 DataTime=0.275 Loss=0.830 Prec@1=79.102 Prec@5=93.601 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=16:36 IST
=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.470 DataTime=0.275 Loss=0.830 Prec@1=79.102 Prec@5=93.601 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=16:37 IST
=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.469 DataTime=0.274 Loss=0.829 Prec@1=79.095 Prec@5=93.620 rate=2.16 Hz, eta=0:14:40, total=0:04:38, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.469 DataTime=0.274 Loss=0.829 Prec@1=79.095 Prec@5=93.620 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.469 DataTime=0.274 Loss=0.829 Prec@1=79.095 Prec@5=93.620 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=16:37 IST
=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.098 Prec@5=93.605 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=16:37 IST
=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.098 Prec@5=93.605 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:37 IST
=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.098 Prec@5=93.605 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:38 IST
=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.829 Prec@1=79.091 Prec@5=93.630 rate=2.17 Hz, eta=0:13:05, total=0:06:09, wall=16:38 IST
=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.829 Prec@1=79.091 Prec@5=93.630 rate=2.16 Hz, eta=0:12:19, total=0:06:56, wall=16:38 IST
=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.829 Prec@1=79.091 Prec@5=93.630 rate=2.16 Hz, eta=0:12:19, total=0:06:56, wall=16:39 IST
=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.271 Loss=0.830 Prec@1=79.072 Prec@5=93.624 rate=2.16 Hz, eta=0:12:19, total=0:06:56, wall=16:39 IST
=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.271 Loss=0.830 Prec@1=79.072 Prec@5=93.624 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=16:39 IST
=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.271 Loss=0.830 Prec@1=79.072 Prec@5=93.624 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=16:40 IST
=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.468 DataTime=0.273 Loss=0.830 Prec@1=79.069 Prec@5=93.624 rate=2.16 Hz, eta=0:11:34, total=0:07:42, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.468 DataTime=0.273 Loss=0.830 Prec@1=79.069 Prec@5=93.624 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.468 DataTime=0.273 Loss=0.830 Prec@1=79.069 Prec@5=93.624 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=16:40 IST
=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.039 Prec@5=93.611 rate=2.16 Hz, eta=0:10:49, total=0:08:30, wall=16:40 IST
=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.039 Prec@5=93.611 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:40 IST
=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.039 Prec@5=93.611 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:41 IST
=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.831 Prec@1=79.025 Prec@5=93.622 rate=2.16 Hz, eta=0:10:03, total=0:09:16, wall=16:41 IST
=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.831 Prec@1=79.025 Prec@5=93.622 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=16:41 IST
=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.831 Prec@1=79.025 Prec@5=93.622 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=16:42 IST
=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.009 Prec@5=93.624 rate=2.16 Hz, eta=0:09:16, total=0:10:02, wall=16:42 IST
=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.009 Prec@5=93.624 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:42 IST
=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.009 Prec@5=93.624 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:43 IST
=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.021 Prec@5=93.630 rate=2.15 Hz, eta=0:08:31, total=0:10:50, wall=16:43 IST
=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.021 Prec@5=93.630 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=16:43 IST
=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.021 Prec@5=93.630 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=16:44 IST
=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.630 rate=2.16 Hz, eta=0:07:44, total=0:11:35, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.630 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.630 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=16:44 IST
=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.037 Prec@5=93.632 rate=2.15 Hz, eta=0:06:58, total=0:12:23, wall=16:44 IST
=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.037 Prec@5=93.632 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=16:44 IST
=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.037 Prec@5=93.632 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=16:45 IST
=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.041 Prec@5=93.631 rate=2.16 Hz, eta=0:06:11, total=0:13:08, wall=16:45 IST
=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.041 Prec@5=93.631 rate=2.16 Hz, eta=0:05:25, total=0:13:53, wall=16:45 IST
=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.041 Prec@5=93.631 rate=2.16 Hz, eta=0:05:25, total=0:13:53, wall=16:46 IST
=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.829 Prec@1=79.051 Prec@5=93.639 rate=2.16 Hz, eta=0:05:25, total=0:13:53, wall=16:46 IST
=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.829 Prec@1=79.051 Prec@5=93.639 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=16:46 IST
=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.829 Prec@1=79.051 Prec@5=93.639 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=16:47 IST
=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.047 Prec@5=93.634 rate=2.16 Hz, eta=0:04:39, total=0:14:41, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.047 Prec@5=93.634 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.047 Prec@5=93.634 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=16:47 IST
=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.035 Prec@5=93.630 rate=2.16 Hz, eta=0:03:52, total=0:15:27, wall=16:47 IST
=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.035 Prec@5=93.630 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=16:47 IST
=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.271 Loss=0.830 Prec@1=79.035 Prec@5=93.630 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=16:48 IST
=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.028 Prec@5=93.620 rate=2.16 Hz, eta=0:03:06, total=0:16:14, wall=16:48 IST
=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.028 Prec@5=93.620 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:48 IST
=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.466 DataTime=0.272 Loss=0.830 Prec@1=79.028 Prec@5=93.620 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:49 IST
=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.013 Prec@5=93.622 rate=2.15 Hz, eta=0:02:20, total=0:17:02, wall=16:49 IST
=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.013 Prec@5=93.622 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=16:49 IST
=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.831 Prec@1=79.013 Prec@5=93.622 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=16:50 IST
=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.629 rate=2.15 Hz, eta=0:01:33, total=0:17:48, wall=16:50 IST
=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.629 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:50 IST
=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.272 Loss=0.830 Prec@1=79.025 Prec@5=93.629 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:51 IST
=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.010 Prec@5=93.627 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.010 Prec@5=93.627 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.010 Prec@5=93.627 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:51 IST
=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.009 Prec@5=93.627 rate=2.15 Hz, eta=0:00:00, total=0:19:24, wall=16:51 IST
=> training   100.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.467 DataTime=0.273 Loss=0.831 Prec@1=79.009 Prec@5=93.627 rate=2.15 Hz, eta=0:00:00, total=0:19:25, wall=16:51 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 0.00% of 1x98...Epoch=143/150 LR=0.00070 Time=7.031 Loss=0.645 Prec@1=83.984 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST
=> validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.031 Loss=0.645 Prec@1=83.984 Prec@5=95.508 rate=5708.84 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.031 Loss=0.645 Prec@1=83.984 Prec@5=95.508 rate=5708.84 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=0.557 Loss=1.137 Prec@1=72.096 Prec@5=90.504 rate=5708.84 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST
** validation 100.00% of 1x98...Epoch=143/150 LR=0.00070 Time=0.557 Loss=1.137 Prec@1=72.096 Prec@5=90.504 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=16:51 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=4.602 DataTime=4.363 Loss=0.911 Prec@1=80.078 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=4.602 DataTime=4.363 Loss=0.911 Prec@1=80.078 Prec@5=91.992 rate=7217.56 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=4.602 DataTime=4.363 Loss=0.911 Prec@1=80.078 Prec@5=91.992 rate=7217.56 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.503 DataTime=0.307 Loss=0.823 Prec@1=79.223 Prec@5=93.721 rate=7217.56 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST
=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.503 DataTime=0.307 Loss=0.823 Prec@1=79.223 Prec@5=93.721 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=16:52 IST
=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.503 DataTime=0.307 Loss=0.823 Prec@1=79.223 Prec@5=93.721 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=16:53 IST
=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.481 DataTime=0.285 Loss=0.824 Prec@1=79.206 Prec@5=93.783 rate=2.19 Hz, eta=0:18:18, total=0:00:46, wall=16:53 IST
=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.481 DataTime=0.285 Loss=0.824 Prec@1=79.206 Prec@5=93.783 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=16:53 IST
=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.481 DataTime=0.285 Loss=0.824 Prec@1=79.206 Prec@5=93.783 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=16:54 IST
=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.478 DataTime=0.282 Loss=0.828 Prec@1=79.075 Prec@5=93.723 rate=2.18 Hz, eta=0:17:34, total=0:01:32, wall=16:54 IST
=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.478 DataTime=0.282 Loss=0.828 Prec@1=79.075 Prec@5=93.723 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=16:54 IST
=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.478 DataTime=0.282 Loss=0.828 Prec@1=79.075 Prec@5=93.723 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=16:55 IST
=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.828 Prec@1=79.090 Prec@5=93.715 rate=2.16 Hz, eta=0:16:58, total=0:02:19, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.828 Prec@1=79.090 Prec@5=93.715 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.828 Prec@1=79.090 Prec@5=93.715 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=16:55 IST
=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.072 Prec@5=93.677 rate=2.16 Hz, eta=0:16:12, total=0:03:05, wall=16:55 IST
=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.072 Prec@5=93.677 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:55 IST
=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.072 Prec@5=93.677 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:56 IST
=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.114 Prec@5=93.683 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=16:56 IST
=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.114 Prec@5=93.683 rate=2.15 Hz, eta=0:14:46, total=0:04:39, wall=16:56 IST
=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.278 Loss=0.829 Prec@1=79.114 Prec@5=93.683 rate=2.15 Hz, eta=0:14:46, total=0:04:39, wall=16:57 IST
=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.827 Prec@1=79.160 Prec@5=93.704 rate=2.15 Hz, eta=0:14:46, total=0:04:39, wall=16:57 IST
=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.827 Prec@1=79.160 Prec@5=93.704 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:57 IST
=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.827 Prec@1=79.160 Prec@5=93.704 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:58 IST
=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.475 DataTime=0.280 Loss=0.828 Prec@1=79.117 Prec@5=93.685 rate=2.14 Hz, eta=0:14:02, total=0:05:27, wall=16:58 IST
=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.475 DataTime=0.280 Loss=0.828 Prec@1=79.117 Prec@5=93.685 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=16:58 IST
=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.475 DataTime=0.280 Loss=0.828 Prec@1=79.117 Prec@5=93.685 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=16:59 IST
=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.827 Prec@1=79.118 Prec@5=93.692 rate=2.13 Hz, eta=0:13:18, total=0:06:15, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.827 Prec@1=79.118 Prec@5=93.692 rate=2.12 Hz, eta=0:12:34, total=0:07:04, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.827 Prec@1=79.118 Prec@5=93.692 rate=2.12 Hz, eta=0:12:34, total=0:07:04, wall=16:59 IST
=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.826 Prec@1=79.143 Prec@5=93.690 rate=2.12 Hz, eta=0:12:34, total=0:07:04, wall=16:59 IST
=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.826 Prec@1=79.143 Prec@5=93.690 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=16:59 IST
=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.476 DataTime=0.281 Loss=0.826 Prec@1=79.143 Prec@5=93.690 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=17:00 IST
=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.280 Loss=0.826 Prec@1=79.146 Prec@5=93.687 rate=2.12 Hz, eta=0:11:47, total=0:07:51, wall=17:00 IST
=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.280 Loss=0.826 Prec@1=79.146 Prec@5=93.687 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=17:00 IST
=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.280 Loss=0.826 Prec@1=79.146 Prec@5=93.687 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=17:01 IST
=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.826 Prec@1=79.149 Prec@5=93.684 rate=2.13 Hz, eta=0:10:59, total=0:08:37, wall=17:01 IST
=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.826 Prec@1=79.149 Prec@5=93.684 rate=2.13 Hz, eta=0:10:11, total=0:09:24, wall=17:01 IST
=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.474 DataTime=0.279 Loss=0.826 Prec@1=79.149 Prec@5=93.684 rate=2.13 Hz, eta=0:10:11, total=0:09:24, wall=17:02 IST
=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.473 DataTime=0.278 Loss=0.826 Prec@1=79.149 Prec@5=93.682 rate=2.13 Hz, eta=0:10:11, total=0:09:24, wall=17:02 IST
=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.473 DataTime=0.278 Loss=0.826 Prec@1=79.149 Prec@5=93.682 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=17:02 IST
=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.473 DataTime=0.278 Loss=0.826 Prec@1=79.149 Prec@5=93.682 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=17:03 IST
=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.151 Prec@5=93.681 rate=2.13 Hz, eta=0:09:23, total=0:10:10, wall=17:03 IST
=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.151 Prec@5=93.681 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=17:03 IST
=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.151 Prec@5=93.681 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=17:03 IST
=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.153 Prec@5=93.686 rate=2.13 Hz, eta=0:08:37, total=0:10:57, wall=17:03 IST
=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.153 Prec@5=93.686 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=17:03 IST
=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.153 Prec@5=93.686 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=17:04 IST
=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.159 Prec@5=93.692 rate=2.13 Hz, eta=0:07:50, total=0:11:44, wall=17:04 IST
=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.159 Prec@5=93.692 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=17:04 IST
=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.159 Prec@5=93.692 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=17:05 IST
=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.166 Prec@5=93.693 rate=2.13 Hz, eta=0:07:03, total=0:12:31, wall=17:05 IST
=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.166 Prec@5=93.693 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=17:05 IST
=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.472 DataTime=0.278 Loss=0.826 Prec@1=79.166 Prec@5=93.693 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=17:06 IST
=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.168 Prec@5=93.693 rate=2.13 Hz, eta=0:06:16, total=0:13:18, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.168 Prec@5=93.693 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.168 Prec@5=93.693 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=17:06 IST
=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.169 Prec@5=93.684 rate=2.13 Hz, eta=0:05:29, total=0:14:04, wall=17:06 IST
=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.169 Prec@5=93.684 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=17:06 IST
=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.277 Loss=0.826 Prec@1=79.169 Prec@5=93.684 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=17:07 IST
=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.168 Prec@5=93.683 rate=2.13 Hz, eta=0:04:42, total=0:14:51, wall=17:07 IST
=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.168 Prec@5=93.683 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=17:07 IST
=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.168 Prec@5=93.683 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=17:08 IST
=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.166 Prec@5=93.682 rate=2.13 Hz, eta=0:03:55, total=0:15:37, wall=17:08 IST
=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.166 Prec@5=93.682 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=17:08 IST
=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.826 Prec@1=79.166 Prec@5=93.682 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=17:09 IST
=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.142 Prec@5=93.678 rate=2.13 Hz, eta=0:03:08, total=0:16:24, wall=17:09 IST
=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.142 Prec@5=93.678 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=17:09 IST
=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.142 Prec@5=93.678 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=17:10 IST
=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.126 Prec@5=93.666 rate=2.13 Hz, eta=0:02:21, total=0:17:12, wall=17:10 IST
=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.126 Prec@5=93.666 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=17:10 IST
=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.827 Prec@1=79.126 Prec@5=93.666 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=17:10 IST
=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.828 Prec@1=79.116 Prec@5=93.662 rate=2.13 Hz, eta=0:01:34, total=0:17:59, wall=17:10 IST
=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.828 Prec@1=79.116 Prec@5=93.662 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=17:10 IST
=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.471 DataTime=0.276 Loss=0.828 Prec@1=79.116 Prec@5=93.662 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=17:11 IST
=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.470 DataTime=0.275 Loss=0.828 Prec@1=79.112 Prec@5=93.660 rate=2.13 Hz, eta=0:00:47, total=0:18:45, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.470 DataTime=0.275 Loss=0.828 Prec@1=79.112 Prec@5=93.660 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.470 DataTime=0.275 Loss=0.828 Prec@1=79.112 Prec@5=93.660 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=17:11 IST
=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.470 DataTime=0.275 Loss=0.828 Prec@1=79.111 Prec@5=93.659 rate=2.13 Hz, eta=0:00:00, total=0:19:31, wall=17:11 IST
=> training   100.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.470 DataTime=0.275 Loss=0.828 Prec@1=79.111 Prec@5=93.659 rate=2.14 Hz, eta=0:00:00, total=0:19:32, wall=17:11 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 0.00% of 1x98...Epoch=144/150 LR=0.00054 Time=6.625 Loss=0.643 Prec@1=83.789 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=17:11 IST
=> validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=6.625 Loss=0.643 Prec@1=83.789 Prec@5=95.117 rate=3954.48 Hz, eta=0:00:00, total=0:00:00, wall=17:11 IST
** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=6.625 Loss=0.643 Prec@1=83.789 Prec@5=95.117 rate=3954.48 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=0.555 Loss=1.137 Prec@1=72.086 Prec@5=90.456 rate=3954.48 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
** validation 100.00% of 1x98...Epoch=144/150 LR=0.00054 Time=0.555 Loss=1.137 Prec@1=72.086 Prec@5=90.456 rate=2.05 Hz, eta=0:00:00, total=0:00:47, wall=17:12 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=4.898 DataTime=4.680 Loss=0.792 Prec@1=79.297 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=17:12 IST
=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=4.898 DataTime=4.680 Loss=0.792 Prec@1=79.297 Prec@5=93.750 rate=8083.03 Hz, eta=0:00:00, total=0:00:00, wall=17:12 IST
=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=4.898 DataTime=4.680 Loss=0.792 Prec@1=79.297 Prec@5=93.750 rate=8083.03 Hz, eta=0:00:00, total=0:00:00, wall=17:13 IST
=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.493 DataTime=0.296 Loss=0.814 Prec@1=79.479 Prec@5=93.891 rate=8083.03 Hz, eta=0:00:00, total=0:00:00, wall=17:13 IST
=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.493 DataTime=0.296 Loss=0.814 Prec@1=79.479 Prec@5=93.891 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=17:13 IST
=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.493 DataTime=0.296 Loss=0.814 Prec@1=79.479 Prec@5=93.891 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=17:14 IST
=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.476 DataTime=0.277 Loss=0.820 Prec@1=79.402 Prec@5=93.762 rate=2.25 Hz, eta=0:17:46, total=0:00:44, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.476 DataTime=0.277 Loss=0.820 Prec@1=79.402 Prec@5=93.762 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.476 DataTime=0.277 Loss=0.820 Prec@1=79.402 Prec@5=93.762 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=17:14 IST
=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.473 DataTime=0.276 Loss=0.818 Prec@1=79.379 Prec@5=93.803 rate=2.22 Hz, eta=0:17:18, total=0:01:30, wall=17:14 IST
=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.473 DataTime=0.276 Loss=0.818 Prec@1=79.379 Prec@5=93.803 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=17:14 IST
=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.473 DataTime=0.276 Loss=0.818 Prec@1=79.379 Prec@5=93.803 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=17:15 IST
=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.470 DataTime=0.273 Loss=0.821 Prec@1=79.292 Prec@5=93.759 rate=2.19 Hz, eta=0:16:45, total=0:02:17, wall=17:15 IST
=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.470 DataTime=0.273 Loss=0.821 Prec@1=79.292 Prec@5=93.759 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=17:15 IST
=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.470 DataTime=0.273 Loss=0.821 Prec@1=79.292 Prec@5=93.759 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=17:16 IST
=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.822 Prec@1=79.248 Prec@5=93.739 rate=2.19 Hz, eta=0:16:01, total=0:03:03, wall=17:16 IST
=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.822 Prec@1=79.248 Prec@5=93.739 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=17:16 IST
=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.822 Prec@1=79.248 Prec@5=93.739 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=17:17 IST
=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.820 Prec@1=79.314 Prec@5=93.756 rate=2.18 Hz, eta=0:15:17, total=0:03:49, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.820 Prec@1=79.314 Prec@5=93.756 rate=2.18 Hz, eta=0:14:33, total=0:04:36, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.272 Loss=0.820 Prec@1=79.314 Prec@5=93.756 rate=2.18 Hz, eta=0:14:33, total=0:04:36, wall=17:17 IST
=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.822 Prec@1=79.294 Prec@5=93.725 rate=2.18 Hz, eta=0:14:33, total=0:04:36, wall=17:17 IST
=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.822 Prec@1=79.294 Prec@5=93.725 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:17 IST
=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.822 Prec@1=79.294 Prec@5=93.725 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:18 IST
=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.270 Loss=0.822 Prec@1=79.278 Prec@5=93.708 rate=2.18 Hz, eta=0:13:46, total=0:05:21, wall=17:18 IST
=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.270 Loss=0.822 Prec@1=79.278 Prec@5=93.708 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=17:18 IST
=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.270 Loss=0.822 Prec@1=79.278 Prec@5=93.708 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=17:19 IST
=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.271 Loss=0.823 Prec@1=79.242 Prec@5=93.696 rate=2.18 Hz, eta=0:13:01, total=0:06:07, wall=17:19 IST
=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.271 Loss=0.823 Prec@1=79.242 Prec@5=93.696 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=17:19 IST
=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.465 DataTime=0.271 Loss=0.823 Prec@1=79.242 Prec@5=93.696 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=17:20 IST
=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.825 Prec@1=79.203 Prec@5=93.685 rate=2.17 Hz, eta=0:12:16, total=0:06:54, wall=17:20 IST
=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.825 Prec@1=79.203 Prec@5=93.685 rate=2.17 Hz, eta=0:11:33, total=0:07:41, wall=17:20 IST
=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.466 DataTime=0.271 Loss=0.825 Prec@1=79.203 Prec@5=93.685 rate=2.17 Hz, eta=0:11:33, total=0:07:41, wall=17:21 IST
=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.179 Prec@5=93.683 rate=2.17 Hz, eta=0:11:33, total=0:07:41, wall=17:21 IST
=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.179 Prec@5=93.683 rate=2.16 Hz, eta=0:10:47, total=0:08:28, wall=17:21 IST
=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.179 Prec@5=93.683 rate=2.16 Hz, eta=0:10:47, total=0:08:28, wall=17:21 IST
=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.186 Prec@5=93.676 rate=2.16 Hz, eta=0:10:47, total=0:08:28, wall=17:21 IST
=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.186 Prec@5=93.676 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=17:21 IST
=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.186 Prec@5=93.676 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=17:22 IST
=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.165 Prec@5=93.684 rate=2.16 Hz, eta=0:10:02, total=0:09:15, wall=17:22 IST
=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.165 Prec@5=93.684 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:22 IST
=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.165 Prec@5=93.684 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:23 IST
=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.184 Prec@5=93.677 rate=2.16 Hz, eta=0:09:17, total=0:10:03, wall=17:23 IST
=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.184 Prec@5=93.677 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=17:23 IST
=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.825 Prec@1=79.184 Prec@5=93.677 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=17:24 IST
=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.166 Prec@5=93.675 rate=2.16 Hz, eta=0:08:30, total=0:10:49, wall=17:24 IST
=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.166 Prec@5=93.675 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=17:24 IST
=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.166 Prec@5=93.675 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=17:25 IST
=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.159 Prec@5=93.676 rate=2.15 Hz, eta=0:07:45, total=0:11:36, wall=17:25 IST
=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.159 Prec@5=93.676 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:25 IST
=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.159 Prec@5=93.676 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:25 IST
=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.137 Prec@5=93.673 rate=2.16 Hz, eta=0:06:58, total=0:12:22, wall=17:25 IST
=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.137 Prec@5=93.673 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=17:25 IST
=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.137 Prec@5=93.673 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=17:26 IST
=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.827 Prec@1=79.135 Prec@5=93.669 rate=2.15 Hz, eta=0:06:12, total=0:13:09, wall=17:26 IST
=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.827 Prec@1=79.135 Prec@5=93.669 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=17:26 IST
=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.827 Prec@1=79.135 Prec@5=93.669 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=17:27 IST
=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.273 Loss=0.826 Prec@1=79.161 Prec@5=93.674 rate=2.15 Hz, eta=0:05:26, total=0:13:56, wall=17:27 IST
=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.273 Loss=0.826 Prec@1=79.161 Prec@5=93.674 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:27 IST
=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.273 Loss=0.826 Prec@1=79.161 Prec@5=93.674 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:28 IST
=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.164 Prec@5=93.681 rate=2.15 Hz, eta=0:04:39, total=0:14:43, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.164 Prec@5=93.681 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.164 Prec@5=93.681 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=17:28 IST
=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.169 Prec@5=93.688 rate=2.15 Hz, eta=0:03:53, total=0:15:30, wall=17:28 IST
=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.169 Prec@5=93.688 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=17:28 IST
=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.825 Prec@1=79.169 Prec@5=93.688 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=17:29 IST
=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.159 Prec@5=93.682 rate=2.15 Hz, eta=0:03:07, total=0:16:17, wall=17:29 IST
=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.159 Prec@5=93.682 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:29 IST
=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.468 DataTime=0.273 Loss=0.826 Prec@1=79.159 Prec@5=93.682 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:30 IST
=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.158 Prec@5=93.684 rate=2.15 Hz, eta=0:02:20, total=0:17:04, wall=17:30 IST
=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.158 Prec@5=93.684 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=17:30 IST
=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.158 Prec@5=93.684 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=17:31 IST
=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.147 Prec@5=93.674 rate=2.15 Hz, eta=0:01:33, total=0:17:50, wall=17:31 IST
=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.147 Prec@5=93.674 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:31 IST
=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.147 Prec@5=93.674 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:32 IST
=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.152 Prec@5=93.678 rate=2.15 Hz, eta=0:00:47, total=0:18:36, wall=17:32 IST
=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.152 Prec@5=93.678 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:32 IST
=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.152 Prec@5=93.678 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:32 IST
=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.151 Prec@5=93.677 rate=2.15 Hz, eta=0:00:00, total=0:19:22, wall=17:32 IST
=> training   100.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.467 DataTime=0.272 Loss=0.826 Prec@1=79.151 Prec@5=93.677 rate=2.15 Hz, eta=0:00:00, total=0:19:23, wall=17:32 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 0.00% of 1x98...Epoch=145/150 LR=0.00039 Time=6.941 Loss=0.635 Prec@1=83.789 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST
=> validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=6.941 Loss=0.635 Prec@1=83.789 Prec@5=95.508 rate=8185.12 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=6.941 Loss=0.635 Prec@1=83.789 Prec@5=95.508 rate=8185.12 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=0.554 Loss=1.134 Prec@1=72.134 Prec@5=90.508 rate=8185.12 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST
** validation 100.00% of 1x98...Epoch=145/150 LR=0.00039 Time=0.554 Loss=1.134 Prec@1=72.134 Prec@5=90.508 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=17:32 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:33 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:33 IST
=> training   0.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=4.856 DataTime=4.619 Loss=0.814 Prec@1=76.953 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=17:33 IST
=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=4.856 DataTime=4.619 Loss=0.814 Prec@1=76.953 Prec@5=94.922 rate=7057.92 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST
=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=4.856 DataTime=4.619 Loss=0.814 Prec@1=76.953 Prec@5=94.922 rate=7057.92 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST
=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.495 DataTime=0.302 Loss=0.808 Prec@1=79.401 Prec@5=93.932 rate=7057.92 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST
=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.495 DataTime=0.302 Loss=0.808 Prec@1=79.401 Prec@5=93.932 rate=2.24 Hz, eta=0:17:53, total=0:00:45, wall=17:33 IST
=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.495 DataTime=0.302 Loss=0.808 Prec@1=79.401 Prec@5=93.932 rate=2.24 Hz, eta=0:17:53, total=0:00:45, wall=17:34 IST
=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.476 DataTime=0.282 Loss=0.812 Prec@1=79.365 Prec@5=93.847 rate=2.24 Hz, eta=0:17:53, total=0:00:45, wall=17:34 IST
=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.476 DataTime=0.282 Loss=0.812 Prec@1=79.365 Prec@5=93.847 rate=2.22 Hz, eta=0:17:19, total=0:01:30, wall=17:34 IST
=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.476 DataTime=0.282 Loss=0.812 Prec@1=79.365 Prec@5=93.847 rate=2.22 Hz, eta=0:17:19, total=0:01:30, wall=17:35 IST
=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.268 Prec@5=93.818 rate=2.22 Hz, eta=0:17:19, total=0:01:30, wall=17:35 IST
=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.268 Prec@5=93.818 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=17:35 IST
=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.268 Prec@5=93.818 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=17:36 IST
=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.468 DataTime=0.273 Loss=0.818 Prec@1=79.303 Prec@5=93.771 rate=2.20 Hz, eta=0:16:40, total=0:02:16, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.468 DataTime=0.273 Loss=0.818 Prec@1=79.303 Prec@5=93.771 rate=2.19 Hz, eta=0:15:57, total=0:03:02, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.468 DataTime=0.273 Loss=0.818 Prec@1=79.303 Prec@5=93.771 rate=2.19 Hz, eta=0:15:57, total=0:03:02, wall=17:36 IST
=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.465 DataTime=0.270 Loss=0.817 Prec@1=79.330 Prec@5=93.792 rate=2.19 Hz, eta=0:15:57, total=0:03:02, wall=17:36 IST
=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.465 DataTime=0.270 Loss=0.817 Prec@1=79.330 Prec@5=93.792 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=17:36 IST
=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.465 DataTime=0.270 Loss=0.817 Prec@1=79.330 Prec@5=93.792 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=17:37 IST
=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.466 DataTime=0.270 Loss=0.817 Prec@1=79.310 Prec@5=93.793 rate=2.20 Hz, eta=0:15:10, total=0:03:47, wall=17:37 IST
=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.466 DataTime=0.270 Loss=0.817 Prec@1=79.310 Prec@5=93.793 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:37 IST
=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.466 DataTime=0.270 Loss=0.817 Prec@1=79.310 Prec@5=93.793 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:38 IST
=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.269 Loss=0.818 Prec@1=79.312 Prec@5=93.779 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=17:38 IST
=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.269 Loss=0.818 Prec@1=79.312 Prec@5=93.779 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=17:38 IST
=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.269 Loss=0.818 Prec@1=79.312 Prec@5=93.779 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=17:39 IST
=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.268 Loss=0.820 Prec@1=79.269 Prec@5=93.763 rate=2.19 Hz, eta=0:13:44, total=0:05:20, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.268 Loss=0.820 Prec@1=79.269 Prec@5=93.763 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.464 DataTime=0.268 Loss=0.820 Prec@1=79.269 Prec@5=93.763 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=17:39 IST
=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.820 Prec@1=79.254 Prec@5=93.761 rate=2.18 Hz, eta=0:12:59, total=0:06:06, wall=17:39 IST
=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.820 Prec@1=79.254 Prec@5=93.761 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=17:39 IST
=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.820 Prec@1=79.254 Prec@5=93.761 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=17:40 IST
=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.821 Prec@1=79.259 Prec@5=93.747 rate=2.18 Hz, eta=0:12:13, total=0:06:52, wall=17:40 IST
=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.821 Prec@1=79.259 Prec@5=93.747 rate=2.18 Hz, eta=0:11:28, total=0:07:38, wall=17:40 IST
=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.463 DataTime=0.267 Loss=0.821 Prec@1=79.259 Prec@5=93.747 rate=2.18 Hz, eta=0:11:28, total=0:07:38, wall=17:41 IST
=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.254 Prec@5=93.742 rate=2.18 Hz, eta=0:11:28, total=0:07:38, wall=17:41 IST
=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.254 Prec@5=93.742 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=17:41 IST
=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.254 Prec@5=93.742 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=17:42 IST
=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.223 Prec@5=93.732 rate=2.18 Hz, eta=0:10:42, total=0:08:24, wall=17:42 IST
=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.223 Prec@5=93.732 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=17:42 IST
=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.223 Prec@5=93.732 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=17:42 IST
=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.215 Prec@5=93.714 rate=2.18 Hz, eta=0:09:56, total=0:09:10, wall=17:42 IST
=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.215 Prec@5=93.714 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=17:42 IST
=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.267 Loss=0.823 Prec@1=79.215 Prec@5=93.714 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=17:43 IST
=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.824 Prec@1=79.192 Prec@5=93.711 rate=2.18 Hz, eta=0:09:11, total=0:09:56, wall=17:43 IST
=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.824 Prec@1=79.192 Prec@5=93.711 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=17:43 IST
=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.824 Prec@1=79.192 Prec@5=93.711 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=17:44 IST
=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.203 Prec@5=93.719 rate=2.18 Hz, eta=0:08:25, total=0:10:42, wall=17:44 IST
=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.203 Prec@5=93.719 rate=2.18 Hz, eta=0:07:39, total=0:11:28, wall=17:44 IST
=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.203 Prec@5=93.719 rate=2.18 Hz, eta=0:07:39, total=0:11:28, wall=17:45 IST
=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.210 Prec@5=93.720 rate=2.18 Hz, eta=0:07:39, total=0:11:28, wall=17:45 IST
=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.210 Prec@5=93.720 rate=2.18 Hz, eta=0:06:53, total=0:12:14, wall=17:45 IST
=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.823 Prec@1=79.210 Prec@5=93.720 rate=2.18 Hz, eta=0:06:53, total=0:12:14, wall=17:46 IST
=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.823 Prec@1=79.217 Prec@5=93.721 rate=2.18 Hz, eta=0:06:53, total=0:12:14, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.823 Prec@1=79.217 Prec@5=93.721 rate=2.18 Hz, eta=0:06:07, total=0:13:00, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.823 Prec@1=79.217 Prec@5=93.721 rate=2.18 Hz, eta=0:06:07, total=0:13:00, wall=17:46 IST
=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.222 Prec@5=93.720 rate=2.18 Hz, eta=0:06:07, total=0:13:00, wall=17:46 IST
=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.222 Prec@5=93.720 rate=2.18 Hz, eta=0:05:21, total=0:13:45, wall=17:46 IST
=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.222 Prec@5=93.720 rate=2.18 Hz, eta=0:05:21, total=0:13:45, wall=17:47 IST
=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.233 Prec@5=93.719 rate=2.18 Hz, eta=0:05:21, total=0:13:45, wall=17:47 IST
=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.233 Prec@5=93.719 rate=2.18 Hz, eta=0:04:36, total=0:14:31, wall=17:47 IST
=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.265 Loss=0.822 Prec@1=79.233 Prec@5=93.719 rate=2.18 Hz, eta=0:04:36, total=0:14:31, wall=17:48 IST
=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.238 Prec@5=93.726 rate=2.18 Hz, eta=0:04:36, total=0:14:31, wall=17:48 IST
=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.238 Prec@5=93.726 rate=2.18 Hz, eta=0:03:50, total=0:15:18, wall=17:48 IST
=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.238 Prec@5=93.726 rate=2.18 Hz, eta=0:03:50, total=0:15:18, wall=17:49 IST
=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.243 Prec@5=93.726 rate=2.18 Hz, eta=0:03:50, total=0:15:18, wall=17:49 IST
=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.243 Prec@5=93.726 rate=2.18 Hz, eta=0:03:04, total=0:16:05, wall=17:49 IST
=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.243 Prec@5=93.726 rate=2.18 Hz, eta=0:03:04, total=0:16:05, wall=17:49 IST
=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.245 Prec@5=93.727 rate=2.18 Hz, eta=0:03:04, total=0:16:05, wall=17:49 IST
=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.245 Prec@5=93.727 rate=2.18 Hz, eta=0:02:18, total=0:16:51, wall=17:49 IST
=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.245 Prec@5=93.727 rate=2.18 Hz, eta=0:02:18, total=0:16:51, wall=17:50 IST
=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.249 Prec@5=93.728 rate=2.18 Hz, eta=0:02:18, total=0:16:51, wall=17:50 IST
=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.249 Prec@5=93.728 rate=2.17 Hz, eta=0:01:32, total=0:17:38, wall=17:50 IST
=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.249 Prec@5=93.728 rate=2.17 Hz, eta=0:01:32, total=0:17:38, wall=17:51 IST
=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.263 Prec@5=93.732 rate=2.17 Hz, eta=0:01:32, total=0:17:38, wall=17:51 IST
=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.263 Prec@5=93.732 rate=2.17 Hz, eta=0:00:46, total=0:18:24, wall=17:51 IST
=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.462 DataTime=0.266 Loss=0.822 Prec@1=79.263 Prec@5=93.732 rate=2.17 Hz, eta=0:00:46, total=0:18:24, wall=17:52 IST
=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.822 Prec@1=79.261 Prec@5=93.728 rate=2.17 Hz, eta=0:00:46, total=0:18:24, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.822 Prec@1=79.261 Prec@5=93.728 rate=2.18 Hz, eta=0:00:00, total=0:19:09, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.822 Prec@1=79.261 Prec@5=93.728 rate=2.18 Hz, eta=0:00:00, total=0:19:09, wall=17:52 IST
=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.822 Prec@1=79.262 Prec@5=93.728 rate=2.18 Hz, eta=0:00:00, total=0:19:09, wall=17:52 IST
=> training   100.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.461 DataTime=0.266 Loss=0.822 Prec@1=79.262 Prec@5=93.728 rate=2.18 Hz, eta=0:00:00, total=0:19:10, wall=17:52 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 0.00% of 1x98...Epoch=146/150 LR=0.00027 Time=6.839 Loss=0.648 Prec@1=83.594 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=17:52 IST
=> validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=6.839 Loss=0.648 Prec@1=83.594 Prec@5=95.312 rate=6756.44 Hz, eta=0:00:00, total=0:00:00, wall=17:52 IST
** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=6.839 Loss=0.648 Prec@1=83.594 Prec@5=95.312 rate=6756.44 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=0.553 Loss=1.134 Prec@1=72.124 Prec@5=90.538 rate=6756.44 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
** validation 100.00% of 1x98...Epoch=146/150 LR=0.00027 Time=0.553 Loss=1.134 Prec@1=72.124 Prec@5=90.538 rate=2.07 Hz, eta=0:00:00, total=0:00:47, wall=17:53 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=4.607 DataTime=4.344 Loss=0.869 Prec@1=77.539 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=17:53 IST
=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=4.607 DataTime=4.344 Loss=0.869 Prec@1=77.539 Prec@5=93.359 rate=8362.67 Hz, eta=0:00:00, total=0:00:00, wall=17:53 IST
=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=4.607 DataTime=4.344 Loss=0.869 Prec@1=77.539 Prec@5=93.359 rate=8362.67 Hz, eta=0:00:00, total=0:00:00, wall=17:54 IST
=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.505 DataTime=0.311 Loss=0.822 Prec@1=79.320 Prec@5=93.729 rate=8362.67 Hz, eta=0:00:00, total=0:00:00, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.505 DataTime=0.311 Loss=0.822 Prec@1=79.320 Prec@5=93.729 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.505 DataTime=0.311 Loss=0.822 Prec@1=79.320 Prec@5=93.729 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=17:54 IST
=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.486 DataTime=0.292 Loss=0.814 Prec@1=79.466 Prec@5=93.826 rate=2.18 Hz, eta=0:18:23, total=0:00:46, wall=17:54 IST
=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.486 DataTime=0.292 Loss=0.814 Prec@1=79.466 Prec@5=93.826 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=17:54 IST
=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.486 DataTime=0.292 Loss=0.814 Prec@1=79.466 Prec@5=93.826 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=17:55 IST
=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.478 DataTime=0.284 Loss=0.816 Prec@1=79.384 Prec@5=93.795 rate=2.16 Hz, eta=0:17:45, total=0:01:33, wall=17:55 IST
=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.478 DataTime=0.284 Loss=0.816 Prec@1=79.384 Prec@5=93.795 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:55 IST
=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.478 DataTime=0.284 Loss=0.816 Prec@1=79.384 Prec@5=93.795 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:56 IST
=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.473 DataTime=0.279 Loss=0.816 Prec@1=79.424 Prec@5=93.783 rate=2.16 Hz, eta=0:16:59, total=0:02:19, wall=17:56 IST
=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.473 DataTime=0.279 Loss=0.816 Prec@1=79.424 Prec@5=93.783 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=17:56 IST
=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.473 DataTime=0.279 Loss=0.816 Prec@1=79.424 Prec@5=93.783 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=17:57 IST
=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.472 DataTime=0.279 Loss=0.815 Prec@1=79.449 Prec@5=93.774 rate=2.17 Hz, eta=0:16:09, total=0:03:05, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.472 DataTime=0.279 Loss=0.815 Prec@1=79.449 Prec@5=93.774 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.472 DataTime=0.279 Loss=0.815 Prec@1=79.449 Prec@5=93.774 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=17:57 IST
=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.817 Prec@1=79.369 Prec@5=93.748 rate=2.16 Hz, eta=0:15:27, total=0:03:52, wall=17:57 IST
=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.817 Prec@1=79.369 Prec@5=93.748 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=17:57 IST
=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.817 Prec@1=79.369 Prec@5=93.748 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=17:58 IST
=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.277 Loss=0.818 Prec@1=79.378 Prec@5=93.751 rate=2.16 Hz, eta=0:14:41, total=0:04:38, wall=17:58 IST
=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.277 Loss=0.818 Prec@1=79.378 Prec@5=93.751 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=17:58 IST
=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.277 Loss=0.818 Prec@1=79.378 Prec@5=93.751 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=17:59 IST
=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.818 Prec@1=79.363 Prec@5=93.760 rate=2.16 Hz, eta=0:13:55, total=0:05:25, wall=17:59 IST
=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.818 Prec@1=79.363 Prec@5=93.760 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=17:59 IST
=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.471 DataTime=0.277 Loss=0.818 Prec@1=79.363 Prec@5=93.760 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=18:00 IST
=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.818 Prec@1=79.380 Prec@5=93.762 rate=2.15 Hz, eta=0:13:11, total=0:06:12, wall=18:00 IST
=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.818 Prec@1=79.380 Prec@5=93.762 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=18:00 IST
=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.818 Prec@1=79.380 Prec@5=93.762 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=18:01 IST
=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.349 Prec@5=93.766 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.349 Prec@5=93.766 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.349 Prec@5=93.766 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=18:01 IST
=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.339 Prec@5=93.757 rate=2.15 Hz, eta=0:11:38, total=0:07:45, wall=18:01 IST
=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.339 Prec@5=93.757 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:01 IST
=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.470 DataTime=0.276 Loss=0.819 Prec@1=79.339 Prec@5=93.757 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:02 IST
=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.319 Prec@5=93.754 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=18:02 IST
=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.319 Prec@5=93.754 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=18:02 IST
=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.319 Prec@5=93.754 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=18:03 IST
=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.311 Prec@5=93.753 rate=2.15 Hz, eta=0:10:06, total=0:09:19, wall=18:03 IST
=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.311 Prec@5=93.753 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=18:03 IST
=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.819 Prec@1=79.311 Prec@5=93.753 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=18:04 IST
=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.274 Loss=0.820 Prec@1=79.297 Prec@5=93.739 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.274 Loss=0.820 Prec@1=79.297 Prec@5=93.739 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.274 Loss=0.820 Prec@1=79.297 Prec@5=93.739 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:04 IST
=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.276 Prec@5=93.740 rate=2.15 Hz, eta=0:08:32, total=0:10:51, wall=18:04 IST
=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.276 Prec@5=93.740 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:04 IST
=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.276 Prec@5=93.740 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:05 IST
=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.277 Prec@5=93.745 rate=2.15 Hz, eta=0:07:46, total=0:11:38, wall=18:05 IST
=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.277 Prec@5=93.745 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=18:05 IST
=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.277 Prec@5=93.745 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=18:06 IST
=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.739 rate=2.15 Hz, eta=0:06:59, total=0:12:24, wall=18:06 IST
=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.739 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=18:06 IST
=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.739 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=18:07 IST
=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.732 rate=2.15 Hz, eta=0:06:13, total=0:13:11, wall=18:07 IST
=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.732 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:07 IST
=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.732 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:07 IST
=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.740 rate=2.15 Hz, eta=0:05:26, total=0:13:58, wall=18:07 IST
=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.740 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=18:07 IST
=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.262 Prec@5=93.740 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=18:08 IST
=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.263 Prec@5=93.735 rate=2.15 Hz, eta=0:04:40, total=0:14:45, wall=18:08 IST
=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.263 Prec@5=93.735 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:08 IST
=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.263 Prec@5=93.735 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:09 IST
=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.264 Prec@5=93.735 rate=2.15 Hz, eta=0:03:53, total=0:15:31, wall=18:09 IST
=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.264 Prec@5=93.735 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=18:09 IST
=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.264 Prec@5=93.735 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=18:10 IST
=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.734 rate=2.15 Hz, eta=0:03:07, total=0:16:18, wall=18:10 IST
=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.734 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:10 IST
=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.267 Prec@5=93.734 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:11 IST
=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.266 Prec@5=93.732 rate=2.15 Hz, eta=0:02:20, total=0:17:05, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.266 Prec@5=93.732 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.266 Prec@5=93.732 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=18:11 IST
=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.271 Prec@5=93.736 rate=2.14 Hz, eta=0:01:34, total=0:17:52, wall=18:11 IST
=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.271 Prec@5=93.736 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=18:11 IST
=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.468 DataTime=0.274 Loss=0.821 Prec@1=79.271 Prec@5=93.736 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=18:12 IST
=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.821 Prec@1=79.257 Prec@5=93.731 rate=2.15 Hz, eta=0:00:47, total=0:18:39, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.821 Prec@1=79.257 Prec@5=93.731 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.821 Prec@1=79.257 Prec@5=93.731 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=18:12 IST
=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.821 Prec@1=79.255 Prec@5=93.729 rate=2.14 Hz, eta=0:00:00, total=0:19:27, wall=18:12 IST
=> training   100.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.469 DataTime=0.275 Loss=0.821 Prec@1=79.255 Prec@5=93.729 rate=2.14 Hz, eta=0:00:00, total=0:19:28, wall=18:12 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 0.00% of 1x98...Epoch=147/150 LR=0.00018 Time=6.751 Loss=0.642 Prec@1=83.789 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST
=> validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=6.751 Loss=0.642 Prec@1=83.789 Prec@5=95.117 rate=7982.44 Hz, eta=0:00:00, total=0:00:00, wall=18:12 IST
** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=6.751 Loss=0.642 Prec@1=83.789 Prec@5=95.117 rate=7982.44 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=0.549 Loss=1.133 Prec@1=72.114 Prec@5=90.534 rate=7982.44 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
** validation 100.00% of 1x98...Epoch=147/150 LR=0.00018 Time=0.549 Loss=1.133 Prec@1=72.114 Prec@5=90.534 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=18:13 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.067 DataTime=4.813 Loss=0.841 Prec@1=79.297 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST
=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.067 DataTime=4.813 Loss=0.841 Prec@1=79.297 Prec@5=93.359 rate=8413.26 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST
=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.067 DataTime=4.813 Loss=0.841 Prec@1=79.297 Prec@5=93.359 rate=8413.26 Hz, eta=0:00:00, total=0:00:00, wall=18:14 IST
=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.518 DataTime=0.320 Loss=0.812 Prec@1=79.488 Prec@5=93.858 rate=8413.26 Hz, eta=0:00:00, total=0:00:00, wall=18:14 IST
=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.518 DataTime=0.320 Loss=0.812 Prec@1=79.488 Prec@5=93.858 rate=2.14 Hz, eta=0:18:43, total=0:00:47, wall=18:14 IST
=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.518 DataTime=0.320 Loss=0.812 Prec@1=79.488 Prec@5=93.858 rate=2.14 Hz, eta=0:18:43, total=0:00:47, wall=18:15 IST
=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.489 DataTime=0.293 Loss=0.815 Prec@1=79.330 Prec@5=93.866 rate=2.14 Hz, eta=0:18:43, total=0:00:47, wall=18:15 IST
=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.489 DataTime=0.293 Loss=0.815 Prec@1=79.330 Prec@5=93.866 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=18:15 IST
=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.489 DataTime=0.293 Loss=0.815 Prec@1=79.330 Prec@5=93.866 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=18:16 IST
=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.820 Prec@1=79.250 Prec@5=93.736 rate=2.15 Hz, eta=0:17:48, total=0:01:33, wall=18:16 IST
=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.820 Prec@1=79.250 Prec@5=93.736 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=18:16 IST
=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.820 Prec@1=79.250 Prec@5=93.736 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=18:16 IST
=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.334 Prec@5=93.790 rate=2.17 Hz, eta=0:16:56, total=0:02:18, wall=18:16 IST
=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.334 Prec@5=93.790 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=18:16 IST
=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.334 Prec@5=93.790 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=18:17 IST
=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.280 Loss=0.818 Prec@1=79.329 Prec@5=93.761 rate=2.17 Hz, eta=0:16:10, total=0:03:05, wall=18:17 IST
=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.280 Loss=0.818 Prec@1=79.329 Prec@5=93.761 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=18:17 IST
=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.280 Loss=0.818 Prec@1=79.329 Prec@5=93.761 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=18:18 IST
=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.791 rate=2.15 Hz, eta=0:15:31, total=0:03:53, wall=18:18 IST
=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.791 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=18:18 IST
=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.791 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=18:19 IST
=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.346 Prec@5=93.793 rate=2.13 Hz, eta=0:14:52, total=0:04:42, wall=18:19 IST
=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.346 Prec@5=93.793 rate=2.12 Hz, eta=0:14:10, total=0:05:30, wall=18:19 IST
=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.346 Prec@5=93.793 rate=2.12 Hz, eta=0:14:10, total=0:05:30, wall=18:20 IST
=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.324 Prec@5=93.782 rate=2.12 Hz, eta=0:14:10, total=0:05:30, wall=18:20 IST
=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.324 Prec@5=93.782 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=18:20 IST
=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.324 Prec@5=93.782 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=18:20 IST
=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.331 Prec@5=93.783 rate=2.12 Hz, eta=0:13:24, total=0:06:18, wall=18:20 IST
=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.331 Prec@5=93.783 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=18:20 IST
=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.818 Prec@1=79.331 Prec@5=93.783 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=18:21 IST
=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.354 Prec@5=93.802 rate=2.11 Hz, eta=0:12:38, total=0:07:06, wall=18:21 IST
=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.354 Prec@5=93.802 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=18:21 IST
=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.354 Prec@5=93.802 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=18:22 IST
=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.805 rate=2.11 Hz, eta=0:11:51, total=0:07:54, wall=18:22 IST
=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.805 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=18:22 IST
=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.805 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=18:23 IST
=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.359 Prec@5=93.805 rate=2.11 Hz, eta=0:11:03, total=0:08:41, wall=18:23 IST
=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.359 Prec@5=93.805 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=18:23 IST
=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.359 Prec@5=93.805 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=18:24 IST
=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.804 rate=2.11 Hz, eta=0:10:16, total=0:09:28, wall=18:24 IST
=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.804 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=18:24 IST
=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.816 Prec@1=79.366 Prec@5=93.804 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=18:24 IST
=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.363 Prec@5=93.806 rate=2.11 Hz, eta=0:09:29, total=0:10:16, wall=18:24 IST
=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.363 Prec@5=93.806 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=18:24 IST
=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.479 DataTime=0.284 Loss=0.817 Prec@1=79.363 Prec@5=93.806 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=18:25 IST
=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.801 rate=2.11 Hz, eta=0:08:43, total=0:11:05, wall=18:25 IST
=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.801 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=18:25 IST
=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.357 Prec@5=93.801 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=18:26 IST
=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.368 Prec@5=93.804 rate=2.11 Hz, eta=0:07:55, total=0:11:52, wall=18:26 IST
=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.368 Prec@5=93.804 rate=2.11 Hz, eta=0:07:08, total=0:12:40, wall=18:26 IST
=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.478 DataTime=0.283 Loss=0.817 Prec@1=79.368 Prec@5=93.804 rate=2.11 Hz, eta=0:07:08, total=0:12:40, wall=18:27 IST
=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.283 Loss=0.817 Prec@1=79.364 Prec@5=93.800 rate=2.11 Hz, eta=0:07:08, total=0:12:40, wall=18:27 IST
=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.283 Loss=0.817 Prec@1=79.364 Prec@5=93.800 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=18:27 IST
=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.283 Loss=0.817 Prec@1=79.364 Prec@5=93.800 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=18:27 IST
=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.282 Loss=0.818 Prec@1=79.359 Prec@5=93.795 rate=2.11 Hz, eta=0:06:20, total=0:13:26, wall=18:27 IST
=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.282 Loss=0.818 Prec@1=79.359 Prec@5=93.795 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=18:27 IST
=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.477 DataTime=0.282 Loss=0.818 Prec@1=79.359 Prec@5=93.795 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=18:28 IST
=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.281 Loss=0.818 Prec@1=79.353 Prec@5=93.795 rate=2.11 Hz, eta=0:05:32, total=0:14:13, wall=18:28 IST
=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.281 Loss=0.818 Prec@1=79.353 Prec@5=93.795 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=18:28 IST
=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.476 DataTime=0.281 Loss=0.818 Prec@1=79.353 Prec@5=93.795 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=18:29 IST
=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.281 Loss=0.818 Prec@1=79.346 Prec@5=93.797 rate=2.11 Hz, eta=0:04:44, total=0:14:59, wall=18:29 IST
=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.281 Loss=0.818 Prec@1=79.346 Prec@5=93.797 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=18:29 IST
=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.281 Loss=0.818 Prec@1=79.346 Prec@5=93.797 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=18:30 IST
=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.331 Prec@5=93.787 rate=2.11 Hz, eta=0:03:57, total=0:15:46, wall=18:30 IST
=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.331 Prec@5=93.787 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=18:30 IST
=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.331 Prec@5=93.787 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=18:31 IST
=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.333 Prec@5=93.784 rate=2.12 Hz, eta=0:03:10, total=0:16:33, wall=18:31 IST
=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.333 Prec@5=93.784 rate=2.12 Hz, eta=0:02:22, total=0:17:20, wall=18:31 IST
=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.475 DataTime=0.280 Loss=0.818 Prec@1=79.333 Prec@5=93.784 rate=2.12 Hz, eta=0:02:22, total=0:17:20, wall=18:31 IST
=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.333 Prec@5=93.789 rate=2.12 Hz, eta=0:02:22, total=0:17:20, wall=18:31 IST
=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.333 Prec@5=93.789 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=18:31 IST
=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.333 Prec@5=93.789 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=18:32 IST
=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.331 Prec@5=93.784 rate=2.12 Hz, eta=0:01:35, total=0:18:06, wall=18:32 IST
=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.331 Prec@5=93.784 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=18:32 IST
=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.818 Prec@1=79.331 Prec@5=93.784 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=18:33 IST
=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.819 Prec@1=79.315 Prec@5=93.779 rate=2.12 Hz, eta=0:00:48, total=0:18:52, wall=18:33 IST
=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.819 Prec@1=79.315 Prec@5=93.779 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=18:33 IST
=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.819 Prec@1=79.315 Prec@5=93.779 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=18:33 IST
=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.819 Prec@1=79.315 Prec@5=93.779 rate=2.12 Hz, eta=0:00:00, total=0:19:39, wall=18:33 IST
=> training   100.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.474 DataTime=0.279 Loss=0.819 Prec@1=79.315 Prec@5=93.779 rate=2.12 Hz, eta=0:00:00, total=0:19:40, wall=18:33 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 0.00% of 1x98...Epoch=148/150 LR=0.00010 Time=6.861 Loss=0.632 Prec@1=84.180 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=6.861 Loss=0.632 Prec@1=84.180 Prec@5=95.898 rate=8260.98 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=6.861 Loss=0.632 Prec@1=84.180 Prec@5=95.898 rate=8260.98 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=0.551 Loss=1.133 Prec@1=72.120 Prec@5=90.522 rate=8260.98 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
** validation 100.00% of 1x98...Epoch=148/150 LR=0.00010 Time=0.551 Loss=1.133 Prec@1=72.120 Prec@5=90.522 rate=2.08 Hz, eta=0:00:00, total=0:00:47, wall=18:34 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:34 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:34 IST
=> training   0.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.142 DataTime=4.898 Loss=0.787 Prec@1=78.906 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=18:34 IST
=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.142 DataTime=4.898 Loss=0.787 Prec@1=78.906 Prec@5=94.727 rate=7919.85 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.142 DataTime=4.898 Loss=0.787 Prec@1=78.906 Prec@5=94.727 rate=7919.85 Hz, eta=0:00:00, total=0:00:00, wall=18:35 IST
=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.497 DataTime=0.302 Loss=0.821 Prec@1=79.490 Prec@5=93.707 rate=7919.85 Hz, eta=0:00:00, total=0:00:00, wall=18:35 IST
=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.497 DataTime=0.302 Loss=0.821 Prec@1=79.490 Prec@5=93.707 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:35 IST
=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.497 DataTime=0.302 Loss=0.821 Prec@1=79.490 Prec@5=93.707 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:35 IST
=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.478 DataTime=0.281 Loss=0.823 Prec@1=79.403 Prec@5=93.626 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:35 IST
=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.478 DataTime=0.281 Loss=0.823 Prec@1=79.403 Prec@5=93.626 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:35 IST
=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.478 DataTime=0.281 Loss=0.823 Prec@1=79.403 Prec@5=93.626 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:36 IST
=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.471 DataTime=0.273 Loss=0.823 Prec@1=79.288 Prec@5=93.665 rate=2.21 Hz, eta=0:17:20, total=0:01:30, wall=18:36 IST
=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.471 DataTime=0.273 Loss=0.823 Prec@1=79.288 Prec@5=93.665 rate=2.20 Hz, eta=0:16:39, total=0:02:16, wall=18:36 IST
=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.471 DataTime=0.273 Loss=0.823 Prec@1=79.288 Prec@5=93.665 rate=2.20 Hz, eta=0:16:39, total=0:02:16, wall=18:37 IST
=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.821 Prec@1=79.316 Prec@5=93.678 rate=2.20 Hz, eta=0:16:39, total=0:02:16, wall=18:37 IST
=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.821 Prec@1=79.316 Prec@5=93.678 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:37 IST
=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.821 Prec@1=79.316 Prec@5=93.678 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:38 IST
=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.823 Prec@1=79.313 Prec@5=93.654 rate=2.20 Hz, eta=0:15:55, total=0:03:02, wall=18:38 IST
=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.823 Prec@1=79.313 Prec@5=93.654 rate=2.19 Hz, eta=0:15:15, total=0:03:49, wall=18:38 IST
=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.823 Prec@1=79.313 Prec@5=93.654 rate=2.19 Hz, eta=0:15:15, total=0:03:49, wall=18:39 IST
=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.298 Prec@5=93.685 rate=2.19 Hz, eta=0:15:15, total=0:03:49, wall=18:39 IST
=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.298 Prec@5=93.685 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=18:39 IST
=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.298 Prec@5=93.685 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=18:39 IST
=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.315 Prec@5=93.691 rate=2.18 Hz, eta=0:14:30, total=0:04:35, wall=18:39 IST
=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.315 Prec@5=93.691 rate=2.18 Hz, eta=0:13:47, total=0:05:21, wall=18:39 IST
=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.315 Prec@5=93.691 rate=2.18 Hz, eta=0:13:47, total=0:05:21, wall=18:40 IST
=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.822 Prec@1=79.291 Prec@5=93.682 rate=2.18 Hz, eta=0:13:47, total=0:05:21, wall=18:40 IST
=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.822 Prec@1=79.291 Prec@5=93.682 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=18:40 IST
=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.822 Prec@1=79.291 Prec@5=93.682 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=18:41 IST
=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.310 Prec@5=93.693 rate=2.17 Hz, eta=0:13:02, total=0:06:08, wall=18:41 IST
=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.310 Prec@5=93.693 rate=2.17 Hz, eta=0:12:17, total=0:06:54, wall=18:41 IST
=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.310 Prec@5=93.693 rate=2.17 Hz, eta=0:12:17, total=0:06:54, wall=18:42 IST
=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.317 Prec@5=93.708 rate=2.17 Hz, eta=0:12:17, total=0:06:54, wall=18:42 IST
=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.317 Prec@5=93.708 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=18:42 IST
=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.317 Prec@5=93.708 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=18:42 IST
=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.297 Prec@5=93.707 rate=2.17 Hz, eta=0:11:31, total=0:07:40, wall=18:42 IST
=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.297 Prec@5=93.707 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=18:42 IST
=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.269 Loss=0.821 Prec@1=79.297 Prec@5=93.707 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=18:43 IST
=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.820 Prec@1=79.309 Prec@5=93.713 rate=2.17 Hz, eta=0:10:46, total=0:08:27, wall=18:43 IST
=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.820 Prec@1=79.309 Prec@5=93.713 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=18:43 IST
=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.467 DataTime=0.270 Loss=0.820 Prec@1=79.309 Prec@5=93.713 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=18:44 IST
=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.314 Prec@5=93.707 rate=2.16 Hz, eta=0:10:01, total=0:09:15, wall=18:44 IST
=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.314 Prec@5=93.707 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=18:44 IST
=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.314 Prec@5=93.707 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=18:45 IST
=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.315 Prec@5=93.703 rate=2.16 Hz, eta=0:09:15, total=0:10:01, wall=18:45 IST
=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.315 Prec@5=93.703 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=18:45 IST
=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.315 Prec@5=93.703 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=18:45 IST
=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.305 Prec@5=93.702 rate=2.16 Hz, eta=0:08:29, total=0:10:47, wall=18:45 IST
=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.305 Prec@5=93.702 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:45 IST
=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.821 Prec@1=79.305 Prec@5=93.702 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:46 IST
=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.711 rate=2.16 Hz, eta=0:07:43, total=0:11:34, wall=18:46 IST
=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.711 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=18:46 IST
=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.711 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=18:47 IST
=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.343 Prec@5=93.717 rate=2.16 Hz, eta=0:06:57, total=0:12:20, wall=18:47 IST
=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.343 Prec@5=93.717 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=18:47 IST
=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.343 Prec@5=93.717 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=18:48 IST
=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.718 rate=2.16 Hz, eta=0:06:11, total=0:13:07, wall=18:48 IST
=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.718 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=18:48 IST
=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.333 Prec@5=93.718 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=18:49 IST
=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.819 Prec@1=79.335 Prec@5=93.733 rate=2.16 Hz, eta=0:05:25, total=0:13:54, wall=18:49 IST
=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.819 Prec@1=79.335 Prec@5=93.733 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=18:49 IST
=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.819 Prec@1=79.335 Prec@5=93.733 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=18:49 IST
=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.330 Prec@5=93.734 rate=2.16 Hz, eta=0:04:38, total=0:14:40, wall=18:49 IST
=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.330 Prec@5=93.734 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=18:49 IST
=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.330 Prec@5=93.734 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=18:50 IST
=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.336 Prec@5=93.728 rate=2.16 Hz, eta=0:03:52, total=0:15:25, wall=18:50 IST
=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.336 Prec@5=93.728 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=18:50 IST
=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.336 Prec@5=93.728 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=18:51 IST
=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.344 Prec@5=93.725 rate=2.16 Hz, eta=0:03:05, total=0:16:11, wall=18:51 IST
=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.344 Prec@5=93.725 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=18:51 IST
=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.819 Prec@1=79.344 Prec@5=93.725 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=18:52 IST
=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.270 Loss=0.820 Prec@1=79.338 Prec@5=93.726 rate=2.16 Hz, eta=0:02:19, total=0:16:58, wall=18:52 IST
=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.270 Loss=0.820 Prec@1=79.338 Prec@5=93.726 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=18:52 IST
=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.270 Loss=0.820 Prec@1=79.338 Prec@5=93.726 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=18:52 IST
=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.341 Prec@5=93.729 rate=2.16 Hz, eta=0:01:33, total=0:17:45, wall=18:52 IST
=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.341 Prec@5=93.729 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=18:52 IST
=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.466 DataTime=0.270 Loss=0.820 Prec@1=79.341 Prec@5=93.729 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=18:53 IST
=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.820 Prec@1=79.337 Prec@5=93.724 rate=2.16 Hz, eta=0:00:47, total=0:18:32, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.820 Prec@1=79.337 Prec@5=93.724 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.820 Prec@1=79.337 Prec@5=93.724 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=18:53 IST
=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.820 Prec@1=79.337 Prec@5=93.725 rate=2.16 Hz, eta=0:00:00, total=0:19:18, wall=18:53 IST
=> training   100.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.465 DataTime=0.269 Loss=0.820 Prec@1=79.337 Prec@5=93.725 rate=2.16 Hz, eta=0:00:00, total=0:19:19, wall=18:53 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 0.00% of 1x98...Epoch=149/150 LR=0.00004 Time=6.991 Loss=0.640 Prec@1=83.984 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=6.991 Loss=0.640 Prec@1=83.984 Prec@5=95.312 rate=6480.88 Hz, eta=0:00:00, total=0:00:00, wall=18:53 IST
** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=6.991 Loss=0.640 Prec@1=83.984 Prec@5=95.312 rate=6480.88 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=0.550 Loss=1.135 Prec@1=72.064 Prec@5=90.522 rate=6480.88 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** validation 100.00% of 1x98...Epoch=149/150 LR=0.00004 Time=0.550 Loss=1.135 Prec@1=72.064 Prec@5=90.522 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=18:54 IST
[39m[37m
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.623 DataTime=4.352 Loss=0.814 Prec@1=77.734 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.623 DataTime=4.352 Loss=0.814 Prec@1=77.734 Prec@5=94.531 rate=6581.41 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.623 DataTime=4.352 Loss=0.814 Prec@1=77.734 Prec@5=94.531 rate=6581.41 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.492 DataTime=0.296 Loss=0.819 Prec@1=79.183 Prec@5=93.713 rate=6581.41 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.492 DataTime=0.296 Loss=0.819 Prec@1=79.183 Prec@5=93.713 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:55 IST
=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.492 DataTime=0.296 Loss=0.819 Prec@1=79.183 Prec@5=93.713 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:56 IST
=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.481 DataTime=0.284 Loss=0.817 Prec@1=79.332 Prec@5=93.741 rate=2.24 Hz, eta=0:17:51, total=0:00:45, wall=18:56 IST
=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.481 DataTime=0.284 Loss=0.817 Prec@1=79.332 Prec@5=93.741 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=18:56 IST
=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.481 DataTime=0.284 Loss=0.817 Prec@1=79.332 Prec@5=93.741 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=18:57 IST
=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.473 DataTime=0.277 Loss=0.815 Prec@1=79.390 Prec@5=93.779 rate=2.19 Hz, eta=0:17:33, total=0:01:31, wall=18:57 IST
=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.473 DataTime=0.277 Loss=0.815 Prec@1=79.390 Prec@5=93.779 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=18:57 IST
=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.473 DataTime=0.277 Loss=0.815 Prec@1=79.390 Prec@5=93.779 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=18:57 IST
=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.813 Prec@1=79.441 Prec@5=93.790 rate=2.18 Hz, eta=0:16:48, total=0:02:17, wall=18:57 IST
=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.813 Prec@1=79.441 Prec@5=93.790 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=18:57 IST
=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.813 Prec@1=79.441 Prec@5=93.790 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=18:58 IST
=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.273 Loss=0.813 Prec@1=79.449 Prec@5=93.796 rate=2.18 Hz, eta=0:16:04, total=0:03:03, wall=18:58 IST
=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.273 Loss=0.813 Prec@1=79.449 Prec@5=93.796 rate=2.17 Hz, eta=0:15:21, total=0:03:50, wall=18:58 IST
=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.273 Loss=0.813 Prec@1=79.449 Prec@5=93.796 rate=2.17 Hz, eta=0:15:21, total=0:03:50, wall=18:59 IST
=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.814 Prec@1=79.387 Prec@5=93.776 rate=2.17 Hz, eta=0:15:21, total=0:03:50, wall=18:59 IST
=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.814 Prec@1=79.387 Prec@5=93.776 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=18:59 IST
=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.814 Prec@1=79.387 Prec@5=93.776 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=19:00 IST
=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.815 Prec@1=79.378 Prec@5=93.778 rate=2.16 Hz, eta=0:14:38, total=0:04:37, wall=19:00 IST
=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.815 Prec@1=79.378 Prec@5=93.778 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=19:00 IST
=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.815 Prec@1=79.378 Prec@5=93.778 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=19:00 IST
=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.326 Prec@5=93.774 rate=2.16 Hz, eta=0:13:54, total=0:05:24, wall=19:00 IST
=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.326 Prec@5=93.774 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:00 IST
=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.326 Prec@5=93.774 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:01 IST
=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.816 Prec@1=79.368 Prec@5=93.774 rate=2.15 Hz, eta=0:13:10, total=0:06:12, wall=19:01 IST
=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.816 Prec@1=79.368 Prec@5=93.774 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:01 IST
=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.816 Prec@1=79.368 Prec@5=93.774 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:02 IST
=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.362 Prec@5=93.756 rate=2.15 Hz, eta=0:12:24, total=0:06:58, wall=19:02 IST
=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.362 Prec@5=93.756 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:02 IST
=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.362 Prec@5=93.756 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:03 IST
=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.354 Prec@5=93.755 rate=2.15 Hz, eta=0:11:37, total=0:07:45, wall=19:03 IST
=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.354 Prec@5=93.755 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=19:03 IST
=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.354 Prec@5=93.755 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=19:04 IST
=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.354 Prec@5=93.745 rate=2.15 Hz, eta=0:10:52, total=0:08:32, wall=19:04 IST
=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.354 Prec@5=93.745 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:04 IST
=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.354 Prec@5=93.745 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:04 IST
=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.817 Prec@1=79.360 Prec@5=93.759 rate=2.15 Hz, eta=0:10:05, total=0:09:18, wall=19:04 IST
=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.817 Prec@1=79.360 Prec@5=93.759 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:04 IST
=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.817 Prec@1=79.360 Prec@5=93.759 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:05 IST
=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.816 Prec@1=79.375 Prec@5=93.762 rate=2.15 Hz, eta=0:09:19, total=0:10:05, wall=19:05 IST
=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.816 Prec@1=79.375 Prec@5=93.762 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:05 IST
=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.273 Loss=0.816 Prec@1=79.375 Prec@5=93.762 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:06 IST
=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.373 Prec@5=93.764 rate=2.15 Hz, eta=0:08:33, total=0:10:52, wall=19:06 IST
=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.373 Prec@5=93.764 rate=2.15 Hz, eta=0:07:47, total=0:11:39, wall=19:06 IST
=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.816 Prec@1=79.373 Prec@5=93.764 rate=2.15 Hz, eta=0:07:47, total=0:11:39, wall=19:07 IST
=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.372 Prec@5=93.752 rate=2.15 Hz, eta=0:07:47, total=0:11:39, wall=19:07 IST
=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.372 Prec@5=93.752 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=19:07 IST
=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.816 Prec@1=79.372 Prec@5=93.752 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=19:08 IST
=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.751 rate=2.14 Hz, eta=0:07:01, total=0:12:27, wall=19:08 IST
=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.751 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=19:08 IST
=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.751 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=19:08 IST
=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.362 Prec@5=93.739 rate=2.14 Hz, eta=0:06:15, total=0:13:15, wall=19:08 IST
=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.362 Prec@5=93.739 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=19:08 IST
=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.362 Prec@5=93.739 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=19:09 IST
=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.743 rate=2.14 Hz, eta=0:05:28, total=0:14:02, wall=19:09 IST
=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.743 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:09 IST
=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.365 Prec@5=93.743 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:10 IST
=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.382 Prec@5=93.744 rate=2.14 Hz, eta=0:04:41, total=0:14:49, wall=19:10 IST
=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.382 Prec@5=93.744 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=19:10 IST
=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.382 Prec@5=93.744 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=19:11 IST
=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.373 Prec@5=93.746 rate=2.14 Hz, eta=0:03:54, total=0:15:36, wall=19:11 IST
=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.373 Prec@5=93.746 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=19:11 IST
=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.373 Prec@5=93.746 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=19:11 IST
=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.366 Prec@5=93.745 rate=2.14 Hz, eta=0:03:07, total=0:16:22, wall=19:11 IST
=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.366 Prec@5=93.745 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=19:11 IST
=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.275 Loss=0.817 Prec@1=79.366 Prec@5=93.745 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=19:12 IST
=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.357 Prec@5=93.748 rate=2.14 Hz, eta=0:02:21, total=0:17:10, wall=19:12 IST
=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.357 Prec@5=93.748 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=19:12 IST
=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.470 DataTime=0.274 Loss=0.817 Prec@1=79.357 Prec@5=93.748 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=19:13 IST
=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:01:34, total=0:17:56, wall=19:13 IST
=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=19:13 IST
=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=19:14 IST
=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:00:47, total=0:18:42, wall=19:14 IST
=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=19:14 IST
=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.348 Prec@5=93.744 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=19:14 IST
=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.346 Prec@5=93.744 rate=2.14 Hz, eta=0:00:00, total=0:19:29, wall=19:14 IST
=> training   100.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.469 DataTime=0.274 Loss=0.817 Prec@1=79.346 Prec@5=93.744 rate=2.14 Hz, eta=0:00:00, total=0:19:30, wall=19:14 IST
[39m[32m
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> validation 0.00% of 1x98...Epoch=150/150 LR=0.00001 Time=6.744 Loss=0.635 Prec@1=83.594 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=19:14 IST
=> validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=6.744 Loss=0.635 Prec@1=83.594 Prec@5=95.312 rate=7002.56 Hz, eta=0:00:00, total=0:00:00, wall=19:14 IST
** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=6.744 Loss=0.635 Prec@1=83.594 Prec@5=95.312 rate=7002.56 Hz, eta=0:00:00, total=0:00:00, wall=19:15 IST
** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=0.548 Loss=1.134 Prec@1=72.130 Prec@5=90.538 rate=7002.56 Hz, eta=0:00:00, total=0:00:00, wall=19:15 IST
** validation 100.00% of 1x98...Epoch=150/150 LR=0.00001 Time=0.548 Loss=1.134 Prec@1=72.130 Prec@5=90.538 rate=2.09 Hz, eta=0:00:00, total=0:00:46, wall=19:15 IST
[39m[39m

