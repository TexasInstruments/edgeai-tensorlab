[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_large_x1', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training_quantize', 'date': '2021-05-07_11-10-14', 'workers': 12, 'epochs': 10, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 10, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 1e-05, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': './data/checkpoints/image_folder_classification/2021-05-07_11-10-14_image_folder_classification_mobilenetv3_lite_large_x1_resize256_crop224/training/model_best.pth', 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'auto_augument': 'imagenet', 'random_erasing': 0.2, 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': True, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False, 'cur_lr': 1.0965826257725021e-05, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fc3c87e6320>}
auto_augument: imagenet
batch_size: 512
best_prec1: -1
beta: 0.999
bias_calibration: True
bias_decay: None
bitwidth_activations: 8
bitwidth_weights: 8
constrain_bias: None
count_flops: True
cur_lr: 1.0965826257725021e-05
data_augument: inception
data_path: ./data/datasets/image_folder_classification
dataset_config: {}
dataset_name: image_folder_classification
date: 2021-05-07_11-10-14
dist_backend: gloo
dist_url: tcp://224.66.41.62:23456
distributed: False
epoch_size: 0
epoch_size_val: 0
epochs: 10
evaluate_start: True
freeze_bn: False
histogram_range: True
image_mean: (123.675, 116.28, 103.53)
image_scale: (0.017125, 0.017507, 0.017429)
img_crop: 224
img_resize: 256
input_channel_reverse: False
iter_size: 1
logger: <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fc3c87e6320>
lr: 1e-05
lr_calib: 0.05
lr_clips: None
milestones: (30, 60, 90)
model: None
model_config: {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}
model_name: mobilenetv3_lite_large_x1
momentum: 0.9
multi_color_modes: None
multistep_gamma: 0.1
num_inputs: 1
opset_version: 11
optimizer: sgd
parallel_model: True
per_channel_q: False
phase: training_quantize
polystep_power: 1.0
pretrained: ./data/checkpoints/image_folder_classification/2021-05-07_11-10-14_image_folder_classification_mobilenetv3_lite_large_x1_resize256_crop224/training/model_best.pth
print_freq: 100
print_model: False
quantize: True
rand_scale: (0.2, 1.0)
rand_seed: 1
random_erasing: 0.2
resume: None
run_soon: True
save_mod_files: False
save_onnx: True
save_path: None
scheduler: cosine
shuffle: True
shuffle_val: True
start_epoch: 0
step_size: 1
stop_epoch: 10
total_batch_size: 512
transforms: None
warmup_epochs: 5
warmup_factor: 0.001
weight_decay: 4e-05
workers: 12
world_size: 1
=> resize resolution: 256
=> crop resolution  : 224
=> using pre-trained weights from: ./data/checkpoints/image_folder_classification/2021-05-07_11-10-14_image_folder_classification_mobilenetv3_lite_large_x1_resize256_crop224/training/model_best.pth
=> creating model 'mobilenetv3_lite_large_x1'
[33m=> model surgery by 'model_surgery_quantize'[39m
[33m=> The following layers in the model could not be loaded from pre-trained: [39m
features.0.0.activation_in.clips_act
features.0.2.clips_act
features.1.block.0.2.clips_act
features.1.block.1.1.activation_q.clips_act
features.2.block.0.2.clips_act
features.2.block.1.2.clips_act
features.2.block.2.1.activation_q.clips_act
features.3.block.0.2.clips_act
features.3.block.1.2.clips_act
features.3.block.2.1.activation_q.clips_act
features.4.block.0.2.clips_act
features.4.block.1.2.clips_act
features.4.block.2.1.activation_q.clips_act
features.5.block.0.2.clips_act
features.5.block.1.2.clips_act
features.5.block.2.1.activation_q.clips_act
features.6.block.0.2.clips_act
features.6.block.1.2.clips_act
features.6.block.2.1.activation_q.clips_act
features.7.block.0.2.clips_act
features.7.block.1.2.clips_act
features.7.block.2.1.activation_q.clips_act
features.8.block.0.2.clips_act
features.8.block.1.2.clips_act
features.8.block.2.1.activation_q.clips_act
features.9.block.0.2.clips_act
features.9.block.1.2.clips_act
features.9.block.2.1.activation_q.clips_act
features.10.block.0.2.clips_act
features.10.block.1.2.clips_act
features.10.block.2.1.activation_q.clips_act
features.11.block.0.2.clips_act
features.11.block.1.2.clips_act
features.11.block.2.1.activation_q.clips_act
features.12.block.0.2.clips_act
features.12.block.1.2.clips_act
features.12.block.2.1.activation_q.clips_act
features.13.block.0.2.clips_act
features.13.block.1.2.clips_act
features.13.block.2.1.activation_q.clips_act
features.14.block.0.2.clips_act
features.14.block.1.2.clips_act
features.14.block.2.1.activation_q.clips_act
features.15.block.0.2.clips_act
features.15.block.1.2.clips_act
features.15.block.2.1.activation_q.clips_act
features.16.2.clips_act
classifier.1.clips_act
classifier.3.activation_q.clips_act
=> Resize = 256, Crop = 224, GFLOPs = 0.42514752, GMACs = 0.21257376
QuantTrainModule(
  (module): MobileNetV3Lite(
    (features): Sequential(
      (0): ConvBNActivation(
        (0): QuantTrainConv2d(
          3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (activation_in): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
        )
        (1): QuantTrainBatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
            (1): QuantTrainBatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (1): QuantTrainBatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
            (1): QuantTrainBatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)
            (1): QuantTrainBatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): QuantTrainBatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): QuantTrainBatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): QuantTrainBatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)
            (1): QuantTrainBatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): QuantTrainBatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)
            (1): QuantTrainBatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): QuantTrainBatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (12): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)
            (1): QuantTrainBatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (13): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)
            (1): QuantTrainBatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (14): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): QuantTrainBatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (15): InvertedResidual(
        (block): Sequential(
          (0): ConvBNActivation(
            (0): QuantTrainConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (1): ConvBNActivation(
            (0): QuantTrainConv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
            (1): QuantTrainBatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
          )
          (2): ConvBNActivation(
            (0): QuantTrainConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): QuantTrainBatchNorm2d(
              160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True
              (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
            )
            (2): Identity()
          )
        )
      )
      (16): ConvBNActivation(
        (0): QuantTrainConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): QuantTrainBatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): QuantTrainLinear(in_features=960, out_features=1280, bias=True)
      (1): QuantTrainPAct2(inplace=False, signed=False, clips=(tensor(0.), tensor(8.)))
      (2): Dropout(p=0.2, inplace=True)
      (3): QuantTrainLinear(
        in_features=1280, out_features=1000, bias=True
        (activation_q): QuantTrainPAct2(inplace=False, signed=None, clips=(tensor(-8.), tensor(8.)))
      )
    )
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_large_x1', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training_quantize', 'date': '2021-05-07_11-10-14', 'workers': 12, 'epochs': 10, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 10, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 1e-05, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': './data/checkpoints/image_folder_classification/2021-05-07_11-10-14_image_folder_classification_mobilenetv3_lite_large_x1_resize256_crop224/training/model_best.pth', 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'auto_augument': 'imagenet', 'random_erasing': 0.2, 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': True, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False, 'cur_lr': 1.0965826257725021e-05, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fc3c87e6320>}
=> optimizer type   : sgd
=> learning rate    : 1e-05
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 10
[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:59 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:59 IST=> validation 0.00% of 1x98...Epoch=1/10 LR=0.00000 Time=8.120 Loss=3.628 Prec@1=5.273 Prec@5=27.734 rate=0 Hz, eta=?, total=0:00:00, wall=06:59 IST=> validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=8.120 Loss=3.628 Prec@1=5.273 Prec@5=27.734 rate=3090.47 Hz, eta=0:00:00, total=0:00:00, wall=06:59 IST** validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=8.120 Loss=3.628 Prec@1=5.273 Prec@5=27.734 rate=3090.47 Hz, eta=0:00:00, total=0:00:00, wall=07:00 IST** validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=0.701 Loss=3.596 Prec@1=6.550 Prec@5=29.928 rate=3090.47 Hz, eta=0:00:00, total=0:00:00, wall=07:00 IST** validation 100.00% of 1x98...Epoch=1/10 LR=0.00000 Time=0.701 Loss=3.596 Prec@1=6.550 Prec@5=29.928 rate=1.62 Hz, eta=0:00:00, total=0:01:00, wall=07:00 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:00 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:00 IST=> training_quantize   0.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=9.654 DataTime=7.309 Loss=1.741 Prec@1=57.812 Prec@5=81.836 rate=0 Hz, eta=?, total=0:00:00, wall=07:00 IST=> training_quantize   0.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=9.654 DataTime=7.309 Loss=1.741 Prec@1=57.812 Prec@5=81.836 rate=6933.56 Hz, eta=0:00:00, total=0:00:00, wall=07:00 IST=> training_quantize   0.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=9.654 DataTime=7.309 Loss=1.741 Prec@1=57.812 Prec@5=81.836 rate=6933.56 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST=> training_quantize   0.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.405 DataTime=0.337 Loss=1.277 Prec@1=68.586 Prec@5=87.788 rate=6933.56 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST=> training_quantize   4.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.405 DataTime=0.337 Loss=1.277 Prec@1=68.586 Prec@5=87.788 rate=0.76 Hz, eta=0:52:26, total=0:02:12, wall=07:02 IST=> training_quantize   4.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.405 DataTime=0.337 Loss=1.277 Prec@1=68.586 Prec@5=87.788 rate=0.76 Hz, eta=0:52:26, total=0:02:12, wall=07:05 IST=> training_quantize   4.04% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.365 DataTime=0.303 Loss=1.266 Prec@1=68.748 Prec@5=88.002 rate=0.76 Hz, eta=0:52:26, total=0:02:12, wall=07:05 IST=> training_quantize   8.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.365 DataTime=0.303 Loss=1.266 Prec@1=68.748 Prec@5=88.002 rate=0.76 Hz, eta=0:50:32, total=0:04:24, wall=07:05 IST=> training_quantize   8.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.365 DataTime=0.303 Loss=1.266 Prec@1=68.748 Prec@5=88.002 rate=0.76 Hz, eta=0:50:32, total=0:04:24, wall=07:07 IST=> training_quantize   8.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.355 DataTime=0.291 Loss=1.252 Prec@1=68.966 Prec@5=88.243 rate=0.76 Hz, eta=0:50:32, total=0:04:24, wall=07:07 IST=> training_quantize   12.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.355 DataTime=0.291 Loss=1.252 Prec@1=68.966 Prec@5=88.243 rate=0.76 Hz, eta=0:48:32, total=0:06:38, wall=07:07 IST=> training_quantize   12.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.355 DataTime=0.291 Loss=1.252 Prec@1=68.966 Prec@5=88.243 rate=0.76 Hz, eta=0:48:32, total=0:06:38, wall=07:09 IST=> training_quantize   12.03% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.348 DataTime=0.286 Loss=1.245 Prec@1=69.119 Prec@5=88.334 rate=0.76 Hz, eta=0:48:32, total=0:06:38, wall=07:09 IST=> training_quantize   16.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.348 DataTime=0.286 Loss=1.245 Prec@1=69.119 Prec@5=88.334 rate=0.76 Hz, eta=0:46:23, total=0:08:51, wall=07:09 IST=> training_quantize   16.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.348 DataTime=0.286 Loss=1.245 Prec@1=69.119 Prec@5=88.334 rate=0.76 Hz, eta=0:46:23, total=0:08:51, wall=07:11 IST=> training_quantize   16.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.340 DataTime=0.282 Loss=1.241 Prec@1=69.235 Prec@5=88.392 rate=0.76 Hz, eta=0:46:23, total=0:08:51, wall=07:11 IST=> training_quantize   20.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.340 DataTime=0.282 Loss=1.241 Prec@1=69.235 Prec@5=88.392 rate=0.76 Hz, eta=0:44:04, total=0:11:01, wall=07:11 IST=> training_quantize   20.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.340 DataTime=0.282 Loss=1.241 Prec@1=69.235 Prec@5=88.392 rate=0.76 Hz, eta=0:44:04, total=0:11:01, wall=07:13 IST=> training_quantize   20.02% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.332 DataTime=0.279 Loss=1.236 Prec@1=69.343 Prec@5=88.441 rate=0.76 Hz, eta=0:44:04, total=0:11:01, wall=07:13 IST=> training_quantize   24.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.332 DataTime=0.279 Loss=1.236 Prec@1=69.343 Prec@5=88.441 rate=0.76 Hz, eta=0:41:42, total=0:13:10, wall=07:13 IST=> training_quantize   24.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.332 DataTime=0.279 Loss=1.236 Prec@1=69.343 Prec@5=88.441 rate=0.76 Hz, eta=0:41:42, total=0:13:10, wall=07:15 IST=> training_quantize   24.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.327 DataTime=0.276 Loss=1.232 Prec@1=69.415 Prec@5=88.494 rate=0.76 Hz, eta=0:41:42, total=0:13:10, wall=07:15 IST=> training_quantize   28.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.327 DataTime=0.276 Loss=1.232 Prec@1=69.415 Prec@5=88.494 rate=0.76 Hz, eta=0:39:25, total=0:15:20, wall=07:15 IST=> training_quantize   28.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.327 DataTime=0.276 Loss=1.232 Prec@1=69.415 Prec@5=88.494 rate=0.76 Hz, eta=0:39:25, total=0:15:20, wall=07:18 IST=> training_quantize   28.01% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.322 DataTime=0.274 Loss=1.232 Prec@1=69.441 Prec@5=88.502 rate=0.76 Hz, eta=0:39:25, total=0:15:20, wall=07:18 IST=> training_quantize   32.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.322 DataTime=0.274 Loss=1.232 Prec@1=69.441 Prec@5=88.502 rate=0.76 Hz, eta=0:37:09, total=0:17:29, wall=07:18 IST=> training_quantize   32.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.322 DataTime=0.274 Loss=1.232 Prec@1=69.441 Prec@5=88.502 rate=0.76 Hz, eta=0:37:09, total=0:17:29, wall=07:20 IST=> training_quantize   32.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.318 DataTime=0.272 Loss=1.231 Prec@1=69.468 Prec@5=88.534 rate=0.76 Hz, eta=0:37:09, total=0:17:29, wall=07:20 IST=> training_quantize   36.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.318 DataTime=0.272 Loss=1.231 Prec@1=69.468 Prec@5=88.534 rate=0.76 Hz, eta=0:34:54, total=0:19:37, wall=07:20 IST=> training_quantize   36.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.318 DataTime=0.272 Loss=1.231 Prec@1=69.468 Prec@5=88.534 rate=0.76 Hz, eta=0:34:54, total=0:19:37, wall=07:22 IST=> training_quantize   36.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.314 DataTime=0.271 Loss=1.229 Prec@1=69.503 Prec@5=88.558 rate=0.76 Hz, eta=0:34:54, total=0:19:37, wall=07:22 IST=> training_quantize   39.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.314 DataTime=0.271 Loss=1.229 Prec@1=69.503 Prec@5=88.558 rate=0.77 Hz, eta=0:32:39, total=0:21:46, wall=07:22 IST=> training_quantize   39.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.314 DataTime=0.271 Loss=1.229 Prec@1=69.503 Prec@5=88.558 rate=0.77 Hz, eta=0:32:39, total=0:21:46, wall=07:24 IST=> training_quantize   39.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.312 DataTime=0.270 Loss=1.228 Prec@1=69.526 Prec@5=88.580 rate=0.77 Hz, eta=0:32:39, total=0:21:46, wall=07:24 IST=> training_quantize   43.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.312 DataTime=0.270 Loss=1.228 Prec@1=69.526 Prec@5=88.580 rate=0.77 Hz, eta=0:30:26, total=0:23:54, wall=07:24 IST=> training_quantize   43.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.312 DataTime=0.270 Loss=1.228 Prec@1=69.526 Prec@5=88.580 rate=0.77 Hz, eta=0:30:26, total=0:23:54, wall=07:26 IST=> training_quantize   43.99% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.309 DataTime=0.269 Loss=1.227 Prec@1=69.558 Prec@5=88.597 rate=0.77 Hz, eta=0:30:26, total=0:23:54, wall=07:26 IST=> training_quantize   47.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.309 DataTime=0.269 Loss=1.227 Prec@1=69.558 Prec@5=88.597 rate=0.77 Hz, eta=0:28:13, total=0:26:02, wall=07:26 IST=> training_quantize   47.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.309 DataTime=0.269 Loss=1.227 Prec@1=69.558 Prec@5=88.597 rate=0.77 Hz, eta=0:28:13, total=0:26:02, wall=07:28 IST=> training_quantize   47.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.306 DataTime=0.268 Loss=1.227 Prec@1=69.567 Prec@5=88.610 rate=0.77 Hz, eta=0:28:13, total=0:26:02, wall=07:28 IST=> training_quantize   51.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.306 DataTime=0.268 Loss=1.227 Prec@1=69.567 Prec@5=88.610 rate=0.77 Hz, eta=0:26:01, total=0:28:09, wall=07:28 IST=> training_quantize   51.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.306 DataTime=0.268 Loss=1.227 Prec@1=69.567 Prec@5=88.610 rate=0.77 Hz, eta=0:26:01, total=0:28:09, wall=07:30 IST=> training_quantize   51.98% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.305 DataTime=0.268 Loss=1.227 Prec@1=69.559 Prec@5=88.619 rate=0.77 Hz, eta=0:26:01, total=0:28:09, wall=07:30 IST=> training_quantize   55.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.305 DataTime=0.268 Loss=1.227 Prec@1=69.559 Prec@5=88.619 rate=0.77 Hz, eta=0:23:50, total=0:30:18, wall=07:30 IST=> training_quantize   55.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.305 DataTime=0.268 Loss=1.227 Prec@1=69.559 Prec@5=88.619 rate=0.77 Hz, eta=0:23:50, total=0:30:18, wall=07:33 IST=> training_quantize   55.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.304 DataTime=0.267 Loss=1.227 Prec@1=69.563 Prec@5=88.611 rate=0.77 Hz, eta=0:23:50, total=0:30:18, wall=07:33 IST=> training_quantize   59.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.304 DataTime=0.267 Loss=1.227 Prec@1=69.563 Prec@5=88.611 rate=0.77 Hz, eta=0:21:40, total=0:32:27, wall=07:33 IST=> training_quantize   59.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.304 DataTime=0.267 Loss=1.227 Prec@1=69.563 Prec@5=88.611 rate=0.77 Hz, eta=0:21:40, total=0:32:27, wall=07:35 IST=> training_quantize   59.97% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.303 DataTime=0.267 Loss=1.227 Prec@1=69.548 Prec@5=88.603 rate=0.77 Hz, eta=0:21:40, total=0:32:27, wall=07:35 IST=> training_quantize   63.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.303 DataTime=0.267 Loss=1.227 Prec@1=69.548 Prec@5=88.603 rate=0.77 Hz, eta=0:19:29, total=0:34:36, wall=07:35 IST=> training_quantize   63.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.303 DataTime=0.267 Loss=1.227 Prec@1=69.548 Prec@5=88.603 rate=0.77 Hz, eta=0:19:29, total=0:34:36, wall=07:37 IST=> training_quantize   63.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.302 DataTime=0.266 Loss=1.227 Prec@1=69.558 Prec@5=88.602 rate=0.77 Hz, eta=0:19:29, total=0:34:36, wall=07:37 IST=> training_quantize   67.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.302 DataTime=0.266 Loss=1.227 Prec@1=69.558 Prec@5=88.602 rate=0.77 Hz, eta=0:17:19, total=0:36:44, wall=07:37 IST=> training_quantize   67.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.302 DataTime=0.266 Loss=1.227 Prec@1=69.558 Prec@5=88.602 rate=0.77 Hz, eta=0:17:19, total=0:36:44, wall=07:39 IST=> training_quantize   67.96% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.301 DataTime=0.266 Loss=1.226 Prec@1=69.584 Prec@5=88.616 rate=0.77 Hz, eta=0:17:19, total=0:36:44, wall=07:39 IST=> training_quantize   71.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.301 DataTime=0.266 Loss=1.226 Prec@1=69.584 Prec@5=88.616 rate=0.77 Hz, eta=0:15:09, total=0:38:53, wall=07:39 IST=> training_quantize   71.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.301 DataTime=0.266 Loss=1.226 Prec@1=69.584 Prec@5=88.616 rate=0.77 Hz, eta=0:15:09, total=0:38:53, wall=07:41 IST=> training_quantize   71.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.603 Prec@5=88.639 rate=0.77 Hz, eta=0:15:09, total=0:38:53, wall=07:41 IST=> training_quantize   75.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.603 Prec@5=88.639 rate=0.77 Hz, eta=0:12:59, total=0:41:02, wall=07:41 IST=> training_quantize   75.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.603 Prec@5=88.639 rate=0.77 Hz, eta=0:12:59, total=0:41:02, wall=07:43 IST=> training_quantize   75.95% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.223 Prec@1=69.627 Prec@5=88.652 rate=0.77 Hz, eta=0:12:59, total=0:41:02, wall=07:43 IST=> training_quantize   79.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.223 Prec@1=69.627 Prec@5=88.652 rate=0.77 Hz, eta=0:10:50, total=0:43:11, wall=07:43 IST=> training_quantize   79.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.223 Prec@1=69.627 Prec@5=88.652 rate=0.77 Hz, eta=0:10:50, total=0:43:11, wall=07:45 IST=> training_quantize   79.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.266 Loss=1.222 Prec@1=69.649 Prec@5=88.666 rate=0.77 Hz, eta=0:10:50, total=0:43:11, wall=07:45 IST=> training_quantize   83.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.266 Loss=1.222 Prec@1=69.649 Prec@5=88.666 rate=0.77 Hz, eta=0:08:40, total=0:45:20, wall=07:45 IST=> training_quantize   83.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.266 Loss=1.222 Prec@1=69.649 Prec@5=88.666 rate=0.77 Hz, eta=0:08:40, total=0:45:20, wall=07:48 IST=> training_quantize   83.94% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.654 Prec@5=88.675 rate=0.77 Hz, eta=0:08:40, total=0:45:20, wall=07:48 IST=> training_quantize   87.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.654 Prec@5=88.675 rate=0.77 Hz, eta=0:06:31, total=0:47:30, wall=07:48 IST=> training_quantize   87.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.654 Prec@5=88.675 rate=0.77 Hz, eta=0:06:31, total=0:47:30, wall=07:50 IST=> training_quantize   87.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.655 Prec@5=88.670 rate=0.77 Hz, eta=0:06:31, total=0:47:30, wall=07:50 IST=> training_quantize   91.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.655 Prec@5=88.670 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=07:50 IST=> training_quantize   91.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.222 Prec@1=69.655 Prec@5=88.670 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=07:52 IST=> training_quantize   91.93% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.222 Prec@1=69.631 Prec@5=88.662 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=07:52 IST=> training_quantize   95.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.222 Prec@1=69.631 Prec@5=88.662 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=07:52 IST=> training_quantize   95.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.222 Prec@1=69.631 Prec@5=88.662 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=07:54 IST=> training_quantize   95.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.622 Prec@5=88.661 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=07:54 IST=> training_quantize   99.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.622 Prec@5=88.661 rate=0.77 Hz, eta=0:00:02, total=0:53:54, wall=07:54 IST=> training_quantize   99.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.622 Prec@5=88.661 rate=0.77 Hz, eta=0:00:02, total=0:53:54, wall=07:54 IST=> training_quantize   99.92% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.621 Prec@5=88.661 rate=0.77 Hz, eta=0:00:02, total=0:53:54, wall=07:54 IST=> training_quantize   100.00% of 1x2503...Epoch=1/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.621 Prec@5=88.661 rate=0.77 Hz, eta=0:00:00, total=0:53:56, wall=07:54 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 4.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 1.000 2.000 2.000 4.000 8.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:54 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:54 IST=> validation 0.00% of 1x98...Epoch=1/10 LR=0.00000 Time=7.861 Loss=1.045 Prec@1=73.828 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=07:54 IST=> validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=7.861 Loss=1.045 Prec@1=73.828 Prec@5=90.820 rate=4318.87 Hz, eta=0:00:00, total=0:00:00, wall=07:54 IST** validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=7.861 Loss=1.045 Prec@1=73.828 Prec@5=90.820 rate=4318.87 Hz, eta=0:00:00, total=0:00:00, wall=07:55 IST** validation 1.02% of 1x98...Epoch=1/10 LR=0.00000 Time=0.689 Loss=1.153 Prec@1=71.238 Prec@5=90.138 rate=4318.87 Hz, eta=0:00:00, total=0:00:00, wall=07:55 IST** validation 100.00% of 1x98...Epoch=1/10 LR=0.00000 Time=0.689 Loss=1.153 Prec@1=71.238 Prec@5=90.138 rate=1.64 Hz, eta=0:00:00, total=0:00:59, wall=07:55 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST=> training_quantize   0.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=8.247 DataTime=5.908 Loss=1.225 Prec@1=71.484 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST=> training_quantize   0.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=8.247 DataTime=5.908 Loss=1.225 Prec@1=71.484 Prec@5=89.648 rate=7478.21 Hz, eta=0:00:00, total=0:00:00, wall=07:55 IST=> training_quantize   0.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=8.247 DataTime=5.908 Loss=1.225 Prec@1=71.484 Prec@5=89.648 rate=7478.21 Hz, eta=0:00:00, total=0:00:00, wall=07:58 IST=> training_quantize   0.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.361 DataTime=0.317 Loss=1.215 Prec@1=69.845 Prec@5=88.738 rate=7478.21 Hz, eta=0:00:00, total=0:00:00, wall=07:58 IST=> training_quantize   4.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.361 DataTime=0.317 Loss=1.215 Prec@1=69.845 Prec@5=88.738 rate=0.78 Hz, eta=0:51:14, total=0:02:09, wall=07:58 IST=> training_quantize   4.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.361 DataTime=0.317 Loss=1.215 Prec@1=69.845 Prec@5=88.738 rate=0.78 Hz, eta=0:51:14, total=0:02:09, wall=08:00 IST=> training_quantize   4.04% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.324 DataTime=0.289 Loss=1.224 Prec@1=69.755 Prec@5=88.655 rate=0.78 Hz, eta=0:51:14, total=0:02:09, wall=08:00 IST=> training_quantize   8.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.324 DataTime=0.289 Loss=1.224 Prec@1=69.755 Prec@5=88.655 rate=0.78 Hz, eta=0:49:13, total=0:04:17, wall=08:00 IST=> training_quantize   8.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.324 DataTime=0.289 Loss=1.224 Prec@1=69.755 Prec@5=88.655 rate=0.78 Hz, eta=0:49:13, total=0:04:17, wall=08:02 IST=> training_quantize   8.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.312 DataTime=0.280 Loss=1.222 Prec@1=69.747 Prec@5=88.689 rate=0.78 Hz, eta=0:49:13, total=0:04:17, wall=08:02 IST=> training_quantize   12.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.312 DataTime=0.280 Loss=1.222 Prec@1=69.747 Prec@5=88.689 rate=0.78 Hz, eta=0:47:08, total=0:06:26, wall=08:02 IST=> training_quantize   12.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.312 DataTime=0.280 Loss=1.222 Prec@1=69.747 Prec@5=88.689 rate=0.78 Hz, eta=0:47:08, total=0:06:26, wall=08:04 IST=> training_quantize   12.03% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.307 DataTime=0.275 Loss=1.223 Prec@1=69.708 Prec@5=88.663 rate=0.78 Hz, eta=0:47:08, total=0:06:26, wall=08:04 IST=> training_quantize   16.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.307 DataTime=0.275 Loss=1.223 Prec@1=69.708 Prec@5=88.663 rate=0.78 Hz, eta=0:45:03, total=0:08:35, wall=08:04 IST=> training_quantize   16.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.307 DataTime=0.275 Loss=1.223 Prec@1=69.708 Prec@5=88.663 rate=0.78 Hz, eta=0:45:03, total=0:08:35, wall=08:06 IST=> training_quantize   16.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.303 DataTime=0.272 Loss=1.221 Prec@1=69.729 Prec@5=88.693 rate=0.78 Hz, eta=0:45:03, total=0:08:35, wall=08:06 IST=> training_quantize   20.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.303 DataTime=0.272 Loss=1.221 Prec@1=69.729 Prec@5=88.693 rate=0.78 Hz, eta=0:42:55, total=0:10:44, wall=08:06 IST=> training_quantize   20.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.303 DataTime=0.272 Loss=1.221 Prec@1=69.729 Prec@5=88.693 rate=0.78 Hz, eta=0:42:55, total=0:10:44, wall=08:08 IST=> training_quantize   20.02% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.302 DataTime=0.270 Loss=1.222 Prec@1=69.722 Prec@5=88.668 rate=0.78 Hz, eta=0:42:55, total=0:10:44, wall=08:08 IST=> training_quantize   24.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.302 DataTime=0.270 Loss=1.222 Prec@1=69.722 Prec@5=88.668 rate=0.78 Hz, eta=0:40:49, total=0:12:54, wall=08:08 IST=> training_quantize   24.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.302 DataTime=0.270 Loss=1.222 Prec@1=69.722 Prec@5=88.668 rate=0.78 Hz, eta=0:40:49, total=0:12:54, wall=08:10 IST=> training_quantize   24.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.221 Prec@1=69.714 Prec@5=88.677 rate=0.78 Hz, eta=0:40:49, total=0:12:54, wall=08:10 IST=> training_quantize   28.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.221 Prec@1=69.714 Prec@5=88.677 rate=0.78 Hz, eta=0:38:42, total=0:15:03, wall=08:10 IST=> training_quantize   28.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.221 Prec@1=69.714 Prec@5=88.677 rate=0.78 Hz, eta=0:38:42, total=0:15:03, wall=08:13 IST=> training_quantize   28.01% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.712 Prec@5=88.704 rate=0.78 Hz, eta=0:38:42, total=0:15:03, wall=08:13 IST=> training_quantize   32.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.712 Prec@5=88.704 rate=0.77 Hz, eta=0:36:37, total=0:17:14, wall=08:13 IST=> training_quantize   32.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.712 Prec@5=88.704 rate=0.77 Hz, eta=0:36:37, total=0:17:14, wall=08:15 IST=> training_quantize   32.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.710 Prec@5=88.694 rate=0.77 Hz, eta=0:36:37, total=0:17:14, wall=08:15 IST=> training_quantize   36.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.710 Prec@5=88.694 rate=0.77 Hz, eta=0:34:29, total=0:19:24, wall=08:15 IST=> training_quantize   36.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.220 Prec@1=69.710 Prec@5=88.694 rate=0.77 Hz, eta=0:34:29, total=0:19:24, wall=08:17 IST=> training_quantize   36.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.222 Prec@1=69.636 Prec@5=88.659 rate=0.77 Hz, eta=0:34:29, total=0:19:24, wall=08:17 IST=> training_quantize   39.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.222 Prec@1=69.636 Prec@5=88.659 rate=0.77 Hz, eta=0:32:21, total=0:21:33, wall=08:17 IST=> training_quantize   39.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.301 DataTime=0.268 Loss=1.222 Prec@1=69.636 Prec@5=88.659 rate=0.77 Hz, eta=0:32:21, total=0:21:33, wall=08:19 IST=> training_quantize   39.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.223 Prec@1=69.605 Prec@5=88.639 rate=0.77 Hz, eta=0:32:21, total=0:21:33, wall=08:19 IST=> training_quantize   43.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.223 Prec@1=69.605 Prec@5=88.639 rate=0.77 Hz, eta=0:30:12, total=0:23:43, wall=08:19 IST=> training_quantize   43.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.223 Prec@1=69.605 Prec@5=88.639 rate=0.77 Hz, eta=0:30:12, total=0:23:43, wall=08:21 IST=> training_quantize   43.99% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.224 Prec@1=69.610 Prec@5=88.632 rate=0.77 Hz, eta=0:30:12, total=0:23:43, wall=08:21 IST=> training_quantize   47.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.224 Prec@1=69.610 Prec@5=88.632 rate=0.77 Hz, eta=0:28:03, total=0:25:53, wall=08:21 IST=> training_quantize   47.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.267 Loss=1.224 Prec@1=69.610 Prec@5=88.632 rate=0.77 Hz, eta=0:28:03, total=0:25:53, wall=08:23 IST=> training_quantize   47.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.585 Prec@5=88.621 rate=0.77 Hz, eta=0:28:03, total=0:25:53, wall=08:23 IST=> training_quantize   51.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.585 Prec@5=88.621 rate=0.77 Hz, eta=0:25:55, total=0:28:03, wall=08:23 IST=> training_quantize   51.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.585 Prec@5=88.621 rate=0.77 Hz, eta=0:25:55, total=0:28:03, wall=08:26 IST=> training_quantize   51.98% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.584 Prec@5=88.632 rate=0.77 Hz, eta=0:25:55, total=0:28:03, wall=08:26 IST=> training_quantize   55.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.584 Prec@5=88.632 rate=0.77 Hz, eta=0:23:46, total=0:30:13, wall=08:26 IST=> training_quantize   55.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.584 Prec@5=88.632 rate=0.77 Hz, eta=0:23:46, total=0:30:13, wall=08:28 IST=> training_quantize   55.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.597 Prec@5=88.641 rate=0.77 Hz, eta=0:23:46, total=0:30:13, wall=08:28 IST=> training_quantize   59.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.597 Prec@5=88.641 rate=0.77 Hz, eta=0:21:37, total=0:32:23, wall=08:28 IST=> training_quantize   59.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.224 Prec@1=69.597 Prec@5=88.641 rate=0.77 Hz, eta=0:21:37, total=0:32:23, wall=08:30 IST=> training_quantize   59.97% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.582 Prec@5=88.632 rate=0.77 Hz, eta=0:21:37, total=0:32:23, wall=08:30 IST=> training_quantize   63.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.582 Prec@5=88.632 rate=0.77 Hz, eta=0:19:28, total=0:34:33, wall=08:30 IST=> training_quantize   63.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.582 Prec@5=88.632 rate=0.77 Hz, eta=0:19:28, total=0:34:33, wall=08:32 IST=> training_quantize   63.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.576 Prec@5=88.628 rate=0.77 Hz, eta=0:19:28, total=0:34:33, wall=08:32 IST=> training_quantize   67.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.576 Prec@5=88.628 rate=0.77 Hz, eta=0:17:18, total=0:36:42, wall=08:32 IST=> training_quantize   67.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.300 DataTime=0.266 Loss=1.225 Prec@1=69.576 Prec@5=88.628 rate=0.77 Hz, eta=0:17:18, total=0:36:42, wall=08:34 IST=> training_quantize   67.96% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.578 Prec@5=88.632 rate=0.77 Hz, eta=0:17:18, total=0:36:42, wall=08:34 IST=> training_quantize   71.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.578 Prec@5=88.632 rate=0.77 Hz, eta=0:15:08, total=0:38:52, wall=08:34 IST=> training_quantize   71.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.578 Prec@5=88.632 rate=0.77 Hz, eta=0:15:08, total=0:38:52, wall=08:36 IST=> training_quantize   71.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.591 Prec@5=88.637 rate=0.77 Hz, eta=0:15:08, total=0:38:52, wall=08:36 IST=> training_quantize   75.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.591 Prec@5=88.637 rate=0.77 Hz, eta=0:12:59, total=0:41:01, wall=08:36 IST=> training_quantize   75.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.299 DataTime=0.265 Loss=1.224 Prec@1=69.591 Prec@5=88.637 rate=0.77 Hz, eta=0:12:59, total=0:41:01, wall=08:39 IST=> training_quantize   75.95% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.584 Prec@5=88.645 rate=0.77 Hz, eta=0:12:59, total=0:41:01, wall=08:39 IST=> training_quantize   79.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.584 Prec@5=88.645 rate=0.77 Hz, eta=0:10:49, total=0:43:09, wall=08:39 IST=> training_quantize   79.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.584 Prec@5=88.645 rate=0.77 Hz, eta=0:10:49, total=0:43:09, wall=08:41 IST=> training_quantize   79.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.223 Prec@1=69.593 Prec@5=88.640 rate=0.77 Hz, eta=0:10:49, total=0:43:09, wall=08:41 IST=> training_quantize   83.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.223 Prec@1=69.593 Prec@5=88.640 rate=0.77 Hz, eta=0:08:40, total=0:45:19, wall=08:41 IST=> training_quantize   83.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.223 Prec@1=69.593 Prec@5=88.640 rate=0.77 Hz, eta=0:08:40, total=0:45:19, wall=08:43 IST=> training_quantize   83.94% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.594 Prec@5=88.638 rate=0.77 Hz, eta=0:08:40, total=0:45:19, wall=08:43 IST=> training_quantize   87.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.594 Prec@5=88.638 rate=0.77 Hz, eta=0:06:30, total=0:47:29, wall=08:43 IST=> training_quantize   87.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.265 Loss=1.224 Prec@1=69.594 Prec@5=88.638 rate=0.77 Hz, eta=0:06:30, total=0:47:29, wall=08:45 IST=> training_quantize   87.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.224 Prec@1=69.597 Prec@5=88.642 rate=0.77 Hz, eta=0:06:30, total=0:47:29, wall=08:45 IST=> training_quantize   91.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.224 Prec@1=69.597 Prec@5=88.642 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=08:45 IST=> training_quantize   91.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.224 Prec@1=69.597 Prec@5=88.642 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=08:47 IST=> training_quantize   91.93% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.223 Prec@1=69.598 Prec@5=88.647 rate=0.77 Hz, eta=0:04:21, total=0:49:38, wall=08:47 IST=> training_quantize   95.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.223 Prec@1=69.598 Prec@5=88.647 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=08:47 IST=> training_quantize   95.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.298 DataTime=0.264 Loss=1.223 Prec@1=69.598 Prec@5=88.647 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=08:49 IST=> training_quantize   95.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.297 DataTime=0.264 Loss=1.223 Prec@1=69.604 Prec@5=88.659 rate=0.77 Hz, eta=0:02:12, total=0:51:47, wall=08:49 IST=> training_quantize   99.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.297 DataTime=0.264 Loss=1.223 Prec@1=69.604 Prec@5=88.659 rate=0.77 Hz, eta=0:00:02, total=0:53:55, wall=08:49 IST=> training_quantize   99.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.297 DataTime=0.264 Loss=1.223 Prec@1=69.604 Prec@5=88.659 rate=0.77 Hz, eta=0:00:02, total=0:53:55, wall=08:49 IST=> training_quantize   99.92% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.297 DataTime=0.264 Loss=1.223 Prec@1=69.605 Prec@5=88.659 rate=0.77 Hz, eta=0:00:02, total=0:53:55, wall=08:49 IST=> training_quantize   100.00% of 1x2503...Epoch=2/10 LR=0.00000 Time=1.297 DataTime=0.264 Loss=1.223 Prec@1=69.605 Prec@5=88.659 rate=0.77 Hz, eta=0:00:00, total=0:53:57, wall=08:49 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 4.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 2.000 2.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 1.000 2.000 2.000 4.000 8.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST=> validation 0.00% of 1x98...Epoch=2/10 LR=0.00000 Time=8.247 Loss=1.124 Prec@1=72.461 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST=> validation 1.02% of 1x98...Epoch=2/10 LR=0.00000 Time=8.247 Loss=1.124 Prec@1=72.461 Prec@5=90.625 rate=6807.26 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST** validation 1.02% of 1x98...Epoch=2/10 LR=0.00000 Time=8.247 Loss=1.124 Prec@1=72.461 Prec@5=90.625 rate=6807.26 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST** validation 1.02% of 1x98...Epoch=2/10 LR=0.00000 Time=0.694 Loss=1.162 Prec@1=71.136 Prec@5=90.046 rate=6807.26 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST** validation 100.00% of 1x98...Epoch=2/10 LR=0.00000 Time=0.694 Loss=1.162 Prec@1=71.136 Prec@5=90.046 rate=1.64 Hz, eta=0:00:00, total=0:00:59, wall=08:50 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training_quantize   0.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=8.685 DataTime=6.869 Loss=1.326 Prec@1=67.188 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training_quantize   0.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=8.685 DataTime=6.869 Loss=1.326 Prec@1=67.188 Prec@5=88.086 rate=6384.56 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST=> training_quantize   0.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=8.685 DataTime=6.869 Loss=1.326 Prec@1=67.188 Prec@5=88.086 rate=6384.56 Hz, eta=0:00:00, total=0:00:00, wall=08:53 IST=> training_quantize   0.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.373 DataTime=0.329 Loss=1.228 Prec@1=69.566 Prec@5=88.500 rate=6384.56 Hz, eta=0:00:00, total=0:00:00, wall=08:53 IST=> training_quantize   4.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.373 DataTime=0.329 Loss=1.228 Prec@1=69.566 Prec@5=88.500 rate=0.78 Hz, eta=0:51:32, total=0:02:10, wall=08:53 IST=> training_quantize   4.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.373 DataTime=0.329 Loss=1.228 Prec@1=69.566 Prec@5=88.500 rate=0.78 Hz, eta=0:51:32, total=0:02:10, wall=08:55 IST=> training_quantize   4.04% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.330 DataTime=0.294 Loss=1.228 Prec@1=69.470 Prec@5=88.604 rate=0.78 Hz, eta=0:51:32, total=0:02:10, wall=08:55 IST=> training_quantize   8.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.330 DataTime=0.294 Loss=1.228 Prec@1=69.470 Prec@5=88.604 rate=0.78 Hz, eta=0:49:21, total=0:04:18, wall=08:55 IST=> training_quantize   8.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.330 DataTime=0.294 Loss=1.228 Prec@1=69.470 Prec@5=88.604 rate=0.78 Hz, eta=0:49:21, total=0:04:18, wall=08:57 IST=> training_quantize   8.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.316 DataTime=0.283 Loss=1.227 Prec@1=69.424 Prec@5=88.613 rate=0.78 Hz, eta=0:49:21, total=0:04:18, wall=08:57 IST=> training_quantize   12.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.316 DataTime=0.283 Loss=1.227 Prec@1=69.424 Prec@5=88.613 rate=0.78 Hz, eta=0:47:15, total=0:06:27, wall=08:57 IST=> training_quantize   12.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.316 DataTime=0.283 Loss=1.227 Prec@1=69.424 Prec@5=88.613 rate=0.78 Hz, eta=0:47:15, total=0:06:27, wall=08:59 IST=> training_quantize   12.03% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.311 DataTime=0.278 Loss=1.228 Prec@1=69.419 Prec@5=88.576 rate=0.78 Hz, eta=0:47:15, total=0:06:27, wall=08:59 IST=> training_quantize   16.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.311 DataTime=0.278 Loss=1.228 Prec@1=69.419 Prec@5=88.576 rate=0.78 Hz, eta=0:45:10, total=0:08:37, wall=08:59 IST=> training_quantize   16.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.311 DataTime=0.278 Loss=1.228 Prec@1=69.419 Prec@5=88.576 rate=0.78 Hz, eta=0:45:10, total=0:08:37, wall=09:01 IST=> training_quantize   16.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.309 DataTime=0.275 Loss=1.224 Prec@1=69.515 Prec@5=88.628 rate=0.78 Hz, eta=0:45:10, total=0:08:37, wall=09:01 IST=> training_quantize   20.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.309 DataTime=0.275 Loss=1.224 Prec@1=69.515 Prec@5=88.628 rate=0.77 Hz, eta=0:43:05, total=0:10:47, wall=09:01 IST=> training_quantize   20.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.309 DataTime=0.275 Loss=1.224 Prec@1=69.515 Prec@5=88.628 rate=0.77 Hz, eta=0:43:05, total=0:10:47, wall=09:04 IST=> training_quantize   20.02% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.306 DataTime=0.273 Loss=1.225 Prec@1=69.498 Prec@5=88.628 rate=0.77 Hz, eta=0:43:05, total=0:10:47, wall=09:04 IST=> training_quantize   24.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.306 DataTime=0.273 Loss=1.225 Prec@1=69.498 Prec@5=88.628 rate=0.77 Hz, eta=0:40:57, total=0:12:56, wall=09:04 IST=> training_quantize   24.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.306 DataTime=0.273 Loss=1.225 Prec@1=69.498 Prec@5=88.628 rate=0.77 Hz, eta=0:40:57, total=0:12:56, wall=09:06 IST=> training_quantize   24.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.304 DataTime=0.271 Loss=1.225 Prec@1=69.476 Prec@5=88.626 rate=0.77 Hz, eta=0:40:57, total=0:12:56, wall=09:06 IST=> training_quantize   28.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.304 DataTime=0.271 Loss=1.225 Prec@1=69.476 Prec@5=88.626 rate=0.77 Hz, eta=0:38:47, total=0:15:05, wall=09:06 IST=> training_quantize   28.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.304 DataTime=0.271 Loss=1.225 Prec@1=69.476 Prec@5=88.626 rate=0.77 Hz, eta=0:38:47, total=0:15:05, wall=09:08 IST=> training_quantize   28.01% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.226 Prec@1=69.444 Prec@5=88.599 rate=0.77 Hz, eta=0:38:47, total=0:15:05, wall=09:08 IST=> training_quantize   32.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.226 Prec@1=69.444 Prec@5=88.599 rate=0.78 Hz, eta=0:36:35, total=0:17:13, wall=09:08 IST=> training_quantize   32.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.301 DataTime=0.269 Loss=1.226 Prec@1=69.444 Prec@5=88.599 rate=0.78 Hz, eta=0:36:35, total=0:17:13, wall=09:10 IST=> training_quantize   32.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.299 DataTime=0.268 Loss=1.227 Prec@1=69.416 Prec@5=88.595 rate=0.78 Hz, eta=0:36:35, total=0:17:13, wall=09:10 IST=> training_quantize   36.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.299 DataTime=0.268 Loss=1.227 Prec@1=69.416 Prec@5=88.595 rate=0.78 Hz, eta=0:34:25, total=0:19:21, wall=09:10 IST=> training_quantize   36.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.299 DataTime=0.268 Loss=1.227 Prec@1=69.416 Prec@5=88.595 rate=0.78 Hz, eta=0:34:25, total=0:19:21, wall=09:12 IST=> training_quantize   36.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.223 Prec@1=69.489 Prec@5=88.649 rate=0.78 Hz, eta=0:34:25, total=0:19:21, wall=09:12 IST=> training_quantize   39.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.223 Prec@1=69.489 Prec@5=88.649 rate=0.78 Hz, eta=0:32:15, total=0:21:29, wall=09:12 IST=> training_quantize   39.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.223 Prec@1=69.489 Prec@5=88.649 rate=0.78 Hz, eta=0:32:15, total=0:21:29, wall=09:14 IST=> training_quantize   39.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.222 Prec@1=69.523 Prec@5=88.651 rate=0.78 Hz, eta=0:32:15, total=0:21:29, wall=09:14 IST=> training_quantize   43.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.222 Prec@1=69.523 Prec@5=88.651 rate=0.78 Hz, eta=0:30:07, total=0:23:39, wall=09:14 IST=> training_quantize   43.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.267 Loss=1.222 Prec@1=69.523 Prec@5=88.651 rate=0.78 Hz, eta=0:30:07, total=0:23:39, wall=09:17 IST=> training_quantize   43.99% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.558 Prec@5=88.668 rate=0.78 Hz, eta=0:30:07, total=0:23:39, wall=09:17 IST=> training_quantize   47.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.558 Prec@5=88.668 rate=0.78 Hz, eta=0:27:58, total=0:25:48, wall=09:17 IST=> training_quantize   47.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.558 Prec@5=88.668 rate=0.78 Hz, eta=0:27:58, total=0:25:48, wall=09:19 IST=> training_quantize   47.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.571 Prec@5=88.671 rate=0.78 Hz, eta=0:27:58, total=0:25:48, wall=09:19 IST=> training_quantize   51.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.571 Prec@5=88.671 rate=0.78 Hz, eta=0:25:50, total=0:27:58, wall=09:19 IST=> training_quantize   51.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.571 Prec@5=88.671 rate=0.78 Hz, eta=0:25:50, total=0:27:58, wall=09:21 IST=> training_quantize   51.98% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.221 Prec@1=69.594 Prec@5=88.679 rate=0.78 Hz, eta=0:25:50, total=0:27:58, wall=09:21 IST=> training_quantize   55.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.221 Prec@1=69.594 Prec@5=88.679 rate=0.77 Hz, eta=0:23:42, total=0:30:08, wall=09:21 IST=> training_quantize   55.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.221 Prec@1=69.594 Prec@5=88.679 rate=0.77 Hz, eta=0:23:42, total=0:30:08, wall=09:23 IST=> training_quantize   55.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.597 Prec@5=88.667 rate=0.77 Hz, eta=0:23:42, total=0:30:08, wall=09:23 IST=> training_quantize   59.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.597 Prec@5=88.667 rate=0.77 Hz, eta=0:21:33, total=0:32:18, wall=09:23 IST=> training_quantize   59.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.266 Loss=1.222 Prec@1=69.597 Prec@5=88.667 rate=0.77 Hz, eta=0:21:33, total=0:32:18, wall=09:25 IST=> training_quantize   59.97% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.599 Prec@5=88.666 rate=0.77 Hz, eta=0:21:33, total=0:32:18, wall=09:25 IST=> training_quantize   63.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.599 Prec@5=88.666 rate=0.77 Hz, eta=0:19:24, total=0:34:27, wall=09:25 IST=> training_quantize   63.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.297 DataTime=0.265 Loss=1.222 Prec@1=69.599 Prec@5=88.666 rate=0.77 Hz, eta=0:19:24, total=0:34:27, wall=09:27 IST=> training_quantize   63.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.222 Prec@1=69.607 Prec@5=88.672 rate=0.77 Hz, eta=0:19:24, total=0:34:27, wall=09:27 IST=> training_quantize   67.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.222 Prec@1=69.607 Prec@5=88.672 rate=0.77 Hz, eta=0:17:15, total=0:36:36, wall=09:27 IST=> training_quantize   67.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.222 Prec@1=69.607 Prec@5=88.672 rate=0.77 Hz, eta=0:17:15, total=0:36:36, wall=09:29 IST=> training_quantize   67.96% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.221 Prec@1=69.626 Prec@5=88.686 rate=0.77 Hz, eta=0:17:15, total=0:36:36, wall=09:29 IST=> training_quantize   71.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.221 Prec@1=69.626 Prec@5=88.686 rate=0.77 Hz, eta=0:15:06, total=0:38:45, wall=09:29 IST=> training_quantize   71.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.221 Prec@1=69.626 Prec@5=88.686 rate=0.77 Hz, eta=0:15:06, total=0:38:45, wall=09:32 IST=> training_quantize   71.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.220 Prec@1=69.644 Prec@5=88.698 rate=0.77 Hz, eta=0:15:06, total=0:38:45, wall=09:32 IST=> training_quantize   75.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.220 Prec@1=69.644 Prec@5=88.698 rate=0.77 Hz, eta=0:12:57, total=0:40:54, wall=09:32 IST=> training_quantize   75.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.296 DataTime=0.265 Loss=1.220 Prec@1=69.644 Prec@5=88.698 rate=0.77 Hz, eta=0:12:57, total=0:40:54, wall=09:34 IST=> training_quantize   75.95% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.289 DataTime=0.263 Loss=1.220 Prec@1=69.661 Prec@5=88.695 rate=0.77 Hz, eta=0:12:57, total=0:40:54, wall=09:34 IST=> training_quantize   79.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.289 DataTime=0.263 Loss=1.220 Prec@1=69.661 Prec@5=88.695 rate=0.78 Hz, eta=0:10:44, total=0:42:50, wall=09:34 IST=> training_quantize   79.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.289 DataTime=0.263 Loss=1.220 Prec@1=69.661 Prec@5=88.695 rate=0.78 Hz, eta=0:10:44, total=0:42:50, wall=09:35 IST=> training_quantize   79.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.283 DataTime=0.261 Loss=1.220 Prec@1=69.670 Prec@5=88.689 rate=0.78 Hz, eta=0:10:44, total=0:42:50, wall=09:35 IST=> training_quantize   83.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.283 DataTime=0.261 Loss=1.220 Prec@1=69.670 Prec@5=88.689 rate=0.78 Hz, eta=0:08:33, total=0:44:45, wall=09:35 IST=> training_quantize   83.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.283 DataTime=0.261 Loss=1.220 Prec@1=69.670 Prec@5=88.689 rate=0.78 Hz, eta=0:08:33, total=0:44:45, wall=09:37 IST=> training_quantize   83.94% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.277 DataTime=0.260 Loss=1.219 Prec@1=69.660 Prec@5=88.696 rate=0.78 Hz, eta=0:08:33, total=0:44:45, wall=09:37 IST=> training_quantize   87.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.277 DataTime=0.260 Loss=1.219 Prec@1=69.660 Prec@5=88.696 rate=0.79 Hz, eta=0:06:24, total=0:46:40, wall=09:37 IST=> training_quantize   87.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.277 DataTime=0.260 Loss=1.219 Prec@1=69.660 Prec@5=88.696 rate=0.79 Hz, eta=0:06:24, total=0:46:40, wall=09:39 IST=> training_quantize   87.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.271 DataTime=0.258 Loss=1.219 Prec@1=69.680 Prec@5=88.704 rate=0.79 Hz, eta=0:06:24, total=0:46:40, wall=09:39 IST=> training_quantize   91.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.271 DataTime=0.258 Loss=1.219 Prec@1=69.680 Prec@5=88.704 rate=0.79 Hz, eta=0:04:15, total=0:48:35, wall=09:39 IST=> training_quantize   91.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.271 DataTime=0.258 Loss=1.219 Prec@1=69.680 Prec@5=88.704 rate=0.79 Hz, eta=0:04:15, total=0:48:35, wall=09:41 IST=> training_quantize   91.93% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.266 DataTime=0.257 Loss=1.219 Prec@1=69.673 Prec@5=88.703 rate=0.79 Hz, eta=0:04:15, total=0:48:35, wall=09:41 IST=> training_quantize   95.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.266 DataTime=0.257 Loss=1.219 Prec@1=69.673 Prec@5=88.703 rate=0.79 Hz, eta=0:02:08, total=0:50:31, wall=09:41 IST=> training_quantize   95.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.266 DataTime=0.257 Loss=1.219 Prec@1=69.673 Prec@5=88.703 rate=0.79 Hz, eta=0:02:08, total=0:50:31, wall=09:43 IST=> training_quantize   95.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.261 DataTime=0.256 Loss=1.219 Prec@1=69.679 Prec@5=88.705 rate=0.79 Hz, eta=0:02:08, total=0:50:31, wall=09:43 IST=> training_quantize   99.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.261 DataTime=0.256 Loss=1.219 Prec@1=69.679 Prec@5=88.705 rate=0.80 Hz, eta=0:00:02, total=0:52:24, wall=09:43 IST=> training_quantize   99.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.261 DataTime=0.256 Loss=1.219 Prec@1=69.679 Prec@5=88.705 rate=0.80 Hz, eta=0:00:02, total=0:52:24, wall=09:43 IST=> training_quantize   99.92% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.261 DataTime=0.256 Loss=1.219 Prec@1=69.680 Prec@5=88.706 rate=0.80 Hz, eta=0:00:02, total=0:52:24, wall=09:43 IST=> training_quantize   100.00% of 1x2503...Epoch=3/10 LR=0.00000 Time=1.261 DataTime=0.256 Loss=1.219 Prec@1=69.680 Prec@5=88.706 rate=0.80 Hz, eta=0:00:00, total=0:52:26, wall=09:43 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 4.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 1.000 2.000 2.000 4.000 8.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:43 IST=> validation 0.00% of 1x98...Epoch=3/10 LR=0.00000 Time=7.248 Loss=1.121 Prec@1=71.094 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=09:43 IST=> validation 1.02% of 1x98...Epoch=3/10 LR=0.00000 Time=7.248 Loss=1.121 Prec@1=71.094 Prec@5=92.188 rate=5199.37 Hz, eta=0:00:00, total=0:00:00, wall=09:43 IST** validation 1.02% of 1x98...Epoch=3/10 LR=0.00000 Time=7.248 Loss=1.121 Prec@1=71.094 Prec@5=92.188 rate=5199.37 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST** validation 1.02% of 1x98...Epoch=3/10 LR=0.00000 Time=0.641 Loss=1.153 Prec@1=71.348 Prec@5=90.092 rate=5199.37 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST** validation 100.00% of 1x98...Epoch=3/10 LR=0.00000 Time=0.641 Loss=1.153 Prec@1=71.348 Prec@5=90.092 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=09:44 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> training_quantize   0.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=7.304 DataTime=6.009 Loss=1.173 Prec@1=71.484 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> training_quantize   0.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=7.304 DataTime=6.009 Loss=1.173 Prec@1=71.484 Prec@5=88.281 rate=7356.68 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST=> training_quantize   0.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=7.304 DataTime=6.009 Loss=1.173 Prec@1=71.484 Prec@5=88.281 rate=7356.68 Hz, eta=0:00:00, total=0:00:00, wall=09:46 IST=> training_quantize   0.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.226 DataTime=0.285 Loss=1.222 Prec@1=69.676 Prec@5=88.504 rate=7356.68 Hz, eta=0:00:00, total=0:00:00, wall=09:46 IST=> training_quantize   4.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.226 DataTime=0.285 Loss=1.222 Prec@1=69.676 Prec@5=88.504 rate=0.87 Hz, eta=0:46:11, total=0:01:56, wall=09:46 IST=> training_quantize   4.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.226 DataTime=0.285 Loss=1.222 Prec@1=69.676 Prec@5=88.504 rate=0.87 Hz, eta=0:46:11, total=0:01:56, wall=09:48 IST=> training_quantize   4.04% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.194 DataTime=0.258 Loss=1.220 Prec@1=69.624 Prec@5=88.596 rate=0.87 Hz, eta=0:46:11, total=0:01:56, wall=09:48 IST=> training_quantize   8.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.194 DataTime=0.258 Loss=1.220 Prec@1=69.624 Prec@5=88.596 rate=0.86 Hz, eta=0:44:24, total=0:03:52, wall=09:48 IST=> training_quantize   8.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.194 DataTime=0.258 Loss=1.220 Prec@1=69.624 Prec@5=88.596 rate=0.86 Hz, eta=0:44:24, total=0:03:52, wall=09:50 IST=> training_quantize   8.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.181 DataTime=0.248 Loss=1.217 Prec@1=69.658 Prec@5=88.667 rate=0.86 Hz, eta=0:44:24, total=0:03:52, wall=09:50 IST=> training_quantize   12.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.181 DataTime=0.248 Loss=1.217 Prec@1=69.658 Prec@5=88.667 rate=0.86 Hz, eta=0:42:28, total=0:05:48, wall=09:50 IST=> training_quantize   12.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.181 DataTime=0.248 Loss=1.217 Prec@1=69.658 Prec@5=88.667 rate=0.86 Hz, eta=0:42:28, total=0:05:48, wall=09:52 IST=> training_quantize   12.03% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.173 DataTime=0.243 Loss=1.219 Prec@1=69.645 Prec@5=88.652 rate=0.86 Hz, eta=0:42:28, total=0:05:48, wall=09:52 IST=> training_quantize   16.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.173 DataTime=0.243 Loss=1.219 Prec@1=69.645 Prec@5=88.652 rate=0.87 Hz, eta=0:40:27, total=0:07:43, wall=09:52 IST=> training_quantize   16.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.173 DataTime=0.243 Loss=1.219 Prec@1=69.645 Prec@5=88.652 rate=0.87 Hz, eta=0:40:27, total=0:07:43, wall=09:54 IST=> training_quantize   16.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.168 DataTime=0.239 Loss=1.217 Prec@1=69.685 Prec@5=88.691 rate=0.87 Hz, eta=0:40:27, total=0:07:43, wall=09:54 IST=> training_quantize   20.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.168 DataTime=0.239 Loss=1.217 Prec@1=69.685 Prec@5=88.691 rate=0.87 Hz, eta=0:38:28, total=0:09:37, wall=09:54 IST=> training_quantize   20.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.168 DataTime=0.239 Loss=1.217 Prec@1=69.685 Prec@5=88.691 rate=0.87 Hz, eta=0:38:28, total=0:09:37, wall=09:56 IST=> training_quantize   20.02% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.165 DataTime=0.237 Loss=1.217 Prec@1=69.707 Prec@5=88.692 rate=0.87 Hz, eta=0:38:28, total=0:09:37, wall=09:56 IST=> training_quantize   24.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.165 DataTime=0.237 Loss=1.217 Prec@1=69.707 Prec@5=88.692 rate=0.87 Hz, eta=0:36:32, total=0:11:32, wall=09:56 IST=> training_quantize   24.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.165 DataTime=0.237 Loss=1.217 Prec@1=69.707 Prec@5=88.692 rate=0.87 Hz, eta=0:36:32, total=0:11:32, wall=09:58 IST=> training_quantize   24.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.164 DataTime=0.236 Loss=1.217 Prec@1=69.726 Prec@5=88.689 rate=0.87 Hz, eta=0:36:32, total=0:11:32, wall=09:58 IST=> training_quantize   28.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.164 DataTime=0.236 Loss=1.217 Prec@1=69.726 Prec@5=88.689 rate=0.87 Hz, eta=0:34:38, total=0:13:28, wall=09:58 IST=> training_quantize   28.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.164 DataTime=0.236 Loss=1.217 Prec@1=69.726 Prec@5=88.689 rate=0.87 Hz, eta=0:34:38, total=0:13:28, wall=10:00 IST=> training_quantize   28.01% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.162 DataTime=0.235 Loss=1.217 Prec@1=69.707 Prec@5=88.687 rate=0.87 Hz, eta=0:34:38, total=0:13:28, wall=10:00 IST=> training_quantize   32.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.162 DataTime=0.235 Loss=1.217 Prec@1=69.707 Prec@5=88.687 rate=0.87 Hz, eta=0:32:42, total=0:15:23, wall=10:00 IST=> training_quantize   32.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.162 DataTime=0.235 Loss=1.217 Prec@1=69.707 Prec@5=88.687 rate=0.87 Hz, eta=0:32:42, total=0:15:23, wall=10:02 IST=> training_quantize   32.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.161 DataTime=0.234 Loss=1.217 Prec@1=69.688 Prec@5=88.699 rate=0.87 Hz, eta=0:32:42, total=0:15:23, wall=10:02 IST=> training_quantize   36.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.161 DataTime=0.234 Loss=1.217 Prec@1=69.688 Prec@5=88.699 rate=0.87 Hz, eta=0:30:46, total=0:17:18, wall=10:02 IST=> training_quantize   36.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.161 DataTime=0.234 Loss=1.217 Prec@1=69.688 Prec@5=88.699 rate=0.87 Hz, eta=0:30:46, total=0:17:18, wall=10:04 IST=> training_quantize   36.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.217 Prec@1=69.692 Prec@5=88.723 rate=0.87 Hz, eta=0:30:46, total=0:17:18, wall=10:04 IST=> training_quantize   39.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.217 Prec@1=69.692 Prec@5=88.723 rate=0.87 Hz, eta=0:28:51, total=0:19:13, wall=10:04 IST=> training_quantize   39.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.217 Prec@1=69.692 Prec@5=88.723 rate=0.87 Hz, eta=0:28:51, total=0:19:13, wall=10:06 IST=> training_quantize   39.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.216 Prec@1=69.708 Prec@5=88.744 rate=0.87 Hz, eta=0:28:51, total=0:19:13, wall=10:06 IST=> training_quantize   43.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.216 Prec@1=69.708 Prec@5=88.744 rate=0.87 Hz, eta=0:26:57, total=0:21:10, wall=10:06 IST=> training_quantize   43.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.216 Prec@1=69.708 Prec@5=88.744 rate=0.87 Hz, eta=0:26:57, total=0:21:10, wall=10:08 IST=> training_quantize   43.99% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.215 Prec@1=69.706 Prec@5=88.753 rate=0.87 Hz, eta=0:26:57, total=0:21:10, wall=10:08 IST=> training_quantize   47.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.215 Prec@1=69.706 Prec@5=88.753 rate=0.87 Hz, eta=0:25:02, total=0:23:06, wall=10:08 IST=> training_quantize   47.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.233 Loss=1.215 Prec@1=69.706 Prec@5=88.753 rate=0.87 Hz, eta=0:25:02, total=0:23:06, wall=10:09 IST=> training_quantize   47.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.232 Loss=1.216 Prec@1=69.702 Prec@5=88.749 rate=0.87 Hz, eta=0:25:02, total=0:23:06, wall=10:09 IST=> training_quantize   51.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.232 Loss=1.216 Prec@1=69.702 Prec@5=88.749 rate=0.87 Hz, eta=0:23:07, total=0:25:01, wall=10:09 IST=> training_quantize   51.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.160 DataTime=0.232 Loss=1.216 Prec@1=69.702 Prec@5=88.749 rate=0.87 Hz, eta=0:23:07, total=0:25:01, wall=10:11 IST=> training_quantize   51.98% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.232 Loss=1.217 Prec@1=69.682 Prec@5=88.730 rate=0.87 Hz, eta=0:23:07, total=0:25:01, wall=10:11 IST=> training_quantize   55.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.232 Loss=1.217 Prec@1=69.682 Prec@5=88.730 rate=0.87 Hz, eta=0:21:11, total=0:26:57, wall=10:11 IST=> training_quantize   55.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.232 Loss=1.217 Prec@1=69.682 Prec@5=88.730 rate=0.87 Hz, eta=0:21:11, total=0:26:57, wall=10:13 IST=> training_quantize   55.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.686 Prec@5=88.747 rate=0.87 Hz, eta=0:21:11, total=0:26:57, wall=10:13 IST=> training_quantize   59.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.686 Prec@5=88.747 rate=0.87 Hz, eta=0:19:16, total=0:28:52, wall=10:13 IST=> training_quantize   59.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.686 Prec@5=88.747 rate=0.87 Hz, eta=0:19:16, total=0:28:52, wall=10:15 IST=> training_quantize   59.97% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.683 Prec@5=88.754 rate=0.87 Hz, eta=0:19:16, total=0:28:52, wall=10:15 IST=> training_quantize   63.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.683 Prec@5=88.754 rate=0.87 Hz, eta=0:17:21, total=0:30:48, wall=10:15 IST=> training_quantize   63.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.683 Prec@5=88.754 rate=0.87 Hz, eta=0:17:21, total=0:30:48, wall=10:17 IST=> training_quantize   63.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.698 Prec@5=88.752 rate=0.87 Hz, eta=0:17:21, total=0:30:48, wall=10:17 IST=> training_quantize   67.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.698 Prec@5=88.752 rate=0.87 Hz, eta=0:15:26, total=0:32:44, wall=10:17 IST=> training_quantize   67.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.159 DataTime=0.231 Loss=1.216 Prec@1=69.698 Prec@5=88.752 rate=0.87 Hz, eta=0:15:26, total=0:32:44, wall=10:19 IST=> training_quantize   67.96% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.231 Loss=1.216 Prec@1=69.701 Prec@5=88.748 rate=0.87 Hz, eta=0:15:26, total=0:32:44, wall=10:19 IST=> training_quantize   71.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.231 Loss=1.216 Prec@1=69.701 Prec@5=88.748 rate=0.87 Hz, eta=0:13:30, total=0:34:38, wall=10:19 IST=> training_quantize   71.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.231 Loss=1.216 Prec@1=69.701 Prec@5=88.748 rate=0.87 Hz, eta=0:13:30, total=0:34:38, wall=10:21 IST=> training_quantize   71.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.217 Prec@1=69.690 Prec@5=88.738 rate=0.87 Hz, eta=0:13:30, total=0:34:38, wall=10:21 IST=> training_quantize   75.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.217 Prec@1=69.690 Prec@5=88.738 rate=0.87 Hz, eta=0:11:34, total=0:36:34, wall=10:21 IST=> training_quantize   75.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.217 Prec@1=69.690 Prec@5=88.738 rate=0.87 Hz, eta=0:11:34, total=0:36:34, wall=10:23 IST=> training_quantize   75.95% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.695 Prec@5=88.741 rate=0.87 Hz, eta=0:11:34, total=0:36:34, wall=10:23 IST=> training_quantize   79.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.695 Prec@5=88.741 rate=0.87 Hz, eta=0:09:39, total=0:38:30, wall=10:23 IST=> training_quantize   79.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.695 Prec@5=88.741 rate=0.87 Hz, eta=0:09:39, total=0:38:30, wall=10:25 IST=> training_quantize   79.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.707 Prec@5=88.744 rate=0.87 Hz, eta=0:09:39, total=0:38:30, wall=10:25 IST=> training_quantize   83.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.707 Prec@5=88.744 rate=0.87 Hz, eta=0:07:44, total=0:40:25, wall=10:25 IST=> training_quantize   83.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.707 Prec@5=88.744 rate=0.87 Hz, eta=0:07:44, total=0:40:25, wall=10:27 IST=> training_quantize   83.94% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.713 Prec@5=88.745 rate=0.87 Hz, eta=0:07:44, total=0:40:25, wall=10:27 IST=> training_quantize   87.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.713 Prec@5=88.745 rate=0.87 Hz, eta=0:05:48, total=0:42:21, wall=10:27 IST=> training_quantize   87.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.713 Prec@5=88.745 rate=0.87 Hz, eta=0:05:48, total=0:42:21, wall=10:29 IST=> training_quantize   87.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.722 Prec@5=88.746 rate=0.87 Hz, eta=0:05:48, total=0:42:21, wall=10:29 IST=> training_quantize   91.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.722 Prec@5=88.746 rate=0.87 Hz, eta=0:03:53, total=0:44:16, wall=10:29 IST=> training_quantize   91.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.722 Prec@5=88.746 rate=0.87 Hz, eta=0:03:53, total=0:44:16, wall=10:31 IST=> training_quantize   91.93% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.723 Prec@5=88.743 rate=0.87 Hz, eta=0:03:53, total=0:44:16, wall=10:31 IST=> training_quantize   95.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.723 Prec@5=88.743 rate=0.87 Hz, eta=0:01:57, total=0:46:11, wall=10:31 IST=> training_quantize   95.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.158 DataTime=0.230 Loss=1.216 Prec@1=69.723 Prec@5=88.743 rate=0.87 Hz, eta=0:01:57, total=0:46:11, wall=10:33 IST=> training_quantize   95.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.724 Prec@5=88.748 rate=0.87 Hz, eta=0:01:57, total=0:46:11, wall=10:33 IST=> training_quantize   99.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.724 Prec@5=88.748 rate=0.87 Hz, eta=0:00:02, total=0:48:05, wall=10:33 IST=> training_quantize   99.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.724 Prec@5=88.748 rate=0.87 Hz, eta=0:00:02, total=0:48:05, wall=10:33 IST=> training_quantize   99.92% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.721 Prec@5=88.747 rate=0.87 Hz, eta=0:00:02, total=0:48:05, wall=10:33 IST=> training_quantize   100.00% of 1x2503...Epoch=4/10 LR=0.00001 Time=1.157 DataTime=0.230 Loss=1.216 Prec@1=69.721 Prec@5=88.747 rate=0.87 Hz, eta=0:00:00, total=0:48:07, wall=10:33 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 4.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 1.000 2.000 2.000 4.000 8.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:33 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:33 IST=> validation 0.00% of 1x98...Epoch=4/10 LR=0.00001 Time=8.421 Loss=1.251 Prec@1=71.484 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=10:33 IST=> validation 1.02% of 1x98...Epoch=4/10 LR=0.00001 Time=8.421 Loss=1.251 Prec@1=71.484 Prec@5=88.477 rate=3526.45 Hz, eta=0:00:00, total=0:00:00, wall=10:33 IST** validation 1.02% of 1x98...Epoch=4/10 LR=0.00001 Time=8.421 Loss=1.251 Prec@1=71.484 Prec@5=88.477 rate=3526.45 Hz, eta=0:00:00, total=0:00:00, wall=10:34 IST** validation 1.02% of 1x98...Epoch=4/10 LR=0.00001 Time=0.654 Loss=1.147 Prec@1=71.360 Prec@5=90.184 rate=3526.45 Hz, eta=0:00:00, total=0:00:00, wall=10:34 IST** validation 100.00% of 1x98...Epoch=4/10 LR=0.00001 Time=0.654 Loss=1.147 Prec@1=71.360 Prec@5=90.184 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=10:34 IST
[39mFreezing BN for subsequent epochs
[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> training_quantize   0.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=7.138 DataTime=5.475 Loss=1.145 Prec@1=71.484 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> training_quantize   0.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=7.138 DataTime=5.475 Loss=1.145 Prec@1=71.484 Prec@5=89.062 rate=7928.27 Hz, eta=0:00:00, total=0:00:00, wall=10:34 IST=> training_quantize   0.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=7.138 DataTime=5.475 Loss=1.145 Prec@1=71.484 Prec@5=89.062 rate=7928.27 Hz, eta=0:00:00, total=0:00:00, wall=10:36 IST=> training_quantize   0.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.214 DataTime=0.279 Loss=1.205 Prec@1=69.966 Prec@5=88.838 rate=7928.27 Hz, eta=0:00:00, total=0:00:00, wall=10:36 IST=> training_quantize   4.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.214 DataTime=0.279 Loss=1.205 Prec@1=69.966 Prec@5=88.838 rate=0.87 Hz, eta=0:45:47, total=0:01:55, wall=10:36 IST=> training_quantize   4.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.214 DataTime=0.279 Loss=1.205 Prec@1=69.966 Prec@5=88.838 rate=0.87 Hz, eta=0:45:47, total=0:01:55, wall=10:38 IST=> training_quantize   4.04% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.183 DataTime=0.255 Loss=1.210 Prec@1=70.022 Prec@5=88.803 rate=0.87 Hz, eta=0:45:47, total=0:01:55, wall=10:38 IST=> training_quantize   8.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.183 DataTime=0.255 Loss=1.210 Prec@1=70.022 Prec@5=88.803 rate=0.87 Hz, eta=0:44:02, total=0:03:50, wall=10:38 IST=> training_quantize   8.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.183 DataTime=0.255 Loss=1.210 Prec@1=70.022 Prec@5=88.803 rate=0.87 Hz, eta=0:44:02, total=0:03:50, wall=10:40 IST=> training_quantize   8.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.170 DataTime=0.246 Loss=1.206 Prec@1=70.067 Prec@5=88.864 rate=0.87 Hz, eta=0:44:02, total=0:03:50, wall=10:40 IST=> training_quantize   12.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.170 DataTime=0.246 Loss=1.206 Prec@1=70.067 Prec@5=88.864 rate=0.87 Hz, eta=0:42:03, total=0:05:44, wall=10:40 IST=> training_quantize   12.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.170 DataTime=0.246 Loss=1.206 Prec@1=70.067 Prec@5=88.864 rate=0.87 Hz, eta=0:42:03, total=0:05:44, wall=10:42 IST=> training_quantize   12.03% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.164 DataTime=0.241 Loss=1.209 Prec@1=69.975 Prec@5=88.834 rate=0.87 Hz, eta=0:42:03, total=0:05:44, wall=10:42 IST=> training_quantize   16.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.164 DataTime=0.241 Loss=1.209 Prec@1=69.975 Prec@5=88.834 rate=0.87 Hz, eta=0:40:08, total=0:07:39, wall=10:42 IST=> training_quantize   16.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.164 DataTime=0.241 Loss=1.209 Prec@1=69.975 Prec@5=88.834 rate=0.87 Hz, eta=0:40:08, total=0:07:39, wall=10:43 IST=> training_quantize   16.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.160 DataTime=0.239 Loss=1.205 Prec@1=70.018 Prec@5=88.901 rate=0.87 Hz, eta=0:40:08, total=0:07:39, wall=10:43 IST=> training_quantize   20.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.160 DataTime=0.239 Loss=1.205 Prec@1=70.018 Prec@5=88.901 rate=0.87 Hz, eta=0:38:13, total=0:09:34, wall=10:43 IST=> training_quantize   20.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.160 DataTime=0.239 Loss=1.205 Prec@1=70.018 Prec@5=88.901 rate=0.87 Hz, eta=0:38:13, total=0:09:34, wall=10:45 IST=> training_quantize   20.02% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.156 DataTime=0.237 Loss=1.204 Prec@1=70.003 Prec@5=88.923 rate=0.87 Hz, eta=0:38:13, total=0:09:34, wall=10:45 IST=> training_quantize   24.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.156 DataTime=0.237 Loss=1.204 Prec@1=70.003 Prec@5=88.923 rate=0.87 Hz, eta=0:36:15, total=0:11:27, wall=10:45 IST=> training_quantize   24.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.156 DataTime=0.237 Loss=1.204 Prec@1=70.003 Prec@5=88.923 rate=0.87 Hz, eta=0:36:15, total=0:11:27, wall=10:47 IST=> training_quantize   24.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.152 DataTime=0.236 Loss=1.203 Prec@1=70.002 Prec@5=88.958 rate=0.87 Hz, eta=0:36:15, total=0:11:27, wall=10:47 IST=> training_quantize   28.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.152 DataTime=0.236 Loss=1.203 Prec@1=70.002 Prec@5=88.958 rate=0.88 Hz, eta=0:34:17, total=0:13:20, wall=10:47 IST=> training_quantize   28.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.152 DataTime=0.236 Loss=1.203 Prec@1=70.002 Prec@5=88.958 rate=0.88 Hz, eta=0:34:17, total=0:13:20, wall=10:49 IST=> training_quantize   28.01% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.150 DataTime=0.235 Loss=1.201 Prec@1=70.019 Prec@5=88.969 rate=0.88 Hz, eta=0:34:17, total=0:13:20, wall=10:49 IST=> training_quantize   32.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.150 DataTime=0.235 Loss=1.201 Prec@1=70.019 Prec@5=88.969 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=10:49 IST=> training_quantize   32.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.150 DataTime=0.235 Loss=1.201 Prec@1=70.019 Prec@5=88.969 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=10:51 IST=> training_quantize   32.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.148 DataTime=0.235 Loss=1.202 Prec@1=70.023 Prec@5=88.971 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=10:51 IST=> training_quantize   36.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.148 DataTime=0.235 Loss=1.202 Prec@1=70.023 Prec@5=88.971 rate=0.88 Hz, eta=0:30:26, total=0:17:07, wall=10:51 IST=> training_quantize   36.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.148 DataTime=0.235 Loss=1.202 Prec@1=70.023 Prec@5=88.971 rate=0.88 Hz, eta=0:30:26, total=0:17:07, wall=10:53 IST=> training_quantize   36.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.201 Prec@1=70.053 Prec@5=88.977 rate=0.88 Hz, eta=0:30:26, total=0:17:07, wall=10:53 IST=> training_quantize   39.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.201 Prec@1=70.053 Prec@5=88.977 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=10:53 IST=> training_quantize   39.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.201 Prec@1=70.053 Prec@5=88.977 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=10:55 IST=> training_quantize   39.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.201 Prec@1=70.060 Prec@5=88.957 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=10:55 IST=> training_quantize   43.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.201 Prec@1=70.060 Prec@5=88.957 rate=0.88 Hz, eta=0:26:35, total=0:20:53, wall=10:55 IST=> training_quantize   43.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.201 Prec@1=70.060 Prec@5=88.957 rate=0.88 Hz, eta=0:26:35, total=0:20:53, wall=10:57 IST=> training_quantize   43.99% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.201 Prec@1=70.060 Prec@5=88.958 rate=0.88 Hz, eta=0:26:35, total=0:20:53, wall=10:57 IST=> training_quantize   47.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.201 Prec@1=70.060 Prec@5=88.958 rate=0.88 Hz, eta=0:24:41, total=0:22:46, wall=10:57 IST=> training_quantize   47.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.201 Prec@1=70.060 Prec@5=88.958 rate=0.88 Hz, eta=0:24:41, total=0:22:46, wall=10:59 IST=> training_quantize   47.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.200 Prec@1=70.071 Prec@5=88.958 rate=0.88 Hz, eta=0:24:41, total=0:22:46, wall=10:59 IST=> training_quantize   51.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.200 Prec@1=70.071 Prec@5=88.958 rate=0.88 Hz, eta=0:22:47, total=0:24:40, wall=10:59 IST=> training_quantize   51.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.200 Prec@1=70.071 Prec@5=88.958 rate=0.88 Hz, eta=0:22:47, total=0:24:40, wall=11:00 IST=> training_quantize   51.98% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.200 Prec@1=70.090 Prec@5=88.969 rate=0.88 Hz, eta=0:22:47, total=0:24:40, wall=11:00 IST=> training_quantize   55.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.200 Prec@1=70.090 Prec@5=88.969 rate=0.88 Hz, eta=0:20:53, total=0:26:34, wall=11:00 IST=> training_quantize   55.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.200 Prec@1=70.090 Prec@5=88.969 rate=0.88 Hz, eta=0:20:53, total=0:26:34, wall=11:02 IST=> training_quantize   55.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.198 Prec@1=70.118 Prec@5=88.987 rate=0.88 Hz, eta=0:20:53, total=0:26:34, wall=11:02 IST=> training_quantize   59.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.198 Prec@1=70.118 Prec@5=88.987 rate=0.88 Hz, eta=0:18:59, total=0:28:27, wall=11:02 IST=> training_quantize   59.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.198 Prec@1=70.118 Prec@5=88.987 rate=0.88 Hz, eta=0:18:59, total=0:28:27, wall=11:04 IST=> training_quantize   59.97% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.198 Prec@1=70.126 Prec@5=88.992 rate=0.88 Hz, eta=0:18:59, total=0:28:27, wall=11:04 IST=> training_quantize   63.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.198 Prec@1=70.126 Prec@5=88.992 rate=0.88 Hz, eta=0:17:05, total=0:30:20, wall=11:04 IST=> training_quantize   63.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.198 Prec@1=70.126 Prec@5=88.992 rate=0.88 Hz, eta=0:17:05, total=0:30:20, wall=11:06 IST=> training_quantize   63.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.131 Prec@5=89.009 rate=0.88 Hz, eta=0:17:05, total=0:30:20, wall=11:06 IST=> training_quantize   67.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.131 Prec@5=89.009 rate=0.88 Hz, eta=0:15:12, total=0:32:14, wall=11:06 IST=> training_quantize   67.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.131 Prec@5=89.009 rate=0.88 Hz, eta=0:15:12, total=0:32:14, wall=11:08 IST=> training_quantize   67.96% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.136 Prec@5=89.011 rate=0.88 Hz, eta=0:15:12, total=0:32:14, wall=11:08 IST=> training_quantize   71.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.136 Prec@5=89.011 rate=0.88 Hz, eta=0:13:18, total=0:34:07, wall=11:08 IST=> training_quantize   71.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.197 Prec@1=70.136 Prec@5=89.011 rate=0.88 Hz, eta=0:13:18, total=0:34:07, wall=11:10 IST=> training_quantize   71.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.014 rate=0.88 Hz, eta=0:13:18, total=0:34:07, wall=11:10 IST=> training_quantize   75.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.014 rate=0.88 Hz, eta=0:11:24, total=0:36:00, wall=11:10 IST=> training_quantize   75.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.014 rate=0.88 Hz, eta=0:11:24, total=0:36:00, wall=11:12 IST=> training_quantize   75.95% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.020 rate=0.88 Hz, eta=0:11:24, total=0:36:00, wall=11:12 IST=> training_quantize   79.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.020 rate=0.88 Hz, eta=0:09:30, total=0:37:54, wall=11:12 IST=> training_quantize   79.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.196 Prec@1=70.138 Prec@5=89.020 rate=0.88 Hz, eta=0:09:30, total=0:37:54, wall=11:14 IST=> training_quantize   79.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.130 Prec@5=89.022 rate=0.88 Hz, eta=0:09:30, total=0:37:54, wall=11:14 IST=> training_quantize   83.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.130 Prec@5=89.022 rate=0.88 Hz, eta=0:07:36, total=0:39:46, wall=11:14 IST=> training_quantize   83.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.130 Prec@5=89.022 rate=0.88 Hz, eta=0:07:36, total=0:39:46, wall=11:16 IST=> training_quantize   83.94% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.132 Prec@5=89.026 rate=0.88 Hz, eta=0:07:36, total=0:39:46, wall=11:16 IST=> training_quantize   87.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.132 Prec@5=89.026 rate=0.88 Hz, eta=0:05:43, total=0:41:40, wall=11:16 IST=> training_quantize   87.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.132 Prec@5=89.026 rate=0.88 Hz, eta=0:05:43, total=0:41:40, wall=11:17 IST=> training_quantize   87.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.136 Prec@5=89.027 rate=0.88 Hz, eta=0:05:43, total=0:41:40, wall=11:17 IST=> training_quantize   91.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.136 Prec@5=89.027 rate=0.88 Hz, eta=0:03:49, total=0:43:33, wall=11:17 IST=> training_quantize   91.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.136 Prec@5=89.027 rate=0.88 Hz, eta=0:03:49, total=0:43:33, wall=11:19 IST=> training_quantize   91.93% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.024 rate=0.88 Hz, eta=0:03:49, total=0:43:33, wall=11:19 IST=> training_quantize   95.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.024 rate=0.88 Hz, eta=0:01:55, total=0:45:27, wall=11:19 IST=> training_quantize   95.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.024 rate=0.88 Hz, eta=0:01:55, total=0:45:27, wall=11:21 IST=> training_quantize   95.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.021 rate=0.88 Hz, eta=0:01:55, total=0:45:27, wall=11:21 IST=> training_quantize   99.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.021 rate=0.88 Hz, eta=0:00:02, total=0:47:20, wall=11:21 IST=> training_quantize   99.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.134 Prec@5=89.021 rate=0.88 Hz, eta=0:00:02, total=0:47:20, wall=11:21 IST=> training_quantize   99.92% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.133 Prec@5=89.021 rate=0.88 Hz, eta=0:00:02, total=0:47:20, wall=11:21 IST=> training_quantize   100.00% of 1x2503...Epoch=5/10 LR=0.00001 Time=1.139 DataTime=0.231 Loss=1.196 Prec@1=70.133 Prec@5=89.021 rate=0.88 Hz, eta=0:00:00, total=0:47:22, wall=11:21 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 4.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 4.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> validation 0.00% of 1x98...Epoch=5/10 LR=0.00001 Time=7.260 Loss=1.176 Prec@1=72.070 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> validation 1.02% of 1x98...Epoch=5/10 LR=0.00001 Time=7.260 Loss=1.176 Prec@1=72.070 Prec@5=89.844 rate=4395.87 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST** validation 1.02% of 1x98...Epoch=5/10 LR=0.00001 Time=7.260 Loss=1.176 Prec@1=72.070 Prec@5=89.844 rate=4395.87 Hz, eta=0:00:00, total=0:00:00, wall=11:22 IST** validation 1.02% of 1x98...Epoch=5/10 LR=0.00001 Time=0.640 Loss=1.146 Prec@1=71.520 Prec@5=90.192 rate=4395.87 Hz, eta=0:00:00, total=0:00:00, wall=11:22 IST** validation 100.00% of 1x98...Epoch=5/10 LR=0.00001 Time=0.640 Loss=1.146 Prec@1=71.520 Prec@5=90.192 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=11:22 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST=> training_quantize   0.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=7.490 DataTime=6.192 Loss=1.257 Prec@1=67.969 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=11:23 IST=> training_quantize   0.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=7.490 DataTime=6.192 Loss=1.257 Prec@1=67.969 Prec@5=86.914 rate=5831.07 Hz, eta=0:00:00, total=0:00:00, wall=11:23 IST=> training_quantize   0.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=7.490 DataTime=6.192 Loss=1.257 Prec@1=67.969 Prec@5=86.914 rate=5831.07 Hz, eta=0:00:00, total=0:00:00, wall=11:24 IST=> training_quantize   0.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.210 DataTime=0.288 Loss=1.203 Prec@1=69.895 Prec@5=89.006 rate=5831.07 Hz, eta=0:00:00, total=0:00:00, wall=11:24 IST=> training_quantize   4.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.210 DataTime=0.288 Loss=1.203 Prec@1=69.895 Prec@5=89.006 rate=0.88 Hz, eta=0:45:29, total=0:01:54, wall=11:24 IST=> training_quantize   4.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.210 DataTime=0.288 Loss=1.203 Prec@1=69.895 Prec@5=89.006 rate=0.88 Hz, eta=0:45:29, total=0:01:54, wall=11:26 IST=> training_quantize   4.04% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.183 DataTime=0.259 Loss=1.196 Prec@1=70.056 Prec@5=89.043 rate=0.88 Hz, eta=0:45:29, total=0:01:54, wall=11:26 IST=> training_quantize   8.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.183 DataTime=0.259 Loss=1.196 Prec@1=70.056 Prec@5=89.043 rate=0.87 Hz, eta=0:43:58, total=0:03:50, wall=11:26 IST=> training_quantize   8.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.183 DataTime=0.259 Loss=1.196 Prec@1=70.056 Prec@5=89.043 rate=0.87 Hz, eta=0:43:58, total=0:03:50, wall=11:28 IST=> training_quantize   8.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.169 DataTime=0.250 Loss=1.195 Prec@1=70.101 Prec@5=89.070 rate=0.87 Hz, eta=0:43:58, total=0:03:50, wall=11:28 IST=> training_quantize   12.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.169 DataTime=0.250 Loss=1.195 Prec@1=70.101 Prec@5=89.070 rate=0.87 Hz, eta=0:41:59, total=0:05:44, wall=11:28 IST=> training_quantize   12.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.169 DataTime=0.250 Loss=1.195 Prec@1=70.101 Prec@5=89.070 rate=0.87 Hz, eta=0:41:59, total=0:05:44, wall=11:30 IST=> training_quantize   12.03% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.161 DataTime=0.245 Loss=1.196 Prec@1=70.176 Prec@5=89.041 rate=0.87 Hz, eta=0:41:59, total=0:05:44, wall=11:30 IST=> training_quantize   16.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.161 DataTime=0.245 Loss=1.196 Prec@1=70.176 Prec@5=89.041 rate=0.88 Hz, eta=0:40:02, total=0:07:38, wall=11:30 IST=> training_quantize   16.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.161 DataTime=0.245 Loss=1.196 Prec@1=70.176 Prec@5=89.041 rate=0.88 Hz, eta=0:40:02, total=0:07:38, wall=11:32 IST=> training_quantize   16.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.156 DataTime=0.242 Loss=1.192 Prec@1=70.257 Prec@5=89.079 rate=0.88 Hz, eta=0:40:02, total=0:07:38, wall=11:32 IST=> training_quantize   20.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.156 DataTime=0.242 Loss=1.192 Prec@1=70.257 Prec@5=89.079 rate=0.88 Hz, eta=0:38:05, total=0:09:31, wall=11:32 IST=> training_quantize   20.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.156 DataTime=0.242 Loss=1.192 Prec@1=70.257 Prec@5=89.079 rate=0.88 Hz, eta=0:38:05, total=0:09:31, wall=11:34 IST=> training_quantize   20.02% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.155 DataTime=0.239 Loss=1.195 Prec@1=70.229 Prec@5=89.044 rate=0.88 Hz, eta=0:38:05, total=0:09:31, wall=11:34 IST=> training_quantize   24.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.155 DataTime=0.239 Loss=1.195 Prec@1=70.229 Prec@5=89.044 rate=0.88 Hz, eta=0:36:12, total=0:11:26, wall=11:34 IST=> training_quantize   24.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.155 DataTime=0.239 Loss=1.195 Prec@1=70.229 Prec@5=89.044 rate=0.88 Hz, eta=0:36:12, total=0:11:26, wall=11:36 IST=> training_quantize   24.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.151 DataTime=0.238 Loss=1.195 Prec@1=70.214 Prec@5=89.034 rate=0.88 Hz, eta=0:36:12, total=0:11:26, wall=11:36 IST=> training_quantize   28.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.151 DataTime=0.238 Loss=1.195 Prec@1=70.214 Prec@5=89.034 rate=0.88 Hz, eta=0:34:15, total=0:13:19, wall=11:36 IST=> training_quantize   28.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.151 DataTime=0.238 Loss=1.195 Prec@1=70.214 Prec@5=89.034 rate=0.88 Hz, eta=0:34:15, total=0:13:19, wall=11:38 IST=> training_quantize   28.01% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.150 DataTime=0.237 Loss=1.194 Prec@1=70.240 Prec@5=89.046 rate=0.88 Hz, eta=0:34:15, total=0:13:19, wall=11:38 IST=> training_quantize   32.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.150 DataTime=0.237 Loss=1.194 Prec@1=70.240 Prec@5=89.046 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=11:38 IST=> training_quantize   32.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.150 DataTime=0.237 Loss=1.194 Prec@1=70.240 Prec@5=89.046 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=11:40 IST=> training_quantize   32.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.148 DataTime=0.236 Loss=1.196 Prec@1=70.223 Prec@5=89.010 rate=0.88 Hz, eta=0:32:21, total=0:15:13, wall=11:40 IST=> training_quantize   36.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.148 DataTime=0.236 Loss=1.196 Prec@1=70.223 Prec@5=89.010 rate=0.88 Hz, eta=0:30:25, total=0:17:06, wall=11:40 IST=> training_quantize   36.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.148 DataTime=0.236 Loss=1.196 Prec@1=70.223 Prec@5=89.010 rate=0.88 Hz, eta=0:30:25, total=0:17:06, wall=11:42 IST=> training_quantize   36.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.147 DataTime=0.235 Loss=1.195 Prec@1=70.233 Prec@5=89.003 rate=0.88 Hz, eta=0:30:25, total=0:17:06, wall=11:42 IST=> training_quantize   39.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.147 DataTime=0.235 Loss=1.195 Prec@1=70.233 Prec@5=89.003 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=11:42 IST=> training_quantize   39.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.147 DataTime=0.235 Loss=1.195 Prec@1=70.233 Prec@5=89.003 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=11:43 IST=> training_quantize   39.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.196 Prec@1=70.190 Prec@5=88.992 rate=0.88 Hz, eta=0:28:31, total=0:19:00, wall=11:43 IST=> training_quantize   43.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.196 Prec@1=70.190 Prec@5=88.992 rate=0.88 Hz, eta=0:26:36, total=0:20:53, wall=11:43 IST=> training_quantize   43.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.146 DataTime=0.234 Loss=1.196 Prec@1=70.190 Prec@5=88.992 rate=0.88 Hz, eta=0:26:36, total=0:20:53, wall=11:45 IST=> training_quantize   43.99% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.197 Prec@1=70.165 Prec@5=88.995 rate=0.88 Hz, eta=0:26:36, total=0:20:53, wall=11:45 IST=> training_quantize   47.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.197 Prec@1=70.165 Prec@5=88.995 rate=0.88 Hz, eta=0:24:42, total=0:22:47, wall=11:45 IST=> training_quantize   47.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.197 Prec@1=70.165 Prec@5=88.995 rate=0.88 Hz, eta=0:24:42, total=0:22:47, wall=11:47 IST=> training_quantize   47.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:24:42, total=0:22:47, wall=11:47 IST=> training_quantize   51.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:22:49, total=0:24:42, wall=11:47 IST=> training_quantize   51.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.145 DataTime=0.234 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:22:49, total=0:24:42, wall=11:49 IST=> training_quantize   51.98% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.172 Prec@5=89.016 rate=0.88 Hz, eta=0:22:49, total=0:24:42, wall=11:49 IST=> training_quantize   55.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.172 Prec@5=89.016 rate=0.88 Hz, eta=0:20:54, total=0:26:35, wall=11:49 IST=> training_quantize   55.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.172 Prec@5=89.016 rate=0.88 Hz, eta=0:20:54, total=0:26:35, wall=11:51 IST=> training_quantize   55.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.196 Prec@1=70.165 Prec@5=89.010 rate=0.88 Hz, eta=0:20:54, total=0:26:35, wall=11:51 IST=> training_quantize   59.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.196 Prec@1=70.165 Prec@5=89.010 rate=0.88 Hz, eta=0:19:01, total=0:28:29, wall=11:51 IST=> training_quantize   59.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.196 Prec@1=70.165 Prec@5=89.010 rate=0.88 Hz, eta=0:19:01, total=0:28:29, wall=11:53 IST=> training_quantize   59.97% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.175 Prec@5=89.010 rate=0.88 Hz, eta=0:19:01, total=0:28:29, wall=11:53 IST=> training_quantize   63.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.175 Prec@5=89.010 rate=0.88 Hz, eta=0:17:07, total=0:30:23, wall=11:53 IST=> training_quantize   63.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.144 DataTime=0.233 Loss=1.195 Prec@1=70.175 Prec@5=89.010 rate=0.88 Hz, eta=0:17:07, total=0:30:23, wall=11:55 IST=> training_quantize   63.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.196 Prec@1=70.166 Prec@5=89.011 rate=0.88 Hz, eta=0:17:07, total=0:30:23, wall=11:55 IST=> training_quantize   67.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.196 Prec@1=70.166 Prec@5=89.011 rate=0.88 Hz, eta=0:15:13, total=0:32:17, wall=11:55 IST=> training_quantize   67.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.233 Loss=1.196 Prec@1=70.166 Prec@5=89.011 rate=0.88 Hz, eta=0:15:13, total=0:32:17, wall=11:57 IST=> training_quantize   67.96% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.232 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:15:13, total=0:32:17, wall=11:57 IST=> training_quantize   71.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.232 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:13:19, total=0:34:10, wall=11:57 IST=> training_quantize   71.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.143 DataTime=0.232 Loss=1.195 Prec@1=70.187 Prec@5=89.010 rate=0.88 Hz, eta=0:13:19, total=0:34:10, wall=11:59 IST=> training_quantize   71.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.017 rate=0.88 Hz, eta=0:13:19, total=0:34:10, wall=11:59 IST=> training_quantize   75.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.017 rate=0.88 Hz, eta=0:11:25, total=0:36:04, wall=11:59 IST=> training_quantize   75.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.017 rate=0.88 Hz, eta=0:11:25, total=0:36:04, wall=12:00 IST=> training_quantize   75.95% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.191 Prec@5=89.014 rate=0.88 Hz, eta=0:11:25, total=0:36:04, wall=12:00 IST=> training_quantize   79.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.191 Prec@5=89.014 rate=0.88 Hz, eta=0:09:31, total=0:37:58, wall=12:00 IST=> training_quantize   79.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.191 Prec@5=89.014 rate=0.88 Hz, eta=0:09:31, total=0:37:58, wall=12:02 IST=> training_quantize   79.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.201 Prec@5=89.012 rate=0.88 Hz, eta=0:09:31, total=0:37:58, wall=12:02 IST=> training_quantize   83.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.201 Prec@5=89.012 rate=0.88 Hz, eta=0:07:37, total=0:39:51, wall=12:02 IST=> training_quantize   83.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.195 Prec@1=70.201 Prec@5=89.012 rate=0.88 Hz, eta=0:07:37, total=0:39:51, wall=12:04 IST=> training_quantize   83.94% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.021 rate=0.88 Hz, eta=0:07:37, total=0:39:51, wall=12:04 IST=> training_quantize   87.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.021 rate=0.88 Hz, eta=0:05:43, total=0:41:45, wall=12:04 IST=> training_quantize   87.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.142 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.021 rate=0.88 Hz, eta=0:05:43, total=0:41:45, wall=12:06 IST=> training_quantize   87.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.025 rate=0.88 Hz, eta=0:05:43, total=0:41:45, wall=12:06 IST=> training_quantize   91.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.025 rate=0.88 Hz, eta=0:03:49, total=0:43:38, wall=12:06 IST=> training_quantize   91.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.201 Prec@5=89.025 rate=0.88 Hz, eta=0:03:49, total=0:43:38, wall=12:08 IST=> training_quantize   91.93% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.027 rate=0.88 Hz, eta=0:03:49, total=0:43:38, wall=12:08 IST=> training_quantize   95.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.027 rate=0.88 Hz, eta=0:01:56, total=0:45:32, wall=12:08 IST=> training_quantize   95.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.141 DataTime=0.232 Loss=1.194 Prec@1=70.202 Prec@5=89.027 rate=0.88 Hz, eta=0:01:56, total=0:45:32, wall=12:10 IST=> training_quantize   95.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.194 Prec@1=70.208 Prec@5=89.040 rate=0.88 Hz, eta=0:01:56, total=0:45:32, wall=12:10 IST=> training_quantize   99.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.194 Prec@1=70.208 Prec@5=89.040 rate=0.88 Hz, eta=0:00:02, total=0:47:24, wall=12:10 IST=> training_quantize   99.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.194 Prec@1=70.208 Prec@5=89.040 rate=0.88 Hz, eta=0:00:02, total=0:47:24, wall=12:10 IST=> training_quantize   99.92% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.194 Prec@1=70.208 Prec@5=89.040 rate=0.88 Hz, eta=0:00:02, total=0:47:24, wall=12:10 IST=> training_quantize   100.00% of 1x2503...Epoch=6/10 LR=0.00001 Time=1.140 DataTime=0.232 Loss=1.194 Prec@1=70.208 Prec@5=89.040 rate=0.88 Hz, eta=0:00:00, total=0:47:26, wall=12:10 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 8.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 8.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:10 IST=> validation 0.00% of 1x98...Epoch=6/10 LR=0.00001 Time=8.035 Loss=1.220 Prec@1=68.164 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=12:10 IST=> validation 1.02% of 1x98...Epoch=6/10 LR=0.00001 Time=8.035 Loss=1.220 Prec@1=68.164 Prec@5=90.039 rate=5366.80 Hz, eta=0:00:00, total=0:00:00, wall=12:10 IST** validation 1.02% of 1x98...Epoch=6/10 LR=0.00001 Time=8.035 Loss=1.220 Prec@1=68.164 Prec@5=90.039 rate=5366.80 Hz, eta=0:00:00, total=0:00:00, wall=12:11 IST** validation 1.02% of 1x98...Epoch=6/10 LR=0.00001 Time=0.648 Loss=1.147 Prec@1=71.504 Prec@5=90.222 rate=5366.80 Hz, eta=0:00:00, total=0:00:00, wall=12:11 IST** validation 100.00% of 1x98...Epoch=6/10 LR=0.00001 Time=0.648 Loss=1.147 Prec@1=71.504 Prec@5=90.222 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=12:11 IST
[39mFreezing ranges for subsequent epochs
[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:11 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:11 IST=> training_quantize   0.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=7.484 DataTime=6.328 Loss=1.063 Prec@1=71.875 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=12:11 IST=> training_quantize   0.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=7.484 DataTime=6.328 Loss=1.063 Prec@1=71.875 Prec@5=90.820 rate=7634.52 Hz, eta=0:00:00, total=0:00:00, wall=12:11 IST=> training_quantize   0.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=7.484 DataTime=6.328 Loss=1.063 Prec@1=71.875 Prec@5=90.820 rate=7634.52 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST=> training_quantize   0.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.152 DataTime=0.288 Loss=1.192 Prec@1=70.295 Prec@5=89.018 rate=7634.52 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST=> training_quantize   4.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.152 DataTime=0.288 Loss=1.192 Prec@1=70.295 Prec@5=89.018 rate=0.93 Hz, eta=0:43:09, total=0:01:48, wall=12:13 IST=> training_quantize   4.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.152 DataTime=0.288 Loss=1.192 Prec@1=70.295 Prec@5=89.018 rate=0.93 Hz, eta=0:43:09, total=0:01:48, wall=12:15 IST=> training_quantize   4.04% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.119 DataTime=0.259 Loss=1.194 Prec@1=70.100 Prec@5=89.019 rate=0.93 Hz, eta=0:43:09, total=0:01:48, wall=12:15 IST=> training_quantize   8.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.119 DataTime=0.259 Loss=1.194 Prec@1=70.100 Prec@5=89.019 rate=0.92 Hz, eta=0:41:30, total=0:03:37, wall=12:15 IST=> training_quantize   8.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.119 DataTime=0.259 Loss=1.194 Prec@1=70.100 Prec@5=89.019 rate=0.92 Hz, eta=0:41:30, total=0:03:37, wall=12:17 IST=> training_quantize   8.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.107 DataTime=0.250 Loss=1.188 Prec@1=70.276 Prec@5=89.077 rate=0.92 Hz, eta=0:41:30, total=0:03:37, wall=12:17 IST=> training_quantize   12.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.107 DataTime=0.250 Loss=1.188 Prec@1=70.276 Prec@5=89.077 rate=0.92 Hz, eta=0:39:43, total=0:05:25, wall=12:17 IST=> training_quantize   12.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.107 DataTime=0.250 Loss=1.188 Prec@1=70.276 Prec@5=89.077 rate=0.92 Hz, eta=0:39:43, total=0:05:25, wall=12:18 IST=> training_quantize   12.03% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.102 DataTime=0.245 Loss=1.191 Prec@1=70.260 Prec@5=89.033 rate=0.92 Hz, eta=0:39:43, total=0:05:25, wall=12:18 IST=> training_quantize   16.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.102 DataTime=0.245 Loss=1.191 Prec@1=70.260 Prec@5=89.033 rate=0.92 Hz, eta=0:37:56, total=0:07:14, wall=12:18 IST=> training_quantize   16.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.102 DataTime=0.245 Loss=1.191 Prec@1=70.260 Prec@5=89.033 rate=0.92 Hz, eta=0:37:56, total=0:07:14, wall=12:20 IST=> training_quantize   16.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.228 Prec@5=89.036 rate=0.92 Hz, eta=0:37:56, total=0:07:14, wall=12:20 IST=> training_quantize   20.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.228 Prec@5=89.036 rate=0.92 Hz, eta=0:36:07, total=0:09:02, wall=12:20 IST=> training_quantize   20.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.228 Prec@5=89.036 rate=0.92 Hz, eta=0:36:07, total=0:09:02, wall=12:22 IST=> training_quantize   20.02% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.096 DataTime=0.240 Loss=1.192 Prec@1=70.291 Prec@5=89.055 rate=0.92 Hz, eta=0:36:07, total=0:09:02, wall=12:22 IST=> training_quantize   24.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.096 DataTime=0.240 Loss=1.192 Prec@1=70.291 Prec@5=89.055 rate=0.92 Hz, eta=0:34:20, total=0:10:51, wall=12:22 IST=> training_quantize   24.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.096 DataTime=0.240 Loss=1.192 Prec@1=70.291 Prec@5=89.055 rate=0.92 Hz, eta=0:34:20, total=0:10:51, wall=12:24 IST=> training_quantize   24.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.094 DataTime=0.239 Loss=1.190 Prec@1=70.324 Prec@5=89.062 rate=0.92 Hz, eta=0:34:20, total=0:10:51, wall=12:24 IST=> training_quantize   28.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.094 DataTime=0.239 Loss=1.190 Prec@1=70.324 Prec@5=89.062 rate=0.92 Hz, eta=0:32:32, total=0:12:39, wall=12:24 IST=> training_quantize   28.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.094 DataTime=0.239 Loss=1.190 Prec@1=70.324 Prec@5=89.062 rate=0.92 Hz, eta=0:32:32, total=0:12:39, wall=12:26 IST=> training_quantize   28.01% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.093 DataTime=0.238 Loss=1.191 Prec@1=70.288 Prec@5=89.044 rate=0.92 Hz, eta=0:32:32, total=0:12:39, wall=12:26 IST=> training_quantize   32.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.093 DataTime=0.238 Loss=1.191 Prec@1=70.288 Prec@5=89.044 rate=0.92 Hz, eta=0:30:43, total=0:14:27, wall=12:26 IST=> training_quantize   32.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.093 DataTime=0.238 Loss=1.191 Prec@1=70.288 Prec@5=89.044 rate=0.92 Hz, eta=0:30:43, total=0:14:27, wall=12:27 IST=> training_quantize   32.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.274 Prec@5=89.051 rate=0.92 Hz, eta=0:30:43, total=0:14:27, wall=12:27 IST=> training_quantize   36.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.274 Prec@5=89.051 rate=0.92 Hz, eta=0:28:55, total=0:16:15, wall=12:27 IST=> training_quantize   36.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.274 Prec@5=89.051 rate=0.92 Hz, eta=0:28:55, total=0:16:15, wall=12:29 IST=> training_quantize   36.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.288 Prec@5=89.048 rate=0.92 Hz, eta=0:28:55, total=0:16:15, wall=12:29 IST=> training_quantize   39.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.288 Prec@5=89.048 rate=0.92 Hz, eta=0:27:06, total=0:18:04, wall=12:29 IST=> training_quantize   39.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.288 Prec@5=89.048 rate=0.92 Hz, eta=0:27:06, total=0:18:04, wall=12:31 IST=> training_quantize   39.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.191 Prec@1=70.282 Prec@5=89.053 rate=0.92 Hz, eta=0:27:06, total=0:18:04, wall=12:31 IST=> training_quantize   43.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.191 Prec@1=70.282 Prec@5=89.053 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=12:31 IST=> training_quantize   43.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.191 Prec@1=70.282 Prec@5=89.053 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=12:33 IST=> training_quantize   43.99% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.277 Prec@5=89.078 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=12:33 IST=> training_quantize   47.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.277 Prec@5=89.078 rate=0.92 Hz, eta=0:23:29, total=0:21:40, wall=12:33 IST=> training_quantize   47.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.277 Prec@5=89.078 rate=0.92 Hz, eta=0:23:29, total=0:21:40, wall=12:35 IST=> training_quantize   47.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.110 rate=0.92 Hz, eta=0:23:29, total=0:21:40, wall=12:35 IST=> training_quantize   51.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.110 rate=0.92 Hz, eta=0:21:41, total=0:23:28, wall=12:35 IST=> training_quantize   51.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.110 rate=0.92 Hz, eta=0:21:41, total=0:23:28, wall=12:36 IST=> training_quantize   51.98% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.107 rate=0.92 Hz, eta=0:21:41, total=0:23:28, wall=12:36 IST=> training_quantize   55.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.107 rate=0.92 Hz, eta=0:19:52, total=0:25:15, wall=12:36 IST=> training_quantize   55.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.235 Loss=1.190 Prec@1=70.282 Prec@5=89.107 rate=0.92 Hz, eta=0:19:52, total=0:25:15, wall=12:38 IST=> training_quantize   55.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.294 Prec@5=89.104 rate=0.92 Hz, eta=0:19:52, total=0:25:15, wall=12:38 IST=> training_quantize   59.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.294 Prec@5=89.104 rate=0.92 Hz, eta=0:18:04, total=0:27:04, wall=12:38 IST=> training_quantize   59.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.294 Prec@5=89.104 rate=0.92 Hz, eta=0:18:04, total=0:27:04, wall=12:40 IST=> training_quantize   59.97% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.191 Prec@1=70.294 Prec@5=89.090 rate=0.92 Hz, eta=0:18:04, total=0:27:04, wall=12:40 IST=> training_quantize   63.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.191 Prec@1=70.294 Prec@5=89.090 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=12:40 IST=> training_quantize   63.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.191 Prec@1=70.294 Prec@5=89.090 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=12:42 IST=> training_quantize   63.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.295 Prec@5=89.097 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=12:42 IST=> training_quantize   67.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.295 Prec@5=89.097 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=12:42 IST=> training_quantize   67.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.295 Prec@5=89.097 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=12:44 IST=> training_quantize   67.96% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.306 Prec@5=89.102 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=12:44 IST=> training_quantize   71.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.306 Prec@5=89.102 rate=0.92 Hz, eta=0:12:40, total=0:32:30, wall=12:44 IST=> training_quantize   71.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.306 Prec@5=89.102 rate=0.92 Hz, eta=0:12:40, total=0:32:30, wall=12:45 IST=> training_quantize   71.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.189 Prec@1=70.333 Prec@5=89.106 rate=0.92 Hz, eta=0:12:40, total=0:32:30, wall=12:45 IST=> training_quantize   75.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.189 Prec@1=70.333 Prec@5=89.106 rate=0.92 Hz, eta=0:10:51, total=0:34:18, wall=12:45 IST=> training_quantize   75.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.189 Prec@1=70.333 Prec@5=89.106 rate=0.92 Hz, eta=0:10:51, total=0:34:18, wall=12:47 IST=> training_quantize   75.95% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.322 Prec@5=89.102 rate=0.92 Hz, eta=0:10:51, total=0:34:18, wall=12:47 IST=> training_quantize   79.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.322 Prec@5=89.102 rate=0.92 Hz, eta=0:09:03, total=0:36:07, wall=12:47 IST=> training_quantize   79.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.190 Prec@1=70.322 Prec@5=89.102 rate=0.92 Hz, eta=0:09:03, total=0:36:07, wall=12:49 IST=> training_quantize   79.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.190 Prec@1=70.313 Prec@5=89.079 rate=0.92 Hz, eta=0:09:03, total=0:36:07, wall=12:49 IST=> training_quantize   83.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.190 Prec@1=70.313 Prec@5=89.079 rate=0.92 Hz, eta=0:07:15, total=0:37:56, wall=12:49 IST=> training_quantize   83.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.190 Prec@1=70.313 Prec@5=89.079 rate=0.92 Hz, eta=0:07:15, total=0:37:56, wall=12:51 IST=> training_quantize   83.94% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.319 Prec@5=89.078 rate=0.92 Hz, eta=0:07:15, total=0:37:56, wall=12:51 IST=> training_quantize   87.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.319 Prec@5=89.078 rate=0.92 Hz, eta=0:05:27, total=0:39:43, wall=12:51 IST=> training_quantize   87.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.319 Prec@5=89.078 rate=0.92 Hz, eta=0:05:27, total=0:39:43, wall=12:53 IST=> training_quantize   87.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.298 Prec@5=89.071 rate=0.92 Hz, eta=0:05:27, total=0:39:43, wall=12:53 IST=> training_quantize   91.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.298 Prec@5=89.071 rate=0.92 Hz, eta=0:03:38, total=0:41:32, wall=12:53 IST=> training_quantize   91.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.191 Prec@1=70.298 Prec@5=89.071 rate=0.92 Hz, eta=0:03:38, total=0:41:32, wall=12:55 IST=> training_quantize   91.93% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.278 Prec@5=89.062 rate=0.92 Hz, eta=0:03:38, total=0:41:32, wall=12:55 IST=> training_quantize   95.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.278 Prec@5=89.062 rate=0.92 Hz, eta=0:01:50, total=0:43:20, wall=12:55 IST=> training_quantize   95.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.278 Prec@5=89.062 rate=0.92 Hz, eta=0:01:50, total=0:43:20, wall=12:56 IST=> training_quantize   95.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.192 Prec@1=70.283 Prec@5=89.057 rate=0.92 Hz, eta=0:01:50, total=0:43:20, wall=12:56 IST=> training_quantize   99.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.192 Prec@1=70.283 Prec@5=89.057 rate=0.92 Hz, eta=0:00:02, total=0:45:10, wall=12:56 IST=> training_quantize   99.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.192 Prec@1=70.283 Prec@5=89.057 rate=0.92 Hz, eta=0:00:02, total=0:45:10, wall=12:56 IST=> training_quantize   99.92% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.192 Prec@1=70.282 Prec@5=89.056 rate=0.92 Hz, eta=0:00:02, total=0:45:10, wall=12:56 IST=> training_quantize   100.00% of 1x2503...Epoch=7/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.192 Prec@1=70.282 Prec@5=89.056 rate=0.92 Hz, eta=0:00:00, total=0:45:12, wall=12:56 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 8.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 8.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:57 IST=> validation 0.00% of 1x98...Epoch=7/10 LR=0.00000 Time=7.794 Loss=1.063 Prec@1=73.047 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=12:57 IST=> validation 1.02% of 1x98...Epoch=7/10 LR=0.00000 Time=7.794 Loss=1.063 Prec@1=73.047 Prec@5=92.188 rate=3337.16 Hz, eta=0:00:00, total=0:00:00, wall=12:57 IST** validation 1.02% of 1x98...Epoch=7/10 LR=0.00000 Time=7.794 Loss=1.063 Prec@1=73.047 Prec@5=92.188 rate=3337.16 Hz, eta=0:00:00, total=0:00:00, wall=12:57 IST** validation 1.02% of 1x98...Epoch=7/10 LR=0.00000 Time=0.646 Loss=1.151 Prec@1=71.462 Prec@5=90.136 rate=3337.16 Hz, eta=0:00:00, total=0:00:00, wall=12:57 IST** validation 100.00% of 1x98...Epoch=7/10 LR=0.00000 Time=0.646 Loss=1.151 Prec@1=71.462 Prec@5=90.136 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=12:57 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:58 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:58 IST=> training_quantize   0.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=6.980 DataTime=5.429 Loss=1.206 Prec@1=70.508 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=12:58 IST=> training_quantize   0.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=6.980 DataTime=5.429 Loss=1.206 Prec@1=70.508 Prec@5=87.695 rate=8443.03 Hz, eta=0:00:00, total=0:00:00, wall=12:58 IST=> training_quantize   0.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=6.980 DataTime=5.429 Loss=1.206 Prec@1=70.508 Prec@5=87.695 rate=8443.03 Hz, eta=0:00:00, total=0:00:00, wall=12:59 IST=> training_quantize   0.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.149 DataTime=0.281 Loss=1.203 Prec@1=69.926 Prec@5=88.935 rate=8443.03 Hz, eta=0:00:00, total=0:00:00, wall=12:59 IST=> training_quantize   4.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.149 DataTime=0.281 Loss=1.203 Prec@1=69.926 Prec@5=88.935 rate=0.93 Hz, eta=0:43:14, total=0:01:49, wall=12:59 IST=> training_quantize   4.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.149 DataTime=0.281 Loss=1.203 Prec@1=69.926 Prec@5=88.935 rate=0.93 Hz, eta=0:43:14, total=0:01:49, wall=13:01 IST=> training_quantize   4.04% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.115 DataTime=0.255 Loss=1.196 Prec@1=70.057 Prec@5=89.009 rate=0.93 Hz, eta=0:43:14, total=0:01:49, wall=13:01 IST=> training_quantize   8.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.115 DataTime=0.255 Loss=1.196 Prec@1=70.057 Prec@5=89.009 rate=0.93 Hz, eta=0:41:25, total=0:03:37, wall=13:01 IST=> training_quantize   8.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.115 DataTime=0.255 Loss=1.196 Prec@1=70.057 Prec@5=89.009 rate=0.93 Hz, eta=0:41:25, total=0:03:37, wall=13:03 IST=> training_quantize   8.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.194 Prec@1=70.148 Prec@5=89.047 rate=0.93 Hz, eta=0:41:25, total=0:03:37, wall=13:03 IST=> training_quantize   12.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.194 Prec@1=70.148 Prec@5=89.047 rate=0.92 Hz, eta=0:39:40, total=0:05:25, wall=13:03 IST=> training_quantize   12.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.194 Prec@1=70.148 Prec@5=89.047 rate=0.92 Hz, eta=0:39:40, total=0:05:25, wall=13:05 IST=> training_quantize   12.03% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.097 DataTime=0.243 Loss=1.197 Prec@1=70.094 Prec@5=89.012 rate=0.92 Hz, eta=0:39:40, total=0:05:25, wall=13:05 IST=> training_quantize   16.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.097 DataTime=0.243 Loss=1.197 Prec@1=70.094 Prec@5=89.012 rate=0.93 Hz, eta=0:37:49, total=0:07:12, wall=13:05 IST=> training_quantize   16.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.097 DataTime=0.243 Loss=1.197 Prec@1=70.094 Prec@5=89.012 rate=0.93 Hz, eta=0:37:49, total=0:07:12, wall=13:07 IST=> training_quantize   16.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.094 DataTime=0.240 Loss=1.199 Prec@1=70.055 Prec@5=88.962 rate=0.93 Hz, eta=0:37:49, total=0:07:12, wall=13:07 IST=> training_quantize   20.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.094 DataTime=0.240 Loss=1.199 Prec@1=70.055 Prec@5=88.962 rate=0.93 Hz, eta=0:36:02, total=0:09:01, wall=13:07 IST=> training_quantize   20.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.094 DataTime=0.240 Loss=1.199 Prec@1=70.055 Prec@5=88.962 rate=0.93 Hz, eta=0:36:02, total=0:09:01, wall=13:08 IST=> training_quantize   20.02% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.092 DataTime=0.239 Loss=1.197 Prec@1=70.134 Prec@5=88.958 rate=0.93 Hz, eta=0:36:02, total=0:09:01, wall=13:08 IST=> training_quantize   24.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.092 DataTime=0.239 Loss=1.197 Prec@1=70.134 Prec@5=88.958 rate=0.93 Hz, eta=0:34:15, total=0:10:49, wall=13:08 IST=> training_quantize   24.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.092 DataTime=0.239 Loss=1.197 Prec@1=70.134 Prec@5=88.958 rate=0.93 Hz, eta=0:34:15, total=0:10:49, wall=13:10 IST=> training_quantize   24.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.195 Prec@1=70.158 Prec@5=88.985 rate=0.93 Hz, eta=0:34:15, total=0:10:49, wall=13:10 IST=> training_quantize   28.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.195 Prec@1=70.158 Prec@5=88.985 rate=0.92 Hz, eta=0:32:28, total=0:12:37, wall=13:10 IST=> training_quantize   28.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.195 Prec@1=70.158 Prec@5=88.985 rate=0.92 Hz, eta=0:32:28, total=0:12:37, wall=13:12 IST=> training_quantize   28.01% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.125 Prec@5=88.977 rate=0.92 Hz, eta=0:32:28, total=0:12:37, wall=13:12 IST=> training_quantize   32.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.125 Prec@5=88.977 rate=0.93 Hz, eta=0:30:39, total=0:14:25, wall=13:12 IST=> training_quantize   32.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.125 Prec@5=88.977 rate=0.93 Hz, eta=0:30:39, total=0:14:25, wall=13:14 IST=> training_quantize   32.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.129 Prec@5=88.990 rate=0.93 Hz, eta=0:30:39, total=0:14:25, wall=13:14 IST=> training_quantize   36.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.129 Prec@5=88.990 rate=0.92 Hz, eta=0:28:52, total=0:16:14, wall=13:14 IST=> training_quantize   36.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.236 Loss=1.197 Prec@1=70.129 Prec@5=88.990 rate=0.92 Hz, eta=0:28:52, total=0:16:14, wall=13:16 IST=> training_quantize   36.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.139 Prec@5=88.995 rate=0.92 Hz, eta=0:28:52, total=0:16:14, wall=13:16 IST=> training_quantize   39.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.139 Prec@5=88.995 rate=0.92 Hz, eta=0:27:05, total=0:18:03, wall=13:16 IST=> training_quantize   39.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.139 Prec@5=88.995 rate=0.92 Hz, eta=0:27:05, total=0:18:03, wall=13:17 IST=> training_quantize   39.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.122 Prec@5=89.006 rate=0.92 Hz, eta=0:27:05, total=0:18:03, wall=13:17 IST=> training_quantize   43.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.122 Prec@5=89.006 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=13:17 IST=> training_quantize   43.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.197 Prec@1=70.122 Prec@5=89.006 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=13:19 IST=> training_quantize   43.99% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.195 Prec@1=70.178 Prec@5=89.040 rate=0.92 Hz, eta=0:25:17, total=0:19:51, wall=13:19 IST=> training_quantize   47.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.195 Prec@1=70.178 Prec@5=89.040 rate=0.92 Hz, eta=0:23:30, total=0:21:40, wall=13:19 IST=> training_quantize   47.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.195 Prec@1=70.178 Prec@5=89.040 rate=0.92 Hz, eta=0:23:30, total=0:21:40, wall=13:21 IST=> training_quantize   47.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.194 Prec@1=70.192 Prec@5=89.045 rate=0.92 Hz, eta=0:23:30, total=0:21:40, wall=13:21 IST=> training_quantize   51.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.194 Prec@1=70.192 Prec@5=89.045 rate=0.92 Hz, eta=0:21:41, total=0:23:29, wall=13:21 IST=> training_quantize   51.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.194 Prec@1=70.192 Prec@5=89.045 rate=0.92 Hz, eta=0:21:41, total=0:23:29, wall=13:23 IST=> training_quantize   51.98% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.193 Prec@1=70.215 Prec@5=89.046 rate=0.92 Hz, eta=0:21:41, total=0:23:29, wall=13:23 IST=> training_quantize   55.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.193 Prec@1=70.215 Prec@5=89.046 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=13:23 IST=> training_quantize   55.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.193 Prec@1=70.215 Prec@5=89.046 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=13:25 IST=> training_quantize   55.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.193 Prec@1=70.217 Prec@5=89.040 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=13:25 IST=> training_quantize   59.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.193 Prec@1=70.217 Prec@5=89.040 rate=0.92 Hz, eta=0:18:05, total=0:27:05, wall=13:25 IST=> training_quantize   59.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.193 Prec@1=70.217 Prec@5=89.040 rate=0.92 Hz, eta=0:18:05, total=0:27:05, wall=13:27 IST=> training_quantize   59.97% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.193 Prec@1=70.221 Prec@5=89.036 rate=0.92 Hz, eta=0:18:05, total=0:27:05, wall=13:27 IST=> training_quantize   63.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.193 Prec@1=70.221 Prec@5=89.036 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=13:27 IST=> training_quantize   63.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.234 Loss=1.193 Prec@1=70.221 Prec@5=89.036 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=13:28 IST=> training_quantize   63.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.193 Prec@1=70.212 Prec@5=89.045 rate=0.92 Hz, eta=0:16:16, total=0:28:53, wall=13:28 IST=> training_quantize   67.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.193 Prec@1=70.212 Prec@5=89.045 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=13:28 IST=> training_quantize   67.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.193 Prec@1=70.212 Prec@5=89.045 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=13:30 IST=> training_quantize   67.96% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.193 Prec@1=70.208 Prec@5=89.043 rate=0.92 Hz, eta=0:14:28, total=0:30:41, wall=13:30 IST=> training_quantize   71.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.193 Prec@1=70.208 Prec@5=89.043 rate=0.92 Hz, eta=0:12:39, total=0:32:29, wall=13:30 IST=> training_quantize   71.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.193 Prec@1=70.208 Prec@5=89.043 rate=0.92 Hz, eta=0:12:39, total=0:32:29, wall=13:32 IST=> training_quantize   71.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.215 Prec@5=89.038 rate=0.92 Hz, eta=0:12:39, total=0:32:29, wall=13:32 IST=> training_quantize   75.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.215 Prec@5=89.038 rate=0.92 Hz, eta=0:10:51, total=0:34:16, wall=13:32 IST=> training_quantize   75.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.215 Prec@5=89.038 rate=0.92 Hz, eta=0:10:51, total=0:34:16, wall=13:34 IST=> training_quantize   75.95% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.244 Prec@5=89.050 rate=0.92 Hz, eta=0:10:51, total=0:34:16, wall=13:34 IST=> training_quantize   79.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.244 Prec@5=89.050 rate=0.92 Hz, eta=0:09:02, total=0:36:04, wall=13:34 IST=> training_quantize   79.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.192 Prec@1=70.244 Prec@5=89.050 rate=0.92 Hz, eta=0:09:02, total=0:36:04, wall=13:36 IST=> training_quantize   79.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.251 Prec@5=89.056 rate=0.92 Hz, eta=0:09:02, total=0:36:04, wall=13:36 IST=> training_quantize   83.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.251 Prec@5=89.056 rate=0.92 Hz, eta=0:07:15, total=0:37:53, wall=13:36 IST=> training_quantize   83.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.251 Prec@5=89.056 rate=0.92 Hz, eta=0:07:15, total=0:37:53, wall=13:37 IST=> training_quantize   83.94% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.262 Prec@5=89.050 rate=0.92 Hz, eta=0:07:15, total=0:37:53, wall=13:37 IST=> training_quantize   87.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.262 Prec@5=89.050 rate=0.92 Hz, eta=0:05:26, total=0:39:41, wall=13:37 IST=> training_quantize   87.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.262 Prec@5=89.050 rate=0.92 Hz, eta=0:05:26, total=0:39:41, wall=13:39 IST=> training_quantize   87.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.268 Prec@5=89.052 rate=0.92 Hz, eta=0:05:26, total=0:39:41, wall=13:39 IST=> training_quantize   91.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.268 Prec@5=89.052 rate=0.92 Hz, eta=0:03:38, total=0:41:30, wall=13:39 IST=> training_quantize   91.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.268 Prec@5=89.052 rate=0.92 Hz, eta=0:03:38, total=0:41:30, wall=13:41 IST=> training_quantize   91.93% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.263 Prec@5=89.048 rate=0.92 Hz, eta=0:03:38, total=0:41:30, wall=13:41 IST=> training_quantize   95.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.263 Prec@5=89.048 rate=0.92 Hz, eta=0:01:50, total=0:43:18, wall=13:41 IST=> training_quantize   95.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.085 DataTime=0.233 Loss=1.191 Prec@1=70.263 Prec@5=89.048 rate=0.92 Hz, eta=0:01:50, total=0:43:18, wall=13:43 IST=> training_quantize   95.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.243 Prec@5=89.039 rate=0.92 Hz, eta=0:01:50, total=0:43:18, wall=13:43 IST=> training_quantize   99.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.243 Prec@5=89.039 rate=0.92 Hz, eta=0:00:02, total=0:45:09, wall=13:43 IST=> training_quantize   99.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.243 Prec@5=89.039 rate=0.92 Hz, eta=0:00:02, total=0:45:09, wall=13:43 IST=> training_quantize   99.92% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.245 Prec@5=89.039 rate=0.92 Hz, eta=0:00:02, total=0:45:09, wall=13:43 IST=> training_quantize   100.00% of 1x2503...Epoch=8/10 LR=0.00000 Time=1.086 DataTime=0.233 Loss=1.192 Prec@1=70.245 Prec@5=89.039 rate=0.92 Hz, eta=0:00:00, total=0:45:11, wall=13:43 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 8.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 8.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:43 IST=> validation 0.00% of 1x98...Epoch=8/10 LR=0.00000 Time=7.867 Loss=1.075 Prec@1=73.828 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=13:43 IST=> validation 1.02% of 1x98...Epoch=8/10 LR=0.00000 Time=7.867 Loss=1.075 Prec@1=73.828 Prec@5=90.820 rate=5579.11 Hz, eta=0:00:00, total=0:00:00, wall=13:43 IST** validation 1.02% of 1x98...Epoch=8/10 LR=0.00000 Time=7.867 Loss=1.075 Prec@1=73.828 Prec@5=90.820 rate=5579.11 Hz, eta=0:00:00, total=0:00:00, wall=13:44 IST** validation 1.02% of 1x98...Epoch=8/10 LR=0.00000 Time=0.647 Loss=1.143 Prec@1=71.532 Prec@5=90.266 rate=5579.11 Hz, eta=0:00:00, total=0:00:00, wall=13:44 IST** validation 100.00% of 1x98...Epoch=8/10 LR=0.00000 Time=0.647 Loss=1.143 Prec@1=71.532 Prec@5=90.266 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=13:44 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:44 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:44 IST=> training_quantize   0.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=6.914 DataTime=5.377 Loss=1.264 Prec@1=69.141 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=13:44 IST=> training_quantize   0.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=6.914 DataTime=5.377 Loss=1.264 Prec@1=69.141 Prec@5=87.695 rate=8314.83 Hz, eta=0:00:00, total=0:00:00, wall=13:44 IST=> training_quantize   0.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=6.914 DataTime=5.377 Loss=1.264 Prec@1=69.141 Prec@5=87.695 rate=8314.83 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST=> training_quantize   0.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.147 DataTime=0.282 Loss=1.188 Prec@1=70.427 Prec@5=89.088 rate=8314.83 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST=> training_quantize   4.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.147 DataTime=0.282 Loss=1.188 Prec@1=70.427 Prec@5=89.088 rate=0.93 Hz, eta=0:43:10, total=0:01:48, wall=13:46 IST=> training_quantize   4.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.147 DataTime=0.282 Loss=1.188 Prec@1=70.427 Prec@5=89.088 rate=0.93 Hz, eta=0:43:10, total=0:01:48, wall=13:48 IST=> training_quantize   4.04% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.118 DataTime=0.258 Loss=1.189 Prec@1=70.482 Prec@5=89.125 rate=0.93 Hz, eta=0:43:10, total=0:01:48, wall=13:48 IST=> training_quantize   8.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.118 DataTime=0.258 Loss=1.189 Prec@1=70.482 Prec@5=89.125 rate=0.92 Hz, eta=0:41:34, total=0:03:37, wall=13:48 IST=> training_quantize   8.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.118 DataTime=0.258 Loss=1.189 Prec@1=70.482 Prec@5=89.125 rate=0.92 Hz, eta=0:41:34, total=0:03:37, wall=13:50 IST=> training_quantize   8.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.108 DataTime=0.249 Loss=1.192 Prec@1=70.412 Prec@5=89.109 rate=0.92 Hz, eta=0:41:34, total=0:03:37, wall=13:50 IST=> training_quantize   12.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.108 DataTime=0.249 Loss=1.192 Prec@1=70.412 Prec@5=89.109 rate=0.92 Hz, eta=0:39:48, total=0:05:26, wall=13:50 IST=> training_quantize   12.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.108 DataTime=0.249 Loss=1.192 Prec@1=70.412 Prec@5=89.109 rate=0.92 Hz, eta=0:39:48, total=0:05:26, wall=13:51 IST=> training_quantize   12.03% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.102 DataTime=0.244 Loss=1.194 Prec@1=70.406 Prec@5=89.060 rate=0.92 Hz, eta=0:39:48, total=0:05:26, wall=13:51 IST=> training_quantize   16.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.102 DataTime=0.244 Loss=1.194 Prec@1=70.406 Prec@5=89.060 rate=0.92 Hz, eta=0:38:00, total=0:07:15, wall=13:51 IST=> training_quantize   16.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.102 DataTime=0.244 Loss=1.194 Prec@1=70.406 Prec@5=89.060 rate=0.92 Hz, eta=0:38:00, total=0:07:15, wall=13:53 IST=> training_quantize   16.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.411 Prec@5=89.117 rate=0.92 Hz, eta=0:38:00, total=0:07:15, wall=13:53 IST=> training_quantize   20.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.411 Prec@5=89.117 rate=0.92 Hz, eta=0:36:11, total=0:09:03, wall=13:53 IST=> training_quantize   20.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.192 Prec@1=70.411 Prec@5=89.117 rate=0.92 Hz, eta=0:36:11, total=0:09:03, wall=13:55 IST=> training_quantize   20.02% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.097 DataTime=0.240 Loss=1.191 Prec@1=70.452 Prec@5=89.097 rate=0.92 Hz, eta=0:36:11, total=0:09:03, wall=13:55 IST=> training_quantize   24.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.097 DataTime=0.240 Loss=1.191 Prec@1=70.452 Prec@5=89.097 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=13:55 IST=> training_quantize   24.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.097 DataTime=0.240 Loss=1.191 Prec@1=70.452 Prec@5=89.097 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=13:57 IST=> training_quantize   24.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.095 DataTime=0.239 Loss=1.191 Prec@1=70.428 Prec@5=89.116 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=13:57 IST=> training_quantize   28.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.095 DataTime=0.239 Loss=1.191 Prec@1=70.428 Prec@5=89.116 rate=0.92 Hz, eta=0:32:36, total=0:12:40, wall=13:57 IST=> training_quantize   28.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.095 DataTime=0.239 Loss=1.191 Prec@1=70.428 Prec@5=89.116 rate=0.92 Hz, eta=0:32:36, total=0:12:40, wall=13:59 IST=> training_quantize   28.01% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.094 DataTime=0.238 Loss=1.190 Prec@1=70.415 Prec@5=89.127 rate=0.92 Hz, eta=0:32:36, total=0:12:40, wall=13:59 IST=> training_quantize   32.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.094 DataTime=0.238 Loss=1.190 Prec@1=70.415 Prec@5=89.127 rate=0.92 Hz, eta=0:30:46, total=0:14:29, wall=13:59 IST=> training_quantize   32.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.094 DataTime=0.238 Loss=1.190 Prec@1=70.415 Prec@5=89.127 rate=0.92 Hz, eta=0:30:46, total=0:14:29, wall=14:00 IST=> training_quantize   32.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.093 DataTime=0.237 Loss=1.189 Prec@1=70.404 Prec@5=89.137 rate=0.92 Hz, eta=0:30:46, total=0:14:29, wall=14:00 IST=> training_quantize   36.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.093 DataTime=0.237 Loss=1.189 Prec@1=70.404 Prec@5=89.137 rate=0.92 Hz, eta=0:28:57, total=0:16:17, wall=14:00 IST=> training_quantize   36.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.093 DataTime=0.237 Loss=1.189 Prec@1=70.404 Prec@5=89.137 rate=0.92 Hz, eta=0:28:57, total=0:16:17, wall=14:02 IST=> training_quantize   36.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.189 Prec@1=70.379 Prec@5=89.149 rate=0.92 Hz, eta=0:28:57, total=0:16:17, wall=14:02 IST=> training_quantize   39.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.189 Prec@1=70.379 Prec@5=89.149 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:02 IST=> training_quantize   39.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.189 Prec@1=70.379 Prec@5=89.149 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:04 IST=> training_quantize   39.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.188 Prec@1=70.385 Prec@5=89.139 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:04 IST=> training_quantize   43.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.188 Prec@1=70.385 Prec@5=89.139 rate=0.92 Hz, eta=0:25:19, total=0:19:53, wall=14:04 IST=> training_quantize   43.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.188 Prec@1=70.385 Prec@5=89.139 rate=0.92 Hz, eta=0:25:19, total=0:19:53, wall=14:06 IST=> training_quantize   43.99% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.138 rate=0.92 Hz, eta=0:25:19, total=0:19:53, wall=14:06 IST=> training_quantize   47.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.138 rate=0.92 Hz, eta=0:23:31, total=0:21:41, wall=14:06 IST=> training_quantize   47.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.138 rate=0.92 Hz, eta=0:23:31, total=0:21:41, wall=14:08 IST=> training_quantize   47.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.188 Prec@1=70.382 Prec@5=89.130 rate=0.92 Hz, eta=0:23:31, total=0:21:41, wall=14:08 IST=> training_quantize   51.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.188 Prec@1=70.382 Prec@5=89.130 rate=0.92 Hz, eta=0:21:42, total=0:23:29, wall=14:08 IST=> training_quantize   51.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.188 Prec@1=70.382 Prec@5=89.130 rate=0.92 Hz, eta=0:21:42, total=0:23:29, wall=14:09 IST=> training_quantize   51.98% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.128 rate=0.92 Hz, eta=0:21:42, total=0:23:29, wall=14:09 IST=> training_quantize   55.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.128 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=14:09 IST=> training_quantize   55.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.235 Loss=1.188 Prec@1=70.385 Prec@5=89.128 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=14:11 IST=> training_quantize   55.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.189 Prec@1=70.377 Prec@5=89.125 rate=0.92 Hz, eta=0:19:53, total=0:25:17, wall=14:11 IST=> training_quantize   59.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.189 Prec@1=70.377 Prec@5=89.125 rate=0.92 Hz, eta=0:18:06, total=0:27:07, wall=14:11 IST=> training_quantize   59.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.189 Prec@1=70.377 Prec@5=89.125 rate=0.92 Hz, eta=0:18:06, total=0:27:07, wall=14:13 IST=> training_quantize   59.97% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.188 Prec@1=70.378 Prec@5=89.125 rate=0.92 Hz, eta=0:18:06, total=0:27:07, wall=14:13 IST=> training_quantize   63.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.188 Prec@1=70.378 Prec@5=89.125 rate=0.92 Hz, eta=0:16:17, total=0:28:55, wall=14:13 IST=> training_quantize   63.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.188 Prec@1=70.378 Prec@5=89.125 rate=0.92 Hz, eta=0:16:17, total=0:28:55, wall=14:15 IST=> training_quantize   63.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.189 Prec@1=70.367 Prec@5=89.116 rate=0.92 Hz, eta=0:16:17, total=0:28:55, wall=14:15 IST=> training_quantize   67.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.189 Prec@1=70.367 Prec@5=89.116 rate=0.92 Hz, eta=0:14:29, total=0:30:44, wall=14:15 IST=> training_quantize   67.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.189 Prec@1=70.367 Prec@5=89.116 rate=0.92 Hz, eta=0:14:29, total=0:30:44, wall=14:17 IST=> training_quantize   67.96% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.379 Prec@5=89.131 rate=0.92 Hz, eta=0:14:29, total=0:30:44, wall=14:17 IST=> training_quantize   71.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.379 Prec@5=89.131 rate=0.92 Hz, eta=0:12:41, total=0:32:33, wall=14:17 IST=> training_quantize   71.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.379 Prec@5=89.131 rate=0.92 Hz, eta=0:12:41, total=0:32:33, wall=14:18 IST=> training_quantize   71.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.370 Prec@5=89.122 rate=0.92 Hz, eta=0:12:41, total=0:32:33, wall=14:18 IST=> training_quantize   75.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.370 Prec@5=89.122 rate=0.92 Hz, eta=0:10:52, total=0:34:22, wall=14:18 IST=> training_quantize   75.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.370 Prec@5=89.122 rate=0.92 Hz, eta=0:10:52, total=0:34:22, wall=14:20 IST=> training_quantize   75.95% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.365 Prec@5=89.120 rate=0.92 Hz, eta=0:10:52, total=0:34:22, wall=14:20 IST=> training_quantize   79.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.365 Prec@5=89.120 rate=0.92 Hz, eta=0:09:04, total=0:36:10, wall=14:20 IST=> training_quantize   79.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.234 Loss=1.188 Prec@1=70.365 Prec@5=89.120 rate=0.92 Hz, eta=0:09:04, total=0:36:10, wall=14:22 IST=> training_quantize   79.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.378 Prec@5=89.131 rate=0.92 Hz, eta=0:09:04, total=0:36:10, wall=14:22 IST=> training_quantize   83.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.378 Prec@5=89.131 rate=0.92 Hz, eta=0:07:16, total=0:37:58, wall=14:22 IST=> training_quantize   83.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.378 Prec@5=89.131 rate=0.92 Hz, eta=0:07:16, total=0:37:58, wall=14:24 IST=> training_quantize   83.94% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.374 Prec@5=89.127 rate=0.92 Hz, eta=0:07:16, total=0:37:58, wall=14:24 IST=> training_quantize   87.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.374 Prec@5=89.127 rate=0.92 Hz, eta=0:05:27, total=0:39:46, wall=14:24 IST=> training_quantize   87.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.374 Prec@5=89.127 rate=0.92 Hz, eta=0:05:27, total=0:39:46, wall=14:26 IST=> training_quantize   87.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.368 Prec@5=89.115 rate=0.92 Hz, eta=0:05:27, total=0:39:46, wall=14:26 IST=> training_quantize   91.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.368 Prec@5=89.115 rate=0.92 Hz, eta=0:03:39, total=0:41:35, wall=14:26 IST=> training_quantize   91.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.188 Prec@1=70.368 Prec@5=89.115 rate=0.92 Hz, eta=0:03:39, total=0:41:35, wall=14:27 IST=> training_quantize   91.93% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.189 Prec@1=70.350 Prec@5=89.109 rate=0.92 Hz, eta=0:03:39, total=0:41:35, wall=14:27 IST=> training_quantize   95.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.189 Prec@1=70.350 Prec@5=89.109 rate=0.92 Hz, eta=0:01:50, total=0:43:23, wall=14:27 IST=> training_quantize   95.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.087 DataTime=0.233 Loss=1.189 Prec@1=70.350 Prec@5=89.109 rate=0.92 Hz, eta=0:01:50, total=0:43:23, wall=14:29 IST=> training_quantize   95.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.189 Prec@1=70.352 Prec@5=89.107 rate=0.92 Hz, eta=0:01:50, total=0:43:23, wall=14:29 IST=> training_quantize   99.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.189 Prec@1=70.352 Prec@5=89.107 rate=0.92 Hz, eta=0:00:02, total=0:45:14, wall=14:29 IST=> training_quantize   99.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.189 Prec@1=70.352 Prec@5=89.107 rate=0.92 Hz, eta=0:00:02, total=0:45:14, wall=14:29 IST=> training_quantize   99.92% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.189 Prec@1=70.353 Prec@5=89.107 rate=0.92 Hz, eta=0:00:02, total=0:45:14, wall=14:29 IST=> training_quantize   100.00% of 1x2503...Epoch=9/10 LR=0.00000 Time=1.088 DataTime=0.233 Loss=1.189 Prec@1=70.353 Prec@5=89.107 rate=0.92 Hz, eta=0:00:00, total=0:45:16, wall=14:29 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 8.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 8.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 0.00% of 1x98...Epoch=9/10 LR=0.00000 Time=7.572 Loss=1.102 Prec@1=74.805 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 1.02% of 1x98...Epoch=9/10 LR=0.00000 Time=7.572 Loss=1.102 Prec@1=74.805 Prec@5=90.820 rate=3673.42 Hz, eta=0:00:00, total=0:00:00, wall=14:30 IST** validation 1.02% of 1x98...Epoch=9/10 LR=0.00000 Time=7.572 Loss=1.102 Prec@1=74.805 Prec@5=90.820 rate=3673.42 Hz, eta=0:00:00, total=0:00:00, wall=14:30 IST** validation 1.02% of 1x98...Epoch=9/10 LR=0.00000 Time=0.641 Loss=1.144 Prec@1=71.538 Prec@5=90.250 rate=3673.42 Hz, eta=0:00:00, total=0:00:00, wall=14:30 IST** validation 100.00% of 1x98...Epoch=9/10 LR=0.00000 Time=0.641 Loss=1.144 Prec@1=71.538 Prec@5=90.250 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=14:30 IST
[39m[33m=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training_quantize   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training_quantize   0.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=7.488 DataTime=6.516 Loss=1.235 Prec@1=71.484 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training_quantize   0.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=7.488 DataTime=6.516 Loss=1.235 Prec@1=71.484 Prec@5=89.258 rate=7702.20 Hz, eta=0:00:00, total=0:00:00, wall=14:31 IST=> training_quantize   0.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=7.488 DataTime=6.516 Loss=1.235 Prec@1=71.484 Prec@5=89.258 rate=7702.20 Hz, eta=0:00:00, total=0:00:00, wall=14:32 IST=> training_quantize   0.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.152 DataTime=0.290 Loss=1.192 Prec@1=70.198 Prec@5=89.177 rate=7702.20 Hz, eta=0:00:00, total=0:00:00, wall=14:32 IST=> training_quantize   4.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.152 DataTime=0.290 Loss=1.192 Prec@1=70.198 Prec@5=89.177 rate=0.93 Hz, eta=0:43:08, total=0:01:48, wall=14:32 IST=> training_quantize   4.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.152 DataTime=0.290 Loss=1.192 Prec@1=70.198 Prec@5=89.177 rate=0.93 Hz, eta=0:43:08, total=0:01:48, wall=14:34 IST=> training_quantize   4.04% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.121 DataTime=0.260 Loss=1.188 Prec@1=70.376 Prec@5=89.151 rate=0.93 Hz, eta=0:43:08, total=0:01:48, wall=14:34 IST=> training_quantize   8.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.121 DataTime=0.260 Loss=1.188 Prec@1=70.376 Prec@5=89.151 rate=0.92 Hz, eta=0:41:35, total=0:03:37, wall=14:34 IST=> training_quantize   8.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.121 DataTime=0.260 Loss=1.188 Prec@1=70.376 Prec@5=89.151 rate=0.92 Hz, eta=0:41:35, total=0:03:37, wall=14:36 IST=> training_quantize   8.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.109 DataTime=0.251 Loss=1.187 Prec@1=70.383 Prec@5=89.143 rate=0.92 Hz, eta=0:41:35, total=0:03:37, wall=14:36 IST=> training_quantize   12.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.109 DataTime=0.251 Loss=1.187 Prec@1=70.383 Prec@5=89.143 rate=0.92 Hz, eta=0:39:47, total=0:05:26, wall=14:36 IST=> training_quantize   12.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.109 DataTime=0.251 Loss=1.187 Prec@1=70.383 Prec@5=89.143 rate=0.92 Hz, eta=0:39:47, total=0:05:26, wall=14:38 IST=> training_quantize   12.03% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.187 Prec@1=70.343 Prec@5=89.146 rate=0.92 Hz, eta=0:39:47, total=0:05:26, wall=14:38 IST=> training_quantize   16.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.187 Prec@1=70.343 Prec@5=89.146 rate=0.92 Hz, eta=0:38:01, total=0:07:15, wall=14:38 IST=> training_quantize   16.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.104 DataTime=0.246 Loss=1.187 Prec@1=70.343 Prec@5=89.146 rate=0.92 Hz, eta=0:38:01, total=0:07:15, wall=14:40 IST=> training_quantize   16.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.100 DataTime=0.243 Loss=1.188 Prec@1=70.277 Prec@5=89.135 rate=0.92 Hz, eta=0:38:01, total=0:07:15, wall=14:40 IST=> training_quantize   20.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.100 DataTime=0.243 Loss=1.188 Prec@1=70.277 Prec@5=89.135 rate=0.92 Hz, eta=0:36:12, total=0:09:03, wall=14:40 IST=> training_quantize   20.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.100 DataTime=0.243 Loss=1.188 Prec@1=70.277 Prec@5=89.135 rate=0.92 Hz, eta=0:36:12, total=0:09:03, wall=14:42 IST=> training_quantize   20.02% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.189 Prec@1=70.293 Prec@5=89.111 rate=0.92 Hz, eta=0:36:12, total=0:09:03, wall=14:42 IST=> training_quantize   24.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.189 Prec@1=70.293 Prec@5=89.111 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=14:42 IST=> training_quantize   24.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.098 DataTime=0.242 Loss=1.189 Prec@1=70.293 Prec@5=89.111 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=14:43 IST=> training_quantize   24.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.095 DataTime=0.240 Loss=1.190 Prec@1=70.235 Prec@5=89.088 rate=0.92 Hz, eta=0:34:23, total=0:10:52, wall=14:43 IST=> training_quantize   28.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.095 DataTime=0.240 Loss=1.190 Prec@1=70.235 Prec@5=89.088 rate=0.92 Hz, eta=0:32:33, total=0:12:40, wall=14:43 IST=> training_quantize   28.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.095 DataTime=0.240 Loss=1.190 Prec@1=70.235 Prec@5=89.088 rate=0.92 Hz, eta=0:32:33, total=0:12:40, wall=14:45 IST=> training_quantize   28.01% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.093 DataTime=0.239 Loss=1.192 Prec@1=70.224 Prec@5=89.084 rate=0.92 Hz, eta=0:32:33, total=0:12:40, wall=14:45 IST=> training_quantize   32.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.093 DataTime=0.239 Loss=1.192 Prec@1=70.224 Prec@5=89.084 rate=0.92 Hz, eta=0:30:44, total=0:14:28, wall=14:45 IST=> training_quantize   32.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.093 DataTime=0.239 Loss=1.192 Prec@1=70.224 Prec@5=89.084 rate=0.92 Hz, eta=0:30:44, total=0:14:28, wall=14:47 IST=> training_quantize   32.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.191 Prec@1=70.238 Prec@5=89.099 rate=0.92 Hz, eta=0:30:44, total=0:14:28, wall=14:47 IST=> training_quantize   36.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.191 Prec@1=70.238 Prec@5=89.099 rate=0.92 Hz, eta=0:28:56, total=0:16:16, wall=14:47 IST=> training_quantize   36.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.191 Prec@1=70.238 Prec@5=89.099 rate=0.92 Hz, eta=0:28:56, total=0:16:16, wall=14:49 IST=> training_quantize   36.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.190 Prec@1=70.252 Prec@5=89.106 rate=0.92 Hz, eta=0:28:56, total=0:16:16, wall=14:49 IST=> training_quantize   39.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.190 Prec@1=70.252 Prec@5=89.106 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:49 IST=> training_quantize   39.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.092 DataTime=0.238 Loss=1.190 Prec@1=70.252 Prec@5=89.106 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:51 IST=> training_quantize   39.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.228 Prec@5=89.061 rate=0.92 Hz, eta=0:27:08, total=0:18:05, wall=14:51 IST=> training_quantize   43.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.228 Prec@5=89.061 rate=0.92 Hz, eta=0:25:20, total=0:19:53, wall=14:51 IST=> training_quantize   43.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.192 Prec@1=70.228 Prec@5=89.061 rate=0.92 Hz, eta=0:25:20, total=0:19:53, wall=14:52 IST=> training_quantize   43.99% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.191 Prec@1=70.237 Prec@5=89.087 rate=0.92 Hz, eta=0:25:20, total=0:19:53, wall=14:52 IST=> training_quantize   47.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.191 Prec@1=70.237 Prec@5=89.087 rate=0.92 Hz, eta=0:23:32, total=0:21:42, wall=14:52 IST=> training_quantize   47.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.091 DataTime=0.237 Loss=1.191 Prec@1=70.237 Prec@5=89.087 rate=0.92 Hz, eta=0:23:32, total=0:21:42, wall=14:54 IST=> training_quantize   47.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.232 Prec@5=89.090 rate=0.92 Hz, eta=0:23:32, total=0:21:42, wall=14:54 IST=> training_quantize   51.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.232 Prec@5=89.090 rate=0.92 Hz, eta=0:21:43, total=0:23:30, wall=14:54 IST=> training_quantize   51.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.232 Prec@5=89.090 rate=0.92 Hz, eta=0:21:43, total=0:23:30, wall=14:56 IST=> training_quantize   51.98% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.241 Prec@5=89.069 rate=0.92 Hz, eta=0:21:43, total=0:23:30, wall=14:56 IST=> training_quantize   55.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.241 Prec@5=89.069 rate=0.92 Hz, eta=0:19:55, total=0:25:19, wall=14:56 IST=> training_quantize   55.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.191 Prec@1=70.241 Prec@5=89.069 rate=0.92 Hz, eta=0:19:55, total=0:25:19, wall=14:58 IST=> training_quantize   55.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.192 Prec@1=70.225 Prec@5=89.052 rate=0.92 Hz, eta=0:19:55, total=0:25:19, wall=14:58 IST=> training_quantize   59.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.192 Prec@1=70.225 Prec@5=89.052 rate=0.92 Hz, eta=0:18:06, total=0:27:08, wall=14:58 IST=> training_quantize   59.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.236 Loss=1.192 Prec@1=70.225 Prec@5=89.052 rate=0.92 Hz, eta=0:18:06, total=0:27:08, wall=15:00 IST=> training_quantize   59.97% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.192 Prec@1=70.240 Prec@5=89.043 rate=0.92 Hz, eta=0:18:06, total=0:27:08, wall=15:00 IST=> training_quantize   63.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.192 Prec@1=70.240 Prec@5=89.043 rate=0.92 Hz, eta=0:16:18, total=0:28:56, wall=15:00 IST=> training_quantize   63.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.192 Prec@1=70.240 Prec@5=89.043 rate=0.92 Hz, eta=0:16:18, total=0:28:56, wall=15:01 IST=> training_quantize   63.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.254 Prec@5=89.060 rate=0.92 Hz, eta=0:16:18, total=0:28:56, wall=15:01 IST=> training_quantize   67.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.254 Prec@5=89.060 rate=0.92 Hz, eta=0:14:30, total=0:30:45, wall=15:01 IST=> training_quantize   67.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.254 Prec@5=89.060 rate=0.92 Hz, eta=0:14:30, total=0:30:45, wall=15:03 IST=> training_quantize   67.96% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.191 Prec@1=70.256 Prec@5=89.055 rate=0.92 Hz, eta=0:14:30, total=0:30:45, wall=15:03 IST=> training_quantize   71.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.191 Prec@1=70.256 Prec@5=89.055 rate=0.92 Hz, eta=0:12:41, total=0:32:34, wall=15:03 IST=> training_quantize   71.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.090 DataTime=0.235 Loss=1.191 Prec@1=70.256 Prec@5=89.055 rate=0.92 Hz, eta=0:12:41, total=0:32:34, wall=15:05 IST=> training_quantize   71.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.264 Prec@5=89.052 rate=0.92 Hz, eta=0:12:41, total=0:32:34, wall=15:05 IST=> training_quantize   75.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.264 Prec@5=89.052 rate=0.92 Hz, eta=0:10:53, total=0:34:23, wall=15:05 IST=> training_quantize   75.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.235 Loss=1.191 Prec@1=70.264 Prec@5=89.052 rate=0.92 Hz, eta=0:10:53, total=0:34:23, wall=15:07 IST=> training_quantize   75.95% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.286 Prec@5=89.058 rate=0.92 Hz, eta=0:10:53, total=0:34:23, wall=15:07 IST=> training_quantize   79.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.286 Prec@5=89.058 rate=0.92 Hz, eta=0:09:04, total=0:36:12, wall=15:07 IST=> training_quantize   79.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.286 Prec@5=89.058 rate=0.92 Hz, eta=0:09:04, total=0:36:12, wall=15:09 IST=> training_quantize   79.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.287 Prec@5=89.060 rate=0.92 Hz, eta=0:09:04, total=0:36:12, wall=15:09 IST=> training_quantize   83.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.287 Prec@5=89.060 rate=0.92 Hz, eta=0:07:16, total=0:38:00, wall=15:09 IST=> training_quantize   83.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.287 Prec@5=89.060 rate=0.92 Hz, eta=0:07:16, total=0:38:00, wall=15:10 IST=> training_quantize   83.94% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.306 Prec@5=89.072 rate=0.92 Hz, eta=0:07:16, total=0:38:00, wall=15:10 IST=> training_quantize   87.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.306 Prec@5=89.072 rate=0.92 Hz, eta=0:05:27, total=0:39:49, wall=15:10 IST=> training_quantize   87.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.306 Prec@5=89.072 rate=0.92 Hz, eta=0:05:27, total=0:39:49, wall=15:12 IST=> training_quantize   87.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.300 Prec@5=89.064 rate=0.92 Hz, eta=0:05:27, total=0:39:49, wall=15:12 IST=> training_quantize   91.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.300 Prec@5=89.064 rate=0.92 Hz, eta=0:03:39, total=0:41:37, wall=15:12 IST=> training_quantize   91.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.189 Prec@1=70.300 Prec@5=89.064 rate=0.92 Hz, eta=0:03:39, total=0:41:37, wall=15:14 IST=> training_quantize   91.93% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.280 Prec@5=89.049 rate=0.92 Hz, eta=0:03:39, total=0:41:37, wall=15:14 IST=> training_quantize   95.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.280 Prec@5=89.049 rate=0.92 Hz, eta=0:01:50, total=0:43:26, wall=15:14 IST=> training_quantize   95.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.190 Prec@1=70.280 Prec@5=89.049 rate=0.92 Hz, eta=0:01:50, total=0:43:26, wall=15:16 IST=> training_quantize   95.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.191 Prec@1=70.273 Prec@5=89.050 rate=0.92 Hz, eta=0:01:50, total=0:43:26, wall=15:16 IST=> training_quantize   99.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.191 Prec@1=70.273 Prec@5=89.050 rate=0.92 Hz, eta=0:00:02, total=0:45:16, wall=15:16 IST=> training_quantize   99.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.191 Prec@1=70.273 Prec@5=89.050 rate=0.92 Hz, eta=0:00:02, total=0:45:16, wall=15:16 IST=> training_quantize   99.92% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.191 Prec@1=70.274 Prec@5=89.051 rate=0.92 Hz, eta=0:00:02, total=0:45:16, wall=15:16 IST=> training_quantize   100.00% of 1x2503...Epoch=10/10 LR=0.00000 Time=1.089 DataTime=0.234 Loss=1.191 Prec@1=70.274 Prec@5=89.051 rate=0.92 Hz, eta=0:00:00, total=0:45:18, wall=15:16 IST
[39m
clips_act : 4.000 8.000 8.000 16.000 8.000 8.000 8.000 4.000 4.000 8.000 4.000 4.000 4.000 2.000 8.000 8.000 2.000 4.000 8.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 4.000 4.000 2.000 2.000 4.000 1.000 2.000 4.000 2.000 4.000 2.000 2.000 2.000 2.000 2.000 2.000 8.000 16.000 8.000 32.000[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST=> validation 0.00% of 1x98...Epoch=10/10 LR=0.00000 Time=7.967 Loss=1.233 Prec@1=69.531 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST=> validation 1.02% of 1x98...Epoch=10/10 LR=0.00000 Time=7.967 Loss=1.233 Prec@1=69.531 Prec@5=90.430 rate=5839.93 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST** validation 1.02% of 1x98...Epoch=10/10 LR=0.00000 Time=7.967 Loss=1.233 Prec@1=69.531 Prec@5=90.430 rate=5839.93 Hz, eta=0:00:00, total=0:00:00, wall=15:17 IST** validation 1.02% of 1x98...Epoch=10/10 LR=0.00000 Time=0.648 Loss=1.142 Prec@1=71.614 Prec@5=90.242 rate=5839.93 Hz, eta=0:00:00, total=0:00:00, wall=15:17 IST** validation 100.00% of 1x98...Epoch=10/10 LR=0.00000 Time=0.648 Loss=1.142 Prec@1=71.614 Prec@5=90.242 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=15:17 IST
[39m