[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_large_x2', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training', 'date': '2021-05-22_14-26-12', 'workers': 12, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fe177c627b8>, 'epochs': 150, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'auto_augument': None, 'random_erasing': 0, 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
auto_augument: None
batch_size: 512
best_prec1: -1
beta: 0.999
bias_calibration: True
bias_decay: None
bitwidth_activations: 8
bitwidth_weights: 8
constrain_bias: None
count_flops: True
data_augument: inception
data_path: ./data/datasets/image_folder_classification
dataset_config: {}
dataset_name: image_folder_classification
date: 2021-05-22_14-26-12
dist_backend: gloo
dist_url: tcp://224.66.41.62:23456
distributed: False
epoch_size: 0
epoch_size_val: 0
epochs: 150
evaluate_start: True
freeze_bn: False
histogram_range: True
image_mean: (123.675, 116.28, 103.53)
image_scale: (0.017125, 0.017507, 0.017429)
img_crop: 224
img_resize: 256
input_channel_reverse: False
iter_size: 1
logger: <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fe177c627b8>
lr: 0.1
lr_calib: 0.05
lr_clips: None
milestones: (30, 60, 90)
model: None
model_config: {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}
model_name: mobilenetv3_lite_large_x2
momentum: 0.9
multi_color_modes: None
multistep_gamma: 0.1
num_inputs: 1
opset_version: 11
optimizer: sgd
parallel_model: True
per_channel_q: False
phase: training
polystep_power: 1.0
pretrained: None
print_freq: 100
print_model: False
quantize: False
rand_scale: (0.2, 1.0)
rand_seed: 1
random_erasing: 0
resume: None
run_soon: True
save_mod_files: False
save_onnx: True
save_path: None
scheduler: cosine
shuffle: True
shuffle_val: True
start_epoch: 0
step_size: 1
stop_epoch: 150
total_batch_size: 512
transforms: None
warmup_epochs: 5
warmup_factor: 0.001
weight_decay: 4e-05
workers: 12
world_size: 1
=> resize resolution: 256
=> crop resolution  : 224
=> creating model 'mobilenetv3_lite_large_x2'
=> Resize = 256, Crop = 224, GFLOPs = 1.353773568, GMACs = 0.676886784
MobileNetV3Lite(
  (features): Sequential(
    (0): ConvBNActivation(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (2): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (3): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (4): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(144, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (5): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (6): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (7): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (8): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 400, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(400, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400, bias=False)
          (1): BatchNorm2d(400, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (9): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(368, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=368, bias=False)
          (1): BatchNorm2d(368, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(368, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (10): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 368, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(368, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(368, 368, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=368, bias=False)
          (1): BatchNorm2d(368, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(368, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (11): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (12): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1344, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
          (1): BatchNorm2d(1344, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(224, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (13): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1344, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(1344, 1344, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=1344, bias=False)
          (1): BatchNorm2d(1344, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(1344, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (14): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (15): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (16): ConvBNActivation(
      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Linear(in_features=960, out_features=1280, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=True)
    (3): Linear(in_features=1280, out_features=1000, bias=True)
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_large_x2', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training', 'date': '2021-05-22_14-26-12', 'workers': 12, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fe177c627b8>, 'epochs': 150, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'auto_augument': None, 'random_erasing': 0, 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
=> optimizer type   : sgd
=> learning rate    : 0.1
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 150
[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> validation 0.00% of 1x98...Epoch=1/150 LR=0.00010 Time=13.218 Loss=6.908 Prec@1=0.000 Prec@5=0.391 rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=13.218 Loss=6.908 Prec@1=0.000 Prec@5=0.391 rate=2566.72 Hz, eta=0:00:00, total=0:00:00, wall=14:26 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=13.218 Loss=6.908 Prec@1=0.000 Prec@5=0.391 rate=2566.72 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=0.439 Loss=6.908 Prec@1=0.092 Prec@5=0.544 rate=2566.72 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST** validation 100.00% of 1x98...Epoch=1/150 LR=0.00010 Time=0.439 Loss=6.908 Prec@1=0.092 Prec@5=0.544 rate=3.28 Hz, eta=0:00:00, total=0:00:29, wall=14:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST=> training   0.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=9.095 DataTime=5.664 Loss=6.910 Prec@1=0.000 Prec@5=0.195 rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=9.095 DataTime=5.664 Loss=6.910 Prec@1=0.000 Prec@5=0.195 rate=5355.47 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=9.095 DataTime=5.664 Loss=6.910 Prec@1=0.000 Prec@5=0.195 rate=5355.47 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.700 DataTime=0.425 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=5355.47 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.700 DataTime=0.425 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=14:28 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.700 DataTime=0.425 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=14:29 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.657 DataTime=0.408 Loss=6.908 Prec@1=0.113 Prec@5=0.535 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=14:29 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.657 DataTime=0.408 Loss=6.908 Prec@1=0.113 Prec@5=0.535 rate=1.63 Hz, eta=0:23:32, total=0:02:03, wall=14:29 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.657 DataTime=0.408 Loss=6.908 Prec@1=0.113 Prec@5=0.535 rate=1.63 Hz, eta=0:23:32, total=0:02:03, wall=14:30 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.643 DataTime=0.402 Loss=6.908 Prec@1=0.110 Prec@5=0.513 rate=1.63 Hz, eta=0:23:32, total=0:02:03, wall=14:30 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.643 DataTime=0.402 Loss=6.908 Prec@1=0.110 Prec@5=0.513 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=14:30 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.643 DataTime=0.402 Loss=6.908 Prec@1=0.110 Prec@5=0.513 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=14:31 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.635 DataTime=0.399 Loss=6.908 Prec@1=0.104 Prec@5=0.509 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=14:31 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.635 DataTime=0.399 Loss=6.908 Prec@1=0.104 Prec@5=0.509 rate=1.63 Hz, eta=0:21:29, total=0:04:06, wall=14:31 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.635 DataTime=0.399 Loss=6.908 Prec@1=0.104 Prec@5=0.509 rate=1.63 Hz, eta=0:21:29, total=0:04:06, wall=14:32 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.631 DataTime=0.397 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:21:29, total=0:04:06, wall=14:32 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.631 DataTime=0.397 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:32 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.631 DataTime=0.397 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:33 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.628 DataTime=0.396 Loss=6.908 Prec@1=0.098 Prec@5=0.513 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:33 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.628 DataTime=0.396 Loss=6.908 Prec@1=0.098 Prec@5=0.513 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=14:33 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.628 DataTime=0.396 Loss=6.908 Prec@1=0.098 Prec@5=0.513 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=14:34 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.626 DataTime=0.395 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=14:34 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.626 DataTime=0.395 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:34 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.626 DataTime=0.395 Loss=6.908 Prec@1=0.099 Prec@5=0.515 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:35 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.624 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.514 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:35 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.624 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.514 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:35 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.624 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.514 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:36 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.623 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.521 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:36 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.623 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.521 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:36 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.623 DataTime=0.394 Loss=6.908 Prec@1=0.101 Prec@5=0.521 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:37 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.622 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.522 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.622 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.522 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.622 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.522 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:38 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.527 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:38 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.527 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:38 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.102 Prec@5=0.527 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:39 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.103 Prec@5=0.527 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:39 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.103 Prec@5=0.527 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:39 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.621 DataTime=0.393 Loss=6.908 Prec@1=0.103 Prec@5=0.527 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:40 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.529 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:40 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.529 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:40 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.529 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:41 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.104 Prec@5=0.527 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:41 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.104 Prec@5=0.527 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:41 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.620 DataTime=0.392 Loss=6.908 Prec@1=0.104 Prec@5=0.527 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:42 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.526 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:42 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.526 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:42 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.105 Prec@5=0.526 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:43 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:43 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:43 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.619 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:44 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:44 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:44 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.531 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:45 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:45 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:45 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:46 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.533 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:46 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.533 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:46 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.107 Prec@5=0.533 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:47 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.109 Prec@5=0.534 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:47 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.109 Prec@5=0.534 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:47 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.908 Prec@1=0.109 Prec@5=0.534 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:48 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:48 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:48 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.532 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:49 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:49 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:49 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:50 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:50 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:50 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:51 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:51 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:51 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.527 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:52 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:52 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:52 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.617 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:52 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:52 IST=> training   100.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.618 DataTime=0.391 Loss=6.907 Prec@1=0.108 Prec@5=0.528 rate=1.63 Hz, eta=0:00:00, total=0:25:38, wall=14:52 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> validation 0.00% of 1x98...Epoch=1/150 LR=0.00010 Time=5.366 Loss=6.904 Prec@1=0.195 Prec@5=0.781 rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=5.366 Loss=6.904 Prec@1=0.195 Prec@5=0.781 rate=6883.68 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=5.366 Loss=6.904 Prec@1=0.195 Prec@5=0.781 rate=6883.68 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=0.390 Loss=6.906 Prec@1=0.146 Prec@5=0.528 rate=6883.68 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST** validation 100.00% of 1x98...Epoch=1/150 LR=0.00010 Time=0.390 Loss=6.906 Prec@1=0.146 Prec@5=0.528 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=14:53 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:53 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:53 IST=> training   0.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=7.049 DataTime=5.499 Loss=6.909 Prec@1=0.000 Prec@5=0.195 rate=0 Hz, eta=?, total=0:00:00, wall=14:53 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=7.049 DataTime=5.499 Loss=6.909 Prec@1=0.000 Prec@5=0.195 rate=9204.46 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=7.049 DataTime=5.499 Loss=6.909 Prec@1=0.000 Prec@5=0.195 rate=9204.46 Hz, eta=0:00:00, total=0:00:00, wall=14:54 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.672 DataTime=0.434 Loss=6.887 Prec@1=0.143 Prec@5=0.690 rate=9204.46 Hz, eta=0:00:00, total=0:00:00, wall=14:54 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.672 DataTime=0.434 Loss=6.887 Prec@1=0.143 Prec@5=0.690 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=14:54 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.672 DataTime=0.434 Loss=6.887 Prec@1=0.143 Prec@5=0.690 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=14:55 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.644 DataTime=0.413 Loss=6.852 Prec@1=0.223 Prec@5=1.014 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=14:55 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.644 DataTime=0.413 Loss=6.852 Prec@1=0.223 Prec@5=1.014 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=14:55 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.644 DataTime=0.413 Loss=6.852 Prec@1=0.223 Prec@5=1.014 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=14:56 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.634 DataTime=0.405 Loss=6.786 Prec@1=0.312 Prec@5=1.327 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=14:56 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.634 DataTime=0.405 Loss=6.786 Prec@1=0.312 Prec@5=1.327 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=14:56 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.634 DataTime=0.405 Loss=6.786 Prec@1=0.312 Prec@5=1.327 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=14:57 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.629 DataTime=0.401 Loss=6.710 Prec@1=0.406 Prec@5=1.667 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=14:57 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.629 DataTime=0.401 Loss=6.710 Prec@1=0.406 Prec@5=1.667 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=14:57 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.629 DataTime=0.401 Loss=6.710 Prec@1=0.406 Prec@5=1.667 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=14:58 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.627 DataTime=0.399 Loss=6.632 Prec@1=0.508 Prec@5=2.094 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=14:58 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.627 DataTime=0.399 Loss=6.632 Prec@1=0.508 Prec@5=2.094 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:58 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.627 DataTime=0.399 Loss=6.632 Prec@1=0.508 Prec@5=2.094 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:59 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.625 DataTime=0.398 Loss=6.554 Prec@1=0.631 Prec@5=2.563 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=14:59 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.625 DataTime=0.398 Loss=6.554 Prec@1=0.631 Prec@5=2.563 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=14:59 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.625 DataTime=0.398 Loss=6.554 Prec@1=0.631 Prec@5=2.563 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=15:00 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.623 DataTime=0.397 Loss=6.476 Prec@1=0.771 Prec@5=3.095 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=15:00 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.623 DataTime=0.397 Loss=6.476 Prec@1=0.771 Prec@5=3.095 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=15:00 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.623 DataTime=0.397 Loss=6.476 Prec@1=0.771 Prec@5=3.095 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=15:01 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.622 DataTime=0.396 Loss=6.400 Prec@1=0.941 Prec@5=3.704 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=15:01 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.622 DataTime=0.396 Loss=6.400 Prec@1=0.941 Prec@5=3.704 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=15:01 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.622 DataTime=0.396 Loss=6.400 Prec@1=0.941 Prec@5=3.704 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=15:02 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.621 DataTime=0.395 Loss=6.327 Prec@1=1.129 Prec@5=4.333 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=15:02 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.621 DataTime=0.395 Loss=6.327 Prec@1=1.129 Prec@5=4.333 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=15:02 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.621 DataTime=0.395 Loss=6.327 Prec@1=1.129 Prec@5=4.333 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=15:03 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.395 Loss=6.258 Prec@1=1.343 Prec@5=5.013 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=15:03 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.395 Loss=6.258 Prec@1=1.343 Prec@5=5.013 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:03 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.395 Loss=6.258 Prec@1=1.343 Prec@5=5.013 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:04 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.394 Loss=6.191 Prec@1=1.562 Prec@5=5.702 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:04 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.394 Loss=6.191 Prec@1=1.562 Prec@5=5.702 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:04 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.620 DataTime=0.394 Loss=6.191 Prec@1=1.562 Prec@5=5.702 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:05 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.129 Prec@1=1.783 Prec@5=6.400 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:05 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.129 Prec@1=1.783 Prec@5=6.400 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=15:05 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.129 Prec@1=1.783 Prec@5=6.400 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=15:06 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.069 Prec@1=2.023 Prec@5=7.110 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=15:06 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.069 Prec@1=2.023 Prec@5=7.110 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:06 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.069 Prec@1=2.023 Prec@5=7.110 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:07 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.012 Prec@1=2.274 Prec@5=7.804 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:07 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.012 Prec@1=2.274 Prec@5=7.804 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=15:07 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.619 DataTime=0.394 Loss=6.012 Prec@1=2.274 Prec@5=7.804 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=15:09 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.958 Prec@1=2.498 Prec@5=8.478 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=15:09 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.958 Prec@1=2.498 Prec@5=8.478 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=15:09 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.958 Prec@1=2.498 Prec@5=8.478 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=15:10 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.905 Prec@1=2.760 Prec@5=9.167 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=15:10 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.905 Prec@1=2.760 Prec@5=9.167 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:10 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.905 Prec@1=2.760 Prec@5=9.167 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:11 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.855 Prec@1=3.021 Prec@5=9.860 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:11 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.855 Prec@1=3.021 Prec@5=9.860 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:11 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.855 Prec@1=3.021 Prec@5=9.860 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:12 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.806 Prec@1=3.283 Prec@5=10.551 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:12 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.806 Prec@1=3.283 Prec@5=10.551 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:12 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.618 DataTime=0.393 Loss=5.806 Prec@1=3.283 Prec@5=10.551 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:13 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.759 Prec@1=3.549 Prec@5=11.218 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:13 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.759 Prec@1=3.549 Prec@5=11.218 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=15:13 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.759 Prec@1=3.549 Prec@5=11.218 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=15:14 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.714 Prec@1=3.824 Prec@5=11.896 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=15:14 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.714 Prec@1=3.824 Prec@5=11.896 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:14 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.714 Prec@1=3.824 Prec@5=11.896 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:15 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.669 Prec@1=4.111 Prec@5=12.580 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:15 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.669 Prec@1=4.111 Prec@5=12.580 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:15 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.669 Prec@1=4.111 Prec@5=12.580 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:16 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.626 Prec@1=4.386 Prec@5=13.242 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:16 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.626 Prec@1=4.386 Prec@5=13.242 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:16 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.626 Prec@1=4.386 Prec@5=13.242 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:17 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.584 Prec@1=4.680 Prec@5=13.907 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:17 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.584 Prec@1=4.680 Prec@5=13.907 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:17 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.584 Prec@1=4.680 Prec@5=13.907 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:18 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.543 Prec@1=4.963 Prec@5=14.569 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:18 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.543 Prec@1=4.963 Prec@5=14.569 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:18 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.543 Prec@1=4.963 Prec@5=14.569 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:19 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.504 Prec@1=5.251 Prec@5=15.213 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:19 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.504 Prec@1=5.251 Prec@5=15.213 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:19 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.504 Prec@1=5.251 Prec@5=15.213 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:19 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.503 Prec@1=5.255 Prec@5=15.219 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:19 IST=> training   100.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.617 DataTime=0.392 Loss=5.503 Prec@1=5.255 Prec@5=15.219 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=15:19 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:19 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:19 IST=> validation 0.00% of 1x98...Epoch=2/150 LR=0.02000 Time=6.970 Loss=4.732 Prec@1=8.008 Prec@5=25.781 rate=0 Hz, eta=?, total=0:00:00, wall=15:19 IST=> validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=6.970 Loss=4.732 Prec@1=8.008 Prec@5=25.781 rate=7291.66 Hz, eta=0:00:00, total=0:00:00, wall=15:19 IST** validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=6.970 Loss=4.732 Prec@1=8.008 Prec@5=25.781 rate=7291.66 Hz, eta=0:00:00, total=0:00:00, wall=15:19 IST** validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=0.400 Loss=4.686 Prec@1=10.878 Prec@5=28.210 rate=7291.66 Hz, eta=0:00:00, total=0:00:00, wall=15:19 IST** validation 100.00% of 1x98...Epoch=2/150 LR=0.02000 Time=0.400 Loss=4.686 Prec@1=10.878 Prec@5=28.210 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=15:19 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:20 IST=> training   0.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=5.908 DataTime=5.686 Loss=4.482 Prec@1=17.578 Prec@5=30.078 rate=0 Hz, eta=?, total=0:00:00, wall=15:20 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=5.908 DataTime=5.686 Loss=4.482 Prec@1=17.578 Prec@5=30.078 rate=7800.37 Hz, eta=0:00:00, total=0:00:00, wall=15:20 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=5.908 DataTime=5.686 Loss=4.482 Prec@1=17.578 Prec@5=30.078 rate=7800.37 Hz, eta=0:00:00, total=0:00:00, wall=15:21 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.663 DataTime=0.439 Loss=4.637 Prec@1=11.044 Prec@5=28.829 rate=7800.37 Hz, eta=0:00:00, total=0:00:00, wall=15:21 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.663 DataTime=0.439 Loss=4.637 Prec@1=11.044 Prec@5=28.829 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=15:21 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.663 DataTime=0.439 Loss=4.637 Prec@1=11.044 Prec@5=28.829 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=15:22 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.638 DataTime=0.414 Loss=4.603 Prec@1=11.526 Prec@5=29.568 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=15:22 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.638 DataTime=0.414 Loss=4.603 Prec@1=11.526 Prec@5=29.568 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=15:22 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.638 DataTime=0.414 Loss=4.603 Prec@1=11.526 Prec@5=29.568 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=15:23 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.630 DataTime=0.405 Loss=4.565 Prec@1=12.019 Prec@5=30.388 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=15:23 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.630 DataTime=0.405 Loss=4.565 Prec@1=12.019 Prec@5=30.388 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=15:23 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.630 DataTime=0.405 Loss=4.565 Prec@1=12.019 Prec@5=30.388 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=15:24 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.627 DataTime=0.401 Loss=4.530 Prec@1=12.384 Prec@5=31.039 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=15:24 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.627 DataTime=0.401 Loss=4.530 Prec@1=12.384 Prec@5=31.039 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:24 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.627 DataTime=0.401 Loss=4.530 Prec@1=12.384 Prec@5=31.039 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:25 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.624 DataTime=0.399 Loss=4.491 Prec@1=12.802 Prec@5=31.807 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:25 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.624 DataTime=0.399 Loss=4.491 Prec@1=12.802 Prec@5=31.807 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=15:25 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.624 DataTime=0.399 Loss=4.491 Prec@1=12.802 Prec@5=31.807 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=15:26 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.623 DataTime=0.397 Loss=4.459 Prec@1=13.172 Prec@5=32.462 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=15:26 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.623 DataTime=0.397 Loss=4.459 Prec@1=13.172 Prec@5=32.462 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:26 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.623 DataTime=0.397 Loss=4.459 Prec@1=13.172 Prec@5=32.462 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:27 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.621 DataTime=0.396 Loss=4.426 Prec@1=13.542 Prec@5=33.106 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:27 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.621 DataTime=0.396 Loss=4.426 Prec@1=13.542 Prec@5=33.106 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:27 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.621 DataTime=0.396 Loss=4.426 Prec@1=13.542 Prec@5=33.106 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:28 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.396 Prec@1=13.905 Prec@5=33.694 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:28 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.396 Prec@1=13.905 Prec@5=33.694 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:28 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.396 Prec@1=13.905 Prec@5=33.694 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:29 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.364 Prec@1=14.303 Prec@5=34.335 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:29 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.364 Prec@1=14.303 Prec@5=34.335 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:29 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.620 DataTime=0.395 Loss=4.364 Prec@1=14.303 Prec@5=34.335 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:30 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.336 Prec@1=14.686 Prec@5=34.929 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:30 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.336 Prec@1=14.686 Prec@5=34.929 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:30 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.336 Prec@1=14.686 Prec@5=34.929 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:31 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.308 Prec@1=15.043 Prec@5=35.484 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:31 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.308 Prec@1=15.043 Prec@5=35.484 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:31 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.619 DataTime=0.394 Loss=4.308 Prec@1=15.043 Prec@5=35.484 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:32 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.394 Loss=4.280 Prec@1=15.409 Prec@5=36.036 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:32 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.394 Loss=4.280 Prec@1=15.409 Prec@5=36.036 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:32 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.394 Loss=4.280 Prec@1=15.409 Prec@5=36.036 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:33 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.254 Prec@1=15.746 Prec@5=36.575 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:33 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.254 Prec@1=15.746 Prec@5=36.575 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:33 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.254 Prec@1=15.746 Prec@5=36.575 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:34 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.228 Prec@1=16.106 Prec@5=37.098 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:34 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.228 Prec@1=16.106 Prec@5=37.098 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:34 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.228 Prec@1=16.106 Prec@5=37.098 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:35 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.204 Prec@1=16.426 Prec@5=37.587 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:35 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.204 Prec@1=16.426 Prec@5=37.587 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:35 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.618 DataTime=0.393 Loss=4.204 Prec@1=16.426 Prec@5=37.587 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:36 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.179 Prec@1=16.769 Prec@5=38.084 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:36 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.179 Prec@1=16.769 Prec@5=38.084 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:36 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.179 Prec@1=16.769 Prec@5=38.084 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:37 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.156 Prec@1=17.068 Prec@5=38.559 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:37 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.156 Prec@1=17.068 Prec@5=38.559 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:37 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.393 Loss=4.156 Prec@1=17.068 Prec@5=38.559 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:38 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.133 Prec@1=17.388 Prec@5=39.028 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=15:38 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.133 Prec@1=17.388 Prec@5=39.028 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:38 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.133 Prec@1=17.388 Prec@5=39.028 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:39 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.111 Prec@1=17.680 Prec@5=39.472 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:39 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.111 Prec@1=17.680 Prec@5=39.472 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:39 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.111 Prec@1=17.680 Prec@5=39.472 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:40 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.089 Prec@1=17.985 Prec@5=39.901 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:40 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.089 Prec@1=17.985 Prec@5=39.901 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:40 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.089 Prec@1=17.985 Prec@5=39.901 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:41 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.067 Prec@1=18.269 Prec@5=40.330 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:41 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.067 Prec@1=18.269 Prec@5=40.330 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:41 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.067 Prec@1=18.269 Prec@5=40.330 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:42 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.047 Prec@1=18.546 Prec@5=40.734 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:42 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.047 Prec@1=18.546 Prec@5=40.734 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:42 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.047 Prec@1=18.546 Prec@5=40.734 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:43 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.026 Prec@1=18.833 Prec@5=41.134 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=15:43 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.026 Prec@1=18.833 Prec@5=41.134 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:43 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.617 DataTime=0.392 Loss=4.026 Prec@1=18.833 Prec@5=41.134 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:44 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=4.007 Prec@1=19.095 Prec@5=41.528 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=4.007 Prec@1=19.095 Prec@5=41.528 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=4.007 Prec@1=19.095 Prec@5=41.528 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:45 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=3.987 Prec@1=19.385 Prec@5=41.923 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=15:45 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=3.987 Prec@1=19.385 Prec@5=41.923 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:45 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=3.987 Prec@1=19.385 Prec@5=41.923 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:45 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=3.987 Prec@1=19.389 Prec@5=41.927 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=15:45 IST=> training   100.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.616 DataTime=0.392 Loss=3.987 Prec@1=19.389 Prec@5=41.927 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=15:45 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> validation 0.00% of 1x98...Epoch=3/150 LR=0.04000 Time=6.134 Loss=3.676 Prec@1=24.023 Prec@5=48.633 rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=6.134 Loss=3.676 Prec@1=24.023 Prec@5=48.633 rate=3593.77 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST** validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=6.134 Loss=3.676 Prec@1=24.023 Prec@5=48.633 rate=3593.77 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST** validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=0.404 Loss=3.556 Prec@1=24.848 Prec@5=50.212 rate=3593.77 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST** validation 100.00% of 1x98...Epoch=3/150 LR=0.04000 Time=0.404 Loss=3.556 Prec@1=24.848 Prec@5=50.212 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=15:46 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:46 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:46 IST=> training   0.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.458 DataTime=5.161 Loss=3.528 Prec@1=24.805 Prec@5=49.023 rate=0 Hz, eta=?, total=0:00:00, wall=15:46 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.458 DataTime=5.161 Loss=3.528 Prec@1=24.805 Prec@5=49.023 rate=5769.67 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.458 DataTime=5.161 Loss=3.528 Prec@1=24.805 Prec@5=49.023 rate=5769.67 Hz, eta=0:00:00, total=0:00:00, wall=15:47 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.659 DataTime=0.433 Loss=3.580 Prec@1=24.836 Prec@5=49.778 rate=5769.67 Hz, eta=0:00:00, total=0:00:00, wall=15:47 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.659 DataTime=0.433 Loss=3.580 Prec@1=24.836 Prec@5=49.778 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=15:47 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.659 DataTime=0.433 Loss=3.580 Prec@1=24.836 Prec@5=49.778 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=15:48 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.637 DataTime=0.411 Loss=3.575 Prec@1=24.922 Prec@5=49.790 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=15:48 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.637 DataTime=0.411 Loss=3.575 Prec@1=24.922 Prec@5=49.790 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=15:48 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.637 DataTime=0.411 Loss=3.575 Prec@1=24.922 Prec@5=49.790 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=15:49 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.629 DataTime=0.404 Loss=3.565 Prec@1=25.070 Prec@5=50.094 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=15:49 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.629 DataTime=0.404 Loss=3.565 Prec@1=25.070 Prec@5=50.094 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=15:49 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.629 DataTime=0.404 Loss=3.565 Prec@1=25.070 Prec@5=50.094 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=15:50 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.626 DataTime=0.400 Loss=3.554 Prec@1=25.256 Prec@5=50.360 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=15:50 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.626 DataTime=0.400 Loss=3.554 Prec@1=25.256 Prec@5=50.360 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:50 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.626 DataTime=0.400 Loss=3.554 Prec@1=25.256 Prec@5=50.360 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:51 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.623 DataTime=0.398 Loss=3.537 Prec@1=25.546 Prec@5=50.667 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=15:51 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.623 DataTime=0.398 Loss=3.537 Prec@1=25.546 Prec@5=50.667 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:51 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.623 DataTime=0.398 Loss=3.537 Prec@1=25.546 Prec@5=50.667 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:52 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.622 DataTime=0.397 Loss=3.524 Prec@1=25.769 Prec@5=50.937 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:52 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.622 DataTime=0.397 Loss=3.524 Prec@1=25.769 Prec@5=50.937 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:52 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.622 DataTime=0.397 Loss=3.524 Prec@1=25.769 Prec@5=50.937 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:53 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.621 DataTime=0.396 Loss=3.506 Prec@1=26.092 Prec@5=51.305 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:53 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.621 DataTime=0.396 Loss=3.506 Prec@1=26.092 Prec@5=51.305 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:53 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.621 DataTime=0.396 Loss=3.506 Prec@1=26.092 Prec@5=51.305 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:54 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.620 DataTime=0.395 Loss=3.492 Prec@1=26.326 Prec@5=51.585 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:54 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.620 DataTime=0.395 Loss=3.492 Prec@1=26.326 Prec@5=51.585 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:54 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.620 DataTime=0.395 Loss=3.492 Prec@1=26.326 Prec@5=51.585 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:55 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.477 Prec@1=26.593 Prec@5=51.876 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:55 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.477 Prec@1=26.593 Prec@5=51.876 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:55 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.477 Prec@1=26.593 Prec@5=51.876 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:56 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.464 Prec@1=26.797 Prec@5=52.129 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:56 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.464 Prec@1=26.797 Prec@5=52.129 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:56 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.394 Loss=3.464 Prec@1=26.797 Prec@5=52.129 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:57 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.393 Loss=3.449 Prec@1=27.033 Prec@5=52.407 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=15:57 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.393 Loss=3.449 Prec@1=27.033 Prec@5=52.407 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:57 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.619 DataTime=0.393 Loss=3.449 Prec@1=27.033 Prec@5=52.407 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:58 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.434 Prec@1=27.262 Prec@5=52.702 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=15:58 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.434 Prec@1=27.262 Prec@5=52.702 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=15:58 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.434 Prec@1=27.262 Prec@5=52.702 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=15:59 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.420 Prec@1=27.479 Prec@5=52.950 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=15:59 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.420 Prec@1=27.479 Prec@5=52.950 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:59 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.420 Prec@1=27.479 Prec@5=52.950 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:00 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.407 Prec@1=27.715 Prec@5=53.215 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:00 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.407 Prec@1=27.715 Prec@5=53.215 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:00 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.618 DataTime=0.393 Loss=3.407 Prec@1=27.715 Prec@5=53.215 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:01 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.393 Prec@1=27.942 Prec@5=53.470 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:01 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.393 Prec@1=27.942 Prec@5=53.470 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:01 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.393 Prec@1=27.942 Prec@5=53.470 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:02 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.381 Prec@1=28.137 Prec@5=53.706 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:02 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.381 Prec@1=28.137 Prec@5=53.706 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:02 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.393 Loss=3.381 Prec@1=28.137 Prec@5=53.706 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:03 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.369 Prec@1=28.324 Prec@5=53.935 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:03 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.369 Prec@1=28.324 Prec@5=53.935 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:03 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.369 Prec@1=28.324 Prec@5=53.935 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:04 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.357 Prec@1=28.520 Prec@5=54.165 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:04 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.357 Prec@1=28.520 Prec@5=54.165 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:04 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.357 Prec@1=28.520 Prec@5=54.165 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:05 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.344 Prec@1=28.736 Prec@5=54.407 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:05 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.344 Prec@1=28.736 Prec@5=54.407 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:05 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.344 Prec@1=28.736 Prec@5=54.407 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:06 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.333 Prec@1=28.905 Prec@5=54.619 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:06 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.333 Prec@1=28.905 Prec@5=54.619 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:06 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.333 Prec@1=28.905 Prec@5=54.619 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:08 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.322 Prec@1=29.099 Prec@5=54.848 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:08 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.322 Prec@1=29.099 Prec@5=54.848 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=16:08 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.617 DataTime=0.392 Loss=3.322 Prec@1=29.099 Prec@5=54.848 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=16:09 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.310 Prec@1=29.285 Prec@5=55.076 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=16:09 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.310 Prec@1=29.285 Prec@5=55.076 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:09 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.310 Prec@1=29.285 Prec@5=55.076 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:10 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.299 Prec@1=29.479 Prec@5=55.295 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:10 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.299 Prec@1=29.479 Prec@5=55.295 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:10 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.299 Prec@1=29.479 Prec@5=55.295 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:11 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.288 Prec@1=29.651 Prec@5=55.492 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:11 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.288 Prec@1=29.651 Prec@5=55.492 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:11 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.288 Prec@1=29.651 Prec@5=55.492 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:12 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.278 Prec@1=29.841 Prec@5=55.690 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:12 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.278 Prec@1=29.841 Prec@5=55.690 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:12 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.278 Prec@1=29.841 Prec@5=55.690 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:12 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.277 Prec@1=29.843 Prec@5=55.693 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:12 IST=> training   100.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.616 DataTime=0.392 Loss=3.277 Prec@1=29.843 Prec@5=55.693 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=16:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> validation 0.00% of 1x98...Epoch=4/150 LR=0.06000 Time=6.538 Loss=2.902 Prec@1=36.133 Prec@5=62.695 rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=6.538 Loss=2.902 Prec@1=36.133 Prec@5=62.695 rate=2529.20 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST** validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=6.538 Loss=2.902 Prec@1=36.133 Prec@5=62.695 rate=2529.20 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST** validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=0.407 Loss=2.877 Prec@1=35.588 Prec@5=63.290 rate=2529.20 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST** validation 100.00% of 1x98...Epoch=4/150 LR=0.06000 Time=0.407 Loss=2.877 Prec@1=35.588 Prec@5=63.290 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=16:12 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> training   0.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=4.918 DataTime=4.603 Loss=2.930 Prec@1=35.352 Prec@5=62.305 rate=0 Hz, eta=?, total=0:00:00, wall=16:12 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=4.918 DataTime=4.603 Loss=2.930 Prec@1=35.352 Prec@5=62.305 rate=6266.22 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=4.918 DataTime=4.603 Loss=2.930 Prec@1=35.352 Prec@5=62.305 rate=6266.22 Hz, eta=0:00:00, total=0:00:00, wall=16:13 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.655 DataTime=0.429 Loss=3.089 Prec@1=32.938 Prec@5=59.302 rate=6266.22 Hz, eta=0:00:00, total=0:00:00, wall=16:13 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.655 DataTime=0.429 Loss=3.089 Prec@1=32.938 Prec@5=59.302 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=16:13 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.655 DataTime=0.429 Loss=3.089 Prec@1=32.938 Prec@5=59.302 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=16:15 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.635 DataTime=0.410 Loss=3.075 Prec@1=32.994 Prec@5=59.473 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=16:15 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.635 DataTime=0.410 Loss=3.075 Prec@1=32.994 Prec@5=59.473 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:15 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.635 DataTime=0.410 Loss=3.075 Prec@1=32.994 Prec@5=59.473 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:16 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.627 DataTime=0.403 Loss=3.068 Prec@1=33.166 Prec@5=59.633 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:16 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.627 DataTime=0.403 Loss=3.068 Prec@1=33.166 Prec@5=59.633 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:16 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.627 DataTime=0.403 Loss=3.068 Prec@1=33.166 Prec@5=59.633 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:17 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.624 DataTime=0.399 Loss=3.063 Prec@1=33.276 Prec@5=59.735 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:17 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.624 DataTime=0.399 Loss=3.063 Prec@1=33.276 Prec@5=59.735 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=16:17 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.624 DataTime=0.399 Loss=3.063 Prec@1=33.276 Prec@5=59.735 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=16:18 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.622 DataTime=0.397 Loss=3.057 Prec@1=33.432 Prec@5=59.903 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=16:18 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.622 DataTime=0.397 Loss=3.057 Prec@1=33.432 Prec@5=59.903 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=16:18 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.622 DataTime=0.397 Loss=3.057 Prec@1=33.432 Prec@5=59.903 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=16:19 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.621 DataTime=0.396 Loss=3.047 Prec@1=33.583 Prec@5=60.091 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=16:19 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.621 DataTime=0.396 Loss=3.047 Prec@1=33.583 Prec@5=60.091 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:19 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.621 DataTime=0.396 Loss=3.047 Prec@1=33.583 Prec@5=60.091 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:20 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.620 DataTime=0.395 Loss=3.037 Prec@1=33.754 Prec@5=60.252 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:20 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.620 DataTime=0.395 Loss=3.037 Prec@1=33.754 Prec@5=60.252 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=16:20 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.620 DataTime=0.395 Loss=3.037 Prec@1=33.754 Prec@5=60.252 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=16:21 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.394 Loss=3.029 Prec@1=33.864 Prec@5=60.426 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=16:21 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.394 Loss=3.029 Prec@1=33.864 Prec@5=60.426 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:21 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.394 Loss=3.029 Prec@1=33.864 Prec@5=60.426 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:22 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.393 Loss=3.019 Prec@1=34.019 Prec@5=60.611 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:22 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.393 Loss=3.019 Prec@1=34.019 Prec@5=60.611 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:22 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.619 DataTime=0.393 Loss=3.019 Prec@1=34.019 Prec@5=60.611 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:23 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.012 Prec@1=34.127 Prec@5=60.771 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:23 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.012 Prec@1=34.127 Prec@5=60.771 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=16:23 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.012 Prec@1=34.127 Prec@5=60.771 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=16:24 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.003 Prec@1=34.264 Prec@5=60.930 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=16:24 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.003 Prec@1=34.264 Prec@5=60.930 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:24 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.393 Loss=3.003 Prec@1=34.264 Prec@5=60.930 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:25 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.392 Loss=2.997 Prec@1=34.382 Prec@5=61.050 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:25 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.392 Loss=2.997 Prec@1=34.382 Prec@5=61.050 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:25 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.618 DataTime=0.392 Loss=2.997 Prec@1=34.382 Prec@5=61.050 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:26 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.990 Prec@1=34.537 Prec@5=61.189 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:26 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.990 Prec@1=34.537 Prec@5=61.189 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:26 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.990 Prec@1=34.537 Prec@5=61.189 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:27 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.981 Prec@1=34.684 Prec@5=61.329 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:27 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.981 Prec@1=34.684 Prec@5=61.329 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:27 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.981 Prec@1=34.684 Prec@5=61.329 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:28 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.973 Prec@1=34.825 Prec@5=61.474 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:28 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.973 Prec@1=34.825 Prec@5=61.474 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=16:28 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.392 Loss=2.973 Prec@1=34.825 Prec@5=61.474 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=16:29 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.965 Prec@1=34.970 Prec@5=61.630 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=16:29 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.965 Prec@1=34.970 Prec@5=61.630 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:29 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.965 Prec@1=34.970 Prec@5=61.630 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:30 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.957 Prec@1=35.099 Prec@5=61.750 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:30 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.957 Prec@1=35.099 Prec@5=61.750 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:30 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.957 Prec@1=35.099 Prec@5=61.750 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:31 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.951 Prec@1=35.218 Prec@5=61.868 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:31 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.951 Prec@1=35.218 Prec@5=61.868 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:31 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.617 DataTime=0.391 Loss=2.951 Prec@1=35.218 Prec@5=61.868 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:32 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.944 Prec@1=35.340 Prec@5=62.002 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=16:32 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.944 Prec@1=35.340 Prec@5=62.002 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:32 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.944 Prec@1=35.340 Prec@5=62.002 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:33 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.936 Prec@1=35.467 Prec@5=62.123 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:33 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.936 Prec@1=35.467 Prec@5=62.123 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:33 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.936 Prec@1=35.467 Prec@5=62.123 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:34 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.929 Prec@1=35.598 Prec@5=62.263 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:34 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.929 Prec@1=35.598 Prec@5=62.263 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:34 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.929 Prec@1=35.598 Prec@5=62.263 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:35 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.923 Prec@1=35.715 Prec@5=62.369 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:35 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.923 Prec@1=35.715 Prec@5=62.369 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:35 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.923 Prec@1=35.715 Prec@5=62.369 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:36 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.917 Prec@1=35.815 Prec@5=62.476 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=16:36 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.917 Prec@1=35.815 Prec@5=62.476 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:36 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.917 Prec@1=35.815 Prec@5=62.476 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:37 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.909 Prec@1=35.941 Prec@5=62.609 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=16:37 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.909 Prec@1=35.941 Prec@5=62.609 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:37 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.909 Prec@1=35.941 Prec@5=62.609 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:38 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.903 Prec@1=36.052 Prec@5=62.717 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:38 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.903 Prec@1=36.052 Prec@5=62.717 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:38 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.903 Prec@1=36.052 Prec@5=62.717 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:38 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.903 Prec@1=36.053 Prec@5=62.720 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=16:38 IST=> training   100.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.616 DataTime=0.391 Loss=2.903 Prec@1=36.053 Prec@5=62.720 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=16:38 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> validation 0.00% of 1x98...Epoch=5/150 LR=0.08000 Time=6.314 Loss=2.658 Prec@1=41.602 Prec@5=65.625 rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=6.314 Loss=2.658 Prec@1=41.602 Prec@5=65.625 rate=6075.41 Hz, eta=0:00:00, total=0:00:00, wall=16:38 IST** validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=6.314 Loss=2.658 Prec@1=41.602 Prec@5=65.625 rate=6075.41 Hz, eta=0:00:00, total=0:00:00, wall=16:39 IST** validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=0.406 Loss=2.604 Prec@1=41.142 Prec@5=68.158 rate=6075.41 Hz, eta=0:00:00, total=0:00:00, wall=16:39 IST** validation 100.00% of 1x98...Epoch=5/150 LR=0.08000 Time=0.406 Loss=2.604 Prec@1=41.142 Prec@5=68.158 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=16:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST=> training   0.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.948 DataTime=4.658 Loss=2.738 Prec@1=36.719 Prec@5=65.820 rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.948 DataTime=4.658 Loss=2.738 Prec@1=36.719 Prec@5=65.820 rate=2458.40 Hz, eta=0:00:01, total=0:00:00, wall=16:39 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.948 DataTime=4.658 Loss=2.738 Prec@1=36.719 Prec@5=65.820 rate=2458.40 Hz, eta=0:00:01, total=0:00:00, wall=16:40 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.655 DataTime=0.431 Loss=2.775 Prec@1=38.448 Prec@5=65.033 rate=2458.40 Hz, eta=0:00:01, total=0:00:00, wall=16:40 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.655 DataTime=0.431 Loss=2.775 Prec@1=38.448 Prec@5=65.033 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=16:40 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.655 DataTime=0.431 Loss=2.775 Prec@1=38.448 Prec@5=65.033 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=16:41 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.636 DataTime=0.411 Loss=2.767 Prec@1=38.466 Prec@5=65.148 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=16:41 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.636 DataTime=0.411 Loss=2.767 Prec@1=38.466 Prec@5=65.148 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=16:41 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.636 DataTime=0.411 Loss=2.767 Prec@1=38.466 Prec@5=65.148 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=16:42 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.629 DataTime=0.403 Loss=2.765 Prec@1=38.436 Prec@5=65.179 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=16:42 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.629 DataTime=0.403 Loss=2.765 Prec@1=38.436 Prec@5=65.179 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=16:42 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.629 DataTime=0.403 Loss=2.765 Prec@1=38.436 Prec@5=65.179 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=16:43 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.625 DataTime=0.400 Loss=2.765 Prec@1=38.415 Prec@5=65.242 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=16:43 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.625 DataTime=0.400 Loss=2.765 Prec@1=38.415 Prec@5=65.242 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=16:43 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.625 DataTime=0.400 Loss=2.765 Prec@1=38.415 Prec@5=65.242 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=16:44 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.623 DataTime=0.398 Loss=2.760 Prec@1=38.524 Prec@5=65.291 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=16:44 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.623 DataTime=0.398 Loss=2.760 Prec@1=38.524 Prec@5=65.291 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:44 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.623 DataTime=0.398 Loss=2.760 Prec@1=38.524 Prec@5=65.291 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:45 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.622 DataTime=0.397 Loss=2.753 Prec@1=38.675 Prec@5=65.461 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:45 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.622 DataTime=0.397 Loss=2.753 Prec@1=38.675 Prec@5=65.461 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:45 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.622 DataTime=0.397 Loss=2.753 Prec@1=38.675 Prec@5=65.461 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:46 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.621 DataTime=0.396 Loss=2.744 Prec@1=38.845 Prec@5=65.630 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:46 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.621 DataTime=0.396 Loss=2.744 Prec@1=38.845 Prec@5=65.630 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=16:46 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.621 DataTime=0.396 Loss=2.744 Prec@1=38.845 Prec@5=65.630 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=16:47 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.620 DataTime=0.395 Loss=2.740 Prec@1=38.918 Prec@5=65.709 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=16:47 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.620 DataTime=0.395 Loss=2.740 Prec@1=38.918 Prec@5=65.709 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:47 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.620 DataTime=0.395 Loss=2.740 Prec@1=38.918 Prec@5=65.709 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:48 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.737 Prec@1=38.996 Prec@5=65.756 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:48 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.737 Prec@1=38.996 Prec@5=65.756 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:48 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.737 Prec@1=38.996 Prec@5=65.756 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:49 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.733 Prec@1=39.091 Prec@5=65.826 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:49 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.733 Prec@1=39.091 Prec@5=65.826 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:49 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.394 Loss=2.733 Prec@1=39.091 Prec@5=65.826 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:50 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.393 Loss=2.728 Prec@1=39.182 Prec@5=65.907 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:50 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.393 Loss=2.728 Prec@1=39.182 Prec@5=65.907 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=16:50 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.619 DataTime=0.393 Loss=2.728 Prec@1=39.182 Prec@5=65.907 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=16:51 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.724 Prec@1=39.247 Prec@5=65.967 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.724 Prec@1=39.247 Prec@5=65.967 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.724 Prec@1=39.247 Prec@5=65.967 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:52 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.718 Prec@1=39.361 Prec@5=66.055 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:52 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.718 Prec@1=39.361 Prec@5=66.055 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:52 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.718 Prec@1=39.361 Prec@5=66.055 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:53 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.711 Prec@1=39.452 Prec@5=66.189 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:53 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.711 Prec@1=39.452 Prec@5=66.189 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:53 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.711 Prec@1=39.452 Prec@5=66.189 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:54 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.707 Prec@1=39.525 Prec@5=66.279 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:54 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.707 Prec@1=39.525 Prec@5=66.279 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=16:54 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.618 DataTime=0.393 Loss=2.707 Prec@1=39.525 Prec@5=66.279 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=16:55 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.702 Prec@1=39.613 Prec@5=66.368 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=16:55 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.702 Prec@1=39.613 Prec@5=66.368 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:55 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.702 Prec@1=39.613 Prec@5=66.368 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:56 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.697 Prec@1=39.695 Prec@5=66.447 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:56 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.697 Prec@1=39.695 Prec@5=66.447 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:56 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.697 Prec@1=39.695 Prec@5=66.447 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:57 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.693 Prec@1=39.759 Prec@5=66.517 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:57 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.693 Prec@1=39.759 Prec@5=66.517 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:57 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.693 Prec@1=39.759 Prec@5=66.517 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:58 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.688 Prec@1=39.853 Prec@5=66.606 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:58 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.688 Prec@1=39.853 Prec@5=66.606 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=16:58 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.688 Prec@1=39.853 Prec@5=66.606 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=16:59 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.683 Prec@1=39.942 Prec@5=66.683 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=16:59 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.683 Prec@1=39.942 Prec@5=66.683 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=16:59 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.683 Prec@1=39.942 Prec@5=66.683 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=17:00 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.678 Prec@1=40.021 Prec@5=66.765 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=17:00 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.678 Prec@1=40.021 Prec@5=66.765 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:00 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.678 Prec@1=40.021 Prec@5=66.765 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:01 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.674 Prec@1=40.106 Prec@5=66.840 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:01 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.674 Prec@1=40.106 Prec@5=66.840 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:01 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.674 Prec@1=40.106 Prec@5=66.840 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:02 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.669 Prec@1=40.187 Prec@5=66.915 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:02 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.669 Prec@1=40.187 Prec@5=66.915 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:02 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.617 DataTime=0.392 Loss=2.669 Prec@1=40.187 Prec@5=66.915 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:03 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.665 Prec@1=40.273 Prec@5=67.001 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:03 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.665 Prec@1=40.273 Prec@5=67.001 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:03 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.665 Prec@1=40.273 Prec@5=67.001 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:04 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.660 Prec@1=40.357 Prec@5=67.084 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:04 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.660 Prec@1=40.357 Prec@5=67.084 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:04 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.660 Prec@1=40.357 Prec@5=67.084 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:04 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.660 Prec@1=40.357 Prec@5=67.083 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:04 IST=> training   100.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.616 DataTime=0.391 Loss=2.660 Prec@1=40.357 Prec@5=67.083 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=17:04 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> validation 0.00% of 1x98...Epoch=6/150 LR=0.10000 Time=7.549 Loss=2.799 Prec@1=38.281 Prec@5=65.039 rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=7.549 Loss=2.799 Prec@1=38.281 Prec@5=65.039 rate=4546.88 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST** validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=7.549 Loss=2.799 Prec@1=38.281 Prec@5=65.039 rate=4546.88 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST** validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=0.415 Loss=2.731 Prec@1=38.624 Prec@5=65.742 rate=4546.88 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST** validation 100.00% of 1x98...Epoch=6/150 LR=0.10000 Time=0.415 Loss=2.731 Prec@1=38.624 Prec@5=65.742 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=17:05 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> training   0.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=5.298 DataTime=4.990 Loss=2.662 Prec@1=39.648 Prec@5=66.797 rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=5.298 DataTime=4.990 Loss=2.662 Prec@1=39.648 Prec@5=66.797 rate=6820.03 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=5.298 DataTime=4.990 Loss=2.662 Prec@1=39.648 Prec@5=66.797 rate=6820.03 Hz, eta=0:00:00, total=0:00:00, wall=17:06 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.659 DataTime=0.431 Loss=2.489 Prec@1=43.473 Prec@5=69.761 rate=6820.03 Hz, eta=0:00:00, total=0:00:00, wall=17:06 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.659 DataTime=0.431 Loss=2.489 Prec@1=43.473 Prec@5=69.761 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:06 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.659 DataTime=0.431 Loss=2.489 Prec@1=43.473 Prec@5=69.761 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:07 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.636 DataTime=0.411 Loss=2.481 Prec@1=43.534 Prec@5=69.893 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:07 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.636 DataTime=0.411 Loss=2.481 Prec@1=43.534 Prec@5=69.893 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=17:07 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.636 DataTime=0.411 Loss=2.481 Prec@1=43.534 Prec@5=69.893 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=17:08 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.628 DataTime=0.403 Loss=2.481 Prec@1=43.506 Prec@5=69.998 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=17:08 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.628 DataTime=0.403 Loss=2.481 Prec@1=43.506 Prec@5=69.998 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:08 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.628 DataTime=0.403 Loss=2.481 Prec@1=43.506 Prec@5=69.998 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:09 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.624 DataTime=0.400 Loss=2.483 Prec@1=43.562 Prec@5=70.039 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:09 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.624 DataTime=0.400 Loss=2.483 Prec@1=43.562 Prec@5=70.039 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:09 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.624 DataTime=0.400 Loss=2.483 Prec@1=43.562 Prec@5=70.039 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:10 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.622 DataTime=0.398 Loss=2.482 Prec@1=43.580 Prec@5=70.098 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:10 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.622 DataTime=0.398 Loss=2.482 Prec@1=43.580 Prec@5=70.098 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=17:10 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.622 DataTime=0.398 Loss=2.482 Prec@1=43.580 Prec@5=70.098 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=17:11 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.621 DataTime=0.396 Loss=2.482 Prec@1=43.567 Prec@5=70.125 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=17:11 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.621 DataTime=0.396 Loss=2.482 Prec@1=43.567 Prec@5=70.125 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:11 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.621 DataTime=0.396 Loss=2.482 Prec@1=43.567 Prec@5=70.125 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:12 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.620 DataTime=0.395 Loss=2.482 Prec@1=43.576 Prec@5=70.119 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:12 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.620 DataTime=0.395 Loss=2.482 Prec@1=43.576 Prec@5=70.119 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:12 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.620 DataTime=0.395 Loss=2.482 Prec@1=43.576 Prec@5=70.119 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:13 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.395 Loss=2.481 Prec@1=43.617 Prec@5=70.147 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:13 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.395 Loss=2.481 Prec@1=43.617 Prec@5=70.147 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:13 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.395 Loss=2.481 Prec@1=43.617 Prec@5=70.147 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:15 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.394 Loss=2.477 Prec@1=43.669 Prec@5=70.197 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:15 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.394 Loss=2.477 Prec@1=43.669 Prec@5=70.197 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:15 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.619 DataTime=0.394 Loss=2.477 Prec@1=43.669 Prec@5=70.197 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:16 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.394 Loss=2.476 Prec@1=43.693 Prec@5=70.236 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:16 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.394 Loss=2.476 Prec@1=43.693 Prec@5=70.236 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:16 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.394 Loss=2.476 Prec@1=43.693 Prec@5=70.236 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:17 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.474 Prec@1=43.740 Prec@5=70.285 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:17 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.474 Prec@1=43.740 Prec@5=70.285 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=17:17 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.474 Prec@1=43.740 Prec@5=70.285 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=17:18 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.471 Prec@1=43.786 Prec@5=70.324 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=17:18 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.471 Prec@1=43.786 Prec@5=70.324 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:18 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.618 DataTime=0.393 Loss=2.471 Prec@1=43.786 Prec@5=70.324 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:19 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.467 Prec@1=43.868 Prec@5=70.397 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:19 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.467 Prec@1=43.868 Prec@5=70.397 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:19 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.467 Prec@1=43.868 Prec@5=70.397 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:20 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.464 Prec@1=43.920 Prec@5=70.466 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:20 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.464 Prec@1=43.920 Prec@5=70.466 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:20 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.393 Loss=2.464 Prec@1=43.920 Prec@5=70.466 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:21 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.461 Prec@1=43.967 Prec@5=70.514 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:21 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.461 Prec@1=43.967 Prec@5=70.514 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:21 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.461 Prec@1=43.967 Prec@5=70.514 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:22 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.458 Prec@1=44.046 Prec@5=70.549 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:22 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.458 Prec@1=44.046 Prec@5=70.549 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:22 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.458 Prec@1=44.046 Prec@5=70.549 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:23 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.455 Prec@1=44.096 Prec@5=70.589 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:23 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.455 Prec@1=44.096 Prec@5=70.589 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:23 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.455 Prec@1=44.096 Prec@5=70.589 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:24 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.453 Prec@1=44.166 Prec@5=70.634 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.453 Prec@1=44.166 Prec@5=70.634 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.453 Prec@1=44.166 Prec@5=70.634 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=17:25 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.450 Prec@1=44.209 Prec@5=70.667 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=17:25 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.450 Prec@1=44.209 Prec@5=70.667 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:25 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.617 DataTime=0.392 Loss=2.450 Prec@1=44.209 Prec@5=70.667 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:26 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.447 Prec@1=44.277 Prec@5=70.718 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:26 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.447 Prec@1=44.277 Prec@5=70.718 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:26 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.447 Prec@1=44.277 Prec@5=70.718 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:27 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.444 Prec@1=44.338 Prec@5=70.768 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:27 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.444 Prec@1=44.338 Prec@5=70.768 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=17:27 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.444 Prec@1=44.338 Prec@5=70.768 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=17:28 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.441 Prec@1=44.403 Prec@5=70.829 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=17:28 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.441 Prec@1=44.403 Prec@5=70.829 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:28 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.441 Prec@1=44.403 Prec@5=70.829 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:29 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.438 Prec@1=44.459 Prec@5=70.864 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:29 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.438 Prec@1=44.459 Prec@5=70.864 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=17:29 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.438 Prec@1=44.459 Prec@5=70.864 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=17:30 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.435 Prec@1=44.528 Prec@5=70.921 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=17:30 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.435 Prec@1=44.528 Prec@5=70.921 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:30 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.392 Loss=2.435 Prec@1=44.528 Prec@5=70.921 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:31 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.391 Loss=2.432 Prec@1=44.584 Prec@5=70.968 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:31 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.391 Loss=2.432 Prec@1=44.584 Prec@5=70.968 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:31 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.391 Loss=2.432 Prec@1=44.584 Prec@5=70.968 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:31 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.391 Loss=2.432 Prec@1=44.584 Prec@5=70.968 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:31 IST=> training   100.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.616 DataTime=0.391 Loss=2.432 Prec@1=44.584 Prec@5=70.968 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=17:31 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:31 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:31 IST=> validation 0.00% of 1x98...Epoch=7/150 LR=0.09961 Time=5.825 Loss=2.373 Prec@1=46.289 Prec@5=72.461 rate=0 Hz, eta=?, total=0:00:00, wall=17:31 IST=> validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=5.825 Loss=2.373 Prec@1=46.289 Prec@5=72.461 rate=4193.86 Hz, eta=0:00:00, total=0:00:00, wall=17:31 IST** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=5.825 Loss=2.373 Prec@1=46.289 Prec@5=72.461 rate=4193.86 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=0.403 Loss=2.277 Prec@1=46.620 Prec@5=73.576 rate=4193.86 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST** validation 100.00% of 1x98...Epoch=7/150 LR=0.09961 Time=0.403 Loss=2.277 Prec@1=46.620 Prec@5=73.576 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=17:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST=> training   0.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=5.404 DataTime=5.132 Loss=2.439 Prec@1=43.555 Prec@5=73.828 rate=0 Hz, eta=?, total=0:00:00, wall=17:32 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=5.404 DataTime=5.132 Loss=2.439 Prec@1=43.555 Prec@5=73.828 rate=5357.33 Hz, eta=0:00:00, total=0:00:00, wall=17:32 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=5.404 DataTime=5.132 Loss=2.439 Prec@1=43.555 Prec@5=73.828 rate=5357.33 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.658 DataTime=0.434 Loss=2.328 Prec@1=46.511 Prec@5=72.602 rate=5357.33 Hz, eta=0:00:00, total=0:00:00, wall=17:33 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.658 DataTime=0.434 Loss=2.328 Prec@1=46.511 Prec@5=72.602 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:33 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.658 DataTime=0.434 Loss=2.328 Prec@1=46.511 Prec@5=72.602 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:34 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.636 DataTime=0.411 Loss=2.321 Prec@1=46.645 Prec@5=72.643 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:34 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.636 DataTime=0.411 Loss=2.321 Prec@1=46.645 Prec@5=72.643 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:34 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.636 DataTime=0.411 Loss=2.321 Prec@1=46.645 Prec@5=72.643 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:35 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.628 DataTime=0.404 Loss=2.319 Prec@1=46.735 Prec@5=72.692 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:35 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.628 DataTime=0.404 Loss=2.319 Prec@1=46.735 Prec@5=72.692 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:35 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.628 DataTime=0.404 Loss=2.319 Prec@1=46.735 Prec@5=72.692 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:36 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.624 DataTime=0.401 Loss=2.319 Prec@1=46.712 Prec@5=72.707 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=17:36 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.624 DataTime=0.401 Loss=2.319 Prec@1=46.712 Prec@5=72.707 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:36 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.624 DataTime=0.401 Loss=2.319 Prec@1=46.712 Prec@5=72.707 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:37 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.622 DataTime=0.398 Loss=2.318 Prec@1=46.709 Prec@5=72.751 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=17:37 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.622 DataTime=0.398 Loss=2.318 Prec@1=46.709 Prec@5=72.751 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=17:37 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.622 DataTime=0.398 Loss=2.318 Prec@1=46.709 Prec@5=72.751 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=17:38 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.621 DataTime=0.397 Loss=2.315 Prec@1=46.763 Prec@5=72.815 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=17:38 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.621 DataTime=0.397 Loss=2.315 Prec@1=46.763 Prec@5=72.815 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:38 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.621 DataTime=0.397 Loss=2.315 Prec@1=46.763 Prec@5=72.815 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:39 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.620 DataTime=0.396 Loss=2.314 Prec@1=46.799 Prec@5=72.850 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:39 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.620 DataTime=0.396 Loss=2.314 Prec@1=46.799 Prec@5=72.850 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:39 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.620 DataTime=0.396 Loss=2.314 Prec@1=46.799 Prec@5=72.850 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:40 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.395 Loss=2.313 Prec@1=46.849 Prec@5=72.879 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:40 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.395 Loss=2.313 Prec@1=46.849 Prec@5=72.879 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:40 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.395 Loss=2.313 Prec@1=46.849 Prec@5=72.879 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:41 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.394 Loss=2.310 Prec@1=46.915 Prec@5=72.925 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:41 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.394 Loss=2.310 Prec@1=46.915 Prec@5=72.925 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:41 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.619 DataTime=0.394 Loss=2.310 Prec@1=46.915 Prec@5=72.925 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:42 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.394 Loss=2.308 Prec@1=46.936 Prec@5=72.958 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:42 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.394 Loss=2.308 Prec@1=46.936 Prec@5=72.958 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:42 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.394 Loss=2.308 Prec@1=46.936 Prec@5=72.958 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:43 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.979 Prec@5=73.004 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:43 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.979 Prec@5=73.004 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:43 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.979 Prec@5=73.004 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:44 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.989 Prec@5=73.014 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:44 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.989 Prec@5=73.014 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:44 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.618 DataTime=0.393 Loss=2.306 Prec@1=46.989 Prec@5=73.014 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:45 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.303 Prec@1=47.033 Prec@5=73.058 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:45 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.303 Prec@1=47.033 Prec@5=73.058 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:45 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.303 Prec@1=47.033 Prec@5=73.058 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:46 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.301 Prec@1=47.059 Prec@5=73.090 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:46 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.301 Prec@1=47.059 Prec@5=73.090 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=17:46 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.393 Loss=2.301 Prec@1=47.059 Prec@5=73.090 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=17:47 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.300 Prec@1=47.082 Prec@5=73.113 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=17:47 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.300 Prec@1=47.082 Prec@5=73.113 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=17:47 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.300 Prec@1=47.082 Prec@5=73.113 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=17:48 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.299 Prec@1=47.113 Prec@5=73.131 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=17:48 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.299 Prec@1=47.113 Prec@5=73.131 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:48 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.299 Prec@1=47.113 Prec@5=73.131 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:49 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.298 Prec@1=47.132 Prec@5=73.139 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:49 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.298 Prec@1=47.132 Prec@5=73.139 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:49 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.617 DataTime=0.392 Loss=2.298 Prec@1=47.132 Prec@5=73.139 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:50 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.297 Prec@1=47.163 Prec@5=73.174 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:50 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.297 Prec@1=47.163 Prec@5=73.174 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:50 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.297 Prec@1=47.163 Prec@5=73.174 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:51 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.295 Prec@1=47.207 Prec@5=73.206 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:51 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.295 Prec@1=47.207 Prec@5=73.206 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=17:51 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.295 Prec@1=47.207 Prec@5=73.206 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=17:52 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.293 Prec@1=47.233 Prec@5=73.237 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=17:52 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.293 Prec@1=47.233 Prec@5=73.237 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:52 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.293 Prec@1=47.233 Prec@5=73.237 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:53 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.291 Prec@1=47.281 Prec@5=73.262 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:53 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.291 Prec@1=47.281 Prec@5=73.262 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:53 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.392 Loss=2.291 Prec@1=47.281 Prec@5=73.262 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:54 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.291 Prec@1=47.308 Prec@5=73.276 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:54 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.291 Prec@1=47.308 Prec@5=73.276 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=17:54 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.291 Prec@1=47.308 Prec@5=73.276 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=17:55 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.289 Prec@1=47.336 Prec@5=73.307 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=17:55 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.289 Prec@1=47.336 Prec@5=73.307 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:55 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.289 Prec@1=47.336 Prec@5=73.307 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:56 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.287 Prec@1=47.367 Prec@5=73.331 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:56 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.287 Prec@1=47.367 Prec@5=73.331 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=17:56 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.616 DataTime=0.391 Loss=2.287 Prec@1=47.367 Prec@5=73.331 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=17:57 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.615 DataTime=0.391 Loss=2.286 Prec@1=47.392 Prec@5=73.351 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.615 DataTime=0.391 Loss=2.286 Prec@1=47.392 Prec@5=73.351 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.615 DataTime=0.391 Loss=2.286 Prec@1=47.392 Prec@5=73.351 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.615 DataTime=0.391 Loss=2.286 Prec@1=47.394 Prec@5=73.353 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=17:57 IST=> training   100.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.615 DataTime=0.391 Loss=2.286 Prec@1=47.394 Prec@5=73.353 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=17:57 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 0.00% of 1x98...Epoch=8/150 LR=0.09946 Time=7.037 Loss=2.071 Prec@1=51.172 Prec@5=79.492 rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=7.037 Loss=2.071 Prec@1=51.172 Prec@5=79.492 rate=6058.03 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=7.037 Loss=2.071 Prec@1=51.172 Prec@5=79.492 rate=6058.03 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=0.410 Loss=2.171 Prec@1=49.016 Prec@5=75.290 rate=6058.03 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST** validation 100.00% of 1x98...Epoch=8/150 LR=0.09946 Time=0.410 Loss=2.171 Prec@1=49.016 Prec@5=75.290 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=17:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.513 DataTime=5.219 Loss=2.271 Prec@1=48.047 Prec@5=73.047 rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.513 DataTime=5.219 Loss=2.271 Prec@1=48.047 Prec@5=73.047 rate=7141.43 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.513 DataTime=5.219 Loss=2.271 Prec@1=48.047 Prec@5=73.047 rate=7141.43 Hz, eta=0:00:00, total=0:00:00, wall=17:59 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.661 DataTime=0.436 Loss=2.182 Prec@1=49.134 Prec@5=74.917 rate=7141.43 Hz, eta=0:00:00, total=0:00:00, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.661 DataTime=0.436 Loss=2.182 Prec@1=49.134 Prec@5=74.917 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.661 DataTime=0.436 Loss=2.182 Prec@1=49.134 Prec@5=74.917 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=18:00 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.637 DataTime=0.413 Loss=2.175 Prec@1=49.325 Prec@5=75.058 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=18:00 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.637 DataTime=0.413 Loss=2.175 Prec@1=49.325 Prec@5=75.058 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=18:00 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.637 DataTime=0.413 Loss=2.175 Prec@1=49.325 Prec@5=75.058 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=18:01 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.629 DataTime=0.405 Loss=2.181 Prec@1=49.299 Prec@5=75.037 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=18:01 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.629 DataTime=0.405 Loss=2.181 Prec@1=49.299 Prec@5=75.037 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=18:01 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.629 DataTime=0.405 Loss=2.181 Prec@1=49.299 Prec@5=75.037 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=18:02 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.625 DataTime=0.401 Loss=2.182 Prec@1=49.355 Prec@5=75.042 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=18:02 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.625 DataTime=0.401 Loss=2.182 Prec@1=49.355 Prec@5=75.042 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:02 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.625 DataTime=0.401 Loss=2.182 Prec@1=49.355 Prec@5=75.042 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:03 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.623 DataTime=0.399 Loss=2.186 Prec@1=49.310 Prec@5=75.012 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:03 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.623 DataTime=0.399 Loss=2.186 Prec@1=49.310 Prec@5=75.012 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:03 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.623 DataTime=0.399 Loss=2.186 Prec@1=49.310 Prec@5=75.012 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:04 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.622 DataTime=0.398 Loss=2.188 Prec@1=49.264 Prec@5=74.968 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:04 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.622 DataTime=0.398 Loss=2.188 Prec@1=49.264 Prec@5=74.968 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=18:04 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.622 DataTime=0.398 Loss=2.188 Prec@1=49.264 Prec@5=74.968 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=18:05 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.621 DataTime=0.396 Loss=2.189 Prec@1=49.273 Prec@5=74.961 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=18:05 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.621 DataTime=0.396 Loss=2.189 Prec@1=49.273 Prec@5=74.961 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:05 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.621 DataTime=0.396 Loss=2.189 Prec@1=49.273 Prec@5=74.961 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:06 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.620 DataTime=0.396 Loss=2.189 Prec@1=49.286 Prec@5=74.951 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:06 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.620 DataTime=0.396 Loss=2.189 Prec@1=49.286 Prec@5=74.951 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:06 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.620 DataTime=0.396 Loss=2.189 Prec@1=49.286 Prec@5=74.951 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:07 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.619 DataTime=0.395 Loss=2.189 Prec@1=49.284 Prec@5=74.960 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:07 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.619 DataTime=0.395 Loss=2.189 Prec@1=49.284 Prec@5=74.960 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=18:07 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.619 DataTime=0.395 Loss=2.189 Prec@1=49.284 Prec@5=74.960 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=18:08 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.958 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=18:08 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.958 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:08 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.958 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:09 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.189 Prec@1=49.297 Prec@5=74.954 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:09 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.189 Prec@1=49.297 Prec@5=74.954 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:09 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.189 Prec@1=49.297 Prec@5=74.954 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:10 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.961 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:10 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.961 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:10 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.394 Loss=2.188 Prec@1=49.296 Prec@5=74.961 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:11 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.393 Loss=2.187 Prec@1=49.293 Prec@5=74.977 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:11 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.393 Loss=2.187 Prec@1=49.293 Prec@5=74.977 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=18:11 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.618 DataTime=0.393 Loss=2.187 Prec@1=49.293 Prec@5=74.977 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=18:12 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.187 Prec@1=49.303 Prec@5=74.997 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=18:12 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.187 Prec@1=49.303 Prec@5=74.997 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:12 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.187 Prec@1=49.303 Prec@5=74.997 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:13 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.186 Prec@1=49.320 Prec@5=75.026 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:13 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.186 Prec@1=49.320 Prec@5=75.026 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:13 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.186 Prec@1=49.320 Prec@5=75.026 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:15 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.185 Prec@1=49.341 Prec@5=75.045 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:15 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.185 Prec@1=49.341 Prec@5=75.045 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:15 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.185 Prec@1=49.341 Prec@5=75.045 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:16 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.184 Prec@1=49.361 Prec@5=75.053 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:16 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.184 Prec@1=49.361 Prec@5=75.053 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:16 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.393 Loss=2.184 Prec@1=49.361 Prec@5=75.053 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:17 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.392 Loss=2.184 Prec@1=49.376 Prec@5=75.066 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:17 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.392 Loss=2.184 Prec@1=49.376 Prec@5=75.066 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:17 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.617 DataTime=0.392 Loss=2.184 Prec@1=49.376 Prec@5=75.066 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:18 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.183 Prec@1=49.390 Prec@5=75.071 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:18 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.183 Prec@1=49.390 Prec@5=75.071 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:18 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.183 Prec@1=49.390 Prec@5=75.071 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:19 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.395 Prec@5=75.085 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:19 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.395 Prec@5=75.085 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:19 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.395 Prec@5=75.085 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:20 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.411 Prec@5=75.090 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:20 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.411 Prec@5=75.090 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:20 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.182 Prec@1=49.411 Prec@5=75.090 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:21 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.181 Prec@1=49.434 Prec@5=75.101 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:21 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.181 Prec@1=49.434 Prec@5=75.101 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:21 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.181 Prec@1=49.434 Prec@5=75.101 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:22 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.179 Prec@1=49.477 Prec@5=75.132 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:22 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.179 Prec@1=49.477 Prec@5=75.132 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:22 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.179 Prec@1=49.477 Prec@5=75.132 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:23 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.178 Prec@1=49.497 Prec@5=75.144 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:23 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.178 Prec@1=49.497 Prec@5=75.144 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:23 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.178 Prec@1=49.497 Prec@5=75.144 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:24 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.177 Prec@1=49.517 Prec@5=75.155 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:24 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.177 Prec@1=49.517 Prec@5=75.155 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:24 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.177 Prec@1=49.517 Prec@5=75.155 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:24 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.177 Prec@1=49.517 Prec@5=75.154 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:24 IST=> training   100.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.616 DataTime=0.392 Loss=2.177 Prec@1=49.517 Prec@5=75.154 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=18:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:24 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:24 IST=> validation 0.00% of 1x98...Epoch=9/150 LR=0.09930 Time=6.023 Loss=2.075 Prec@1=50.391 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=18:24 IST=> validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.023 Loss=2.075 Prec@1=50.391 Prec@5=76.367 rate=5312.45 Hz, eta=0:00:00, total=0:00:00, wall=18:24 IST** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.023 Loss=2.075 Prec@1=50.391 Prec@5=76.367 rate=5312.45 Hz, eta=0:00:00, total=0:00:00, wall=18:24 IST** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=0.400 Loss=2.127 Prec@1=50.068 Prec@5=76.140 rate=5312.45 Hz, eta=0:00:00, total=0:00:00, wall=18:24 IST** validation 100.00% of 1x98...Epoch=9/150 LR=0.09930 Time=0.400 Loss=2.127 Prec@1=50.068 Prec@5=76.140 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=18:24 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:25 IST=> training   0.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=5.340 DataTime=4.933 Loss=2.266 Prec@1=49.219 Prec@5=75.586 rate=0 Hz, eta=?, total=0:00:00, wall=18:25 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=5.340 DataTime=4.933 Loss=2.266 Prec@1=49.219 Prec@5=75.586 rate=6505.71 Hz, eta=0:00:00, total=0:00:00, wall=18:25 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=5.340 DataTime=4.933 Loss=2.266 Prec@1=49.219 Prec@5=75.586 rate=6505.71 Hz, eta=0:00:00, total=0:00:00, wall=18:26 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.658 DataTime=0.434 Loss=2.101 Prec@1=50.936 Prec@5=76.377 rate=6505.71 Hz, eta=0:00:00, total=0:00:00, wall=18:26 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.658 DataTime=0.434 Loss=2.101 Prec@1=50.936 Prec@5=76.377 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:26 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.658 DataTime=0.434 Loss=2.101 Prec@1=50.936 Prec@5=76.377 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:27 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.638 DataTime=0.413 Loss=2.093 Prec@1=50.950 Prec@5=76.636 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:27 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.638 DataTime=0.413 Loss=2.093 Prec@1=50.950 Prec@5=76.636 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=18:27 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.638 DataTime=0.413 Loss=2.093 Prec@1=50.950 Prec@5=76.636 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=18:28 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.630 DataTime=0.406 Loss=2.095 Prec@1=51.036 Prec@5=76.575 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=18:28 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.630 DataTime=0.406 Loss=2.095 Prec@1=51.036 Prec@5=76.575 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=18:28 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.630 DataTime=0.406 Loss=2.095 Prec@1=51.036 Prec@5=76.575 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=18:29 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.626 DataTime=0.402 Loss=2.097 Prec@1=50.988 Prec@5=76.506 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=18:29 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.626 DataTime=0.402 Loss=2.097 Prec@1=50.988 Prec@5=76.506 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=18:29 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.626 DataTime=0.402 Loss=2.097 Prec@1=50.988 Prec@5=76.506 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=18:30 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.624 DataTime=0.400 Loss=2.100 Prec@1=50.950 Prec@5=76.455 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=18:30 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.624 DataTime=0.400 Loss=2.100 Prec@1=50.950 Prec@5=76.455 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=18:30 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.624 DataTime=0.400 Loss=2.100 Prec@1=50.950 Prec@5=76.455 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=18:31 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.623 DataTime=0.399 Loss=2.101 Prec@1=50.933 Prec@5=76.436 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=18:31 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.623 DataTime=0.399 Loss=2.101 Prec@1=50.933 Prec@5=76.436 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=18:31 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.623 DataTime=0.399 Loss=2.101 Prec@1=50.933 Prec@5=76.436 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=18:32 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.621 DataTime=0.397 Loss=2.102 Prec@1=50.956 Prec@5=76.413 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=18:32 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.621 DataTime=0.397 Loss=2.102 Prec@1=50.956 Prec@5=76.413 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:32 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.621 DataTime=0.397 Loss=2.102 Prec@1=50.956 Prec@5=76.413 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:33 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.620 DataTime=0.396 Loss=2.100 Prec@1=50.982 Prec@5=76.420 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:33 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.620 DataTime=0.396 Loss=2.100 Prec@1=50.982 Prec@5=76.420 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:33 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.620 DataTime=0.396 Loss=2.100 Prec@1=50.982 Prec@5=76.420 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:34 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.101 Prec@1=50.934 Prec@5=76.412 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:34 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.101 Prec@1=50.934 Prec@5=76.412 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:34 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.101 Prec@1=50.934 Prec@5=76.412 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:35 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.100 Prec@1=50.966 Prec@5=76.417 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:35 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.100 Prec@1=50.966 Prec@5=76.417 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:35 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.395 Loss=2.100 Prec@1=50.966 Prec@5=76.417 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:36 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.394 Loss=2.100 Prec@1=50.980 Prec@5=76.427 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:36 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.394 Loss=2.100 Prec@1=50.980 Prec@5=76.427 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:36 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.619 DataTime=0.394 Loss=2.100 Prec@1=50.980 Prec@5=76.427 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:37 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.971 Prec@5=76.437 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:37 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.971 Prec@5=76.437 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:37 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.971 Prec@5=76.437 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:38 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.979 Prec@5=76.426 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:38 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.979 Prec@5=76.426 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:38 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.618 DataTime=0.394 Loss=2.100 Prec@1=50.979 Prec@5=76.426 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:39 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.966 Prec@5=76.412 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:39 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.966 Prec@5=76.412 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:39 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.966 Prec@5=76.412 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:40 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.979 Prec@5=76.412 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:40 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.979 Prec@5=76.412 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:40 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.979 Prec@5=76.412 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:41 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.985 Prec@5=76.409 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:41 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.985 Prec@5=76.409 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:41 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.100 Prec@1=50.985 Prec@5=76.409 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:42 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.976 Prec@5=76.394 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:42 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.976 Prec@5=76.394 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:42 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.976 Prec@5=76.394 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:43 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.983 Prec@5=76.388 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=18:43 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.983 Prec@5=76.388 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:43 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.983 Prec@5=76.388 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:44 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.988 Prec@5=76.389 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:44 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.988 Prec@5=76.389 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:44 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.617 DataTime=0.393 Loss=2.101 Prec@1=50.988 Prec@5=76.389 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:45 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=50.999 Prec@5=76.396 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:45 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=50.999 Prec@5=76.396 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:45 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=50.999 Prec@5=76.396 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:46 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.021 Prec@5=76.406 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:46 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.021 Prec@5=76.406 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:46 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.021 Prec@5=76.406 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:47 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=51.023 Prec@5=76.395 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:47 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=51.023 Prec@5=76.395 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:47 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.100 Prec@1=51.023 Prec@5=76.395 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:48 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.045 Prec@5=76.405 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:48 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.045 Prec@5=76.405 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=18:48 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.099 Prec@1=51.045 Prec@5=76.405 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=18:49 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.098 Prec@1=51.068 Prec@5=76.424 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=18:49 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.098 Prec@1=51.068 Prec@5=76.424 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:49 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.098 Prec@1=51.068 Prec@5=76.424 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:50 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.097 Prec@1=51.092 Prec@5=76.438 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:50 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.097 Prec@1=51.092 Prec@5=76.438 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=18:50 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.097 Prec@1=51.092 Prec@5=76.438 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=18:50 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.097 Prec@1=51.090 Prec@5=76.438 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=18:50 IST=> training   100.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.616 DataTime=0.392 Loss=2.097 Prec@1=51.090 Prec@5=76.438 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=18:50 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> validation 0.00% of 1x98...Epoch=10/150 LR=0.09911 Time=7.494 Loss=2.073 Prec@1=50.195 Prec@5=79.297 rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=7.494 Loss=2.073 Prec@1=50.195 Prec@5=79.297 rate=4075.15 Hz, eta=0:00:00, total=0:00:00, wall=18:50 IST** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=7.494 Loss=2.073 Prec@1=50.195 Prec@5=79.297 rate=4075.15 Hz, eta=0:00:00, total=0:00:00, wall=18:51 IST** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=0.404 Loss=2.102 Prec@1=50.582 Prec@5=76.612 rate=4075.15 Hz, eta=0:00:00, total=0:00:00, wall=18:51 IST** validation 100.00% of 1x98...Epoch=10/150 LR=0.09911 Time=0.404 Loss=2.102 Prec@1=50.582 Prec@5=76.612 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=18:51 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:51 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:51 IST=> training   0.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.509 DataTime=5.092 Loss=1.984 Prec@1=53.711 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=18:51 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.509 DataTime=5.092 Loss=1.984 Prec@1=53.711 Prec@5=76.953 rate=8004.93 Hz, eta=0:00:00, total=0:00:00, wall=18:51 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.509 DataTime=5.092 Loss=1.984 Prec@1=53.711 Prec@5=76.953 rate=8004.93 Hz, eta=0:00:00, total=0:00:00, wall=18:52 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.657 DataTime=0.430 Loss=2.019 Prec@1=52.680 Prec@5=77.632 rate=8004.93 Hz, eta=0:00:00, total=0:00:00, wall=18:52 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.657 DataTime=0.430 Loss=2.019 Prec@1=52.680 Prec@5=77.632 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=18:52 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.657 DataTime=0.430 Loss=2.019 Prec@1=52.680 Prec@5=77.632 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=18:53 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.636 DataTime=0.410 Loss=2.022 Prec@1=52.551 Prec@5=77.534 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=18:53 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.636 DataTime=0.410 Loss=2.022 Prec@1=52.551 Prec@5=77.534 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:53 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.636 DataTime=0.410 Loss=2.022 Prec@1=52.551 Prec@5=77.534 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:54 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.629 DataTime=0.403 Loss=2.029 Prec@1=52.354 Prec@5=77.485 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:54 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.629 DataTime=0.403 Loss=2.029 Prec@1=52.354 Prec@5=77.485 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:54 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.629 DataTime=0.403 Loss=2.029 Prec@1=52.354 Prec@5=77.485 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:55 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.625 DataTime=0.400 Loss=2.025 Prec@1=52.457 Prec@5=77.547 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:55 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.625 DataTime=0.400 Loss=2.025 Prec@1=52.457 Prec@5=77.547 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:55 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.625 DataTime=0.400 Loss=2.025 Prec@1=52.457 Prec@5=77.547 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:56 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.623 DataTime=0.398 Loss=2.023 Prec@1=52.470 Prec@5=77.583 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=18:56 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.623 DataTime=0.398 Loss=2.023 Prec@1=52.470 Prec@5=77.583 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:56 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.623 DataTime=0.398 Loss=2.023 Prec@1=52.470 Prec@5=77.583 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:57 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.622 DataTime=0.397 Loss=2.026 Prec@1=52.458 Prec@5=77.546 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:57 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.622 DataTime=0.397 Loss=2.026 Prec@1=52.458 Prec@5=77.546 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:57 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.622 DataTime=0.397 Loss=2.026 Prec@1=52.458 Prec@5=77.546 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:58 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.621 DataTime=0.396 Loss=2.029 Prec@1=52.376 Prec@5=77.499 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:58 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.621 DataTime=0.396 Loss=2.029 Prec@1=52.376 Prec@5=77.499 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:58 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.621 DataTime=0.396 Loss=2.029 Prec@1=52.376 Prec@5=77.499 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:59 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.620 DataTime=0.395 Loss=2.027 Prec@1=52.383 Prec@5=77.529 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:59 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.620 DataTime=0.395 Loss=2.027 Prec@1=52.383 Prec@5=77.529 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=18:59 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.620 DataTime=0.395 Loss=2.027 Prec@1=52.383 Prec@5=77.529 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:00 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.619 DataTime=0.395 Loss=2.029 Prec@1=52.344 Prec@5=77.509 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:00 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.619 DataTime=0.395 Loss=2.029 Prec@1=52.344 Prec@5=77.509 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=19:00 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.619 DataTime=0.395 Loss=2.029 Prec@1=52.344 Prec@5=77.509 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=19:01 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.364 Prec@5=77.503 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=19:01 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.364 Prec@5=77.503 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:01 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.364 Prec@5=77.503 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:02 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.359 Prec@5=77.520 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:02 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.359 Prec@5=77.520 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:02 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.618 DataTime=0.394 Loss=2.028 Prec@1=52.359 Prec@5=77.520 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:03 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.369 Prec@5=77.515 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:03 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.369 Prec@5=77.515 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=19:03 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.369 Prec@5=77.515 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=19:04 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.352 Prec@5=77.471 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=19:04 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.352 Prec@5=77.471 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:04 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.352 Prec@5=77.471 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:05 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.363 Prec@5=77.476 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:05 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.363 Prec@5=77.476 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:05 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.363 Prec@5=77.476 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:06 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.364 Prec@5=77.487 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.364 Prec@5=77.487 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.029 Prec@1=52.364 Prec@5=77.487 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:07 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.336 Prec@5=77.466 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:07 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.336 Prec@5=77.466 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=19:07 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.393 Loss=2.030 Prec@1=52.336 Prec@5=77.466 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=19:08 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.392 Loss=2.031 Prec@1=52.330 Prec@5=77.452 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=19:08 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.392 Loss=2.031 Prec@1=52.330 Prec@5=77.452 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=19:08 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.617 DataTime=0.392 Loss=2.031 Prec@1=52.330 Prec@5=77.452 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=19:09 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.337 Prec@5=77.461 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=19:09 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.337 Prec@5=77.461 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=19:09 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.337 Prec@5=77.461 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=19:10 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.032 Prec@1=52.334 Prec@5=77.441 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=19:10 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.032 Prec@1=52.334 Prec@5=77.441 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:10 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.032 Prec@1=52.334 Prec@5=77.441 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:11 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.356 Prec@5=77.455 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:11 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.356 Prec@5=77.455 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=19:11 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.356 Prec@5=77.455 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=19:12 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.358 Prec@5=77.453 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=19:12 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.358 Prec@5=77.453 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=19:12 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.358 Prec@5=77.453 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=19:13 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.376 Prec@5=77.451 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=19:13 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.376 Prec@5=77.451 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:13 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.376 Prec@5=77.451 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:15 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.030 Prec@1=52.378 Prec@5=77.455 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:15 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.030 Prec@1=52.378 Prec@5=77.455 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=19:15 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.030 Prec@1=52.378 Prec@5=77.455 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=19:16 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.372 Prec@5=77.461 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=19:16 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.372 Prec@5=77.461 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:16 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.616 DataTime=0.392 Loss=2.031 Prec@1=52.372 Prec@5=77.461 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:17 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.615 DataTime=0.391 Loss=2.030 Prec@1=52.381 Prec@5=77.483 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:17 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.615 DataTime=0.391 Loss=2.030 Prec@1=52.381 Prec@5=77.483 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:17 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.615 DataTime=0.391 Loss=2.030 Prec@1=52.381 Prec@5=77.483 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:17 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.615 DataTime=0.391 Loss=2.030 Prec@1=52.382 Prec@5=77.485 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:17 IST=> training   100.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.615 DataTime=0.391 Loss=2.030 Prec@1=52.382 Prec@5=77.485 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=19:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> validation 0.00% of 1x98...Epoch=11/150 LR=0.09891 Time=6.238 Loss=1.979 Prec@1=51.367 Prec@5=78.516 rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.238 Loss=1.979 Prec@1=51.367 Prec@5=78.516 rate=4739.90 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.238 Loss=1.979 Prec@1=51.367 Prec@5=78.516 rate=4739.90 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=0.406 Loss=2.020 Prec@1=52.398 Prec@5=77.752 rate=4739.90 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST** validation 100.00% of 1x98...Epoch=11/150 LR=0.09891 Time=0.406 Loss=2.020 Prec@1=52.398 Prec@5=77.752 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=19:17 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> training   0.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.926 DataTime=4.625 Loss=1.800 Prec@1=56.055 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=19:17 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.926 DataTime=4.625 Loss=1.800 Prec@1=56.055 Prec@5=81.250 rate=6836.58 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=4.926 DataTime=4.625 Loss=1.800 Prec@1=56.055 Prec@5=81.250 rate=6836.58 Hz, eta=0:00:00, total=0:00:00, wall=19:18 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.655 DataTime=0.429 Loss=1.944 Prec@1=54.328 Prec@5=78.784 rate=6836.58 Hz, eta=0:00:00, total=0:00:00, wall=19:18 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.655 DataTime=0.429 Loss=1.944 Prec@1=54.328 Prec@5=78.784 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:18 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.655 DataTime=0.429 Loss=1.944 Prec@1=54.328 Prec@5=78.784 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:19 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.633 DataTime=0.409 Loss=1.956 Prec@1=54.002 Prec@5=78.674 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:19 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.633 DataTime=0.409 Loss=1.956 Prec@1=54.002 Prec@5=78.674 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=19:19 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.633 DataTime=0.409 Loss=1.956 Prec@1=54.002 Prec@5=78.674 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=19:20 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.626 DataTime=0.402 Loss=1.962 Prec@1=53.797 Prec@5=78.584 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=19:20 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.626 DataTime=0.402 Loss=1.962 Prec@1=53.797 Prec@5=78.584 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=19:20 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.626 DataTime=0.402 Loss=1.962 Prec@1=53.797 Prec@5=78.584 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=19:21 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.623 DataTime=0.399 Loss=1.964 Prec@1=53.736 Prec@5=78.533 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=19:21 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.623 DataTime=0.399 Loss=1.964 Prec@1=53.736 Prec@5=78.533 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=19:21 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.623 DataTime=0.399 Loss=1.964 Prec@1=53.736 Prec@5=78.533 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=19:22 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.621 DataTime=0.397 Loss=1.967 Prec@1=53.636 Prec@5=78.479 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=19:22 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.621 DataTime=0.397 Loss=1.967 Prec@1=53.636 Prec@5=78.479 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:22 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.621 DataTime=0.397 Loss=1.967 Prec@1=53.636 Prec@5=78.479 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:24 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.396 Loss=1.970 Prec@1=53.590 Prec@5=78.403 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:24 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.396 Loss=1.970 Prec@1=53.590 Prec@5=78.403 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=19:24 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.396 Loss=1.970 Prec@1=53.590 Prec@5=78.403 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=19:25 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.395 Loss=1.972 Prec@1=53.516 Prec@5=78.381 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=19:25 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.395 Loss=1.972 Prec@1=53.516 Prec@5=78.381 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:25 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.620 DataTime=0.395 Loss=1.972 Prec@1=53.516 Prec@5=78.381 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:26 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.395 Loss=1.974 Prec@1=53.497 Prec@5=78.370 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:26 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.395 Loss=1.974 Prec@1=53.497 Prec@5=78.370 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:26 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.395 Loss=1.974 Prec@1=53.497 Prec@5=78.370 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:27 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.394 Loss=1.974 Prec@1=53.512 Prec@5=78.355 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:27 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.394 Loss=1.974 Prec@1=53.512 Prec@5=78.355 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:27 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.619 DataTime=0.394 Loss=1.974 Prec@1=53.512 Prec@5=78.355 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:28 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.543 Prec@5=78.364 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:28 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.543 Prec@5=78.364 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=19:28 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.543 Prec@5=78.364 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=19:29 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.546 Prec@5=78.358 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=19:29 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.546 Prec@5=78.358 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:29 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.546 Prec@5=78.358 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:30 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.517 Prec@5=78.342 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:30 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.517 Prec@5=78.342 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:30 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.618 DataTime=0.393 Loss=1.973 Prec@1=53.517 Prec@5=78.342 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:31 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.525 Prec@5=78.335 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:31 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.525 Prec@5=78.335 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:31 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.525 Prec@5=78.335 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:32 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.520 Prec@5=78.355 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:32 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.520 Prec@5=78.355 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=19:32 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.520 Prec@5=78.355 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=19:33 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.529 Prec@5=78.350 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=19:33 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.529 Prec@5=78.350 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=19:33 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.529 Prec@5=78.350 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=19:34 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.972 Prec@1=53.566 Prec@5=78.378 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=19:34 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.972 Prec@1=53.566 Prec@5=78.378 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:34 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.972 Prec@1=53.566 Prec@5=78.378 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:35 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.552 Prec@5=78.368 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:35 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.552 Prec@5=78.368 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:35 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.973 Prec@1=53.552 Prec@5=78.368 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:36 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.360 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:36 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.360 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:36 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.360 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:37 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.975 Prec@1=53.524 Prec@5=78.347 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:37 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.975 Prec@1=53.524 Prec@5=78.347 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:37 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.617 DataTime=0.392 Loss=1.975 Prec@1=53.524 Prec@5=78.347 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:38 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.533 Prec@5=78.361 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.533 Prec@5=78.361 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.533 Prec@5=78.361 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:39 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.545 Prec@5=78.377 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:39 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.545 Prec@5=78.377 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:39 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.545 Prec@5=78.377 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:40 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.385 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:40 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.385 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:40 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.385 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:41 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.563 Prec@5=78.386 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:41 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.563 Prec@5=78.386 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=19:41 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.563 Prec@5=78.386 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=19:42 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.375 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=19:42 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.375 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=19:42 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.973 Prec@1=53.553 Prec@5=78.375 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=19:43 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.367 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=19:43 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.367 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:43 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.367 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:43 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.367 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:43 IST=> training   100.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.616 DataTime=0.392 Loss=1.974 Prec@1=53.542 Prec@5=78.367 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=19:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST=> validation 0.00% of 1x98...Epoch=12/150 LR=0.09868 Time=6.644 Loss=1.944 Prec@1=51.953 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST=> validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=6.644 Loss=1.944 Prec@1=51.953 Prec@5=77.344 rate=5533.51 Hz, eta=0:00:00, total=0:00:00, wall=19:43 IST** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=6.644 Loss=1.944 Prec@1=51.953 Prec@5=77.344 rate=5533.51 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=0.402 Loss=1.966 Prec@1=52.982 Prec@5=78.662 rate=5533.51 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST** validation 100.00% of 1x98...Epoch=12/150 LR=0.09868 Time=0.402 Loss=1.966 Prec@1=52.982 Prec@5=78.662 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=19:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:44 IST=> training   0.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.458 DataTime=5.161 Loss=1.682 Prec@1=58.594 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=19:44 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.458 DataTime=5.161 Loss=1.682 Prec@1=58.594 Prec@5=82.617 rate=4138.49 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=5.458 DataTime=5.161 Loss=1.682 Prec@1=58.594 Prec@5=82.617 rate=4138.49 Hz, eta=0:00:00, total=0:00:00, wall=19:45 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.660 DataTime=0.434 Loss=1.906 Prec@1=54.585 Prec@5=79.316 rate=4138.49 Hz, eta=0:00:00, total=0:00:00, wall=19:45 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.660 DataTime=0.434 Loss=1.906 Prec@1=54.585 Prec@5=79.316 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=19:45 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.660 DataTime=0.434 Loss=1.906 Prec@1=54.585 Prec@5=79.316 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=19:46 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.636 DataTime=0.411 Loss=1.905 Prec@1=54.735 Prec@5=79.337 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=19:46 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.636 DataTime=0.411 Loss=1.905 Prec@1=54.735 Prec@5=79.337 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=19:46 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.636 DataTime=0.411 Loss=1.905 Prec@1=54.735 Prec@5=79.337 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=19:47 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.627 DataTime=0.403 Loss=1.907 Prec@1=54.647 Prec@5=79.392 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=19:47 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.627 DataTime=0.403 Loss=1.907 Prec@1=54.647 Prec@5=79.392 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=19:47 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.627 DataTime=0.403 Loss=1.907 Prec@1=54.647 Prec@5=79.392 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=19:48 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.623 DataTime=0.400 Loss=1.910 Prec@1=54.639 Prec@5=79.327 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=19:48 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.623 DataTime=0.400 Loss=1.910 Prec@1=54.639 Prec@5=79.327 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=19:48 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.623 DataTime=0.400 Loss=1.910 Prec@1=54.639 Prec@5=79.327 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=19:49 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.622 DataTime=0.398 Loss=1.914 Prec@1=54.576 Prec@5=79.270 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=19:49 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.622 DataTime=0.398 Loss=1.914 Prec@1=54.576 Prec@5=79.270 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=19:49 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.622 DataTime=0.398 Loss=1.914 Prec@1=54.576 Prec@5=79.270 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=19:50 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.917 Prec@1=54.481 Prec@5=79.195 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=19:50 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.917 Prec@1=54.481 Prec@5=79.195 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=19:50 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.917 Prec@1=54.481 Prec@5=79.195 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=19:51 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.921 Prec@1=54.481 Prec@5=79.131 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=19:51 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.921 Prec@1=54.481 Prec@5=79.131 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=19:51 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.620 DataTime=0.396 Loss=1.921 Prec@1=54.481 Prec@5=79.131 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=19:52 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.619 DataTime=0.395 Loss=1.924 Prec@1=54.440 Prec@5=79.066 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=19:52 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.619 DataTime=0.395 Loss=1.924 Prec@1=54.440 Prec@5=79.066 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:52 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.619 DataTime=0.395 Loss=1.924 Prec@1=54.440 Prec@5=79.066 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:53 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.392 Prec@5=79.052 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:53 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.392 Prec@5=79.052 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=19:53 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.392 Prec@5=79.052 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=19:54 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.927 Prec@1=54.375 Prec@5=79.050 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=19:54 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.927 Prec@1=54.375 Prec@5=79.050 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:54 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.927 Prec@1=54.375 Prec@5=79.050 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:55 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.391 Prec@5=79.043 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:55 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.391 Prec@5=79.043 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:55 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.618 DataTime=0.394 Loss=1.926 Prec@1=54.391 Prec@5=79.043 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:56 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.925 Prec@1=54.429 Prec@5=79.057 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:56 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.925 Prec@1=54.429 Prec@5=79.057 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=19:56 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.925 Prec@1=54.429 Prec@5=79.057 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=19:57 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.926 Prec@1=54.418 Prec@5=79.057 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=19:57 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.926 Prec@1=54.418 Prec@5=79.057 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:57 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.926 Prec@1=54.418 Prec@5=79.057 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:58 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.927 Prec@1=54.398 Prec@5=79.047 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:58 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.927 Prec@1=54.398 Prec@5=79.047 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:58 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.927 Prec@1=54.398 Prec@5=79.047 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:59 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.397 Prec@5=79.045 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=19:59 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.397 Prec@5=79.045 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:59 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.397 Prec@5=79.045 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:00 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.400 Prec@5=79.050 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:00 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.400 Prec@5=79.050 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:00 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.393 Loss=1.928 Prec@1=54.400 Prec@5=79.050 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:01 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.392 Loss=1.927 Prec@1=54.435 Prec@5=79.063 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:01 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.392 Loss=1.927 Prec@1=54.435 Prec@5=79.063 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:01 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.617 DataTime=0.392 Loss=1.927 Prec@1=54.435 Prec@5=79.063 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:02 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.926 Prec@1=54.456 Prec@5=79.078 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:02 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.926 Prec@1=54.456 Prec@5=79.078 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:02 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.926 Prec@1=54.456 Prec@5=79.078 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:03 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.447 Prec@5=79.067 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:03 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.447 Prec@5=79.067 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:03 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.447 Prec@5=79.067 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:04 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.436 Prec@5=79.060 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:04 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.436 Prec@5=79.060 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:04 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.436 Prec@5=79.060 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:05 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.450 Prec@5=79.081 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:05 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.450 Prec@5=79.081 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:05 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.450 Prec@5=79.081 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:06 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.442 Prec@5=79.082 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:06 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.442 Prec@5=79.082 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:06 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.927 Prec@1=54.442 Prec@5=79.082 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:07 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.414 Prec@5=79.071 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:07 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.414 Prec@5=79.071 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:07 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.928 Prec@1=54.414 Prec@5=79.071 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:08 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.397 Prec@5=79.063 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:08 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.397 Prec@5=79.063 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:08 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.397 Prec@5=79.063 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:09 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.390 Prec@5=79.060 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:09 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.390 Prec@5=79.060 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=20:09 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.390 Prec@5=79.060 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=20:09 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.389 Prec@5=79.060 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=20:09 IST=> training   100.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.616 DataTime=0.392 Loss=1.929 Prec@1=54.389 Prec@5=79.060 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=20:09 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 0.00% of 1x98...Epoch=13/150 LR=0.09843 Time=6.224 Loss=1.837 Prec@1=55.469 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.224 Loss=1.837 Prec@1=55.469 Prec@5=82.422 rate=6166.79 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.224 Loss=1.837 Prec@1=55.469 Prec@5=82.422 rate=6166.79 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=0.403 Loss=1.880 Prec@1=54.962 Prec@5=79.918 rate=6166.79 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST** validation 100.00% of 1x98...Epoch=13/150 LR=0.09843 Time=0.403 Loss=1.880 Prec@1=54.962 Prec@5=79.918 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=20:10 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> training   0.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.101 DataTime=4.796 Loss=1.824 Prec@1=56.641 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.101 DataTime=4.796 Loss=1.824 Prec@1=56.641 Prec@5=81.641 rate=6428.72 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.101 DataTime=4.796 Loss=1.824 Prec@1=56.641 Prec@5=81.641 rate=6428.72 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.657 DataTime=0.431 Loss=1.883 Prec@1=55.119 Prec@5=79.887 rate=6428.72 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.657 DataTime=0.431 Loss=1.883 Prec@1=55.119 Prec@5=79.887 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=20:11 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.657 DataTime=0.431 Loss=1.883 Prec@1=55.119 Prec@5=79.887 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=20:12 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.636 DataTime=0.409 Loss=1.879 Prec@1=55.310 Prec@5=79.811 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=20:12 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.636 DataTime=0.409 Loss=1.879 Prec@1=55.310 Prec@5=79.811 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=20:12 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.636 DataTime=0.409 Loss=1.879 Prec@1=55.310 Prec@5=79.811 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=20:13 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.629 DataTime=0.403 Loss=1.876 Prec@1=55.335 Prec@5=79.896 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.629 DataTime=0.403 Loss=1.876 Prec@1=55.335 Prec@5=79.896 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.629 DataTime=0.403 Loss=1.876 Prec@1=55.335 Prec@5=79.896 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=20:14 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.625 DataTime=0.399 Loss=1.878 Prec@1=55.384 Prec@5=79.834 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=20:14 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.625 DataTime=0.399 Loss=1.878 Prec@1=55.384 Prec@5=79.834 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:14 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.625 DataTime=0.399 Loss=1.878 Prec@1=55.384 Prec@5=79.834 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:15 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.623 DataTime=0.397 Loss=1.877 Prec@1=55.383 Prec@5=79.832 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:15 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.623 DataTime=0.397 Loss=1.877 Prec@1=55.383 Prec@5=79.832 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:15 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.623 DataTime=0.397 Loss=1.877 Prec@1=55.383 Prec@5=79.832 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:16 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.621 DataTime=0.396 Loss=1.880 Prec@1=55.349 Prec@5=79.792 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:16 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.621 DataTime=0.396 Loss=1.880 Prec@1=55.349 Prec@5=79.792 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:16 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.621 DataTime=0.396 Loss=1.880 Prec@1=55.349 Prec@5=79.792 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:17 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.879 Prec@1=55.351 Prec@5=79.799 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:17 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.879 Prec@1=55.351 Prec@5=79.799 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=20:17 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.879 Prec@1=55.351 Prec@5=79.799 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=20:18 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.878 Prec@1=55.374 Prec@5=79.817 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=20:18 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.878 Prec@1=55.374 Prec@5=79.817 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:18 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.620 DataTime=0.395 Loss=1.878 Prec@1=55.374 Prec@5=79.817 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:19 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.619 DataTime=0.394 Loss=1.881 Prec@1=55.355 Prec@5=79.778 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:19 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.619 DataTime=0.394 Loss=1.881 Prec@1=55.355 Prec@5=79.778 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:19 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.619 DataTime=0.394 Loss=1.881 Prec@1=55.355 Prec@5=79.778 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:20 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.880 Prec@1=55.354 Prec@5=79.779 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:20 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.880 Prec@1=55.354 Prec@5=79.779 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:20 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.880 Prec@1=55.354 Prec@5=79.779 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:21 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.881 Prec@1=55.355 Prec@5=79.776 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:21 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.881 Prec@1=55.355 Prec@5=79.776 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:21 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.881 Prec@1=55.355 Prec@5=79.776 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:23 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.882 Prec@1=55.325 Prec@5=79.758 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:23 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.882 Prec@1=55.325 Prec@5=79.758 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:23 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.882 Prec@1=55.325 Prec@5=79.758 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:24 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.884 Prec@1=55.302 Prec@5=79.726 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:24 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.884 Prec@1=55.302 Prec@5=79.726 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:24 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.618 DataTime=0.393 Loss=1.884 Prec@1=55.302 Prec@5=79.726 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:25 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.292 Prec@5=79.728 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:25 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.292 Prec@5=79.728 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=20:25 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.292 Prec@5=79.728 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=20:26 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.299 Prec@5=79.721 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=20:26 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.299 Prec@5=79.721 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=20:26 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.299 Prec@5=79.721 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=20:27 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.284 Prec@5=79.717 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=20:27 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.284 Prec@5=79.717 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:27 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.884 Prec@1=55.284 Prec@5=79.717 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:28 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.885 Prec@1=55.268 Prec@5=79.699 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:28 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.885 Prec@1=55.268 Prec@5=79.699 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:28 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.885 Prec@1=55.268 Prec@5=79.699 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:29 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.886 Prec@1=55.245 Prec@5=79.690 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:29 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.886 Prec@1=55.245 Prec@5=79.690 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=20:29 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.886 Prec@1=55.245 Prec@5=79.690 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=20:30 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.887 Prec@1=55.223 Prec@5=79.684 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=20:30 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.887 Prec@1=55.223 Prec@5=79.684 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:30 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.617 DataTime=0.392 Loss=1.887 Prec@1=55.223 Prec@5=79.684 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:31 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.886 Prec@1=55.237 Prec@5=79.685 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:31 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.886 Prec@1=55.237 Prec@5=79.685 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:31 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.886 Prec@1=55.237 Prec@5=79.685 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:32 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.887 Prec@1=55.234 Prec@5=79.690 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:32 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.887 Prec@1=55.234 Prec@5=79.690 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:32 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.392 Loss=1.887 Prec@1=55.234 Prec@5=79.690 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:33 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.220 Prec@5=79.682 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:33 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.220 Prec@5=79.682 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=20:33 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.220 Prec@5=79.682 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=20:34 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.230 Prec@5=79.692 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=20:34 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.230 Prec@5=79.692 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=20:34 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.230 Prec@5=79.692 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=20:35 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.888 Prec@1=55.201 Prec@5=79.687 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=20:35 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.888 Prec@1=55.201 Prec@5=79.687 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=20:35 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.888 Prec@1=55.201 Prec@5=79.687 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=20:36 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.216 Prec@5=79.694 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=20:36 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.216 Prec@5=79.694 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:36 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.216 Prec@5=79.694 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:36 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.215 Prec@5=79.693 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:36 IST=> training   100.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.616 DataTime=0.391 Loss=1.887 Prec@1=55.215 Prec@5=79.693 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=20:36 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST=> validation 0.00% of 1x98...Epoch=14/150 LR=0.09816 Time=7.057 Loss=1.804 Prec@1=53.516 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=20:36 IST=> validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=7.057 Loss=1.804 Prec@1=53.516 Prec@5=82.422 rate=6320.04 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=7.057 Loss=1.804 Prec@1=53.516 Prec@5=82.422 rate=6320.04 Hz, eta=0:00:00, total=0:00:00, wall=20:37 IST** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=0.414 Loss=1.916 Prec@1=54.198 Prec@5=79.554 rate=6320.04 Hz, eta=0:00:00, total=0:00:00, wall=20:37 IST** validation 100.00% of 1x98...Epoch=14/150 LR=0.09816 Time=0.414 Loss=1.916 Prec@1=54.198 Prec@5=79.554 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=20:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:37 IST=> training   0.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.984 DataTime=5.748 Loss=1.934 Prec@1=52.148 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=20:37 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.984 DataTime=5.748 Loss=1.934 Prec@1=52.148 Prec@5=76.953 rate=7882.95 Hz, eta=0:00:00, total=0:00:00, wall=20:37 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.984 DataTime=5.748 Loss=1.934 Prec@1=52.148 Prec@5=76.953 rate=7882.95 Hz, eta=0:00:00, total=0:00:00, wall=20:38 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.665 DataTime=0.439 Loss=1.837 Prec@1=56.469 Prec@5=80.424 rate=7882.95 Hz, eta=0:00:00, total=0:00:00, wall=20:38 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.665 DataTime=0.439 Loss=1.837 Prec@1=56.469 Prec@5=80.424 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:38 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.665 DataTime=0.439 Loss=1.837 Prec@1=56.469 Prec@5=80.424 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:39 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.640 DataTime=0.415 Loss=1.838 Prec@1=56.288 Prec@5=80.458 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:39 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.640 DataTime=0.415 Loss=1.838 Prec@1=56.288 Prec@5=80.458 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:39 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.640 DataTime=0.415 Loss=1.838 Prec@1=56.288 Prec@5=80.458 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:40 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.631 DataTime=0.406 Loss=1.839 Prec@1=56.218 Prec@5=80.375 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:40 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.631 DataTime=0.406 Loss=1.839 Prec@1=56.218 Prec@5=80.375 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=20:40 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.631 DataTime=0.406 Loss=1.839 Prec@1=56.218 Prec@5=80.375 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=20:41 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.627 DataTime=0.402 Loss=1.837 Prec@1=56.227 Prec@5=80.365 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=20:41 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.627 DataTime=0.402 Loss=1.837 Prec@1=56.227 Prec@5=80.365 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:41 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.627 DataTime=0.402 Loss=1.837 Prec@1=56.227 Prec@5=80.365 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:42 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.625 DataTime=0.400 Loss=1.838 Prec@1=56.179 Prec@5=80.401 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=20:42 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.625 DataTime=0.400 Loss=1.838 Prec@1=56.179 Prec@5=80.401 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:42 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.625 DataTime=0.400 Loss=1.838 Prec@1=56.179 Prec@5=80.401 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:43 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.623 DataTime=0.398 Loss=1.842 Prec@1=56.123 Prec@5=80.352 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:43 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.623 DataTime=0.398 Loss=1.842 Prec@1=56.123 Prec@5=80.352 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:43 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.623 DataTime=0.398 Loss=1.842 Prec@1=56.123 Prec@5=80.352 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:44 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.622 DataTime=0.397 Loss=1.842 Prec@1=56.120 Prec@5=80.343 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:44 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.622 DataTime=0.397 Loss=1.842 Prec@1=56.120 Prec@5=80.343 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=20:44 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.622 DataTime=0.397 Loss=1.842 Prec@1=56.120 Prec@5=80.343 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=20:45 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.621 DataTime=0.396 Loss=1.844 Prec@1=56.086 Prec@5=80.296 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=20:45 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.621 DataTime=0.396 Loss=1.844 Prec@1=56.086 Prec@5=80.296 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:45 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.621 DataTime=0.396 Loss=1.844 Prec@1=56.086 Prec@5=80.296 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:46 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.396 Loss=1.846 Prec@1=56.053 Prec@5=80.277 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.396 Loss=1.846 Prec@1=56.053 Prec@5=80.277 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.396 Loss=1.846 Prec@1=56.053 Prec@5=80.277 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:47 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.395 Loss=1.847 Prec@1=56.018 Prec@5=80.253 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=20:47 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.395 Loss=1.847 Prec@1=56.018 Prec@5=80.253 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:47 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.620 DataTime=0.395 Loss=1.847 Prec@1=56.018 Prec@5=80.253 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:48 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.395 Loss=1.848 Prec@1=56.015 Prec@5=80.256 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=20:48 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.395 Loss=1.848 Prec@1=56.015 Prec@5=80.256 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:48 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.395 Loss=1.848 Prec@1=56.015 Prec@5=80.256 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:49 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.394 Loss=1.848 Prec@1=56.024 Prec@5=80.248 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=20:49 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.394 Loss=1.848 Prec@1=56.024 Prec@5=80.248 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=20:49 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.619 DataTime=0.394 Loss=1.848 Prec@1=56.024 Prec@5=80.248 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=20:50 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.023 Prec@5=80.246 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=20:50 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.023 Prec@5=80.246 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:50 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.023 Prec@5=80.246 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:51 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.009 Prec@5=80.236 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=20:51 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.009 Prec@5=80.236 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=20:51 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.849 Prec@1=56.009 Prec@5=80.236 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=20:52 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.850 Prec@1=56.003 Prec@5=80.211 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=20:52 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.850 Prec@1=56.003 Prec@5=80.211 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=20:52 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.394 Loss=1.850 Prec@1=56.003 Prec@5=80.211 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=20:53 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.393 Loss=1.850 Prec@1=56.021 Prec@5=80.224 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=20:53 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.393 Loss=1.850 Prec@1=56.021 Prec@5=80.224 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:53 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.618 DataTime=0.393 Loss=1.850 Prec@1=56.021 Prec@5=80.224 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:54 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.014 Prec@5=80.224 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=20:54 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.014 Prec@5=80.224 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:54 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.014 Prec@5=80.224 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:55 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.013 Prec@5=80.228 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=20:55 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.013 Prec@5=80.228 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=20:55 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.013 Prec@5=80.228 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=20:56 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.000 Prec@5=80.217 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=20:56 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.000 Prec@5=80.217 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:56 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.393 Loss=1.850 Prec@1=56.000 Prec@5=80.217 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:57 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.850 Prec@1=56.003 Prec@5=80.222 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=20:57 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.850 Prec@1=56.003 Prec@5=80.222 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:57 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.850 Prec@1=56.003 Prec@5=80.222 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:58 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.987 Prec@5=80.216 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=20:58 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.987 Prec@5=80.216 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=20:58 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.987 Prec@5=80.216 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=20:59 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.981 Prec@5=80.220 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=20:59 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.981 Prec@5=80.220 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=20:59 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.851 Prec@1=55.981 Prec@5=80.220 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:00 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.962 Prec@5=80.198 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:00 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.962 Prec@5=80.198 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:00 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.962 Prec@5=80.198 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:01 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.957 Prec@5=80.196 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:01 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.957 Prec@5=80.196 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:01 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.617 DataTime=0.392 Loss=1.852 Prec@1=55.957 Prec@5=80.196 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:02 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.616 DataTime=0.392 Loss=1.852 Prec@1=55.955 Prec@5=80.199 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:02 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.616 DataTime=0.392 Loss=1.852 Prec@1=55.955 Prec@5=80.199 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:02 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.616 DataTime=0.392 Loss=1.852 Prec@1=55.955 Prec@5=80.199 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:02 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.616 DataTime=0.392 Loss=1.852 Prec@1=55.954 Prec@5=80.199 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:02 IST=> training   100.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.616 DataTime=0.392 Loss=1.852 Prec@1=55.954 Prec@5=80.199 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=21:02 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> validation 0.00% of 1x98...Epoch=15/150 LR=0.09787 Time=6.021 Loss=1.725 Prec@1=60.742 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.021 Loss=1.725 Prec@1=60.742 Prec@5=83.398 rate=4189.90 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.021 Loss=1.725 Prec@1=60.742 Prec@5=83.398 rate=4189.90 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=0.400 Loss=1.853 Prec@1=55.552 Prec@5=80.618 rate=4189.90 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST** validation 100.00% of 1x98...Epoch=15/150 LR=0.09787 Time=0.400 Loss=1.853 Prec@1=55.552 Prec@5=80.618 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=21:03 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:03 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:03 IST=> training   0.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=5.643 DataTime=5.362 Loss=1.697 Prec@1=58.203 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=21:03 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=5.643 DataTime=5.362 Loss=1.697 Prec@1=58.203 Prec@5=82.617 rate=7588.29 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=5.643 DataTime=5.362 Loss=1.697 Prec@1=58.203 Prec@5=82.617 rate=7588.29 Hz, eta=0:00:00, total=0:00:00, wall=21:04 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.663 DataTime=0.437 Loss=1.795 Prec@1=57.248 Prec@5=80.898 rate=7588.29 Hz, eta=0:00:00, total=0:00:00, wall=21:04 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.663 DataTime=0.437 Loss=1.795 Prec@1=57.248 Prec@5=80.898 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=21:04 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.663 DataTime=0.437 Loss=1.795 Prec@1=57.248 Prec@5=80.898 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=21:05 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.637 DataTime=0.413 Loss=1.796 Prec@1=57.128 Prec@5=80.982 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=21:05 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.637 DataTime=0.413 Loss=1.796 Prec@1=57.128 Prec@5=80.982 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:05 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.637 DataTime=0.413 Loss=1.796 Prec@1=57.128 Prec@5=80.982 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:06 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.628 DataTime=0.406 Loss=1.803 Prec@1=56.962 Prec@5=80.878 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:06 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.628 DataTime=0.406 Loss=1.803 Prec@1=56.962 Prec@5=80.878 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:06 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.628 DataTime=0.406 Loss=1.803 Prec@1=56.962 Prec@5=80.878 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:07 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.624 DataTime=0.402 Loss=1.806 Prec@1=56.899 Prec@5=80.897 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:07 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.624 DataTime=0.402 Loss=1.806 Prec@1=56.899 Prec@5=80.897 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=21:07 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.624 DataTime=0.402 Loss=1.806 Prec@1=56.899 Prec@5=80.897 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=21:08 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.622 DataTime=0.399 Loss=1.804 Prec@1=56.937 Prec@5=80.931 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=21:08 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.622 DataTime=0.399 Loss=1.804 Prec@1=56.937 Prec@5=80.931 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:08 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.622 DataTime=0.399 Loss=1.804 Prec@1=56.937 Prec@5=80.931 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:09 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.621 DataTime=0.398 Loss=1.805 Prec@1=56.940 Prec@5=80.945 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:09 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.621 DataTime=0.398 Loss=1.805 Prec@1=56.940 Prec@5=80.945 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:09 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.621 DataTime=0.398 Loss=1.805 Prec@1=56.940 Prec@5=80.945 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:10 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.397 Loss=1.806 Prec@1=56.889 Prec@5=80.931 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:10 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.397 Loss=1.806 Prec@1=56.889 Prec@5=80.931 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:10 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.397 Loss=1.806 Prec@1=56.889 Prec@5=80.931 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:11 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.396 Loss=1.809 Prec@1=56.818 Prec@5=80.892 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:11 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.396 Loss=1.809 Prec@1=56.818 Prec@5=80.892 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=21:11 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.620 DataTime=0.396 Loss=1.809 Prec@1=56.818 Prec@5=80.892 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=21:12 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.812 Prec@5=80.878 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=21:12 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.812 Prec@5=80.878 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=21:12 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.812 Prec@5=80.878 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=21:13 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.816 Prec@5=80.864 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=21:13 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.816 Prec@5=80.864 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:13 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.619 DataTime=0.395 Loss=1.810 Prec@1=56.816 Prec@5=80.864 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:14 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.810 Prec@1=56.823 Prec@5=80.881 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:14 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.810 Prec@1=56.823 Prec@5=80.881 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=21:14 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.810 Prec@1=56.823 Prec@5=80.881 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=21:15 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.812 Prec@1=56.785 Prec@5=80.836 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=21:15 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.812 Prec@1=56.785 Prec@5=80.836 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:15 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.812 Prec@1=56.785 Prec@5=80.836 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:16 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.813 Prec@1=56.762 Prec@5=80.828 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:16 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.813 Prec@1=56.762 Prec@5=80.828 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:16 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.618 DataTime=0.394 Loss=1.813 Prec@1=56.762 Prec@5=80.828 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:17 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.813 Prec@1=56.754 Prec@5=80.824 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:17 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.813 Prec@1=56.754 Prec@5=80.824 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:17 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.813 Prec@1=56.754 Prec@5=80.824 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:18 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.743 Prec@5=80.807 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:18 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.743 Prec@5=80.807 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:18 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.743 Prec@5=80.807 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:19 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.729 Prec@5=80.792 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:19 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.729 Prec@5=80.792 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:19 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.814 Prec@1=56.729 Prec@5=80.792 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:20 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.815 Prec@1=56.724 Prec@5=80.795 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.815 Prec@1=56.724 Prec@5=80.795 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.815 Prec@1=56.724 Prec@5=80.795 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=21:22 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.817 Prec@1=56.691 Prec@5=80.763 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=21:22 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.817 Prec@1=56.691 Prec@5=80.763 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=21:22 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.817 Prec@1=56.691 Prec@5=80.763 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=21:23 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.818 Prec@1=56.649 Prec@5=80.741 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=21:23 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.818 Prec@1=56.649 Prec@5=80.741 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:23 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.393 Loss=1.818 Prec@1=56.649 Prec@5=80.741 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:24 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.392 Loss=1.819 Prec@1=56.662 Prec@5=80.727 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:24 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.392 Loss=1.819 Prec@1=56.662 Prec@5=80.727 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:24 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.617 DataTime=0.392 Loss=1.819 Prec@1=56.662 Prec@5=80.727 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:25 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.819 Prec@1=56.653 Prec@5=80.718 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:25 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.819 Prec@1=56.653 Prec@5=80.718 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:25 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.819 Prec@1=56.653 Prec@5=80.718 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:26 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.637 Prec@5=80.707 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:26 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.637 Prec@5=80.707 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:26 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.637 Prec@5=80.707 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:27 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.629 Prec@5=80.707 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:27 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.629 Prec@5=80.707 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=21:27 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.629 Prec@5=80.707 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=21:28 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.628 Prec@5=80.707 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=21:28 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.628 Prec@5=80.707 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=21:28 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.628 Prec@5=80.707 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=21:29 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.620 Prec@5=80.696 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=21:29 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.620 Prec@5=80.696 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:29 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.620 Prec@5=80.696 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:29 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.620 Prec@5=80.698 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:29 IST=> training   100.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.616 DataTime=0.392 Loss=1.820 Prec@1=56.620 Prec@5=80.698 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=21:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:29 IST=> validation 0.00% of 1x98...Epoch=16/150 LR=0.09755 Time=6.624 Loss=1.804 Prec@1=55.273 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=21:29 IST=> validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.624 Loss=1.804 Prec@1=55.273 Prec@5=82.617 rate=5832.60 Hz, eta=0:00:00, total=0:00:00, wall=21:29 IST** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.624 Loss=1.804 Prec@1=55.273 Prec@5=82.617 rate=5832.60 Hz, eta=0:00:00, total=0:00:00, wall=21:29 IST** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=0.409 Loss=1.841 Prec@1=56.076 Prec@5=80.528 rate=5832.60 Hz, eta=0:00:00, total=0:00:00, wall=21:29 IST** validation 100.00% of 1x98...Epoch=16/150 LR=0.09755 Time=0.409 Loss=1.841 Prec@1=56.076 Prec@5=80.528 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=21:29 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:30 IST=> training   0.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=6.163 DataTime=5.926 Loss=1.725 Prec@1=55.859 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=21:30 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=6.163 DataTime=5.926 Loss=1.725 Prec@1=55.859 Prec@5=82.422 rate=7724.39 Hz, eta=0:00:00, total=0:00:00, wall=21:30 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=6.163 DataTime=5.926 Loss=1.725 Prec@1=55.859 Prec@5=82.422 rate=7724.39 Hz, eta=0:00:00, total=0:00:00, wall=21:31 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.667 DataTime=0.442 Loss=1.785 Prec@1=57.275 Prec@5=81.269 rate=7724.39 Hz, eta=0:00:00, total=0:00:00, wall=21:31 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.667 DataTime=0.442 Loss=1.785 Prec@1=57.275 Prec@5=81.269 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=21:31 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.667 DataTime=0.442 Loss=1.785 Prec@1=57.275 Prec@5=81.269 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=21:32 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.641 DataTime=0.415 Loss=1.778 Prec@1=57.535 Prec@5=81.311 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=21:32 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.641 DataTime=0.415 Loss=1.778 Prec@1=57.535 Prec@5=81.311 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:32 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.641 DataTime=0.415 Loss=1.778 Prec@1=57.535 Prec@5=81.311 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:33 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.632 DataTime=0.407 Loss=1.775 Prec@1=57.515 Prec@5=81.497 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:33 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.632 DataTime=0.407 Loss=1.775 Prec@1=57.515 Prec@5=81.497 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=21:33 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.632 DataTime=0.407 Loss=1.775 Prec@1=57.515 Prec@5=81.497 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=21:34 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.627 DataTime=0.403 Loss=1.780 Prec@1=57.312 Prec@5=81.421 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=21:34 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.627 DataTime=0.403 Loss=1.780 Prec@1=57.312 Prec@5=81.421 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=21:34 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.627 DataTime=0.403 Loss=1.780 Prec@1=57.312 Prec@5=81.421 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=21:35 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.625 DataTime=0.401 Loss=1.782 Prec@1=57.341 Prec@5=81.354 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=21:35 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.625 DataTime=0.401 Loss=1.782 Prec@1=57.341 Prec@5=81.354 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=21:35 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.625 DataTime=0.401 Loss=1.782 Prec@1=57.341 Prec@5=81.354 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=21:36 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.623 DataTime=0.399 Loss=1.782 Prec@1=57.336 Prec@5=81.372 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=21:36 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.623 DataTime=0.399 Loss=1.782 Prec@1=57.336 Prec@5=81.372 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=21:36 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.623 DataTime=0.399 Loss=1.782 Prec@1=57.336 Prec@5=81.372 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=21:37 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.622 DataTime=0.397 Loss=1.783 Prec@1=57.341 Prec@5=81.336 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=21:37 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.622 DataTime=0.397 Loss=1.783 Prec@1=57.341 Prec@5=81.336 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=21:37 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.622 DataTime=0.397 Loss=1.783 Prec@1=57.341 Prec@5=81.336 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=21:38 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.621 DataTime=0.397 Loss=1.781 Prec@1=57.373 Prec@5=81.342 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=21:38 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.621 DataTime=0.397 Loss=1.781 Prec@1=57.373 Prec@5=81.342 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=21:38 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.621 DataTime=0.397 Loss=1.781 Prec@1=57.373 Prec@5=81.342 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=21:39 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.396 Loss=1.782 Prec@1=57.347 Prec@5=81.327 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=21:39 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.396 Loss=1.782 Prec@1=57.347 Prec@5=81.327 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:39 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.396 Loss=1.782 Prec@1=57.347 Prec@5=81.327 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:40 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.395 Loss=1.783 Prec@1=57.316 Prec@5=81.303 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:40 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.395 Loss=1.783 Prec@1=57.316 Prec@5=81.303 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=21:40 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.620 DataTime=0.395 Loss=1.783 Prec@1=57.316 Prec@5=81.303 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=21:41 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.395 Loss=1.785 Prec@1=57.293 Prec@5=81.284 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=21:41 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.395 Loss=1.785 Prec@1=57.293 Prec@5=81.284 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:41 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.395 Loss=1.785 Prec@1=57.293 Prec@5=81.284 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:42 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.394 Loss=1.785 Prec@1=57.299 Prec@5=81.270 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:42 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.394 Loss=1.785 Prec@1=57.299 Prec@5=81.270 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:42 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.619 DataTime=0.394 Loss=1.785 Prec@1=57.299 Prec@5=81.270 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:43 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.289 Prec@5=81.256 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:43 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.289 Prec@5=81.256 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:43 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.289 Prec@5=81.256 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:44 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.286 Prec@5=81.253 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:44 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.286 Prec@5=81.253 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:44 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.394 Loss=1.785 Prec@1=57.286 Prec@5=81.253 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:45 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.253 Prec@5=81.207 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:45 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.253 Prec@5=81.207 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:45 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.253 Prec@5=81.207 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:46 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.248 Prec@5=81.194 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:46 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.248 Prec@5=81.194 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:46 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.248 Prec@5=81.194 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:47 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.245 Prec@5=81.199 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:47 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.245 Prec@5=81.199 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:47 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.618 DataTime=0.393 Loss=1.788 Prec@1=57.245 Prec@5=81.199 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:48 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.790 Prec@1=57.212 Prec@5=81.169 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:48 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.790 Prec@1=57.212 Prec@5=81.169 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=21:48 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.790 Prec@1=57.212 Prec@5=81.169 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=21:49 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.791 Prec@1=57.195 Prec@5=81.151 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=21:49 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.791 Prec@1=57.195 Prec@5=81.151 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:49 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.393 Loss=1.791 Prec@1=57.195 Prec@5=81.151 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:50 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.791 Prec@1=57.212 Prec@5=81.155 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:50 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.791 Prec@1=57.212 Prec@5=81.155 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:50 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.791 Prec@1=57.212 Prec@5=81.155 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:51 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.197 Prec@5=81.128 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:51 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.197 Prec@5=81.128 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:51 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.197 Prec@5=81.128 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:52 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.201 Prec@5=81.136 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.201 Prec@5=81.136 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.792 Prec@1=57.201 Prec@5=81.136 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:53 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.193 Prec@5=81.121 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:53 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.193 Prec@5=81.121 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:53 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.193 Prec@5=81.121 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:54 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.198 Prec@5=81.120 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=21:54 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.198 Prec@5=81.120 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:54 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.198 Prec@5=81.120 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:55 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.214 Prec@5=81.125 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:55 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.214 Prec@5=81.125 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:55 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.617 DataTime=0.392 Loss=1.793 Prec@1=57.214 Prec@5=81.125 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:55 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.616 DataTime=0.392 Loss=1.793 Prec@1=57.212 Prec@5=81.124 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=21:55 IST=> training   100.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.616 DataTime=0.392 Loss=1.793 Prec@1=57.212 Prec@5=81.124 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=21:55 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> validation 0.00% of 1x98...Epoch=17/150 LR=0.09722 Time=6.228 Loss=1.686 Prec@1=59.766 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=6.228 Loss=1.686 Prec@1=59.766 Prec@5=84.375 rate=3536.82 Hz, eta=0:00:00, total=0:00:00, wall=21:55 IST** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=6.228 Loss=1.686 Prec@1=59.766 Prec@5=84.375 rate=3536.82 Hz, eta=0:00:00, total=0:00:00, wall=21:56 IST** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=0.402 Loss=1.750 Prec@1=57.812 Prec@5=82.186 rate=3536.82 Hz, eta=0:00:00, total=0:00:00, wall=21:56 IST** validation 100.00% of 1x98...Epoch=17/150 LR=0.09722 Time=0.402 Loss=1.750 Prec@1=57.812 Prec@5=82.186 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=21:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:56 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:56 IST=> training   0.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.174 DataTime=4.891 Loss=1.812 Prec@1=56.445 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=21:56 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.174 DataTime=4.891 Loss=1.812 Prec@1=56.445 Prec@5=81.641 rate=5719.02 Hz, eta=0:00:00, total=0:00:00, wall=21:56 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.174 DataTime=4.891 Loss=1.812 Prec@1=56.445 Prec@5=81.641 rate=5719.02 Hz, eta=0:00:00, total=0:00:00, wall=21:57 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.660 DataTime=0.435 Loss=1.741 Prec@1=58.019 Prec@5=81.964 rate=5719.02 Hz, eta=0:00:00, total=0:00:00, wall=21:57 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.660 DataTime=0.435 Loss=1.741 Prec@1=58.019 Prec@5=81.964 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=21:57 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.660 DataTime=0.435 Loss=1.741 Prec@1=58.019 Prec@5=81.964 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=21:58 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.636 DataTime=0.412 Loss=1.753 Prec@1=57.945 Prec@5=81.725 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=21:58 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.636 DataTime=0.412 Loss=1.753 Prec@1=57.945 Prec@5=81.725 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:58 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.636 DataTime=0.412 Loss=1.753 Prec@1=57.945 Prec@5=81.725 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:59 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.627 DataTime=0.403 Loss=1.754 Prec@1=57.977 Prec@5=81.715 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=21:59 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.627 DataTime=0.403 Loss=1.754 Prec@1=57.977 Prec@5=81.715 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=21:59 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.627 DataTime=0.403 Loss=1.754 Prec@1=57.977 Prec@5=81.715 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=22:00 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.623 DataTime=0.400 Loss=1.753 Prec@1=58.052 Prec@5=81.745 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=22:00 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.623 DataTime=0.400 Loss=1.753 Prec@1=58.052 Prec@5=81.745 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=22:00 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.623 DataTime=0.400 Loss=1.753 Prec@1=58.052 Prec@5=81.745 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=22:01 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.621 DataTime=0.398 Loss=1.755 Prec@1=58.008 Prec@5=81.730 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=22:01 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.621 DataTime=0.398 Loss=1.755 Prec@1=58.008 Prec@5=81.730 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:01 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.621 DataTime=0.398 Loss=1.755 Prec@1=58.008 Prec@5=81.730 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:02 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.620 DataTime=0.396 Loss=1.758 Prec@1=57.893 Prec@5=81.673 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:02 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.620 DataTime=0.396 Loss=1.758 Prec@1=57.893 Prec@5=81.673 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:02 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.620 DataTime=0.396 Loss=1.758 Prec@1=57.893 Prec@5=81.673 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:03 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.619 DataTime=0.395 Loss=1.758 Prec@1=57.863 Prec@5=81.679 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:03 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.619 DataTime=0.395 Loss=1.758 Prec@1=57.863 Prec@5=81.679 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:03 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.619 DataTime=0.395 Loss=1.758 Prec@1=57.863 Prec@5=81.679 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:04 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.395 Loss=1.758 Prec@1=57.874 Prec@5=81.652 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:04 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.395 Loss=1.758 Prec@1=57.874 Prec@5=81.652 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:04 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.395 Loss=1.758 Prec@1=57.874 Prec@5=81.652 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:05 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.394 Loss=1.759 Prec@1=57.843 Prec@5=81.630 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:05 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.394 Loss=1.759 Prec@1=57.843 Prec@5=81.630 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:05 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.618 DataTime=0.394 Loss=1.759 Prec@1=57.843 Prec@5=81.630 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:06 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.760 Prec@1=57.839 Prec@5=81.625 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:06 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.760 Prec@1=57.839 Prec@5=81.625 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:06 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.760 Prec@1=57.839 Prec@5=81.625 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:07 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.759 Prec@1=57.850 Prec@5=81.620 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:07 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.759 Prec@1=57.850 Prec@5=81.620 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:07 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.759 Prec@1=57.850 Prec@5=81.620 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:08 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.828 Prec@5=81.606 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:08 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.828 Prec@5=81.606 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:08 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.828 Prec@5=81.606 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:09 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.817 Prec@5=81.609 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:09 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.817 Prec@5=81.609 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:09 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.393 Loss=1.761 Prec@1=57.817 Prec@5=81.609 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:10 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.392 Loss=1.763 Prec@1=57.771 Prec@5=81.589 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:10 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.392 Loss=1.763 Prec@1=57.771 Prec@5=81.589 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:10 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.617 DataTime=0.392 Loss=1.763 Prec@1=57.771 Prec@5=81.589 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:11 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.764 Prec@1=57.752 Prec@5=81.565 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:11 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.764 Prec@1=57.752 Prec@5=81.565 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:11 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.764 Prec@1=57.752 Prec@5=81.565 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:12 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.747 Prec@5=81.574 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:12 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.747 Prec@5=81.574 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=22:12 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.747 Prec@5=81.574 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=22:13 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.718 Prec@5=81.571 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=22:13 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.718 Prec@5=81.571 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:13 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.765 Prec@1=57.718 Prec@5=81.571 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:14 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.766 Prec@1=57.699 Prec@5=81.551 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:14 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.766 Prec@1=57.699 Prec@5=81.551 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:14 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.766 Prec@1=57.699 Prec@5=81.551 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:15 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.675 Prec@5=81.537 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:15 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.675 Prec@5=81.537 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:15 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.675 Prec@5=81.537 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:16 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.668 Prec@5=81.534 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:16 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.668 Prec@5=81.534 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:16 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.767 Prec@1=57.668 Prec@5=81.534 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:17 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.654 Prec@5=81.538 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:17 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.654 Prec@5=81.538 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:17 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.654 Prec@5=81.538 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:18 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.655 Prec@5=81.534 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:18 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.655 Prec@5=81.534 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=22:18 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.392 Loss=1.768 Prec@1=57.655 Prec@5=81.534 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=22:19 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.635 Prec@5=81.518 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=22:19 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.635 Prec@5=81.518 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:19 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.635 Prec@5=81.518 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:21 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.630 Prec@5=81.518 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:21 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.630 Prec@5=81.518 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:21 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.630 Prec@5=81.518 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:22 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.625 Prec@5=81.514 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:22 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.625 Prec@5=81.514 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:22 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.625 Prec@5=81.514 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:22 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.623 Prec@5=81.513 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:22 IST=> training   100.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.616 DataTime=0.391 Loss=1.769 Prec@1=57.623 Prec@5=81.513 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=22:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> validation 0.00% of 1x98...Epoch=18/150 LR=0.09686 Time=7.566 Loss=1.861 Prec@1=54.688 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=7.566 Loss=1.861 Prec@1=54.688 Prec@5=81.250 rate=3166.06 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=7.566 Loss=1.861 Prec@1=54.688 Prec@5=81.250 rate=3166.06 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=0.422 Loss=1.858 Prec@1=55.528 Prec@5=80.458 rate=3166.06 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST** validation 100.00% of 1x98...Epoch=18/150 LR=0.09686 Time=0.422 Loss=1.858 Prec@1=55.528 Prec@5=80.458 rate=2.90 Hz, eta=0:00:00, total=0:00:33, wall=22:22 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> training   0.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.541 DataTime=5.237 Loss=1.671 Prec@1=59.766 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.541 DataTime=5.237 Loss=1.671 Prec@1=59.766 Prec@5=83.789 rate=7753.75 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.541 DataTime=5.237 Loss=1.671 Prec@1=59.766 Prec@5=83.789 rate=7753.75 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.657 DataTime=0.434 Loss=1.716 Prec@1=58.644 Prec@5=82.076 rate=7753.75 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.657 DataTime=0.434 Loss=1.716 Prec@1=58.644 Prec@5=82.076 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=22:23 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.657 DataTime=0.434 Loss=1.716 Prec@1=58.644 Prec@5=82.076 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=22:24 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.635 DataTime=0.411 Loss=1.721 Prec@1=58.522 Prec@5=82.065 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=22:24 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.635 DataTime=0.411 Loss=1.721 Prec@1=58.522 Prec@5=82.065 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=22:24 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.635 DataTime=0.411 Loss=1.721 Prec@1=58.522 Prec@5=82.065 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=22:25 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.628 DataTime=0.404 Loss=1.721 Prec@1=58.461 Prec@5=82.164 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=22:25 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.628 DataTime=0.404 Loss=1.721 Prec@1=58.461 Prec@5=82.164 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=22:25 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.628 DataTime=0.404 Loss=1.721 Prec@1=58.461 Prec@5=82.164 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=22:26 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.624 DataTime=0.400 Loss=1.720 Prec@1=58.509 Prec@5=82.184 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=22:26 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.624 DataTime=0.400 Loss=1.720 Prec@1=58.509 Prec@5=82.184 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=22:26 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.624 DataTime=0.400 Loss=1.720 Prec@1=58.509 Prec@5=82.184 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=22:27 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.622 DataTime=0.398 Loss=1.720 Prec@1=58.577 Prec@5=82.215 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.622 DataTime=0.398 Loss=1.720 Prec@1=58.577 Prec@5=82.215 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.622 DataTime=0.398 Loss=1.720 Prec@1=58.577 Prec@5=82.215 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:29 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.621 DataTime=0.397 Loss=1.723 Prec@1=58.495 Prec@5=82.148 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=22:29 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.621 DataTime=0.397 Loss=1.723 Prec@1=58.495 Prec@5=82.148 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=22:29 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.621 DataTime=0.397 Loss=1.723 Prec@1=58.495 Prec@5=82.148 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=22:30 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.620 DataTime=0.396 Loss=1.726 Prec@1=58.429 Prec@5=82.117 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=22:30 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.620 DataTime=0.396 Loss=1.726 Prec@1=58.429 Prec@5=82.117 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:30 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.620 DataTime=0.396 Loss=1.726 Prec@1=58.429 Prec@5=82.117 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:31 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.619 DataTime=0.395 Loss=1.728 Prec@1=58.401 Prec@5=82.091 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:31 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.619 DataTime=0.395 Loss=1.728 Prec@1=58.401 Prec@5=82.091 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:31 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.619 DataTime=0.395 Loss=1.728 Prec@1=58.401 Prec@5=82.091 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:32 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.395 Loss=1.728 Prec@1=58.387 Prec@5=82.088 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:32 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.395 Loss=1.728 Prec@1=58.387 Prec@5=82.088 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:32 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.395 Loss=1.728 Prec@1=58.387 Prec@5=82.088 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:33 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.731 Prec@1=58.317 Prec@5=82.046 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:33 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.731 Prec@1=58.317 Prec@5=82.046 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:33 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.731 Prec@1=58.317 Prec@5=82.046 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:34 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.735 Prec@1=58.256 Prec@5=81.993 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:34 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.735 Prec@1=58.256 Prec@5=81.993 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:34 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.618 DataTime=0.394 Loss=1.735 Prec@1=58.256 Prec@5=81.993 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:35 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.233 Prec@5=82.002 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:35 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.233 Prec@5=82.002 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:35 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.233 Prec@5=82.002 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:36 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.241 Prec@5=82.003 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:36 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.241 Prec@5=82.003 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:36 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.735 Prec@1=58.241 Prec@5=82.003 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:37 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.737 Prec@1=58.223 Prec@5=81.985 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:37 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.737 Prec@1=58.223 Prec@5=81.985 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:37 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.737 Prec@1=58.223 Prec@5=81.985 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:38 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.738 Prec@1=58.213 Prec@5=81.966 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:38 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.738 Prec@1=58.213 Prec@5=81.966 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:38 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.617 DataTime=0.393 Loss=1.738 Prec@1=58.213 Prec@5=81.966 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:39 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.739 Prec@1=58.192 Prec@5=81.950 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=22:39 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.739 Prec@1=58.192 Prec@5=81.950 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:39 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.739 Prec@1=58.192 Prec@5=81.950 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:40 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.740 Prec@1=58.160 Prec@5=81.925 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:40 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.740 Prec@1=58.160 Prec@5=81.925 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:40 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.740 Prec@1=58.160 Prec@5=81.925 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:41 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.138 Prec@5=81.908 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:41 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.138 Prec@5=81.908 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:41 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.138 Prec@5=81.908 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:42 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.146 Prec@5=81.908 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=22:42 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.146 Prec@5=81.908 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:42 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.741 Prec@1=58.146 Prec@5=81.908 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:43 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.742 Prec@1=58.149 Prec@5=81.892 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=22:43 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.742 Prec@1=58.149 Prec@5=81.892 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:43 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.742 Prec@1=58.149 Prec@5=81.892 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:44 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.743 Prec@1=58.138 Prec@5=81.877 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=22:44 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.743 Prec@1=58.138 Prec@5=81.877 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:44 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.743 Prec@1=58.138 Prec@5=81.877 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:45 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.744 Prec@1=58.118 Prec@5=81.860 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=22:45 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.744 Prec@1=58.118 Prec@5=81.860 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:45 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.744 Prec@1=58.118 Prec@5=81.860 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:46 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.745 Prec@1=58.110 Prec@5=81.858 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:46 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.745 Prec@1=58.110 Prec@5=81.858 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:46 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.745 Prec@1=58.110 Prec@5=81.858 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:47 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.098 Prec@5=81.843 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=22:47 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.098 Prec@5=81.843 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:47 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.098 Prec@5=81.843 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:48 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.106 Prec@5=81.840 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=22:48 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.106 Prec@5=81.840 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:48 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.106 Prec@5=81.840 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:48 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.104 Prec@5=81.839 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=22:48 IST=> training   100.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.616 DataTime=0.392 Loss=1.746 Prec@1=58.104 Prec@5=81.839 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=22:48 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:48 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:48 IST=> validation 0.00% of 1x98...Epoch=19/150 LR=0.09649 Time=6.739 Loss=1.857 Prec@1=53.711 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=22:48 IST=> validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.739 Loss=1.857 Prec@1=53.711 Prec@5=80.664 rate=3649.45 Hz, eta=0:00:00, total=0:00:00, wall=22:48 IST** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.739 Loss=1.857 Prec@1=53.711 Prec@5=80.664 rate=3649.45 Hz, eta=0:00:00, total=0:00:00, wall=22:49 IST** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=0.404 Loss=1.845 Prec@1=55.646 Prec@5=80.768 rate=3649.45 Hz, eta=0:00:00, total=0:00:00, wall=22:49 IST** validation 100.00% of 1x98...Epoch=19/150 LR=0.09649 Time=0.404 Loss=1.845 Prec@1=55.646 Prec@5=80.768 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=22:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:49 IST=> training   0.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.900 DataTime=5.611 Loss=1.798 Prec@1=58.203 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=22:49 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.900 DataTime=5.611 Loss=1.798 Prec@1=58.203 Prec@5=80.664 rate=9551.10 Hz, eta=0:00:00, total=0:00:00, wall=22:49 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.900 DataTime=5.611 Loss=1.798 Prec@1=58.203 Prec@5=80.664 rate=9551.10 Hz, eta=0:00:00, total=0:00:00, wall=22:50 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.666 DataTime=0.440 Loss=1.688 Prec@1=59.124 Prec@5=82.596 rate=9551.10 Hz, eta=0:00:00, total=0:00:00, wall=22:50 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.666 DataTime=0.440 Loss=1.688 Prec@1=59.124 Prec@5=82.596 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=22:50 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.666 DataTime=0.440 Loss=1.688 Prec@1=59.124 Prec@5=82.596 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=22:51 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.641 DataTime=0.416 Loss=1.698 Prec@1=59.054 Prec@5=82.459 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=22:51 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.641 DataTime=0.416 Loss=1.698 Prec@1=59.054 Prec@5=82.459 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=22:51 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.641 DataTime=0.416 Loss=1.698 Prec@1=59.054 Prec@5=82.459 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=22:52 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.632 DataTime=0.407 Loss=1.705 Prec@1=58.877 Prec@5=82.354 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=22:52 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.632 DataTime=0.407 Loss=1.705 Prec@1=58.877 Prec@5=82.354 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=22:52 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.632 DataTime=0.407 Loss=1.705 Prec@1=58.877 Prec@5=82.354 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=22:53 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.628 DataTime=0.403 Loss=1.702 Prec@1=58.975 Prec@5=82.392 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=22:53 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.628 DataTime=0.403 Loss=1.702 Prec@1=58.975 Prec@5=82.392 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=22:53 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.628 DataTime=0.403 Loss=1.702 Prec@1=58.975 Prec@5=82.392 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=22:54 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.625 DataTime=0.400 Loss=1.704 Prec@1=58.919 Prec@5=82.326 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=22:54 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.625 DataTime=0.400 Loss=1.704 Prec@1=58.919 Prec@5=82.326 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=22:54 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.625 DataTime=0.400 Loss=1.704 Prec@1=58.919 Prec@5=82.326 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=22:55 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.623 DataTime=0.398 Loss=1.708 Prec@1=58.823 Prec@5=82.313 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=22:55 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.623 DataTime=0.398 Loss=1.708 Prec@1=58.823 Prec@5=82.313 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=22:55 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.623 DataTime=0.398 Loss=1.708 Prec@1=58.823 Prec@5=82.313 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=22:56 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.622 DataTime=0.397 Loss=1.707 Prec@1=58.820 Prec@5=82.349 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=22:56 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.622 DataTime=0.397 Loss=1.707 Prec@1=58.820 Prec@5=82.349 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=22:56 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.622 DataTime=0.397 Loss=1.707 Prec@1=58.820 Prec@5=82.349 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=22:57 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.621 DataTime=0.396 Loss=1.709 Prec@1=58.771 Prec@5=82.317 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=22:57 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.621 DataTime=0.396 Loss=1.709 Prec@1=58.771 Prec@5=82.317 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=22:57 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.621 DataTime=0.396 Loss=1.709 Prec@1=58.771 Prec@5=82.317 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=22:58 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.620 DataTime=0.395 Loss=1.709 Prec@1=58.750 Prec@5=82.326 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=22:58 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.620 DataTime=0.395 Loss=1.709 Prec@1=58.750 Prec@5=82.326 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:58 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.620 DataTime=0.395 Loss=1.709 Prec@1=58.750 Prec@5=82.326 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:59 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.395 Loss=1.714 Prec@1=58.698 Prec@5=82.274 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:59 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.395 Loss=1.714 Prec@1=58.698 Prec@5=82.274 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:59 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.395 Loss=1.714 Prec@1=58.698 Prec@5=82.274 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=23:00 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.713 Prec@1=58.704 Prec@5=82.297 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.713 Prec@1=58.704 Prec@5=82.297 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.713 Prec@1=58.704 Prec@5=82.297 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:01 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.714 Prec@1=58.681 Prec@5=82.287 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:01 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.714 Prec@1=58.681 Prec@5=82.287 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:01 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.619 DataTime=0.394 Loss=1.714 Prec@1=58.681 Prec@5=82.287 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:02 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.394 Loss=1.715 Prec@1=58.672 Prec@5=82.286 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:02 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.394 Loss=1.715 Prec@1=58.672 Prec@5=82.286 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:02 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.394 Loss=1.715 Prec@1=58.672 Prec@5=82.286 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:03 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.655 Prec@5=82.273 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:03 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.655 Prec@5=82.273 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:03 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.655 Prec@5=82.273 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:04 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.637 Prec@5=82.266 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:04 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.637 Prec@5=82.266 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:04 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.618 DataTime=0.393 Loss=1.716 Prec@1=58.637 Prec@5=82.266 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:05 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.717 Prec@1=58.642 Prec@5=82.259 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:05 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.717 Prec@1=58.642 Prec@5=82.259 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:05 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.717 Prec@1=58.642 Prec@5=82.259 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:06 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.718 Prec@1=58.625 Prec@5=82.240 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:06 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.718 Prec@1=58.625 Prec@5=82.240 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:06 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.718 Prec@1=58.625 Prec@5=82.240 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:07 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.720 Prec@1=58.615 Prec@5=82.221 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:07 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.720 Prec@1=58.615 Prec@5=82.221 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:07 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.393 Loss=1.720 Prec@1=58.615 Prec@5=82.221 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:08 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.606 Prec@5=82.209 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:08 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.606 Prec@5=82.209 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:08 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.606 Prec@5=82.209 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:09 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.623 Prec@5=82.220 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:09 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.623 Prec@5=82.220 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=23:09 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.720 Prec@1=58.623 Prec@5=82.220 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=23:10 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.721 Prec@1=58.611 Prec@5=82.214 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=23:10 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.721 Prec@1=58.611 Prec@5=82.214 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:10 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.617 DataTime=0.392 Loss=1.721 Prec@1=58.611 Prec@5=82.214 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:11 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.721 Prec@1=58.613 Prec@5=82.200 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:11 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.721 Prec@1=58.613 Prec@5=82.200 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:11 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.721 Prec@1=58.613 Prec@5=82.200 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:12 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.722 Prec@1=58.597 Prec@5=82.181 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:12 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.722 Prec@1=58.597 Prec@5=82.181 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=23:12 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.722 Prec@1=58.597 Prec@5=82.181 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=23:13 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.586 Prec@5=82.169 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=23:13 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.586 Prec@5=82.169 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:13 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.586 Prec@5=82.169 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:14 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.583 Prec@5=82.160 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:14 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.583 Prec@5=82.160 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:14 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.583 Prec@5=82.160 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:14 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.583 Prec@5=82.160 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:14 IST=> training   100.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.616 DataTime=0.392 Loss=1.723 Prec@1=58.583 Prec@5=82.160 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=23:14 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 0.00% of 1x98...Epoch=20/150 LR=0.09609 Time=7.155 Loss=1.646 Prec@1=61.133 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=7.155 Loss=1.646 Prec@1=61.133 Prec@5=84.375 rate=6504.54 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=7.155 Loss=1.646 Prec@1=61.133 Prec@5=84.375 rate=6504.54 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=0.406 Loss=1.758 Prec@1=57.706 Prec@5=81.956 rate=6504.54 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 100.00% of 1x98...Epoch=20/150 LR=0.09609 Time=0.406 Loss=1.758 Prec@1=57.706 Prec@5=81.956 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=23:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.210 DataTime=4.924 Loss=1.629 Prec@1=60.547 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.210 DataTime=4.924 Loss=1.629 Prec@1=60.547 Prec@5=83.984 rate=6093.81 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=5.210 DataTime=4.924 Loss=1.629 Prec@1=60.547 Prec@5=83.984 rate=6093.81 Hz, eta=0:00:00, total=0:00:00, wall=23:16 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.660 DataTime=0.433 Loss=1.677 Prec@1=59.209 Prec@5=82.698 rate=6093.81 Hz, eta=0:00:00, total=0:00:00, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.660 DataTime=0.433 Loss=1.677 Prec@1=59.209 Prec@5=82.698 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.660 DataTime=0.433 Loss=1.677 Prec@1=59.209 Prec@5=82.698 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=23:17 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.637 DataTime=0.412 Loss=1.680 Prec@1=59.212 Prec@5=82.704 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=23:17 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.637 DataTime=0.412 Loss=1.680 Prec@1=59.212 Prec@5=82.704 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=23:17 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.637 DataTime=0.412 Loss=1.680 Prec@1=59.212 Prec@5=82.704 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=23:18 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.629 DataTime=0.404 Loss=1.683 Prec@1=59.289 Prec@5=82.699 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=23:18 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.629 DataTime=0.404 Loss=1.683 Prec@1=59.289 Prec@5=82.699 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:18 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.629 DataTime=0.404 Loss=1.683 Prec@1=59.289 Prec@5=82.699 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:19 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.625 DataTime=0.400 Loss=1.684 Prec@1=59.343 Prec@5=82.671 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:19 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.625 DataTime=0.400 Loss=1.684 Prec@1=59.343 Prec@5=82.671 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=23:19 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.625 DataTime=0.400 Loss=1.684 Prec@1=59.343 Prec@5=82.671 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=23:20 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.623 DataTime=0.398 Loss=1.690 Prec@1=59.278 Prec@5=82.622 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=23:20 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.623 DataTime=0.398 Loss=1.690 Prec@1=59.278 Prec@5=82.622 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=23:20 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.623 DataTime=0.398 Loss=1.690 Prec@1=59.278 Prec@5=82.622 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=23:21 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.621 DataTime=0.397 Loss=1.693 Prec@1=59.218 Prec@5=82.551 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=23:21 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.621 DataTime=0.397 Loss=1.693 Prec@1=59.218 Prec@5=82.551 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=23:21 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.621 DataTime=0.397 Loss=1.693 Prec@1=59.218 Prec@5=82.551 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=23:22 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.620 DataTime=0.396 Loss=1.691 Prec@1=59.271 Prec@5=82.614 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=23:22 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.620 DataTime=0.396 Loss=1.691 Prec@1=59.271 Prec@5=82.614 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=23:22 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.620 DataTime=0.396 Loss=1.691 Prec@1=59.271 Prec@5=82.614 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=23:23 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.690 Prec@1=59.340 Prec@5=82.615 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=23:23 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.690 Prec@1=59.340 Prec@5=82.615 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:23 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.690 Prec@1=59.340 Prec@5=82.615 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:24 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.689 Prec@1=59.349 Prec@5=82.647 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:24 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.689 Prec@1=59.349 Prec@5=82.647 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:24 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.619 DataTime=0.395 Loss=1.689 Prec@1=59.349 Prec@5=82.647 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:25 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.690 Prec@1=59.338 Prec@5=82.625 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:25 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.690 Prec@1=59.338 Prec@5=82.625 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:25 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.690 Prec@1=59.338 Prec@5=82.625 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:26 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.692 Prec@1=59.307 Prec@5=82.587 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:26 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.692 Prec@1=59.307 Prec@5=82.587 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:26 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.618 DataTime=0.394 Loss=1.692 Prec@1=59.307 Prec@5=82.587 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:27 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.394 Loss=1.694 Prec@1=59.253 Prec@5=82.557 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:27 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.394 Loss=1.694 Prec@1=59.253 Prec@5=82.557 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:27 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.394 Loss=1.694 Prec@1=59.253 Prec@5=82.557 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:28 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.695 Prec@1=59.212 Prec@5=82.543 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:28 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.695 Prec@1=59.212 Prec@5=82.543 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=23:28 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.695 Prec@1=59.212 Prec@5=82.543 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=23:29 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.696 Prec@1=59.195 Prec@5=82.522 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=23:29 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.696 Prec@1=59.195 Prec@5=82.522 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=23:29 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.696 Prec@1=59.195 Prec@5=82.522 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=23:31 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.698 Prec@1=59.147 Prec@5=82.497 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=23:31 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.698 Prec@1=59.147 Prec@5=82.497 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:31 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.698 Prec@1=59.147 Prec@5=82.497 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:32 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.699 Prec@1=59.127 Prec@5=82.498 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:32 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.699 Prec@1=59.127 Prec@5=82.498 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:32 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.617 DataTime=0.393 Loss=1.699 Prec@1=59.127 Prec@5=82.498 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:33 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.097 Prec@5=82.472 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:33 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.097 Prec@5=82.472 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:33 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.097 Prec@5=82.472 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:34 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.099 Prec@5=82.476 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:34 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.099 Prec@5=82.476 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:34 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.701 Prec@1=59.099 Prec@5=82.476 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:35 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.702 Prec@1=59.088 Prec@5=82.456 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:35 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.702 Prec@1=59.088 Prec@5=82.456 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:35 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.702 Prec@1=59.088 Prec@5=82.456 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:36 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.703 Prec@1=59.054 Prec@5=82.446 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:36 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.703 Prec@1=59.054 Prec@5=82.446 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:36 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.703 Prec@1=59.054 Prec@5=82.446 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:37 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.704 Prec@1=59.042 Prec@5=82.431 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:37 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.704 Prec@1=59.042 Prec@5=82.431 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:37 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.704 Prec@1=59.042 Prec@5=82.431 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:38 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.014 Prec@5=82.415 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:38 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.014 Prec@5=82.415 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:38 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.014 Prec@5=82.415 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:39 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.005 Prec@5=82.411 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:39 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.005 Prec@5=82.411 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:39 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.705 Prec@1=59.005 Prec@5=82.411 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:40 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.002 Prec@5=82.414 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:40 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.002 Prec@5=82.414 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:40 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.002 Prec@5=82.414 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:41 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.008 Prec@5=82.408 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:41 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.008 Prec@5=82.408 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:41 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.616 DataTime=0.392 Loss=1.706 Prec@1=59.008 Prec@5=82.408 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:41 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.615 DataTime=0.392 Loss=1.706 Prec@1=59.007 Prec@5=82.406 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:41 IST=> training   100.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.615 DataTime=0.392 Loss=1.706 Prec@1=59.007 Prec@5=82.406 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=23:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST=> validation 0.00% of 1x98...Epoch=21/150 LR=0.09568 Time=6.757 Loss=1.652 Prec@1=58.008 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=23:41 IST=> validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=6.757 Loss=1.652 Prec@1=58.008 Prec@5=84.180 rate=3700.17 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=6.757 Loss=1.652 Prec@1=58.008 Prec@5=84.180 rate=3700.17 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=0.405 Loss=1.737 Prec@1=57.976 Prec@5=82.214 rate=3700.17 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST** validation 100.00% of 1x98...Epoch=21/150 LR=0.09568 Time=0.405 Loss=1.737 Prec@1=57.976 Prec@5=82.214 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=23:41 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST=> training   0.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.253 DataTime=4.957 Loss=1.553 Prec@1=63.477 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=23:42 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.253 DataTime=4.957 Loss=1.553 Prec@1=63.477 Prec@5=85.547 rate=2673.26 Hz, eta=0:00:00, total=0:00:00, wall=23:42 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.253 DataTime=4.957 Loss=1.553 Prec@1=63.477 Prec@5=85.547 rate=2673.26 Hz, eta=0:00:00, total=0:00:00, wall=23:43 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.654 DataTime=0.429 Loss=1.665 Prec@1=59.909 Prec@5=82.923 rate=2673.26 Hz, eta=0:00:00, total=0:00:00, wall=23:43 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.654 DataTime=0.429 Loss=1.665 Prec@1=59.909 Prec@5=82.923 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=23:43 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.654 DataTime=0.429 Loss=1.665 Prec@1=59.909 Prec@5=82.923 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=23:44 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.633 DataTime=0.409 Loss=1.663 Prec@1=59.932 Prec@5=82.896 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=23:44 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.633 DataTime=0.409 Loss=1.663 Prec@1=59.932 Prec@5=82.896 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=23:44 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.633 DataTime=0.409 Loss=1.663 Prec@1=59.932 Prec@5=82.896 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=23:45 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.626 DataTime=0.402 Loss=1.665 Prec@1=59.860 Prec@5=82.942 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=23:45 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.626 DataTime=0.402 Loss=1.665 Prec@1=59.860 Prec@5=82.942 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=23:45 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.626 DataTime=0.402 Loss=1.665 Prec@1=59.860 Prec@5=82.942 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=23:46 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.623 DataTime=0.399 Loss=1.670 Prec@1=59.748 Prec@5=82.886 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=23:46 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.623 DataTime=0.399 Loss=1.670 Prec@1=59.748 Prec@5=82.886 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=23:46 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.623 DataTime=0.399 Loss=1.670 Prec@1=59.748 Prec@5=82.886 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=23:47 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.621 DataTime=0.397 Loss=1.674 Prec@1=59.638 Prec@5=82.822 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=23:47 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.621 DataTime=0.397 Loss=1.674 Prec@1=59.638 Prec@5=82.822 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=23:47 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.621 DataTime=0.397 Loss=1.674 Prec@1=59.638 Prec@5=82.822 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=23:48 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.396 Loss=1.674 Prec@1=59.611 Prec@5=82.855 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=23:48 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.396 Loss=1.674 Prec@1=59.611 Prec@5=82.855 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=23:48 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.396 Loss=1.674 Prec@1=59.611 Prec@5=82.855 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=23:49 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.395 Loss=1.676 Prec@1=59.571 Prec@5=82.843 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=23:49 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.395 Loss=1.676 Prec@1=59.571 Prec@5=82.843 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=23:49 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.619 DataTime=0.395 Loss=1.676 Prec@1=59.571 Prec@5=82.843 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=23:50 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.395 Loss=1.678 Prec@1=59.518 Prec@5=82.812 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=23:50 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.395 Loss=1.678 Prec@1=59.518 Prec@5=82.812 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:50 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.395 Loss=1.678 Prec@1=59.518 Prec@5=82.812 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:51 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.394 Loss=1.679 Prec@1=59.506 Prec@5=82.795 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:51 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.394 Loss=1.679 Prec@1=59.506 Prec@5=82.795 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:51 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.618 DataTime=0.394 Loss=1.679 Prec@1=59.506 Prec@5=82.795 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:52 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.489 Prec@5=82.806 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:52 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.489 Prec@5=82.806 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=23:52 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.489 Prec@5=82.806 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=23:53 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.491 Prec@5=82.783 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=23:53 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.491 Prec@5=82.783 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:53 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.679 Prec@1=59.491 Prec@5=82.783 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:54 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.478 Prec@5=82.766 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:54 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.478 Prec@5=82.766 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:54 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.478 Prec@5=82.766 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:55 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.474 Prec@5=82.771 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:55 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.474 Prec@5=82.771 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:55 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.681 Prec@1=59.474 Prec@5=82.771 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:56 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.682 Prec@1=59.435 Prec@5=82.745 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:56 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.682 Prec@1=59.435 Prec@5=82.745 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:56 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.617 DataTime=0.393 Loss=1.682 Prec@1=59.435 Prec@5=82.745 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:57 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.417 Prec@5=82.719 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:57 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.417 Prec@5=82.719 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=23:57 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.417 Prec@5=82.719 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=23:58 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.406 Prec@5=82.725 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=23:58 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.406 Prec@5=82.725 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:58 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.406 Prec@5=82.725 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:59 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.403 Prec@5=82.726 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:59 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.403 Prec@5=82.726 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=23:59 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.403 Prec@5=82.726 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=00:00 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.396 Prec@5=82.714 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=00:00 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.396 Prec@5=82.714 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:00 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.683 Prec@1=59.396 Prec@5=82.714 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:01 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.386 Prec@5=82.718 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:01 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.386 Prec@5=82.718 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=00:01 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.386 Prec@5=82.718 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=00:02 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.382 Prec@5=82.717 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=00:02 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.382 Prec@5=82.717 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=00:02 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.684 Prec@1=59.382 Prec@5=82.717 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=00:03 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.685 Prec@1=59.366 Prec@5=82.708 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=00:03 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.685 Prec@1=59.366 Prec@5=82.708 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=00:03 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.685 Prec@1=59.366 Prec@5=82.708 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=00:04 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.686 Prec@1=59.338 Prec@5=82.686 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=00:04 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.686 Prec@1=59.338 Prec@5=82.686 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=00:04 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.616 DataTime=0.392 Loss=1.686 Prec@1=59.338 Prec@5=82.686 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=00:05 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.392 Loss=1.687 Prec@1=59.324 Prec@5=82.685 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=00:05 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.392 Loss=1.687 Prec@1=59.324 Prec@5=82.685 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=00:05 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.392 Loss=1.687 Prec@1=59.324 Prec@5=82.685 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=00:06 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.686 Prec@1=59.324 Prec@5=82.682 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.686 Prec@1=59.324 Prec@5=82.682 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.686 Prec@1=59.324 Prec@5=82.682 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=00:07 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.688 Prec@1=59.297 Prec@5=82.672 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=00:07 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.688 Prec@1=59.297 Prec@5=82.672 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=00:07 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.688 Prec@1=59.297 Prec@5=82.672 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=00:07 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.688 Prec@1=59.297 Prec@5=82.672 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=00:07 IST=> training   100.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.615 DataTime=0.391 Loss=1.688 Prec@1=59.297 Prec@5=82.672 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=00:07 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 0.00% of 1x98...Epoch=22/150 LR=0.09524 Time=9.314 Loss=1.609 Prec@1=60.352 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=9.314 Loss=1.609 Prec@1=60.352 Prec@5=84.961 rate=4732.61 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=9.314 Loss=1.609 Prec@1=60.352 Prec@5=84.961 rate=4732.61 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=0.436 Loss=1.700 Prec@1=58.654 Prec@5=82.650 rate=4732.61 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST** validation 100.00% of 1x98...Epoch=22/150 LR=0.09524 Time=0.436 Loss=1.700 Prec@1=58.654 Prec@5=82.650 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=00:08 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:08 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:08 IST=> training   0.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.563 DataTime=5.278 Loss=1.641 Prec@1=61.328 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=00:08 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.563 DataTime=5.278 Loss=1.641 Prec@1=61.328 Prec@5=84.375 rate=5232.20 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.563 DataTime=5.278 Loss=1.641 Prec@1=61.328 Prec@5=84.375 rate=5232.20 Hz, eta=0:00:00, total=0:00:00, wall=00:09 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.659 DataTime=0.435 Loss=1.646 Prec@1=60.141 Prec@5=83.346 rate=5232.20 Hz, eta=0:00:00, total=0:00:00, wall=00:09 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.659 DataTime=0.435 Loss=1.646 Prec@1=60.141 Prec@5=83.346 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=00:09 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.659 DataTime=0.435 Loss=1.646 Prec@1=60.141 Prec@5=83.346 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=00:10 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.635 DataTime=0.412 Loss=1.654 Prec@1=60.003 Prec@5=83.193 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=00:10 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.635 DataTime=0.412 Loss=1.654 Prec@1=60.003 Prec@5=83.193 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=00:10 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.635 DataTime=0.412 Loss=1.654 Prec@1=60.003 Prec@5=83.193 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=00:11 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.627 DataTime=0.404 Loss=1.651 Prec@1=59.968 Prec@5=83.193 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=00:11 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.627 DataTime=0.404 Loss=1.651 Prec@1=59.968 Prec@5=83.193 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=00:11 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.627 DataTime=0.404 Loss=1.651 Prec@1=59.968 Prec@5=83.193 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=00:12 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.623 DataTime=0.401 Loss=1.648 Prec@1=60.007 Prec@5=83.217 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=00:12 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.623 DataTime=0.401 Loss=1.648 Prec@1=60.007 Prec@5=83.217 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=00:12 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.623 DataTime=0.401 Loss=1.648 Prec@1=60.007 Prec@5=83.217 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=00:13 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.621 DataTime=0.399 Loss=1.651 Prec@1=59.927 Prec@5=83.193 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=00:13 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.621 DataTime=0.399 Loss=1.651 Prec@1=59.927 Prec@5=83.193 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=00:13 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.621 DataTime=0.399 Loss=1.651 Prec@1=59.927 Prec@5=83.193 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=00:14 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.620 DataTime=0.397 Loss=1.653 Prec@1=59.929 Prec@5=83.175 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=00:14 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.620 DataTime=0.397 Loss=1.653 Prec@1=59.929 Prec@5=83.175 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=00:14 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.620 DataTime=0.397 Loss=1.653 Prec@1=59.929 Prec@5=83.175 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=00:15 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.655 Prec@1=59.885 Prec@5=83.155 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=00:15 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.655 Prec@1=59.885 Prec@5=83.155 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=00:15 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.655 Prec@1=59.885 Prec@5=83.155 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=00:16 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.657 Prec@1=59.829 Prec@5=83.099 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=00:16 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.657 Prec@1=59.829 Prec@5=83.099 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:16 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.619 DataTime=0.396 Loss=1.657 Prec@1=59.829 Prec@5=83.099 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:17 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.818 Prec@5=83.089 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:17 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.818 Prec@5=83.089 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=00:17 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.818 Prec@5=83.089 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=00:18 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.816 Prec@5=83.089 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=00:18 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.816 Prec@5=83.089 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:18 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.395 Loss=1.658 Prec@1=59.816 Prec@5=83.089 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:19 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.659 Prec@1=59.779 Prec@5=83.097 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:19 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.659 Prec@1=59.779 Prec@5=83.097 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:19 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.659 Prec@1=59.779 Prec@5=83.097 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:20 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.661 Prec@1=59.744 Prec@5=83.077 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:20 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.661 Prec@1=59.744 Prec@5=83.077 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=00:20 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.618 DataTime=0.394 Loss=1.661 Prec@1=59.744 Prec@5=83.077 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=00:21 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.394 Loss=1.660 Prec@1=59.773 Prec@5=83.094 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=00:21 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.394 Loss=1.660 Prec@1=59.773 Prec@5=83.094 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=00:21 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.394 Loss=1.660 Prec@1=59.773 Prec@5=83.094 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=00:22 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.780 Prec@5=83.087 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=00:22 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.780 Prec@5=83.087 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=00:22 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.780 Prec@5=83.087 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=00:23 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.789 Prec@5=83.081 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.789 Prec@5=83.081 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.789 Prec@5=83.081 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=00:24 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.803 Prec@5=83.085 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=00:24 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.803 Prec@5=83.085 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=00:24 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.661 Prec@1=59.803 Prec@5=83.085 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=00:25 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.663 Prec@1=59.771 Prec@5=83.065 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=00:25 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.663 Prec@1=59.771 Prec@5=83.065 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=00:25 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.617 DataTime=0.393 Loss=1.663 Prec@1=59.771 Prec@5=83.065 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=00:26 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.664 Prec@1=59.756 Prec@5=83.042 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=00:26 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.664 Prec@1=59.756 Prec@5=83.042 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:26 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.664 Prec@1=59.756 Prec@5=83.042 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:28 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.666 Prec@1=59.734 Prec@5=83.014 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=00:28 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.666 Prec@1=59.734 Prec@5=83.014 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:28 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.393 Loss=1.666 Prec@1=59.734 Prec@5=83.014 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:29 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.701 Prec@5=82.988 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:29 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.701 Prec@5=82.988 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=00:29 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.701 Prec@5=82.988 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=00:30 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.707 Prec@5=82.996 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=00:30 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.707 Prec@5=82.996 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:30 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.667 Prec@1=59.707 Prec@5=82.996 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:31 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.668 Prec@1=59.692 Prec@5=82.978 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:31 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.668 Prec@1=59.692 Prec@5=82.978 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=00:31 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.668 Prec@1=59.692 Prec@5=82.978 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=00:32 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.669 Prec@1=59.678 Prec@5=82.958 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=00:32 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.669 Prec@1=59.678 Prec@5=82.958 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=00:32 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.669 Prec@1=59.678 Prec@5=82.958 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=00:33 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.681 Prec@5=82.959 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=00:33 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.681 Prec@5=82.959 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=00:33 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.681 Prec@5=82.959 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=00:34 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.668 Prec@5=82.953 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=00:34 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.668 Prec@5=82.953 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=00:34 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.668 Prec@5=82.953 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=00:34 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.668 Prec@5=82.952 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=00:34 IST=> training   100.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.616 DataTime=0.392 Loss=1.670 Prec@1=59.668 Prec@5=82.952 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=00:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> validation 0.00% of 1x98...Epoch=23/150 LR=0.09479 Time=6.986 Loss=1.732 Prec@1=57.227 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=6.986 Loss=1.732 Prec@1=57.227 Prec@5=83.203 rate=4414.62 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=6.986 Loss=1.732 Prec@1=57.227 Prec@5=83.203 rate=4414.62 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=0.413 Loss=1.753 Prec@1=57.752 Prec@5=82.262 rate=4414.62 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST** validation 100.00% of 1x98...Epoch=23/150 LR=0.09479 Time=0.413 Loss=1.753 Prec@1=57.752 Prec@5=82.262 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=00:34 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> training   0.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.945 DataTime=5.655 Loss=1.454 Prec@1=63.281 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=00:34 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.945 DataTime=5.655 Loss=1.454 Prec@1=63.281 Prec@5=84.961 rate=7689.58 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=5.945 DataTime=5.655 Loss=1.454 Prec@1=63.281 Prec@5=84.961 rate=7689.58 Hz, eta=0:00:00, total=0:00:00, wall=00:36 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.664 DataTime=0.440 Loss=1.640 Prec@1=60.528 Prec@5=83.116 rate=7689.58 Hz, eta=0:00:00, total=0:00:00, wall=00:36 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.664 DataTime=0.440 Loss=1.640 Prec@1=60.528 Prec@5=83.116 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=00:36 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.664 DataTime=0.440 Loss=1.640 Prec@1=60.528 Prec@5=83.116 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=00:37 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.640 DataTime=0.416 Loss=1.628 Prec@1=60.652 Prec@5=83.354 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=00:37 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.640 DataTime=0.416 Loss=1.628 Prec@1=60.652 Prec@5=83.354 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=00:37 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.640 DataTime=0.416 Loss=1.628 Prec@1=60.652 Prec@5=83.354 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=00:38 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.632 DataTime=0.407 Loss=1.628 Prec@1=60.614 Prec@5=83.448 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=00:38 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.632 DataTime=0.407 Loss=1.628 Prec@1=60.614 Prec@5=83.448 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=00:38 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.632 DataTime=0.407 Loss=1.628 Prec@1=60.614 Prec@5=83.448 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=00:39 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.627 DataTime=0.403 Loss=1.625 Prec@1=60.658 Prec@5=83.505 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=00:39 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.627 DataTime=0.403 Loss=1.625 Prec@1=60.658 Prec@5=83.505 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=00:39 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.627 DataTime=0.403 Loss=1.625 Prec@1=60.658 Prec@5=83.505 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=00:40 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.625 DataTime=0.401 Loss=1.630 Prec@1=60.554 Prec@5=83.434 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=00:40 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.625 DataTime=0.401 Loss=1.630 Prec@1=60.554 Prec@5=83.434 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:40 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.625 DataTime=0.401 Loss=1.630 Prec@1=60.554 Prec@5=83.434 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:41 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.623 DataTime=0.399 Loss=1.632 Prec@1=60.483 Prec@5=83.435 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:41 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.623 DataTime=0.399 Loss=1.632 Prec@1=60.483 Prec@5=83.435 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:41 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.623 DataTime=0.399 Loss=1.632 Prec@1=60.483 Prec@5=83.435 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:42 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.622 DataTime=0.398 Loss=1.633 Prec@1=60.460 Prec@5=83.448 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:42 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.622 DataTime=0.398 Loss=1.633 Prec@1=60.460 Prec@5=83.448 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=00:42 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.622 DataTime=0.398 Loss=1.633 Prec@1=60.460 Prec@5=83.448 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=00:43 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.621 DataTime=0.397 Loss=1.636 Prec@1=60.398 Prec@5=83.426 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=00:43 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.621 DataTime=0.397 Loss=1.636 Prec@1=60.398 Prec@5=83.426 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:43 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.621 DataTime=0.397 Loss=1.636 Prec@1=60.398 Prec@5=83.426 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:44 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.396 Loss=1.637 Prec@1=60.346 Prec@5=83.409 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:44 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.396 Loss=1.637 Prec@1=60.346 Prec@5=83.409 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:44 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.396 Loss=1.637 Prec@1=60.346 Prec@5=83.409 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:45 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.395 Loss=1.639 Prec@1=60.278 Prec@5=83.380 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:45 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.395 Loss=1.639 Prec@1=60.278 Prec@5=83.380 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:45 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.620 DataTime=0.395 Loss=1.639 Prec@1=60.278 Prec@5=83.380 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:46 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.395 Loss=1.639 Prec@1=60.264 Prec@5=83.367 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:46 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.395 Loss=1.639 Prec@1=60.264 Prec@5=83.367 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:46 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.395 Loss=1.639 Prec@1=60.264 Prec@5=83.367 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:47 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.642 Prec@1=60.216 Prec@5=83.328 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:47 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.642 Prec@1=60.216 Prec@5=83.328 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:47 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.642 Prec@1=60.216 Prec@5=83.328 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:48 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.644 Prec@1=60.197 Prec@5=83.308 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:48 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.644 Prec@1=60.197 Prec@5=83.308 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=00:48 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.619 DataTime=0.394 Loss=1.644 Prec@1=60.197 Prec@5=83.308 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=00:49 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.394 Loss=1.646 Prec@1=60.172 Prec@5=83.283 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=00:49 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.394 Loss=1.646 Prec@1=60.172 Prec@5=83.283 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:49 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.394 Loss=1.646 Prec@1=60.172 Prec@5=83.283 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:50 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.648 Prec@1=60.148 Prec@5=83.249 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:50 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.648 Prec@1=60.148 Prec@5=83.249 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:50 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.648 Prec@1=60.148 Prec@5=83.249 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:51 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.138 Prec@5=83.232 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:51 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.138 Prec@5=83.232 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:51 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.138 Prec@5=83.232 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:52 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.121 Prec@5=83.220 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:52 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.121 Prec@5=83.220 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:52 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.649 Prec@1=60.121 Prec@5=83.220 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:53 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.650 Prec@1=60.110 Prec@5=83.211 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:53 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.650 Prec@1=60.110 Prec@5=83.211 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:53 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.618 DataTime=0.393 Loss=1.650 Prec@1=60.110 Prec@5=83.211 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:54 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.652 Prec@1=60.073 Prec@5=83.184 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:54 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.652 Prec@1=60.073 Prec@5=83.184 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:54 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.652 Prec@1=60.073 Prec@5=83.184 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:55 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.049 Prec@5=83.178 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:55 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.049 Prec@5=83.178 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:55 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.049 Prec@5=83.178 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:56 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.032 Prec@5=83.174 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:56 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.032 Prec@5=83.174 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=00:56 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.653 Prec@1=60.032 Prec@5=83.174 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=00:57 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.654 Prec@1=60.020 Prec@5=83.167 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=00:57 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.654 Prec@1=60.020 Prec@5=83.167 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:57 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.393 Loss=1.654 Prec@1=60.020 Prec@5=83.167 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:58 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.004 Prec@5=83.153 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:58 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.004 Prec@5=83.153 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:58 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.004 Prec@5=83.153 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:59 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.001 Prec@5=83.152 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:59 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.001 Prec@5=83.152 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=00:59 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.655 Prec@1=60.001 Prec@5=83.152 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=01:00 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.656 Prec@1=59.991 Prec@5=83.146 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=01:00 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.656 Prec@1=59.991 Prec@5=83.146 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=01:00 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.656 Prec@1=59.991 Prec@5=83.146 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=01:00 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.656 Prec@1=59.991 Prec@5=83.146 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=01:00 IST=> training   100.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.617 DataTime=0.392 Loss=1.656 Prec@1=59.991 Prec@5=83.146 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=01:00 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST=> validation 0.00% of 1x98...Epoch=24/150 LR=0.09431 Time=6.169 Loss=1.838 Prec@1=56.445 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST=> validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.169 Loss=1.838 Prec@1=56.445 Prec@5=80.273 rate=6314.88 Hz, eta=0:00:00, total=0:00:00, wall=01:00 IST** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.169 Loss=1.838 Prec@1=56.445 Prec@5=80.273 rate=6314.88 Hz, eta=0:00:00, total=0:00:00, wall=01:01 IST** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=0.399 Loss=1.719 Prec@1=58.058 Prec@5=82.560 rate=6314.88 Hz, eta=0:00:00, total=0:00:00, wall=01:01 IST** validation 100.00% of 1x98...Epoch=24/150 LR=0.09431 Time=0.399 Loss=1.719 Prec@1=58.058 Prec@5=82.560 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=01:01 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST=> training   0.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=5.993 DataTime=5.755 Loss=1.595 Prec@1=61.914 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=5.993 DataTime=5.755 Loss=1.595 Prec@1=61.914 Prec@5=83.984 rate=6959.48 Hz, eta=0:00:00, total=0:00:00, wall=01:01 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=5.993 DataTime=5.755 Loss=1.595 Prec@1=61.914 Prec@5=83.984 rate=6959.48 Hz, eta=0:00:00, total=0:00:00, wall=01:02 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.663 DataTime=0.441 Loss=1.602 Prec@1=61.440 Prec@5=83.834 rate=6959.48 Hz, eta=0:00:00, total=0:00:00, wall=01:02 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.663 DataTime=0.441 Loss=1.602 Prec@1=61.440 Prec@5=83.834 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=01:02 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.663 DataTime=0.441 Loss=1.602 Prec@1=61.440 Prec@5=83.834 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=01:03 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.640 DataTime=0.415 Loss=1.609 Prec@1=61.046 Prec@5=83.759 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=01:03 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.640 DataTime=0.415 Loss=1.609 Prec@1=61.046 Prec@5=83.759 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=01:03 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.640 DataTime=0.415 Loss=1.609 Prec@1=61.046 Prec@5=83.759 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=01:04 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.632 DataTime=0.407 Loss=1.610 Prec@1=60.933 Prec@5=83.831 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=01:04 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.632 DataTime=0.407 Loss=1.610 Prec@1=60.933 Prec@5=83.831 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=01:04 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.632 DataTime=0.407 Loss=1.610 Prec@1=60.933 Prec@5=83.831 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=01:05 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.627 DataTime=0.402 Loss=1.613 Prec@1=60.803 Prec@5=83.806 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=01:05 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.627 DataTime=0.402 Loss=1.613 Prec@1=60.803 Prec@5=83.806 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:05 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.627 DataTime=0.402 Loss=1.613 Prec@1=60.803 Prec@5=83.806 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:06 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.625 DataTime=0.400 Loss=1.614 Prec@1=60.814 Prec@5=83.758 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:06 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.625 DataTime=0.400 Loss=1.614 Prec@1=60.814 Prec@5=83.758 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=01:06 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.625 DataTime=0.400 Loss=1.614 Prec@1=60.814 Prec@5=83.758 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=01:07 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.623 DataTime=0.399 Loss=1.616 Prec@1=60.762 Prec@5=83.720 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=01:07 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.623 DataTime=0.399 Loss=1.616 Prec@1=60.762 Prec@5=83.720 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:07 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.623 DataTime=0.399 Loss=1.616 Prec@1=60.762 Prec@5=83.720 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:08 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.622 DataTime=0.398 Loss=1.619 Prec@1=60.703 Prec@5=83.683 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:08 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.622 DataTime=0.398 Loss=1.619 Prec@1=60.703 Prec@5=83.683 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:08 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.622 DataTime=0.398 Loss=1.619 Prec@1=60.703 Prec@5=83.683 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:09 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.621 DataTime=0.397 Loss=1.620 Prec@1=60.676 Prec@5=83.674 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:09 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.621 DataTime=0.397 Loss=1.620 Prec@1=60.676 Prec@5=83.674 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:09 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.621 DataTime=0.397 Loss=1.620 Prec@1=60.676 Prec@5=83.674 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:10 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.620 DataTime=0.396 Loss=1.620 Prec@1=60.681 Prec@5=83.662 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:10 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.620 DataTime=0.396 Loss=1.620 Prec@1=60.681 Prec@5=83.662 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:10 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.620 DataTime=0.396 Loss=1.620 Prec@1=60.681 Prec@5=83.662 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:11 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.622 Prec@1=60.638 Prec@5=83.627 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:11 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.622 Prec@1=60.638 Prec@5=83.627 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:11 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.622 Prec@1=60.638 Prec@5=83.627 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:12 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.625 Prec@1=60.615 Prec@5=83.593 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:12 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.625 Prec@1=60.615 Prec@5=83.593 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:12 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.619 DataTime=0.395 Loss=1.625 Prec@1=60.615 Prec@5=83.593 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:13 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.627 Prec@1=60.581 Prec@5=83.552 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:13 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.627 Prec@1=60.581 Prec@5=83.552 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:13 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.627 Prec@1=60.581 Prec@5=83.552 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:14 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.529 Prec@5=83.528 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.529 Prec@5=83.528 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.529 Prec@5=83.528 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:15 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.541 Prec@5=83.535 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:15 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.541 Prec@5=83.535 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:15 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.394 Loss=1.629 Prec@1=60.541 Prec@5=83.535 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:16 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.393 Loss=1.630 Prec@1=60.507 Prec@5=83.522 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:16 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.393 Loss=1.630 Prec@1=60.507 Prec@5=83.522 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:16 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.618 DataTime=0.393 Loss=1.630 Prec@1=60.507 Prec@5=83.522 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:17 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.632 Prec@1=60.485 Prec@5=83.499 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:17 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.632 Prec@1=60.485 Prec@5=83.499 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:17 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.632 Prec@1=60.485 Prec@5=83.499 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:18 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.633 Prec@1=60.445 Prec@5=83.476 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:18 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.633 Prec@1=60.445 Prec@5=83.476 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:18 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.633 Prec@1=60.445 Prec@5=83.476 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:19 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.634 Prec@1=60.441 Prec@5=83.462 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:19 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.634 Prec@1=60.441 Prec@5=83.462 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=01:19 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.634 Prec@1=60.441 Prec@5=83.462 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=01:20 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.635 Prec@1=60.427 Prec@5=83.445 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=01:20 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.635 Prec@1=60.427 Prec@5=83.445 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:20 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.635 Prec@1=60.427 Prec@5=83.445 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:21 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.637 Prec@1=60.395 Prec@5=83.426 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:21 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.637 Prec@1=60.395 Prec@5=83.426 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:21 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.393 Loss=1.637 Prec@1=60.395 Prec@5=83.426 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:22 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.638 Prec@1=60.378 Prec@5=83.405 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:22 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.638 Prec@1=60.378 Prec@5=83.405 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:22 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.638 Prec@1=60.378 Prec@5=83.405 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:23 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.364 Prec@5=83.398 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:23 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.364 Prec@5=83.398 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:23 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.364 Prec@5=83.398 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:24 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.366 Prec@5=83.397 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:24 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.366 Prec@5=83.397 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:24 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.617 DataTime=0.392 Loss=1.639 Prec@1=60.366 Prec@5=83.397 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:25 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.639 Prec@1=60.365 Prec@5=83.386 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:25 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.639 Prec@1=60.365 Prec@5=83.386 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:25 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.639 Prec@1=60.365 Prec@5=83.386 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:27 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.640 Prec@1=60.356 Prec@5=83.371 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:27 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.640 Prec@1=60.356 Prec@5=83.371 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:27 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.640 Prec@1=60.356 Prec@5=83.371 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:27 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.640 Prec@1=60.357 Prec@5=83.371 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:27 IST=> training   100.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.616 DataTime=0.392 Loss=1.640 Prec@1=60.357 Prec@5=83.371 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=01:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> validation 0.00% of 1x98...Epoch=25/150 LR=0.09382 Time=7.325 Loss=1.724 Prec@1=58.789 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=7.325 Loss=1.724 Prec@1=58.789 Prec@5=82.422 rate=2718.91 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=7.325 Loss=1.724 Prec@1=58.789 Prec@5=82.422 rate=2718.91 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=0.405 Loss=1.689 Prec@1=58.980 Prec@5=82.908 rate=2718.91 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST** validation 100.00% of 1x98...Epoch=25/150 LR=0.09382 Time=0.405 Loss=1.689 Prec@1=58.980 Prec@5=82.908 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=01:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> training   0.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=5.132 DataTime=4.821 Loss=1.593 Prec@1=61.914 Prec@5=85.352 rate=0 Hz, eta=?, total=0:00:00, wall=01:27 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=5.132 DataTime=4.821 Loss=1.593 Prec@1=61.914 Prec@5=85.352 rate=5106.81 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=5.132 DataTime=4.821 Loss=1.593 Prec@1=61.914 Prec@5=85.352 rate=5106.81 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.660 DataTime=0.437 Loss=1.595 Prec@1=61.481 Prec@5=84.025 rate=5106.81 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.660 DataTime=0.437 Loss=1.595 Prec@1=61.481 Prec@5=84.025 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=01:28 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.660 DataTime=0.437 Loss=1.595 Prec@1=61.481 Prec@5=84.025 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=01:29 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.637 DataTime=0.413 Loss=1.601 Prec@1=61.315 Prec@5=83.970 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=01:29 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.637 DataTime=0.413 Loss=1.601 Prec@1=61.315 Prec@5=83.970 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=01:29 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.637 DataTime=0.413 Loss=1.601 Prec@1=61.315 Prec@5=83.970 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=01:30 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.629 DataTime=0.404 Loss=1.599 Prec@1=61.242 Prec@5=83.960 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=01:30 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.629 DataTime=0.404 Loss=1.599 Prec@1=61.242 Prec@5=83.960 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=01:30 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.629 DataTime=0.404 Loss=1.599 Prec@1=61.242 Prec@5=83.960 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=01:31 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.625 DataTime=0.401 Loss=1.599 Prec@1=61.165 Prec@5=83.950 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.625 DataTime=0.401 Loss=1.599 Prec@1=61.165 Prec@5=83.950 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.625 DataTime=0.401 Loss=1.599 Prec@1=61.165 Prec@5=83.950 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:32 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.623 DataTime=0.399 Loss=1.603 Prec@1=61.038 Prec@5=83.903 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=01:32 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.623 DataTime=0.399 Loss=1.603 Prec@1=61.038 Prec@5=83.903 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:32 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.623 DataTime=0.399 Loss=1.603 Prec@1=61.038 Prec@5=83.903 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:33 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.622 DataTime=0.398 Loss=1.602 Prec@1=61.075 Prec@5=83.915 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:33 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.622 DataTime=0.398 Loss=1.602 Prec@1=61.075 Prec@5=83.915 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:33 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.622 DataTime=0.398 Loss=1.602 Prec@1=61.075 Prec@5=83.915 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:35 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.621 DataTime=0.396 Loss=1.606 Prec@1=60.985 Prec@5=83.875 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:35 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.621 DataTime=0.396 Loss=1.606 Prec@1=60.985 Prec@5=83.875 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:35 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.621 DataTime=0.396 Loss=1.606 Prec@1=60.985 Prec@5=83.875 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:36 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.620 DataTime=0.396 Loss=1.608 Prec@1=60.943 Prec@5=83.836 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:36 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.620 DataTime=0.396 Loss=1.608 Prec@1=60.943 Prec@5=83.836 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:36 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.620 DataTime=0.396 Loss=1.608 Prec@1=60.943 Prec@5=83.836 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:37 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.608 Prec@1=60.962 Prec@5=83.830 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:37 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.608 Prec@1=60.962 Prec@5=83.830 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=01:37 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.608 Prec@1=60.962 Prec@5=83.830 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=01:38 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.609 Prec@1=60.951 Prec@5=83.804 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=01:38 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.609 Prec@1=60.951 Prec@5=83.804 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:38 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.619 DataTime=0.395 Loss=1.609 Prec@1=60.951 Prec@5=83.804 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:39 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.609 Prec@1=60.937 Prec@5=83.799 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:39 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.609 Prec@1=60.937 Prec@5=83.799 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:39 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.609 Prec@1=60.937 Prec@5=83.799 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:40 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.892 Prec@5=83.785 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:40 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.892 Prec@5=83.785 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:40 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.892 Prec@5=83.785 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:41 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.878 Prec@5=83.783 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:41 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.878 Prec@5=83.783 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:41 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.394 Loss=1.611 Prec@1=60.878 Prec@5=83.783 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:42 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.393 Loss=1.614 Prec@1=60.823 Prec@5=83.742 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:42 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.393 Loss=1.614 Prec@1=60.823 Prec@5=83.742 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:42 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.618 DataTime=0.393 Loss=1.614 Prec@1=60.823 Prec@5=83.742 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:43 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.802 Prec@5=83.719 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:43 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.802 Prec@5=83.719 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:43 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.802 Prec@5=83.719 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:44 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.785 Prec@5=83.710 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:44 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.785 Prec@5=83.710 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:44 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.616 Prec@1=60.785 Prec@5=83.710 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:45 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.617 Prec@1=60.760 Prec@5=83.691 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:45 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.617 Prec@1=60.760 Prec@5=83.691 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:45 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.617 Prec@1=60.760 Prec@5=83.691 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:46 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.619 Prec@1=60.740 Prec@5=83.674 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:46 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.619 Prec@1=60.740 Prec@5=83.674 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:46 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.619 Prec@1=60.740 Prec@5=83.674 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:47 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.620 Prec@1=60.711 Prec@5=83.658 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:47 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.620 Prec@1=60.711 Prec@5=83.658 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:47 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.620 Prec@1=60.711 Prec@5=83.658 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:48 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.621 Prec@1=60.688 Prec@5=83.636 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:48 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.621 Prec@1=60.688 Prec@5=83.636 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:48 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.393 Loss=1.621 Prec@1=60.688 Prec@5=83.636 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:49 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.392 Loss=1.622 Prec@1=60.659 Prec@5=83.626 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:49 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.392 Loss=1.622 Prec@1=60.659 Prec@5=83.626 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:49 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.617 DataTime=0.392 Loss=1.622 Prec@1=60.659 Prec@5=83.626 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:50 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.623 Prec@1=60.650 Prec@5=83.606 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:50 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.623 Prec@1=60.650 Prec@5=83.606 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:50 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.623 Prec@1=60.650 Prec@5=83.606 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:51 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.633 Prec@5=83.599 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:51 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.633 Prec@5=83.599 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:51 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.633 Prec@5=83.599 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:52 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.632 Prec@5=83.598 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:52 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.632 Prec@5=83.598 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:52 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.624 Prec@1=60.632 Prec@5=83.598 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:53 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.625 Prec@1=60.631 Prec@5=83.584 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:53 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.625 Prec@1=60.631 Prec@5=83.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:53 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.625 Prec@1=60.631 Prec@5=83.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:53 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.625 Prec@1=60.633 Prec@5=83.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:53 IST=> training   100.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.616 DataTime=0.392 Loss=1.625 Prec@1=60.633 Prec@5=83.584 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=01:53 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST=> validation 0.00% of 1x98...Epoch=26/150 LR=0.09330 Time=5.902 Loss=1.764 Prec@1=57.812 Prec@5=82.031 rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST=> validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=5.902 Loss=1.764 Prec@1=57.812 Prec@5=82.031 rate=3407.43 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=5.902 Loss=1.764 Prec@1=57.812 Prec@5=82.031 rate=3407.43 Hz, eta=0:00:00, total=0:00:00, wall=01:54 IST** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=0.403 Loss=1.705 Prec@1=58.638 Prec@5=82.678 rate=3407.43 Hz, eta=0:00:00, total=0:00:00, wall=01:54 IST** validation 100.00% of 1x98...Epoch=26/150 LR=0.09330 Time=0.403 Loss=1.705 Prec@1=58.638 Prec@5=82.678 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=01:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST=> training   0.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.600 DataTime=5.314 Loss=1.526 Prec@1=64.453 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.600 DataTime=5.314 Loss=1.526 Prec@1=64.453 Prec@5=84.180 rate=2134.12 Hz, eta=0:00:01, total=0:00:00, wall=01:54 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.600 DataTime=5.314 Loss=1.526 Prec@1=64.453 Prec@5=84.180 rate=2134.12 Hz, eta=0:00:01, total=0:00:00, wall=01:55 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.660 DataTime=0.435 Loss=1.583 Prec@1=61.514 Prec@5=84.156 rate=2134.12 Hz, eta=0:00:01, total=0:00:00, wall=01:55 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.660 DataTime=0.435 Loss=1.583 Prec@1=61.514 Prec@5=84.156 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=01:55 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.660 DataTime=0.435 Loss=1.583 Prec@1=61.514 Prec@5=84.156 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=01:56 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.637 DataTime=0.412 Loss=1.580 Prec@1=61.544 Prec@5=84.249 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=01:56 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.637 DataTime=0.412 Loss=1.580 Prec@1=61.544 Prec@5=84.249 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:56 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.637 DataTime=0.412 Loss=1.580 Prec@1=61.544 Prec@5=84.249 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:57 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.630 DataTime=0.405 Loss=1.581 Prec@1=61.487 Prec@5=84.250 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:57 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.630 DataTime=0.405 Loss=1.581 Prec@1=61.487 Prec@5=84.250 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:57 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.630 DataTime=0.405 Loss=1.581 Prec@1=61.487 Prec@5=84.250 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:58 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.626 DataTime=0.401 Loss=1.584 Prec@1=61.364 Prec@5=84.179 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:58 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.626 DataTime=0.401 Loss=1.584 Prec@1=61.364 Prec@5=84.179 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=01:58 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.626 DataTime=0.401 Loss=1.584 Prec@1=61.364 Prec@5=84.179 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=01:59 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.624 DataTime=0.399 Loss=1.587 Prec@1=61.340 Prec@5=84.149 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=01:59 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.624 DataTime=0.399 Loss=1.587 Prec@1=61.340 Prec@5=84.149 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=01:59 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.624 DataTime=0.399 Loss=1.587 Prec@1=61.340 Prec@5=84.149 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:00 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.622 DataTime=0.397 Loss=1.589 Prec@1=61.284 Prec@5=84.122 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:00 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.622 DataTime=0.397 Loss=1.589 Prec@1=61.284 Prec@5=84.122 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:00 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.622 DataTime=0.397 Loss=1.589 Prec@1=61.284 Prec@5=84.122 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:01 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.621 DataTime=0.397 Loss=1.594 Prec@1=61.177 Prec@5=84.044 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:01 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.621 DataTime=0.397 Loss=1.594 Prec@1=61.177 Prec@5=84.044 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:01 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.621 DataTime=0.397 Loss=1.594 Prec@1=61.177 Prec@5=84.044 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:02 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.620 DataTime=0.396 Loss=1.596 Prec@1=61.181 Prec@5=84.016 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:02 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.620 DataTime=0.396 Loss=1.596 Prec@1=61.181 Prec@5=84.016 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:02 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.620 DataTime=0.396 Loss=1.596 Prec@1=61.181 Prec@5=84.016 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:03 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.597 Prec@1=61.160 Prec@5=84.002 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:03 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.597 Prec@1=61.160 Prec@5=84.002 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:03 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.597 Prec@1=61.160 Prec@5=84.002 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:04 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.598 Prec@1=61.155 Prec@5=83.995 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:04 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.598 Prec@1=61.155 Prec@5=83.995 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:04 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.619 DataTime=0.395 Loss=1.598 Prec@1=61.155 Prec@5=83.995 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:05 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.599 Prec@1=61.133 Prec@5=83.986 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:05 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.599 Prec@1=61.133 Prec@5=83.986 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:05 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.599 Prec@1=61.133 Prec@5=83.986 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:06 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.600 Prec@1=61.124 Prec@5=83.973 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:06 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.600 Prec@1=61.124 Prec@5=83.973 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:06 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.600 Prec@1=61.124 Prec@5=83.973 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:07 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.602 Prec@1=61.104 Prec@5=83.947 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:07 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.602 Prec@1=61.104 Prec@5=83.947 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:07 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.394 Loss=1.602 Prec@1=61.104 Prec@5=83.947 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:08 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.393 Loss=1.604 Prec@1=61.057 Prec@5=83.910 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:08 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.393 Loss=1.604 Prec@1=61.057 Prec@5=83.910 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:08 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.618 DataTime=0.393 Loss=1.604 Prec@1=61.057 Prec@5=83.910 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:09 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.605 Prec@1=61.036 Prec@5=83.894 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:09 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.605 Prec@1=61.036 Prec@5=83.894 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:09 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.605 Prec@1=61.036 Prec@5=83.894 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:10 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.035 Prec@5=83.886 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:10 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.035 Prec@5=83.886 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:10 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.035 Prec@5=83.886 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:11 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.018 Prec@5=83.871 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:11 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.018 Prec@5=83.871 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:11 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.606 Prec@1=61.018 Prec@5=83.871 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:12 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.608 Prec@1=61.003 Prec@5=83.861 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:12 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.608 Prec@1=61.003 Prec@5=83.861 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:12 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.617 DataTime=0.393 Loss=1.608 Prec@1=61.003 Prec@5=83.861 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:13 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.608 Prec@1=61.011 Prec@5=83.840 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:13 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.608 Prec@1=61.011 Prec@5=83.840 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=02:13 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.608 Prec@1=61.011 Prec@5=83.840 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=02:14 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.989 Prec@5=83.836 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=02:14 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.989 Prec@5=83.836 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:14 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.989 Prec@5=83.836 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:15 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.978 Prec@5=83.830 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:15 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.978 Prec@5=83.830 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:15 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.609 Prec@1=60.978 Prec@5=83.830 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:16 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.610 Prec@1=60.959 Prec@5=83.809 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:16 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.610 Prec@1=60.959 Prec@5=83.809 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:16 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.610 Prec@1=60.959 Prec@5=83.809 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:17 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.611 Prec@1=60.931 Prec@5=83.799 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:17 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.611 Prec@1=60.931 Prec@5=83.799 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:17 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.611 Prec@1=60.931 Prec@5=83.799 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:18 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.912 Prec@5=83.793 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:18 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.912 Prec@5=83.793 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:18 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.912 Prec@5=83.793 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:19 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.901 Prec@5=83.784 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:19 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.901 Prec@5=83.784 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:19 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.901 Prec@5=83.784 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:19 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.901 Prec@5=83.785 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:19 IST=> training   100.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.616 DataTime=0.392 Loss=1.612 Prec@1=60.901 Prec@5=83.785 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=02:19 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> validation 0.00% of 1x98...Epoch=27/150 LR=0.09277 Time=7.336 Loss=1.673 Prec@1=60.938 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=7.336 Loss=1.673 Prec@1=60.938 Prec@5=83.008 rate=4326.63 Hz, eta=0:00:00, total=0:00:00, wall=02:19 IST** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=7.336 Loss=1.673 Prec@1=60.938 Prec@5=83.008 rate=4326.63 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=0.414 Loss=1.668 Prec@1=59.430 Prec@5=83.558 rate=4326.63 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST** validation 100.00% of 1x98...Epoch=27/150 LR=0.09277 Time=0.414 Loss=1.668 Prec@1=59.430 Prec@5=83.558 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=02:20 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.104 DataTime=4.821 Loss=1.587 Prec@1=63.281 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.104 DataTime=4.821 Loss=1.587 Prec@1=63.281 Prec@5=83.984 rate=6605.63 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.104 DataTime=4.821 Loss=1.587 Prec@1=63.281 Prec@5=83.984 rate=6605.63 Hz, eta=0:00:00, total=0:00:00, wall=02:21 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.662 DataTime=0.436 Loss=1.570 Prec@1=61.603 Prec@5=84.443 rate=6605.63 Hz, eta=0:00:00, total=0:00:00, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.662 DataTime=0.436 Loss=1.570 Prec@1=61.603 Prec@5=84.443 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.662 DataTime=0.436 Loss=1.570 Prec@1=61.603 Prec@5=84.443 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=02:22 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.637 DataTime=0.413 Loss=1.570 Prec@1=61.759 Prec@5=84.465 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=02:22 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.637 DataTime=0.413 Loss=1.570 Prec@1=61.759 Prec@5=84.465 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=02:22 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.637 DataTime=0.413 Loss=1.570 Prec@1=61.759 Prec@5=84.465 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=02:23 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.629 DataTime=0.405 Loss=1.569 Prec@1=61.739 Prec@5=84.533 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=02:23 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.629 DataTime=0.405 Loss=1.569 Prec@1=61.739 Prec@5=84.533 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=02:23 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.629 DataTime=0.405 Loss=1.569 Prec@1=61.739 Prec@5=84.533 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=02:24 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.625 DataTime=0.401 Loss=1.571 Prec@1=61.732 Prec@5=84.465 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=02:24 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.625 DataTime=0.401 Loss=1.571 Prec@1=61.732 Prec@5=84.465 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:24 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.625 DataTime=0.401 Loss=1.571 Prec@1=61.732 Prec@5=84.465 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:25 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.623 DataTime=0.398 Loss=1.573 Prec@1=61.714 Prec@5=84.439 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:25 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.623 DataTime=0.398 Loss=1.573 Prec@1=61.714 Prec@5=84.439 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:25 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.623 DataTime=0.398 Loss=1.573 Prec@1=61.714 Prec@5=84.439 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:26 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.621 DataTime=0.397 Loss=1.575 Prec@1=61.682 Prec@5=84.349 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:26 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.621 DataTime=0.397 Loss=1.575 Prec@1=61.682 Prec@5=84.349 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:26 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.621 DataTime=0.397 Loss=1.575 Prec@1=61.682 Prec@5=84.349 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:27 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.620 DataTime=0.396 Loss=1.577 Prec@1=61.670 Prec@5=84.298 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:27 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.620 DataTime=0.396 Loss=1.577 Prec@1=61.670 Prec@5=84.298 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:27 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.620 DataTime=0.396 Loss=1.577 Prec@1=61.670 Prec@5=84.298 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:28 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.580 Prec@1=61.616 Prec@5=84.266 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:28 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.580 Prec@1=61.616 Prec@5=84.266 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:28 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.580 Prec@1=61.616 Prec@5=84.266 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:29 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.582 Prec@1=61.539 Prec@5=84.227 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:29 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.582 Prec@1=61.539 Prec@5=84.227 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:29 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.619 DataTime=0.395 Loss=1.582 Prec@1=61.539 Prec@5=84.227 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:30 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.583 Prec@1=61.537 Prec@5=84.201 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:30 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.583 Prec@1=61.537 Prec@5=84.201 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:30 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.583 Prec@1=61.537 Prec@5=84.201 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:31 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.584 Prec@1=61.514 Prec@5=84.168 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:31 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.584 Prec@1=61.514 Prec@5=84.168 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:31 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.584 Prec@1=61.514 Prec@5=84.168 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:32 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.586 Prec@1=61.468 Prec@5=84.141 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:32 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.586 Prec@1=61.468 Prec@5=84.141 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:32 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.618 DataTime=0.394 Loss=1.586 Prec@1=61.468 Prec@5=84.141 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:33 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.588 Prec@1=61.423 Prec@5=84.124 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:33 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.588 Prec@1=61.423 Prec@5=84.124 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:33 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.588 Prec@1=61.423 Prec@5=84.124 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:35 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.589 Prec@1=61.380 Prec@5=84.096 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:35 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.589 Prec@1=61.380 Prec@5=84.096 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:35 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.589 Prec@1=61.380 Prec@5=84.096 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:36 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.361 Prec@5=84.063 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:36 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.361 Prec@5=84.063 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:36 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.361 Prec@5=84.063 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:37 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.355 Prec@5=84.052 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:37 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.355 Prec@5=84.052 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:37 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.393 Loss=1.591 Prec@1=61.355 Prec@5=84.052 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:38 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.392 Loss=1.593 Prec@1=61.313 Prec@5=84.036 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:38 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.392 Loss=1.593 Prec@1=61.313 Prec@5=84.036 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:38 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.617 DataTime=0.392 Loss=1.593 Prec@1=61.313 Prec@5=84.036 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:39 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.595 Prec@1=61.279 Prec@5=84.016 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:39 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.595 Prec@1=61.279 Prec@5=84.016 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:39 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.595 Prec@1=61.279 Prec@5=84.016 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:40 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.596 Prec@1=61.253 Prec@5=84.000 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:40 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.596 Prec@1=61.253 Prec@5=84.000 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:40 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.596 Prec@1=61.253 Prec@5=84.000 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:41 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.243 Prec@5=83.989 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:41 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.243 Prec@5=83.989 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:41 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.243 Prec@5=83.989 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:42 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.230 Prec@5=83.977 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:42 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.230 Prec@5=83.977 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:42 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.597 Prec@1=61.230 Prec@5=83.977 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:43 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.598 Prec@1=61.206 Prec@5=83.965 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:43 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.598 Prec@1=61.206 Prec@5=83.965 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:43 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.598 Prec@1=61.206 Prec@5=83.965 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:44 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.182 Prec@5=83.958 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:44 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.182 Prec@5=83.958 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:44 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.182 Prec@5=83.958 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:45 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.186 Prec@5=83.968 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:45 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.186 Prec@5=83.968 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:45 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.599 Prec@1=61.186 Prec@5=83.968 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:46 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.600 Prec@1=61.175 Prec@5=83.954 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:46 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.600 Prec@1=61.175 Prec@5=83.954 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:46 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.600 Prec@1=61.175 Prec@5=83.954 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:46 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.600 Prec@1=61.176 Prec@5=83.955 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:46 IST=> training   100.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.616 DataTime=0.392 Loss=1.600 Prec@1=61.176 Prec@5=83.955 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=02:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:46 IST=> validation 0.00% of 1x98...Epoch=28/150 LR=0.09222 Time=7.180 Loss=1.658 Prec@1=59.766 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=02:46 IST=> validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=7.180 Loss=1.658 Prec@1=59.766 Prec@5=85.742 rate=7038.78 Hz, eta=0:00:00, total=0:00:00, wall=02:46 IST** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=7.180 Loss=1.658 Prec@1=59.766 Prec@5=85.742 rate=7038.78 Hz, eta=0:00:00, total=0:00:00, wall=02:46 IST** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=0.400 Loss=1.700 Prec@1=58.742 Prec@5=82.764 rate=7038.78 Hz, eta=0:00:00, total=0:00:00, wall=02:46 IST** validation 100.00% of 1x98...Epoch=28/150 LR=0.09222 Time=0.400 Loss=1.700 Prec@1=58.742 Prec@5=82.764 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=02:46 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:47 IST=> training   0.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=4.810 DataTime=4.492 Loss=1.483 Prec@1=62.305 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=02:47 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=4.810 DataTime=4.492 Loss=1.483 Prec@1=62.305 Prec@5=84.570 rate=3416.23 Hz, eta=0:00:00, total=0:00:00, wall=02:47 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=4.810 DataTime=4.492 Loss=1.483 Prec@1=62.305 Prec@5=84.570 rate=3416.23 Hz, eta=0:00:00, total=0:00:00, wall=02:48 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.653 DataTime=0.426 Loss=1.554 Prec@1=62.026 Prec@5=84.601 rate=3416.23 Hz, eta=0:00:00, total=0:00:00, wall=02:48 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.653 DataTime=0.426 Loss=1.554 Prec@1=62.026 Prec@5=84.601 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=02:48 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.653 DataTime=0.426 Loss=1.554 Prec@1=62.026 Prec@5=84.601 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=02:49 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.634 DataTime=0.408 Loss=1.553 Prec@1=62.171 Prec@5=84.645 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=02:49 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.634 DataTime=0.408 Loss=1.553 Prec@1=62.171 Prec@5=84.645 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=02:49 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.634 DataTime=0.408 Loss=1.553 Prec@1=62.171 Prec@5=84.645 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=02:50 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.627 DataTime=0.402 Loss=1.556 Prec@1=62.066 Prec@5=84.672 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=02:50 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.627 DataTime=0.402 Loss=1.556 Prec@1=62.066 Prec@5=84.672 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=02:50 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.627 DataTime=0.402 Loss=1.556 Prec@1=62.066 Prec@5=84.672 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=02:51 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.624 DataTime=0.399 Loss=1.560 Prec@1=61.957 Prec@5=84.571 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=02:51 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.624 DataTime=0.399 Loss=1.560 Prec@1=61.957 Prec@5=84.571 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:51 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.624 DataTime=0.399 Loss=1.560 Prec@1=61.957 Prec@5=84.571 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:52 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.622 DataTime=0.397 Loss=1.563 Prec@1=61.906 Prec@5=84.535 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=02:52 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.622 DataTime=0.397 Loss=1.563 Prec@1=61.906 Prec@5=84.535 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:52 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.622 DataTime=0.397 Loss=1.563 Prec@1=61.906 Prec@5=84.535 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:53 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.621 DataTime=0.396 Loss=1.564 Prec@1=61.883 Prec@5=84.508 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=02:53 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.621 DataTime=0.396 Loss=1.564 Prec@1=61.883 Prec@5=84.508 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:53 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.621 DataTime=0.396 Loss=1.564 Prec@1=61.883 Prec@5=84.508 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:54 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.620 DataTime=0.395 Loss=1.564 Prec@1=61.878 Prec@5=84.493 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:54 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.620 DataTime=0.395 Loss=1.564 Prec@1=61.878 Prec@5=84.493 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:54 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.620 DataTime=0.395 Loss=1.564 Prec@1=61.878 Prec@5=84.493 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:55 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.619 DataTime=0.394 Loss=1.565 Prec@1=61.887 Prec@5=84.460 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:55 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.619 DataTime=0.394 Loss=1.565 Prec@1=61.887 Prec@5=84.460 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:55 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.619 DataTime=0.394 Loss=1.565 Prec@1=61.887 Prec@5=84.460 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:56 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.394 Loss=1.568 Prec@1=61.807 Prec@5=84.414 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:56 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.394 Loss=1.568 Prec@1=61.807 Prec@5=84.414 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:56 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.394 Loss=1.568 Prec@1=61.807 Prec@5=84.414 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:57 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.393 Loss=1.569 Prec@1=61.804 Prec@5=84.404 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:57 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.393 Loss=1.569 Prec@1=61.804 Prec@5=84.404 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:57 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.618 DataTime=0.393 Loss=1.569 Prec@1=61.804 Prec@5=84.404 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:58 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.755 Prec@5=84.369 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:58 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.755 Prec@5=84.369 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=02:58 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.755 Prec@5=84.369 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=02:59 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.736 Prec@5=84.372 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=02:59 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.736 Prec@5=84.372 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:59 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.571 Prec@1=61.736 Prec@5=84.372 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:00 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.572 Prec@1=61.707 Prec@5=84.355 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:00 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.572 Prec@1=61.707 Prec@5=84.355 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=03:00 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.393 Loss=1.572 Prec@1=61.707 Prec@5=84.355 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=03:01 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.392 Loss=1.574 Prec@1=61.663 Prec@5=84.328 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=03:01 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.392 Loss=1.574 Prec@1=61.663 Prec@5=84.328 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:01 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.617 DataTime=0.392 Loss=1.574 Prec@1=61.663 Prec@5=84.328 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:02 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.639 Prec@5=84.308 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:02 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.639 Prec@5=84.308 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:02 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.639 Prec@5=84.308 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:03 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.577 Prec@1=61.608 Prec@5=84.274 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:03 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.577 Prec@1=61.608 Prec@5=84.274 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:03 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.577 Prec@1=61.608 Prec@5=84.274 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:04 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.579 Prec@1=61.581 Prec@5=84.250 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:04 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.579 Prec@1=61.581 Prec@5=84.250 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:04 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.579 Prec@1=61.581 Prec@5=84.250 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:05 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.580 Prec@1=61.550 Prec@5=84.228 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:05 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.580 Prec@1=61.550 Prec@5=84.228 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:05 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.580 Prec@1=61.550 Prec@5=84.228 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:06 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.582 Prec@1=61.520 Prec@5=84.195 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:06 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.582 Prec@1=61.520 Prec@5=84.195 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:06 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.582 Prec@1=61.520 Prec@5=84.195 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:07 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.583 Prec@1=61.499 Prec@5=84.181 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:07 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.583 Prec@1=61.499 Prec@5=84.181 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:07 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.392 Loss=1.583 Prec@1=61.499 Prec@5=84.181 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:08 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.585 Prec@1=61.480 Prec@5=84.160 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:08 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.585 Prec@1=61.480 Prec@5=84.160 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:08 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.585 Prec@1=61.480 Prec@5=84.160 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:09 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.586 Prec@1=61.455 Prec@5=84.151 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:09 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.586 Prec@1=61.455 Prec@5=84.151 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=03:09 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.616 DataTime=0.391 Loss=1.586 Prec@1=61.455 Prec@5=84.151 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=03:10 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.586 Prec@1=61.451 Prec@5=84.139 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.586 Prec@1=61.451 Prec@5=84.139 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.586 Prec@1=61.451 Prec@5=84.139 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=03:11 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.587 Prec@1=61.426 Prec@5=84.124 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=03:11 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.587 Prec@1=61.426 Prec@5=84.124 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:11 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.587 Prec@1=61.426 Prec@5=84.124 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:12 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.588 Prec@1=61.410 Prec@5=84.120 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:12 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.588 Prec@1=61.410 Prec@5=84.120 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:12 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.588 Prec@1=61.410 Prec@5=84.120 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:12 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.588 Prec@1=61.410 Prec@5=84.120 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:12 IST=> training   100.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.615 DataTime=0.391 Loss=1.588 Prec@1=61.410 Prec@5=84.120 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=03:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> validation 0.00% of 1x98...Epoch=29/150 LR=0.09165 Time=7.055 Loss=1.494 Prec@1=62.109 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=7.055 Loss=1.494 Prec@1=62.109 Prec@5=86.719 rate=7629.63 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=7.055 Loss=1.494 Prec@1=62.109 Prec@5=86.719 rate=7629.63 Hz, eta=0:00:00, total=0:00:00, wall=03:13 IST** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=0.408 Loss=1.664 Prec@1=59.534 Prec@5=83.484 rate=7629.63 Hz, eta=0:00:00, total=0:00:00, wall=03:13 IST** validation 100.00% of 1x98...Epoch=29/150 LR=0.09165 Time=0.408 Loss=1.664 Prec@1=59.534 Prec@5=83.484 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=03:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST=> training   0.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.962 DataTime=5.550 Loss=1.690 Prec@1=61.328 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=03:13 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.962 DataTime=5.550 Loss=1.690 Prec@1=61.328 Prec@5=81.445 rate=9274.54 Hz, eta=0:00:00, total=0:00:00, wall=03:13 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.962 DataTime=5.550 Loss=1.690 Prec@1=61.328 Prec@5=81.445 rate=9274.54 Hz, eta=0:00:00, total=0:00:00, wall=03:14 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.661 DataTime=0.436 Loss=1.532 Prec@1=62.631 Prec@5=84.909 rate=9274.54 Hz, eta=0:00:00, total=0:00:00, wall=03:14 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.661 DataTime=0.436 Loss=1.532 Prec@1=62.631 Prec@5=84.909 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=03:14 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.661 DataTime=0.436 Loss=1.532 Prec@1=62.631 Prec@5=84.909 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=03:15 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.637 DataTime=0.413 Loss=1.526 Prec@1=62.724 Prec@5=85.004 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=03:15 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.637 DataTime=0.413 Loss=1.526 Prec@1=62.724 Prec@5=85.004 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=03:15 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.637 DataTime=0.413 Loss=1.526 Prec@1=62.724 Prec@5=85.004 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=03:16 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.555 Prec@5=84.910 rate=1.65 Hz, eta=0:23:17, total=0:02:02, wall=03:16 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.555 Prec@5=84.910 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=03:16 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.555 Prec@5=84.910 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=03:17 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.624 DataTime=0.400 Loss=1.540 Prec@1=62.407 Prec@5=84.863 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=03:17 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.624 DataTime=0.400 Loss=1.540 Prec@1=62.407 Prec@5=84.863 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=03:17 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.624 DataTime=0.400 Loss=1.540 Prec@1=62.407 Prec@5=84.863 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=03:18 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.622 DataTime=0.398 Loss=1.543 Prec@1=62.289 Prec@5=84.859 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=03:18 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.622 DataTime=0.398 Loss=1.543 Prec@1=62.289 Prec@5=84.859 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=03:18 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.622 DataTime=0.398 Loss=1.543 Prec@1=62.289 Prec@5=84.859 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=03:19 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.620 DataTime=0.397 Loss=1.548 Prec@1=62.222 Prec@5=84.753 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=03:19 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.620 DataTime=0.397 Loss=1.548 Prec@1=62.222 Prec@5=84.753 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=03:19 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.620 DataTime=0.397 Loss=1.548 Prec@1=62.222 Prec@5=84.753 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=03:20 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.396 Loss=1.552 Prec@1=62.133 Prec@5=84.701 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=03:20 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.396 Loss=1.552 Prec@1=62.133 Prec@5=84.701 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=03:20 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.396 Loss=1.552 Prec@1=62.133 Prec@5=84.701 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=03:21 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.395 Loss=1.555 Prec@1=62.071 Prec@5=84.650 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=03:21 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.395 Loss=1.555 Prec@1=62.071 Prec@5=84.650 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=03:21 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.619 DataTime=0.395 Loss=1.555 Prec@1=62.071 Prec@5=84.650 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=03:22 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.555 Prec@1=62.075 Prec@5=84.639 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=03:22 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.555 Prec@1=62.075 Prec@5=84.639 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=03:22 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.555 Prec@1=62.075 Prec@5=84.639 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=03:23 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.557 Prec@1=62.029 Prec@5=84.610 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=03:23 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.557 Prec@1=62.029 Prec@5=84.610 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=03:23 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.618 DataTime=0.394 Loss=1.557 Prec@1=62.029 Prec@5=84.610 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=03:24 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.394 Loss=1.558 Prec@1=61.997 Prec@5=84.596 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=03:24 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.394 Loss=1.558 Prec@1=61.997 Prec@5=84.596 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=03:24 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.394 Loss=1.558 Prec@1=61.997 Prec@5=84.596 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=03:25 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.560 Prec@1=61.970 Prec@5=84.555 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=03:25 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.560 Prec@1=61.970 Prec@5=84.555 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=03:25 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.560 Prec@1=61.970 Prec@5=84.555 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=03:26 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.562 Prec@1=61.910 Prec@5=84.505 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=03:26 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.562 Prec@1=61.910 Prec@5=84.505 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:26 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.562 Prec@1=61.910 Prec@5=84.505 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:27 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.565 Prec@1=61.870 Prec@5=84.470 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:27 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.565 Prec@1=61.870 Prec@5=84.470 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:27 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.565 Prec@1=61.870 Prec@5=84.470 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:28 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.567 Prec@1=61.830 Prec@5=84.438 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.567 Prec@1=61.830 Prec@5=84.438 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.393 Loss=1.567 Prec@1=61.830 Prec@5=84.438 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=03:29 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.392 Loss=1.569 Prec@1=61.809 Prec@5=84.423 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=03:29 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.392 Loss=1.569 Prec@1=61.809 Prec@5=84.423 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:29 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.617 DataTime=0.392 Loss=1.569 Prec@1=61.809 Prec@5=84.423 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:30 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.570 Prec@1=61.787 Prec@5=84.407 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:30 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.570 Prec@1=61.787 Prec@5=84.407 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:30 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.570 Prec@1=61.787 Prec@5=84.407 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:31 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.571 Prec@1=61.752 Prec@5=84.393 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:31 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.571 Prec@1=61.752 Prec@5=84.393 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:31 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.571 Prec@1=61.752 Prec@5=84.393 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:32 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.572 Prec@1=61.742 Prec@5=84.369 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:32 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.572 Prec@1=61.742 Prec@5=84.369 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=03:32 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.572 Prec@1=61.742 Prec@5=84.369 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=03:33 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.722 Prec@5=84.350 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=03:33 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.722 Prec@5=84.350 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:33 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.722 Prec@5=84.350 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:34 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.707 Prec@5=84.349 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:34 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.707 Prec@5=84.349 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:34 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.574 Prec@1=61.707 Prec@5=84.349 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:35 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.690 Prec@5=84.332 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:35 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.690 Prec@5=84.332 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:35 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.392 Loss=1.575 Prec@1=61.690 Prec@5=84.332 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:37 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.575 Prec@1=61.676 Prec@5=84.322 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:37 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.575 Prec@1=61.676 Prec@5=84.322 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:37 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.575 Prec@1=61.676 Prec@5=84.322 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:38 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.672 Prec@5=84.323 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:38 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.672 Prec@5=84.323 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:38 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.672 Prec@5=84.323 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:39 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.654 Prec@5=84.304 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:39 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.654 Prec@5=84.304 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:39 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.616 DataTime=0.391 Loss=1.576 Prec@1=61.654 Prec@5=84.304 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:39 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.615 DataTime=0.391 Loss=1.576 Prec@1=61.656 Prec@5=84.304 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:39 IST=> training   100.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.615 DataTime=0.391 Loss=1.576 Prec@1=61.656 Prec@5=84.304 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=03:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> validation 0.00% of 1x98...Epoch=30/150 LR=0.09106 Time=6.155 Loss=1.728 Prec@1=57.227 Prec@5=82.031 rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.155 Loss=1.728 Prec@1=57.227 Prec@5=82.031 rate=5705.29 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.155 Loss=1.728 Prec@1=57.227 Prec@5=82.031 rate=5705.29 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=0.400 Loss=1.676 Prec@1=59.438 Prec@5=83.170 rate=5705.29 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST** validation 100.00% of 1x98...Epoch=30/150 LR=0.09106 Time=0.400 Loss=1.676 Prec@1=59.438 Prec@5=83.170 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=03:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> training   0.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=4.488 DataTime=4.259 Loss=1.611 Prec@1=62.695 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=03:39 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=4.488 DataTime=4.259 Loss=1.611 Prec@1=62.695 Prec@5=84.766 rate=7037.79 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=4.488 DataTime=4.259 Loss=1.611 Prec@1=62.695 Prec@5=84.766 rate=7037.79 Hz, eta=0:00:00, total=0:00:00, wall=03:40 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.654 DataTime=0.431 Loss=1.543 Prec@1=62.438 Prec@5=84.526 rate=7037.79 Hz, eta=0:00:00, total=0:00:00, wall=03:40 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.654 DataTime=0.431 Loss=1.543 Prec@1=62.438 Prec@5=84.526 rate=1.64 Hz, eta=0:24:23, total=0:01:01, wall=03:40 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.654 DataTime=0.431 Loss=1.543 Prec@1=62.438 Prec@5=84.526 rate=1.64 Hz, eta=0:24:23, total=0:01:01, wall=03:41 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.634 DataTime=0.411 Loss=1.541 Prec@1=62.382 Prec@5=84.770 rate=1.64 Hz, eta=0:24:23, total=0:01:01, wall=03:41 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.634 DataTime=0.411 Loss=1.541 Prec@1=62.382 Prec@5=84.770 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=03:41 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.634 DataTime=0.411 Loss=1.541 Prec@1=62.382 Prec@5=84.770 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=03:42 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.406 Prec@5=84.821 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=03:42 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.406 Prec@5=84.821 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=03:42 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.628 DataTime=0.404 Loss=1.537 Prec@1=62.406 Prec@5=84.821 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=03:43 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.624 DataTime=0.400 Loss=1.539 Prec@1=62.368 Prec@5=84.743 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=03:43 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.624 DataTime=0.400 Loss=1.539 Prec@1=62.368 Prec@5=84.743 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=03:43 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.624 DataTime=0.400 Loss=1.539 Prec@1=62.368 Prec@5=84.743 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=03:44 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.621 DataTime=0.398 Loss=1.539 Prec@1=62.358 Prec@5=84.780 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=03:44 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.621 DataTime=0.398 Loss=1.539 Prec@1=62.358 Prec@5=84.780 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:44 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.621 DataTime=0.398 Loss=1.539 Prec@1=62.358 Prec@5=84.780 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:45 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.620 DataTime=0.396 Loss=1.543 Prec@1=62.344 Prec@5=84.731 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.620 DataTime=0.396 Loss=1.543 Prec@1=62.344 Prec@5=84.731 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.620 DataTime=0.396 Loss=1.543 Prec@1=62.344 Prec@5=84.731 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:46 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.546 Prec@1=62.285 Prec@5=84.677 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:46 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.546 Prec@1=62.285 Prec@5=84.677 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:46 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.546 Prec@1=62.285 Prec@5=84.677 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:48 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.547 Prec@1=62.303 Prec@5=84.675 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:48 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.547 Prec@1=62.303 Prec@5=84.675 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:48 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.619 DataTime=0.395 Loss=1.547 Prec@1=62.303 Prec@5=84.675 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:49 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.618 DataTime=0.394 Loss=1.547 Prec@1=62.309 Prec@5=84.674 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:49 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.618 DataTime=0.394 Loss=1.547 Prec@1=62.309 Prec@5=84.674 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:49 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.618 DataTime=0.394 Loss=1.547 Prec@1=62.309 Prec@5=84.674 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:50 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.394 Loss=1.549 Prec@1=62.238 Prec@5=84.659 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:50 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.394 Loss=1.549 Prec@1=62.238 Prec@5=84.659 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:50 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.394 Loss=1.549 Prec@1=62.238 Prec@5=84.659 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:51 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.550 Prec@1=62.204 Prec@5=84.652 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:51 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.550 Prec@1=62.204 Prec@5=84.652 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:51 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.550 Prec@1=62.204 Prec@5=84.652 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:52 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.551 Prec@1=62.171 Prec@5=84.642 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:52 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.551 Prec@1=62.171 Prec@5=84.642 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:52 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.617 DataTime=0.393 Loss=1.551 Prec@1=62.171 Prec@5=84.642 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:53 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.393 Loss=1.552 Prec@1=62.171 Prec@5=84.631 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:53 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.393 Loss=1.552 Prec@1=62.171 Prec@5=84.631 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:53 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.393 Loss=1.552 Prec@1=62.171 Prec@5=84.631 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:54 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.554 Prec@1=62.118 Prec@5=84.602 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:54 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.554 Prec@1=62.118 Prec@5=84.602 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:54 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.554 Prec@1=62.118 Prec@5=84.602 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:55 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.083 Prec@5=84.598 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:55 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.083 Prec@5=84.598 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:55 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.083 Prec@5=84.598 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:56 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.057 Prec@5=84.587 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:56 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.057 Prec@5=84.587 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:56 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.057 Prec@5=84.587 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:57 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.558 Prec@1=62.019 Prec@5=84.564 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=03:57 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.558 Prec@1=62.019 Prec@5=84.564 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:57 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.616 DataTime=0.392 Loss=1.558 Prec@1=62.019 Prec@5=84.564 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:58 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.558 Prec@1=62.028 Prec@5=84.579 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:58 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.558 Prec@1=62.028 Prec@5=84.579 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:58 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.558 Prec@1=62.028 Prec@5=84.579 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:59 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=62.016 Prec@5=84.565 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:59 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=62.016 Prec@5=84.565 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=03:59 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=62.016 Prec@5=84.565 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=04:00 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=61.998 Prec@5=84.555 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=04:00 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=61.998 Prec@5=84.555 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=04:00 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.559 Prec@1=61.998 Prec@5=84.555 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=04:01 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.560 Prec@1=61.983 Prec@5=84.538 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=04:01 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.560 Prec@1=61.983 Prec@5=84.538 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=04:01 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.392 Loss=1.560 Prec@1=61.983 Prec@5=84.538 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=04:02 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.561 Prec@1=61.971 Prec@5=84.526 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=04:02 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.561 Prec@1=61.971 Prec@5=84.526 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=04:02 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.561 Prec@1=61.971 Prec@5=84.526 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=04:03 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.562 Prec@1=61.931 Prec@5=84.504 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=04:03 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.562 Prec@1=61.931 Prec@5=84.504 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=04:03 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.562 Prec@1=61.931 Prec@5=84.504 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=04:04 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.909 Prec@5=84.495 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=04:04 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.909 Prec@5=84.495 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=04:04 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.909 Prec@5=84.495 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=04:05 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.891 Prec@5=84.485 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=04:05 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.891 Prec@5=84.485 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=04:05 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.891 Prec@5=84.485 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=04:05 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.892 Prec@5=84.485 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=04:05 IST=> training   100.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.615 DataTime=0.391 Loss=1.564 Prec@1=61.892 Prec@5=84.485 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=04:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST=> validation 0.00% of 1x98...Epoch=31/150 LR=0.09045 Time=7.479 Loss=1.556 Prec@1=59.766 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST=> validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=7.479 Loss=1.556 Prec@1=59.766 Prec@5=84.375 rate=5254.47 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=7.479 Loss=1.556 Prec@1=59.766 Prec@5=84.375 rate=5254.47 Hz, eta=0:00:00, total=0:00:00, wall=04:06 IST** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=0.417 Loss=1.643 Prec@1=59.984 Prec@5=83.548 rate=5254.47 Hz, eta=0:00:00, total=0:00:00, wall=04:06 IST** validation 100.00% of 1x98...Epoch=31/150 LR=0.09045 Time=0.417 Loss=1.643 Prec@1=59.984 Prec@5=83.548 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=04:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:06 IST=> training   0.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.627 DataTime=5.336 Loss=1.386 Prec@1=64.258 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=04:06 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.627 DataTime=5.336 Loss=1.386 Prec@1=64.258 Prec@5=88.867 rate=6175.81 Hz, eta=0:00:00, total=0:00:00, wall=04:06 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.627 DataTime=5.336 Loss=1.386 Prec@1=64.258 Prec@5=88.867 rate=6175.81 Hz, eta=0:00:00, total=0:00:00, wall=04:07 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.656 DataTime=0.431 Loss=1.519 Prec@1=63.026 Prec@5=85.120 rate=6175.81 Hz, eta=0:00:00, total=0:00:00, wall=04:07 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.656 DataTime=0.431 Loss=1.519 Prec@1=63.026 Prec@5=85.120 rate=1.66 Hz, eta=0:24:02, total=0:01:00, wall=04:07 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.656 DataTime=0.431 Loss=1.519 Prec@1=63.026 Prec@5=85.120 rate=1.66 Hz, eta=0:24:02, total=0:01:00, wall=04:08 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.635 DataTime=0.410 Loss=1.527 Prec@1=62.690 Prec@5=85.072 rate=1.66 Hz, eta=0:24:02, total=0:01:00, wall=04:08 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.635 DataTime=0.410 Loss=1.527 Prec@1=62.690 Prec@5=85.072 rate=1.65 Hz, eta=0:23:17, total=0:02:01, wall=04:08 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.635 DataTime=0.410 Loss=1.527 Prec@1=62.690 Prec@5=85.072 rate=1.65 Hz, eta=0:23:17, total=0:02:01, wall=04:09 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.627 DataTime=0.403 Loss=1.527 Prec@1=62.684 Prec@5=85.038 rate=1.65 Hz, eta=0:23:17, total=0:02:01, wall=04:09 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.627 DataTime=0.403 Loss=1.527 Prec@1=62.684 Prec@5=85.038 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=04:09 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.627 DataTime=0.403 Loss=1.527 Prec@1=62.684 Prec@5=85.038 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=04:10 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.624 DataTime=0.400 Loss=1.528 Prec@1=62.661 Prec@5=84.982 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=04:10 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.624 DataTime=0.400 Loss=1.528 Prec@1=62.661 Prec@5=84.982 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=04:10 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.624 DataTime=0.400 Loss=1.528 Prec@1=62.661 Prec@5=84.982 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=04:11 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.622 DataTime=0.398 Loss=1.532 Prec@1=62.587 Prec@5=84.908 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=04:11 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.622 DataTime=0.398 Loss=1.532 Prec@1=62.587 Prec@5=84.908 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=04:11 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.622 DataTime=0.398 Loss=1.532 Prec@1=62.587 Prec@5=84.908 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=04:12 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.621 DataTime=0.397 Loss=1.533 Prec@1=62.557 Prec@5=84.918 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=04:12 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.621 DataTime=0.397 Loss=1.533 Prec@1=62.557 Prec@5=84.918 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=04:12 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.621 DataTime=0.397 Loss=1.533 Prec@1=62.557 Prec@5=84.918 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=04:13 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.526 Prec@5=84.896 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=04:13 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.526 Prec@5=84.896 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=04:13 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.526 Prec@5=84.896 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=04:14 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.506 Prec@5=84.886 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=04:14 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.506 Prec@5=84.886 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=04:14 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.620 DataTime=0.396 Loss=1.534 Prec@1=62.506 Prec@5=84.886 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=04:15 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.395 Loss=1.538 Prec@1=62.435 Prec@5=84.834 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=04:15 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.395 Loss=1.538 Prec@1=62.435 Prec@5=84.834 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:15 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.395 Loss=1.538 Prec@1=62.435 Prec@5=84.834 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:16 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.394 Loss=1.540 Prec@1=62.402 Prec@5=84.813 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:16 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.394 Loss=1.540 Prec@1=62.402 Prec@5=84.813 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=04:16 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.619 DataTime=0.394 Loss=1.540 Prec@1=62.402 Prec@5=84.813 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=04:17 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.541 Prec@1=62.406 Prec@5=84.812 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=04:17 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.541 Prec@1=62.406 Prec@5=84.812 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=04:17 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.541 Prec@1=62.406 Prec@5=84.812 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=04:18 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.543 Prec@1=62.348 Prec@5=84.774 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=04:18 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.543 Prec@1=62.348 Prec@5=84.774 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:18 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.394 Loss=1.543 Prec@1=62.348 Prec@5=84.774 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:19 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.393 Loss=1.545 Prec@1=62.327 Prec@5=84.761 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:19 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.393 Loss=1.545 Prec@1=62.327 Prec@5=84.761 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=04:19 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.618 DataTime=0.393 Loss=1.545 Prec@1=62.327 Prec@5=84.761 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=04:20 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.547 Prec@1=62.290 Prec@5=84.731 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=04:20 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.547 Prec@1=62.290 Prec@5=84.731 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=04:20 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.547 Prec@1=62.290 Prec@5=84.731 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=04:21 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.548 Prec@1=62.275 Prec@5=84.703 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=04:21 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.548 Prec@1=62.275 Prec@5=84.703 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:21 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.548 Prec@1=62.275 Prec@5=84.703 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:22 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.549 Prec@1=62.258 Prec@5=84.689 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:22 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.549 Prec@1=62.258 Prec@5=84.689 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:22 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.393 Loss=1.549 Prec@1=62.258 Prec@5=84.689 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:23 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.549 Prec@1=62.247 Prec@5=84.685 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:23 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.549 Prec@1=62.247 Prec@5=84.685 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=04:23 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.549 Prec@1=62.247 Prec@5=84.685 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=04:24 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.550 Prec@1=62.255 Prec@5=84.688 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=04:24 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.550 Prec@1=62.255 Prec@5=84.688 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=04:24 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.617 DataTime=0.392 Loss=1.550 Prec@1=62.255 Prec@5=84.688 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=04:25 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.550 Prec@1=62.244 Prec@5=84.680 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=04:25 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.550 Prec@1=62.244 Prec@5=84.680 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:25 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.550 Prec@1=62.244 Prec@5=84.680 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:26 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.552 Prec@1=62.208 Prec@5=84.656 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:26 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.552 Prec@1=62.208 Prec@5=84.656 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=04:26 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.552 Prec@1=62.208 Prec@5=84.656 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=04:27 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.199 Prec@5=84.641 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=04:27 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.199 Prec@5=84.641 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:27 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.199 Prec@5=84.641 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:28 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.183 Prec@5=84.625 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:28 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.183 Prec@5=84.625 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:28 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.553 Prec@1=62.183 Prec@5=84.625 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:29 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.155 Prec@5=84.609 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:29 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.155 Prec@5=84.609 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:29 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.555 Prec@1=62.155 Prec@5=84.609 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:30 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.128 Prec@5=84.587 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:30 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.128 Prec@5=84.587 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:30 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.128 Prec@5=84.587 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:31 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.123 Prec@5=84.590 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:31 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.123 Prec@5=84.590 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:31 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.556 Prec@1=62.123 Prec@5=84.590 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:31 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.557 Prec@1=62.121 Prec@5=84.589 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:31 IST=> training   100.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.616 DataTime=0.392 Loss=1.557 Prec@1=62.121 Prec@5=84.589 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=04:31 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> validation 0.00% of 1x98...Epoch=32/150 LR=0.08983 Time=6.545 Loss=1.698 Prec@1=58.203 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=6.545 Loss=1.698 Prec@1=58.203 Prec@5=83.203 rate=5909.71 Hz, eta=0:00:00, total=0:00:00, wall=04:31 IST** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=6.545 Loss=1.698 Prec@1=58.203 Prec@5=83.203 rate=5909.71 Hz, eta=0:00:00, total=0:00:00, wall=04:32 IST** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=0.401 Loss=1.710 Prec@1=58.662 Prec@5=82.658 rate=5909.71 Hz, eta=0:00:00, total=0:00:00, wall=04:32 IST** validation 100.00% of 1x98...Epoch=32/150 LR=0.08983 Time=0.401 Loss=1.710 Prec@1=58.662 Prec@5=82.658 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=04:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:32 IST=> training   0.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.114 DataTime=4.807 Loss=1.529 Prec@1=61.914 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=04:32 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.114 DataTime=4.807 Loss=1.529 Prec@1=61.914 Prec@5=86.523 rate=3472.15 Hz, eta=0:00:00, total=0:00:00, wall=04:32 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.114 DataTime=4.807 Loss=1.529 Prec@1=61.914 Prec@5=86.523 rate=3472.15 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.661 DataTime=0.433 Loss=1.518 Prec@1=62.842 Prec@5=85.108 rate=3472.15 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.661 DataTime=0.433 Loss=1.518 Prec@1=62.842 Prec@5=85.108 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=04:33 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.661 DataTime=0.433 Loss=1.518 Prec@1=62.842 Prec@5=85.108 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=04:34 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.639 DataTime=0.413 Loss=1.511 Prec@1=62.988 Prec@5=85.201 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=04:34 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.639 DataTime=0.413 Loss=1.511 Prec@1=62.988 Prec@5=85.201 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=04:34 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.639 DataTime=0.413 Loss=1.511 Prec@1=62.988 Prec@5=85.201 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=04:35 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.631 DataTime=0.405 Loss=1.516 Prec@1=62.810 Prec@5=85.151 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.631 DataTime=0.405 Loss=1.516 Prec@1=62.810 Prec@5=85.151 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.631 DataTime=0.405 Loss=1.516 Prec@1=62.810 Prec@5=85.151 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=04:36 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.627 DataTime=0.401 Loss=1.520 Prec@1=62.726 Prec@5=85.110 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=04:36 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.627 DataTime=0.401 Loss=1.520 Prec@1=62.726 Prec@5=85.110 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=04:36 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.627 DataTime=0.401 Loss=1.520 Prec@1=62.726 Prec@5=85.110 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=04:37 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.624 DataTime=0.399 Loss=1.522 Prec@1=62.694 Prec@5=85.067 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=04:37 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.624 DataTime=0.399 Loss=1.522 Prec@1=62.694 Prec@5=85.067 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=04:37 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.624 DataTime=0.399 Loss=1.522 Prec@1=62.694 Prec@5=85.067 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=04:38 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.622 DataTime=0.398 Loss=1.526 Prec@1=62.644 Prec@5=85.014 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=04:38 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.622 DataTime=0.398 Loss=1.526 Prec@1=62.644 Prec@5=85.014 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=04:38 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.622 DataTime=0.398 Loss=1.526 Prec@1=62.644 Prec@5=85.014 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=04:39 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.621 DataTime=0.397 Loss=1.530 Prec@1=62.579 Prec@5=84.956 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=04:39 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.621 DataTime=0.397 Loss=1.530 Prec@1=62.579 Prec@5=84.956 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=04:39 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.621 DataTime=0.397 Loss=1.530 Prec@1=62.579 Prec@5=84.956 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=04:40 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.396 Loss=1.531 Prec@1=62.551 Prec@5=84.955 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=04:40 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.396 Loss=1.531 Prec@1=62.551 Prec@5=84.955 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=04:40 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.396 Loss=1.531 Prec@1=62.551 Prec@5=84.955 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=04:41 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.395 Loss=1.533 Prec@1=62.501 Prec@5=84.909 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=04:41 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.395 Loss=1.533 Prec@1=62.501 Prec@5=84.909 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=04:41 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.620 DataTime=0.395 Loss=1.533 Prec@1=62.501 Prec@5=84.909 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=04:42 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.534 Prec@1=62.470 Prec@5=84.897 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=04:42 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.534 Prec@1=62.470 Prec@5=84.897 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=04:42 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.534 Prec@1=62.470 Prec@5=84.897 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=04:43 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.536 Prec@1=62.455 Prec@5=84.872 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=04:43 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.536 Prec@1=62.455 Prec@5=84.872 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=04:43 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.619 DataTime=0.394 Loss=1.536 Prec@1=62.455 Prec@5=84.872 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=04:44 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.394 Loss=1.538 Prec@1=62.422 Prec@5=84.844 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=04:44 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.394 Loss=1.538 Prec@1=62.422 Prec@5=84.844 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:44 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.394 Loss=1.538 Prec@1=62.422 Prec@5=84.844 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:45 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.538 Prec@1=62.414 Prec@5=84.847 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:45 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.538 Prec@1=62.414 Prec@5=84.847 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=04:45 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.538 Prec@1=62.414 Prec@5=84.847 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=04:47 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.539 Prec@1=62.393 Prec@5=84.832 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=04:47 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.539 Prec@1=62.393 Prec@5=84.832 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=04:47 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.539 Prec@1=62.393 Prec@5=84.832 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=04:48 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.540 Prec@1=62.372 Prec@5=84.806 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=04:48 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.540 Prec@1=62.372 Prec@5=84.806 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=04:48 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.540 Prec@1=62.372 Prec@5=84.806 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=04:49 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.541 Prec@1=62.370 Prec@5=84.790 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=04:49 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.541 Prec@1=62.370 Prec@5=84.790 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=04:49 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.541 Prec@1=62.370 Prec@5=84.790 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=04:50 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.542 Prec@1=62.344 Prec@5=84.769 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=04:50 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.542 Prec@1=62.344 Prec@5=84.769 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=04:50 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.618 DataTime=0.393 Loss=1.542 Prec@1=62.344 Prec@5=84.769 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=04:51 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.543 Prec@1=62.323 Prec@5=84.757 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=04:51 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.543 Prec@1=62.323 Prec@5=84.757 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=04:51 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.543 Prec@1=62.323 Prec@5=84.757 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=04:52 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.304 Prec@5=84.731 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=04:52 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.304 Prec@5=84.731 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=04:52 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.304 Prec@5=84.731 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=04:53 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.297 Prec@5=84.725 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=04:53 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.297 Prec@5=84.725 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=04:53 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.393 Loss=1.544 Prec@1=62.297 Prec@5=84.725 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=04:54 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.545 Prec@1=62.274 Prec@5=84.721 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=04:54 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.545 Prec@1=62.274 Prec@5=84.721 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:54 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.545 Prec@1=62.274 Prec@5=84.721 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:55 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.274 Prec@5=84.707 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:55 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.274 Prec@5=84.707 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=04:55 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.274 Prec@5=84.707 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=04:56 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.271 Prec@5=84.707 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=04:56 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.271 Prec@5=84.707 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=04:56 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.617 DataTime=0.392 Loss=1.546 Prec@1=62.271 Prec@5=84.707 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=04:57 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.248 Prec@5=84.689 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=04:57 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.248 Prec@5=84.689 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=04:57 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.248 Prec@5=84.689 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=04:58 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.234 Prec@5=84.685 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=04:58 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.234 Prec@5=84.685 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=04:58 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.234 Prec@5=84.685 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=04:58 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.233 Prec@5=84.686 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=04:58 IST=> training   100.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.616 DataTime=0.392 Loss=1.547 Prec@1=62.233 Prec@5=84.686 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=04:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:58 IST=> validation 0.00% of 1x98...Epoch=33/150 LR=0.08918 Time=7.177 Loss=1.558 Prec@1=62.695 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=04:58 IST=> validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=7.177 Loss=1.558 Prec@1=62.695 Prec@5=84.570 rate=5923.64 Hz, eta=0:00:00, total=0:00:00, wall=04:58 IST** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=7.177 Loss=1.558 Prec@1=62.695 Prec@5=84.570 rate=5923.64 Hz, eta=0:00:00, total=0:00:00, wall=04:58 IST** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=0.415 Loss=1.566 Prec@1=61.572 Prec@5=84.810 rate=5923.64 Hz, eta=0:00:00, total=0:00:00, wall=04:58 IST** validation 100.00% of 1x98...Epoch=33/150 LR=0.08918 Time=0.415 Loss=1.566 Prec@1=61.572 Prec@5=84.810 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=04:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST=> training   0.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.156 DataTime=5.935 Loss=1.662 Prec@1=58.594 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=04:59 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.156 DataTime=5.935 Loss=1.662 Prec@1=58.594 Prec@5=83.203 rate=7546.09 Hz, eta=0:00:00, total=0:00:00, wall=04:59 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.156 DataTime=5.935 Loss=1.662 Prec@1=58.594 Prec@5=83.203 rate=7546.09 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.664 DataTime=0.440 Loss=1.503 Prec@1=63.144 Prec@5=85.390 rate=7546.09 Hz, eta=0:00:00, total=0:00:00, wall=05:00 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.664 DataTime=0.440 Loss=1.503 Prec@1=63.144 Prec@5=85.390 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=05:00 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.664 DataTime=0.440 Loss=1.503 Prec@1=63.144 Prec@5=85.390 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=05:01 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.640 DataTime=0.416 Loss=1.505 Prec@1=63.080 Prec@5=85.321 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=05:01 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.640 DataTime=0.416 Loss=1.505 Prec@1=63.080 Prec@5=85.321 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:01 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.640 DataTime=0.416 Loss=1.505 Prec@1=63.080 Prec@5=85.321 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:02 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.631 DataTime=0.407 Loss=1.507 Prec@1=63.011 Prec@5=85.313 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:02 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.631 DataTime=0.407 Loss=1.507 Prec@1=63.011 Prec@5=85.313 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:02 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.631 DataTime=0.407 Loss=1.507 Prec@1=63.011 Prec@5=85.313 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:03 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.627 DataTime=0.403 Loss=1.508 Prec@1=63.035 Prec@5=85.322 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:03 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.627 DataTime=0.403 Loss=1.508 Prec@1=63.035 Prec@5=85.322 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=05:03 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.627 DataTime=0.403 Loss=1.508 Prec@1=63.035 Prec@5=85.322 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=05:04 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.624 DataTime=0.401 Loss=1.512 Prec@1=62.935 Prec@5=85.242 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=05:04 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.624 DataTime=0.401 Loss=1.512 Prec@1=62.935 Prec@5=85.242 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=05:04 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.624 DataTime=0.401 Loss=1.512 Prec@1=62.935 Prec@5=85.242 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=05:05 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.623 DataTime=0.400 Loss=1.514 Prec@1=62.912 Prec@5=85.226 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=05:05 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.623 DataTime=0.400 Loss=1.514 Prec@1=62.912 Prec@5=85.226 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=05:05 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.623 DataTime=0.400 Loss=1.514 Prec@1=62.912 Prec@5=85.226 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=05:06 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.622 DataTime=0.399 Loss=1.517 Prec@1=62.870 Prec@5=85.184 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=05:06 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.622 DataTime=0.399 Loss=1.517 Prec@1=62.870 Prec@5=85.184 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:06 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.622 DataTime=0.399 Loss=1.517 Prec@1=62.870 Prec@5=85.184 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:07 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.621 DataTime=0.398 Loss=1.518 Prec@1=62.831 Prec@5=85.167 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:07 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.621 DataTime=0.398 Loss=1.518 Prec@1=62.831 Prec@5=85.167 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=05:07 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.621 DataTime=0.398 Loss=1.518 Prec@1=62.831 Prec@5=85.167 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=05:08 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.620 DataTime=0.398 Loss=1.520 Prec@1=62.794 Prec@5=85.135 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=05:08 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.620 DataTime=0.398 Loss=1.520 Prec@1=62.794 Prec@5=85.135 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:08 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.620 DataTime=0.398 Loss=1.520 Prec@1=62.794 Prec@5=85.135 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:09 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.521 Prec@1=62.760 Prec@5=85.125 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:09 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.521 Prec@1=62.760 Prec@5=85.125 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=05:09 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.521 Prec@1=62.760 Prec@5=85.125 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=05:10 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.091 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=05:10 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.091 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:10 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.091 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:11 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.080 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:11 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.080 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:11 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.619 DataTime=0.397 Loss=1.523 Prec@1=62.706 Prec@5=85.080 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:12 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.525 Prec@1=62.669 Prec@5=85.048 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:12 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.525 Prec@1=62.669 Prec@5=85.048 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:12 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.525 Prec@1=62.669 Prec@5=85.048 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:13 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.527 Prec@1=62.641 Prec@5=85.008 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:13 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.527 Prec@1=62.641 Prec@5=85.008 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:13 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.527 Prec@1=62.641 Prec@5=85.008 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:14 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.528 Prec@1=62.626 Prec@5=84.990 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:14 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.528 Prec@1=62.626 Prec@5=84.990 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=05:14 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.528 Prec@1=62.626 Prec@5=84.990 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=05:15 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.530 Prec@1=62.588 Prec@5=84.967 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=05:15 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.530 Prec@1=62.588 Prec@5=84.967 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:15 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.618 DataTime=0.396 Loss=1.530 Prec@1=62.588 Prec@5=84.967 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:16 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.396 Loss=1.531 Prec@1=62.584 Prec@5=84.950 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:16 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.396 Loss=1.531 Prec@1=62.584 Prec@5=84.950 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:16 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.396 Loss=1.531 Prec@1=62.584 Prec@5=84.950 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:17 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.532 Prec@1=62.569 Prec@5=84.938 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:17 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.532 Prec@1=62.569 Prec@5=84.938 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:17 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.532 Prec@1=62.569 Prec@5=84.938 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:18 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.559 Prec@5=84.920 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:18 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.559 Prec@5=84.920 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=05:18 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.559 Prec@5=84.920 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=05:19 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.557 Prec@5=84.910 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=05:19 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.557 Prec@5=84.910 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:19 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.395 Loss=1.533 Prec@1=62.557 Prec@5=84.910 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:20 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.533 Prec@1=62.563 Prec@5=84.905 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:20 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.533 Prec@1=62.563 Prec@5=84.905 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=05:20 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.533 Prec@1=62.563 Prec@5=84.905 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=05:21 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.535 Prec@1=62.531 Prec@5=84.891 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=05:21 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.535 Prec@1=62.531 Prec@5=84.891 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=05:21 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.535 Prec@1=62.531 Prec@5=84.891 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=05:22 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.536 Prec@1=62.521 Prec@5=84.874 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=05:22 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.536 Prec@1=62.521 Prec@5=84.874 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=05:22 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.617 DataTime=0.394 Loss=1.536 Prec@1=62.521 Prec@5=84.874 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=05:23 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.536 Prec@1=62.508 Prec@5=84.861 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=05:23 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.536 Prec@1=62.508 Prec@5=84.861 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=05:23 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.536 Prec@1=62.508 Prec@5=84.861 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=05:24 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.537 Prec@1=62.486 Prec@5=84.843 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.537 Prec@1=62.486 Prec@5=84.843 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.537 Prec@1=62.486 Prec@5=84.843 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.537 Prec@1=62.486 Prec@5=84.842 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:24 IST=> training   100.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.616 DataTime=0.394 Loss=1.537 Prec@1=62.486 Prec@5=84.842 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=05:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 0.00% of 1x98...Epoch=34/150 LR=0.08853 Time=5.805 Loss=1.604 Prec@1=57.812 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=5.805 Loss=1.604 Prec@1=57.812 Prec@5=83.789 rate=2733.41 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=5.805 Loss=1.604 Prec@1=57.812 Prec@5=83.789 rate=2733.41 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=0.403 Loss=1.618 Prec@1=60.600 Prec@5=84.064 rate=2733.41 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST** validation 100.00% of 1x98...Epoch=34/150 LR=0.08853 Time=0.403 Loss=1.618 Prec@1=60.600 Prec@5=84.064 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=05:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=5.769 DataTime=5.500 Loss=1.406 Prec@1=63.477 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=5.769 DataTime=5.500 Loss=1.406 Prec@1=63.477 Prec@5=87.109 rate=7093.05 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=5.769 DataTime=5.500 Loss=1.406 Prec@1=63.477 Prec@5=87.109 rate=7093.05 Hz, eta=0:00:00, total=0:00:00, wall=05:26 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.662 DataTime=0.436 Loss=1.492 Prec@1=63.391 Prec@5=85.493 rate=7093.05 Hz, eta=0:00:00, total=0:00:00, wall=05:26 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.662 DataTime=0.436 Loss=1.492 Prec@1=63.391 Prec@5=85.493 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=05:26 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.662 DataTime=0.436 Loss=1.492 Prec@1=63.391 Prec@5=85.493 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=05:27 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.638 DataTime=0.413 Loss=1.497 Prec@1=63.323 Prec@5=85.389 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=05:27 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.638 DataTime=0.413 Loss=1.497 Prec@1=63.323 Prec@5=85.389 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:27 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.638 DataTime=0.413 Loss=1.497 Prec@1=63.323 Prec@5=85.389 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:28 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.631 DataTime=0.406 Loss=1.499 Prec@1=63.209 Prec@5=85.302 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:28 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.631 DataTime=0.406 Loss=1.499 Prec@1=63.209 Prec@5=85.302 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=05:28 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.631 DataTime=0.406 Loss=1.499 Prec@1=63.209 Prec@5=85.302 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=05:29 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.627 DataTime=0.402 Loss=1.502 Prec@1=63.120 Prec@5=85.296 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=05:29 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.627 DataTime=0.402 Loss=1.502 Prec@1=63.120 Prec@5=85.296 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:29 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.627 DataTime=0.402 Loss=1.502 Prec@1=63.120 Prec@5=85.296 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:30 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.625 DataTime=0.400 Loss=1.508 Prec@1=63.020 Prec@5=85.224 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:30 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.625 DataTime=0.400 Loss=1.508 Prec@1=63.020 Prec@5=85.224 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=05:30 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.625 DataTime=0.400 Loss=1.508 Prec@1=63.020 Prec@5=85.224 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=05:31 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.623 DataTime=0.398 Loss=1.507 Prec@1=63.047 Prec@5=85.234 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=05:31 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.623 DataTime=0.398 Loss=1.507 Prec@1=63.047 Prec@5=85.234 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:31 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.623 DataTime=0.398 Loss=1.507 Prec@1=63.047 Prec@5=85.234 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:32 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.622 DataTime=0.397 Loss=1.508 Prec@1=63.036 Prec@5=85.247 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:32 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.622 DataTime=0.397 Loss=1.508 Prec@1=63.036 Prec@5=85.247 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=05:32 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.622 DataTime=0.397 Loss=1.508 Prec@1=63.036 Prec@5=85.247 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=05:33 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.621 DataTime=0.396 Loss=1.509 Prec@1=63.043 Prec@5=85.240 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=05:33 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.621 DataTime=0.396 Loss=1.509 Prec@1=63.043 Prec@5=85.240 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=05:33 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.621 DataTime=0.396 Loss=1.509 Prec@1=63.043 Prec@5=85.240 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=05:34 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.396 Loss=1.510 Prec@1=63.006 Prec@5=85.208 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=05:34 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.396 Loss=1.510 Prec@1=63.006 Prec@5=85.208 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=05:34 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.396 Loss=1.510 Prec@1=63.006 Prec@5=85.208 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=05:35 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.395 Loss=1.512 Prec@1=62.982 Prec@5=85.186 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=05:35 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.395 Loss=1.512 Prec@1=62.982 Prec@5=85.186 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=05:35 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.620 DataTime=0.395 Loss=1.512 Prec@1=62.982 Prec@5=85.186 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=05:36 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.395 Loss=1.514 Prec@1=62.943 Prec@5=85.154 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=05:36 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.395 Loss=1.514 Prec@1=62.943 Prec@5=85.154 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=05:36 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.395 Loss=1.514 Prec@1=62.943 Prec@5=85.154 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=05:37 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.131 rate=1.63 Hz, eta=0:14:20, total=0:11:16, wall=05:37 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.131 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:37 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.131 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:38 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.127 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:38 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.127 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=05:38 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.619 DataTime=0.394 Loss=1.516 Prec@1=62.917 Prec@5=85.127 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=05:39 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.517 Prec@1=62.897 Prec@5=85.131 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=05:39 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.517 Prec@1=62.897 Prec@5=85.131 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=05:39 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.517 Prec@1=62.897 Prec@5=85.131 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=05:40 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.861 Prec@5=85.115 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=05:40 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.861 Prec@5=85.115 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=05:40 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.861 Prec@5=85.115 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=05:41 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.850 Prec@5=85.113 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=05:41 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.850 Prec@5=85.113 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=05:41 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.394 Loss=1.519 Prec@1=62.850 Prec@5=85.113 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=05:42 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.520 Prec@1=62.843 Prec@5=85.102 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=05:42 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.520 Prec@1=62.843 Prec@5=85.102 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:42 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.520 Prec@1=62.843 Prec@5=85.102 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:43 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.521 Prec@1=62.818 Prec@5=85.085 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.521 Prec@1=62.818 Prec@5=85.085 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.618 DataTime=0.393 Loss=1.521 Prec@1=62.818 Prec@5=85.085 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=05:44 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.780 Prec@5=85.066 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=05:44 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.780 Prec@5=85.066 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:44 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.780 Prec@5=85.066 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:46 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.758 Prec@5=85.051 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:46 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.758 Prec@5=85.051 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=05:46 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.522 Prec@1=62.758 Prec@5=85.051 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=05:47 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.524 Prec@1=62.734 Prec@5=85.040 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=05:47 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.524 Prec@1=62.734 Prec@5=85.040 rate=1.63 Hz, eta=0:04:07, total=0:21:30, wall=05:47 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.524 Prec@1=62.734 Prec@5=85.040 rate=1.63 Hz, eta=0:04:07, total=0:21:30, wall=05:48 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.525 Prec@1=62.715 Prec@5=85.023 rate=1.63 Hz, eta=0:04:07, total=0:21:30, wall=05:48 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.525 Prec@1=62.715 Prec@5=85.023 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=05:48 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.525 Prec@1=62.715 Prec@5=85.023 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=05:49 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.526 Prec@1=62.702 Prec@5=85.010 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=05:49 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.526 Prec@1=62.702 Prec@5=85.010 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:49 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.393 Loss=1.526 Prec@1=62.702 Prec@5=85.010 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:50 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.392 Loss=1.526 Prec@1=62.685 Prec@5=85.006 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:50 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.392 Loss=1.526 Prec@1=62.685 Prec@5=85.006 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:50 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.617 DataTime=0.392 Loss=1.526 Prec@1=62.685 Prec@5=85.006 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:51 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.616 DataTime=0.392 Loss=1.527 Prec@1=62.671 Prec@5=84.993 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.616 DataTime=0.392 Loss=1.527 Prec@1=62.671 Prec@5=84.993 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.616 DataTime=0.392 Loss=1.527 Prec@1=62.671 Prec@5=84.993 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.616 DataTime=0.392 Loss=1.527 Prec@1=62.670 Prec@5=84.992 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:51 IST=> training   100.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.616 DataTime=0.392 Loss=1.527 Prec@1=62.670 Prec@5=84.992 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=05:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> validation 0.00% of 1x98...Epoch=35/150 LR=0.08785 Time=6.023 Loss=1.543 Prec@1=60.938 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.023 Loss=1.543 Prec@1=60.938 Prec@5=85.156 rate=5877.20 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.023 Loss=1.543 Prec@1=60.938 Prec@5=85.156 rate=5877.20 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=0.398 Loss=1.672 Prec@1=59.254 Prec@5=83.452 rate=5877.20 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST** validation 100.00% of 1x98...Epoch=35/150 LR=0.08785 Time=0.398 Loss=1.672 Prec@1=59.254 Prec@5=83.452 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=05:51 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> training   0.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=4.955 DataTime=4.684 Loss=1.602 Prec@1=60.742 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=05:51 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=4.955 DataTime=4.684 Loss=1.602 Prec@1=60.742 Prec@5=83.594 rate=7956.91 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=4.955 DataTime=4.684 Loss=1.602 Prec@1=60.742 Prec@5=83.594 rate=7956.91 Hz, eta=0:00:00, total=0:00:00, wall=05:52 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.659 DataTime=0.433 Loss=1.483 Prec@1=63.881 Prec@5=85.773 rate=7956.91 Hz, eta=0:00:00, total=0:00:00, wall=05:52 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.659 DataTime=0.433 Loss=1.483 Prec@1=63.881 Prec@5=85.773 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=05:52 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.659 DataTime=0.433 Loss=1.483 Prec@1=63.881 Prec@5=85.773 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=05:53 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.637 DataTime=0.412 Loss=1.487 Prec@1=63.622 Prec@5=85.708 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=05:53 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.637 DataTime=0.412 Loss=1.487 Prec@1=63.622 Prec@5=85.708 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=05:53 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.637 DataTime=0.412 Loss=1.487 Prec@1=63.622 Prec@5=85.708 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=05:54 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.629 DataTime=0.404 Loss=1.489 Prec@1=63.600 Prec@5=85.605 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=05:54 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.629 DataTime=0.404 Loss=1.489 Prec@1=63.600 Prec@5=85.605 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=05:54 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.629 DataTime=0.404 Loss=1.489 Prec@1=63.600 Prec@5=85.605 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=05:56 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.625 DataTime=0.400 Loss=1.493 Prec@1=63.466 Prec@5=85.559 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=05:56 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.625 DataTime=0.400 Loss=1.493 Prec@1=63.466 Prec@5=85.559 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:56 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.625 DataTime=0.400 Loss=1.493 Prec@1=63.466 Prec@5=85.559 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:57 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.623 DataTime=0.398 Loss=1.494 Prec@1=63.379 Prec@5=85.530 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=05:57 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.623 DataTime=0.398 Loss=1.494 Prec@1=63.379 Prec@5=85.530 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=05:57 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.623 DataTime=0.398 Loss=1.494 Prec@1=63.379 Prec@5=85.530 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=05:58 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.622 DataTime=0.397 Loss=1.496 Prec@1=63.305 Prec@5=85.482 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=05:58 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.622 DataTime=0.397 Loss=1.496 Prec@1=63.305 Prec@5=85.482 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:58 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.622 DataTime=0.397 Loss=1.496 Prec@1=63.305 Prec@5=85.482 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:59 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.396 Loss=1.496 Prec@1=63.307 Prec@5=85.471 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=05:59 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.396 Loss=1.496 Prec@1=63.307 Prec@5=85.471 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:59 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.396 Loss=1.496 Prec@1=63.307 Prec@5=85.471 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:00 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.395 Loss=1.500 Prec@1=63.231 Prec@5=85.420 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:00 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.395 Loss=1.500 Prec@1=63.231 Prec@5=85.420 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:00 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.620 DataTime=0.395 Loss=1.500 Prec@1=63.231 Prec@5=85.420 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:01 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.619 DataTime=0.394 Loss=1.501 Prec@1=63.229 Prec@5=85.393 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:01 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.619 DataTime=0.394 Loss=1.501 Prec@1=63.229 Prec@5=85.393 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:01 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.619 DataTime=0.394 Loss=1.501 Prec@1=63.229 Prec@5=85.393 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:02 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.502 Prec@1=63.170 Prec@5=85.373 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:02 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.502 Prec@1=63.170 Prec@5=85.373 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:02 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.502 Prec@1=63.170 Prec@5=85.373 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:03 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.505 Prec@1=63.119 Prec@5=85.323 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:03 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.505 Prec@1=63.119 Prec@5=85.323 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:03 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.394 Loss=1.505 Prec@1=63.119 Prec@5=85.323 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:04 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.393 Loss=1.507 Prec@1=63.086 Prec@5=85.300 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:04 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.393 Loss=1.507 Prec@1=63.086 Prec@5=85.300 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:04 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.618 DataTime=0.393 Loss=1.507 Prec@1=63.086 Prec@5=85.300 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:05 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.507 Prec@1=63.064 Prec@5=85.294 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:05 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.507 Prec@1=63.064 Prec@5=85.294 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:05 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.507 Prec@1=63.064 Prec@5=85.294 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:06 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.510 Prec@1=63.008 Prec@5=85.253 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:06 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.510 Prec@1=63.008 Prec@5=85.253 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:06 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.510 Prec@1=63.008 Prec@5=85.253 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:07 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.511 Prec@1=62.984 Prec@5=85.237 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:07 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.511 Prec@1=62.984 Prec@5=85.237 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=06:07 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.511 Prec@1=62.984 Prec@5=85.237 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=06:08 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.513 Prec@1=62.943 Prec@5=85.208 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=06:08 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.513 Prec@1=62.943 Prec@5=85.208 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:08 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.393 Loss=1.513 Prec@1=62.943 Prec@5=85.208 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:09 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.392 Loss=1.513 Prec@1=62.941 Prec@5=85.194 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:09 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.392 Loss=1.513 Prec@1=62.941 Prec@5=85.194 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=06:09 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.617 DataTime=0.392 Loss=1.513 Prec@1=62.941 Prec@5=85.194 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=06:10 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.936 Prec@5=85.187 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=06:10 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.936 Prec@5=85.187 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:10 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.936 Prec@5=85.187 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:11 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.928 Prec@5=85.175 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:11 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.928 Prec@5=85.175 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:11 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.928 Prec@5=85.175 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:12 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.920 Prec@5=85.175 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:12 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.920 Prec@5=85.175 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=06:12 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.514 Prec@1=62.920 Prec@5=85.175 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=06:13 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.515 Prec@1=62.922 Prec@5=85.163 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=06:13 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.515 Prec@1=62.922 Prec@5=85.163 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=06:13 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.515 Prec@1=62.922 Prec@5=85.163 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=06:14 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.516 Prec@1=62.911 Prec@5=85.149 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=06:14 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.516 Prec@1=62.911 Prec@5=85.149 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:14 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.516 Prec@1=62.911 Prec@5=85.149 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:15 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.517 Prec@1=62.907 Prec@5=85.147 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:15 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.517 Prec@1=62.907 Prec@5=85.147 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:15 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.517 Prec@1=62.907 Prec@5=85.147 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:16 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.901 Prec@5=85.133 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:16 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.901 Prec@5=85.133 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:16 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.901 Prec@5=85.133 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:17 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.893 Prec@5=85.122 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:17 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.893 Prec@5=85.122 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:17 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.616 DataTime=0.392 Loss=1.518 Prec@1=62.893 Prec@5=85.122 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:17 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.615 DataTime=0.392 Loss=1.518 Prec@1=62.892 Prec@5=85.122 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:17 IST=> training   100.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.615 DataTime=0.392 Loss=1.518 Prec@1=62.892 Prec@5=85.122 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=06:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 0.00% of 1x98...Epoch=36/150 LR=0.08716 Time=6.539 Loss=1.615 Prec@1=60.547 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.539 Loss=1.615 Prec@1=60.547 Prec@5=86.328 rate=2031.96 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.539 Loss=1.615 Prec@1=60.547 Prec@5=86.328 rate=2031.96 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=0.403 Loss=1.559 Prec@1=61.760 Prec@5=84.826 rate=2031.96 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST** validation 100.00% of 1x98...Epoch=36/150 LR=0.08716 Time=0.403 Loss=1.559 Prec@1=61.760 Prec@5=84.826 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=06:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST=> training   0.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.813 DataTime=5.511 Loss=1.553 Prec@1=61.914 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.813 DataTime=5.511 Loss=1.553 Prec@1=61.914 Prec@5=85.742 rate=6355.87 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.813 DataTime=5.511 Loss=1.553 Prec@1=61.914 Prec@5=85.742 rate=6355.87 Hz, eta=0:00:00, total=0:00:00, wall=06:19 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.661 DataTime=0.438 Loss=1.477 Prec@1=63.811 Prec@5=85.636 rate=6355.87 Hz, eta=0:00:00, total=0:00:00, wall=06:19 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.661 DataTime=0.438 Loss=1.477 Prec@1=63.811 Prec@5=85.636 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=06:19 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.661 DataTime=0.438 Loss=1.477 Prec@1=63.811 Prec@5=85.636 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=06:20 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.637 DataTime=0.414 Loss=1.480 Prec@1=63.724 Prec@5=85.684 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=06:20 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.637 DataTime=0.414 Loss=1.480 Prec@1=63.724 Prec@5=85.684 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=06:20 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.637 DataTime=0.414 Loss=1.480 Prec@1=63.724 Prec@5=85.684 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=06:21 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.627 DataTime=0.405 Loss=1.479 Prec@1=63.731 Prec@5=85.694 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=06:21 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.627 DataTime=0.405 Loss=1.479 Prec@1=63.731 Prec@5=85.694 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=06:21 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.627 DataTime=0.405 Loss=1.479 Prec@1=63.731 Prec@5=85.694 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=06:22 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.623 DataTime=0.401 Loss=1.480 Prec@1=63.672 Prec@5=85.641 rate=1.64 Hz, eta=0:22:18, total=0:03:03, wall=06:22 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.623 DataTime=0.401 Loss=1.480 Prec@1=63.672 Prec@5=85.641 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=06:22 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.623 DataTime=0.401 Loss=1.480 Prec@1=63.672 Prec@5=85.641 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=06:23 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.621 DataTime=0.399 Loss=1.482 Prec@1=63.609 Prec@5=85.605 rate=1.64 Hz, eta=0:21:19, total=0:04:04, wall=06:23 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.621 DataTime=0.399 Loss=1.482 Prec@1=63.609 Prec@5=85.605 rate=1.64 Hz, eta=0:20:20, total=0:05:05, wall=06:23 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.621 DataTime=0.399 Loss=1.482 Prec@1=63.609 Prec@5=85.605 rate=1.64 Hz, eta=0:20:20, total=0:05:05, wall=06:24 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.620 DataTime=0.397 Loss=1.487 Prec@1=63.482 Prec@5=85.519 rate=1.64 Hz, eta=0:20:20, total=0:05:05, wall=06:24 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.620 DataTime=0.397 Loss=1.487 Prec@1=63.482 Prec@5=85.519 rate=1.64 Hz, eta=0:19:20, total=0:06:06, wall=06:24 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.620 DataTime=0.397 Loss=1.487 Prec@1=63.482 Prec@5=85.519 rate=1.64 Hz, eta=0:19:20, total=0:06:06, wall=06:25 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.619 DataTime=0.396 Loss=1.487 Prec@1=63.508 Prec@5=85.503 rate=1.64 Hz, eta=0:19:20, total=0:06:06, wall=06:25 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.619 DataTime=0.396 Loss=1.487 Prec@1=63.508 Prec@5=85.503 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=06:25 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.619 DataTime=0.396 Loss=1.487 Prec@1=63.508 Prec@5=85.503 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=06:26 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.490 Prec@1=63.479 Prec@5=85.458 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=06:26 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.490 Prec@1=63.479 Prec@5=85.458 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=06:26 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.490 Prec@1=63.479 Prec@5=85.458 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=06:27 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.491 Prec@1=63.447 Prec@5=85.445 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=06:27 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.491 Prec@1=63.447 Prec@5=85.445 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=06:27 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.395 Loss=1.491 Prec@1=63.447 Prec@5=85.445 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=06:28 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.394 Loss=1.494 Prec@1=63.383 Prec@5=85.415 rate=1.64 Hz, eta=0:16:19, total=0:09:11, wall=06:28 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.394 Loss=1.494 Prec@1=63.383 Prec@5=85.415 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=06:28 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.618 DataTime=0.394 Loss=1.494 Prec@1=63.383 Prec@5=85.415 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=06:29 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.495 Prec@1=63.315 Prec@5=85.398 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=06:29 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.495 Prec@1=63.315 Prec@5=85.398 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=06:29 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.495 Prec@1=63.315 Prec@5=85.398 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=06:30 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.498 Prec@1=63.258 Prec@5=85.355 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=06:30 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.498 Prec@1=63.258 Prec@5=85.355 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=06:30 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.394 Loss=1.498 Prec@1=63.258 Prec@5=85.355 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=06:31 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.393 Loss=1.499 Prec@1=63.261 Prec@5=85.347 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.393 Loss=1.499 Prec@1=63.261 Prec@5=85.347 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.617 DataTime=0.393 Loss=1.499 Prec@1=63.261 Prec@5=85.347 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=06:32 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.500 Prec@1=63.231 Prec@5=85.338 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=06:32 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.500 Prec@1=63.231 Prec@5=85.338 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=06:32 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.500 Prec@1=63.231 Prec@5=85.338 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=06:33 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.502 Prec@1=63.197 Prec@5=85.314 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=06:33 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.502 Prec@1=63.197 Prec@5=85.314 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=06:33 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.502 Prec@1=63.197 Prec@5=85.314 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=06:34 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.504 Prec@1=63.176 Prec@5=85.303 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=06:34 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.504 Prec@1=63.176 Prec@5=85.303 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=06:34 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.504 Prec@1=63.176 Prec@5=85.303 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=06:35 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.505 Prec@1=63.155 Prec@5=85.285 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=06:35 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.505 Prec@1=63.155 Prec@5=85.285 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=06:35 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.393 Loss=1.505 Prec@1=63.155 Prec@5=85.285 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=06:36 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.505 Prec@1=63.159 Prec@5=85.288 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=06:36 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.505 Prec@1=63.159 Prec@5=85.288 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=06:36 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.505 Prec@1=63.159 Prec@5=85.288 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=06:37 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.136 Prec@5=85.277 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=06:37 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.136 Prec@5=85.277 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=06:37 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.136 Prec@5=85.277 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=06:38 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.123 Prec@5=85.278 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=06:38 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.123 Prec@5=85.278 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=06:38 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.506 Prec@1=63.123 Prec@5=85.278 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=06:39 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.507 Prec@1=63.120 Prec@5=85.275 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=06:39 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.507 Prec@1=63.120 Prec@5=85.275 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=06:39 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.507 Prec@1=63.120 Prec@5=85.275 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=06:40 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.508 Prec@1=63.096 Prec@5=85.268 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=06:40 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.508 Prec@1=63.096 Prec@5=85.268 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=06:40 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.616 DataTime=0.392 Loss=1.508 Prec@1=63.096 Prec@5=85.268 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=06:41 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.509 Prec@1=63.086 Prec@5=85.254 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=06:41 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.509 Prec@1=63.086 Prec@5=85.254 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=06:41 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.509 Prec@1=63.086 Prec@5=85.254 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=06:42 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.072 Prec@5=85.231 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=06:42 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.072 Prec@5=85.231 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=06:42 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.072 Prec@5=85.231 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=06:43 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.071 Prec@5=85.230 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=06:43 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.071 Prec@5=85.230 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=06:43 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.071 Prec@5=85.230 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=06:43 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.070 Prec@5=85.229 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=06:43 IST=> training   100.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.615 DataTime=0.392 Loss=1.510 Prec@1=63.070 Prec@5=85.229 rate=1.63 Hz, eta=0:00:00, total=0:25:33, wall=06:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> validation 0.00% of 1x98...Epoch=37/150 LR=0.08645 Time=7.102 Loss=1.494 Prec@1=63.672 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=7.102 Loss=1.494 Prec@1=63.672 Prec@5=85.547 rate=4254.36 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=7.102 Loss=1.494 Prec@1=63.672 Prec@5=85.547 rate=4254.36 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=0.410 Loss=1.609 Prec@1=60.866 Prec@5=84.170 rate=4254.36 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST** validation 100.00% of 1x98...Epoch=37/150 LR=0.08645 Time=0.410 Loss=1.609 Prec@1=60.866 Prec@5=84.170 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=06:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> training   0.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.493 DataTime=5.206 Loss=1.364 Prec@1=66.211 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=06:44 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.493 DataTime=5.206 Loss=1.364 Prec@1=66.211 Prec@5=88.281 rate=7682.21 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.493 DataTime=5.206 Loss=1.364 Prec@1=66.211 Prec@5=88.281 rate=7682.21 Hz, eta=0:00:00, total=0:00:00, wall=06:45 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.658 DataTime=0.435 Loss=1.475 Prec@1=63.513 Prec@5=85.680 rate=7682.21 Hz, eta=0:00:00, total=0:00:00, wall=06:45 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.658 DataTime=0.435 Loss=1.475 Prec@1=63.513 Prec@5=85.680 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=06:45 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.658 DataTime=0.435 Loss=1.475 Prec@1=63.513 Prec@5=85.680 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=06:46 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.636 DataTime=0.412 Loss=1.463 Prec@1=63.816 Prec@5=85.833 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=06:46 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.636 DataTime=0.412 Loss=1.463 Prec@1=63.816 Prec@5=85.833 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=06:46 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.636 DataTime=0.412 Loss=1.463 Prec@1=63.816 Prec@5=85.833 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=06:47 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.628 DataTime=0.404 Loss=1.461 Prec@1=63.976 Prec@5=85.877 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=06:47 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.628 DataTime=0.404 Loss=1.461 Prec@1=63.976 Prec@5=85.877 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=06:47 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.628 DataTime=0.404 Loss=1.461 Prec@1=63.976 Prec@5=85.877 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=06:48 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.624 DataTime=0.400 Loss=1.467 Prec@1=63.828 Prec@5=85.834 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=06:48 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.624 DataTime=0.400 Loss=1.467 Prec@1=63.828 Prec@5=85.834 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=06:48 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.624 DataTime=0.400 Loss=1.467 Prec@1=63.828 Prec@5=85.834 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=06:49 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.622 DataTime=0.398 Loss=1.474 Prec@1=63.669 Prec@5=85.747 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=06:49 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.622 DataTime=0.398 Loss=1.474 Prec@1=63.669 Prec@5=85.747 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=06:49 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.622 DataTime=0.398 Loss=1.474 Prec@1=63.669 Prec@5=85.747 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=06:50 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.620 DataTime=0.397 Loss=1.476 Prec@1=63.634 Prec@5=85.712 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=06:50 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.620 DataTime=0.397 Loss=1.476 Prec@1=63.634 Prec@5=85.712 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:50 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.620 DataTime=0.397 Loss=1.476 Prec@1=63.634 Prec@5=85.712 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:51 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.396 Loss=1.476 Prec@1=63.684 Prec@5=85.707 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.396 Loss=1.476 Prec@1=63.684 Prec@5=85.707 rate=1.64 Hz, eta=0:18:22, total=0:07:08, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.396 Loss=1.476 Prec@1=63.684 Prec@5=85.707 rate=1.64 Hz, eta=0:18:22, total=0:07:08, wall=06:52 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.395 Loss=1.480 Prec@1=63.562 Prec@5=85.654 rate=1.64 Hz, eta=0:18:22, total=0:07:08, wall=06:52 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.395 Loss=1.480 Prec@1=63.562 Prec@5=85.654 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:52 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.619 DataTime=0.395 Loss=1.480 Prec@1=63.562 Prec@5=85.654 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:53 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.483 Prec@1=63.488 Prec@5=85.622 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:53 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.483 Prec@1=63.488 Prec@5=85.622 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:53 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.483 Prec@1=63.488 Prec@5=85.622 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:54 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.485 Prec@1=63.468 Prec@5=85.600 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:54 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.485 Prec@1=63.468 Prec@5=85.600 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:54 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.618 DataTime=0.394 Loss=1.485 Prec@1=63.468 Prec@5=85.600 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:55 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.394 Loss=1.485 Prec@1=63.481 Prec@5=85.619 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:55 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.394 Loss=1.485 Prec@1=63.481 Prec@5=85.619 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:55 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.394 Loss=1.485 Prec@1=63.481 Prec@5=85.619 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:56 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.486 Prec@1=63.463 Prec@5=85.598 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:56 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.486 Prec@1=63.463 Prec@5=85.598 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:56 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.486 Prec@1=63.463 Prec@5=85.598 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:58 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.488 Prec@1=63.413 Prec@5=85.581 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:58 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.488 Prec@1=63.413 Prec@5=85.581 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:58 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.488 Prec@1=63.413 Prec@5=85.581 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:59 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.394 Prec@5=85.565 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:59 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.394 Prec@5=85.565 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=06:59 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.394 Prec@5=85.565 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=07:00 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.398 Prec@5=85.558 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=07:00 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.398 Prec@5=85.558 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:00 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.393 Loss=1.489 Prec@1=63.398 Prec@5=85.558 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:01 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.392 Loss=1.491 Prec@1=63.383 Prec@5=85.533 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:01 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.392 Loss=1.491 Prec@1=63.383 Prec@5=85.533 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=07:01 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.617 DataTime=0.392 Loss=1.491 Prec@1=63.383 Prec@5=85.533 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=07:02 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.492 Prec@1=63.368 Prec@5=85.527 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=07:02 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.492 Prec@1=63.368 Prec@5=85.527 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:02 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.492 Prec@1=63.368 Prec@5=85.527 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:03 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.493 Prec@1=63.336 Prec@5=85.507 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:03 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.493 Prec@1=63.336 Prec@5=85.507 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:03 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.493 Prec@1=63.336 Prec@5=85.507 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:04 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.494 Prec@1=63.323 Prec@5=85.495 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:04 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.494 Prec@1=63.323 Prec@5=85.495 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:04 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.494 Prec@1=63.323 Prec@5=85.495 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:05 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.495 Prec@1=63.302 Prec@5=85.472 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:05 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.495 Prec@1=63.302 Prec@5=85.472 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:05 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.495 Prec@1=63.302 Prec@5=85.472 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:06 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.496 Prec@1=63.292 Prec@5=85.451 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:06 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.496 Prec@1=63.292 Prec@5=85.451 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:06 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.496 Prec@1=63.292 Prec@5=85.451 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:07 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.498 Prec@1=63.281 Prec@5=85.434 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:07 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.498 Prec@1=63.281 Prec@5=85.434 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:07 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.498 Prec@1=63.281 Prec@5=85.434 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:08 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.499 Prec@1=63.267 Prec@5=85.411 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:08 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.499 Prec@1=63.267 Prec@5=85.411 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:08 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.499 Prec@1=63.267 Prec@5=85.411 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:09 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.500 Prec@1=63.240 Prec@5=85.387 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:09 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.500 Prec@1=63.240 Prec@5=85.387 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:09 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.500 Prec@1=63.240 Prec@5=85.387 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:10 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.501 Prec@1=63.235 Prec@5=85.365 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:10 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.501 Prec@1=63.235 Prec@5=85.365 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:10 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.501 Prec@1=63.235 Prec@5=85.365 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:10 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.501 Prec@1=63.235 Prec@5=85.364 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:10 IST=> training   100.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.616 DataTime=0.392 Loss=1.501 Prec@1=63.235 Prec@5=85.364 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=07:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:10 IST=> validation 0.00% of 1x98...Epoch=38/150 LR=0.08572 Time=6.871 Loss=1.387 Prec@1=66.211 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=07:10 IST=> validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=6.871 Loss=1.387 Prec@1=66.211 Prec@5=88.086 rate=2732.25 Hz, eta=0:00:00, total=0:00:00, wall=07:10 IST** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=6.871 Loss=1.387 Prec@1=66.211 Prec@5=88.086 rate=2732.25 Hz, eta=0:00:00, total=0:00:00, wall=07:10 IST** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=0.410 Loss=1.521 Prec@1=62.568 Prec@5=85.478 rate=2732.25 Hz, eta=0:00:00, total=0:00:00, wall=07:10 IST** validation 100.00% of 1x98...Epoch=38/150 LR=0.08572 Time=0.410 Loss=1.521 Prec@1=62.568 Prec@5=85.478 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=07:10 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST=> training   0.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.174 DataTime=4.848 Loss=1.443 Prec@1=66.016 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.174 DataTime=4.848 Loss=1.443 Prec@1=66.016 Prec@5=83.984 rate=3270.06 Hz, eta=0:00:00, total=0:00:00, wall=07:11 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.174 DataTime=4.848 Loss=1.443 Prec@1=66.016 Prec@5=83.984 rate=3270.06 Hz, eta=0:00:00, total=0:00:00, wall=07:12 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.656 DataTime=0.430 Loss=1.456 Prec@1=64.122 Prec@5=86.026 rate=3270.06 Hz, eta=0:00:00, total=0:00:00, wall=07:12 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.656 DataTime=0.430 Loss=1.456 Prec@1=64.122 Prec@5=86.026 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=07:12 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.656 DataTime=0.430 Loss=1.456 Prec@1=64.122 Prec@5=86.026 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=07:13 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.634 DataTime=0.409 Loss=1.456 Prec@1=64.055 Prec@5=86.096 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=07:13 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.634 DataTime=0.409 Loss=1.456 Prec@1=64.055 Prec@5=86.096 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=07:13 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.634 DataTime=0.409 Loss=1.456 Prec@1=64.055 Prec@5=86.096 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=07:14 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.627 DataTime=0.402 Loss=1.461 Prec@1=63.960 Prec@5=85.965 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=07:14 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.627 DataTime=0.402 Loss=1.461 Prec@1=63.960 Prec@5=85.965 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=07:14 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.627 DataTime=0.402 Loss=1.461 Prec@1=63.960 Prec@5=85.965 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=07:15 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.623 DataTime=0.399 Loss=1.461 Prec@1=63.973 Prec@5=85.967 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=07:15 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.623 DataTime=0.399 Loss=1.461 Prec@1=63.973 Prec@5=85.967 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=07:15 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.623 DataTime=0.399 Loss=1.461 Prec@1=63.973 Prec@5=85.967 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=07:16 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.621 DataTime=0.396 Loss=1.462 Prec@1=63.943 Prec@5=85.934 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=07:16 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.621 DataTime=0.396 Loss=1.462 Prec@1=63.943 Prec@5=85.934 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=07:16 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.621 DataTime=0.396 Loss=1.462 Prec@1=63.943 Prec@5=85.934 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=07:17 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.619 DataTime=0.395 Loss=1.465 Prec@1=63.895 Prec@5=85.856 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=07:17 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.619 DataTime=0.395 Loss=1.465 Prec@1=63.895 Prec@5=85.856 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=07:17 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.619 DataTime=0.395 Loss=1.465 Prec@1=63.895 Prec@5=85.856 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=07:18 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.844 Prec@5=85.829 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=07:18 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.844 Prec@5=85.829 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=07:18 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.844 Prec@5=85.829 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=07:19 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.470 Prec@1=63.776 Prec@5=85.778 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=07:19 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.470 Prec@1=63.776 Prec@5=85.778 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=07:19 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.618 DataTime=0.394 Loss=1.470 Prec@1=63.776 Prec@5=85.778 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=07:20 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.471 Prec@1=63.737 Prec@5=85.758 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=07:20 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.471 Prec@1=63.737 Prec@5=85.758 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=07:20 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.471 Prec@1=63.737 Prec@5=85.758 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=07:21 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.473 Prec@1=63.711 Prec@5=85.730 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=07:21 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.473 Prec@1=63.711 Prec@5=85.730 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=07:21 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.617 DataTime=0.393 Loss=1.473 Prec@1=63.711 Prec@5=85.730 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=07:22 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.694 Prec@5=85.724 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=07:22 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.694 Prec@5=85.724 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=07:22 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.694 Prec@5=85.724 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=07:23 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.650 Prec@5=85.693 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=07:23 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.650 Prec@5=85.693 rate=1.63 Hz, eta=0:13:16, total=0:12:14, wall=07:23 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.650 Prec@5=85.693 rate=1.63 Hz, eta=0:13:16, total=0:12:14, wall=07:24 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.478 Prec@1=63.603 Prec@5=85.663 rate=1.63 Hz, eta=0:13:16, total=0:12:14, wall=07:24 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.478 Prec@1=63.603 Prec@5=85.663 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=07:24 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.478 Prec@1=63.603 Prec@5=85.663 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=07:25 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.479 Prec@1=63.594 Prec@5=85.649 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=07:25 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.479 Prec@1=63.594 Prec@5=85.649 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=07:25 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.479 Prec@1=63.594 Prec@5=85.649 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=07:26 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.482 Prec@1=63.562 Prec@5=85.609 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=07:26 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.482 Prec@1=63.562 Prec@5=85.609 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:26 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.616 DataTime=0.392 Loss=1.482 Prec@1=63.562 Prec@5=85.609 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:27 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.560 Prec@5=85.592 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:27 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.560 Prec@5=85.592 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:27 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.560 Prec@5=85.592 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:28 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.515 Prec@5=85.559 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:28 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.515 Prec@5=85.559 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=07:28 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.515 Prec@5=85.559 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=07:29 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.490 Prec@5=85.552 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=07:29 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.490 Prec@5=85.552 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:29 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.490 Prec@5=85.552 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:30 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.489 Prec@5=85.546 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:30 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.489 Prec@5=85.546 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:30 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.489 Prec@5=85.546 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:31 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.477 Prec@5=85.543 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:31 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.477 Prec@5=85.543 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=07:31 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.477 Prec@5=85.543 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=07:32 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.488 Prec@1=63.469 Prec@5=85.537 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=07:32 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.488 Prec@1=63.469 Prec@5=85.537 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:32 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.488 Prec@1=63.469 Prec@5=85.537 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:33 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.489 Prec@1=63.449 Prec@5=85.519 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:33 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.489 Prec@1=63.449 Prec@5=85.519 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=07:33 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.489 Prec@1=63.449 Prec@5=85.519 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=07:34 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.490 Prec@1=63.440 Prec@5=85.518 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=07:34 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.490 Prec@1=63.440 Prec@5=85.518 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=07:34 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.490 Prec@1=63.440 Prec@5=85.518 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=07:35 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.491 Prec@1=63.429 Prec@5=85.501 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=07:35 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.491 Prec@1=63.429 Prec@5=85.501 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=07:35 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.491 Prec@1=63.429 Prec@5=85.501 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=07:36 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.492 Prec@1=63.411 Prec@5=85.483 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=07:36 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.492 Prec@1=63.411 Prec@5=85.483 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=07:36 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.492 Prec@1=63.411 Prec@5=85.483 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=07:36 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.492 Prec@1=63.412 Prec@5=85.483 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=07:36 IST=> training   100.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.615 DataTime=0.391 Loss=1.492 Prec@1=63.412 Prec@5=85.483 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=07:36 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> validation 0.00% of 1x98...Epoch=39/150 LR=0.08498 Time=7.479 Loss=1.585 Prec@1=62.500 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.479 Loss=1.585 Prec@1=62.500 Prec@5=83.203 rate=7442.58 Hz, eta=0:00:00, total=0:00:00, wall=07:36 IST** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.479 Loss=1.585 Prec@1=62.500 Prec@5=83.203 rate=7442.58 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=0.405 Loss=1.540 Prec@1=62.456 Prec@5=85.092 rate=7442.58 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST** validation 100.00% of 1x98...Epoch=39/150 LR=0.08498 Time=0.405 Loss=1.540 Prec@1=62.456 Prec@5=85.092 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=07:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.939 DataTime=4.619 Loss=1.499 Prec@1=61.523 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.939 DataTime=4.619 Loss=1.499 Prec@1=61.523 Prec@5=85.156 rate=5280.72 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.939 DataTime=4.619 Loss=1.499 Prec@1=61.523 Prec@5=85.156 rate=5280.72 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.654 DataTime=0.427 Loss=1.465 Prec@1=63.921 Prec@5=85.762 rate=5280.72 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.654 DataTime=0.427 Loss=1.465 Prec@1=63.921 Prec@5=85.762 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=07:38 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.654 DataTime=0.427 Loss=1.465 Prec@1=63.921 Prec@5=85.762 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=07:39 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.634 DataTime=0.408 Loss=1.463 Prec@1=63.976 Prec@5=85.809 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.634 DataTime=0.408 Loss=1.463 Prec@1=63.976 Prec@5=85.809 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.634 DataTime=0.408 Loss=1.463 Prec@1=63.976 Prec@5=85.809 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:40 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.627 DataTime=0.401 Loss=1.456 Prec@1=64.221 Prec@5=85.904 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:40 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.627 DataTime=0.401 Loss=1.456 Prec@1=64.221 Prec@5=85.904 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:40 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.627 DataTime=0.401 Loss=1.456 Prec@1=64.221 Prec@5=85.904 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:41 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.623 DataTime=0.399 Loss=1.460 Prec@1=64.112 Prec@5=85.892 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:41 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.623 DataTime=0.399 Loss=1.460 Prec@1=64.112 Prec@5=85.892 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:41 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.623 DataTime=0.399 Loss=1.460 Prec@1=64.112 Prec@5=85.892 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:42 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.622 DataTime=0.397 Loss=1.464 Prec@1=63.983 Prec@5=85.851 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:42 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.622 DataTime=0.397 Loss=1.464 Prec@1=63.983 Prec@5=85.851 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:42 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.622 DataTime=0.397 Loss=1.464 Prec@1=63.983 Prec@5=85.851 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:43 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.620 DataTime=0.396 Loss=1.462 Prec@1=64.014 Prec@5=85.883 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:43 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.620 DataTime=0.396 Loss=1.462 Prec@1=64.014 Prec@5=85.883 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=07:43 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.620 DataTime=0.396 Loss=1.462 Prec@1=64.014 Prec@5=85.883 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=07:44 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.619 DataTime=0.395 Loss=1.463 Prec@1=63.976 Prec@5=85.864 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=07:44 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.619 DataTime=0.395 Loss=1.463 Prec@1=63.976 Prec@5=85.864 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=07:44 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.619 DataTime=0.395 Loss=1.463 Prec@1=63.976 Prec@5=85.864 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=07:45 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.899 Prec@5=85.833 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=07:45 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.899 Prec@5=85.833 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:45 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.467 Prec@1=63.899 Prec@5=85.833 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:46 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.469 Prec@1=63.871 Prec@5=85.799 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:46 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.469 Prec@1=63.871 Prec@5=85.799 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:46 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.618 DataTime=0.394 Loss=1.469 Prec@1=63.871 Prec@5=85.799 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:47 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.470 Prec@1=63.852 Prec@5=85.785 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:47 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.470 Prec@1=63.852 Prec@5=85.785 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=07:47 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.470 Prec@1=63.852 Prec@5=85.785 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=07:48 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.472 Prec@1=63.793 Prec@5=85.765 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=07:48 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.472 Prec@1=63.793 Prec@5=85.765 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:48 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.472 Prec@1=63.793 Prec@5=85.765 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:49 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.474 Prec@1=63.774 Prec@5=85.743 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:49 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.474 Prec@1=63.774 Prec@5=85.743 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:49 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.617 DataTime=0.393 Loss=1.474 Prec@1=63.774 Prec@5=85.743 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:50 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.727 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:50 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.727 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=07:50 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.727 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=07:51 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.721 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=07:51 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.721 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=07:51 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.736 Prec@5=85.721 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=07:52 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.744 Prec@5=85.712 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=07:52 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.744 Prec@5=85.712 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:52 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.744 Prec@5=85.712 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:53 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.721 Prec@5=85.672 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=07:53 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.721 Prec@5=85.672 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:53 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.721 Prec@5=85.672 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:54 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.478 Prec@1=63.710 Prec@5=85.665 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=07:54 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.478 Prec@1=63.710 Prec@5=85.665 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:54 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.478 Prec@1=63.710 Prec@5=85.665 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:55 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.479 Prec@1=63.692 Prec@5=85.651 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:55 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.479 Prec@1=63.692 Prec@5=85.651 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:55 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.392 Loss=1.479 Prec@1=63.692 Prec@5=85.651 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:56 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.480 Prec@1=63.653 Prec@5=85.633 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=07:56 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.480 Prec@1=63.653 Prec@5=85.633 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:56 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.480 Prec@1=63.653 Prec@5=85.633 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:57 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.481 Prec@1=63.627 Prec@5=85.621 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=07:57 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.481 Prec@1=63.627 Prec@5=85.621 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:57 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.481 Prec@1=63.627 Prec@5=85.621 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:58 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.599 Prec@5=85.598 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.599 Prec@5=85.598 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.483 Prec@1=63.599 Prec@5=85.598 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:59 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.484 Prec@1=63.582 Prec@5=85.585 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=07:59 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.484 Prec@1=63.582 Prec@5=85.585 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=07:59 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.484 Prec@1=63.582 Prec@5=85.585 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=08:00 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.554 Prec@5=85.567 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=08:00 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.554 Prec@5=85.567 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:00 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.485 Prec@1=63.554 Prec@5=85.567 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:02 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.547 Prec@5=85.564 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:02 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.547 Prec@5=85.564 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:02 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.486 Prec@1=63.547 Prec@5=85.564 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:03 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.521 Prec@5=85.548 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:03 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.521 Prec@5=85.548 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:03 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.521 Prec@5=85.548 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:03 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.521 Prec@5=85.549 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:03 IST=> training   100.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.615 DataTime=0.391 Loss=1.487 Prec@1=63.521 Prec@5=85.549 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=08:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> validation 0.00% of 1x98...Epoch=40/150 LR=0.08423 Time=6.298 Loss=1.587 Prec@1=61.328 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=6.298 Loss=1.587 Prec@1=61.328 Prec@5=85.547 rate=3450.86 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=6.298 Loss=1.587 Prec@1=61.328 Prec@5=85.547 rate=3450.86 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=0.400 Loss=1.561 Prec@1=61.588 Prec@5=84.930 rate=3450.86 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST** validation 100.00% of 1x98...Epoch=40/150 LR=0.08423 Time=0.400 Loss=1.561 Prec@1=61.588 Prec@5=84.930 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=08:03 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> training   0.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=5.719 DataTime=5.405 Loss=1.460 Prec@1=63.477 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=08:03 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=5.719 DataTime=5.405 Loss=1.460 Prec@1=63.477 Prec@5=85.156 rate=7725.64 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=5.719 DataTime=5.405 Loss=1.460 Prec@1=63.477 Prec@5=85.156 rate=7725.64 Hz, eta=0:00:00, total=0:00:00, wall=08:04 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.660 DataTime=0.434 Loss=1.436 Prec@1=64.585 Prec@5=86.288 rate=7725.64 Hz, eta=0:00:00, total=0:00:00, wall=08:04 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.660 DataTime=0.434 Loss=1.436 Prec@1=64.585 Prec@5=86.288 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=08:04 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.660 DataTime=0.434 Loss=1.436 Prec@1=64.585 Prec@5=86.288 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=08:05 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.636 DataTime=0.412 Loss=1.435 Prec@1=64.598 Prec@5=86.253 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=08:05 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.636 DataTime=0.412 Loss=1.435 Prec@1=64.598 Prec@5=86.253 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=08:05 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.636 DataTime=0.412 Loss=1.435 Prec@1=64.598 Prec@5=86.253 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=08:06 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.628 DataTime=0.404 Loss=1.438 Prec@1=64.526 Prec@5=86.233 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=08:06 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.628 DataTime=0.404 Loss=1.438 Prec@1=64.526 Prec@5=86.233 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=08:06 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.628 DataTime=0.404 Loss=1.438 Prec@1=64.526 Prec@5=86.233 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=08:07 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.624 DataTime=0.400 Loss=1.440 Prec@1=64.425 Prec@5=86.197 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=08:07 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.624 DataTime=0.400 Loss=1.440 Prec@1=64.425 Prec@5=86.197 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=08:07 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.624 DataTime=0.400 Loss=1.440 Prec@1=64.425 Prec@5=86.197 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=08:08 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.622 DataTime=0.398 Loss=1.444 Prec@1=64.299 Prec@5=86.152 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=08:08 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.622 DataTime=0.398 Loss=1.444 Prec@1=64.299 Prec@5=86.152 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=08:08 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.622 DataTime=0.398 Loss=1.444 Prec@1=64.299 Prec@5=86.152 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=08:09 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.621 DataTime=0.397 Loss=1.447 Prec@1=64.252 Prec@5=86.115 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=08:09 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.621 DataTime=0.397 Loss=1.447 Prec@1=64.252 Prec@5=86.115 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=08:09 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.621 DataTime=0.397 Loss=1.447 Prec@1=64.252 Prec@5=86.115 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=08:11 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.620 DataTime=0.396 Loss=1.451 Prec@1=64.215 Prec@5=86.050 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=08:11 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.620 DataTime=0.396 Loss=1.451 Prec@1=64.215 Prec@5=86.050 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=08:11 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.620 DataTime=0.396 Loss=1.451 Prec@1=64.215 Prec@5=86.050 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=08:12 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.453 Prec@1=64.185 Prec@5=86.033 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=08:12 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.453 Prec@1=64.185 Prec@5=86.033 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:12 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.453 Prec@1=64.185 Prec@5=86.033 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:13 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.456 Prec@1=64.114 Prec@5=85.987 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:13 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.456 Prec@1=64.114 Prec@5=85.987 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:13 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.619 DataTime=0.395 Loss=1.456 Prec@1=64.114 Prec@5=85.987 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:14 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.457 Prec@1=64.076 Prec@5=85.951 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:14 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.457 Prec@1=64.076 Prec@5=85.951 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:14 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.457 Prec@1=64.076 Prec@5=85.951 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:15 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.459 Prec@1=64.030 Prec@5=85.912 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:15 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.459 Prec@1=64.030 Prec@5=85.912 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:15 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.394 Loss=1.459 Prec@1=64.030 Prec@5=85.912 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:16 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.393 Loss=1.462 Prec@1=63.985 Prec@5=85.874 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:16 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.393 Loss=1.462 Prec@1=63.985 Prec@5=85.874 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:16 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.618 DataTime=0.393 Loss=1.462 Prec@1=63.985 Prec@5=85.874 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:17 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.465 Prec@1=63.945 Prec@5=85.828 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:17 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.465 Prec@1=63.945 Prec@5=85.828 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:17 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.465 Prec@1=63.945 Prec@5=85.828 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:18 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.466 Prec@1=63.912 Prec@5=85.793 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:18 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.466 Prec@1=63.912 Prec@5=85.793 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:18 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.466 Prec@1=63.912 Prec@5=85.793 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:19 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.468 Prec@1=63.893 Prec@5=85.763 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:19 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.468 Prec@1=63.893 Prec@5=85.763 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:19 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.468 Prec@1=63.893 Prec@5=85.763 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:20 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.469 Prec@1=63.879 Prec@5=85.743 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:20 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.469 Prec@1=63.879 Prec@5=85.743 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=08:20 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.393 Loss=1.469 Prec@1=63.879 Prec@5=85.743 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=08:21 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.469 Prec@1=63.872 Prec@5=85.744 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=08:21 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.469 Prec@1=63.872 Prec@5=85.744 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=08:21 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.469 Prec@1=63.872 Prec@5=85.744 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=08:22 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.470 Prec@1=63.864 Prec@5=85.745 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=08:22 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.470 Prec@1=63.864 Prec@5=85.745 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:22 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.617 DataTime=0.392 Loss=1.470 Prec@1=63.864 Prec@5=85.745 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:23 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.471 Prec@1=63.839 Prec@5=85.734 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:23 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.471 Prec@1=63.839 Prec@5=85.734 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:23 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.471 Prec@1=63.839 Prec@5=85.734 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:24 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.472 Prec@1=63.816 Prec@5=85.727 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:24 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.472 Prec@1=63.816 Prec@5=85.727 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=08:24 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.472 Prec@1=63.816 Prec@5=85.727 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=08:25 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.473 Prec@1=63.793 Prec@5=85.704 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=08:25 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.473 Prec@1=63.793 Prec@5=85.704 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:25 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.473 Prec@1=63.793 Prec@5=85.704 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:26 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.768 Prec@5=85.692 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:26 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.768 Prec@5=85.692 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:26 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.474 Prec@1=63.768 Prec@5=85.692 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:27 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.750 Prec@5=85.679 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:27 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.750 Prec@5=85.679 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:27 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.475 Prec@1=63.750 Prec@5=85.679 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:28 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.755 Prec@5=85.671 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:28 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.755 Prec@5=85.671 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:28 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.476 Prec@1=63.755 Prec@5=85.671 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:29 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.747 Prec@5=85.658 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:29 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.747 Prec@5=85.658 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:29 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.747 Prec@5=85.658 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:29 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.745 Prec@5=85.657 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:29 IST=> training   100.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.616 DataTime=0.392 Loss=1.477 Prec@1=63.745 Prec@5=85.657 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=08:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:29 IST=> validation 0.00% of 1x98...Epoch=41/150 LR=0.08346 Time=6.469 Loss=1.390 Prec@1=64.062 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=08:29 IST=> validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=6.469 Loss=1.390 Prec@1=64.062 Prec@5=86.719 rate=8066.92 Hz, eta=0:00:00, total=0:00:00, wall=08:29 IST** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=6.469 Loss=1.390 Prec@1=64.062 Prec@5=86.719 rate=8066.92 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=0.399 Loss=1.595 Prec@1=60.986 Prec@5=84.166 rate=8066.92 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST** validation 100.00% of 1x98...Epoch=41/150 LR=0.08346 Time=0.399 Loss=1.595 Prec@1=60.986 Prec@5=84.166 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=08:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> training   0.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.622 DataTime=4.322 Loss=1.619 Prec@1=59.961 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.622 DataTime=4.322 Loss=1.619 Prec@1=59.961 Prec@5=83.789 rate=2509.83 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.622 DataTime=4.322 Loss=1.619 Prec@1=59.961 Prec@5=83.789 rate=2509.83 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.654 DataTime=0.428 Loss=1.437 Prec@1=64.345 Prec@5=86.361 rate=2509.83 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.654 DataTime=0.428 Loss=1.437 Prec@1=64.345 Prec@5=86.361 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=08:31 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.654 DataTime=0.428 Loss=1.437 Prec@1=64.345 Prec@5=86.361 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=08:32 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.634 DataTime=0.410 Loss=1.435 Prec@1=64.415 Prec@5=86.355 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=08:32 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.634 DataTime=0.410 Loss=1.435 Prec@1=64.415 Prec@5=86.355 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=08:32 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.634 DataTime=0.410 Loss=1.435 Prec@1=64.415 Prec@5=86.355 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=08:33 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.627 DataTime=0.402 Loss=1.440 Prec@1=64.302 Prec@5=86.303 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=08:33 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.627 DataTime=0.402 Loss=1.440 Prec@1=64.302 Prec@5=86.303 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=08:33 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.627 DataTime=0.402 Loss=1.440 Prec@1=64.302 Prec@5=86.303 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=08:34 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.623 DataTime=0.399 Loss=1.442 Prec@1=64.316 Prec@5=86.255 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=08:34 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.623 DataTime=0.399 Loss=1.442 Prec@1=64.316 Prec@5=86.255 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:34 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.623 DataTime=0.399 Loss=1.442 Prec@1=64.316 Prec@5=86.255 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:35 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.621 DataTime=0.397 Loss=1.445 Prec@1=64.282 Prec@5=86.183 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:35 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.621 DataTime=0.397 Loss=1.445 Prec@1=64.282 Prec@5=86.183 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:35 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.621 DataTime=0.397 Loss=1.445 Prec@1=64.282 Prec@5=86.183 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:36 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.620 DataTime=0.396 Loss=1.447 Prec@1=64.232 Prec@5=86.162 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:36 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.620 DataTime=0.396 Loss=1.447 Prec@1=64.232 Prec@5=86.162 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:36 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.620 DataTime=0.396 Loss=1.447 Prec@1=64.232 Prec@5=86.162 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:37 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.619 DataTime=0.395 Loss=1.447 Prec@1=64.242 Prec@5=86.179 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:37 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.619 DataTime=0.395 Loss=1.447 Prec@1=64.242 Prec@5=86.179 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:37 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.619 DataTime=0.395 Loss=1.447 Prec@1=64.242 Prec@5=86.179 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:38 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.449 Prec@1=64.198 Prec@5=86.155 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:38 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.449 Prec@1=64.198 Prec@5=86.155 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:38 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.449 Prec@1=64.198 Prec@5=86.155 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:39 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.450 Prec@1=64.227 Prec@5=86.136 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:39 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.450 Prec@1=64.227 Prec@5=86.136 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:39 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.618 DataTime=0.394 Loss=1.450 Prec@1=64.227 Prec@5=86.136 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:40 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.451 Prec@1=64.192 Prec@5=86.124 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:40 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.451 Prec@1=64.192 Prec@5=86.124 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:40 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.451 Prec@1=64.192 Prec@5=86.124 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:41 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.171 Prec@5=86.083 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:41 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.171 Prec@5=86.083 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:41 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.171 Prec@5=86.083 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:42 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.183 Prec@5=86.073 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:42 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.183 Prec@5=86.073 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:42 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.617 DataTime=0.393 Loss=1.453 Prec@1=64.183 Prec@5=86.073 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:43 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.393 Loss=1.455 Prec@1=64.157 Prec@5=86.031 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:43 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.393 Loss=1.455 Prec@1=64.157 Prec@5=86.031 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:43 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.393 Loss=1.455 Prec@1=64.157 Prec@5=86.031 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:44 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.143 Prec@5=86.026 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:44 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.143 Prec@5=86.026 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:44 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.143 Prec@5=86.026 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:45 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.105 Prec@5=86.008 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:45 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.105 Prec@5=86.008 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:45 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.105 Prec@5=86.008 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:46 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.072 Prec@5=85.985 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:46 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.072 Prec@5=85.985 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:46 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.072 Prec@5=85.985 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:47 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.059 Prec@5=85.972 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:47 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.059 Prec@5=85.972 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:47 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.459 Prec@1=64.059 Prec@5=85.972 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:48 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.461 Prec@1=64.029 Prec@5=85.938 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:48 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.461 Prec@1=64.029 Prec@5=85.938 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:48 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.616 DataTime=0.392 Loss=1.461 Prec@1=64.029 Prec@5=85.938 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:49 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.462 Prec@1=64.009 Prec@5=85.927 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:49 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.462 Prec@1=64.009 Prec@5=85.927 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:49 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.462 Prec@1=64.009 Prec@5=85.927 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:50 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.463 Prec@1=63.981 Prec@5=85.918 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:50 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.463 Prec@1=63.981 Prec@5=85.918 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:50 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.463 Prec@1=63.981 Prec@5=85.918 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:51 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.464 Prec@1=63.966 Prec@5=85.908 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:51 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.464 Prec@1=63.966 Prec@5=85.908 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=08:51 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.392 Loss=1.464 Prec@1=63.966 Prec@5=85.908 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=08:52 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.465 Prec@1=63.967 Prec@5=85.894 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=08:52 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.465 Prec@1=63.967 Prec@5=85.894 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:52 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.465 Prec@1=63.967 Prec@5=85.894 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:53 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.466 Prec@1=63.947 Prec@5=85.879 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:53 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.466 Prec@1=63.947 Prec@5=85.879 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:53 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.466 Prec@1=63.947 Prec@5=85.879 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:54 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.467 Prec@1=63.925 Prec@5=85.853 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=08:54 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.467 Prec@1=63.925 Prec@5=85.853 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:54 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.467 Prec@1=63.925 Prec@5=85.853 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:55 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.468 Prec@1=63.906 Prec@5=85.844 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:55 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.468 Prec@1=63.906 Prec@5=85.844 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:55 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.468 Prec@1=63.906 Prec@5=85.844 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:55 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.468 Prec@1=63.907 Prec@5=85.843 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=08:55 IST=> training   100.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.615 DataTime=0.391 Loss=1.468 Prec@1=63.907 Prec@5=85.843 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=08:55 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:55 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:55 IST=> validation 0.00% of 1x98...Epoch=42/150 LR=0.08267 Time=6.174 Loss=1.473 Prec@1=61.523 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=08:55 IST=> validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=6.174 Loss=1.473 Prec@1=61.523 Prec@5=84.570 rate=3060.14 Hz, eta=0:00:00, total=0:00:00, wall=08:55 IST** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=6.174 Loss=1.473 Prec@1=61.523 Prec@5=84.570 rate=3060.14 Hz, eta=0:00:00, total=0:00:00, wall=08:56 IST** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=0.406 Loss=1.506 Prec@1=62.942 Prec@5=85.630 rate=3060.14 Hz, eta=0:00:00, total=0:00:00, wall=08:56 IST** validation 100.00% of 1x98...Epoch=42/150 LR=0.08267 Time=0.406 Loss=1.506 Prec@1=62.942 Prec@5=85.630 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=08:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:56 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:56 IST=> training   0.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.829 DataTime=4.522 Loss=1.342 Prec@1=66.602 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=08:56 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.829 DataTime=4.522 Loss=1.342 Prec@1=66.602 Prec@5=88.281 rate=8398.63 Hz, eta=0:00:00, total=0:00:00, wall=08:56 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=4.829 DataTime=4.522 Loss=1.342 Prec@1=66.602 Prec@5=88.281 rate=8398.63 Hz, eta=0:00:00, total=0:00:00, wall=08:57 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.655 DataTime=0.428 Loss=1.417 Prec@1=64.782 Prec@5=86.593 rate=8398.63 Hz, eta=0:00:00, total=0:00:00, wall=08:57 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.655 DataTime=0.428 Loss=1.417 Prec@1=64.782 Prec@5=86.593 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:57 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.655 DataTime=0.428 Loss=1.417 Prec@1=64.782 Prec@5=86.593 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:58 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.634 DataTime=0.408 Loss=1.420 Prec@1=64.808 Prec@5=86.561 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:58 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.634 DataTime=0.408 Loss=1.420 Prec@1=64.808 Prec@5=86.561 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=08:58 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.634 DataTime=0.408 Loss=1.420 Prec@1=64.808 Prec@5=86.561 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=08:59 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.627 DataTime=0.402 Loss=1.421 Prec@1=64.783 Prec@5=86.538 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=08:59 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.627 DataTime=0.402 Loss=1.421 Prec@1=64.783 Prec@5=86.538 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=08:59 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.627 DataTime=0.402 Loss=1.421 Prec@1=64.783 Prec@5=86.538 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=09:00 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.623 DataTime=0.399 Loss=1.428 Prec@1=64.646 Prec@5=86.448 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=09:00 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.623 DataTime=0.399 Loss=1.428 Prec@1=64.646 Prec@5=86.448 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=09:00 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.623 DataTime=0.399 Loss=1.428 Prec@1=64.646 Prec@5=86.448 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=09:01 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.621 DataTime=0.397 Loss=1.435 Prec@1=64.522 Prec@5=86.336 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=09:01 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.621 DataTime=0.397 Loss=1.435 Prec@1=64.522 Prec@5=86.336 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=09:01 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.621 DataTime=0.397 Loss=1.435 Prec@1=64.522 Prec@5=86.336 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=09:02 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.620 DataTime=0.395 Loss=1.437 Prec@1=64.519 Prec@5=86.297 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=09:02 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.620 DataTime=0.395 Loss=1.437 Prec@1=64.519 Prec@5=86.297 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=09:02 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.620 DataTime=0.395 Loss=1.437 Prec@1=64.519 Prec@5=86.297 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=09:03 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.619 DataTime=0.395 Loss=1.441 Prec@1=64.472 Prec@5=86.240 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=09:03 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.619 DataTime=0.395 Loss=1.441 Prec@1=64.472 Prec@5=86.240 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=09:03 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.619 DataTime=0.395 Loss=1.441 Prec@1=64.472 Prec@5=86.240 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=09:04 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.394 Loss=1.443 Prec@1=64.446 Prec@5=86.227 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=09:04 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.394 Loss=1.443 Prec@1=64.446 Prec@5=86.227 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=09:04 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.394 Loss=1.443 Prec@1=64.446 Prec@5=86.227 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=09:05 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.393 Loss=1.444 Prec@1=64.405 Prec@5=86.197 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=09:05 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.393 Loss=1.444 Prec@1=64.405 Prec@5=86.197 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=09:05 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.618 DataTime=0.393 Loss=1.444 Prec@1=64.405 Prec@5=86.197 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=09:06 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.353 Prec@5=86.161 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=09:06 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.353 Prec@5=86.161 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:06 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.353 Prec@5=86.161 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:07 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.368 Prec@5=86.154 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:07 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.368 Prec@5=86.154 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:07 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.447 Prec@1=64.368 Prec@5=86.154 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:08 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.448 Prec@1=64.343 Prec@5=86.111 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:08 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.448 Prec@1=64.343 Prec@5=86.111 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:08 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.393 Loss=1.448 Prec@1=64.343 Prec@5=86.111 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:09 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.392 Loss=1.449 Prec@1=64.317 Prec@5=86.091 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:09 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.392 Loss=1.449 Prec@1=64.317 Prec@5=86.091 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:09 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.617 DataTime=0.392 Loss=1.449 Prec@1=64.317 Prec@5=86.091 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:10 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.451 Prec@1=64.286 Prec@5=86.075 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:10 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.451 Prec@1=64.286 Prec@5=86.075 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:10 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.451 Prec@1=64.286 Prec@5=86.075 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:11 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.276 Prec@5=86.063 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:11 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.276 Prec@5=86.063 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:11 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.276 Prec@5=86.063 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:12 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.453 Prec@1=64.235 Prec@5=86.049 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:12 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.453 Prec@1=64.235 Prec@5=86.049 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:12 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.453 Prec@1=64.235 Prec@5=86.049 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:14 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.202 Prec@5=86.018 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:14 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.202 Prec@5=86.018 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:14 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.202 Prec@5=86.018 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:15 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.171 Prec@5=85.995 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:15 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.171 Prec@5=85.995 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:15 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.171 Prec@5=85.995 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:16 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.150 Prec@5=85.985 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:16 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.150 Prec@5=85.985 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:16 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.616 DataTime=0.392 Loss=1.457 Prec@1=64.150 Prec@5=85.985 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:17 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.458 Prec@1=64.129 Prec@5=85.960 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:17 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.458 Prec@1=64.129 Prec@5=85.960 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=09:17 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.458 Prec@1=64.129 Prec@5=85.960 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=09:18 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.459 Prec@1=64.123 Prec@5=85.952 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=09:18 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.459 Prec@1=64.123 Prec@5=85.952 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:18 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.459 Prec@1=64.123 Prec@5=85.952 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:19 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.095 Prec@5=85.935 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:19 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.095 Prec@5=85.935 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:19 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.095 Prec@5=85.935 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:20 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.082 Prec@5=85.924 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:20 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.082 Prec@5=85.924 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:20 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.460 Prec@1=64.082 Prec@5=85.924 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:21 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.461 Prec@1=64.076 Prec@5=85.908 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:21 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.461 Prec@1=64.076 Prec@5=85.908 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:21 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.461 Prec@1=64.076 Prec@5=85.908 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:22 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.462 Prec@1=64.053 Prec@5=85.895 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:22 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.462 Prec@1=64.053 Prec@5=85.895 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:22 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.462 Prec@1=64.053 Prec@5=85.895 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:22 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.462 Prec@1=64.053 Prec@5=85.894 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:22 IST=> training   100.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.615 DataTime=0.391 Loss=1.462 Prec@1=64.053 Prec@5=85.894 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=09:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST=> validation 0.00% of 1x98...Epoch=43/150 LR=0.08187 Time=6.798 Loss=1.502 Prec@1=61.914 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST=> validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.798 Loss=1.502 Prec@1=61.914 Prec@5=86.328 rate=9268.61 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.798 Loss=1.502 Prec@1=61.914 Prec@5=86.328 rate=9268.61 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=0.404 Loss=1.626 Prec@1=60.774 Prec@5=83.984 rate=9268.61 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST** validation 100.00% of 1x98...Epoch=43/150 LR=0.08187 Time=0.404 Loss=1.626 Prec@1=60.774 Prec@5=83.984 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=09:22 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST=> training   0.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.509 DataTime=5.113 Loss=1.422 Prec@1=63.086 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.509 DataTime=5.113 Loss=1.422 Prec@1=63.086 Prec@5=87.305 rate=2723.88 Hz, eta=0:00:00, total=0:00:00, wall=09:23 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.509 DataTime=5.113 Loss=1.422 Prec@1=63.086 Prec@5=87.305 rate=2723.88 Hz, eta=0:00:00, total=0:00:00, wall=09:24 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.658 DataTime=0.433 Loss=1.432 Prec@1=64.525 Prec@5=86.293 rate=2723.88 Hz, eta=0:00:00, total=0:00:00, wall=09:24 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.658 DataTime=0.433 Loss=1.432 Prec@1=64.525 Prec@5=86.293 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=09:24 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.658 DataTime=0.433 Loss=1.432 Prec@1=64.525 Prec@5=86.293 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=09:25 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.636 DataTime=0.412 Loss=1.423 Prec@1=64.762 Prec@5=86.405 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=09:25 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.636 DataTime=0.412 Loss=1.423 Prec@1=64.762 Prec@5=86.405 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:25 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.636 DataTime=0.412 Loss=1.423 Prec@1=64.762 Prec@5=86.405 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:26 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.629 DataTime=0.405 Loss=1.421 Prec@1=64.867 Prec@5=86.435 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:26 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.629 DataTime=0.405 Loss=1.421 Prec@1=64.867 Prec@5=86.435 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=09:26 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.629 DataTime=0.405 Loss=1.421 Prec@1=64.867 Prec@5=86.435 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=09:27 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.625 DataTime=0.401 Loss=1.423 Prec@1=64.809 Prec@5=86.410 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=09:27 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.625 DataTime=0.401 Loss=1.423 Prec@1=64.809 Prec@5=86.410 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:27 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.625 DataTime=0.401 Loss=1.423 Prec@1=64.809 Prec@5=86.410 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:28 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.623 DataTime=0.399 Loss=1.426 Prec@1=64.732 Prec@5=86.368 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:28 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.623 DataTime=0.399 Loss=1.426 Prec@1=64.732 Prec@5=86.368 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:28 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.623 DataTime=0.399 Loss=1.426 Prec@1=64.732 Prec@5=86.368 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:29 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.622 DataTime=0.397 Loss=1.430 Prec@1=64.653 Prec@5=86.306 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:29 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.622 DataTime=0.397 Loss=1.430 Prec@1=64.653 Prec@5=86.306 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=09:29 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.622 DataTime=0.397 Loss=1.430 Prec@1=64.653 Prec@5=86.306 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=09:30 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.620 DataTime=0.396 Loss=1.432 Prec@1=64.612 Prec@5=86.278 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=09:30 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.620 DataTime=0.396 Loss=1.432 Prec@1=64.612 Prec@5=86.278 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=09:30 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.620 DataTime=0.396 Loss=1.432 Prec@1=64.612 Prec@5=86.278 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=09:31 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.433 Prec@1=64.606 Prec@5=86.257 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=09:31 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.433 Prec@1=64.606 Prec@5=86.257 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=09:31 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.433 Prec@1=64.606 Prec@5=86.257 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=09:32 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.435 Prec@1=64.556 Prec@5=86.231 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=09:32 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.435 Prec@1=64.556 Prec@5=86.231 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=09:32 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.395 Loss=1.435 Prec@1=64.556 Prec@5=86.231 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=09:33 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.394 Loss=1.437 Prec@1=64.512 Prec@5=86.205 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=09:33 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.394 Loss=1.437 Prec@1=64.512 Prec@5=86.205 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:33 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.619 DataTime=0.394 Loss=1.437 Prec@1=64.512 Prec@5=86.205 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:34 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.394 Loss=1.437 Prec@1=64.500 Prec@5=86.212 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:34 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.394 Loss=1.437 Prec@1=64.500 Prec@5=86.212 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:34 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.394 Loss=1.437 Prec@1=64.500 Prec@5=86.212 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:35 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.393 Loss=1.439 Prec@1=64.467 Prec@5=86.186 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=09:35 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.393 Loss=1.439 Prec@1=64.467 Prec@5=86.186 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:35 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.618 DataTime=0.393 Loss=1.439 Prec@1=64.467 Prec@5=86.186 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:36 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.442 Prec@1=64.410 Prec@5=86.151 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:36 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.442 Prec@1=64.410 Prec@5=86.151 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:36 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.442 Prec@1=64.410 Prec@5=86.151 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:37 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.444 Prec@1=64.378 Prec@5=86.139 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=09:37 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.444 Prec@1=64.378 Prec@5=86.139 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:37 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.444 Prec@1=64.378 Prec@5=86.139 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:38 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.445 Prec@1=64.356 Prec@5=86.124 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=09:38 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.445 Prec@1=64.356 Prec@5=86.124 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=09:38 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.445 Prec@1=64.356 Prec@5=86.124 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=09:39 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.446 Prec@1=64.337 Prec@5=86.106 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=09:39 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.446 Prec@1=64.337 Prec@5=86.106 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:39 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.617 DataTime=0.393 Loss=1.446 Prec@1=64.337 Prec@5=86.106 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:40 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.302 Prec@5=86.085 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:40 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.302 Prec@5=86.085 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:40 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.302 Prec@5=86.085 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:41 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.305 Prec@5=86.082 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=09:41 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.305 Prec@5=86.082 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:41 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.448 Prec@1=64.305 Prec@5=86.082 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:42 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.449 Prec@1=64.295 Prec@5=86.065 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:42 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.449 Prec@1=64.295 Prec@5=86.065 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:42 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.449 Prec@1=64.295 Prec@5=86.065 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:43 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.450 Prec@1=64.286 Prec@5=86.050 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=09:43 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.450 Prec@1=64.286 Prec@5=86.050 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:43 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.450 Prec@1=64.286 Prec@5=86.050 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:44 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.252 Prec@5=86.023 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:44 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.252 Prec@5=86.023 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:44 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.452 Prec@1=64.252 Prec@5=86.023 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:45 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.454 Prec@1=64.223 Prec@5=86.002 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:45 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.454 Prec@1=64.223 Prec@5=86.002 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:45 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.454 Prec@1=64.223 Prec@5=86.002 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:46 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.210 Prec@5=85.984 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=09:46 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.210 Prec@5=85.984 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:46 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.455 Prec@1=64.210 Prec@5=85.984 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:47 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.188 Prec@5=85.968 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:47 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.188 Prec@5=85.968 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:47 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.188 Prec@5=85.968 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:48 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.182 Prec@5=85.961 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=09:48 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.182 Prec@5=85.961 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:48 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.616 DataTime=0.392 Loss=1.456 Prec@1=64.182 Prec@5=85.961 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:48 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.615 DataTime=0.392 Loss=1.456 Prec@1=64.181 Prec@5=85.960 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=09:48 IST=> training   100.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.615 DataTime=0.392 Loss=1.456 Prec@1=64.181 Prec@5=85.960 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=09:48 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:48 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:48 IST=> validation 0.00% of 1x98...Epoch=44/150 LR=0.08106 Time=7.376 Loss=1.631 Prec@1=60.547 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=09:48 IST=> validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=7.376 Loss=1.631 Prec@1=60.547 Prec@5=83.789 rate=4220.07 Hz, eta=0:00:00, total=0:00:00, wall=09:48 IST** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=7.376 Loss=1.631 Prec@1=60.547 Prec@5=83.789 rate=4220.07 Hz, eta=0:00:00, total=0:00:00, wall=09:49 IST** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=0.481 Loss=1.539 Prec@1=62.234 Prec@5=85.168 rate=4220.07 Hz, eta=0:00:00, total=0:00:00, wall=09:49 IST** validation 100.00% of 1x98...Epoch=44/150 LR=0.08106 Time=0.481 Loss=1.539 Prec@1=62.234 Prec@5=85.168 rate=2.46 Hz, eta=0:00:00, total=0:00:39, wall=09:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:49 IST=> training   0.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=6.258 DataTime=6.026 Loss=1.356 Prec@1=67.578 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=09:49 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=6.258 DataTime=6.026 Loss=1.356 Prec@1=67.578 Prec@5=87.305 rate=8317.53 Hz, eta=0:00:00, total=0:00:00, wall=09:49 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=6.258 DataTime=6.026 Loss=1.356 Prec@1=67.578 Prec@5=87.305 rate=8317.53 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.665 DataTime=0.445 Loss=1.413 Prec@1=65.085 Prec@5=86.525 rate=8317.53 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.665 DataTime=0.445 Loss=1.413 Prec@1=65.085 Prec@5=86.525 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=09:50 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.665 DataTime=0.445 Loss=1.413 Prec@1=65.085 Prec@5=86.525 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=09:51 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.640 DataTime=0.420 Loss=1.412 Prec@1=65.080 Prec@5=86.491 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=09:51 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.640 DataTime=0.420 Loss=1.412 Prec@1=65.080 Prec@5=86.491 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:51 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.640 DataTime=0.420 Loss=1.412 Prec@1=65.080 Prec@5=86.491 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:52 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.630 DataTime=0.410 Loss=1.416 Prec@1=65.009 Prec@5=86.470 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=09:52 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.630 DataTime=0.410 Loss=1.416 Prec@1=65.009 Prec@5=86.470 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=09:52 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.630 DataTime=0.410 Loss=1.416 Prec@1=65.009 Prec@5=86.470 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=09:53 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.627 DataTime=0.407 Loss=1.420 Prec@1=64.886 Prec@5=86.431 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=09:53 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.627 DataTime=0.407 Loss=1.420 Prec@1=64.886 Prec@5=86.431 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:53 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.627 DataTime=0.407 Loss=1.420 Prec@1=64.886 Prec@5=86.431 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:54 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.624 DataTime=0.404 Loss=1.421 Prec@1=64.825 Prec@5=86.410 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=09:54 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.624 DataTime=0.404 Loss=1.421 Prec@1=64.825 Prec@5=86.410 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:54 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.624 DataTime=0.404 Loss=1.421 Prec@1=64.825 Prec@5=86.410 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:55 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.623 DataTime=0.402 Loss=1.422 Prec@1=64.805 Prec@5=86.387 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=09:55 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.623 DataTime=0.402 Loss=1.422 Prec@1=64.805 Prec@5=86.387 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:55 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.623 DataTime=0.402 Loss=1.422 Prec@1=64.805 Prec@5=86.387 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:56 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.622 DataTime=0.401 Loss=1.422 Prec@1=64.773 Prec@5=86.396 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:56 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.622 DataTime=0.401 Loss=1.422 Prec@1=64.773 Prec@5=86.396 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=09:56 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.622 DataTime=0.401 Loss=1.422 Prec@1=64.773 Prec@5=86.396 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=09:57 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.621 DataTime=0.400 Loss=1.424 Prec@1=64.761 Prec@5=86.371 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=09:57 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.621 DataTime=0.400 Loss=1.424 Prec@1=64.761 Prec@5=86.371 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:57 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.621 DataTime=0.400 Loss=1.424 Prec@1=64.761 Prec@5=86.371 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:58 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.426 Prec@1=64.734 Prec@5=86.330 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:58 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.426 Prec@1=64.734 Prec@5=86.330 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:58 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.426 Prec@1=64.734 Prec@5=86.330 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:59 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.429 Prec@1=64.710 Prec@5=86.298 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:59 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.429 Prec@1=64.710 Prec@5=86.298 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=09:59 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.620 DataTime=0.399 Loss=1.429 Prec@1=64.710 Prec@5=86.298 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=10:00 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.430 Prec@1=64.689 Prec@5=86.273 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=10:00 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.430 Prec@1=64.689 Prec@5=86.273 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=10:00 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.430 Prec@1=64.689 Prec@5=86.273 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=10:01 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.431 Prec@1=64.687 Prec@5=86.233 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=10:01 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.431 Prec@1=64.687 Prec@5=86.233 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:01 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.619 DataTime=0.398 Loss=1.431 Prec@1=64.687 Prec@5=86.233 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:02 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.434 Prec@1=64.634 Prec@5=86.214 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:02 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.434 Prec@1=64.634 Prec@5=86.214 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=10:02 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.434 Prec@1=64.634 Prec@5=86.214 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=10:03 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.435 Prec@1=64.602 Prec@5=86.211 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=10:03 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.435 Prec@1=64.602 Prec@5=86.211 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=10:03 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.397 Loss=1.435 Prec@1=64.602 Prec@5=86.211 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=10:04 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.396 Loss=1.437 Prec@1=64.556 Prec@5=86.194 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=10:04 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.396 Loss=1.437 Prec@1=64.556 Prec@5=86.194 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:04 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.618 DataTime=0.396 Loss=1.437 Prec@1=64.556 Prec@5=86.194 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:05 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.438 Prec@1=64.565 Prec@5=86.191 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:05 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.438 Prec@1=64.565 Prec@5=86.191 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:05 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.438 Prec@1=64.565 Prec@5=86.191 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:06 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.439 Prec@1=64.543 Prec@5=86.180 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:06 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.439 Prec@1=64.543 Prec@5=86.180 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:06 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.396 Loss=1.439 Prec@1=64.543 Prec@5=86.180 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:07 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.538 Prec@5=86.159 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:07 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.538 Prec@5=86.159 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:07 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.538 Prec@5=86.159 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:08 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.521 Prec@5=86.143 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:08 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.521 Prec@5=86.143 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:08 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.440 Prec@1=64.521 Prec@5=86.143 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:09 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.442 Prec@1=64.498 Prec@5=86.138 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:09 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.442 Prec@1=64.498 Prec@5=86.138 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=10:09 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.617 DataTime=0.395 Loss=1.442 Prec@1=64.498 Prec@5=86.138 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=10:11 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.443 Prec@1=64.478 Prec@5=86.123 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=10:11 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.443 Prec@1=64.478 Prec@5=86.123 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:11 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.443 Prec@1=64.478 Prec@5=86.123 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:12 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.444 Prec@1=64.444 Prec@5=86.108 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:12 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.444 Prec@1=64.444 Prec@5=86.108 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:12 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.444 Prec@1=64.444 Prec@5=86.108 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:13 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.438 Prec@5=86.091 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:13 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.438 Prec@5=86.091 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:13 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.438 Prec@5=86.091 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:14 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.435 Prec@5=86.087 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:14 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.435 Prec@5=86.087 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:14 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.445 Prec@1=64.435 Prec@5=86.087 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:15 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.447 Prec@1=64.408 Prec@5=86.068 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:15 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.447 Prec@1=64.408 Prec@5=86.068 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:15 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.447 Prec@1=64.408 Prec@5=86.068 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:15 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.447 Prec@1=64.408 Prec@5=86.067 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:15 IST=> training   100.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.616 DataTime=0.394 Loss=1.447 Prec@1=64.408 Prec@5=86.067 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=10:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> validation 0.00% of 1x98...Epoch=45/150 LR=0.08023 Time=6.583 Loss=1.586 Prec@1=62.305 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.583 Loss=1.586 Prec@1=62.305 Prec@5=84.180 rate=4993.68 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.583 Loss=1.586 Prec@1=62.305 Prec@5=84.180 rate=4993.68 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=0.406 Loss=1.506 Prec@1=62.896 Prec@5=85.510 rate=4993.68 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST** validation 100.00% of 1x98...Epoch=45/150 LR=0.08023 Time=0.406 Loss=1.506 Prec@1=62.896 Prec@5=85.510 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=10:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> training   0.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.984 DataTime=5.692 Loss=1.324 Prec@1=66.992 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=10:15 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.984 DataTime=5.692 Loss=1.324 Prec@1=66.992 Prec@5=87.500 rate=8032.38 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=5.984 DataTime=5.692 Loss=1.324 Prec@1=66.992 Prec@5=87.500 rate=8032.38 Hz, eta=0:00:00, total=0:00:00, wall=10:16 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.662 DataTime=0.438 Loss=1.394 Prec@1=65.729 Prec@5=86.808 rate=8032.38 Hz, eta=0:00:00, total=0:00:00, wall=10:16 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.662 DataTime=0.438 Loss=1.394 Prec@1=65.729 Prec@5=86.808 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=10:16 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.662 DataTime=0.438 Loss=1.394 Prec@1=65.729 Prec@5=86.808 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=10:17 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.639 DataTime=0.415 Loss=1.404 Prec@1=65.397 Prec@5=86.681 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=10:17 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.639 DataTime=0.415 Loss=1.404 Prec@1=65.397 Prec@5=86.681 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=10:17 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.639 DataTime=0.415 Loss=1.404 Prec@1=65.397 Prec@5=86.681 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=10:18 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.630 DataTime=0.406 Loss=1.405 Prec@1=65.354 Prec@5=86.605 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=10:18 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.630 DataTime=0.406 Loss=1.405 Prec@1=65.354 Prec@5=86.605 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=10:18 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.630 DataTime=0.406 Loss=1.405 Prec@1=65.354 Prec@5=86.605 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=10:19 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.625 DataTime=0.402 Loss=1.409 Prec@1=65.190 Prec@5=86.560 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=10:19 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.625 DataTime=0.402 Loss=1.409 Prec@1=65.190 Prec@5=86.560 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=10:19 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.625 DataTime=0.402 Loss=1.409 Prec@1=65.190 Prec@5=86.560 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=10:21 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.623 DataTime=0.400 Loss=1.413 Prec@1=65.141 Prec@5=86.513 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=10:21 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.623 DataTime=0.400 Loss=1.413 Prec@1=65.141 Prec@5=86.513 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=10:21 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.623 DataTime=0.400 Loss=1.413 Prec@1=65.141 Prec@5=86.513 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=10:22 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.621 DataTime=0.398 Loss=1.416 Prec@1=65.080 Prec@5=86.504 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=10:22 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.621 DataTime=0.398 Loss=1.416 Prec@1=65.080 Prec@5=86.504 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=10:22 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.621 DataTime=0.398 Loss=1.416 Prec@1=65.080 Prec@5=86.504 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=10:23 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.620 DataTime=0.397 Loss=1.418 Prec@1=65.032 Prec@5=86.491 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=10:23 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.620 DataTime=0.397 Loss=1.418 Prec@1=65.032 Prec@5=86.491 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=10:23 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.620 DataTime=0.397 Loss=1.418 Prec@1=65.032 Prec@5=86.491 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=10:24 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.421 Prec@1=64.989 Prec@5=86.452 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=10:24 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.421 Prec@1=64.989 Prec@5=86.452 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=10:24 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.421 Prec@1=64.989 Prec@5=86.452 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=10:25 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.423 Prec@1=64.944 Prec@5=86.417 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=10:25 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.423 Prec@1=64.944 Prec@5=86.417 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=10:25 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.619 DataTime=0.396 Loss=1.423 Prec@1=64.944 Prec@5=86.417 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=10:26 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.424 Prec@1=64.906 Prec@5=86.409 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=10:26 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.424 Prec@1=64.906 Prec@5=86.409 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=10:26 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.424 Prec@1=64.906 Prec@5=86.409 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=10:27 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.425 Prec@1=64.855 Prec@5=86.395 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=10:27 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.425 Prec@1=64.855 Prec@5=86.395 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=10:27 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.618 DataTime=0.395 Loss=1.425 Prec@1=64.855 Prec@5=86.395 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=10:28 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.427 Prec@1=64.812 Prec@5=86.371 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=10:28 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.427 Prec@1=64.812 Prec@5=86.371 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=10:28 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.427 Prec@1=64.812 Prec@5=86.371 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=10:29 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.429 Prec@1=64.782 Prec@5=86.357 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=10:29 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.429 Prec@1=64.782 Prec@5=86.357 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=10:29 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.394 Loss=1.429 Prec@1=64.782 Prec@5=86.357 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=10:30 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.760 Prec@5=86.350 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=10:30 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.760 Prec@5=86.350 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:30 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.760 Prec@5=86.350 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:31 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.746 Prec@5=86.334 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:31 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.746 Prec@5=86.334 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=10:31 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.430 Prec@1=64.746 Prec@5=86.334 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=10:32 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.432 Prec@1=64.701 Prec@5=86.310 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=10:32 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.432 Prec@1=64.701 Prec@5=86.310 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:32 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.617 DataTime=0.393 Loss=1.432 Prec@1=64.701 Prec@5=86.310 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:33 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.672 Prec@5=86.292 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:33 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.672 Prec@5=86.292 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:33 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.672 Prec@5=86.292 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:34 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.670 Prec@5=86.291 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:34 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.670 Prec@5=86.291 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:34 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.433 Prec@1=64.670 Prec@5=86.291 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:35 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.434 Prec@1=64.654 Prec@5=86.281 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:35 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.434 Prec@1=64.654 Prec@5=86.281 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:35 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.434 Prec@1=64.654 Prec@5=86.281 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:36 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.435 Prec@1=64.629 Prec@5=86.268 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:36 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.435 Prec@1=64.629 Prec@5=86.268 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:36 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.393 Loss=1.435 Prec@1=64.629 Prec@5=86.268 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:37 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.436 Prec@1=64.600 Prec@5=86.250 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:37 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.436 Prec@1=64.600 Prec@5=86.250 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:37 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.436 Prec@1=64.600 Prec@5=86.250 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:38 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.570 Prec@5=86.229 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:38 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.570 Prec@5=86.229 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:38 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.570 Prec@5=86.229 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:39 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.556 Prec@5=86.228 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:39 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.556 Prec@5=86.228 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=10:39 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.438 Prec@1=64.556 Prec@5=86.228 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=10:40 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.439 Prec@1=64.523 Prec@5=86.212 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=10:40 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.439 Prec@1=64.523 Prec@5=86.212 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=10:40 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.616 DataTime=0.392 Loss=1.439 Prec@1=64.523 Prec@5=86.212 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=10:41 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.615 DataTime=0.392 Loss=1.440 Prec@1=64.507 Prec@5=86.204 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=10:41 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.615 DataTime=0.392 Loss=1.440 Prec@1=64.507 Prec@5=86.204 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:41 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.615 DataTime=0.392 Loss=1.440 Prec@1=64.507 Prec@5=86.204 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:41 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.615 DataTime=0.392 Loss=1.440 Prec@1=64.507 Prec@5=86.206 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:41 IST=> training   100.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.615 DataTime=0.392 Loss=1.440 Prec@1=64.507 Prec@5=86.206 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=10:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST=> validation 0.00% of 1x98...Epoch=46/150 LR=0.07939 Time=6.949 Loss=1.631 Prec@1=59.375 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=10:41 IST=> validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=6.949 Loss=1.631 Prec@1=59.375 Prec@5=85.156 rate=5971.89 Hz, eta=0:00:00, total=0:00:00, wall=10:41 IST** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=6.949 Loss=1.631 Prec@1=59.375 Prec@5=85.156 rate=5971.89 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=0.407 Loss=1.566 Prec@1=61.630 Prec@5=84.874 rate=5971.89 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST** validation 100.00% of 1x98...Epoch=46/150 LR=0.07939 Time=0.407 Loss=1.566 Prec@1=61.630 Prec@5=84.874 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=10:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST=> training   0.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=5.312 DataTime=5.017 Loss=1.265 Prec@1=68.945 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=10:42 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=5.312 DataTime=5.017 Loss=1.265 Prec@1=68.945 Prec@5=88.672 rate=7325.36 Hz, eta=0:00:00, total=0:00:00, wall=10:42 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=5.312 DataTime=5.017 Loss=1.265 Prec@1=68.945 Prec@5=88.672 rate=7325.36 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.659 DataTime=0.432 Loss=1.396 Prec@1=65.445 Prec@5=86.808 rate=7325.36 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.659 DataTime=0.432 Loss=1.396 Prec@1=65.445 Prec@5=86.808 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=10:43 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.659 DataTime=0.432 Loss=1.396 Prec@1=65.445 Prec@5=86.808 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=10:44 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.636 DataTime=0.410 Loss=1.400 Prec@1=65.508 Prec@5=86.750 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=10:44 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.636 DataTime=0.410 Loss=1.400 Prec@1=65.508 Prec@5=86.750 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:44 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.636 DataTime=0.410 Loss=1.400 Prec@1=65.508 Prec@5=86.750 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:45 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.628 DataTime=0.403 Loss=1.404 Prec@1=65.391 Prec@5=86.657 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:45 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.628 DataTime=0.403 Loss=1.404 Prec@1=65.391 Prec@5=86.657 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=10:45 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.628 DataTime=0.403 Loss=1.404 Prec@1=65.391 Prec@5=86.657 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=10:46 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.624 DataTime=0.400 Loss=1.406 Prec@1=65.360 Prec@5=86.672 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=10:46 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.624 DataTime=0.400 Loss=1.406 Prec@1=65.360 Prec@5=86.672 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:46 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.624 DataTime=0.400 Loss=1.406 Prec@1=65.360 Prec@5=86.672 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:47 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.622 DataTime=0.398 Loss=1.408 Prec@1=65.260 Prec@5=86.667 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:47 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.622 DataTime=0.398 Loss=1.408 Prec@1=65.260 Prec@5=86.667 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:47 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.622 DataTime=0.398 Loss=1.408 Prec@1=65.260 Prec@5=86.667 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:48 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.621 DataTime=0.396 Loss=1.409 Prec@1=65.163 Prec@5=86.667 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:48 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.621 DataTime=0.396 Loss=1.409 Prec@1=65.163 Prec@5=86.667 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:48 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.621 DataTime=0.396 Loss=1.409 Prec@1=65.163 Prec@5=86.667 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:49 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.620 DataTime=0.396 Loss=1.411 Prec@1=65.119 Prec@5=86.638 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:49 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.620 DataTime=0.396 Loss=1.411 Prec@1=65.119 Prec@5=86.638 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:49 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.620 DataTime=0.396 Loss=1.411 Prec@1=65.119 Prec@5=86.638 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:50 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.395 Loss=1.413 Prec@1=65.076 Prec@5=86.613 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:50 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.395 Loss=1.413 Prec@1=65.076 Prec@5=86.613 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:50 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.395 Loss=1.413 Prec@1=65.076 Prec@5=86.613 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:51 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.394 Loss=1.415 Prec@1=65.039 Prec@5=86.585 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:51 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.394 Loss=1.415 Prec@1=65.039 Prec@5=86.585 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:51 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.619 DataTime=0.394 Loss=1.415 Prec@1=65.039 Prec@5=86.585 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:52 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.394 Loss=1.416 Prec@1=65.032 Prec@5=86.573 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:52 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.394 Loss=1.416 Prec@1=65.032 Prec@5=86.573 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:52 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.394 Loss=1.416 Prec@1=65.032 Prec@5=86.573 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:53 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.393 Loss=1.418 Prec@1=64.992 Prec@5=86.536 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:53 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.393 Loss=1.418 Prec@1=64.992 Prec@5=86.536 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:53 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.618 DataTime=0.393 Loss=1.418 Prec@1=64.992 Prec@5=86.536 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:54 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.420 Prec@1=64.961 Prec@5=86.499 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:54 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.420 Prec@1=64.961 Prec@5=86.499 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=10:54 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.420 Prec@1=64.961 Prec@5=86.499 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=10:55 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.421 Prec@1=64.934 Prec@5=86.481 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=10:55 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.421 Prec@1=64.934 Prec@5=86.481 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:55 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.421 Prec@1=64.934 Prec@5=86.481 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:56 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.422 Prec@1=64.916 Prec@5=86.483 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:56 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.422 Prec@1=64.916 Prec@5=86.483 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:56 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.393 Loss=1.422 Prec@1=64.916 Prec@5=86.483 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:57 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.392 Loss=1.423 Prec@1=64.882 Prec@5=86.457 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:57 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.392 Loss=1.423 Prec@1=64.882 Prec@5=86.457 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:57 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.617 DataTime=0.392 Loss=1.423 Prec@1=64.882 Prec@5=86.457 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:58 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.853 Prec@5=86.437 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:58 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.853 Prec@5=86.437 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:58 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.853 Prec@5=86.437 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:59 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.425 Prec@1=64.817 Prec@5=86.434 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:59 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.425 Prec@1=64.817 Prec@5=86.434 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:59 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.425 Prec@1=64.817 Prec@5=86.434 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:00 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.427 Prec@1=64.803 Prec@5=86.402 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:00 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.427 Prec@1=64.803 Prec@5=86.402 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=11:00 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.427 Prec@1=64.803 Prec@5=86.402 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=11:01 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.428 Prec@1=64.783 Prec@5=86.386 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=11:01 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.428 Prec@1=64.783 Prec@5=86.386 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=11:01 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.428 Prec@1=64.783 Prec@5=86.386 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=11:02 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.429 Prec@1=64.750 Prec@5=86.369 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=11:02 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.429 Prec@1=64.750 Prec@5=86.369 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:02 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.392 Loss=1.429 Prec@1=64.750 Prec@5=86.369 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:03 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.430 Prec@1=64.737 Prec@5=86.353 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:03 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.430 Prec@1=64.737 Prec@5=86.353 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=11:03 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.430 Prec@1=64.737 Prec@5=86.353 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=11:04 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.431 Prec@1=64.698 Prec@5=86.334 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=11:04 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.431 Prec@1=64.698 Prec@5=86.334 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:04 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.616 DataTime=0.391 Loss=1.431 Prec@1=64.698 Prec@5=86.334 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:05 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.679 Prec@5=86.332 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:05 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.679 Prec@5=86.332 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:05 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.679 Prec@5=86.332 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:06 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.690 Prec@5=86.329 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:06 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.690 Prec@5=86.329 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:06 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.432 Prec@1=64.690 Prec@5=86.329 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:07 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.433 Prec@1=64.664 Prec@5=86.310 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:07 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.433 Prec@1=64.664 Prec@5=86.310 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:07 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.433 Prec@1=64.664 Prec@5=86.310 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:07 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.433 Prec@1=64.664 Prec@5=86.311 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:07 IST=> training   100.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.615 DataTime=0.391 Loss=1.433 Prec@1=64.664 Prec@5=86.311 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=11:07 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:07 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:07 IST=> validation 0.00% of 1x98...Epoch=47/150 LR=0.07854 Time=7.022 Loss=1.521 Prec@1=63.281 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=11:07 IST=> validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=7.022 Loss=1.521 Prec@1=63.281 Prec@5=84.570 rate=5771.41 Hz, eta=0:00:00, total=0:00:00, wall=11:07 IST** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=7.022 Loss=1.521 Prec@1=63.281 Prec@5=84.570 rate=5771.41 Hz, eta=0:00:00, total=0:00:00, wall=11:08 IST** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=0.405 Loss=1.509 Prec@1=62.752 Prec@5=85.454 rate=5771.41 Hz, eta=0:00:00, total=0:00:00, wall=11:08 IST** validation 100.00% of 1x98...Epoch=47/150 LR=0.07854 Time=0.405 Loss=1.509 Prec@1=62.752 Prec@5=85.454 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=11:08 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:08 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:08 IST=> training   0.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=6.051 DataTime=5.824 Loss=1.326 Prec@1=66.211 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=11:08 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=6.051 DataTime=5.824 Loss=1.326 Prec@1=66.211 Prec@5=87.500 rate=7865.59 Hz, eta=0:00:00, total=0:00:00, wall=11:08 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=6.051 DataTime=5.824 Loss=1.326 Prec@1=66.211 Prec@5=87.500 rate=7865.59 Hz, eta=0:00:00, total=0:00:00, wall=11:09 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.662 DataTime=0.439 Loss=1.390 Prec@1=65.321 Prec@5=86.991 rate=7865.59 Hz, eta=0:00:00, total=0:00:00, wall=11:09 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.662 DataTime=0.439 Loss=1.390 Prec@1=65.321 Prec@5=86.991 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:09 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.662 DataTime=0.439 Loss=1.390 Prec@1=65.321 Prec@5=86.991 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:10 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.638 DataTime=0.415 Loss=1.399 Prec@1=65.296 Prec@5=86.803 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:10 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.638 DataTime=0.415 Loss=1.399 Prec@1=65.296 Prec@5=86.803 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=11:10 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.638 DataTime=0.415 Loss=1.399 Prec@1=65.296 Prec@5=86.803 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=11:11 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.631 DataTime=0.407 Loss=1.398 Prec@1=65.337 Prec@5=86.749 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=11:11 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.631 DataTime=0.407 Loss=1.398 Prec@1=65.337 Prec@5=86.749 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:11 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.631 DataTime=0.407 Loss=1.398 Prec@1=65.337 Prec@5=86.749 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:12 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.626 DataTime=0.403 Loss=1.395 Prec@1=65.435 Prec@5=86.794 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:12 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.626 DataTime=0.403 Loss=1.395 Prec@1=65.435 Prec@5=86.794 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:12 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.626 DataTime=0.403 Loss=1.395 Prec@1=65.435 Prec@5=86.794 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:13 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.624 DataTime=0.400 Loss=1.399 Prec@1=65.391 Prec@5=86.769 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:13 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.624 DataTime=0.400 Loss=1.399 Prec@1=65.391 Prec@5=86.769 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=11:13 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.624 DataTime=0.400 Loss=1.399 Prec@1=65.391 Prec@5=86.769 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=11:14 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.622 DataTime=0.398 Loss=1.400 Prec@1=65.344 Prec@5=86.714 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=11:14 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.622 DataTime=0.398 Loss=1.400 Prec@1=65.344 Prec@5=86.714 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=11:14 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.622 DataTime=0.398 Loss=1.400 Prec@1=65.344 Prec@5=86.714 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=11:15 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.620 DataTime=0.397 Loss=1.402 Prec@1=65.297 Prec@5=86.701 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=11:15 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.620 DataTime=0.397 Loss=1.402 Prec@1=65.297 Prec@5=86.701 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:15 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.620 DataTime=0.397 Loss=1.402 Prec@1=65.297 Prec@5=86.701 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:16 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.396 Loss=1.403 Prec@1=65.297 Prec@5=86.698 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:16 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.396 Loss=1.403 Prec@1=65.297 Prec@5=86.698 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=11:16 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.396 Loss=1.403 Prec@1=65.297 Prec@5=86.698 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=11:17 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.395 Loss=1.405 Prec@1=65.221 Prec@5=86.661 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=11:17 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.395 Loss=1.405 Prec@1=65.221 Prec@5=86.661 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=11:17 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.619 DataTime=0.395 Loss=1.405 Prec@1=65.221 Prec@5=86.661 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=11:18 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.395 Loss=1.408 Prec@1=65.188 Prec@5=86.626 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=11:18 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.395 Loss=1.408 Prec@1=65.188 Prec@5=86.626 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=11:18 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.395 Loss=1.408 Prec@1=65.188 Prec@5=86.626 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=11:19 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.394 Loss=1.410 Prec@1=65.149 Prec@5=86.609 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=11:19 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.394 Loss=1.410 Prec@1=65.149 Prec@5=86.609 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:19 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.618 DataTime=0.394 Loss=1.410 Prec@1=65.149 Prec@5=86.609 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:20 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.411 Prec@1=65.129 Prec@5=86.589 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:20 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.411 Prec@1=65.129 Prec@5=86.589 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:20 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.411 Prec@1=65.129 Prec@5=86.589 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:21 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.414 Prec@1=65.086 Prec@5=86.558 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:21 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.414 Prec@1=65.086 Prec@5=86.558 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=11:21 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.394 Loss=1.414 Prec@1=65.086 Prec@5=86.558 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=11:22 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.414 Prec@1=65.067 Prec@5=86.552 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=11:22 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.414 Prec@1=65.067 Prec@5=86.552 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:22 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.414 Prec@1=65.067 Prec@5=86.552 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:23 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.040 Prec@5=86.538 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:23 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.040 Prec@5=86.538 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:23 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.040 Prec@5=86.538 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:25 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.027 Prec@5=86.534 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:25 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.027 Prec@5=86.534 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:25 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.617 DataTime=0.393 Loss=1.416 Prec@1=65.027 Prec@5=86.534 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:26 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.393 Loss=1.417 Prec@1=65.003 Prec@5=86.513 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:26 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.393 Loss=1.417 Prec@1=65.003 Prec@5=86.513 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:26 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.393 Loss=1.417 Prec@1=65.003 Prec@5=86.513 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:27 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.990 Prec@5=86.507 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:27 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.990 Prec@5=86.507 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:27 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.990 Prec@5=86.507 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:28 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.973 Prec@5=86.509 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:28 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.973 Prec@5=86.509 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=11:28 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.418 Prec@1=64.973 Prec@5=86.509 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=11:29 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.419 Prec@1=64.964 Prec@5=86.496 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=11:29 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.419 Prec@1=64.964 Prec@5=86.496 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:29 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.419 Prec@1=64.964 Prec@5=86.496 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:30 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.420 Prec@1=64.939 Prec@5=86.480 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:30 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.420 Prec@1=64.939 Prec@5=86.480 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:30 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.420 Prec@1=64.939 Prec@5=86.480 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:31 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.422 Prec@1=64.898 Prec@5=86.453 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:31 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.422 Prec@1=64.898 Prec@5=86.453 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:31 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.422 Prec@1=64.898 Prec@5=86.453 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:32 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.423 Prec@1=64.879 Prec@5=86.436 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:32 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.423 Prec@1=64.879 Prec@5=86.436 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:32 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.423 Prec@1=64.879 Prec@5=86.436 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:33 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.865 Prec@5=86.416 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:33 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.865 Prec@5=86.416 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:33 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.424 Prec@1=64.865 Prec@5=86.416 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:34 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.426 Prec@1=64.837 Prec@5=86.395 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:34 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.426 Prec@1=64.837 Prec@5=86.395 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:34 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.426 Prec@1=64.837 Prec@5=86.395 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:34 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.426 Prec@1=64.836 Prec@5=86.393 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=11:34 IST=> training   100.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.616 DataTime=0.392 Loss=1.426 Prec@1=64.836 Prec@5=86.393 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=11:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> validation 0.00% of 1x98...Epoch=48/150 LR=0.07767 Time=7.106 Loss=1.439 Prec@1=63.672 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=7.106 Loss=1.439 Prec@1=63.672 Prec@5=85.547 rate=6844.86 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=7.106 Loss=1.439 Prec@1=63.672 Prec@5=85.547 rate=6844.86 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=0.409 Loss=1.557 Prec@1=62.134 Prec@5=84.742 rate=6844.86 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST** validation 100.00% of 1x98...Epoch=48/150 LR=0.07767 Time=0.409 Loss=1.557 Prec@1=62.134 Prec@5=84.742 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=11:34 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST=> training   0.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.155 DataTime=4.869 Loss=1.295 Prec@1=66.406 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.155 DataTime=4.869 Loss=1.295 Prec@1=66.406 Prec@5=89.258 rate=2030.30 Hz, eta=0:00:01, total=0:00:00, wall=11:35 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.155 DataTime=4.869 Loss=1.295 Prec@1=66.406 Prec@5=89.258 rate=2030.30 Hz, eta=0:00:01, total=0:00:00, wall=11:36 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.657 DataTime=0.430 Loss=1.384 Prec@1=65.706 Prec@5=87.059 rate=2030.30 Hz, eta=0:00:01, total=0:00:00, wall=11:36 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.657 DataTime=0.430 Loss=1.384 Prec@1=65.706 Prec@5=87.059 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=11:36 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.657 DataTime=0.430 Loss=1.384 Prec@1=65.706 Prec@5=87.059 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=11:37 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.636 DataTime=0.410 Loss=1.384 Prec@1=65.734 Prec@5=87.047 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=11:37 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.636 DataTime=0.410 Loss=1.384 Prec@1=65.734 Prec@5=87.047 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=11:37 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.636 DataTime=0.410 Loss=1.384 Prec@1=65.734 Prec@5=87.047 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=11:38 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.628 DataTime=0.403 Loss=1.385 Prec@1=65.648 Prec@5=86.959 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=11:38 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.628 DataTime=0.403 Loss=1.385 Prec@1=65.648 Prec@5=86.959 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:38 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.628 DataTime=0.403 Loss=1.385 Prec@1=65.648 Prec@5=86.959 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:39 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.624 DataTime=0.399 Loss=1.385 Prec@1=65.630 Prec@5=86.973 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=11:39 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.624 DataTime=0.399 Loss=1.385 Prec@1=65.630 Prec@5=86.973 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:39 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.624 DataTime=0.399 Loss=1.385 Prec@1=65.630 Prec@5=86.973 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:40 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.622 DataTime=0.397 Loss=1.387 Prec@1=65.609 Prec@5=86.941 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=11:40 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.622 DataTime=0.397 Loss=1.387 Prec@1=65.609 Prec@5=86.941 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=11:40 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.622 DataTime=0.397 Loss=1.387 Prec@1=65.609 Prec@5=86.941 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=11:41 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.595 Prec@5=86.895 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=11:41 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.595 Prec@5=86.895 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=11:41 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.595 Prec@5=86.895 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=11:42 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.391 Prec@1=65.545 Prec@5=86.849 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=11:42 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.391 Prec@1=65.545 Prec@5=86.849 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:42 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.391 Prec@1=65.545 Prec@5=86.849 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:43 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.394 Prec@1=65.484 Prec@5=86.825 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=11:43 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.394 Prec@1=65.484 Prec@5=86.825 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:43 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.619 DataTime=0.395 Loss=1.394 Prec@1=65.484 Prec@5=86.825 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:44 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.436 Prec@5=86.780 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:44 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.436 Prec@5=86.780 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=11:44 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.436 Prec@5=86.780 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=11:45 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.397 Prec@1=65.424 Prec@5=86.784 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=11:45 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.397 Prec@1=65.424 Prec@5=86.784 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:45 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.618 DataTime=0.394 Loss=1.397 Prec@1=65.424 Prec@5=86.784 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:46 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.398 Prec@1=65.381 Prec@5=86.768 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:46 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.398 Prec@1=65.381 Prec@5=86.768 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:46 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.398 Prec@1=65.381 Prec@5=86.768 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:47 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.353 Prec@5=86.740 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=11:47 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.353 Prec@5=86.740 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:47 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.353 Prec@5=86.740 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:48 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.331 Prec@5=86.719 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=11:48 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.331 Prec@5=86.719 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=11:48 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.331 Prec@5=86.719 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=11:49 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.294 Prec@5=86.688 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=11:49 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.294 Prec@5=86.688 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:49 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.294 Prec@5=86.688 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:50 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.405 Prec@1=65.264 Prec@5=86.657 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=11:50 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.405 Prec@1=65.264 Prec@5=86.657 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:50 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.405 Prec@1=65.264 Prec@5=86.657 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:51 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.406 Prec@1=65.259 Prec@5=86.642 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=11:51 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.406 Prec@1=65.259 Prec@5=86.642 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:51 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.406 Prec@1=65.259 Prec@5=86.642 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:52 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.219 Prec@5=86.625 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=11:52 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.219 Prec@5=86.625 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:52 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.219 Prec@5=86.625 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:53 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.178 Prec@5=86.600 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=11:53 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.178 Prec@5=86.600 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:53 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.178 Prec@5=86.600 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:54 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.391 Loss=1.410 Prec@1=65.164 Prec@5=86.595 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=11:54 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.391 Loss=1.410 Prec@1=65.164 Prec@5=86.595 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=11:54 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.616 DataTime=0.391 Loss=1.410 Prec@1=65.164 Prec@5=86.595 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=11:55 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.411 Prec@1=65.130 Prec@5=86.581 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=11:55 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.411 Prec@1=65.130 Prec@5=86.581 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:55 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.411 Prec@1=65.130 Prec@5=86.581 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:56 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.412 Prec@1=65.115 Prec@5=86.579 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=11:56 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.412 Prec@1=65.115 Prec@5=86.579 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:56 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.412 Prec@1=65.115 Prec@5=86.579 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:57 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.413 Prec@1=65.076 Prec@5=86.551 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=11:57 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.413 Prec@1=65.076 Prec@5=86.551 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:57 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.413 Prec@1=65.076 Prec@5=86.551 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:58 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.415 Prec@1=65.051 Prec@5=86.521 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=11:58 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.415 Prec@1=65.051 Prec@5=86.521 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:58 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.415 Prec@1=65.051 Prec@5=86.521 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:59 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.416 Prec@1=65.029 Prec@5=86.507 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=11:59 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.416 Prec@1=65.029 Prec@5=86.507 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=11:59 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.416 Prec@1=65.029 Prec@5=86.507 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=12:00 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.417 Prec@1=65.006 Prec@5=86.491 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=12:00 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.417 Prec@1=65.006 Prec@5=86.491 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=12:00 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.417 Prec@1=65.006 Prec@5=86.491 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=12:00 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.417 Prec@1=65.005 Prec@5=86.490 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=12:00 IST=> training   100.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.615 DataTime=0.391 Loss=1.417 Prec@1=65.005 Prec@5=86.490 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=12:00 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST=> validation 0.00% of 1x98...Epoch=49/150 LR=0.07679 Time=6.727 Loss=1.560 Prec@1=62.109 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST=> validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.727 Loss=1.560 Prec@1=62.109 Prec@5=84.961 rate=6046.06 Hz, eta=0:00:00, total=0:00:00, wall=12:00 IST** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.727 Loss=1.560 Prec@1=62.109 Prec@5=84.961 rate=6046.06 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=0.413 Loss=1.535 Prec@1=62.644 Prec@5=85.382 rate=6046.06 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST** validation 100.00% of 1x98...Epoch=49/150 LR=0.07679 Time=0.413 Loss=1.535 Prec@1=62.644 Prec@5=85.382 rate=2.90 Hz, eta=0:00:00, total=0:00:33, wall=12:01 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST=> training   0.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.127 DataTime=5.905 Loss=1.490 Prec@1=64.453 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.127 DataTime=5.905 Loss=1.490 Prec@1=64.453 Prec@5=84.375 rate=6135.12 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.127 DataTime=5.905 Loss=1.490 Prec@1=64.453 Prec@5=84.375 rate=6135.12 Hz, eta=0:00:00, total=0:00:00, wall=12:02 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.662 DataTime=0.439 Loss=1.384 Prec@1=65.619 Prec@5=87.067 rate=6135.12 Hz, eta=0:00:00, total=0:00:00, wall=12:02 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.662 DataTime=0.439 Loss=1.384 Prec@1=65.619 Prec@5=87.067 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=12:02 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.662 DataTime=0.439 Loss=1.384 Prec@1=65.619 Prec@5=87.067 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=12:03 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.639 DataTime=0.414 Loss=1.378 Prec@1=65.695 Prec@5=87.154 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=12:03 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.639 DataTime=0.414 Loss=1.378 Prec@1=65.695 Prec@5=87.154 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=12:03 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.639 DataTime=0.414 Loss=1.378 Prec@1=65.695 Prec@5=87.154 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=12:04 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.630 DataTime=0.406 Loss=1.377 Prec@1=65.768 Prec@5=87.148 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=12:04 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.630 DataTime=0.406 Loss=1.377 Prec@1=65.768 Prec@5=87.148 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=12:04 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.630 DataTime=0.406 Loss=1.377 Prec@1=65.768 Prec@5=87.148 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=12:05 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.626 DataTime=0.402 Loss=1.379 Prec@1=65.774 Prec@5=87.121 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=12:05 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.626 DataTime=0.402 Loss=1.379 Prec@1=65.774 Prec@5=87.121 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:05 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.626 DataTime=0.402 Loss=1.379 Prec@1=65.774 Prec@5=87.121 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:06 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.623 DataTime=0.400 Loss=1.379 Prec@1=65.717 Prec@5=87.088 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:06 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.623 DataTime=0.400 Loss=1.379 Prec@1=65.717 Prec@5=87.088 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=12:06 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.623 DataTime=0.400 Loss=1.379 Prec@1=65.717 Prec@5=87.088 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=12:07 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.621 DataTime=0.398 Loss=1.382 Prec@1=65.665 Prec@5=87.036 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=12:07 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.621 DataTime=0.398 Loss=1.382 Prec@1=65.665 Prec@5=87.036 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=12:07 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.621 DataTime=0.398 Loss=1.382 Prec@1=65.665 Prec@5=87.036 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=12:08 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.386 Prec@1=65.619 Prec@5=86.993 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=12:08 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.386 Prec@1=65.619 Prec@5=86.993 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=12:08 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.386 Prec@1=65.619 Prec@5=86.993 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=12:09 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.590 Prec@5=86.956 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=12:09 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.590 Prec@5=86.956 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=12:09 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.620 DataTime=0.396 Loss=1.388 Prec@1=65.590 Prec@5=86.956 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=12:10 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.390 Prec@1=65.568 Prec@5=86.922 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=12:10 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.390 Prec@1=65.568 Prec@5=86.922 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=12:10 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.390 Prec@1=65.568 Prec@5=86.922 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=12:11 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.392 Prec@1=65.544 Prec@5=86.883 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=12:11 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.392 Prec@1=65.544 Prec@5=86.883 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:11 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.619 DataTime=0.395 Loss=1.392 Prec@1=65.544 Prec@5=86.883 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:12 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.392 Prec@1=65.544 Prec@5=86.878 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:12 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.392 Prec@1=65.544 Prec@5=86.878 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=12:12 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.392 Prec@1=65.544 Prec@5=86.878 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=12:13 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.393 Prec@1=65.525 Prec@5=86.868 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=12:13 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.393 Prec@1=65.525 Prec@5=86.868 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=12:13 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.393 Prec@1=65.525 Prec@5=86.868 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=12:14 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.456 Prec@5=86.820 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=12:14 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.456 Prec@5=86.820 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:14 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.618 DataTime=0.394 Loss=1.396 Prec@1=65.456 Prec@5=86.820 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:15 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.397 Prec@1=65.424 Prec@5=86.800 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:15 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.397 Prec@1=65.424 Prec@5=86.800 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=12:15 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.397 Prec@1=65.424 Prec@5=86.800 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=12:16 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.400 Prec@1=65.381 Prec@5=86.765 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=12:16 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.400 Prec@1=65.381 Prec@5=86.765 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:16 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.400 Prec@1=65.381 Prec@5=86.765 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:17 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.358 Prec@5=86.752 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:17 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.358 Prec@5=86.752 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=12:17 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.401 Prec@1=65.358 Prec@5=86.752 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=12:18 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.403 Prec@1=65.323 Prec@5=86.731 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=12:18 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.403 Prec@1=65.323 Prec@5=86.731 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=12:18 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.393 Loss=1.403 Prec@1=65.323 Prec@5=86.731 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=12:19 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.404 Prec@1=65.287 Prec@5=86.710 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=12:19 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.404 Prec@1=65.287 Prec@5=86.710 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=12:19 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.404 Prec@1=65.287 Prec@5=86.710 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=12:20 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.405 Prec@1=65.282 Prec@5=86.694 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=12:20 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.405 Prec@1=65.282 Prec@5=86.694 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:20 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.405 Prec@1=65.282 Prec@5=86.694 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:21 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.407 Prec@1=65.238 Prec@5=86.680 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:21 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.407 Prec@1=65.238 Prec@5=86.680 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=12:21 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.407 Prec@1=65.238 Prec@5=86.680 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=12:22 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.406 Prec@1=65.253 Prec@5=86.675 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=12:22 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.406 Prec@1=65.253 Prec@5=86.675 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:22 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.617 DataTime=0.392 Loss=1.406 Prec@1=65.253 Prec@5=86.675 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:23 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.211 Prec@5=86.646 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:23 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.211 Prec@5=86.646 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:23 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.408 Prec@1=65.211 Prec@5=86.646 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:24 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.200 Prec@5=86.642 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:24 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.200 Prec@5=86.642 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:24 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.200 Prec@5=86.642 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:25 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.193 Prec@5=86.626 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:25 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.193 Prec@5=86.626 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:25 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.409 Prec@1=65.193 Prec@5=86.626 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:27 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.410 Prec@1=65.165 Prec@5=86.612 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:27 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.410 Prec@1=65.165 Prec@5=86.612 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:27 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.410 Prec@1=65.165 Prec@5=86.612 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:27 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.410 Prec@1=65.166 Prec@5=86.612 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:27 IST=> training   100.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.616 DataTime=0.392 Loss=1.410 Prec@1=65.166 Prec@5=86.612 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=12:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> validation 0.00% of 1x98...Epoch=50/150 LR=0.07590 Time=6.755 Loss=1.611 Prec@1=60.352 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=6.755 Loss=1.611 Prec@1=60.352 Prec@5=83.594 rate=6126.25 Hz, eta=0:00:00, total=0:00:00, wall=12:27 IST** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=6.755 Loss=1.611 Prec@1=60.352 Prec@5=83.594 rate=6126.25 Hz, eta=0:00:00, total=0:00:00, wall=12:27 IST** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=0.405 Loss=1.550 Prec@1=62.134 Prec@5=85.110 rate=6126.25 Hz, eta=0:00:00, total=0:00:00, wall=12:27 IST** validation 100.00% of 1x98...Epoch=50/150 LR=0.07590 Time=0.405 Loss=1.550 Prec@1=62.134 Prec@5=85.110 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=12:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> training   0.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.918 DataTime=5.613 Loss=1.353 Prec@1=67.383 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=12:27 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.918 DataTime=5.613 Loss=1.353 Prec@1=67.383 Prec@5=87.891 rate=6985.78 Hz, eta=0:00:00, total=0:00:00, wall=12:27 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.918 DataTime=5.613 Loss=1.353 Prec@1=67.383 Prec@5=87.891 rate=6985.78 Hz, eta=0:00:00, total=0:00:00, wall=12:28 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.662 DataTime=0.437 Loss=1.364 Prec@1=65.892 Prec@5=87.185 rate=6985.78 Hz, eta=0:00:00, total=0:00:00, wall=12:28 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.662 DataTime=0.437 Loss=1.364 Prec@1=65.892 Prec@5=87.185 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=12:28 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.662 DataTime=0.437 Loss=1.364 Prec@1=65.892 Prec@5=87.185 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=12:29 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.638 DataTime=0.413 Loss=1.365 Prec@1=66.095 Prec@5=87.197 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=12:29 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.638 DataTime=0.413 Loss=1.365 Prec@1=66.095 Prec@5=87.197 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=12:29 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.638 DataTime=0.413 Loss=1.365 Prec@1=66.095 Prec@5=87.197 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=12:30 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.630 DataTime=0.405 Loss=1.372 Prec@1=66.012 Prec@5=87.087 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=12:30 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.630 DataTime=0.405 Loss=1.372 Prec@1=66.012 Prec@5=87.087 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=12:30 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.630 DataTime=0.405 Loss=1.372 Prec@1=66.012 Prec@5=87.087 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=12:31 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.625 DataTime=0.401 Loss=1.378 Prec@1=65.821 Prec@5=87.019 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=12:31 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.625 DataTime=0.401 Loss=1.378 Prec@1=65.821 Prec@5=87.019 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:31 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.625 DataTime=0.401 Loss=1.378 Prec@1=65.821 Prec@5=87.019 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:32 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.623 DataTime=0.399 Loss=1.380 Prec@1=65.783 Prec@5=87.007 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=12:32 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.623 DataTime=0.399 Loss=1.380 Prec@1=65.783 Prec@5=87.007 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=12:32 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.623 DataTime=0.399 Loss=1.380 Prec@1=65.783 Prec@5=87.007 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=12:33 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.622 DataTime=0.398 Loss=1.381 Prec@1=65.732 Prec@5=86.987 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=12:33 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.622 DataTime=0.398 Loss=1.381 Prec@1=65.732 Prec@5=86.987 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=12:33 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.622 DataTime=0.398 Loss=1.381 Prec@1=65.732 Prec@5=86.987 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=12:35 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.621 DataTime=0.396 Loss=1.382 Prec@1=65.687 Prec@5=86.975 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=12:35 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.621 DataTime=0.396 Loss=1.382 Prec@1=65.687 Prec@5=86.975 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:35 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.621 DataTime=0.396 Loss=1.382 Prec@1=65.687 Prec@5=86.975 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:36 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.620 DataTime=0.395 Loss=1.382 Prec@1=65.690 Prec@5=86.945 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:36 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.620 DataTime=0.395 Loss=1.382 Prec@1=65.690 Prec@5=86.945 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:36 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.620 DataTime=0.395 Loss=1.382 Prec@1=65.690 Prec@5=86.945 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:37 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.395 Loss=1.385 Prec@1=65.646 Prec@5=86.912 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:37 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.395 Loss=1.385 Prec@1=65.646 Prec@5=86.912 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:37 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.395 Loss=1.385 Prec@1=65.646 Prec@5=86.912 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:38 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.394 Loss=1.388 Prec@1=65.581 Prec@5=86.855 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:38 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.394 Loss=1.388 Prec@1=65.581 Prec@5=86.855 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:38 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.619 DataTime=0.394 Loss=1.388 Prec@1=65.581 Prec@5=86.855 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:39 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.394 Loss=1.390 Prec@1=65.574 Prec@5=86.829 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:39 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.394 Loss=1.390 Prec@1=65.574 Prec@5=86.829 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:39 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.394 Loss=1.390 Prec@1=65.574 Prec@5=86.829 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:40 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.525 Prec@5=86.802 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:40 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.525 Prec@5=86.802 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:40 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.525 Prec@5=86.802 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:41 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.537 Prec@5=86.810 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:41 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.537 Prec@5=86.810 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:41 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.392 Prec@1=65.537 Prec@5=86.810 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:42 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.522 Prec@5=86.809 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:42 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.522 Prec@5=86.809 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:42 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.522 Prec@5=86.809 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:43 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.534 Prec@5=86.818 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:43 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.534 Prec@5=86.818 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=12:43 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.618 DataTime=0.393 Loss=1.393 Prec@1=65.534 Prec@5=86.818 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=12:44 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.393 Loss=1.394 Prec@1=65.496 Prec@5=86.789 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=12:44 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.393 Loss=1.394 Prec@1=65.496 Prec@5=86.789 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:44 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.393 Loss=1.394 Prec@1=65.496 Prec@5=86.789 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:45 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.396 Prec@1=65.458 Prec@5=86.774 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:45 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.396 Prec@1=65.458 Prec@5=86.774 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:45 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.396 Prec@1=65.458 Prec@5=86.774 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:46 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.397 Prec@1=65.429 Prec@5=86.760 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:46 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.397 Prec@1=65.429 Prec@5=86.760 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:46 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.397 Prec@1=65.429 Prec@5=86.760 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:47 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.399 Prec@1=65.407 Prec@5=86.739 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:47 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.399 Prec@1=65.407 Prec@5=86.739 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:47 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.399 Prec@1=65.407 Prec@5=86.739 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:48 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.400 Prec@1=65.373 Prec@5=86.727 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:48 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.400 Prec@1=65.373 Prec@5=86.727 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:48 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.400 Prec@1=65.373 Prec@5=86.727 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:49 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.401 Prec@1=65.345 Prec@5=86.716 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:49 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.401 Prec@1=65.345 Prec@5=86.716 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:49 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.401 Prec@1=65.345 Prec@5=86.716 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:50 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.318 Prec@5=86.703 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:50 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.318 Prec@5=86.703 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=12:50 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.617 DataTime=0.392 Loss=1.402 Prec@1=65.318 Prec@5=86.703 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=12:51 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.402 Prec@1=65.302 Prec@5=86.698 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=12:51 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.402 Prec@1=65.302 Prec@5=86.698 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:51 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.402 Prec@1=65.302 Prec@5=86.698 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:52 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.403 Prec@1=65.296 Prec@5=86.682 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:52 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.403 Prec@1=65.296 Prec@5=86.682 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:52 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.403 Prec@1=65.296 Prec@5=86.682 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:53 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.288 Prec@5=86.673 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:53 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.288 Prec@5=86.673 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:53 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.288 Prec@5=86.673 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:53 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.286 Prec@5=86.672 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:53 IST=> training   100.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.616 DataTime=0.392 Loss=1.404 Prec@1=65.286 Prec@5=86.672 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=12:53 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:53 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:53 IST=> validation 0.00% of 1x98...Epoch=51/150 LR=0.07500 Time=6.918 Loss=1.342 Prec@1=65.625 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=12:53 IST=> validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.918 Loss=1.342 Prec@1=65.625 Prec@5=87.305 rate=4930.55 Hz, eta=0:00:00, total=0:00:00, wall=12:53 IST** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.918 Loss=1.342 Prec@1=65.625 Prec@5=87.305 rate=4930.55 Hz, eta=0:00:00, total=0:00:00, wall=12:54 IST** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=0.403 Loss=1.469 Prec@1=63.756 Prec@5=86.172 rate=4930.55 Hz, eta=0:00:00, total=0:00:00, wall=12:54 IST** validation 100.00% of 1x98...Epoch=51/150 LR=0.07500 Time=0.403 Loss=1.469 Prec@1=63.756 Prec@5=86.172 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=12:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:54 IST=> training   0.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.828 DataTime=4.517 Loss=1.465 Prec@1=66.016 Prec@5=85.742 rate=0 Hz, eta=?, total=0:00:00, wall=12:54 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.828 DataTime=4.517 Loss=1.465 Prec@1=66.016 Prec@5=85.742 rate=6981.00 Hz, eta=0:00:00, total=0:00:00, wall=12:54 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.828 DataTime=4.517 Loss=1.465 Prec@1=66.016 Prec@5=85.742 rate=6981.00 Hz, eta=0:00:00, total=0:00:00, wall=12:55 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.653 DataTime=0.426 Loss=1.366 Prec@1=66.155 Prec@5=87.295 rate=6981.00 Hz, eta=0:00:00, total=0:00:00, wall=12:55 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.653 DataTime=0.426 Loss=1.366 Prec@1=66.155 Prec@5=87.295 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:55 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.653 DataTime=0.426 Loss=1.366 Prec@1=66.155 Prec@5=87.295 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:56 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.634 DataTime=0.408 Loss=1.363 Prec@1=66.123 Prec@5=87.224 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:56 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.634 DataTime=0.408 Loss=1.363 Prec@1=66.123 Prec@5=87.224 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=12:56 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.634 DataTime=0.408 Loss=1.363 Prec@1=66.123 Prec@5=87.224 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=12:57 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.627 DataTime=0.401 Loss=1.366 Prec@1=66.036 Prec@5=87.198 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=12:57 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.627 DataTime=0.401 Loss=1.366 Prec@1=66.036 Prec@5=87.198 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=12:57 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.627 DataTime=0.401 Loss=1.366 Prec@1=66.036 Prec@5=87.198 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=12:58 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.623 DataTime=0.398 Loss=1.368 Prec@1=66.021 Prec@5=87.143 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=12:58 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.623 DataTime=0.398 Loss=1.368 Prec@1=66.021 Prec@5=87.143 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=12:58 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.623 DataTime=0.398 Loss=1.368 Prec@1=66.021 Prec@5=87.143 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=12:59 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.622 DataTime=0.397 Loss=1.371 Prec@1=65.970 Prec@5=87.140 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=12:59 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.622 DataTime=0.397 Loss=1.371 Prec@1=65.970 Prec@5=87.140 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=12:59 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.622 DataTime=0.397 Loss=1.371 Prec@1=65.970 Prec@5=87.140 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=13:00 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.621 DataTime=0.396 Loss=1.373 Prec@1=65.925 Prec@5=87.133 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=13:00 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.621 DataTime=0.396 Loss=1.373 Prec@1=65.925 Prec@5=87.133 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=13:00 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.621 DataTime=0.396 Loss=1.373 Prec@1=65.925 Prec@5=87.133 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=13:01 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.620 DataTime=0.395 Loss=1.377 Prec@1=65.857 Prec@5=87.081 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=13:01 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.620 DataTime=0.395 Loss=1.377 Prec@1=65.857 Prec@5=87.081 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:01 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.620 DataTime=0.395 Loss=1.377 Prec@1=65.857 Prec@5=87.081 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:02 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.619 DataTime=0.394 Loss=1.378 Prec@1=65.823 Prec@5=87.064 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:02 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.619 DataTime=0.394 Loss=1.378 Prec@1=65.823 Prec@5=87.064 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=13:02 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.619 DataTime=0.394 Loss=1.378 Prec@1=65.823 Prec@5=87.064 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=13:03 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.394 Loss=1.382 Prec@1=65.753 Prec@5=87.012 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=13:03 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.394 Loss=1.382 Prec@1=65.753 Prec@5=87.012 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:03 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.394 Loss=1.382 Prec@1=65.753 Prec@5=87.012 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:04 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.382 Prec@1=65.735 Prec@5=87.010 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:04 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.382 Prec@1=65.735 Prec@5=87.010 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=13:04 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.382 Prec@1=65.735 Prec@5=87.010 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=13:05 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.384 Prec@1=65.721 Prec@5=86.994 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=13:05 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.384 Prec@1=65.721 Prec@5=86.994 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=13:05 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.618 DataTime=0.393 Loss=1.384 Prec@1=65.721 Prec@5=86.994 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=13:06 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.385 Prec@1=65.689 Prec@5=86.976 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=13:06 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.385 Prec@1=65.689 Prec@5=86.976 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:06 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.385 Prec@1=65.689 Prec@5=86.976 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:07 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.386 Prec@1=65.673 Prec@5=86.952 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:07 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.386 Prec@1=65.673 Prec@5=86.952 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:07 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.393 Loss=1.386 Prec@1=65.673 Prec@5=86.952 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:08 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.660 Prec@5=86.949 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:08 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.660 Prec@5=86.949 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:08 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.660 Prec@5=86.949 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:09 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.644 Prec@5=86.938 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:09 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.644 Prec@5=86.938 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=13:09 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.644 Prec@5=86.938 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=13:10 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.627 Prec@5=86.930 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=13:10 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.627 Prec@5=86.930 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:10 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.627 Prec@5=86.930 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:11 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.389 Prec@1=65.602 Prec@5=86.906 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:11 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.389 Prec@1=65.602 Prec@5=86.906 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=13:11 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.617 DataTime=0.392 Loss=1.389 Prec@1=65.602 Prec@5=86.906 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=13:12 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.559 Prec@5=86.890 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=13:12 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.559 Prec@5=86.890 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=13:12 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.559 Prec@5=86.890 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=13:13 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.524 Prec@5=86.874 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=13:13 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.524 Prec@5=86.874 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:13 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.524 Prec@5=86.874 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:14 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.393 Prec@1=65.489 Prec@5=86.855 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:14 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.393 Prec@1=65.489 Prec@5=86.855 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:14 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.392 Loss=1.393 Prec@1=65.489 Prec@5=86.855 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:15 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.394 Prec@1=65.472 Prec@5=86.838 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:15 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.394 Prec@1=65.472 Prec@5=86.838 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:15 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.394 Prec@1=65.472 Prec@5=86.838 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:16 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.395 Prec@1=65.453 Prec@5=86.824 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:16 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.395 Prec@1=65.453 Prec@5=86.824 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:16 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.395 Prec@1=65.453 Prec@5=86.824 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:17 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.431 Prec@5=86.822 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:17 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.431 Prec@5=86.822 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=13:17 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.431 Prec@5=86.822 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=13:18 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.427 Prec@5=86.813 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=13:18 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.427 Prec@5=86.813 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:18 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.396 Prec@1=65.427 Prec@5=86.813 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:19 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.398 Prec@1=65.394 Prec@5=86.792 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:19 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.398 Prec@1=65.394 Prec@5=86.792 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:19 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.398 Prec@1=65.394 Prec@5=86.792 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:19 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.398 Prec@1=65.394 Prec@5=86.792 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:19 IST=> training   100.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.616 DataTime=0.391 Loss=1.398 Prec@1=65.394 Prec@5=86.792 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=13:19 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:19 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:19 IST=> validation 0.00% of 1x98...Epoch=52/150 LR=0.07409 Time=6.437 Loss=1.412 Prec@1=65.820 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=13:19 IST=> validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.437 Loss=1.412 Prec@1=65.820 Prec@5=86.914 rate=5412.02 Hz, eta=0:00:00, total=0:00:00, wall=13:19 IST** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.437 Loss=1.412 Prec@1=65.820 Prec@5=86.914 rate=5412.02 Hz, eta=0:00:00, total=0:00:00, wall=13:20 IST** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=0.402 Loss=1.484 Prec@1=63.610 Prec@5=85.812 rate=5412.02 Hz, eta=0:00:00, total=0:00:00, wall=13:20 IST** validation 100.00% of 1x98...Epoch=52/150 LR=0.07409 Time=0.402 Loss=1.484 Prec@1=63.610 Prec@5=85.812 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=13:20 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:20 IST=> training   0.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.949 DataTime=4.622 Loss=1.226 Prec@1=69.727 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=13:20 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.949 DataTime=4.622 Loss=1.226 Prec@1=69.727 Prec@5=89.453 rate=6927.42 Hz, eta=0:00:00, total=0:00:00, wall=13:20 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=4.949 DataTime=4.622 Loss=1.226 Prec@1=69.727 Prec@5=89.453 rate=6927.42 Hz, eta=0:00:00, total=0:00:00, wall=13:21 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.658 DataTime=0.433 Loss=1.367 Prec@1=66.012 Prec@5=87.254 rate=6927.42 Hz, eta=0:00:00, total=0:00:00, wall=13:21 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.658 DataTime=0.433 Loss=1.367 Prec@1=66.012 Prec@5=87.254 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=13:21 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.658 DataTime=0.433 Loss=1.367 Prec@1=66.012 Prec@5=87.254 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=13:22 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.637 DataTime=0.411 Loss=1.356 Prec@1=66.263 Prec@5=87.384 rate=1.64 Hz, eta=0:24:24, total=0:01:01, wall=13:22 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.637 DataTime=0.411 Loss=1.356 Prec@1=66.263 Prec@5=87.384 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=13:22 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.637 DataTime=0.411 Loss=1.356 Prec@1=66.263 Prec@5=87.384 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=13:23 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.629 DataTime=0.404 Loss=1.361 Prec@1=66.055 Prec@5=87.318 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=13:23 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.629 DataTime=0.404 Loss=1.361 Prec@1=66.055 Prec@5=87.318 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=13:23 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.629 DataTime=0.404 Loss=1.361 Prec@1=66.055 Prec@5=87.318 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=13:24 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.625 DataTime=0.401 Loss=1.365 Prec@1=66.042 Prec@5=87.228 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=13:24 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.625 DataTime=0.401 Loss=1.365 Prec@1=66.042 Prec@5=87.228 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:24 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.625 DataTime=0.401 Loss=1.365 Prec@1=66.042 Prec@5=87.228 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:25 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.623 DataTime=0.399 Loss=1.368 Prec@1=66.000 Prec@5=87.180 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:25 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.623 DataTime=0.399 Loss=1.368 Prec@1=66.000 Prec@5=87.180 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:25 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.623 DataTime=0.399 Loss=1.368 Prec@1=66.000 Prec@5=87.180 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:26 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.621 DataTime=0.397 Loss=1.368 Prec@1=65.957 Prec@5=87.184 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:26 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.621 DataTime=0.397 Loss=1.368 Prec@1=65.957 Prec@5=87.184 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:26 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.621 DataTime=0.397 Loss=1.368 Prec@1=65.957 Prec@5=87.184 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:27 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.396 Loss=1.370 Prec@1=65.935 Prec@5=87.166 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:27 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.396 Loss=1.370 Prec@1=65.935 Prec@5=87.166 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=13:27 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.396 Loss=1.370 Prec@1=65.935 Prec@5=87.166 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=13:28 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.395 Loss=1.373 Prec@1=65.891 Prec@5=87.122 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=13:28 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.395 Loss=1.373 Prec@1=65.891 Prec@5=87.122 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=13:28 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.620 DataTime=0.395 Loss=1.373 Prec@1=65.891 Prec@5=87.122 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=13:29 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.619 DataTime=0.395 Loss=1.374 Prec@1=65.890 Prec@5=87.115 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=13:29 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.619 DataTime=0.395 Loss=1.374 Prec@1=65.890 Prec@5=87.115 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=13:29 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.619 DataTime=0.395 Loss=1.374 Prec@1=65.890 Prec@5=87.115 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=13:30 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.374 Prec@1=65.895 Prec@5=87.108 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=13:30 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.374 Prec@1=65.895 Prec@5=87.108 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:30 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.374 Prec@1=65.895 Prec@5=87.108 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:31 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.377 Prec@1=65.844 Prec@5=87.071 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:31 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.377 Prec@1=65.844 Prec@5=87.071 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:31 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.377 Prec@1=65.844 Prec@5=87.071 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:32 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.379 Prec@1=65.812 Prec@5=87.043 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:32 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.379 Prec@1=65.812 Prec@5=87.043 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=13:32 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.394 Loss=1.379 Prec@1=65.812 Prec@5=87.043 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=13:33 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.393 Loss=1.380 Prec@1=65.803 Prec@5=87.035 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=13:33 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.393 Loss=1.380 Prec@1=65.803 Prec@5=87.035 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:33 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.618 DataTime=0.393 Loss=1.380 Prec@1=65.803 Prec@5=87.035 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:34 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.380 Prec@1=65.793 Prec@5=87.022 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:34 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.380 Prec@1=65.793 Prec@5=87.022 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=13:34 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.380 Prec@1=65.793 Prec@5=87.022 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=13:36 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.381 Prec@1=65.768 Prec@5=87.017 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=13:36 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.381 Prec@1=65.768 Prec@5=87.017 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=13:36 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.381 Prec@1=65.768 Prec@5=87.017 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=13:37 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.382 Prec@1=65.765 Prec@5=87.012 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=13:37 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.382 Prec@1=65.765 Prec@5=87.012 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=13:37 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.393 Loss=1.382 Prec@1=65.765 Prec@5=87.012 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=13:38 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.383 Prec@1=65.743 Prec@5=86.996 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=13:38 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.383 Prec@1=65.743 Prec@5=86.996 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=13:38 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.383 Prec@1=65.743 Prec@5=86.996 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=13:39 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.384 Prec@1=65.727 Prec@5=86.965 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=13:39 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.384 Prec@1=65.727 Prec@5=86.965 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=13:39 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.384 Prec@1=65.727 Prec@5=86.965 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=13:40 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.386 Prec@1=65.691 Prec@5=86.952 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=13:40 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.386 Prec@1=65.691 Prec@5=86.952 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=13:40 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.386 Prec@1=65.691 Prec@5=86.952 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=13:41 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.658 Prec@5=86.939 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=13:41 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.658 Prec@5=86.939 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:41 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.617 DataTime=0.392 Loss=1.387 Prec@1=65.658 Prec@5=86.939 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:42 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.389 Prec@1=65.633 Prec@5=86.911 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:42 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.389 Prec@1=65.633 Prec@5=86.911 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=13:42 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.389 Prec@1=65.633 Prec@5=86.911 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=13:43 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.616 Prec@5=86.888 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=13:43 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.616 Prec@5=86.888 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:43 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.390 Prec@1=65.616 Prec@5=86.888 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:44 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.602 Prec@5=86.882 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=13:44 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.602 Prec@5=86.882 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=13:44 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.602 Prec@5=86.882 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=13:45 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.584 Prec@5=86.877 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=13:45 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.584 Prec@5=86.877 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:45 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.391 Prec@1=65.584 Prec@5=86.877 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:46 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.392 Prec@1=65.563 Prec@5=86.868 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.392 Prec@1=65.563 Prec@5=86.868 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.392 Prec@1=65.563 Prec@5=86.868 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.392 Prec@1=65.563 Prec@5=86.868 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:46 IST=> training   100.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.616 DataTime=0.392 Loss=1.392 Prec@1=65.563 Prec@5=86.868 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=13:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 0.00% of 1x98...Epoch=53/150 LR=0.07316 Time=6.884 Loss=1.624 Prec@1=63.867 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=6.884 Loss=1.624 Prec@1=63.867 Prec@5=83.008 rate=5579.71 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=6.884 Loss=1.624 Prec@1=63.867 Prec@5=83.008 rate=5579.71 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=0.411 Loss=1.491 Prec@1=63.508 Prec@5=85.846 rate=5579.71 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST** validation 100.00% of 1x98...Epoch=53/150 LR=0.07316 Time=0.411 Loss=1.491 Prec@1=63.508 Prec@5=85.846 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=13:46 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.635 DataTime=5.332 Loss=1.374 Prec@1=65.820 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.635 DataTime=5.332 Loss=1.374 Prec@1=65.820 Prec@5=86.719 rate=7867.57 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.635 DataTime=5.332 Loss=1.374 Prec@1=65.820 Prec@5=86.719 rate=7867.57 Hz, eta=0:00:00, total=0:00:00, wall=13:48 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.662 DataTime=0.434 Loss=1.350 Prec@1=66.472 Prec@5=87.450 rate=7867.57 Hz, eta=0:00:00, total=0:00:00, wall=13:48 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.662 DataTime=0.434 Loss=1.350 Prec@1=66.472 Prec@5=87.450 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=13:48 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.662 DataTime=0.434 Loss=1.350 Prec@1=66.472 Prec@5=87.450 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=13:49 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.639 DataTime=0.412 Loss=1.352 Prec@1=66.524 Prec@5=87.442 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=13:49 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.639 DataTime=0.412 Loss=1.352 Prec@1=66.524 Prec@5=87.442 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=13:49 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.639 DataTime=0.412 Loss=1.352 Prec@1=66.524 Prec@5=87.442 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=13:50 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.631 DataTime=0.405 Loss=1.353 Prec@1=66.429 Prec@5=87.404 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=13:50 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.631 DataTime=0.405 Loss=1.353 Prec@1=66.429 Prec@5=87.404 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=13:50 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.631 DataTime=0.405 Loss=1.353 Prec@1=66.429 Prec@5=87.404 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=13:51 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.627 DataTime=0.401 Loss=1.357 Prec@1=66.324 Prec@5=87.366 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=13:51 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.627 DataTime=0.401 Loss=1.357 Prec@1=66.324 Prec@5=87.366 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=13:51 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.627 DataTime=0.401 Loss=1.357 Prec@1=66.324 Prec@5=87.366 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=13:52 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.624 DataTime=0.399 Loss=1.357 Prec@1=66.248 Prec@5=87.358 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=13:52 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.624 DataTime=0.399 Loss=1.357 Prec@1=66.248 Prec@5=87.358 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:52 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.624 DataTime=0.399 Loss=1.357 Prec@1=66.248 Prec@5=87.358 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:53 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.623 DataTime=0.397 Loss=1.359 Prec@1=66.169 Prec@5=87.327 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=13:53 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.623 DataTime=0.397 Loss=1.359 Prec@1=66.169 Prec@5=87.327 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:53 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.623 DataTime=0.397 Loss=1.359 Prec@1=66.169 Prec@5=87.327 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:54 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.621 DataTime=0.396 Loss=1.361 Prec@1=66.160 Prec@5=87.276 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=13:54 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.621 DataTime=0.396 Loss=1.361 Prec@1=66.160 Prec@5=87.276 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:54 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.621 DataTime=0.396 Loss=1.361 Prec@1=66.160 Prec@5=87.276 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:55 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.396 Loss=1.363 Prec@1=66.138 Prec@5=87.243 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:55 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.396 Loss=1.363 Prec@1=66.138 Prec@5=87.243 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:55 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.396 Loss=1.363 Prec@1=66.138 Prec@5=87.243 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:56 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.395 Loss=1.366 Prec@1=66.076 Prec@5=87.193 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:56 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.395 Loss=1.366 Prec@1=66.076 Prec@5=87.193 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:56 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.620 DataTime=0.395 Loss=1.366 Prec@1=66.076 Prec@5=87.193 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:57 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.367 Prec@1=66.058 Prec@5=87.180 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:57 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.367 Prec@1=66.058 Prec@5=87.180 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:57 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.367 Prec@1=66.058 Prec@5=87.180 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:58 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.370 Prec@1=66.028 Prec@5=87.148 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:58 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.370 Prec@1=66.028 Prec@5=87.148 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:58 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.619 DataTime=0.394 Loss=1.370 Prec@1=66.028 Prec@5=87.148 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:59 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.394 Loss=1.372 Prec@1=65.993 Prec@5=87.114 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:59 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.394 Loss=1.372 Prec@1=65.993 Prec@5=87.114 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=13:59 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.394 Loss=1.372 Prec@1=65.993 Prec@5=87.114 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=14:00 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.372 Prec@1=65.982 Prec@5=87.117 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=14:00 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.372 Prec@1=65.982 Prec@5=87.117 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:00 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.372 Prec@1=65.982 Prec@5=87.117 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:01 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.373 Prec@1=65.972 Prec@5=87.103 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:01 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.373 Prec@1=65.972 Prec@5=87.103 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:01 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.618 DataTime=0.393 Loss=1.373 Prec@1=65.972 Prec@5=87.103 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:02 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.393 Loss=1.374 Prec@1=65.939 Prec@5=87.077 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:02 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.393 Loss=1.374 Prec@1=65.939 Prec@5=87.077 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=14:02 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.393 Loss=1.374 Prec@1=65.939 Prec@5=87.077 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=14:03 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.903 Prec@5=87.064 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=14:03 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.903 Prec@5=87.064 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:03 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.903 Prec@5=87.064 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:04 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.880 Prec@5=87.058 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:04 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.880 Prec@5=87.058 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:04 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.376 Prec@1=65.880 Prec@5=87.058 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:05 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.377 Prec@1=65.856 Prec@5=87.040 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:05 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.377 Prec@1=65.856 Prec@5=87.040 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:05 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.377 Prec@1=65.856 Prec@5=87.040 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:06 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.378 Prec@1=65.845 Prec@5=87.034 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:06 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.378 Prec@1=65.845 Prec@5=87.034 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:06 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.378 Prec@1=65.845 Prec@5=87.034 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:07 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.379 Prec@1=65.836 Prec@5=87.024 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:07 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.379 Prec@1=65.836 Prec@5=87.024 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:07 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.617 DataTime=0.392 Loss=1.379 Prec@1=65.836 Prec@5=87.024 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:08 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.380 Prec@1=65.808 Prec@5=87.000 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:08 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.380 Prec@1=65.808 Prec@5=87.000 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:08 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.380 Prec@1=65.808 Prec@5=87.000 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:09 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.381 Prec@1=65.780 Prec@5=86.983 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:09 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.381 Prec@1=65.780 Prec@5=86.983 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:09 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.392 Loss=1.381 Prec@1=65.780 Prec@5=86.983 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:10 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.755 Prec@5=86.962 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:10 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.755 Prec@5=86.962 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:10 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.755 Prec@5=86.962 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:11 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.743 Prec@5=86.954 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:11 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.743 Prec@5=86.954 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:11 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.383 Prec@1=65.743 Prec@5=86.954 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:12 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.384 Prec@1=65.730 Prec@5=86.944 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:12 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.384 Prec@1=65.730 Prec@5=86.944 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:12 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.384 Prec@1=65.730 Prec@5=86.944 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:12 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.384 Prec@1=65.729 Prec@5=86.944 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:12 IST=> training   100.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.616 DataTime=0.391 Loss=1.384 Prec@1=65.729 Prec@5=86.944 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=14:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:12 IST=> validation 0.00% of 1x98...Epoch=54/150 LR=0.07223 Time=7.048 Loss=1.611 Prec@1=62.891 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=14:12 IST=> validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=7.048 Loss=1.611 Prec@1=62.891 Prec@5=83.594 rate=3217.93 Hz, eta=0:00:00, total=0:00:00, wall=14:12 IST** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=7.048 Loss=1.611 Prec@1=62.891 Prec@5=83.594 rate=3217.93 Hz, eta=0:00:00, total=0:00:00, wall=14:13 IST** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=0.404 Loss=1.514 Prec@1=62.884 Prec@5=85.312 rate=3217.93 Hz, eta=0:00:00, total=0:00:00, wall=14:13 IST** validation 100.00% of 1x98...Epoch=54/150 LR=0.07223 Time=0.404 Loss=1.514 Prec@1=62.884 Prec@5=85.312 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=14:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:13 IST=> training   0.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=5.305 DataTime=5.021 Loss=1.303 Prec@1=69.141 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=14:13 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=5.305 DataTime=5.021 Loss=1.303 Prec@1=69.141 Prec@5=87.500 rate=6126.51 Hz, eta=0:00:00, total=0:00:00, wall=14:13 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=5.305 DataTime=5.021 Loss=1.303 Prec@1=69.141 Prec@5=87.500 rate=6126.51 Hz, eta=0:00:00, total=0:00:00, wall=14:14 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.656 DataTime=0.432 Loss=1.331 Prec@1=66.749 Prec@5=87.653 rate=6126.51 Hz, eta=0:00:00, total=0:00:00, wall=14:14 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.656 DataTime=0.432 Loss=1.331 Prec@1=66.749 Prec@5=87.653 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=14:14 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.656 DataTime=0.432 Loss=1.331 Prec@1=66.749 Prec@5=87.653 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=14:15 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.635 DataTime=0.411 Loss=1.340 Prec@1=66.643 Prec@5=87.495 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=14:15 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.635 DataTime=0.411 Loss=1.340 Prec@1=66.643 Prec@5=87.495 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=14:15 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.635 DataTime=0.411 Loss=1.340 Prec@1=66.643 Prec@5=87.495 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=14:16 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.628 DataTime=0.404 Loss=1.345 Prec@1=66.435 Prec@5=87.444 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=14:16 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.628 DataTime=0.404 Loss=1.345 Prec@1=66.435 Prec@5=87.444 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:16 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.628 DataTime=0.404 Loss=1.345 Prec@1=66.435 Prec@5=87.444 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:17 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.625 DataTime=0.401 Loss=1.348 Prec@1=66.343 Prec@5=87.413 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:17 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.625 DataTime=0.401 Loss=1.348 Prec@1=66.343 Prec@5=87.413 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:17 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.625 DataTime=0.401 Loss=1.348 Prec@1=66.343 Prec@5=87.413 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:18 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.623 DataTime=0.399 Loss=1.351 Prec@1=66.320 Prec@5=87.374 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:18 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.623 DataTime=0.399 Loss=1.351 Prec@1=66.320 Prec@5=87.374 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:18 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.623 DataTime=0.399 Loss=1.351 Prec@1=66.320 Prec@5=87.374 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:19 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.621 DataTime=0.397 Loss=1.351 Prec@1=66.334 Prec@5=87.381 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:19 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.621 DataTime=0.397 Loss=1.351 Prec@1=66.334 Prec@5=87.381 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:19 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.621 DataTime=0.397 Loss=1.351 Prec@1=66.334 Prec@5=87.381 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:20 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.620 DataTime=0.396 Loss=1.351 Prec@1=66.409 Prec@5=87.360 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:20 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.620 DataTime=0.396 Loss=1.351 Prec@1=66.409 Prec@5=87.360 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:20 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.620 DataTime=0.396 Loss=1.351 Prec@1=66.409 Prec@5=87.360 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:21 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.395 Loss=1.354 Prec@1=66.343 Prec@5=87.343 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:21 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.395 Loss=1.354 Prec@1=66.343 Prec@5=87.343 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:21 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.395 Loss=1.354 Prec@1=66.343 Prec@5=87.343 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:22 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.394 Loss=1.355 Prec@1=66.322 Prec@5=87.331 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:22 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.394 Loss=1.355 Prec@1=66.322 Prec@5=87.331 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:22 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.619 DataTime=0.394 Loss=1.355 Prec@1=66.322 Prec@5=87.331 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:23 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.394 Loss=1.358 Prec@1=66.270 Prec@5=87.300 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:23 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.394 Loss=1.358 Prec@1=66.270 Prec@5=87.300 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:23 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.394 Loss=1.358 Prec@1=66.270 Prec@5=87.300 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:24 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.360 Prec@1=66.236 Prec@5=87.286 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:24 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.360 Prec@1=66.236 Prec@5=87.286 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=14:24 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.360 Prec@1=66.236 Prec@5=87.286 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=14:25 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.362 Prec@1=66.195 Prec@5=87.251 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=14:25 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.362 Prec@1=66.195 Prec@5=87.251 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:25 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.618 DataTime=0.393 Loss=1.362 Prec@1=66.195 Prec@5=87.251 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:26 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.363 Prec@1=66.176 Prec@5=87.252 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:26 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.363 Prec@1=66.176 Prec@5=87.252 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=14:26 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.363 Prec@1=66.176 Prec@5=87.252 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=14:27 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.365 Prec@1=66.136 Prec@5=87.222 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=14:27 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.365 Prec@1=66.136 Prec@5=87.222 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:27 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.365 Prec@1=66.136 Prec@5=87.222 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:28 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.368 Prec@1=66.073 Prec@5=87.171 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:28 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.368 Prec@1=66.073 Prec@5=87.171 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:28 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.393 Loss=1.368 Prec@1=66.073 Prec@5=87.171 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:29 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.056 Prec@5=87.175 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:29 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.056 Prec@5=87.175 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:29 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.056 Prec@5=87.175 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:30 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.369 Prec@1=66.041 Prec@5=87.165 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:30 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.369 Prec@1=66.041 Prec@5=87.165 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:30 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.369 Prec@1=66.041 Prec@5=87.165 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:31 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.370 Prec@1=66.049 Prec@5=87.160 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=14:31 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.370 Prec@1=66.049 Prec@5=87.160 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:31 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.370 Prec@1=66.049 Prec@5=87.160 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:32 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.371 Prec@1=66.030 Prec@5=87.139 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=14:32 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.371 Prec@1=66.030 Prec@5=87.139 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:32 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.617 DataTime=0.392 Loss=1.371 Prec@1=66.030 Prec@5=87.139 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:33 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.372 Prec@1=65.993 Prec@5=87.118 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:33 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.372 Prec@1=65.993 Prec@5=87.118 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:33 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.372 Prec@1=65.993 Prec@5=87.118 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:34 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.374 Prec@1=65.970 Prec@5=87.100 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:34 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.374 Prec@1=65.970 Prec@5=87.100 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:34 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.374 Prec@1=65.970 Prec@5=87.100 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:35 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.948 Prec@5=87.080 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=14:35 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.948 Prec@5=87.080 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:35 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.948 Prec@5=87.080 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:36 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.928 Prec@5=87.067 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:36 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.928 Prec@5=87.067 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:36 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.375 Prec@1=65.928 Prec@5=87.067 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:38 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.376 Prec@1=65.911 Prec@5=87.047 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=14:38 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.376 Prec@1=65.911 Prec@5=87.047 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:38 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.376 Prec@1=65.911 Prec@5=87.047 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:39 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.377 Prec@1=65.889 Prec@5=87.024 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=14:39 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.377 Prec@1=65.889 Prec@5=87.024 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=14:39 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.377 Prec@1=65.889 Prec@5=87.024 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=14:39 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.377 Prec@1=65.889 Prec@5=87.023 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=14:39 IST=> training   100.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.616 DataTime=0.392 Loss=1.377 Prec@1=65.889 Prec@5=87.023 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=14:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> validation 0.00% of 1x98...Epoch=55/150 LR=0.07129 Time=6.538 Loss=1.577 Prec@1=62.891 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.538 Loss=1.577 Prec@1=62.891 Prec@5=84.375 rate=3825.12 Hz, eta=0:00:00, total=0:00:00, wall=14:39 IST** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.538 Loss=1.577 Prec@1=62.891 Prec@5=84.375 rate=3825.12 Hz, eta=0:00:00, total=0:00:00, wall=14:39 IST** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=0.406 Loss=1.456 Prec@1=64.094 Prec@5=86.328 rate=3825.12 Hz, eta=0:00:00, total=0:00:00, wall=14:39 IST** validation 100.00% of 1x98...Epoch=55/150 LR=0.07129 Time=0.406 Loss=1.456 Prec@1=64.094 Prec@5=86.328 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=14:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> training   0.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=6.527 DataTime=6.281 Loss=1.297 Prec@1=66.797 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=14:39 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=6.527 DataTime=6.281 Loss=1.297 Prec@1=66.797 Prec@5=88.867 rate=6304.81 Hz, eta=0:00:00, total=0:00:00, wall=14:39 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=6.527 DataTime=6.281 Loss=1.297 Prec@1=66.797 Prec@5=88.867 rate=6304.81 Hz, eta=0:00:00, total=0:00:00, wall=14:40 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.670 DataTime=0.445 Loss=1.345 Prec@1=66.306 Prec@5=87.556 rate=6304.81 Hz, eta=0:00:00, total=0:00:00, wall=14:40 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.670 DataTime=0.445 Loss=1.345 Prec@1=66.306 Prec@5=87.556 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=14:40 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.670 DataTime=0.445 Loss=1.345 Prec@1=66.306 Prec@5=87.556 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=14:41 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.642 DataTime=0.417 Loss=1.344 Prec@1=66.490 Prec@5=87.519 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=14:41 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.642 DataTime=0.417 Loss=1.344 Prec@1=66.490 Prec@5=87.519 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=14:41 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.642 DataTime=0.417 Loss=1.344 Prec@1=66.490 Prec@5=87.519 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=14:42 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.632 DataTime=0.408 Loss=1.346 Prec@1=66.496 Prec@5=87.426 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=14:42 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.632 DataTime=0.408 Loss=1.346 Prec@1=66.496 Prec@5=87.426 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:42 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.632 DataTime=0.408 Loss=1.346 Prec@1=66.496 Prec@5=87.426 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:43 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.628 DataTime=0.403 Loss=1.347 Prec@1=66.533 Prec@5=87.443 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=14:43 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.628 DataTime=0.403 Loss=1.347 Prec@1=66.533 Prec@5=87.443 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:43 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.628 DataTime=0.403 Loss=1.347 Prec@1=66.533 Prec@5=87.443 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:45 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.625 DataTime=0.400 Loss=1.347 Prec@1=66.516 Prec@5=87.454 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:45 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.625 DataTime=0.400 Loss=1.347 Prec@1=66.516 Prec@5=87.454 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:45 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.625 DataTime=0.400 Loss=1.347 Prec@1=66.516 Prec@5=87.454 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:46 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.623 DataTime=0.399 Loss=1.350 Prec@1=66.437 Prec@5=87.411 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=14:46 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.623 DataTime=0.399 Loss=1.350 Prec@1=66.437 Prec@5=87.411 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:46 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.623 DataTime=0.399 Loss=1.350 Prec@1=66.437 Prec@5=87.411 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:47 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.622 DataTime=0.397 Loss=1.352 Prec@1=66.414 Prec@5=87.401 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=14:47 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.622 DataTime=0.397 Loss=1.352 Prec@1=66.414 Prec@5=87.401 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:47 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.622 DataTime=0.397 Loss=1.352 Prec@1=66.414 Prec@5=87.401 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:48 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.621 DataTime=0.396 Loss=1.353 Prec@1=66.371 Prec@5=87.383 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=14:48 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.621 DataTime=0.396 Loss=1.353 Prec@1=66.371 Prec@5=87.383 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=14:48 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.621 DataTime=0.396 Loss=1.353 Prec@1=66.371 Prec@5=87.383 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=14:49 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.396 Loss=1.354 Prec@1=66.363 Prec@5=87.366 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=14:49 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.396 Loss=1.354 Prec@1=66.363 Prec@5=87.366 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:49 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.396 Loss=1.354 Prec@1=66.363 Prec@5=87.366 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:50 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.395 Loss=1.355 Prec@1=66.342 Prec@5=87.351 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:50 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.395 Loss=1.355 Prec@1=66.342 Prec@5=87.351 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:50 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.620 DataTime=0.395 Loss=1.355 Prec@1=66.342 Prec@5=87.351 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:51 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.357 Prec@1=66.279 Prec@5=87.324 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:51 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.357 Prec@1=66.279 Prec@5=87.324 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:51 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.357 Prec@1=66.279 Prec@5=87.324 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:52 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.358 Prec@1=66.263 Prec@5=87.308 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:52 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.358 Prec@1=66.263 Prec@5=87.308 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:52 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.619 DataTime=0.395 Loss=1.358 Prec@1=66.263 Prec@5=87.308 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:53 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.359 Prec@1=66.215 Prec@5=87.281 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:53 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.359 Prec@1=66.215 Prec@5=87.281 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:53 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.359 Prec@1=66.215 Prec@5=87.281 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:54 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.361 Prec@1=66.187 Prec@5=87.266 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:54 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.361 Prec@1=66.187 Prec@5=87.266 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:54 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.361 Prec@1=66.187 Prec@5=87.266 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:55 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.362 Prec@1=66.157 Prec@5=87.254 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:55 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.362 Prec@1=66.157 Prec@5=87.254 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:55 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.618 DataTime=0.394 Loss=1.362 Prec@1=66.157 Prec@5=87.254 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:56 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.131 Prec@5=87.231 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:56 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.131 Prec@5=87.231 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:56 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.131 Prec@5=87.231 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:57 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.114 Prec@5=87.217 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:57 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.114 Prec@5=87.217 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:57 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.364 Prec@1=66.114 Prec@5=87.217 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:58 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.086 Prec@5=87.205 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=14:58 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.086 Prec@5=87.205 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:58 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.086 Prec@5=87.205 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:59 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.077 Prec@5=87.191 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:59 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.077 Prec@5=87.191 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=14:59 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.366 Prec@1=66.077 Prec@5=87.191 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:00 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.367 Prec@1=66.067 Prec@5=87.184 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:00 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.367 Prec@1=66.067 Prec@5=87.184 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=15:00 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.393 Loss=1.367 Prec@1=66.067 Prec@5=87.184 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=15:01 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.035 Prec@5=87.170 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=15:01 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.035 Prec@5=87.170 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:01 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.617 DataTime=0.392 Loss=1.368 Prec@1=66.035 Prec@5=87.170 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:02 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.368 Prec@1=66.031 Prec@5=87.161 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:02 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.368 Prec@1=66.031 Prec@5=87.161 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:02 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.368 Prec@1=66.031 Prec@5=87.161 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:03 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.369 Prec@1=66.019 Prec@5=87.152 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:03 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.369 Prec@1=66.019 Prec@5=87.152 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:03 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.369 Prec@1=66.019 Prec@5=87.152 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:04 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.006 Prec@5=87.145 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:04 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.006 Prec@5=87.145 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:04 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.006 Prec@5=87.145 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:05 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.001 Prec@5=87.137 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:05 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.001 Prec@5=87.137 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:05 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.001 Prec@5=87.137 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:05 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.001 Prec@5=87.136 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:05 IST=> training   100.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.616 DataTime=0.392 Loss=1.370 Prec@1=66.001 Prec@5=87.136 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=15:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:05 IST=> validation 0.00% of 1x98...Epoch=56/150 LR=0.07034 Time=6.645 Loss=1.484 Prec@1=62.695 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=15:05 IST=> validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.645 Loss=1.484 Prec@1=62.695 Prec@5=87.109 rate=3306.17 Hz, eta=0:00:00, total=0:00:00, wall=15:05 IST** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.645 Loss=1.484 Prec@1=62.695 Prec@5=87.109 rate=3306.17 Hz, eta=0:00:00, total=0:00:00, wall=15:06 IST** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=0.404 Loss=1.437 Prec@1=64.602 Prec@5=86.598 rate=3306.17 Hz, eta=0:00:00, total=0:00:00, wall=15:06 IST** validation 100.00% of 1x98...Epoch=56/150 LR=0.07034 Time=0.404 Loss=1.437 Prec@1=64.602 Prec@5=86.598 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=15:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:06 IST=> training   0.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.988 DataTime=4.719 Loss=1.320 Prec@1=67.383 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=15:06 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.988 DataTime=4.719 Loss=1.320 Prec@1=67.383 Prec@5=88.086 rate=4194.40 Hz, eta=0:00:00, total=0:00:00, wall=15:06 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=4.988 DataTime=4.719 Loss=1.320 Prec@1=67.383 Prec@5=88.086 rate=4194.40 Hz, eta=0:00:00, total=0:00:00, wall=15:07 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.660 DataTime=0.432 Loss=1.335 Prec@1=66.627 Prec@5=87.653 rate=4194.40 Hz, eta=0:00:00, total=0:00:00, wall=15:07 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.660 DataTime=0.432 Loss=1.335 Prec@1=66.627 Prec@5=87.653 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=15:07 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.660 DataTime=0.432 Loss=1.335 Prec@1=66.627 Prec@5=87.653 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=15:08 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.637 DataTime=0.411 Loss=1.332 Prec@1=66.776 Prec@5=87.650 rate=1.64 Hz, eta=0:24:26, total=0:01:01, wall=15:08 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.637 DataTime=0.411 Loss=1.332 Prec@1=66.776 Prec@5=87.650 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=15:08 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.637 DataTime=0.411 Loss=1.332 Prec@1=66.776 Prec@5=87.650 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=15:09 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.629 DataTime=0.403 Loss=1.331 Prec@1=66.820 Prec@5=87.627 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=15:09 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.629 DataTime=0.403 Loss=1.331 Prec@1=66.820 Prec@5=87.627 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=15:09 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.629 DataTime=0.403 Loss=1.331 Prec@1=66.820 Prec@5=87.627 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=15:10 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.625 DataTime=0.400 Loss=1.335 Prec@1=66.746 Prec@5=87.549 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=15:10 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.625 DataTime=0.400 Loss=1.335 Prec@1=66.746 Prec@5=87.549 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=15:10 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.625 DataTime=0.400 Loss=1.335 Prec@1=66.746 Prec@5=87.549 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=15:11 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.623 DataTime=0.398 Loss=1.336 Prec@1=66.712 Prec@5=87.533 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=15:11 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.623 DataTime=0.398 Loss=1.336 Prec@1=66.712 Prec@5=87.533 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:11 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.623 DataTime=0.398 Loss=1.336 Prec@1=66.712 Prec@5=87.533 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:12 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.621 DataTime=0.396 Loss=1.340 Prec@1=66.667 Prec@5=87.510 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=15:12 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.621 DataTime=0.396 Loss=1.340 Prec@1=66.667 Prec@5=87.510 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:12 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.621 DataTime=0.396 Loss=1.340 Prec@1=66.667 Prec@5=87.510 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:13 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.620 DataTime=0.396 Loss=1.342 Prec@1=66.630 Prec@5=87.479 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=15:13 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.620 DataTime=0.396 Loss=1.342 Prec@1=66.630 Prec@5=87.479 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:13 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.620 DataTime=0.396 Loss=1.342 Prec@1=66.630 Prec@5=87.479 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:14 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.395 Loss=1.343 Prec@1=66.593 Prec@5=87.466 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=15:14 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.395 Loss=1.343 Prec@1=66.593 Prec@5=87.466 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:14 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.395 Loss=1.343 Prec@1=66.593 Prec@5=87.466 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:15 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.394 Loss=1.344 Prec@1=66.569 Prec@5=87.461 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=15:15 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.394 Loss=1.344 Prec@1=66.569 Prec@5=87.461 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:15 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.619 DataTime=0.394 Loss=1.344 Prec@1=66.569 Prec@5=87.461 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:16 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.394 Loss=1.346 Prec@1=66.503 Prec@5=87.436 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=15:16 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.394 Loss=1.346 Prec@1=66.503 Prec@5=87.436 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:16 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.394 Loss=1.346 Prec@1=66.503 Prec@5=87.436 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:17 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.347 Prec@1=66.455 Prec@5=87.434 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:17 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.347 Prec@1=66.455 Prec@5=87.434 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:17 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.347 Prec@1=66.455 Prec@5=87.434 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:18 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.348 Prec@1=66.442 Prec@5=87.425 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:18 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.348 Prec@1=66.442 Prec@5=87.425 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:18 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.618 DataTime=0.393 Loss=1.348 Prec@1=66.442 Prec@5=87.425 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:19 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.350 Prec@1=66.384 Prec@5=87.388 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:19 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.350 Prec@1=66.384 Prec@5=87.388 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:19 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.350 Prec@1=66.384 Prec@5=87.388 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:20 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.352 Prec@1=66.364 Prec@5=87.360 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=15:20 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.352 Prec@1=66.364 Prec@5=87.360 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:20 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.352 Prec@1=66.364 Prec@5=87.360 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:21 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.354 Prec@1=66.323 Prec@5=87.329 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:21 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.354 Prec@1=66.323 Prec@5=87.329 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:21 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.393 Loss=1.354 Prec@1=66.323 Prec@5=87.329 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:22 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.356 Prec@1=66.299 Prec@5=87.308 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=15:22 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.356 Prec@1=66.299 Prec@5=87.308 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:22 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.356 Prec@1=66.299 Prec@5=87.308 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:23 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.357 Prec@1=66.284 Prec@5=87.288 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:23 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.357 Prec@1=66.284 Prec@5=87.288 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=15:23 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.617 DataTime=0.392 Loss=1.357 Prec@1=66.284 Prec@5=87.288 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=15:24 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.358 Prec@1=66.264 Prec@5=87.285 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=15:24 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.358 Prec@1=66.264 Prec@5=87.285 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:24 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.358 Prec@1=66.264 Prec@5=87.285 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:25 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.359 Prec@1=66.243 Prec@5=87.263 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=15:25 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.359 Prec@1=66.243 Prec@5=87.263 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:25 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.359 Prec@1=66.243 Prec@5=87.263 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:26 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.360 Prec@1=66.200 Prec@5=87.252 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:26 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.360 Prec@1=66.200 Prec@5=87.252 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:26 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.360 Prec@1=66.200 Prec@5=87.252 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:27 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.361 Prec@1=66.195 Prec@5=87.239 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=15:27 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.361 Prec@1=66.195 Prec@5=87.239 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:27 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.361 Prec@1=66.195 Prec@5=87.239 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:28 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.362 Prec@1=66.182 Prec@5=87.224 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:28 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.362 Prec@1=66.182 Prec@5=87.224 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:28 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.362 Prec@1=66.182 Prec@5=87.224 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:29 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.363 Prec@1=66.152 Prec@5=87.210 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:29 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.363 Prec@1=66.152 Prec@5=87.210 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=15:29 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.363 Prec@1=66.152 Prec@5=87.210 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=15:30 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.364 Prec@1=66.138 Prec@5=87.196 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=15:30 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.364 Prec@1=66.138 Prec@5=87.196 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:30 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.392 Loss=1.364 Prec@1=66.138 Prec@5=87.196 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:31 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.391 Loss=1.365 Prec@1=66.116 Prec@5=87.186 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:31 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.391 Loss=1.365 Prec@1=66.116 Prec@5=87.186 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:31 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.391 Loss=1.365 Prec@1=66.116 Prec@5=87.186 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:31 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.391 Loss=1.365 Prec@1=66.116 Prec@5=87.187 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:31 IST=> training   100.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.616 DataTime=0.391 Loss=1.365 Prec@1=66.116 Prec@5=87.187 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=15:31 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> validation 0.00% of 1x98...Epoch=57/150 LR=0.06938 Time=6.763 Loss=1.475 Prec@1=64.258 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=6.763 Loss=1.475 Prec@1=64.258 Prec@5=86.523 rate=7744.07 Hz, eta=0:00:00, total=0:00:00, wall=15:32 IST** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=6.763 Loss=1.475 Prec@1=64.258 Prec@5=86.523 rate=7744.07 Hz, eta=0:00:00, total=0:00:00, wall=15:32 IST** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=0.408 Loss=1.458 Prec@1=64.334 Prec@5=86.462 rate=7744.07 Hz, eta=0:00:00, total=0:00:00, wall=15:32 IST** validation 100.00% of 1x98...Epoch=57/150 LR=0.06938 Time=0.408 Loss=1.458 Prec@1=64.334 Prec@5=86.462 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=15:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> training   0.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.559 DataTime=5.265 Loss=1.290 Prec@1=66.406 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=15:32 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.559 DataTime=5.265 Loss=1.290 Prec@1=66.406 Prec@5=88.281 rate=7724.28 Hz, eta=0:00:00, total=0:00:00, wall=15:32 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=5.559 DataTime=5.265 Loss=1.290 Prec@1=66.406 Prec@5=88.281 rate=7724.28 Hz, eta=0:00:00, total=0:00:00, wall=15:33 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.658 DataTime=0.433 Loss=1.335 Prec@1=66.638 Prec@5=87.651 rate=7724.28 Hz, eta=0:00:00, total=0:00:00, wall=15:33 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.658 DataTime=0.433 Loss=1.335 Prec@1=66.638 Prec@5=87.651 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=15:33 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.658 DataTime=0.433 Loss=1.335 Prec@1=66.638 Prec@5=87.651 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=15:34 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.636 DataTime=0.412 Loss=1.327 Prec@1=66.745 Prec@5=87.735 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=15:34 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.636 DataTime=0.412 Loss=1.327 Prec@1=66.745 Prec@5=87.735 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:34 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.636 DataTime=0.412 Loss=1.327 Prec@1=66.745 Prec@5=87.735 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:35 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.628 DataTime=0.405 Loss=1.329 Prec@1=66.695 Prec@5=87.740 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:35 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.628 DataTime=0.405 Loss=1.329 Prec@1=66.695 Prec@5=87.740 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=15:35 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.628 DataTime=0.405 Loss=1.329 Prec@1=66.695 Prec@5=87.740 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=15:36 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.625 DataTime=0.401 Loss=1.329 Prec@1=66.727 Prec@5=87.748 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=15:36 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.625 DataTime=0.401 Loss=1.329 Prec@1=66.727 Prec@5=87.748 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=15:36 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.625 DataTime=0.401 Loss=1.329 Prec@1=66.727 Prec@5=87.748 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=15:37 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.623 DataTime=0.399 Loss=1.331 Prec@1=66.691 Prec@5=87.700 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=15:37 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.623 DataTime=0.399 Loss=1.331 Prec@1=66.691 Prec@5=87.700 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=15:37 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.623 DataTime=0.399 Loss=1.331 Prec@1=66.691 Prec@5=87.700 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=15:38 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.621 DataTime=0.398 Loss=1.333 Prec@1=66.643 Prec@5=87.653 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=15:38 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.621 DataTime=0.398 Loss=1.333 Prec@1=66.643 Prec@5=87.653 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:38 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.621 DataTime=0.398 Loss=1.333 Prec@1=66.643 Prec@5=87.653 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:39 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.620 DataTime=0.397 Loss=1.336 Prec@1=66.585 Prec@5=87.606 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:39 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.620 DataTime=0.397 Loss=1.336 Prec@1=66.585 Prec@5=87.606 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:39 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.620 DataTime=0.397 Loss=1.336 Prec@1=66.585 Prec@5=87.606 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:40 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.396 Loss=1.338 Prec@1=66.570 Prec@5=87.567 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:40 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.396 Loss=1.338 Prec@1=66.570 Prec@5=87.567 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:40 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.396 Loss=1.338 Prec@1=66.570 Prec@5=87.567 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:41 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.395 Loss=1.340 Prec@1=66.568 Prec@5=87.557 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:41 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.395 Loss=1.340 Prec@1=66.568 Prec@5=87.557 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:41 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.619 DataTime=0.395 Loss=1.340 Prec@1=66.568 Prec@5=87.557 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:42 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.343 Prec@1=66.534 Prec@5=87.512 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:42 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.343 Prec@1=66.534 Prec@5=87.512 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:42 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.343 Prec@1=66.534 Prec@5=87.512 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:43 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.344 Prec@1=66.519 Prec@5=87.478 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:43 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.344 Prec@1=66.519 Prec@5=87.478 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:43 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.618 DataTime=0.394 Loss=1.344 Prec@1=66.519 Prec@5=87.478 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:44 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.483 Prec@5=87.461 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:44 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.483 Prec@5=87.461 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=15:44 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.483 Prec@5=87.461 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=15:45 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.482 Prec@5=87.444 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=15:45 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.482 Prec@5=87.444 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:45 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.346 Prec@1=66.482 Prec@5=87.444 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:47 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.348 Prec@1=66.435 Prec@5=87.417 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:47 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.348 Prec@1=66.435 Prec@5=87.417 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:47 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.617 DataTime=0.393 Loss=1.348 Prec@1=66.435 Prec@5=87.417 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:48 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.422 Prec@5=87.411 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:48 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.422 Prec@5=87.411 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=15:48 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.422 Prec@5=87.411 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=15:49 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.423 Prec@5=87.413 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=15:49 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.423 Prec@5=87.413 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:49 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.393 Loss=1.349 Prec@1=66.423 Prec@5=87.413 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:50 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.350 Prec@1=66.384 Prec@5=87.394 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:50 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.350 Prec@1=66.384 Prec@5=87.394 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:50 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.350 Prec@1=66.384 Prec@5=87.394 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:51 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.352 Prec@1=66.349 Prec@5=87.354 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:51 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.352 Prec@1=66.349 Prec@5=87.354 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:51 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.352 Prec@1=66.349 Prec@5=87.354 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:52 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.354 Prec@1=66.317 Prec@5=87.336 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:52 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.354 Prec@1=66.317 Prec@5=87.336 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:52 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.354 Prec@1=66.317 Prec@5=87.336 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:53 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.355 Prec@1=66.298 Prec@5=87.323 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:53 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.355 Prec@1=66.298 Prec@5=87.323 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:53 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.355 Prec@1=66.298 Prec@5=87.323 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:54 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.356 Prec@1=66.287 Prec@5=87.308 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:54 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.356 Prec@1=66.287 Prec@5=87.308 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=15:54 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.616 DataTime=0.392 Loss=1.356 Prec@1=66.287 Prec@5=87.308 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=15:55 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.357 Prec@1=66.272 Prec@5=87.300 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=15:55 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.357 Prec@1=66.272 Prec@5=87.300 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:55 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.357 Prec@1=66.272 Prec@5=87.300 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:56 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.358 Prec@1=66.245 Prec@5=87.281 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:56 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.358 Prec@1=66.245 Prec@5=87.281 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:56 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.358 Prec@1=66.245 Prec@5=87.281 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:57 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.359 Prec@1=66.235 Prec@5=87.277 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:57 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.359 Prec@1=66.235 Prec@5=87.277 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=15:57 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.392 Loss=1.359 Prec@1=66.235 Prec@5=87.277 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=15:58 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.391 Loss=1.360 Prec@1=66.217 Prec@5=87.259 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=15:58 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.391 Loss=1.360 Prec@1=66.217 Prec@5=87.259 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:58 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.391 Loss=1.360 Prec@1=66.217 Prec@5=87.259 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:58 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.391 Loss=1.360 Prec@1=66.217 Prec@5=87.259 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:58 IST=> training   100.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.615 DataTime=0.391 Loss=1.360 Prec@1=66.217 Prec@5=87.259 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=15:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:58 IST=> validation 0.00% of 1x98...Epoch=58/150 LR=0.06841 Time=6.794 Loss=1.308 Prec@1=66.406 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=15:58 IST=> validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=6.794 Loss=1.308 Prec@1=66.406 Prec@5=87.695 rate=5968.79 Hz, eta=0:00:00, total=0:00:00, wall=15:58 IST** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=6.794 Loss=1.308 Prec@1=66.406 Prec@5=87.695 rate=5968.79 Hz, eta=0:00:00, total=0:00:00, wall=15:58 IST** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=0.404 Loss=1.455 Prec@1=64.496 Prec@5=86.460 rate=5968.79 Hz, eta=0:00:00, total=0:00:00, wall=15:58 IST** validation 100.00% of 1x98...Epoch=58/150 LR=0.06841 Time=0.404 Loss=1.455 Prec@1=64.496 Prec@5=86.460 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=15:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> training   0.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.918 DataTime=4.638 Loss=1.327 Prec@1=65.820 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.918 DataTime=4.638 Loss=1.327 Prec@1=65.820 Prec@5=87.695 rate=5881.63 Hz, eta=0:00:00, total=0:00:00, wall=15:59 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=4.918 DataTime=4.638 Loss=1.327 Prec@1=65.820 Prec@5=87.695 rate=5881.63 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.652 DataTime=0.429 Loss=1.332 Prec@1=67.035 Prec@5=87.481 rate=5881.63 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.652 DataTime=0.429 Loss=1.332 Prec@1=67.035 Prec@5=87.481 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=16:00 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.652 DataTime=0.429 Loss=1.332 Prec@1=67.035 Prec@5=87.481 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=16:01 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.634 DataTime=0.409 Loss=1.329 Prec@1=67.019 Prec@5=87.634 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=16:01 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.634 DataTime=0.409 Loss=1.329 Prec@1=67.019 Prec@5=87.634 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:01 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.634 DataTime=0.409 Loss=1.329 Prec@1=67.019 Prec@5=87.634 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:02 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.628 DataTime=0.403 Loss=1.328 Prec@1=66.919 Prec@5=87.654 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:02 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.628 DataTime=0.403 Loss=1.328 Prec@1=66.919 Prec@5=87.654 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:02 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.628 DataTime=0.403 Loss=1.328 Prec@1=66.919 Prec@5=87.654 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:03 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.625 DataTime=0.400 Loss=1.328 Prec@1=66.943 Prec@5=87.647 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:03 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.625 DataTime=0.400 Loss=1.328 Prec@1=66.943 Prec@5=87.647 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:03 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.625 DataTime=0.400 Loss=1.328 Prec@1=66.943 Prec@5=87.647 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:04 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.623 DataTime=0.398 Loss=1.327 Prec@1=66.977 Prec@5=87.661 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:04 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.623 DataTime=0.398 Loss=1.327 Prec@1=66.977 Prec@5=87.661 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:04 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.623 DataTime=0.398 Loss=1.327 Prec@1=66.977 Prec@5=87.661 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:05 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.622 DataTime=0.396 Loss=1.329 Prec@1=66.948 Prec@5=87.649 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=16:05 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.622 DataTime=0.396 Loss=1.329 Prec@1=66.948 Prec@5=87.649 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=16:05 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.622 DataTime=0.396 Loss=1.329 Prec@1=66.948 Prec@5=87.649 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=16:06 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.396 Loss=1.329 Prec@1=66.875 Prec@5=87.663 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=16:06 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.396 Loss=1.329 Prec@1=66.875 Prec@5=87.663 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:06 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.396 Loss=1.329 Prec@1=66.875 Prec@5=87.663 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:07 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.395 Loss=1.331 Prec@1=66.804 Prec@5=87.638 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:07 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.395 Loss=1.331 Prec@1=66.804 Prec@5=87.638 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:07 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.620 DataTime=0.395 Loss=1.331 Prec@1=66.804 Prec@5=87.638 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:08 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.395 Loss=1.331 Prec@1=66.829 Prec@5=87.655 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:08 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.395 Loss=1.331 Prec@1=66.829 Prec@5=87.655 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=16:08 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.395 Loss=1.331 Prec@1=66.829 Prec@5=87.655 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=16:09 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.394 Loss=1.333 Prec@1=66.798 Prec@5=87.642 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=16:09 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.394 Loss=1.333 Prec@1=66.798 Prec@5=87.642 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:09 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.619 DataTime=0.394 Loss=1.333 Prec@1=66.798 Prec@5=87.642 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:10 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.394 Loss=1.335 Prec@1=66.749 Prec@5=87.630 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:10 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.394 Loss=1.335 Prec@1=66.749 Prec@5=87.630 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:10 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.394 Loss=1.335 Prec@1=66.749 Prec@5=87.630 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:11 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.393 Loss=1.338 Prec@1=66.675 Prec@5=87.605 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:11 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.393 Loss=1.338 Prec@1=66.675 Prec@5=87.605 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:11 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.618 DataTime=0.393 Loss=1.338 Prec@1=66.675 Prec@5=87.605 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:12 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.340 Prec@1=66.610 Prec@5=87.563 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:12 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.340 Prec@1=66.610 Prec@5=87.563 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:12 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.340 Prec@1=66.610 Prec@5=87.563 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:13 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.341 Prec@1=66.600 Prec@5=87.543 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:13 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.341 Prec@1=66.600 Prec@5=87.543 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:13 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.341 Prec@1=66.600 Prec@5=87.543 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:14 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.343 Prec@1=66.565 Prec@5=87.509 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=16:14 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.343 Prec@1=66.565 Prec@5=87.509 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:14 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.393 Loss=1.343 Prec@1=66.565 Prec@5=87.509 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:15 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.392 Loss=1.344 Prec@1=66.525 Prec@5=87.483 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:15 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.392 Loss=1.344 Prec@1=66.525 Prec@5=87.483 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:15 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.617 DataTime=0.392 Loss=1.344 Prec@1=66.525 Prec@5=87.483 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:16 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.344 Prec@1=66.520 Prec@5=87.472 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=16:16 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.344 Prec@1=66.520 Prec@5=87.472 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:16 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.344 Prec@1=66.520 Prec@5=87.472 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:17 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.345 Prec@1=66.502 Prec@5=87.465 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:17 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.345 Prec@1=66.502 Prec@5=87.465 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:17 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.345 Prec@1=66.502 Prec@5=87.465 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:18 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.347 Prec@1=66.467 Prec@5=87.450 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:18 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.347 Prec@1=66.467 Prec@5=87.450 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:18 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.347 Prec@1=66.467 Prec@5=87.450 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:19 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.348 Prec@1=66.454 Prec@5=87.440 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:19 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.348 Prec@1=66.454 Prec@5=87.440 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:19 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.348 Prec@1=66.454 Prec@5=87.440 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:20 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.349 Prec@1=66.424 Prec@5=87.416 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:20 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.349 Prec@1=66.424 Prec@5=87.416 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:20 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.349 Prec@1=66.424 Prec@5=87.416 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:21 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.407 Prec@5=87.403 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:21 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.407 Prec@5=87.403 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:21 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.407 Prec@5=87.403 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:22 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.403 Prec@5=87.395 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:22 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.403 Prec@5=87.395 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:22 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.403 Prec@5=87.395 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:23 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.404 Prec@5=87.390 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:23 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.404 Prec@5=87.390 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=16:23 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.392 Loss=1.351 Prec@1=66.404 Prec@5=87.390 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=16:24 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.391 Loss=1.353 Prec@1=66.389 Prec@5=87.374 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=16:24 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.391 Loss=1.353 Prec@1=66.389 Prec@5=87.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:24 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.391 Loss=1.353 Prec@1=66.389 Prec@5=87.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:24 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.391 Loss=1.353 Prec@1=66.388 Prec@5=87.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:24 IST=> training   100.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.616 DataTime=0.391 Loss=1.353 Prec@1=66.388 Prec@5=87.374 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=16:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:24 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:24 IST=> validation 0.00% of 1x98...Epoch=59/150 LR=0.06743 Time=6.787 Loss=1.400 Prec@1=65.625 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=16:24 IST=> validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.787 Loss=1.400 Prec@1=65.625 Prec@5=87.305 rate=5407.13 Hz, eta=0:00:00, total=0:00:00, wall=16:24 IST** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.787 Loss=1.400 Prec@1=65.625 Prec@5=87.305 rate=5407.13 Hz, eta=0:00:00, total=0:00:00, wall=16:25 IST** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=0.398 Loss=1.427 Prec@1=64.704 Prec@5=86.878 rate=5407.13 Hz, eta=0:00:00, total=0:00:00, wall=16:25 IST** validation 100.00% of 1x98...Epoch=59/150 LR=0.06743 Time=0.398 Loss=1.427 Prec@1=64.704 Prec@5=86.878 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=16:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:25 IST=> training   0.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.661 DataTime=5.376 Loss=1.236 Prec@1=67.969 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=16:25 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.661 DataTime=5.376 Loss=1.236 Prec@1=67.969 Prec@5=89.453 rate=7723.62 Hz, eta=0:00:00, total=0:00:00, wall=16:25 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.661 DataTime=5.376 Loss=1.236 Prec@1=67.969 Prec@5=89.453 rate=7723.62 Hz, eta=0:00:00, total=0:00:00, wall=16:26 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.657 DataTime=0.435 Loss=1.311 Prec@1=67.296 Prec@5=88.065 rate=7723.62 Hz, eta=0:00:00, total=0:00:00, wall=16:26 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.657 DataTime=0.435 Loss=1.311 Prec@1=67.296 Prec@5=88.065 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=16:26 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.657 DataTime=0.435 Loss=1.311 Prec@1=67.296 Prec@5=88.065 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=16:27 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.636 DataTime=0.413 Loss=1.309 Prec@1=67.291 Prec@5=88.013 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=16:27 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.636 DataTime=0.413 Loss=1.309 Prec@1=67.291 Prec@5=88.013 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=16:27 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.636 DataTime=0.413 Loss=1.309 Prec@1=67.291 Prec@5=88.013 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=16:28 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.629 DataTime=0.405 Loss=1.313 Prec@1=67.206 Prec@5=87.917 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=16:28 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.629 DataTime=0.405 Loss=1.313 Prec@1=67.206 Prec@5=87.917 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=16:28 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.629 DataTime=0.405 Loss=1.313 Prec@1=67.206 Prec@5=87.917 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=16:29 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.625 DataTime=0.401 Loss=1.316 Prec@1=67.188 Prec@5=87.861 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=16:29 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.625 DataTime=0.401 Loss=1.316 Prec@1=67.188 Prec@5=87.861 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=16:29 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.625 DataTime=0.401 Loss=1.316 Prec@1=67.188 Prec@5=87.861 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=16:30 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.623 DataTime=0.399 Loss=1.318 Prec@1=67.129 Prec@5=87.844 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=16:30 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.623 DataTime=0.399 Loss=1.318 Prec@1=67.129 Prec@5=87.844 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=16:30 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.623 DataTime=0.399 Loss=1.318 Prec@1=67.129 Prec@5=87.844 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=16:31 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.621 DataTime=0.397 Loss=1.321 Prec@1=67.007 Prec@5=87.825 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=16:31 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.621 DataTime=0.397 Loss=1.321 Prec@1=67.007 Prec@5=87.825 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=16:31 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.621 DataTime=0.397 Loss=1.321 Prec@1=67.007 Prec@5=87.825 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=16:32 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.396 Loss=1.322 Prec@1=66.957 Prec@5=87.804 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=16:32 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.396 Loss=1.322 Prec@1=66.957 Prec@5=87.804 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:32 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.396 Loss=1.322 Prec@1=66.957 Prec@5=87.804 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:33 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.395 Loss=1.324 Prec@1=66.939 Prec@5=87.787 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:33 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.395 Loss=1.324 Prec@1=66.939 Prec@5=87.787 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=16:33 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.620 DataTime=0.395 Loss=1.324 Prec@1=66.939 Prec@5=87.787 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=16:34 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.395 Loss=1.325 Prec@1=66.914 Prec@5=87.753 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=16:34 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.395 Loss=1.325 Prec@1=66.914 Prec@5=87.753 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=16:34 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.395 Loss=1.325 Prec@1=66.914 Prec@5=87.753 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=16:35 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.394 Loss=1.327 Prec@1=66.857 Prec@5=87.714 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=16:35 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.394 Loss=1.327 Prec@1=66.857 Prec@5=87.714 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=16:35 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.619 DataTime=0.394 Loss=1.327 Prec@1=66.857 Prec@5=87.714 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=16:36 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.394 Loss=1.330 Prec@1=66.797 Prec@5=87.685 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=16:36 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.394 Loss=1.330 Prec@1=66.797 Prec@5=87.685 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=16:36 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.394 Loss=1.330 Prec@1=66.797 Prec@5=87.685 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=16:37 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.393 Loss=1.332 Prec@1=66.742 Prec@5=87.661 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=16:37 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.393 Loss=1.332 Prec@1=66.742 Prec@5=87.661 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:37 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.618 DataTime=0.393 Loss=1.332 Prec@1=66.742 Prec@5=87.661 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:38 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.333 Prec@1=66.737 Prec@5=87.648 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=16:38 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.333 Prec@1=66.737 Prec@5=87.648 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=16:38 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.333 Prec@1=66.737 Prec@5=87.648 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=16:39 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.334 Prec@1=66.713 Prec@5=87.638 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=16:39 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.334 Prec@1=66.713 Prec@5=87.638 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=16:39 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.334 Prec@1=66.713 Prec@5=87.638 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=16:40 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.335 Prec@1=66.681 Prec@5=87.606 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=16:40 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.335 Prec@1=66.681 Prec@5=87.606 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:40 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.393 Loss=1.335 Prec@1=66.681 Prec@5=87.606 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:41 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.337 Prec@1=66.665 Prec@5=87.580 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:41 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.337 Prec@1=66.665 Prec@5=87.580 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:41 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.337 Prec@1=66.665 Prec@5=87.580 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:42 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.339 Prec@1=66.611 Prec@5=87.558 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:42 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.339 Prec@1=66.611 Prec@5=87.558 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:42 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.617 DataTime=0.392 Loss=1.339 Prec@1=66.611 Prec@5=87.558 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:43 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.340 Prec@1=66.594 Prec@5=87.538 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=16:43 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.340 Prec@1=66.594 Prec@5=87.538 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:43 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.340 Prec@1=66.594 Prec@5=87.538 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:44 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.341 Prec@1=66.583 Prec@5=87.514 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:44 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.341 Prec@1=66.583 Prec@5=87.514 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:44 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.341 Prec@1=66.583 Prec@5=87.514 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:45 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.574 Prec@5=87.501 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=16:45 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.574 Prec@5=87.501 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:45 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.574 Prec@5=87.501 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:46 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.561 Prec@5=87.489 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:46 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.561 Prec@5=87.489 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:46 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.342 Prec@1=66.561 Prec@5=87.489 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:47 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.343 Prec@1=66.546 Prec@5=87.481 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:47 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.343 Prec@1=66.546 Prec@5=87.481 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:47 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.392 Loss=1.343 Prec@1=66.546 Prec@5=87.481 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:49 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.344 Prec@1=66.532 Prec@5=87.467 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:49 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.344 Prec@1=66.532 Prec@5=87.467 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:49 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.344 Prec@1=66.532 Prec@5=87.467 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:50 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.345 Prec@1=66.517 Prec@5=87.454 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:50 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.345 Prec@1=66.517 Prec@5=87.454 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:50 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.345 Prec@1=66.517 Prec@5=87.454 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:51 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.346 Prec@1=66.500 Prec@5=87.440 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:51 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.346 Prec@1=66.500 Prec@5=87.440 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:51 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.346 Prec@1=66.500 Prec@5=87.440 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:51 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.346 Prec@1=66.499 Prec@5=87.440 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:51 IST=> training   100.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.616 DataTime=0.391 Loss=1.346 Prec@1=66.499 Prec@5=87.440 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=16:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> validation 0.00% of 1x98...Epoch=60/150 LR=0.06644 Time=5.597 Loss=1.353 Prec@1=65.625 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=5.597 Loss=1.353 Prec@1=65.625 Prec@5=87.109 rate=3516.16 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=5.597 Loss=1.353 Prec@1=65.625 Prec@5=87.109 rate=3516.16 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=0.401 Loss=1.391 Prec@1=65.560 Prec@5=87.320 rate=3516.16 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST** validation 100.00% of 1x98...Epoch=60/150 LR=0.06644 Time=0.401 Loss=1.391 Prec@1=65.560 Prec@5=87.320 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=16:51 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> training   0.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.753 DataTime=4.415 Loss=1.253 Prec@1=70.703 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=16:51 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.753 DataTime=4.415 Loss=1.253 Prec@1=70.703 Prec@5=88.086 rate=5049.28 Hz, eta=0:00:00, total=0:00:00, wall=16:51 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=4.753 DataTime=4.415 Loss=1.253 Prec@1=70.703 Prec@5=88.086 rate=5049.28 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.655 DataTime=0.427 Loss=1.303 Prec@1=67.338 Prec@5=88.034 rate=5049.28 Hz, eta=0:00:00, total=0:00:00, wall=16:52 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.655 DataTime=0.427 Loss=1.303 Prec@1=67.338 Prec@5=88.034 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=16:52 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.655 DataTime=0.427 Loss=1.303 Prec@1=67.338 Prec@5=88.034 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=16:53 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.634 DataTime=0.409 Loss=1.301 Prec@1=67.381 Prec@5=88.046 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=16:53 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.634 DataTime=0.409 Loss=1.301 Prec@1=67.381 Prec@5=88.046 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=16:53 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.634 DataTime=0.409 Loss=1.301 Prec@1=67.381 Prec@5=88.046 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=16:54 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.628 DataTime=0.402 Loss=1.300 Prec@1=67.391 Prec@5=88.033 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=16:54 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.628 DataTime=0.402 Loss=1.300 Prec@1=67.391 Prec@5=88.033 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:54 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.628 DataTime=0.402 Loss=1.300 Prec@1=67.391 Prec@5=88.033 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:55 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.624 DataTime=0.399 Loss=1.304 Prec@1=67.323 Prec@5=87.995 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=16:55 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.624 DataTime=0.399 Loss=1.304 Prec@1=67.323 Prec@5=87.995 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:55 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.624 DataTime=0.399 Loss=1.304 Prec@1=67.323 Prec@5=87.995 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:56 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.622 DataTime=0.397 Loss=1.308 Prec@1=67.217 Prec@5=87.948 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=16:56 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.622 DataTime=0.397 Loss=1.308 Prec@1=67.217 Prec@5=87.948 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=16:56 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.622 DataTime=0.397 Loss=1.308 Prec@1=67.217 Prec@5=87.948 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=16:58 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.621 DataTime=0.396 Loss=1.311 Prec@1=67.199 Prec@5=87.896 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=16:58 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.621 DataTime=0.396 Loss=1.311 Prec@1=67.199 Prec@5=87.896 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:58 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.621 DataTime=0.396 Loss=1.311 Prec@1=67.199 Prec@5=87.896 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:59 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.197 Prec@5=87.908 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=16:59 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.197 Prec@5=87.908 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=16:59 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.197 Prec@5=87.908 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=17:00 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.314 Prec@1=67.147 Prec@5=87.860 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=17:00 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.314 Prec@1=67.147 Prec@5=87.860 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:00 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.314 Prec@1=67.147 Prec@5=87.860 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:01 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.315 Prec@1=67.124 Prec@5=87.842 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:01 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.315 Prec@1=67.124 Prec@5=87.842 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:01 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.619 DataTime=0.394 Loss=1.315 Prec@1=67.124 Prec@5=87.842 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:02 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.080 Prec@5=87.817 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:02 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.080 Prec@5=87.817 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:02 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.080 Prec@5=87.817 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:03 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.034 Prec@5=87.790 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:03 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.034 Prec@5=87.790 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:03 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.034 Prec@5=87.790 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:04 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.321 Prec@1=67.004 Prec@5=87.767 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:04 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.321 Prec@1=67.004 Prec@5=87.767 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:04 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.393 Loss=1.321 Prec@1=67.004 Prec@5=87.767 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:05 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.323 Prec@1=66.966 Prec@5=87.752 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:05 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.323 Prec@1=66.966 Prec@5=87.752 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:05 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.323 Prec@1=66.966 Prec@5=87.752 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:06 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.324 Prec@1=66.928 Prec@5=87.750 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:06 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.324 Prec@1=66.928 Prec@5=87.750 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=17:06 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.618 DataTime=0.392 Loss=1.324 Prec@1=66.928 Prec@5=87.750 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=17:07 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.883 Prec@5=87.724 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=17:07 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.883 Prec@5=87.724 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:07 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.883 Prec@5=87.724 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:08 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.856 Prec@5=87.699 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:08 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.856 Prec@5=87.699 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=17:08 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.856 Prec@5=87.699 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=17:09 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.834 Prec@5=87.691 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=17:09 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.834 Prec@5=87.691 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:09 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.834 Prec@5=87.691 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:10 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.824 Prec@5=87.687 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:10 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.824 Prec@5=87.687 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=17:10 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.329 Prec@1=66.824 Prec@5=87.687 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=17:11 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.331 Prec@1=66.793 Prec@5=87.659 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=17:11 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.331 Prec@1=66.793 Prec@5=87.659 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:11 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.331 Prec@1=66.793 Prec@5=87.659 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:12 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.332 Prec@1=66.785 Prec@5=87.645 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:12 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.332 Prec@1=66.785 Prec@5=87.645 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=17:12 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.392 Loss=1.332 Prec@1=66.785 Prec@5=87.645 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=17:13 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.333 Prec@1=66.785 Prec@5=87.633 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=17:13 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.333 Prec@1=66.785 Prec@5=87.633 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:13 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.333 Prec@1=66.785 Prec@5=87.633 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:14 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.334 Prec@1=66.756 Prec@5=87.608 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:14 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.334 Prec@1=66.756 Prec@5=87.608 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:14 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.617 DataTime=0.391 Loss=1.334 Prec@1=66.756 Prec@5=87.608 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:15 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.335 Prec@1=66.731 Prec@5=87.599 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=17:15 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.335 Prec@1=66.731 Prec@5=87.599 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:15 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.335 Prec@1=66.731 Prec@5=87.599 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:16 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.336 Prec@1=66.723 Prec@5=87.593 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:16 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.336 Prec@1=66.723 Prec@5=87.593 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:16 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.336 Prec@1=66.723 Prec@5=87.593 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:17 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.337 Prec@1=66.699 Prec@5=87.569 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=17:17 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.337 Prec@1=66.699 Prec@5=87.569 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:17 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.337 Prec@1=66.699 Prec@5=87.569 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:17 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.337 Prec@1=66.698 Prec@5=87.568 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:17 IST=> training   100.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.616 DataTime=0.391 Loss=1.337 Prec@1=66.698 Prec@5=87.568 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=17:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST=> validation 0.00% of 1x98...Epoch=61/150 LR=0.06545 Time=6.648 Loss=1.449 Prec@1=64.062 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST=> validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.648 Loss=1.449 Prec@1=64.062 Prec@5=83.789 rate=3762.14 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.648 Loss=1.449 Prec@1=64.062 Prec@5=83.789 rate=3762.14 Hz, eta=0:00:00, total=0:00:00, wall=17:18 IST** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=0.407 Loss=1.444 Prec@1=64.694 Prec@5=86.474 rate=3762.14 Hz, eta=0:00:00, total=0:00:00, wall=17:18 IST** validation 100.00% of 1x98...Epoch=61/150 LR=0.06545 Time=0.407 Loss=1.444 Prec@1=64.694 Prec@5=86.474 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=17:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST=> training   0.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.372 DataTime=5.075 Loss=1.315 Prec@1=69.141 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.372 DataTime=5.075 Loss=1.315 Prec@1=69.141 Prec@5=87.305 rate=3153.51 Hz, eta=0:00:00, total=0:00:00, wall=17:18 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.372 DataTime=5.075 Loss=1.315 Prec@1=69.141 Prec@5=87.305 rate=3153.51 Hz, eta=0:00:00, total=0:00:00, wall=17:19 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.659 DataTime=0.434 Loss=1.294 Prec@1=67.727 Prec@5=88.181 rate=3153.51 Hz, eta=0:00:00, total=0:00:00, wall=17:19 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.659 DataTime=0.434 Loss=1.294 Prec@1=67.727 Prec@5=88.181 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:19 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.659 DataTime=0.434 Loss=1.294 Prec@1=67.727 Prec@5=88.181 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:20 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.637 DataTime=0.412 Loss=1.299 Prec@1=67.586 Prec@5=88.068 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=17:20 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.637 DataTime=0.412 Loss=1.299 Prec@1=67.586 Prec@5=88.068 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=17:20 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.637 DataTime=0.412 Loss=1.299 Prec@1=67.586 Prec@5=88.068 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=17:21 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.629 DataTime=0.405 Loss=1.300 Prec@1=67.576 Prec@5=88.096 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=17:21 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.629 DataTime=0.405 Loss=1.300 Prec@1=67.576 Prec@5=88.096 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:21 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.629 DataTime=0.405 Loss=1.300 Prec@1=67.576 Prec@5=88.096 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:22 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.626 DataTime=0.401 Loss=1.298 Prec@1=67.593 Prec@5=88.121 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:22 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.626 DataTime=0.401 Loss=1.298 Prec@1=67.593 Prec@5=88.121 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=17:22 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.626 DataTime=0.401 Loss=1.298 Prec@1=67.593 Prec@5=88.121 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=17:23 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.623 DataTime=0.399 Loss=1.301 Prec@1=67.531 Prec@5=88.054 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=17:23 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.623 DataTime=0.399 Loss=1.301 Prec@1=67.531 Prec@5=88.054 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=17:23 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.623 DataTime=0.399 Loss=1.301 Prec@1=67.531 Prec@5=88.054 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=17:24 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.622 DataTime=0.397 Loss=1.304 Prec@1=67.447 Prec@5=88.029 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=17:24 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.622 DataTime=0.397 Loss=1.304 Prec@1=67.447 Prec@5=88.029 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=17:24 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.622 DataTime=0.397 Loss=1.304 Prec@1=67.447 Prec@5=88.029 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=17:25 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.621 DataTime=0.396 Loss=1.306 Prec@1=67.399 Prec@5=88.006 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=17:25 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.621 DataTime=0.396 Loss=1.306 Prec@1=67.399 Prec@5=88.006 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=17:25 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.621 DataTime=0.396 Loss=1.306 Prec@1=67.399 Prec@5=88.006 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=17:26 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.317 Prec@5=87.963 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=17:26 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.317 Prec@5=87.963 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:26 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.310 Prec@1=67.317 Prec@5=87.963 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:27 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.311 Prec@1=67.275 Prec@5=87.948 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=17:27 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.311 Prec@1=67.275 Prec@5=87.948 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:27 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.620 DataTime=0.395 Loss=1.311 Prec@1=67.275 Prec@5=87.948 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:28 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.619 DataTime=0.394 Loss=1.313 Prec@1=67.240 Prec@5=87.929 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=17:28 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.619 DataTime=0.394 Loss=1.313 Prec@1=67.240 Prec@5=87.929 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:28 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.619 DataTime=0.394 Loss=1.313 Prec@1=67.240 Prec@5=87.929 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:29 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.394 Loss=1.316 Prec@1=67.186 Prec@5=87.890 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=17:29 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.394 Loss=1.316 Prec@1=67.186 Prec@5=87.890 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:29 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.394 Loss=1.316 Prec@1=67.186 Prec@5=87.890 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:30 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.142 Prec@5=87.868 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:30 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.142 Prec@5=87.868 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:30 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.317 Prec@1=67.142 Prec@5=87.868 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:31 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.318 Prec@1=67.140 Prec@5=87.873 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:31 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.318 Prec@1=67.140 Prec@5=87.873 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:31 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.318 Prec@1=67.140 Prec@5=87.873 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:32 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.118 Prec@5=87.858 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=17:32 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.118 Prec@5=87.858 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:32 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.618 DataTime=0.393 Loss=1.319 Prec@1=67.118 Prec@5=87.858 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:33 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.321 Prec@1=67.068 Prec@5=87.837 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:33 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.321 Prec@1=67.068 Prec@5=87.837 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:33 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.321 Prec@1=67.068 Prec@5=87.837 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:34 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.322 Prec@1=67.049 Prec@5=87.823 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=17:34 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.322 Prec@1=67.049 Prec@5=87.823 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:34 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.322 Prec@1=67.049 Prec@5=87.823 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:35 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.323 Prec@1=67.027 Prec@5=87.811 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:35 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.323 Prec@1=67.027 Prec@5=87.811 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:35 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.323 Prec@1=67.027 Prec@5=87.811 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:36 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.324 Prec@1=67.007 Prec@5=87.806 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=17:36 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.324 Prec@1=67.007 Prec@5=87.806 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=17:36 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.324 Prec@1=67.007 Prec@5=87.806 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=17:37 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.325 Prec@1=66.974 Prec@5=87.783 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=17:37 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.325 Prec@1=66.974 Prec@5=87.783 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:37 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.325 Prec@1=66.974 Prec@5=87.783 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:38 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.965 Prec@5=87.766 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=17:38 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.965 Prec@5=87.766 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:38 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.965 Prec@5=87.766 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:39 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.942 Prec@5=87.757 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=17:39 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.942 Prec@5=87.757 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:39 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.326 Prec@1=66.942 Prec@5=87.757 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:40 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.918 Prec@5=87.739 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:40 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.918 Prec@5=87.739 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:40 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.617 DataTime=0.392 Loss=1.328 Prec@1=66.918 Prec@5=87.739 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:41 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.329 Prec@1=66.894 Prec@5=87.721 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:41 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.329 Prec@1=66.894 Prec@5=87.721 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=17:41 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.329 Prec@1=66.894 Prec@5=87.721 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=17:42 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.331 Prec@1=66.861 Prec@5=87.698 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=17:42 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.331 Prec@1=66.861 Prec@5=87.698 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:42 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.331 Prec@1=66.861 Prec@5=87.698 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:43 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.332 Prec@1=66.827 Prec@5=87.682 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:43 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.332 Prec@1=66.827 Prec@5=87.682 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:43 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.332 Prec@1=66.827 Prec@5=87.682 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:43 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.332 Prec@1=66.826 Prec@5=87.681 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=17:43 IST=> training   100.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.616 DataTime=0.391 Loss=1.332 Prec@1=66.826 Prec@5=87.681 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=17:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> validation 0.00% of 1x98...Epoch=62/150 LR=0.06445 Time=6.946 Loss=1.503 Prec@1=63.086 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=6.946 Loss=1.503 Prec@1=63.086 Prec@5=84.766 rate=5523.33 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=6.946 Loss=1.503 Prec@1=63.086 Prec@5=84.766 rate=5523.33 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=0.402 Loss=1.456 Prec@1=64.400 Prec@5=86.466 rate=5523.33 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST** validation 100.00% of 1x98...Epoch=62/150 LR=0.06445 Time=0.402 Loss=1.456 Prec@1=64.400 Prec@5=86.466 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=17:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> training   0.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.114 DataTime=4.803 Loss=1.228 Prec@1=71.094 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.114 DataTime=4.803 Loss=1.228 Prec@1=71.094 Prec@5=87.695 rate=5499.91 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.114 DataTime=4.803 Loss=1.228 Prec@1=71.094 Prec@5=87.695 rate=5499.91 Hz, eta=0:00:00, total=0:00:00, wall=17:45 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.657 DataTime=0.431 Loss=1.271 Prec@1=68.170 Prec@5=88.374 rate=5499.91 Hz, eta=0:00:00, total=0:00:00, wall=17:45 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.657 DataTime=0.431 Loss=1.271 Prec@1=68.170 Prec@5=88.374 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:45 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.657 DataTime=0.431 Loss=1.271 Prec@1=68.170 Prec@5=88.374 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:46 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.636 DataTime=0.410 Loss=1.282 Prec@1=67.864 Prec@5=88.223 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:46 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.636 DataTime=0.410 Loss=1.282 Prec@1=67.864 Prec@5=88.223 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:46 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.636 DataTime=0.410 Loss=1.282 Prec@1=67.864 Prec@5=88.223 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:47 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.629 DataTime=0.402 Loss=1.286 Prec@1=67.763 Prec@5=88.188 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:47 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.629 DataTime=0.402 Loss=1.286 Prec@1=67.763 Prec@5=88.188 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=17:47 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.629 DataTime=0.402 Loss=1.286 Prec@1=67.763 Prec@5=88.188 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=17:48 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.625 DataTime=0.399 Loss=1.293 Prec@1=67.613 Prec@5=88.115 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=17:48 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.625 DataTime=0.399 Loss=1.293 Prec@1=67.613 Prec@5=88.115 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=17:48 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.625 DataTime=0.399 Loss=1.293 Prec@1=67.613 Prec@5=88.115 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=17:49 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.623 DataTime=0.397 Loss=1.295 Prec@1=67.573 Prec@5=88.101 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=17:49 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.623 DataTime=0.397 Loss=1.295 Prec@1=67.573 Prec@5=88.101 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=17:49 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.623 DataTime=0.397 Loss=1.295 Prec@1=67.573 Prec@5=88.101 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=17:50 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.622 DataTime=0.396 Loss=1.298 Prec@1=67.498 Prec@5=88.087 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=17:50 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.622 DataTime=0.396 Loss=1.298 Prec@1=67.498 Prec@5=88.087 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=17:50 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.622 DataTime=0.396 Loss=1.298 Prec@1=67.498 Prec@5=88.087 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=17:51 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.621 DataTime=0.395 Loss=1.301 Prec@1=67.460 Prec@5=88.060 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=17:51 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.621 DataTime=0.395 Loss=1.301 Prec@1=67.460 Prec@5=88.060 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=17:51 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.621 DataTime=0.395 Loss=1.301 Prec@1=67.460 Prec@5=88.060 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=17:52 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.620 DataTime=0.395 Loss=1.304 Prec@1=67.374 Prec@5=88.015 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=17:52 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.620 DataTime=0.395 Loss=1.304 Prec@1=67.374 Prec@5=88.015 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=17:52 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.620 DataTime=0.395 Loss=1.304 Prec@1=67.374 Prec@5=88.015 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=17:53 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.394 Loss=1.305 Prec@1=67.368 Prec@5=87.991 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=17:53 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.394 Loss=1.305 Prec@1=67.368 Prec@5=87.991 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=17:53 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.394 Loss=1.305 Prec@1=67.368 Prec@5=87.991 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=17:54 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.306 Prec@1=67.364 Prec@5=87.979 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=17:54 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.306 Prec@1=67.364 Prec@5=87.979 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=17:54 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.306 Prec@1=67.364 Prec@5=87.979 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=17:55 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.308 Prec@1=67.321 Prec@5=87.961 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=17:55 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.308 Prec@1=67.321 Prec@5=87.961 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:55 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.619 DataTime=0.393 Loss=1.308 Prec@1=67.321 Prec@5=87.961 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:57 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.393 Loss=1.310 Prec@1=67.278 Prec@5=87.940 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=17:57 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.393 Loss=1.310 Prec@1=67.278 Prec@5=87.940 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:57 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.393 Loss=1.310 Prec@1=67.278 Prec@5=87.940 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:58 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.312 Prec@1=67.243 Prec@5=87.914 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=17:58 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.312 Prec@1=67.243 Prec@5=87.914 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=17:58 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.312 Prec@1=67.243 Prec@5=87.914 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=17:59 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.314 Prec@1=67.210 Prec@5=87.885 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=17:59 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.314 Prec@1=67.210 Prec@5=87.885 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=17:59 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.618 DataTime=0.392 Loss=1.314 Prec@1=67.210 Prec@5=87.885 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=18:00 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.174 Prec@5=87.850 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=18:00 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.174 Prec@5=87.850 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:00 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.174 Prec@5=87.850 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:01 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.147 Prec@5=87.848 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:01 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.147 Prec@5=87.848 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=18:01 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.147 Prec@5=87.848 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=18:02 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.124 Prec@5=87.834 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=18:02 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.124 Prec@5=87.834 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:02 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.124 Prec@5=87.834 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:03 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.106 Prec@5=87.823 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:03 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.106 Prec@5=87.823 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=18:03 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.392 Loss=1.318 Prec@1=67.106 Prec@5=87.823 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=18:04 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.319 Prec@1=67.094 Prec@5=87.817 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=18:04 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.319 Prec@1=67.094 Prec@5=87.817 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:04 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.319 Prec@1=67.094 Prec@5=87.817 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:05 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.321 Prec@1=67.054 Prec@5=87.797 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:05 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.321 Prec@1=67.054 Prec@5=87.797 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=18:05 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.321 Prec@1=67.054 Prec@5=87.797 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=18:06 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.322 Prec@1=67.049 Prec@5=87.780 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=18:06 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.322 Prec@1=67.049 Prec@5=87.780 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:06 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.322 Prec@1=67.049 Prec@5=87.780 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:07 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.323 Prec@1=67.010 Prec@5=87.764 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:07 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.323 Prec@1=67.010 Prec@5=87.764 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=18:07 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.617 DataTime=0.391 Loss=1.323 Prec@1=67.010 Prec@5=87.764 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=18:08 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.324 Prec@1=66.988 Prec@5=87.749 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=18:08 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.324 Prec@1=66.988 Prec@5=87.749 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:08 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.324 Prec@1=66.988 Prec@5=87.749 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:09 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.325 Prec@1=66.970 Prec@5=87.740 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:09 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.325 Prec@1=66.970 Prec@5=87.740 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:09 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.325 Prec@1=66.970 Prec@5=87.740 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:10 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.326 Prec@1=66.960 Prec@5=87.730 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:10 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.326 Prec@1=66.960 Prec@5=87.730 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:10 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.326 Prec@1=66.960 Prec@5=87.730 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:10 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.326 Prec@1=66.959 Prec@5=87.729 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:10 IST=> training   100.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.616 DataTime=0.391 Loss=1.326 Prec@1=66.959 Prec@5=87.729 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=18:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:10 IST=> validation 0.00% of 1x98...Epoch=63/150 LR=0.06345 Time=7.467 Loss=1.496 Prec@1=63.086 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=18:10 IST=> validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=7.467 Loss=1.496 Prec@1=63.086 Prec@5=84.961 rate=7122.76 Hz, eta=0:00:00, total=0:00:00, wall=18:10 IST** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=7.467 Loss=1.496 Prec@1=63.086 Prec@5=84.961 rate=7122.76 Hz, eta=0:00:00, total=0:00:00, wall=18:11 IST** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=0.425 Loss=1.451 Prec@1=64.358 Prec@5=86.400 rate=7122.76 Hz, eta=0:00:00, total=0:00:00, wall=18:11 IST** validation 100.00% of 1x98...Epoch=63/150 LR=0.06345 Time=0.425 Loss=1.451 Prec@1=64.358 Prec@5=86.400 rate=2.87 Hz, eta=0:00:00, total=0:00:34, wall=18:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:11 IST=> training   0.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.340 DataTime=5.070 Loss=1.218 Prec@1=68.164 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=18:11 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.340 DataTime=5.070 Loss=1.218 Prec@1=68.164 Prec@5=88.086 rate=5946.40 Hz, eta=0:00:00, total=0:00:00, wall=18:11 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.340 DataTime=5.070 Loss=1.218 Prec@1=68.164 Prec@5=88.086 rate=5946.40 Hz, eta=0:00:00, total=0:00:00, wall=18:12 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.659 DataTime=0.433 Loss=1.273 Prec@1=68.218 Prec@5=88.237 rate=5946.40 Hz, eta=0:00:00, total=0:00:00, wall=18:12 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.659 DataTime=0.433 Loss=1.273 Prec@1=68.218 Prec@5=88.237 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:12 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.659 DataTime=0.433 Loss=1.273 Prec@1=68.218 Prec@5=88.237 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:13 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.636 DataTime=0.412 Loss=1.275 Prec@1=68.055 Prec@5=88.287 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:13 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.636 DataTime=0.412 Loss=1.275 Prec@1=68.055 Prec@5=88.287 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:13 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.636 DataTime=0.412 Loss=1.275 Prec@1=68.055 Prec@5=88.287 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:14 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.629 DataTime=0.404 Loss=1.280 Prec@1=67.904 Prec@5=88.248 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:14 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.629 DataTime=0.404 Loss=1.280 Prec@1=67.904 Prec@5=88.248 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=18:14 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.629 DataTime=0.404 Loss=1.280 Prec@1=67.904 Prec@5=88.248 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=18:15 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.626 DataTime=0.401 Loss=1.285 Prec@1=67.790 Prec@5=88.193 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=18:15 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.626 DataTime=0.401 Loss=1.285 Prec@1=67.790 Prec@5=88.193 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=18:15 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.626 DataTime=0.401 Loss=1.285 Prec@1=67.790 Prec@5=88.193 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=18:16 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.624 DataTime=0.399 Loss=1.289 Prec@1=67.688 Prec@5=88.146 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=18:16 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.624 DataTime=0.399 Loss=1.289 Prec@1=67.688 Prec@5=88.146 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=18:16 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.624 DataTime=0.399 Loss=1.289 Prec@1=67.688 Prec@5=88.146 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=18:17 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.622 DataTime=0.397 Loss=1.290 Prec@1=67.672 Prec@5=88.138 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=18:17 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.622 DataTime=0.397 Loss=1.290 Prec@1=67.672 Prec@5=88.138 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=18:17 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.622 DataTime=0.397 Loss=1.290 Prec@1=67.672 Prec@5=88.138 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=18:18 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.621 DataTime=0.396 Loss=1.294 Prec@1=67.586 Prec@5=88.097 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=18:18 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.621 DataTime=0.396 Loss=1.294 Prec@1=67.586 Prec@5=88.097 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:18 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.621 DataTime=0.396 Loss=1.294 Prec@1=67.586 Prec@5=88.097 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:19 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.294 Prec@1=67.611 Prec@5=88.087 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=18:19 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.294 Prec@1=67.611 Prec@5=88.087 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:19 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.294 Prec@1=67.611 Prec@5=88.087 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:20 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.295 Prec@1=67.588 Prec@5=88.069 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=18:20 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.295 Prec@1=67.588 Prec@5=88.069 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=18:20 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.620 DataTime=0.395 Loss=1.295 Prec@1=67.588 Prec@5=88.069 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=18:21 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.297 Prec@1=67.553 Prec@5=88.041 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=18:21 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.297 Prec@1=67.553 Prec@5=88.041 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:21 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.297 Prec@1=67.553 Prec@5=88.041 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:22 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.299 Prec@1=67.524 Prec@5=88.005 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:22 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.299 Prec@1=67.524 Prec@5=88.005 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:22 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.619 DataTime=0.394 Loss=1.299 Prec@1=67.524 Prec@5=88.005 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:23 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.468 Prec@5=87.978 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:23 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.468 Prec@5=87.978 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:23 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.468 Prec@5=87.978 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:24 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.304 Prec@1=67.420 Prec@5=87.942 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:24 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.304 Prec@1=67.420 Prec@5=87.942 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:24 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.304 Prec@1=67.420 Prec@5=87.942 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:25 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.305 Prec@1=67.384 Prec@5=87.930 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:25 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.305 Prec@1=67.384 Prec@5=87.930 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=18:25 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.618 DataTime=0.393 Loss=1.305 Prec@1=67.384 Prec@5=87.930 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=18:26 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.393 Loss=1.307 Prec@1=67.320 Prec@5=87.899 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=18:26 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.393 Loss=1.307 Prec@1=67.320 Prec@5=87.899 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:26 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.393 Loss=1.307 Prec@1=67.320 Prec@5=87.899 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:27 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.308 Prec@1=67.297 Prec@5=87.891 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=18:27 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.308 Prec@1=67.297 Prec@5=87.891 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:27 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.308 Prec@1=67.297 Prec@5=87.891 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:28 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.309 Prec@1=67.287 Prec@5=87.885 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:28 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.309 Prec@1=67.287 Prec@5=87.885 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:28 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.309 Prec@1=67.287 Prec@5=87.885 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:29 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.870 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:29 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.870 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=18:29 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.870 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=18:30 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.248 Prec@5=87.861 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=18:30 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.248 Prec@5=87.861 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:30 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.248 Prec@5=87.861 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:31 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.234 Prec@5=87.855 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=18:31 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.234 Prec@5=87.855 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:31 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.312 Prec@1=67.234 Prec@5=87.855 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:32 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.314 Prec@1=67.207 Prec@5=87.842 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:32 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.314 Prec@1=67.207 Prec@5=87.842 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:32 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.314 Prec@1=67.207 Prec@5=87.842 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:33 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.181 Prec@5=87.832 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=18:33 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.181 Prec@5=87.832 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:33 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.315 Prec@1=67.181 Prec@5=87.832 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:34 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.157 Prec@5=87.817 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=18:34 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.157 Prec@5=87.817 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:34 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.617 DataTime=0.392 Loss=1.316 Prec@1=67.157 Prec@5=87.817 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:35 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.317 Prec@1=67.126 Prec@5=87.810 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=18:35 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.317 Prec@1=67.126 Prec@5=87.810 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:35 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.317 Prec@1=67.126 Prec@5=87.810 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:36 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.318 Prec@1=67.106 Prec@5=87.795 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=18:36 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.318 Prec@1=67.106 Prec@5=87.795 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:36 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.318 Prec@1=67.106 Prec@5=87.795 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:36 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.318 Prec@1=67.103 Prec@5=87.793 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=18:36 IST=> training   100.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.616 DataTime=0.391 Loss=1.318 Prec@1=67.103 Prec@5=87.793 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=18:36 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:36 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:36 IST=> validation 0.00% of 1x98...Epoch=64/150 LR=0.06243 Time=6.134 Loss=1.532 Prec@1=61.328 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=18:36 IST=> validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.134 Loss=1.532 Prec@1=61.328 Prec@5=84.570 rate=5121.11 Hz, eta=0:00:00, total=0:00:00, wall=18:36 IST** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.134 Loss=1.532 Prec@1=61.328 Prec@5=84.570 rate=5121.11 Hz, eta=0:00:00, total=0:00:00, wall=18:37 IST** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=0.404 Loss=1.387 Prec@1=65.838 Prec@5=87.206 rate=5121.11 Hz, eta=0:00:00, total=0:00:00, wall=18:37 IST** validation 100.00% of 1x98...Epoch=64/150 LR=0.06243 Time=0.404 Loss=1.387 Prec@1=65.838 Prec@5=87.206 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=18:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:37 IST=> training   0.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=6.042 DataTime=5.750 Loss=1.235 Prec@1=70.117 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=18:37 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=6.042 DataTime=5.750 Loss=1.235 Prec@1=70.117 Prec@5=88.672 rate=8589.51 Hz, eta=0:00:00, total=0:00:00, wall=18:37 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=6.042 DataTime=5.750 Loss=1.235 Prec@1=70.117 Prec@5=88.672 rate=8589.51 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.663 DataTime=0.439 Loss=1.279 Prec@1=68.112 Prec@5=88.258 rate=8589.51 Hz, eta=0:00:00, total=0:00:00, wall=18:38 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.663 DataTime=0.439 Loss=1.279 Prec@1=68.112 Prec@5=88.258 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=18:38 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.663 DataTime=0.439 Loss=1.279 Prec@1=68.112 Prec@5=88.258 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=18:39 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.639 DataTime=0.415 Loss=1.273 Prec@1=68.126 Prec@5=88.367 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=18:39 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.639 DataTime=0.415 Loss=1.273 Prec@1=68.126 Prec@5=88.367 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:39 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.639 DataTime=0.415 Loss=1.273 Prec@1=68.126 Prec@5=88.367 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:40 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.631 DataTime=0.407 Loss=1.278 Prec@1=67.998 Prec@5=88.277 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=18:40 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.631 DataTime=0.407 Loss=1.278 Prec@1=67.998 Prec@5=88.277 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=18:40 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.631 DataTime=0.407 Loss=1.278 Prec@1=67.998 Prec@5=88.277 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=18:41 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.627 DataTime=0.402 Loss=1.282 Prec@1=67.914 Prec@5=88.252 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=18:41 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.627 DataTime=0.402 Loss=1.282 Prec@1=67.914 Prec@5=88.252 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=18:41 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.627 DataTime=0.402 Loss=1.282 Prec@1=67.914 Prec@5=88.252 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=18:42 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.624 DataTime=0.400 Loss=1.284 Prec@1=67.814 Prec@5=88.247 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=18:42 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.624 DataTime=0.400 Loss=1.284 Prec@1=67.814 Prec@5=88.247 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:42 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.624 DataTime=0.400 Loss=1.284 Prec@1=67.814 Prec@5=88.247 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:43 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.623 DataTime=0.398 Loss=1.283 Prec@1=67.849 Prec@5=88.270 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=18:43 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.623 DataTime=0.398 Loss=1.283 Prec@1=67.849 Prec@5=88.270 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:43 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.623 DataTime=0.398 Loss=1.283 Prec@1=67.849 Prec@5=88.270 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:44 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.622 DataTime=0.397 Loss=1.286 Prec@1=67.751 Prec@5=88.253 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=18:44 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.622 DataTime=0.397 Loss=1.286 Prec@1=67.751 Prec@5=88.253 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:44 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.622 DataTime=0.397 Loss=1.286 Prec@1=67.751 Prec@5=88.253 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:45 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.621 DataTime=0.396 Loss=1.289 Prec@1=67.665 Prec@5=88.218 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=18:45 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.621 DataTime=0.396 Loss=1.289 Prec@1=67.665 Prec@5=88.218 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=18:45 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.621 DataTime=0.396 Loss=1.289 Prec@1=67.665 Prec@5=88.218 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=18:46 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.396 Loss=1.291 Prec@1=67.619 Prec@5=88.203 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=18:46 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.396 Loss=1.291 Prec@1=67.619 Prec@5=88.203 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:46 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.396 Loss=1.291 Prec@1=67.619 Prec@5=88.203 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:47 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.395 Loss=1.292 Prec@1=67.626 Prec@5=88.180 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=18:47 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.395 Loss=1.292 Prec@1=67.626 Prec@5=88.180 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:47 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.620 DataTime=0.395 Loss=1.292 Prec@1=67.626 Prec@5=88.180 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:48 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.395 Loss=1.293 Prec@1=67.622 Prec@5=88.167 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=18:48 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.395 Loss=1.293 Prec@1=67.622 Prec@5=88.167 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:48 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.395 Loss=1.293 Prec@1=67.622 Prec@5=88.167 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:49 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.394 Loss=1.294 Prec@1=67.568 Prec@5=88.155 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=18:49 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.394 Loss=1.294 Prec@1=67.568 Prec@5=88.155 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:49 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.619 DataTime=0.394 Loss=1.294 Prec@1=67.568 Prec@5=88.155 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:50 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.296 Prec@1=67.538 Prec@5=88.132 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=18:50 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.296 Prec@1=67.538 Prec@5=88.132 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:50 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.296 Prec@1=67.538 Prec@5=88.132 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:51 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.298 Prec@1=67.493 Prec@5=88.102 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=18:51 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.298 Prec@1=67.493 Prec@5=88.102 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:51 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.394 Loss=1.298 Prec@1=67.493 Prec@5=88.102 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:52 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.298 Prec@1=67.488 Prec@5=88.100 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=18:52 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.298 Prec@1=67.488 Prec@5=88.100 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:52 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.298 Prec@1=67.488 Prec@5=88.100 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:53 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.441 Prec@5=88.068 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=18:53 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.441 Prec@5=88.068 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:53 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.618 DataTime=0.393 Loss=1.301 Prec@1=67.441 Prec@5=88.068 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:55 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.302 Prec@1=67.419 Prec@5=88.038 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=18:55 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.302 Prec@1=67.419 Prec@5=88.038 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:55 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.302 Prec@1=67.419 Prec@5=88.038 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:56 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.303 Prec@1=67.407 Prec@5=88.031 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=18:56 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.303 Prec@1=67.407 Prec@5=88.031 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:56 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.393 Loss=1.303 Prec@1=67.407 Prec@5=88.031 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:57 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.401 Prec@5=88.025 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=18:57 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.401 Prec@5=88.025 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:57 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.401 Prec@5=88.025 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:58 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.389 Prec@5=88.019 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:58 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.389 Prec@5=88.019 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:58 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.389 Prec@5=88.019 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:59 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.305 Prec@1=67.373 Prec@5=88.003 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=18:59 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.305 Prec@1=67.373 Prec@5=88.003 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:59 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.617 DataTime=0.392 Loss=1.305 Prec@1=67.373 Prec@5=88.003 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:00 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.306 Prec@1=67.343 Prec@5=87.983 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:00 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.306 Prec@1=67.343 Prec@5=87.983 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:00 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.306 Prec@1=67.343 Prec@5=87.983 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:01 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.308 Prec@1=67.309 Prec@5=87.963 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:01 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.308 Prec@1=67.309 Prec@5=87.963 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:01 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.308 Prec@1=67.309 Prec@5=87.963 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:02 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.309 Prec@1=67.284 Prec@5=87.945 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:02 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.309 Prec@1=67.284 Prec@5=87.945 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:02 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.309 Prec@1=67.284 Prec@5=87.945 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:03 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.930 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:03 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.930 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=19:03 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.930 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=19:03 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.930 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=19:03 IST=> training   100.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.616 DataTime=0.392 Loss=1.310 Prec@1=67.265 Prec@5=87.930 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=19:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:03 IST=> validation 0.00% of 1x98...Epoch=65/150 LR=0.06142 Time=7.155 Loss=1.308 Prec@1=67.383 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=19:03 IST=> validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.155 Loss=1.308 Prec@1=67.383 Prec@5=86.328 rate=6696.93 Hz, eta=0:00:00, total=0:00:00, wall=19:03 IST** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.155 Loss=1.308 Prec@1=67.383 Prec@5=86.328 rate=6696.93 Hz, eta=0:00:00, total=0:00:00, wall=19:03 IST** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=0.409 Loss=1.450 Prec@1=64.588 Prec@5=86.572 rate=6696.93 Hz, eta=0:00:00, total=0:00:00, wall=19:03 IST** validation 100.00% of 1x98...Epoch=65/150 LR=0.06142 Time=0.409 Loss=1.450 Prec@1=64.588 Prec@5=86.572 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=19:03 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:04 IST=> training   0.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.733 DataTime=4.350 Loss=1.207 Prec@1=69.922 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=19:04 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.733 DataTime=4.350 Loss=1.207 Prec@1=69.922 Prec@5=88.281 rate=7859.04 Hz, eta=0:00:00, total=0:00:00, wall=19:04 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=4.733 DataTime=4.350 Loss=1.207 Prec@1=69.922 Prec@5=88.281 rate=7859.04 Hz, eta=0:00:00, total=0:00:00, wall=19:05 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.658 DataTime=0.431 Loss=1.252 Prec@1=68.427 Prec@5=88.585 rate=7859.04 Hz, eta=0:00:00, total=0:00:00, wall=19:05 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.658 DataTime=0.431 Loss=1.252 Prec@1=68.427 Prec@5=88.585 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=19:05 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.658 DataTime=0.431 Loss=1.252 Prec@1=68.427 Prec@5=88.585 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=19:06 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.637 DataTime=0.410 Loss=1.262 Prec@1=68.151 Prec@5=88.488 rate=1.64 Hz, eta=0:24:28, total=0:01:01, wall=19:06 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.637 DataTime=0.410 Loss=1.262 Prec@1=68.151 Prec@5=88.488 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=19:06 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.637 DataTime=0.410 Loss=1.262 Prec@1=68.151 Prec@5=88.488 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=19:07 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.628 DataTime=0.402 Loss=1.270 Prec@1=68.092 Prec@5=88.429 rate=1.63 Hz, eta=0:23:30, total=0:02:03, wall=19:07 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.628 DataTime=0.402 Loss=1.270 Prec@1=68.092 Prec@5=88.429 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=19:07 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.628 DataTime=0.402 Loss=1.270 Prec@1=68.092 Prec@5=88.429 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=19:08 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.625 DataTime=0.399 Loss=1.269 Prec@1=68.075 Prec@5=88.465 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=19:08 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.625 DataTime=0.399 Loss=1.269 Prec@1=68.075 Prec@5=88.465 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=19:08 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.625 DataTime=0.399 Loss=1.269 Prec@1=68.075 Prec@5=88.465 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=19:09 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.623 DataTime=0.397 Loss=1.270 Prec@1=68.081 Prec@5=88.458 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=19:09 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.623 DataTime=0.397 Loss=1.270 Prec@1=68.081 Prec@5=88.458 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=19:09 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.623 DataTime=0.397 Loss=1.270 Prec@1=68.081 Prec@5=88.458 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=19:10 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.622 DataTime=0.396 Loss=1.273 Prec@1=68.041 Prec@5=88.437 rate=1.63 Hz, eta=0:20:28, total=0:05:07, wall=19:10 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.622 DataTime=0.396 Loss=1.273 Prec@1=68.041 Prec@5=88.437 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=19:10 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.622 DataTime=0.396 Loss=1.273 Prec@1=68.041 Prec@5=88.437 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=19:11 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.621 DataTime=0.395 Loss=1.275 Prec@1=68.017 Prec@5=88.389 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=19:11 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.621 DataTime=0.395 Loss=1.275 Prec@1=68.017 Prec@5=88.389 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=19:11 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.621 DataTime=0.395 Loss=1.275 Prec@1=68.017 Prec@5=88.389 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=19:12 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.620 DataTime=0.395 Loss=1.277 Prec@1=67.940 Prec@5=88.340 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=19:12 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.620 DataTime=0.395 Loss=1.277 Prec@1=67.940 Prec@5=88.340 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=19:12 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.620 DataTime=0.395 Loss=1.277 Prec@1=67.940 Prec@5=88.340 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=19:13 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.394 Loss=1.280 Prec@1=67.895 Prec@5=88.309 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=19:13 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.394 Loss=1.280 Prec@1=67.895 Prec@5=88.309 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=19:13 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.394 Loss=1.280 Prec@1=67.895 Prec@5=88.309 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=19:14 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.393 Loss=1.281 Prec@1=67.853 Prec@5=88.285 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=19:14 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.393 Loss=1.281 Prec@1=67.853 Prec@5=88.285 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=19:14 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.619 DataTime=0.393 Loss=1.281 Prec@1=67.853 Prec@5=88.285 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=19:15 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.285 Prec@1=67.785 Prec@5=88.239 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=19:15 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.285 Prec@1=67.785 Prec@5=88.239 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=19:15 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.285 Prec@1=67.785 Prec@5=88.239 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=19:16 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.749 Prec@5=88.215 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=19:16 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.749 Prec@5=88.215 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:16 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.749 Prec@5=88.215 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:17 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.714 Prec@5=88.197 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:17 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.714 Prec@5=88.197 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=19:17 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.714 Prec@5=88.197 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=19:18 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.392 Loss=1.291 Prec@1=67.680 Prec@5=88.169 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=19:18 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.392 Loss=1.291 Prec@1=67.680 Prec@5=88.169 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:18 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.618 DataTime=0.392 Loss=1.291 Prec@1=67.680 Prec@5=88.169 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:19 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.632 Prec@5=88.141 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:19 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.632 Prec@5=88.141 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:19 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.632 Prec@5=88.141 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:20 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.604 Prec@5=88.117 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:20 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.604 Prec@5=88.117 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=19:20 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.604 Prec@5=88.117 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=19:21 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.297 Prec@1=67.567 Prec@5=88.095 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=19:21 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.297 Prec@1=67.567 Prec@5=88.095 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=19:21 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.297 Prec@1=67.567 Prec@5=88.095 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=19:22 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.298 Prec@1=67.545 Prec@5=88.082 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=19:22 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.298 Prec@1=67.545 Prec@5=88.082 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=19:22 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.298 Prec@1=67.545 Prec@5=88.082 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=19:23 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.299 Prec@1=67.516 Prec@5=88.059 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=19:23 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.299 Prec@1=67.516 Prec@5=88.059 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:23 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.299 Prec@1=67.516 Prec@5=88.059 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:24 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.300 Prec@1=67.490 Prec@5=88.042 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=19:24 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.300 Prec@1=67.490 Prec@5=88.042 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=19:24 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.300 Prec@1=67.490 Prec@5=88.042 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=19:25 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.301 Prec@1=67.464 Prec@5=88.029 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=19:25 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.301 Prec@1=67.464 Prec@5=88.029 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=19:25 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.301 Prec@1=67.464 Prec@5=88.029 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=19:26 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.431 Prec@5=88.012 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=19:26 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.431 Prec@5=88.012 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=19:26 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.431 Prec@5=88.012 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=19:27 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.439 Prec@5=88.016 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=19:27 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.439 Prec@5=88.016 rate=1.63 Hz, eta=0:02:04, total=0:23:34, wall=19:27 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.439 Prec@5=88.016 rate=1.63 Hz, eta=0:02:04, total=0:23:34, wall=19:28 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.444 Prec@5=88.019 rate=1.63 Hz, eta=0:02:04, total=0:23:34, wall=19:28 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.444 Prec@5=88.019 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=19:28 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.303 Prec@1=67.444 Prec@5=88.019 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=19:29 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.304 Prec@1=67.418 Prec@5=88.008 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=19:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.304 Prec@1=67.418 Prec@5=88.008 rate=1.63 Hz, eta=0:00:01, total=0:25:37, wall=19:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.617 DataTime=0.392 Loss=1.304 Prec@1=67.418 Prec@5=88.008 rate=1.63 Hz, eta=0:00:01, total=0:25:37, wall=19:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.616 DataTime=0.392 Loss=1.304 Prec@1=67.416 Prec@5=88.008 rate=1.63 Hz, eta=0:00:01, total=0:25:37, wall=19:29 IST=> training   100.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.616 DataTime=0.392 Loss=1.304 Prec@1=67.416 Prec@5=88.008 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=19:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:29 IST=> validation 0.00% of 1x98...Epoch=66/150 LR=0.06040 Time=6.823 Loss=1.374 Prec@1=64.844 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=19:29 IST=> validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.823 Loss=1.374 Prec@1=64.844 Prec@5=88.672 rate=3013.84 Hz, eta=0:00:00, total=0:00:00, wall=19:29 IST** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.823 Loss=1.374 Prec@1=64.844 Prec@5=88.672 rate=3013.84 Hz, eta=0:00:00, total=0:00:00, wall=19:30 IST** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=0.401 Loss=1.438 Prec@1=64.516 Prec@5=86.590 rate=3013.84 Hz, eta=0:00:00, total=0:00:00, wall=19:30 IST** validation 100.00% of 1x98...Epoch=66/150 LR=0.06040 Time=0.401 Loss=1.438 Prec@1=64.516 Prec@5=86.590 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=19:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:30 IST=> training   0.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.904 DataTime=5.682 Loss=1.426 Prec@1=63.281 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=19:30 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.904 DataTime=5.682 Loss=1.426 Prec@1=63.281 Prec@5=85.938 rate=7467.14 Hz, eta=0:00:00, total=0:00:00, wall=19:30 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.904 DataTime=5.682 Loss=1.426 Prec@1=63.281 Prec@5=85.938 rate=7467.14 Hz, eta=0:00:00, total=0:00:00, wall=19:31 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.665 DataTime=0.439 Loss=1.270 Prec@1=67.998 Prec@5=88.345 rate=7467.14 Hz, eta=0:00:00, total=0:00:00, wall=19:31 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.665 DataTime=0.439 Loss=1.270 Prec@1=67.998 Prec@5=88.345 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:31 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.665 DataTime=0.439 Loss=1.270 Prec@1=67.998 Prec@5=88.345 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:32 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.639 DataTime=0.414 Loss=1.268 Prec@1=68.166 Prec@5=88.437 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=19:32 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.639 DataTime=0.414 Loss=1.268 Prec@1=68.166 Prec@5=88.437 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=19:32 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.639 DataTime=0.414 Loss=1.268 Prec@1=68.166 Prec@5=88.437 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=19:33 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.631 DataTime=0.407 Loss=1.268 Prec@1=68.154 Prec@5=88.464 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=19:33 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.631 DataTime=0.407 Loss=1.268 Prec@1=68.154 Prec@5=88.464 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=19:33 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.631 DataTime=0.407 Loss=1.268 Prec@1=68.154 Prec@5=88.464 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=19:34 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.627 DataTime=0.402 Loss=1.271 Prec@1=68.063 Prec@5=88.423 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=19:34 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.627 DataTime=0.402 Loss=1.271 Prec@1=68.063 Prec@5=88.423 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=19:34 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.627 DataTime=0.402 Loss=1.271 Prec@1=68.063 Prec@5=88.423 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=19:35 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.624 DataTime=0.400 Loss=1.273 Prec@1=68.070 Prec@5=88.411 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=19:35 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.624 DataTime=0.400 Loss=1.273 Prec@1=68.070 Prec@5=88.411 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=19:35 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.624 DataTime=0.400 Loss=1.273 Prec@1=68.070 Prec@5=88.411 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=19:36 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.623 DataTime=0.398 Loss=1.276 Prec@1=68.031 Prec@5=88.373 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=19:36 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.623 DataTime=0.398 Loss=1.276 Prec@1=68.031 Prec@5=88.373 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:36 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.623 DataTime=0.398 Loss=1.276 Prec@1=68.031 Prec@5=88.373 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:37 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.397 Loss=1.276 Prec@1=68.009 Prec@5=88.368 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:37 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.397 Loss=1.276 Prec@1=68.009 Prec@5=88.368 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:37 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.397 Loss=1.276 Prec@1=68.009 Prec@5=88.368 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:38 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.396 Loss=1.278 Prec@1=67.968 Prec@5=88.347 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:38 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.396 Loss=1.278 Prec@1=67.968 Prec@5=88.347 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:38 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.621 DataTime=0.396 Loss=1.278 Prec@1=67.968 Prec@5=88.347 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:39 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.281 Prec@1=67.960 Prec@5=88.313 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=19:39 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.281 Prec@1=67.960 Prec@5=88.313 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:39 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.281 Prec@1=67.960 Prec@5=88.313 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:40 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.283 Prec@1=67.918 Prec@5=88.266 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:40 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.283 Prec@1=67.918 Prec@5=88.266 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=19:40 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.620 DataTime=0.395 Loss=1.283 Prec@1=67.918 Prec@5=88.266 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=19:41 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.890 Prec@5=88.240 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=19:41 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.890 Prec@5=88.240 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:41 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.890 Prec@5=88.240 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:42 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.870 Prec@5=88.235 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=19:42 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.870 Prec@5=88.235 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:42 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.619 DataTime=0.394 Loss=1.286 Prec@1=67.870 Prec@5=88.235 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:43 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.851 Prec@5=88.220 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=19:43 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.851 Prec@5=88.220 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:43 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.287 Prec@1=67.851 Prec@5=88.220 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:44 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.288 Prec@1=67.812 Prec@5=88.215 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=19:44 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.288 Prec@1=67.812 Prec@5=88.215 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:44 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.288 Prec@1=67.812 Prec@5=88.215 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:45 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.798 Prec@5=88.214 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=19:45 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.798 Prec@5=88.214 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:45 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.289 Prec@1=67.798 Prec@5=88.214 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:46 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.290 Prec@1=67.772 Prec@5=88.191 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=19:46 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.290 Prec@1=67.772 Prec@5=88.191 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:46 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.618 DataTime=0.393 Loss=1.290 Prec@1=67.772 Prec@5=88.191 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:47 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.291 Prec@1=67.756 Prec@5=88.178 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:47 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.291 Prec@1=67.756 Prec@5=88.178 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:47 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.291 Prec@1=67.756 Prec@5=88.178 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:48 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.292 Prec@1=67.730 Prec@5=88.166 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=19:48 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.292 Prec@1=67.730 Prec@5=88.166 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:48 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.292 Prec@1=67.730 Prec@5=88.166 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:49 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.708 Prec@5=88.148 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:49 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.708 Prec@5=88.148 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:49 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.293 Prec@1=67.708 Prec@5=88.148 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:50 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.672 Prec@5=88.129 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:50 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.672 Prec@5=88.129 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:50 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.672 Prec@5=88.129 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:51 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.655 Prec@5=88.122 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:51 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.655 Prec@5=88.122 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:51 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.295 Prec@1=67.655 Prec@5=88.122 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:52 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.296 Prec@1=67.623 Prec@5=88.113 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:52 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.296 Prec@1=67.623 Prec@5=88.113 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:52 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.617 DataTime=0.392 Loss=1.296 Prec@1=67.623 Prec@5=88.113 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:53 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.297 Prec@1=67.598 Prec@5=88.089 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=19:53 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.297 Prec@1=67.598 Prec@5=88.089 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:53 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.297 Prec@1=67.598 Prec@5=88.089 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:54 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.559 Prec@5=88.072 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:54 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.559 Prec@5=88.072 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:54 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.559 Prec@5=88.072 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:56 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.554 Prec@5=88.065 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:56 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.554 Prec@5=88.065 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:56 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.554 Prec@5=88.065 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:56 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.552 Prec@5=88.064 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:56 IST=> training   100.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.616 DataTime=0.392 Loss=1.299 Prec@1=67.552 Prec@5=88.064 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=19:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 0.00% of 1x98...Epoch=67/150 LR=0.05937 Time=6.503 Loss=1.400 Prec@1=65.039 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=6.503 Loss=1.400 Prec@1=65.039 Prec@5=87.500 rate=6097.19 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=6.503 Loss=1.400 Prec@1=65.039 Prec@5=87.500 rate=6097.19 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=0.412 Loss=1.407 Prec@1=65.398 Prec@5=87.148 rate=6097.19 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 100.00% of 1x98...Epoch=67/150 LR=0.05937 Time=0.412 Loss=1.407 Prec@1=65.398 Prec@5=87.148 rate=2.89 Hz, eta=0:00:00, total=0:00:33, wall=19:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.976 DataTime=5.678 Loss=1.151 Prec@1=72.070 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.976 DataTime=5.678 Loss=1.151 Prec@1=72.070 Prec@5=89.258 rate=6497.09 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.976 DataTime=5.678 Loss=1.151 Prec@1=72.070 Prec@5=89.258 rate=6497.09 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.665 DataTime=0.440 Loss=1.246 Prec@1=68.472 Prec@5=88.668 rate=6497.09 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.665 DataTime=0.440 Loss=1.246 Prec@1=68.472 Prec@5=88.668 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=19:57 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.665 DataTime=0.440 Loss=1.246 Prec@1=68.472 Prec@5=88.668 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=19:58 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.640 DataTime=0.415 Loss=1.255 Prec@1=68.364 Prec@5=88.590 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=19:58 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.640 DataTime=0.415 Loss=1.255 Prec@1=68.364 Prec@5=88.590 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:58 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.640 DataTime=0.415 Loss=1.255 Prec@1=68.364 Prec@5=88.590 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:59 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.630 DataTime=0.406 Loss=1.258 Prec@1=68.278 Prec@5=88.602 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:59 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.630 DataTime=0.406 Loss=1.258 Prec@1=68.278 Prec@5=88.602 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=19:59 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.630 DataTime=0.406 Loss=1.258 Prec@1=68.278 Prec@5=88.602 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:00 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.627 DataTime=0.402 Loss=1.261 Prec@1=68.260 Prec@5=88.565 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:00 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.627 DataTime=0.402 Loss=1.261 Prec@1=68.260 Prec@5=88.565 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=20:00 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.627 DataTime=0.402 Loss=1.261 Prec@1=68.260 Prec@5=88.565 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=20:01 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.624 DataTime=0.399 Loss=1.268 Prec@1=68.135 Prec@5=88.465 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=20:01 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.624 DataTime=0.399 Loss=1.268 Prec@1=68.135 Prec@5=88.465 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:01 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.624 DataTime=0.399 Loss=1.268 Prec@1=68.135 Prec@5=88.465 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:02 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.622 DataTime=0.398 Loss=1.266 Prec@1=68.204 Prec@5=88.490 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=20:02 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.622 DataTime=0.398 Loss=1.266 Prec@1=68.204 Prec@5=88.490 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:02 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.622 DataTime=0.398 Loss=1.266 Prec@1=68.204 Prec@5=88.490 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:04 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.621 DataTime=0.396 Loss=1.268 Prec@1=68.186 Prec@5=88.469 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=20:04 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.621 DataTime=0.396 Loss=1.268 Prec@1=68.186 Prec@5=88.469 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:04 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.621 DataTime=0.396 Loss=1.268 Prec@1=68.186 Prec@5=88.469 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:05 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.620 DataTime=0.396 Loss=1.269 Prec@1=68.173 Prec@5=88.466 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:05 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.620 DataTime=0.396 Loss=1.269 Prec@1=68.173 Prec@5=88.466 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:05 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.620 DataTime=0.396 Loss=1.269 Prec@1=68.173 Prec@5=88.466 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:06 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.395 Loss=1.270 Prec@1=68.135 Prec@5=88.450 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:06 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.395 Loss=1.270 Prec@1=68.135 Prec@5=88.450 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:06 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.395 Loss=1.270 Prec@1=68.135 Prec@5=88.450 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:07 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.394 Loss=1.271 Prec@1=68.121 Prec@5=88.446 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:07 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.394 Loss=1.271 Prec@1=68.121 Prec@5=88.446 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:07 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.619 DataTime=0.394 Loss=1.271 Prec@1=68.121 Prec@5=88.446 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:08 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.394 Loss=1.272 Prec@1=68.084 Prec@5=88.410 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:08 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.394 Loss=1.272 Prec@1=68.084 Prec@5=88.410 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:08 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.394 Loss=1.272 Prec@1=68.084 Prec@5=88.410 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:09 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.275 Prec@1=68.026 Prec@5=88.394 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:09 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.275 Prec@1=68.026 Prec@5=88.394 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:09 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.275 Prec@1=68.026 Prec@5=88.394 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:10 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.277 Prec@1=67.979 Prec@5=88.358 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:10 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.277 Prec@1=67.979 Prec@5=88.358 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:10 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.618 DataTime=0.393 Loss=1.277 Prec@1=67.979 Prec@5=88.358 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:11 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.279 Prec@1=67.940 Prec@5=88.329 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:11 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.279 Prec@1=67.940 Prec@5=88.329 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:11 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.279 Prec@1=67.940 Prec@5=88.329 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:12 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.280 Prec@1=67.915 Prec@5=88.312 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:12 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.280 Prec@1=67.915 Prec@5=88.312 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:12 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.393 Loss=1.280 Prec@1=67.915 Prec@5=88.312 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:13 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.281 Prec@1=67.881 Prec@5=88.303 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:13 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.281 Prec@1=67.881 Prec@5=88.303 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:13 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.281 Prec@1=67.881 Prec@5=88.303 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:14 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.282 Prec@1=67.867 Prec@5=88.285 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:14 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.282 Prec@1=67.867 Prec@5=88.285 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:14 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.282 Prec@1=67.867 Prec@5=88.285 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:15 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.283 Prec@1=67.841 Prec@5=88.272 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:15 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.283 Prec@1=67.841 Prec@5=88.272 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:15 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.617 DataTime=0.392 Loss=1.283 Prec@1=67.841 Prec@5=88.272 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:16 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.285 Prec@1=67.811 Prec@5=88.244 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:16 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.285 Prec@1=67.811 Prec@5=88.244 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:16 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.285 Prec@1=67.811 Prec@5=88.244 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:17 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.286 Prec@1=67.778 Prec@5=88.231 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:17 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.286 Prec@1=67.778 Prec@5=88.231 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:17 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.286 Prec@1=67.778 Prec@5=88.231 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:18 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.287 Prec@1=67.760 Prec@5=88.214 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:18 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.287 Prec@1=67.760 Prec@5=88.214 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:18 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.287 Prec@1=67.760 Prec@5=88.214 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:19 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.288 Prec@1=67.745 Prec@5=88.205 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:19 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.288 Prec@1=67.745 Prec@5=88.205 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:19 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.288 Prec@1=67.745 Prec@5=88.205 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:20 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.289 Prec@1=67.727 Prec@5=88.188 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:20 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.289 Prec@1=67.727 Prec@5=88.188 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:20 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.289 Prec@1=67.727 Prec@5=88.188 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:21 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.290 Prec@1=67.704 Prec@5=88.179 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:21 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.290 Prec@1=67.704 Prec@5=88.179 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:21 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.392 Loss=1.290 Prec@1=67.704 Prec@5=88.179 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:22 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.391 Loss=1.291 Prec@1=67.693 Prec@5=88.171 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:22 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.391 Loss=1.291 Prec@1=67.693 Prec@5=88.171 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:22 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.391 Loss=1.291 Prec@1=67.693 Prec@5=88.171 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:22 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.391 Loss=1.291 Prec@1=67.692 Prec@5=88.169 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:22 IST=> training   100.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.616 DataTime=0.391 Loss=1.291 Prec@1=67.692 Prec@5=88.169 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=20:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:22 IST=> validation 0.00% of 1x98...Epoch=68/150 LR=0.05834 Time=6.014 Loss=1.652 Prec@1=61.719 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=20:22 IST=> validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.014 Loss=1.652 Prec@1=61.719 Prec@5=84.961 rate=3417.59 Hz, eta=0:00:00, total=0:00:00, wall=20:22 IST** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.014 Loss=1.652 Prec@1=61.719 Prec@5=84.961 rate=3417.59 Hz, eta=0:00:00, total=0:00:00, wall=20:23 IST** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=0.400 Loss=1.441 Prec@1=64.298 Prec@5=86.574 rate=3417.59 Hz, eta=0:00:00, total=0:00:00, wall=20:23 IST** validation 100.00% of 1x98...Epoch=68/150 LR=0.05834 Time=0.400 Loss=1.441 Prec@1=64.298 Prec@5=86.574 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=20:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:23 IST=> training   0.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.730 DataTime=5.430 Loss=1.228 Prec@1=69.141 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=20:23 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.730 DataTime=5.430 Loss=1.228 Prec@1=69.141 Prec@5=88.477 rate=4922.76 Hz, eta=0:00:00, total=0:00:00, wall=20:23 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.730 DataTime=5.430 Loss=1.228 Prec@1=69.141 Prec@5=88.477 rate=4922.76 Hz, eta=0:00:00, total=0:00:00, wall=20:24 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.663 DataTime=0.436 Loss=1.248 Prec@1=68.835 Prec@5=88.635 rate=4922.76 Hz, eta=0:00:00, total=0:00:00, wall=20:24 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.663 DataTime=0.436 Loss=1.248 Prec@1=68.835 Prec@5=88.635 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:24 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.663 DataTime=0.436 Loss=1.248 Prec@1=68.835 Prec@5=88.635 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:25 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.638 DataTime=0.413 Loss=1.248 Prec@1=68.775 Prec@5=88.702 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=20:25 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.638 DataTime=0.413 Loss=1.248 Prec@1=68.775 Prec@5=88.702 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:25 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.638 DataTime=0.413 Loss=1.248 Prec@1=68.775 Prec@5=88.702 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:26 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.630 DataTime=0.405 Loss=1.254 Prec@1=68.589 Prec@5=88.625 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=20:26 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.630 DataTime=0.405 Loss=1.254 Prec@1=68.589 Prec@5=88.625 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:26 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.630 DataTime=0.405 Loss=1.254 Prec@1=68.589 Prec@5=88.625 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:27 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.625 DataTime=0.401 Loss=1.253 Prec@1=68.500 Prec@5=88.648 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:27 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.625 DataTime=0.401 Loss=1.253 Prec@1=68.500 Prec@5=88.648 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:27 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.625 DataTime=0.401 Loss=1.253 Prec@1=68.500 Prec@5=88.648 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:28 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.623 DataTime=0.399 Loss=1.256 Prec@1=68.450 Prec@5=88.622 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:28 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.623 DataTime=0.399 Loss=1.256 Prec@1=68.450 Prec@5=88.622 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=20:28 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.623 DataTime=0.399 Loss=1.256 Prec@1=68.450 Prec@5=88.622 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=20:29 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.622 DataTime=0.398 Loss=1.259 Prec@1=68.353 Prec@5=88.597 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=20:29 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.622 DataTime=0.398 Loss=1.259 Prec@1=68.353 Prec@5=88.597 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:29 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.622 DataTime=0.398 Loss=1.259 Prec@1=68.353 Prec@5=88.597 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:30 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.621 DataTime=0.397 Loss=1.260 Prec@1=68.322 Prec@5=88.579 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:30 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.621 DataTime=0.397 Loss=1.260 Prec@1=68.322 Prec@5=88.579 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:30 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.621 DataTime=0.397 Loss=1.260 Prec@1=68.322 Prec@5=88.579 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:31 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.620 DataTime=0.396 Loss=1.261 Prec@1=68.274 Prec@5=88.573 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:31 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.620 DataTime=0.396 Loss=1.261 Prec@1=68.274 Prec@5=88.573 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:31 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.620 DataTime=0.396 Loss=1.261 Prec@1=68.274 Prec@5=88.573 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:32 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.263 Prec@1=68.230 Prec@5=88.558 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:32 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.263 Prec@1=68.230 Prec@5=88.558 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:32 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.263 Prec@1=68.230 Prec@5=88.558 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:33 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.266 Prec@1=68.176 Prec@5=88.516 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:33 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.266 Prec@1=68.176 Prec@5=88.516 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:33 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.619 DataTime=0.395 Loss=1.266 Prec@1=68.176 Prec@5=88.516 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:34 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.618 DataTime=0.394 Loss=1.267 Prec@1=68.163 Prec@5=88.516 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:34 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.618 DataTime=0.394 Loss=1.267 Prec@1=68.163 Prec@5=88.516 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:34 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.618 DataTime=0.394 Loss=1.267 Prec@1=68.163 Prec@5=88.516 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:35 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.394 Loss=1.269 Prec@1=68.113 Prec@5=88.490 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:35 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.394 Loss=1.269 Prec@1=68.113 Prec@5=88.490 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=20:35 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.394 Loss=1.269 Prec@1=68.113 Prec@5=88.490 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=20:36 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.071 Prec@5=88.481 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=20:36 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.071 Prec@5=88.481 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:36 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.071 Prec@5=88.481 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:37 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.272 Prec@1=68.034 Prec@5=88.460 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:37 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.272 Prec@1=68.034 Prec@5=88.460 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:37 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.272 Prec@1=68.034 Prec@5=88.460 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:38 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.273 Prec@1=68.021 Prec@5=88.455 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:38 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.273 Prec@1=68.021 Prec@5=88.455 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=20:38 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.617 DataTime=0.393 Loss=1.273 Prec@1=68.021 Prec@5=88.455 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=20:39 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.003 Prec@5=88.427 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=20:39 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.003 Prec@5=88.427 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:39 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.003 Prec@5=88.427 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:40 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.982 Prec@5=88.418 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:40 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.982 Prec@5=88.418 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:40 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.982 Prec@5=88.418 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:41 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.957 Prec@5=88.392 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:41 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.957 Prec@5=88.392 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=20:41 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.957 Prec@5=88.392 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=20:42 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.948 Prec@5=88.379 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=20:42 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.948 Prec@5=88.379 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:42 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.948 Prec@5=88.379 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:43 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.928 Prec@5=88.363 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:43 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.928 Prec@5=88.363 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:43 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.928 Prec@5=88.363 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:44 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.936 Prec@5=88.357 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:44 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.936 Prec@5=88.357 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=20:44 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.280 Prec@1=67.936 Prec@5=88.357 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=20:45 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.281 Prec@1=67.908 Prec@5=88.329 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=20:45 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.281 Prec@1=67.908 Prec@5=88.329 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:45 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.281 Prec@1=67.908 Prec@5=88.329 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:46 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.282 Prec@1=67.892 Prec@5=88.321 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:46 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.282 Prec@1=67.892 Prec@5=88.321 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=20:46 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.616 DataTime=0.392 Loss=1.282 Prec@1=67.892 Prec@5=88.321 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=20:47 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.283 Prec@1=67.861 Prec@5=88.301 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=20:47 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.283 Prec@1=67.861 Prec@5=88.301 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:47 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.283 Prec@1=67.861 Prec@5=88.301 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:48 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.284 Prec@1=67.837 Prec@5=88.287 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:48 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.284 Prec@1=67.837 Prec@5=88.287 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:48 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.284 Prec@1=67.837 Prec@5=88.287 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:48 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.285 Prec@1=67.835 Prec@5=88.286 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:48 IST=> training   100.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.615 DataTime=0.392 Loss=1.285 Prec@1=67.835 Prec@5=88.286 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=20:48 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:48 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:48 IST=> validation 0.00% of 1x98...Epoch=69/150 LR=0.05730 Time=7.660 Loss=1.528 Prec@1=60.352 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=20:48 IST=> validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=7.660 Loss=1.528 Prec@1=60.352 Prec@5=84.766 rate=5567.90 Hz, eta=0:00:00, total=0:00:00, wall=20:48 IST** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=7.660 Loss=1.528 Prec@1=60.352 Prec@5=84.766 rate=5567.90 Hz, eta=0:00:00, total=0:00:00, wall=20:49 IST** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=0.407 Loss=1.405 Prec@1=65.504 Prec@5=86.886 rate=5567.90 Hz, eta=0:00:00, total=0:00:00, wall=20:49 IST** validation 100.00% of 1x98...Epoch=69/150 LR=0.05730 Time=0.407 Loss=1.405 Prec@1=65.504 Prec@5=86.886 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=20:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:49 IST=> training   0.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.596 DataTime=5.331 Loss=1.172 Prec@1=69.531 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=20:49 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.596 DataTime=5.331 Loss=1.172 Prec@1=69.531 Prec@5=90.039 rate=7539.32 Hz, eta=0:00:00, total=0:00:00, wall=20:49 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.596 DataTime=5.331 Loss=1.172 Prec@1=69.531 Prec@5=90.039 rate=7539.32 Hz, eta=0:00:00, total=0:00:00, wall=20:50 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.658 DataTime=0.436 Loss=1.238 Prec@1=68.899 Prec@5=88.883 rate=7539.32 Hz, eta=0:00:00, total=0:00:00, wall=20:50 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.658 DataTime=0.436 Loss=1.238 Prec@1=68.899 Prec@5=88.883 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=20:50 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.658 DataTime=0.436 Loss=1.238 Prec@1=68.899 Prec@5=88.883 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=20:51 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.637 DataTime=0.413 Loss=1.236 Prec@1=68.823 Prec@5=88.920 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=20:51 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.637 DataTime=0.413 Loss=1.236 Prec@1=68.823 Prec@5=88.920 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=20:51 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.637 DataTime=0.413 Loss=1.236 Prec@1=68.823 Prec@5=88.920 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=20:52 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.628 DataTime=0.405 Loss=1.239 Prec@1=68.727 Prec@5=88.861 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=20:52 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.628 DataTime=0.405 Loss=1.239 Prec@1=68.727 Prec@5=88.861 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=20:52 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.628 DataTime=0.405 Loss=1.239 Prec@1=68.727 Prec@5=88.861 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=20:53 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.624 DataTime=0.401 Loss=1.248 Prec@1=68.574 Prec@5=88.731 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=20:53 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.624 DataTime=0.401 Loss=1.248 Prec@1=68.574 Prec@5=88.731 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=20:53 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.624 DataTime=0.401 Loss=1.248 Prec@1=68.574 Prec@5=88.731 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=20:54 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.622 DataTime=0.399 Loss=1.251 Prec@1=68.516 Prec@5=88.706 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=20:54 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.622 DataTime=0.399 Loss=1.251 Prec@1=68.516 Prec@5=88.706 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=20:54 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.622 DataTime=0.399 Loss=1.251 Prec@1=68.516 Prec@5=88.706 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=20:55 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.621 DataTime=0.397 Loss=1.255 Prec@1=68.411 Prec@5=88.612 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=20:55 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.621 DataTime=0.397 Loss=1.255 Prec@1=68.411 Prec@5=88.612 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=20:55 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.621 DataTime=0.397 Loss=1.255 Prec@1=68.411 Prec@5=88.612 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=20:56 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.620 DataTime=0.396 Loss=1.258 Prec@1=68.361 Prec@5=88.595 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=20:56 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.620 DataTime=0.396 Loss=1.258 Prec@1=68.361 Prec@5=88.595 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=20:56 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.620 DataTime=0.396 Loss=1.258 Prec@1=68.361 Prec@5=88.595 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=20:57 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.396 Loss=1.259 Prec@1=68.327 Prec@5=88.590 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=20:57 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.396 Loss=1.259 Prec@1=68.327 Prec@5=88.590 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:57 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.396 Loss=1.259 Prec@1=68.327 Prec@5=88.590 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:58 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.395 Loss=1.259 Prec@1=68.342 Prec@5=88.573 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:58 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.395 Loss=1.259 Prec@1=68.342 Prec@5=88.573 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:58 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.619 DataTime=0.395 Loss=1.259 Prec@1=68.342 Prec@5=88.573 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:59 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.395 Loss=1.261 Prec@1=68.309 Prec@5=88.547 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:59 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.395 Loss=1.261 Prec@1=68.309 Prec@5=88.547 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:59 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.395 Loss=1.261 Prec@1=68.309 Prec@5=88.547 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:00 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.261 Prec@1=68.329 Prec@5=88.560 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:00 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.261 Prec@1=68.329 Prec@5=88.560 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=21:00 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.261 Prec@1=68.329 Prec@5=88.560 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=21:01 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.264 Prec@1=68.267 Prec@5=88.516 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=21:01 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.264 Prec@1=68.267 Prec@5=88.516 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:01 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.264 Prec@1=68.267 Prec@5=88.516 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:02 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.265 Prec@1=68.238 Prec@5=88.495 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:02 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.265 Prec@1=68.238 Prec@5=88.495 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:02 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.618 DataTime=0.394 Loss=1.265 Prec@1=68.238 Prec@5=88.495 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:03 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.394 Loss=1.267 Prec@1=68.211 Prec@5=88.478 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=21:03 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.394 Loss=1.267 Prec@1=68.211 Prec@5=88.478 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:03 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.394 Loss=1.267 Prec@1=68.211 Prec@5=88.478 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:04 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.268 Prec@1=68.182 Prec@5=88.460 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=21:04 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.268 Prec@1=68.182 Prec@5=88.460 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:04 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.268 Prec@1=68.182 Prec@5=88.460 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:05 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.123 Prec@5=88.434 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=21:05 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.123 Prec@5=88.434 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=21:05 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.270 Prec@1=68.123 Prec@5=88.434 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=21:06 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.271 Prec@1=68.102 Prec@5=88.426 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=21:06 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.271 Prec@1=68.102 Prec@5=88.426 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=21:06 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.617 DataTime=0.393 Loss=1.271 Prec@1=68.102 Prec@5=88.426 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=21:08 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.393 Loss=1.272 Prec@1=68.090 Prec@5=88.424 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=21:08 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.393 Loss=1.272 Prec@1=68.090 Prec@5=88.424 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:08 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.393 Loss=1.272 Prec@1=68.090 Prec@5=88.424 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:09 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.273 Prec@1=68.064 Prec@5=88.402 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:09 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.273 Prec@1=68.064 Prec@5=88.402 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:09 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.273 Prec@1=68.064 Prec@5=88.402 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:10 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.274 Prec@1=68.046 Prec@5=88.395 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=21:10 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.274 Prec@1=68.046 Prec@5=88.395 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=21:10 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.274 Prec@1=68.046 Prec@5=88.395 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=21:11 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.023 Prec@5=88.379 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=21:11 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.023 Prec@5=88.379 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:11 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.023 Prec@5=88.379 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:12 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.008 Prec@5=88.371 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:12 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.008 Prec@5=88.371 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:12 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.275 Prec@1=68.008 Prec@5=88.371 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:13 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.978 Prec@5=88.356 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=21:13 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.978 Prec@5=88.356 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:13 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.276 Prec@1=67.978 Prec@5=88.356 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:14 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.277 Prec@1=67.954 Prec@5=88.348 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:14 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.277 Prec@1=67.954 Prec@5=88.348 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:14 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.277 Prec@1=67.954 Prec@5=88.348 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:15 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.942 Prec@5=88.334 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:15 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.942 Prec@5=88.334 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:15 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.942 Prec@5=88.334 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:15 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.943 Prec@5=88.335 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:15 IST=> training   100.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.616 DataTime=0.392 Loss=1.278 Prec@1=67.943 Prec@5=88.335 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=21:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:15 IST=> validation 0.00% of 1x98...Epoch=70/150 LR=0.05627 Time=6.595 Loss=1.461 Prec@1=64.258 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=21:15 IST=> validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=6.595 Loss=1.461 Prec@1=64.258 Prec@5=86.719 rate=6499.41 Hz, eta=0:00:00, total=0:00:00, wall=21:15 IST** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=6.595 Loss=1.461 Prec@1=64.258 Prec@5=86.719 rate=6499.41 Hz, eta=0:00:00, total=0:00:00, wall=21:15 IST** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=0.402 Loss=1.356 Prec@1=66.372 Prec@5=87.736 rate=6499.41 Hz, eta=0:00:00, total=0:00:00, wall=21:15 IST** validation 100.00% of 1x98...Epoch=70/150 LR=0.05627 Time=0.402 Loss=1.356 Prec@1=66.372 Prec@5=87.736 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=21:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:16 IST=> training   0.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.136 DataTime=4.842 Loss=1.254 Prec@1=67.969 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=21:16 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.136 DataTime=4.842 Loss=1.254 Prec@1=67.969 Prec@5=88.867 rate=5206.33 Hz, eta=0:00:00, total=0:00:00, wall=21:16 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.136 DataTime=4.842 Loss=1.254 Prec@1=67.969 Prec@5=88.867 rate=5206.33 Hz, eta=0:00:00, total=0:00:00, wall=21:17 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.662 DataTime=0.434 Loss=1.231 Prec@1=68.862 Prec@5=89.039 rate=5206.33 Hz, eta=0:00:00, total=0:00:00, wall=21:17 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.662 DataTime=0.434 Loss=1.231 Prec@1=68.862 Prec@5=89.039 rate=1.64 Hz, eta=0:24:29, total=0:01:01, wall=21:17 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.662 DataTime=0.434 Loss=1.231 Prec@1=68.862 Prec@5=89.039 rate=1.64 Hz, eta=0:24:29, total=0:01:01, wall=21:18 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.638 DataTime=0.412 Loss=1.238 Prec@1=68.725 Prec@5=88.973 rate=1.64 Hz, eta=0:24:29, total=0:01:01, wall=21:18 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.638 DataTime=0.412 Loss=1.238 Prec@1=68.725 Prec@5=88.973 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=21:18 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.638 DataTime=0.412 Loss=1.238 Prec@1=68.725 Prec@5=88.973 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=21:19 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.631 DataTime=0.405 Loss=1.237 Prec@1=68.755 Prec@5=88.946 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=21:19 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.631 DataTime=0.405 Loss=1.237 Prec@1=68.755 Prec@5=88.946 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=21:19 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.631 DataTime=0.405 Loss=1.237 Prec@1=68.755 Prec@5=88.946 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=21:20 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.627 DataTime=0.401 Loss=1.240 Prec@1=68.728 Prec@5=88.914 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=21:20 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.627 DataTime=0.401 Loss=1.240 Prec@1=68.728 Prec@5=88.914 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=21:20 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.627 DataTime=0.401 Loss=1.240 Prec@1=68.728 Prec@5=88.914 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=21:21 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.624 DataTime=0.399 Loss=1.245 Prec@1=68.618 Prec@5=88.829 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=21:21 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.624 DataTime=0.399 Loss=1.245 Prec@1=68.618 Prec@5=88.829 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=21:21 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.624 DataTime=0.399 Loss=1.245 Prec@1=68.618 Prec@5=88.829 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=21:22 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.622 DataTime=0.397 Loss=1.248 Prec@1=68.615 Prec@5=88.776 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=21:22 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.622 DataTime=0.397 Loss=1.248 Prec@1=68.615 Prec@5=88.776 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=21:22 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.622 DataTime=0.397 Loss=1.248 Prec@1=68.615 Prec@5=88.776 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=21:23 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.397 Loss=1.249 Prec@1=68.607 Prec@5=88.737 rate=1.63 Hz, eta=0:19:27, total=0:06:08, wall=21:23 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.397 Loss=1.249 Prec@1=68.607 Prec@5=88.737 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=21:23 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.397 Loss=1.249 Prec@1=68.607 Prec@5=88.737 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=21:24 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.396 Loss=1.249 Prec@1=68.591 Prec@5=88.746 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=21:24 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.396 Loss=1.249 Prec@1=68.591 Prec@5=88.746 rate=1.63 Hz, eta=0:17:25, total=0:08:12, wall=21:24 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.621 DataTime=0.396 Loss=1.249 Prec@1=68.591 Prec@5=88.746 rate=1.63 Hz, eta=0:17:25, total=0:08:12, wall=21:25 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.251 Prec@1=68.549 Prec@5=88.712 rate=1.63 Hz, eta=0:17:25, total=0:08:12, wall=21:25 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.251 Prec@1=68.549 Prec@5=88.712 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=21:25 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.251 Prec@1=68.549 Prec@5=88.712 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=21:26 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.253 Prec@1=68.495 Prec@5=88.689 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=21:26 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.253 Prec@1=68.495 Prec@5=88.689 rate=1.63 Hz, eta=0:15:22, total=0:10:15, wall=21:26 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.620 DataTime=0.395 Loss=1.253 Prec@1=68.495 Prec@5=88.689 rate=1.63 Hz, eta=0:15:22, total=0:10:15, wall=21:27 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.254 Prec@1=68.476 Prec@5=88.657 rate=1.63 Hz, eta=0:15:22, total=0:10:15, wall=21:27 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.254 Prec@1=68.476 Prec@5=88.657 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=21:27 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.254 Prec@1=68.476 Prec@5=88.657 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=21:28 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.255 Prec@1=68.442 Prec@5=88.644 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=21:28 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.255 Prec@1=68.442 Prec@5=88.644 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:28 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.619 DataTime=0.394 Loss=1.255 Prec@1=68.442 Prec@5=88.644 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:29 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.394 Loss=1.258 Prec@1=68.401 Prec@5=88.617 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=21:29 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.394 Loss=1.258 Prec@1=68.401 Prec@5=88.617 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=21:29 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.394 Loss=1.258 Prec@1=68.401 Prec@5=88.617 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=21:30 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.393 Loss=1.258 Prec@1=68.389 Prec@5=88.605 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=21:30 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.393 Loss=1.258 Prec@1=68.389 Prec@5=88.605 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=21:30 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.618 DataTime=0.393 Loss=1.258 Prec@1=68.389 Prec@5=88.605 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=21:31 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.260 Prec@1=68.342 Prec@5=88.583 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=21:31 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.260 Prec@1=68.342 Prec@5=88.583 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:31 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.260 Prec@1=68.342 Prec@5=88.583 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:32 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.261 Prec@1=68.309 Prec@5=88.568 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=21:32 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.261 Prec@1=68.309 Prec@5=88.568 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=21:32 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.261 Prec@1=68.309 Prec@5=88.568 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=21:33 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.293 Prec@5=88.554 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=21:33 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.293 Prec@5=88.554 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:33 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.293 Prec@5=88.554 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:34 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.294 Prec@5=88.551 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:34 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.294 Prec@5=88.551 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=21:34 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.393 Loss=1.263 Prec@1=68.294 Prec@5=88.551 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=21:35 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.265 Prec@1=68.256 Prec@5=88.532 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=21:35 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.265 Prec@1=68.256 Prec@5=88.532 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:35 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.265 Prec@1=68.256 Prec@5=88.532 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:36 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.266 Prec@1=68.238 Prec@5=88.508 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=21:36 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.266 Prec@1=68.238 Prec@5=88.508 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:36 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.266 Prec@1=68.238 Prec@5=88.508 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:37 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.267 Prec@1=68.232 Prec@5=88.502 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=21:37 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.267 Prec@1=68.232 Prec@5=88.502 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=21:37 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.267 Prec@1=68.232 Prec@5=88.502 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=21:38 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.268 Prec@1=68.208 Prec@5=88.493 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=21:38 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.268 Prec@1=68.208 Prec@5=88.493 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:38 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.617 DataTime=0.392 Loss=1.268 Prec@1=68.208 Prec@5=88.493 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:39 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.201 Prec@5=88.473 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=21:39 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.201 Prec@5=88.473 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=21:39 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.201 Prec@5=88.473 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=21:40 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.184 Prec@5=88.467 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=21:40 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.184 Prec@5=88.467 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:40 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.269 Prec@1=68.184 Prec@5=88.467 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:41 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.270 Prec@1=68.172 Prec@5=88.451 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=21:41 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.270 Prec@1=68.172 Prec@5=88.451 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=21:41 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.270 Prec@1=68.172 Prec@5=88.451 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=21:41 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.270 Prec@1=68.172 Prec@5=88.451 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=21:41 IST=> training   100.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.616 DataTime=0.392 Loss=1.270 Prec@1=68.172 Prec@5=88.451 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=21:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:41 IST=> validation 0.00% of 1x98...Epoch=71/150 LR=0.05523 Time=7.280 Loss=1.429 Prec@1=64.453 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=21:41 IST=> validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=7.280 Loss=1.429 Prec@1=64.453 Prec@5=85.547 rate=4844.14 Hz, eta=0:00:00, total=0:00:00, wall=21:41 IST** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=7.280 Loss=1.429 Prec@1=64.453 Prec@5=85.547 rate=4844.14 Hz, eta=0:00:00, total=0:00:00, wall=21:42 IST** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=0.405 Loss=1.442 Prec@1=64.614 Prec@5=86.512 rate=4844.14 Hz, eta=0:00:00, total=0:00:00, wall=21:42 IST** validation 100.00% of 1x98...Epoch=71/150 LR=0.05523 Time=0.405 Loss=1.442 Prec@1=64.614 Prec@5=86.512 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=21:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST=> training   0.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.772 DataTime=5.385 Loss=1.195 Prec@1=70.117 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=21:42 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.772 DataTime=5.385 Loss=1.195 Prec@1=70.117 Prec@5=88.867 rate=8671.30 Hz, eta=0:00:00, total=0:00:00, wall=21:42 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.772 DataTime=5.385 Loss=1.195 Prec@1=70.117 Prec@5=88.867 rate=8671.30 Hz, eta=0:00:00, total=0:00:00, wall=21:43 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.660 DataTime=0.437 Loss=1.231 Prec@1=69.071 Prec@5=89.003 rate=8671.30 Hz, eta=0:00:00, total=0:00:00, wall=21:43 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.660 DataTime=0.437 Loss=1.231 Prec@1=69.071 Prec@5=89.003 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=21:43 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.660 DataTime=0.437 Loss=1.231 Prec@1=69.071 Prec@5=89.003 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=21:44 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.638 DataTime=0.414 Loss=1.239 Prec@1=68.922 Prec@5=88.846 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=21:44 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.638 DataTime=0.414 Loss=1.239 Prec@1=68.922 Prec@5=88.846 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=21:44 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.638 DataTime=0.414 Loss=1.239 Prec@1=68.922 Prec@5=88.846 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=21:45 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.630 DataTime=0.406 Loss=1.236 Prec@1=68.944 Prec@5=88.915 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=21:45 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.630 DataTime=0.406 Loss=1.236 Prec@1=68.944 Prec@5=88.915 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:45 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.630 DataTime=0.406 Loss=1.236 Prec@1=68.944 Prec@5=88.915 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:46 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.625 DataTime=0.401 Loss=1.238 Prec@1=68.914 Prec@5=88.898 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:46 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.625 DataTime=0.401 Loss=1.238 Prec@1=68.914 Prec@5=88.898 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:46 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.625 DataTime=0.401 Loss=1.238 Prec@1=68.914 Prec@5=88.898 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:47 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.623 DataTime=0.400 Loss=1.242 Prec@1=68.819 Prec@5=88.833 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:47 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.623 DataTime=0.400 Loss=1.242 Prec@1=68.819 Prec@5=88.833 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=21:47 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.623 DataTime=0.400 Loss=1.242 Prec@1=68.819 Prec@5=88.833 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=21:48 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.622 DataTime=0.398 Loss=1.243 Prec@1=68.769 Prec@5=88.824 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=21:48 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.622 DataTime=0.398 Loss=1.243 Prec@1=68.769 Prec@5=88.824 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=21:48 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.622 DataTime=0.398 Loss=1.243 Prec@1=68.769 Prec@5=88.824 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=21:49 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.621 DataTime=0.397 Loss=1.244 Prec@1=68.790 Prec@5=88.825 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=21:49 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.621 DataTime=0.397 Loss=1.244 Prec@1=68.790 Prec@5=88.825 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:49 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.621 DataTime=0.397 Loss=1.244 Prec@1=68.790 Prec@5=88.825 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:50 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.620 DataTime=0.396 Loss=1.245 Prec@1=68.713 Prec@5=88.798 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=21:50 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.620 DataTime=0.396 Loss=1.245 Prec@1=68.713 Prec@5=88.798 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=21:50 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.620 DataTime=0.396 Loss=1.245 Prec@1=68.713 Prec@5=88.798 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=21:51 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.395 Loss=1.246 Prec@1=68.687 Prec@5=88.779 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=21:51 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.395 Loss=1.246 Prec@1=68.687 Prec@5=88.779 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:51 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.395 Loss=1.246 Prec@1=68.687 Prec@5=88.779 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:52 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.247 Prec@1=68.669 Prec@5=88.743 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=21:52 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.247 Prec@1=68.669 Prec@5=88.743 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:52 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.247 Prec@1=68.669 Prec@5=88.743 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:53 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.249 Prec@1=68.626 Prec@5=88.709 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=21:53 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.249 Prec@1=68.626 Prec@5=88.709 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:53 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.619 DataTime=0.394 Loss=1.249 Prec@1=68.626 Prec@5=88.709 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:54 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.394 Loss=1.250 Prec@1=68.603 Prec@5=88.690 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=21:54 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.394 Loss=1.250 Prec@1=68.603 Prec@5=88.690 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:54 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.394 Loss=1.250 Prec@1=68.603 Prec@5=88.690 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:55 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.252 Prec@1=68.564 Prec@5=88.671 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=21:55 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.252 Prec@1=68.564 Prec@5=88.671 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:55 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.252 Prec@1=68.564 Prec@5=88.671 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:56 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.254 Prec@1=68.518 Prec@5=88.644 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=21:56 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.254 Prec@1=68.518 Prec@5=88.644 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:56 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.618 DataTime=0.393 Loss=1.254 Prec@1=68.518 Prec@5=88.644 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:57 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.502 Prec@5=88.636 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=21:57 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.502 Prec@5=88.636 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=21:57 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.502 Prec@5=88.636 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=21:58 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.503 Prec@5=88.636 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=21:58 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.503 Prec@5=88.636 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:58 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.255 Prec@1=68.503 Prec@5=88.636 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:59 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.256 Prec@1=68.480 Prec@5=88.630 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=21:59 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.256 Prec@1=68.480 Prec@5=88.630 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=21:59 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.256 Prec@1=68.480 Prec@5=88.630 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:00 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.257 Prec@1=68.458 Prec@5=88.613 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:00 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.257 Prec@1=68.458 Prec@5=88.613 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:00 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.257 Prec@1=68.458 Prec@5=88.613 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:01 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.258 Prec@1=68.427 Prec@5=88.600 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:01 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.258 Prec@1=68.427 Prec@5=88.600 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=22:01 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.393 Loss=1.258 Prec@1=68.427 Prec@5=88.600 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=22:02 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.260 Prec@1=68.396 Prec@5=88.571 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=22:02 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.260 Prec@1=68.396 Prec@5=88.571 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:02 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.260 Prec@1=68.396 Prec@5=88.571 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:03 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.261 Prec@1=68.371 Prec@5=88.560 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:03 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.261 Prec@1=68.371 Prec@5=88.560 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:03 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.617 DataTime=0.392 Loss=1.261 Prec@1=68.371 Prec@5=88.560 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:04 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.262 Prec@1=68.357 Prec@5=88.549 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:04 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.262 Prec@1=68.357 Prec@5=88.549 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:04 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.262 Prec@1=68.357 Prec@5=88.549 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:05 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.263 Prec@1=68.339 Prec@5=88.531 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:05 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.263 Prec@1=68.339 Prec@5=88.531 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:05 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.263 Prec@1=68.339 Prec@5=88.531 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:06 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.264 Prec@1=68.306 Prec@5=88.521 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:06 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.264 Prec@1=68.306 Prec@5=88.521 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:06 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.264 Prec@1=68.306 Prec@5=88.521 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:08 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.265 Prec@1=68.285 Prec@5=88.509 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.265 Prec@1=68.285 Prec@5=88.509 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.265 Prec@1=68.285 Prec@5=88.509 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.265 Prec@1=68.284 Prec@5=88.509 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:08 IST=> training   100.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.616 DataTime=0.392 Loss=1.265 Prec@1=68.284 Prec@5=88.509 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=22:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> validation 0.00% of 1x98...Epoch=72/150 LR=0.05418 Time=6.030 Loss=1.443 Prec@1=64.648 Prec@5=86.133 rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.030 Loss=1.443 Prec@1=64.648 Prec@5=86.133 rate=4006.41 Hz, eta=0:00:00, total=0:00:00, wall=22:08 IST** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.030 Loss=1.443 Prec@1=64.648 Prec@5=86.133 rate=4006.41 Hz, eta=0:00:00, total=0:00:00, wall=22:08 IST** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=0.410 Loss=1.395 Prec@1=65.636 Prec@5=86.988 rate=4006.41 Hz, eta=0:00:00, total=0:00:00, wall=22:08 IST** validation 100.00% of 1x98...Epoch=72/150 LR=0.05418 Time=0.410 Loss=1.395 Prec@1=65.636 Prec@5=86.988 rate=2.87 Hz, eta=0:00:00, total=0:00:34, wall=22:08 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> training   0.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=5.247 DataTime=4.951 Loss=1.115 Prec@1=70.508 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=22:08 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=5.247 DataTime=4.951 Loss=1.115 Prec@1=70.508 Prec@5=89.648 rate=2756.21 Hz, eta=0:00:00, total=0:00:00, wall=22:08 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=5.247 DataTime=4.951 Loss=1.115 Prec@1=70.508 Prec@5=89.648 rate=2756.21 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.657 DataTime=0.429 Loss=1.192 Prec@1=69.858 Prec@5=89.434 rate=2756.21 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.657 DataTime=0.429 Loss=1.192 Prec@1=69.858 Prec@5=89.434 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:09 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.657 DataTime=0.429 Loss=1.192 Prec@1=69.858 Prec@5=89.434 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:10 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.636 DataTime=0.408 Loss=1.205 Prec@1=69.548 Prec@5=89.321 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:10 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.636 DataTime=0.408 Loss=1.205 Prec@1=69.548 Prec@5=89.321 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=22:10 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.636 DataTime=0.408 Loss=1.205 Prec@1=69.548 Prec@5=89.321 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=22:11 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.629 DataTime=0.402 Loss=1.214 Prec@1=69.317 Prec@5=89.232 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=22:11 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.629 DataTime=0.402 Loss=1.214 Prec@1=69.317 Prec@5=89.232 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=22:11 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.629 DataTime=0.402 Loss=1.214 Prec@1=69.317 Prec@5=89.232 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=22:12 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.625 DataTime=0.399 Loss=1.219 Prec@1=69.176 Prec@5=89.145 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=22:12 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.625 DataTime=0.399 Loss=1.219 Prec@1=69.176 Prec@5=89.145 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:12 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.625 DataTime=0.399 Loss=1.219 Prec@1=69.176 Prec@5=89.145 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:13 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.623 DataTime=0.397 Loss=1.221 Prec@1=69.112 Prec@5=89.133 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:13 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.623 DataTime=0.397 Loss=1.221 Prec@1=69.112 Prec@5=89.133 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:13 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.623 DataTime=0.397 Loss=1.221 Prec@1=69.112 Prec@5=89.133 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:14 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.622 DataTime=0.396 Loss=1.226 Prec@1=68.996 Prec@5=89.055 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:14 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.622 DataTime=0.396 Loss=1.226 Prec@1=68.996 Prec@5=89.055 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:14 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.622 DataTime=0.396 Loss=1.226 Prec@1=68.996 Prec@5=89.055 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:15 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.621 DataTime=0.395 Loss=1.229 Prec@1=68.928 Prec@5=89.013 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:15 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.621 DataTime=0.395 Loss=1.229 Prec@1=68.928 Prec@5=89.013 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:15 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.621 DataTime=0.395 Loss=1.229 Prec@1=68.928 Prec@5=89.013 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:17 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.620 DataTime=0.394 Loss=1.230 Prec@1=68.888 Prec@5=89.017 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:17 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.620 DataTime=0.394 Loss=1.230 Prec@1=68.888 Prec@5=89.017 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:17 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.620 DataTime=0.394 Loss=1.230 Prec@1=68.888 Prec@5=89.017 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:18 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.394 Loss=1.232 Prec@1=68.863 Prec@5=88.995 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:18 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.394 Loss=1.232 Prec@1=68.863 Prec@5=88.995 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:18 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.394 Loss=1.232 Prec@1=68.863 Prec@5=88.995 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:19 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.393 Loss=1.234 Prec@1=68.850 Prec@5=88.954 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:19 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.393 Loss=1.234 Prec@1=68.850 Prec@5=88.954 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:19 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.619 DataTime=0.393 Loss=1.234 Prec@1=68.850 Prec@5=88.954 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:20 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.780 Prec@5=88.909 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:20 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.780 Prec@5=88.909 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:20 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.780 Prec@5=88.909 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:21 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.238 Prec@1=68.757 Prec@5=88.895 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:21 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.238 Prec@1=68.757 Prec@5=88.895 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:21 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.238 Prec@1=68.757 Prec@5=88.895 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:22 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.240 Prec@1=68.741 Prec@5=88.872 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:22 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.240 Prec@1=68.741 Prec@5=88.872 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:22 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.618 DataTime=0.392 Loss=1.240 Prec@1=68.741 Prec@5=88.872 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:23 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.696 Prec@5=88.847 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:23 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.696 Prec@5=88.847 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:23 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.696 Prec@5=88.847 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:24 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.692 Prec@5=88.843 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:24 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.692 Prec@5=88.843 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=22:24 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.692 Prec@5=88.843 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=22:25 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.244 Prec@1=68.657 Prec@5=88.824 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=22:25 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.244 Prec@1=68.657 Prec@5=88.824 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:25 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.244 Prec@1=68.657 Prec@5=88.824 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:26 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.246 Prec@1=68.619 Prec@5=88.794 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:26 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.246 Prec@1=68.619 Prec@5=88.794 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:26 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.246 Prec@1=68.619 Prec@5=88.794 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:27 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.248 Prec@1=68.572 Prec@5=88.769 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:27 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.248 Prec@1=68.572 Prec@5=88.769 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:27 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.248 Prec@1=68.572 Prec@5=88.769 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:28 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.250 Prec@1=68.535 Prec@5=88.750 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:28 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.250 Prec@1=68.535 Prec@5=88.750 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:28 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.250 Prec@1=68.535 Prec@5=88.750 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:29 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.251 Prec@1=68.510 Prec@5=88.738 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:29 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.251 Prec@1=68.510 Prec@5=88.738 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:29 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.617 DataTime=0.391 Loss=1.251 Prec@1=68.510 Prec@5=88.738 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:30 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.252 Prec@1=68.498 Prec@5=88.729 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:30 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.252 Prec@1=68.498 Prec@5=88.729 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:30 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.252 Prec@1=68.498 Prec@5=88.729 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:31 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.253 Prec@1=68.492 Prec@5=88.712 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:31 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.253 Prec@1=68.492 Prec@5=88.712 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:31 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.253 Prec@1=68.492 Prec@5=88.712 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:32 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.254 Prec@1=68.464 Prec@5=88.698 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:32 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.254 Prec@1=68.464 Prec@5=88.698 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:32 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.254 Prec@1=68.464 Prec@5=88.698 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:33 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.255 Prec@1=68.446 Prec@5=88.689 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:33 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.255 Prec@1=68.446 Prec@5=88.689 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:33 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.255 Prec@1=68.446 Prec@5=88.689 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:34 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.256 Prec@1=68.418 Prec@5=88.669 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:34 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.256 Prec@1=68.418 Prec@5=88.669 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:34 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.256 Prec@1=68.418 Prec@5=88.669 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:34 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.256 Prec@1=68.418 Prec@5=88.670 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=22:34 IST=> training   100.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.616 DataTime=0.391 Loss=1.256 Prec@1=68.418 Prec@5=88.670 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=22:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST=> validation 0.00% of 1x98...Epoch=73/150 LR=0.05314 Time=6.814 Loss=1.457 Prec@1=63.867 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST=> validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=6.814 Loss=1.457 Prec@1=63.867 Prec@5=87.305 rate=6048.84 Hz, eta=0:00:00, total=0:00:00, wall=22:34 IST** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=6.814 Loss=1.457 Prec@1=63.867 Prec@5=87.305 rate=6048.84 Hz, eta=0:00:00, total=0:00:00, wall=22:35 IST** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=0.409 Loss=1.415 Prec@1=65.254 Prec@5=86.946 rate=6048.84 Hz, eta=0:00:00, total=0:00:00, wall=22:35 IST** validation 100.00% of 1x98...Epoch=73/150 LR=0.05314 Time=0.409 Loss=1.415 Prec@1=65.254 Prec@5=86.946 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=22:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:35 IST=> training   0.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=6.106 DataTime=5.713 Loss=1.370 Prec@1=65.430 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=22:35 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=6.106 DataTime=5.713 Loss=1.370 Prec@1=65.430 Prec@5=87.891 rate=9190.33 Hz, eta=0:00:00, total=0:00:00, wall=22:35 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=6.106 DataTime=5.713 Loss=1.370 Prec@1=65.430 Prec@5=87.891 rate=9190.33 Hz, eta=0:00:00, total=0:00:00, wall=22:36 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.665 DataTime=0.440 Loss=1.216 Prec@1=69.336 Prec@5=89.258 rate=9190.33 Hz, eta=0:00:00, total=0:00:00, wall=22:36 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.665 DataTime=0.440 Loss=1.216 Prec@1=69.336 Prec@5=89.258 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:36 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.665 DataTime=0.440 Loss=1.216 Prec@1=69.336 Prec@5=89.258 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:37 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.640 DataTime=0.414 Loss=1.216 Prec@1=69.421 Prec@5=89.248 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=22:37 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.640 DataTime=0.414 Loss=1.216 Prec@1=69.421 Prec@5=89.248 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=22:37 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.640 DataTime=0.414 Loss=1.216 Prec@1=69.421 Prec@5=89.248 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=22:38 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.631 DataTime=0.406 Loss=1.221 Prec@1=69.238 Prec@5=89.170 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=22:38 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.631 DataTime=0.406 Loss=1.221 Prec@1=69.238 Prec@5=89.170 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=22:38 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.631 DataTime=0.406 Loss=1.221 Prec@1=69.238 Prec@5=89.170 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=22:39 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.627 DataTime=0.402 Loss=1.221 Prec@1=69.206 Prec@5=89.120 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=22:39 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.627 DataTime=0.402 Loss=1.221 Prec@1=69.206 Prec@5=89.120 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:39 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.627 DataTime=0.402 Loss=1.221 Prec@1=69.206 Prec@5=89.120 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:40 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.625 DataTime=0.399 Loss=1.222 Prec@1=69.203 Prec@5=89.080 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=22:40 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.625 DataTime=0.399 Loss=1.222 Prec@1=69.203 Prec@5=89.080 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:40 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.625 DataTime=0.399 Loss=1.222 Prec@1=69.203 Prec@5=89.080 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:41 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.623 DataTime=0.398 Loss=1.227 Prec@1=69.136 Prec@5=89.010 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=22:41 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.623 DataTime=0.398 Loss=1.227 Prec@1=69.136 Prec@5=89.010 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:41 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.623 DataTime=0.398 Loss=1.227 Prec@1=69.136 Prec@5=89.010 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:42 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.622 DataTime=0.397 Loss=1.227 Prec@1=69.092 Prec@5=89.021 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=22:42 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.622 DataTime=0.397 Loss=1.227 Prec@1=69.092 Prec@5=89.021 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:42 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.622 DataTime=0.397 Loss=1.227 Prec@1=69.092 Prec@5=89.021 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:43 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.621 DataTime=0.396 Loss=1.230 Prec@1=69.017 Prec@5=88.976 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=22:43 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.621 DataTime=0.396 Loss=1.230 Prec@1=69.017 Prec@5=88.976 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:43 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.621 DataTime=0.396 Loss=1.230 Prec@1=69.017 Prec@5=88.976 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:44 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.231 Prec@1=68.972 Prec@5=88.946 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=22:44 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.231 Prec@1=68.972 Prec@5=88.946 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:44 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.231 Prec@1=68.972 Prec@5=88.946 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:45 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.233 Prec@1=68.930 Prec@5=88.928 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=22:45 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.233 Prec@1=68.930 Prec@5=88.928 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:45 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.620 DataTime=0.395 Loss=1.233 Prec@1=68.930 Prec@5=88.928 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:46 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.234 Prec@1=68.912 Prec@5=88.935 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=22:46 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.234 Prec@1=68.912 Prec@5=88.935 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:46 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.234 Prec@1=68.912 Prec@5=88.935 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:47 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.235 Prec@1=68.874 Prec@5=88.917 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=22:47 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.235 Prec@1=68.874 Prec@5=88.917 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:47 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.619 DataTime=0.394 Loss=1.235 Prec@1=68.874 Prec@5=88.917 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:48 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.863 Prec@5=88.893 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=22:48 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.863 Prec@5=88.893 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:48 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.237 Prec@1=68.863 Prec@5=88.893 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:49 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.236 Prec@1=68.865 Prec@5=88.902 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=22:49 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.236 Prec@1=68.865 Prec@5=88.902 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:49 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.236 Prec@1=68.865 Prec@5=88.902 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:50 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.238 Prec@1=68.833 Prec@5=88.882 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=22:50 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.238 Prec@1=68.833 Prec@5=88.882 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=22:50 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.238 Prec@1=68.833 Prec@5=88.882 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=22:51 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.240 Prec@1=68.785 Prec@5=88.854 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=22:51 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.240 Prec@1=68.785 Prec@5=88.854 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:51 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.618 DataTime=0.393 Loss=1.240 Prec@1=68.785 Prec@5=88.854 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:52 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.764 Prec@5=88.835 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=22:52 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.764 Prec@5=88.835 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:52 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.764 Prec@5=88.835 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:53 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.754 Prec@5=88.832 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=22:53 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.754 Prec@5=88.832 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:53 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.242 Prec@1=68.754 Prec@5=88.832 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:54 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.719 Prec@5=88.809 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=22:54 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.719 Prec@5=88.809 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:54 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.719 Prec@5=88.809 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:55 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.245 Prec@1=68.689 Prec@5=88.795 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=22:55 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.245 Prec@1=68.689 Prec@5=88.795 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:55 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.245 Prec@1=68.689 Prec@5=88.795 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:56 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.246 Prec@1=68.660 Prec@5=88.777 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=22:56 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.246 Prec@1=68.660 Prec@5=88.777 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:56 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.246 Prec@1=68.660 Prec@5=88.777 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:57 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.247 Prec@1=68.639 Prec@5=88.762 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=22:57 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.247 Prec@1=68.639 Prec@5=88.762 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:57 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.247 Prec@1=68.639 Prec@5=88.762 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:58 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.248 Prec@1=68.625 Prec@5=88.749 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=22:58 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.248 Prec@1=68.625 Prec@5=88.749 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:58 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.248 Prec@1=68.625 Prec@5=88.749 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:59 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.249 Prec@1=68.611 Prec@5=88.734 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=22:59 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.249 Prec@1=68.611 Prec@5=88.734 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=22:59 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.617 DataTime=0.392 Loss=1.249 Prec@1=68.611 Prec@5=88.734 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:00 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.616 DataTime=0.391 Loss=1.249 Prec@1=68.597 Prec@5=88.727 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:00 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.616 DataTime=0.391 Loss=1.249 Prec@1=68.597 Prec@5=88.727 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:00 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.616 DataTime=0.391 Loss=1.249 Prec@1=68.597 Prec@5=88.727 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:00 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.616 DataTime=0.391 Loss=1.249 Prec@1=68.596 Prec@5=88.727 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:00 IST=> training   100.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.616 DataTime=0.391 Loss=1.249 Prec@1=68.596 Prec@5=88.727 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=23:00 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST=> validation 0.00% of 1x98...Epoch=74/150 LR=0.05209 Time=7.378 Loss=1.325 Prec@1=67.969 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=23:00 IST=> validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=7.378 Loss=1.325 Prec@1=67.969 Prec@5=87.500 rate=5472.11 Hz, eta=0:00:00, total=0:00:00, wall=23:00 IST** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=7.378 Loss=1.325 Prec@1=67.969 Prec@5=87.500 rate=5472.11 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=0.410 Loss=1.387 Prec@1=65.668 Prec@5=87.284 rate=5472.11 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST** validation 100.00% of 1x98...Epoch=74/150 LR=0.05209 Time=0.410 Loss=1.387 Prec@1=65.668 Prec@5=87.284 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=23:01 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST=> training   0.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=5.283 DataTime=4.871 Loss=1.255 Prec@1=67.578 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=5.283 DataTime=4.871 Loss=1.255 Prec@1=67.578 Prec@5=89.258 rate=4860.43 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=5.283 DataTime=4.871 Loss=1.255 Prec@1=67.578 Prec@5=89.258 rate=4860.43 Hz, eta=0:00:00, total=0:00:00, wall=23:02 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.659 DataTime=0.431 Loss=1.209 Prec@1=69.301 Prec@5=89.422 rate=4860.43 Hz, eta=0:00:00, total=0:00:00, wall=23:02 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.659 DataTime=0.431 Loss=1.209 Prec@1=69.301 Prec@5=89.422 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=23:02 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.659 DataTime=0.431 Loss=1.209 Prec@1=69.301 Prec@5=89.422 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=23:03 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.637 DataTime=0.411 Loss=1.207 Prec@1=69.388 Prec@5=89.264 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=23:03 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.637 DataTime=0.411 Loss=1.207 Prec@1=69.388 Prec@5=89.264 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:03 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.637 DataTime=0.411 Loss=1.207 Prec@1=69.388 Prec@5=89.264 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:04 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.629 DataTime=0.404 Loss=1.213 Prec@1=69.268 Prec@5=89.215 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:04 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.629 DataTime=0.404 Loss=1.213 Prec@1=69.268 Prec@5=89.215 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:04 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.629 DataTime=0.404 Loss=1.213 Prec@1=69.268 Prec@5=89.215 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:05 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.626 DataTime=0.400 Loss=1.215 Prec@1=69.277 Prec@5=89.171 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=23:05 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.626 DataTime=0.400 Loss=1.215 Prec@1=69.277 Prec@5=89.171 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=23:05 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.626 DataTime=0.400 Loss=1.215 Prec@1=69.277 Prec@5=89.171 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=23:06 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.623 DataTime=0.398 Loss=1.217 Prec@1=69.246 Prec@5=89.165 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=23:06 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.623 DataTime=0.398 Loss=1.217 Prec@1=69.246 Prec@5=89.165 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=23:06 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.623 DataTime=0.398 Loss=1.217 Prec@1=69.246 Prec@5=89.165 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=23:07 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.622 DataTime=0.396 Loss=1.219 Prec@1=69.187 Prec@5=89.131 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=23:07 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.622 DataTime=0.396 Loss=1.219 Prec@1=69.187 Prec@5=89.131 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=23:07 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.622 DataTime=0.396 Loss=1.219 Prec@1=69.187 Prec@5=89.131 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=23:08 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.621 DataTime=0.395 Loss=1.222 Prec@1=69.131 Prec@5=89.088 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=23:08 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.621 DataTime=0.395 Loss=1.222 Prec@1=69.131 Prec@5=89.088 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=23:08 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.621 DataTime=0.395 Loss=1.222 Prec@1=69.131 Prec@5=89.088 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=23:09 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.620 DataTime=0.395 Loss=1.226 Prec@1=69.088 Prec@5=89.047 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=23:09 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.620 DataTime=0.395 Loss=1.226 Prec@1=69.088 Prec@5=89.047 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=23:09 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.620 DataTime=0.395 Loss=1.226 Prec@1=69.088 Prec@5=89.047 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=23:10 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.226 Prec@1=69.053 Prec@5=89.038 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=23:10 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.226 Prec@1=69.053 Prec@5=89.038 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:10 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.226 Prec@1=69.053 Prec@5=89.038 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:11 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.227 Prec@1=69.057 Prec@5=89.013 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:11 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.227 Prec@1=69.057 Prec@5=89.013 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=23:11 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.619 DataTime=0.394 Loss=1.227 Prec@1=69.057 Prec@5=89.013 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=23:12 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.228 Prec@1=69.024 Prec@5=88.989 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=23:12 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.228 Prec@1=69.024 Prec@5=88.989 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:12 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.228 Prec@1=69.024 Prec@5=88.989 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:13 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.230 Prec@1=68.986 Prec@5=88.953 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=23:13 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.230 Prec@1=68.986 Prec@5=88.953 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:13 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.230 Prec@1=68.986 Prec@5=88.953 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:14 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.232 Prec@1=68.963 Prec@5=88.922 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=23:14 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.232 Prec@1=68.963 Prec@5=88.922 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:14 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.393 Loss=1.232 Prec@1=68.963 Prec@5=88.922 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:16 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.392 Loss=1.233 Prec@1=68.943 Prec@5=88.907 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:16 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.392 Loss=1.233 Prec@1=68.943 Prec@5=88.907 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=23:16 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.618 DataTime=0.392 Loss=1.233 Prec@1=68.943 Prec@5=88.907 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=23:17 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.234 Prec@1=68.933 Prec@5=88.891 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=23:17 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.234 Prec@1=68.933 Prec@5=88.891 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=23:17 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.234 Prec@1=68.933 Prec@5=88.891 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=23:18 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.236 Prec@1=68.898 Prec@5=88.873 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=23:18 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.236 Prec@1=68.898 Prec@5=88.873 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:18 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.236 Prec@1=68.898 Prec@5=88.873 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:19 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.237 Prec@1=68.872 Prec@5=88.856 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:19 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.237 Prec@1=68.872 Prec@5=88.856 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:19 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.237 Prec@1=68.872 Prec@5=88.856 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:20 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.840 Prec@5=88.839 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=23:20 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.840 Prec@5=88.839 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=23:20 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.840 Prec@5=88.839 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=23:21 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.834 Prec@5=88.834 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=23:21 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.834 Prec@5=88.834 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=23:21 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.239 Prec@1=68.834 Prec@5=88.834 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=23:22 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.802 Prec@5=88.814 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=23:22 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.802 Prec@5=88.814 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:22 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.241 Prec@1=68.802 Prec@5=88.814 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:23 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.764 Prec@5=88.791 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:23 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.764 Prec@5=88.791 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=23:23 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.764 Prec@5=88.791 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=23:24 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.758 Prec@5=88.780 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=23:24 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.758 Prec@5=88.780 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:24 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.617 DataTime=0.392 Loss=1.243 Prec@1=68.758 Prec@5=88.780 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:25 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.244 Prec@1=68.735 Prec@5=88.770 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:25 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.244 Prec@1=68.735 Prec@5=88.770 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=23:25 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.244 Prec@1=68.735 Prec@5=88.770 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=23:26 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.245 Prec@1=68.718 Prec@5=88.754 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=23:26 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.245 Prec@1=68.718 Prec@5=88.754 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:26 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.245 Prec@1=68.718 Prec@5=88.754 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:27 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.246 Prec@1=68.706 Prec@5=88.735 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:27 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.246 Prec@1=68.706 Prec@5=88.735 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:27 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.246 Prec@1=68.706 Prec@5=88.735 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:27 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.246 Prec@1=68.708 Prec@5=88.735 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:27 IST=> training   100.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.616 DataTime=0.391 Loss=1.246 Prec@1=68.708 Prec@5=88.735 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=23:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:27 IST=> validation 0.00% of 1x98...Epoch=75/150 LR=0.05105 Time=6.882 Loss=1.351 Prec@1=67.188 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=23:27 IST=> validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.882 Loss=1.351 Prec@1=67.188 Prec@5=86.719 rate=3862.03 Hz, eta=0:00:00, total=0:00:00, wall=23:27 IST** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.882 Loss=1.351 Prec@1=67.188 Prec@5=86.719 rate=3862.03 Hz, eta=0:00:00, total=0:00:00, wall=23:27 IST** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=0.412 Loss=1.371 Prec@1=66.242 Prec@5=87.558 rate=3862.03 Hz, eta=0:00:00, total=0:00:00, wall=23:27 IST** validation 100.00% of 1x98...Epoch=75/150 LR=0.05105 Time=0.412 Loss=1.371 Prec@1=66.242 Prec@5=87.558 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=23:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:28 IST=> training   0.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.716 DataTime=5.326 Loss=1.169 Prec@1=69.336 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=23:28 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.716 DataTime=5.326 Loss=1.169 Prec@1=69.336 Prec@5=89.258 rate=6424.13 Hz, eta=0:00:00, total=0:00:00, wall=23:28 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=5.716 DataTime=5.326 Loss=1.169 Prec@1=69.336 Prec@5=89.258 rate=6424.13 Hz, eta=0:00:00, total=0:00:00, wall=23:29 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.662 DataTime=0.435 Loss=1.187 Prec@1=70.042 Prec@5=89.608 rate=6424.13 Hz, eta=0:00:00, total=0:00:00, wall=23:29 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.662 DataTime=0.435 Loss=1.187 Prec@1=70.042 Prec@5=89.608 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:29 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.662 DataTime=0.435 Loss=1.187 Prec@1=70.042 Prec@5=89.608 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:30 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.639 DataTime=0.412 Loss=1.198 Prec@1=69.839 Prec@5=89.440 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:30 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.639 DataTime=0.412 Loss=1.198 Prec@1=69.839 Prec@5=89.440 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:30 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.639 DataTime=0.412 Loss=1.198 Prec@1=69.839 Prec@5=89.440 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:31 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.629 DataTime=0.404 Loss=1.198 Prec@1=69.804 Prec@5=89.464 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:31 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.629 DataTime=0.404 Loss=1.198 Prec@1=69.804 Prec@5=89.464 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=23:31 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.629 DataTime=0.404 Loss=1.198 Prec@1=69.804 Prec@5=89.464 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=23:32 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.625 DataTime=0.400 Loss=1.200 Prec@1=69.785 Prec@5=89.368 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=23:32 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.625 DataTime=0.400 Loss=1.200 Prec@1=69.785 Prec@5=89.368 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=23:32 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.625 DataTime=0.400 Loss=1.200 Prec@1=69.785 Prec@5=89.368 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=23:33 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.623 DataTime=0.398 Loss=1.202 Prec@1=69.692 Prec@5=89.328 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=23:33 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.623 DataTime=0.398 Loss=1.202 Prec@1=69.692 Prec@5=89.328 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=23:33 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.623 DataTime=0.398 Loss=1.202 Prec@1=69.692 Prec@5=89.328 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=23:34 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.621 DataTime=0.396 Loss=1.208 Prec@1=69.581 Prec@5=89.237 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=23:34 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.621 DataTime=0.396 Loss=1.208 Prec@1=69.581 Prec@5=89.237 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=23:34 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.621 DataTime=0.396 Loss=1.208 Prec@1=69.581 Prec@5=89.237 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=23:35 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.395 Loss=1.212 Prec@1=69.454 Prec@5=89.183 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=23:35 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.395 Loss=1.212 Prec@1=69.454 Prec@5=89.183 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:35 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.395 Loss=1.212 Prec@1=69.454 Prec@5=89.183 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:36 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.394 Loss=1.214 Prec@1=69.396 Prec@5=89.158 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:36 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.394 Loss=1.214 Prec@1=69.396 Prec@5=89.158 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:36 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.620 DataTime=0.394 Loss=1.214 Prec@1=69.396 Prec@5=89.158 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:37 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.394 Loss=1.216 Prec@1=69.327 Prec@5=89.124 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=23:37 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.394 Loss=1.216 Prec@1=69.327 Prec@5=89.124 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:37 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.394 Loss=1.216 Prec@1=69.327 Prec@5=89.124 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:38 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.393 Loss=1.218 Prec@1=69.278 Prec@5=89.101 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=23:38 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.393 Loss=1.218 Prec@1=69.278 Prec@5=89.101 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:38 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.619 DataTime=0.393 Loss=1.218 Prec@1=69.278 Prec@5=89.101 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:39 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.249 Prec@5=89.088 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=23:39 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.249 Prec@5=89.088 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:39 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.249 Prec@5=89.088 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:40 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.251 Prec@5=89.076 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:40 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.251 Prec@5=89.076 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:40 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.219 Prec@1=69.251 Prec@5=89.076 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:41 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.221 Prec@1=69.227 Prec@5=89.062 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:41 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.221 Prec@1=69.227 Prec@5=89.062 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:41 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.393 Loss=1.221 Prec@1=69.227 Prec@5=89.062 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:42 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.392 Loss=1.222 Prec@1=69.206 Prec@5=89.039 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:42 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.392 Loss=1.222 Prec@1=69.206 Prec@5=89.039 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:42 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.618 DataTime=0.392 Loss=1.222 Prec@1=69.206 Prec@5=89.039 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:43 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.186 Prec@5=89.019 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:43 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.186 Prec@5=89.019 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:43 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.186 Prec@5=89.019 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:44 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.225 Prec@1=69.157 Prec@5=88.989 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:44 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.225 Prec@1=69.157 Prec@5=88.989 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:44 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.225 Prec@1=69.157 Prec@5=88.989 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:45 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.226 Prec@1=69.137 Prec@5=88.967 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:45 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.226 Prec@1=69.137 Prec@5=88.967 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:45 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.226 Prec@1=69.137 Prec@5=88.967 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:46 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.227 Prec@1=69.099 Prec@5=88.959 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:46 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.227 Prec@1=69.099 Prec@5=88.959 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:46 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.227 Prec@1=69.099 Prec@5=88.959 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:47 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.228 Prec@1=69.096 Prec@5=88.945 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:47 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.228 Prec@1=69.096 Prec@5=88.945 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:47 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.228 Prec@1=69.096 Prec@5=88.945 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:48 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.229 Prec@1=69.067 Prec@5=88.941 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=23:48 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.229 Prec@1=69.067 Prec@5=88.941 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:48 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.229 Prec@1=69.067 Prec@5=88.941 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:49 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.230 Prec@1=69.043 Prec@5=88.941 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=23:49 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.230 Prec@1=69.043 Prec@5=88.941 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:49 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.392 Loss=1.230 Prec@1=69.043 Prec@5=88.941 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:50 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.391 Loss=1.231 Prec@1=69.031 Prec@5=88.924 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=23:50 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.391 Loss=1.231 Prec@1=69.031 Prec@5=88.924 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:50 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.617 DataTime=0.391 Loss=1.231 Prec@1=69.031 Prec@5=88.924 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:51 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.232 Prec@1=69.009 Prec@5=88.918 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=23:51 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.232 Prec@1=69.009 Prec@5=88.918 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=23:51 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.232 Prec@1=69.009 Prec@5=88.918 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=23:52 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.233 Prec@1=68.991 Prec@5=88.906 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=23:52 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.233 Prec@1=68.991 Prec@5=88.906 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:52 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.233 Prec@1=68.991 Prec@5=88.906 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:53 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.234 Prec@1=68.965 Prec@5=88.897 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=23:53 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.234 Prec@1=68.965 Prec@5=88.897 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:53 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.234 Prec@1=68.965 Prec@5=88.897 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:53 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.234 Prec@1=68.965 Prec@5=88.897 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=23:53 IST=> training   100.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.616 DataTime=0.391 Loss=1.234 Prec@1=68.965 Prec@5=88.897 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=23:53 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:53 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:53 IST=> validation 0.00% of 1x98...Epoch=76/150 LR=0.05000 Time=6.966 Loss=1.447 Prec@1=65.625 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=23:53 IST=> validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.966 Loss=1.447 Prec@1=65.625 Prec@5=87.891 rate=3541.24 Hz, eta=0:00:00, total=0:00:00, wall=23:53 IST** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.966 Loss=1.447 Prec@1=65.625 Prec@5=87.891 rate=3541.24 Hz, eta=0:00:00, total=0:00:00, wall=23:54 IST** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=0.405 Loss=1.346 Prec@1=66.486 Prec@5=87.942 rate=3541.24 Hz, eta=0:00:00, total=0:00:00, wall=23:54 IST** validation 100.00% of 1x98...Epoch=76/150 LR=0.05000 Time=0.405 Loss=1.346 Prec@1=66.486 Prec@5=87.942 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=23:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:54 IST=> training   0.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.977 DataTime=4.599 Loss=1.180 Prec@1=68.945 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=23:54 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.977 DataTime=4.599 Loss=1.180 Prec@1=68.945 Prec@5=91.211 rate=7423.62 Hz, eta=0:00:00, total=0:00:00, wall=23:54 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=4.977 DataTime=4.599 Loss=1.180 Prec@1=68.945 Prec@5=91.211 rate=7423.62 Hz, eta=0:00:00, total=0:00:00, wall=23:55 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.662 DataTime=0.435 Loss=1.185 Prec@1=69.752 Prec@5=89.602 rate=7423.62 Hz, eta=0:00:00, total=0:00:00, wall=23:55 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.662 DataTime=0.435 Loss=1.185 Prec@1=69.752 Prec@5=89.602 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=23:55 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.662 DataTime=0.435 Loss=1.185 Prec@1=69.752 Prec@5=89.602 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=23:56 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.639 DataTime=0.413 Loss=1.189 Prec@1=69.796 Prec@5=89.493 rate=1.63 Hz, eta=0:24:32, total=0:01:01, wall=23:56 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.639 DataTime=0.413 Loss=1.189 Prec@1=69.796 Prec@5=89.493 rate=1.63 Hz, eta=0:23:33, total=0:02:03, wall=23:56 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.639 DataTime=0.413 Loss=1.189 Prec@1=69.796 Prec@5=89.493 rate=1.63 Hz, eta=0:23:33, total=0:02:03, wall=23:57 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.630 DataTime=0.405 Loss=1.193 Prec@1=69.688 Prec@5=89.436 rate=1.63 Hz, eta=0:23:33, total=0:02:03, wall=23:57 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.630 DataTime=0.405 Loss=1.193 Prec@1=69.688 Prec@5=89.436 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=23:57 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.630 DataTime=0.405 Loss=1.193 Prec@1=69.688 Prec@5=89.436 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=23:58 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.626 DataTime=0.401 Loss=1.198 Prec@1=69.665 Prec@5=89.376 rate=1.63 Hz, eta=0:22:31, total=0:03:04, wall=23:58 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.626 DataTime=0.401 Loss=1.198 Prec@1=69.665 Prec@5=89.376 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=23:58 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.626 DataTime=0.401 Loss=1.198 Prec@1=69.665 Prec@5=89.376 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=23:59 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.624 DataTime=0.399 Loss=1.197 Prec@1=69.693 Prec@5=89.381 rate=1.63 Hz, eta=0:21:30, total=0:04:06, wall=23:59 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.624 DataTime=0.399 Loss=1.197 Prec@1=69.693 Prec@5=89.381 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=23:59 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.624 DataTime=0.399 Loss=1.197 Prec@1=69.693 Prec@5=89.381 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=00:00 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.622 DataTime=0.397 Loss=1.201 Prec@1=69.624 Prec@5=89.324 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=00:00 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.622 DataTime=0.397 Loss=1.201 Prec@1=69.624 Prec@5=89.324 rate=1.63 Hz, eta=0:19:27, total=0:06:09, wall=00:00 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.622 DataTime=0.397 Loss=1.201 Prec@1=69.624 Prec@5=89.324 rate=1.63 Hz, eta=0:19:27, total=0:06:09, wall=00:01 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.621 DataTime=0.396 Loss=1.202 Prec@1=69.607 Prec@5=89.317 rate=1.63 Hz, eta=0:19:27, total=0:06:09, wall=00:01 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.621 DataTime=0.396 Loss=1.202 Prec@1=69.607 Prec@5=89.317 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=00:01 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.621 DataTime=0.396 Loss=1.202 Prec@1=69.607 Prec@5=89.317 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=00:02 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.396 Loss=1.200 Prec@1=69.641 Prec@5=89.336 rate=1.63 Hz, eta=0:18:26, total=0:07:10, wall=00:02 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.396 Loss=1.200 Prec@1=69.641 Prec@5=89.336 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=00:02 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.396 Loss=1.200 Prec@1=69.641 Prec@5=89.336 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=00:03 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.395 Loss=1.203 Prec@1=69.584 Prec@5=89.308 rate=1.63 Hz, eta=0:17:25, total=0:08:11, wall=00:03 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.395 Loss=1.203 Prec@1=69.584 Prec@5=89.308 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:03 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.620 DataTime=0.395 Loss=1.203 Prec@1=69.584 Prec@5=89.308 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:04 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.206 Prec@1=69.526 Prec@5=89.288 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=00:04 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.206 Prec@1=69.526 Prec@5=89.288 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:04 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.206 Prec@1=69.526 Prec@5=89.288 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:05 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.208 Prec@1=69.465 Prec@5=89.257 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=00:05 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.208 Prec@1=69.465 Prec@5=89.257 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=00:05 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.208 Prec@1=69.465 Prec@5=89.257 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=00:06 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.210 Prec@1=69.417 Prec@5=89.237 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=00:06 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.210 Prec@1=69.417 Prec@5=89.237 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:06 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.619 DataTime=0.394 Loss=1.210 Prec@1=69.417 Prec@5=89.237 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:07 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.211 Prec@1=69.407 Prec@5=89.217 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:07 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.211 Prec@1=69.407 Prec@5=89.217 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=00:07 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.211 Prec@1=69.407 Prec@5=89.217 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=00:08 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.212 Prec@1=69.384 Prec@5=89.209 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=00:08 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.212 Prec@1=69.384 Prec@5=89.209 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=00:08 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.212 Prec@1=69.384 Prec@5=89.209 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=00:09 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.214 Prec@1=69.338 Prec@5=89.184 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=00:09 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.214 Prec@1=69.338 Prec@5=89.184 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=00:09 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.393 Loss=1.214 Prec@1=69.338 Prec@5=89.184 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=00:10 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.392 Loss=1.215 Prec@1=69.318 Prec@5=89.175 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=00:10 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.392 Loss=1.215 Prec@1=69.318 Prec@5=89.175 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:10 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.618 DataTime=0.392 Loss=1.215 Prec@1=69.318 Prec@5=89.175 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:11 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.216 Prec@1=69.294 Prec@5=89.157 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=00:11 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.216 Prec@1=69.294 Prec@5=89.157 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=00:11 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.216 Prec@1=69.294 Prec@5=89.157 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=00:12 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.218 Prec@1=69.269 Prec@5=89.135 rate=1.63 Hz, eta=0:08:12, total=0:17:25, wall=00:12 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.218 Prec@1=69.269 Prec@5=89.135 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:12 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.218 Prec@1=69.269 Prec@5=89.135 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:13 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.219 Prec@1=69.233 Prec@5=89.117 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=00:13 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.219 Prec@1=69.233 Prec@5=89.117 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=00:13 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.219 Prec@1=69.233 Prec@5=89.117 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=00:15 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.221 Prec@1=69.215 Prec@5=89.100 rate=1.63 Hz, eta=0:06:09, total=0:19:28, wall=00:15 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.221 Prec@1=69.215 Prec@5=89.100 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:15 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.221 Prec@1=69.215 Prec@5=89.100 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:16 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.222 Prec@1=69.190 Prec@5=89.075 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=00:16 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.222 Prec@1=69.190 Prec@5=89.075 rate=1.63 Hz, eta=0:04:07, total=0:21:31, wall=00:16 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.222 Prec@1=69.190 Prec@5=89.075 rate=1.63 Hz, eta=0:04:07, total=0:21:31, wall=00:17 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.158 Prec@5=89.054 rate=1.63 Hz, eta=0:04:07, total=0:21:31, wall=00:17 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.158 Prec@5=89.054 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:17 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.223 Prec@1=69.158 Prec@5=89.054 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:18 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.224 Prec@1=69.135 Prec@5=89.043 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=00:18 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.224 Prec@1=69.135 Prec@5=89.043 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:18 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.392 Loss=1.224 Prec@1=69.135 Prec@5=89.043 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:19 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.391 Loss=1.225 Prec@1=69.115 Prec@5=89.030 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=00:19 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.391 Loss=1.225 Prec@1=69.115 Prec@5=89.030 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=00:19 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.617 DataTime=0.391 Loss=1.225 Prec@1=69.115 Prec@5=89.030 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=00:20 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.616 DataTime=0.391 Loss=1.226 Prec@1=69.108 Prec@5=89.017 rate=1.63 Hz, eta=0:01:02, total=0:24:35, wall=00:20 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.616 DataTime=0.391 Loss=1.226 Prec@1=69.108 Prec@5=89.017 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=00:20 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.616 DataTime=0.391 Loss=1.226 Prec@1=69.108 Prec@5=89.017 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=00:20 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.616 DataTime=0.391 Loss=1.226 Prec@1=69.108 Prec@5=89.018 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=00:20 IST=> training   100.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.616 DataTime=0.391 Loss=1.226 Prec@1=69.108 Prec@5=89.018 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=00:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> validation 0.00% of 1x98...Epoch=77/150 LR=0.04895 Time=6.507 Loss=1.362 Prec@1=65.234 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.507 Loss=1.362 Prec@1=65.234 Prec@5=87.695 rate=2514.12 Hz, eta=0:00:00, total=0:00:00, wall=00:20 IST** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.507 Loss=1.362 Prec@1=65.234 Prec@5=87.695 rate=2514.12 Hz, eta=0:00:00, total=0:00:00, wall=00:20 IST** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=0.408 Loss=1.382 Prec@1=65.844 Prec@5=87.408 rate=2514.12 Hz, eta=0:00:00, total=0:00:00, wall=00:20 IST** validation 100.00% of 1x98...Epoch=77/150 LR=0.04895 Time=0.408 Loss=1.382 Prec@1=65.844 Prec@5=87.408 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=00:20 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> training   0.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.428 DataTime=4.186 Loss=1.143 Prec@1=70.117 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=00:20 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.428 DataTime=4.186 Loss=1.143 Prec@1=70.117 Prec@5=90.625 rate=5972.40 Hz, eta=0:00:00, total=0:00:00, wall=00:20 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=4.428 DataTime=4.186 Loss=1.143 Prec@1=70.117 Prec@5=90.625 rate=5972.40 Hz, eta=0:00:00, total=0:00:00, wall=00:21 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.654 DataTime=0.429 Loss=1.192 Prec@1=69.949 Prec@5=89.333 rate=5972.40 Hz, eta=0:00:00, total=0:00:00, wall=00:21 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.654 DataTime=0.429 Loss=1.192 Prec@1=69.949 Prec@5=89.333 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=00:21 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.654 DataTime=0.429 Loss=1.192 Prec@1=69.949 Prec@5=89.333 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=00:22 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.634 DataTime=0.409 Loss=1.187 Prec@1=69.996 Prec@5=89.475 rate=1.64 Hz, eta=0:24:25, total=0:01:01, wall=00:22 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.634 DataTime=0.409 Loss=1.187 Prec@1=69.996 Prec@5=89.475 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=00:22 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.634 DataTime=0.409 Loss=1.187 Prec@1=69.996 Prec@5=89.475 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=00:24 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.627 DataTime=0.402 Loss=1.192 Prec@1=69.924 Prec@5=89.419 rate=1.63 Hz, eta=0:23:29, total=0:02:03, wall=00:24 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.627 DataTime=0.402 Loss=1.192 Prec@1=69.924 Prec@5=89.419 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=00:24 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.627 DataTime=0.402 Loss=1.192 Prec@1=69.924 Prec@5=89.419 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=00:25 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.624 DataTime=0.399 Loss=1.193 Prec@1=69.842 Prec@5=89.420 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=00:25 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.624 DataTime=0.399 Loss=1.193 Prec@1=69.842 Prec@5=89.420 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=00:25 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.624 DataTime=0.399 Loss=1.193 Prec@1=69.842 Prec@5=89.420 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=00:26 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.622 DataTime=0.397 Loss=1.194 Prec@1=69.782 Prec@5=89.414 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=00:26 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.622 DataTime=0.397 Loss=1.194 Prec@1=69.782 Prec@5=89.414 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:26 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.622 DataTime=0.397 Loss=1.194 Prec@1=69.782 Prec@5=89.414 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:27 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.621 DataTime=0.396 Loss=1.195 Prec@1=69.760 Prec@5=89.393 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=00:27 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.621 DataTime=0.396 Loss=1.195 Prec@1=69.760 Prec@5=89.393 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:27 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.621 DataTime=0.396 Loss=1.195 Prec@1=69.760 Prec@5=89.393 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:28 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.620 DataTime=0.395 Loss=1.198 Prec@1=69.662 Prec@5=89.351 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=00:28 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.620 DataTime=0.395 Loss=1.198 Prec@1=69.662 Prec@5=89.351 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=00:28 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.620 DataTime=0.395 Loss=1.198 Prec@1=69.662 Prec@5=89.351 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=00:29 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.653 Prec@5=89.356 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=00:29 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.653 Prec@5=89.356 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:29 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.653 Prec@5=89.356 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:30 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.672 Prec@5=89.347 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=00:30 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.672 Prec@5=89.347 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=00:30 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.619 DataTime=0.394 Loss=1.200 Prec@1=69.672 Prec@5=89.347 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=00:31 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.202 Prec@1=69.627 Prec@5=89.323 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=00:31 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.202 Prec@1=69.627 Prec@5=89.323 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:31 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.202 Prec@1=69.627 Prec@5=89.323 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:32 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.203 Prec@1=69.607 Prec@5=89.310 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:32 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.203 Prec@1=69.607 Prec@5=89.310 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:32 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.393 Loss=1.203 Prec@1=69.607 Prec@5=89.310 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:33 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.392 Loss=1.205 Prec@1=69.534 Prec@5=89.299 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:33 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.392 Loss=1.205 Prec@1=69.534 Prec@5=89.299 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:33 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.618 DataTime=0.392 Loss=1.205 Prec@1=69.534 Prec@5=89.299 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:34 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.206 Prec@1=69.538 Prec@5=89.285 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=00:34 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.206 Prec@1=69.538 Prec@5=89.285 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:34 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.206 Prec@1=69.538 Prec@5=89.285 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:35 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.207 Prec@1=69.496 Prec@5=89.270 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:35 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.207 Prec@1=69.496 Prec@5=89.270 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:35 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.207 Prec@1=69.496 Prec@5=89.270 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:36 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.209 Prec@1=69.447 Prec@5=89.239 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=00:36 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.209 Prec@1=69.447 Prec@5=89.239 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:36 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.392 Loss=1.209 Prec@1=69.447 Prec@5=89.239 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:37 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.391 Loss=1.209 Prec@1=69.443 Prec@5=89.237 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=00:37 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.391 Loss=1.209 Prec@1=69.443 Prec@5=89.237 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:37 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.617 DataTime=0.391 Loss=1.209 Prec@1=69.443 Prec@5=89.237 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:38 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.210 Prec@1=69.429 Prec@5=89.227 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:38 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.210 Prec@1=69.429 Prec@5=89.227 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:38 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.210 Prec@1=69.429 Prec@5=89.227 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:39 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.212 Prec@1=69.400 Prec@5=89.208 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:39 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.212 Prec@1=69.400 Prec@5=89.208 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:39 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.212 Prec@1=69.400 Prec@5=89.208 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:40 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.213 Prec@1=69.377 Prec@5=89.192 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:40 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.213 Prec@1=69.377 Prec@5=89.192 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:40 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.213 Prec@1=69.377 Prec@5=89.192 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:41 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.214 Prec@1=69.359 Prec@5=89.177 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:41 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.214 Prec@1=69.359 Prec@5=89.177 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:41 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.214 Prec@1=69.359 Prec@5=89.177 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:42 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.216 Prec@1=69.322 Prec@5=89.146 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:42 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.216 Prec@1=69.322 Prec@5=89.146 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:42 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.216 Prec@1=69.322 Prec@5=89.146 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:43 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.308 Prec@5=89.137 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:43 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.308 Prec@5=89.137 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:43 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.308 Prec@5=89.137 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:44 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.283 Prec@5=89.133 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:44 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.283 Prec@5=89.133 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:44 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.217 Prec@1=69.283 Prec@5=89.133 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:45 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.218 Prec@1=69.267 Prec@5=89.118 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:45 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.218 Prec@1=69.267 Prec@5=89.118 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:45 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.218 Prec@1=69.267 Prec@5=89.118 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:46 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.220 Prec@1=69.248 Prec@5=89.108 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:46 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.220 Prec@1=69.248 Prec@5=89.108 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:46 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.220 Prec@1=69.248 Prec@5=89.108 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:46 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.220 Prec@1=69.249 Prec@5=89.108 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:46 IST=> training   100.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.616 DataTime=0.391 Loss=1.220 Prec@1=69.249 Prec@5=89.108 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=00:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST=> validation 0.00% of 1x98...Epoch=78/150 LR=0.04791 Time=7.166 Loss=1.247 Prec@1=67.383 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=00:46 IST=> validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=7.166 Loss=1.247 Prec@1=67.383 Prec@5=89.062 rate=6323.20 Hz, eta=0:00:00, total=0:00:00, wall=00:46 IST** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=7.166 Loss=1.247 Prec@1=67.383 Prec@5=89.062 rate=6323.20 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=0.407 Loss=1.331 Prec@1=67.030 Prec@5=88.100 rate=6323.20 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST** validation 100.00% of 1x98...Epoch=78/150 LR=0.04791 Time=0.407 Loss=1.331 Prec@1=67.030 Prec@5=88.100 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=00:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST=> training   0.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.173 DataTime=4.863 Loss=1.162 Prec@1=70.312 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=00:47 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.173 DataTime=4.863 Loss=1.162 Prec@1=70.312 Prec@5=90.625 rate=7015.67 Hz, eta=0:00:00, total=0:00:00, wall=00:47 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=5.173 DataTime=4.863 Loss=1.162 Prec@1=70.312 Prec@5=90.625 rate=7015.67 Hz, eta=0:00:00, total=0:00:00, wall=00:48 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.654 DataTime=0.430 Loss=1.182 Prec@1=70.353 Prec@5=89.525 rate=7015.67 Hz, eta=0:00:00, total=0:00:00, wall=00:48 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.654 DataTime=0.430 Loss=1.182 Prec@1=70.353 Prec@5=89.525 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=00:48 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.654 DataTime=0.430 Loss=1.182 Prec@1=70.353 Prec@5=89.525 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=00:49 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.633 DataTime=0.410 Loss=1.179 Prec@1=70.269 Prec@5=89.538 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=00:49 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.633 DataTime=0.410 Loss=1.179 Prec@1=70.269 Prec@5=89.538 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=00:49 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.633 DataTime=0.410 Loss=1.179 Prec@1=70.269 Prec@5=89.538 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=00:50 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.626 DataTime=0.403 Loss=1.181 Prec@1=70.144 Prec@5=89.541 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=00:50 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.626 DataTime=0.403 Loss=1.181 Prec@1=70.144 Prec@5=89.541 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=00:50 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.626 DataTime=0.403 Loss=1.181 Prec@1=70.144 Prec@5=89.541 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=00:51 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.623 DataTime=0.400 Loss=1.190 Prec@1=70.006 Prec@5=89.418 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=00:51 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.623 DataTime=0.400 Loss=1.190 Prec@1=70.006 Prec@5=89.418 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=00:51 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.623 DataTime=0.400 Loss=1.190 Prec@1=70.006 Prec@5=89.418 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=00:52 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.621 DataTime=0.397 Loss=1.191 Prec@1=69.957 Prec@5=89.429 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=00:52 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.621 DataTime=0.397 Loss=1.191 Prec@1=69.957 Prec@5=89.429 rate=1.64 Hz, eta=0:20:22, total=0:05:06, wall=00:52 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.621 DataTime=0.397 Loss=1.191 Prec@1=69.957 Prec@5=89.429 rate=1.64 Hz, eta=0:20:22, total=0:05:06, wall=00:53 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.620 DataTime=0.396 Loss=1.190 Prec@1=69.968 Prec@5=89.412 rate=1.64 Hz, eta=0:20:22, total=0:05:06, wall=00:53 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.620 DataTime=0.396 Loss=1.190 Prec@1=69.968 Prec@5=89.412 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=00:53 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.620 DataTime=0.396 Loss=1.190 Prec@1=69.968 Prec@5=89.412 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=00:54 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.193 Prec@1=69.903 Prec@5=89.391 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=00:54 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.193 Prec@1=69.903 Prec@5=89.391 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=00:54 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.193 Prec@1=69.903 Prec@5=89.391 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=00:55 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.194 Prec@1=69.857 Prec@5=89.366 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=00:55 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.194 Prec@1=69.857 Prec@5=89.366 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:55 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.619 DataTime=0.395 Loss=1.194 Prec@1=69.857 Prec@5=89.366 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:56 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.195 Prec@1=69.853 Prec@5=89.369 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=00:56 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.195 Prec@1=69.853 Prec@5=89.369 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=00:56 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.195 Prec@1=69.853 Prec@5=89.369 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=00:57 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.196 Prec@1=69.811 Prec@5=89.355 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=00:57 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.196 Prec@1=69.811 Prec@5=89.355 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:57 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.394 Loss=1.196 Prec@1=69.811 Prec@5=89.355 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:58 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.393 Loss=1.197 Prec@1=69.773 Prec@5=89.343 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=00:58 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.393 Loss=1.197 Prec@1=69.773 Prec@5=89.343 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:58 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.618 DataTime=0.393 Loss=1.197 Prec@1=69.773 Prec@5=89.343 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:59 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.199 Prec@1=69.748 Prec@5=89.309 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=00:59 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.199 Prec@1=69.748 Prec@5=89.309 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:59 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.199 Prec@1=69.748 Prec@5=89.309 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:00 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.200 Prec@1=69.721 Prec@5=89.296 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:00 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.200 Prec@1=69.721 Prec@5=89.296 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=01:00 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.200 Prec@1=69.721 Prec@5=89.296 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=01:01 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.201 Prec@1=69.683 Prec@5=89.279 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=01:01 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.201 Prec@1=69.683 Prec@5=89.279 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=01:01 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.201 Prec@1=69.683 Prec@5=89.279 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=01:02 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.204 Prec@1=69.618 Prec@5=89.248 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=01:02 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.204 Prec@1=69.618 Prec@5=89.248 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:02 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.617 DataTime=0.393 Loss=1.204 Prec@1=69.618 Prec@5=89.248 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:03 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.205 Prec@1=69.605 Prec@5=89.239 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:03 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.205 Prec@1=69.605 Prec@5=89.239 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=01:03 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.205 Prec@1=69.605 Prec@5=89.239 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=01:04 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.206 Prec@1=69.580 Prec@5=89.230 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=01:04 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.206 Prec@1=69.580 Prec@5=89.230 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:04 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.206 Prec@1=69.580 Prec@5=89.230 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:05 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.207 Prec@1=69.550 Prec@5=89.220 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:05 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.207 Prec@1=69.550 Prec@5=89.220 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:05 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.207 Prec@1=69.550 Prec@5=89.220 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:06 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.209 Prec@1=69.514 Prec@5=89.199 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:06 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.209 Prec@1=69.514 Prec@5=89.199 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=01:06 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.209 Prec@1=69.514 Prec@5=89.199 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=01:07 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.494 Prec@5=89.188 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=01:07 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.494 Prec@5=89.188 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=01:07 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.494 Prec@5=89.188 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=01:08 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.473 Prec@5=89.176 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=01:08 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.473 Prec@5=89.176 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=01:08 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.210 Prec@1=69.473 Prec@5=89.176 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=01:09 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.212 Prec@1=69.447 Prec@5=89.165 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=01:09 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.212 Prec@1=69.447 Prec@5=89.165 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=01:09 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.212 Prec@1=69.447 Prec@5=89.165 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=01:10 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.213 Prec@1=69.424 Prec@5=89.153 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=01:10 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.213 Prec@1=69.424 Prec@5=89.153 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=01:10 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.616 DataTime=0.392 Loss=1.213 Prec@1=69.424 Prec@5=89.153 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=01:11 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.392 Loss=1.213 Prec@1=69.417 Prec@5=89.136 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=01:11 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.392 Loss=1.213 Prec@1=69.417 Prec@5=89.136 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=01:11 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.392 Loss=1.213 Prec@1=69.417 Prec@5=89.136 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=01:12 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.391 Loss=1.214 Prec@1=69.402 Prec@5=89.129 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=01:12 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.391 Loss=1.214 Prec@1=69.402 Prec@5=89.129 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=01:12 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.391 Loss=1.214 Prec@1=69.402 Prec@5=89.129 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=01:12 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.391 Loss=1.214 Prec@1=69.400 Prec@5=89.128 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=01:12 IST=> training   100.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.615 DataTime=0.391 Loss=1.214 Prec@1=69.400 Prec@5=89.128 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=01:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 0.00% of 1x98...Epoch=79/150 LR=0.04686 Time=6.777 Loss=1.354 Prec@1=66.016 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=6.777 Loss=1.354 Prec@1=66.016 Prec@5=88.477 rate=5663.76 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=6.777 Loss=1.354 Prec@1=66.016 Prec@5=88.477 rate=5663.76 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=0.404 Loss=1.315 Prec@1=67.338 Prec@5=88.104 rate=5663.76 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 100.00% of 1x98...Epoch=79/150 LR=0.04686 Time=0.404 Loss=1.315 Prec@1=67.338 Prec@5=88.104 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=01:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> training   0.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.783 DataTime=5.466 Loss=1.209 Prec@1=69.727 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.783 DataTime=5.466 Loss=1.209 Prec@1=69.727 Prec@5=87.500 rate=7251.37 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.783 DataTime=5.466 Loss=1.209 Prec@1=69.727 Prec@5=87.500 rate=7251.37 Hz, eta=0:00:00, total=0:00:00, wall=01:14 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.663 DataTime=0.438 Loss=1.175 Prec@1=70.051 Prec@5=89.693 rate=7251.37 Hz, eta=0:00:00, total=0:00:00, wall=01:14 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.663 DataTime=0.438 Loss=1.175 Prec@1=70.051 Prec@5=89.693 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=01:14 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.663 DataTime=0.438 Loss=1.175 Prec@1=70.051 Prec@5=89.693 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=01:15 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.638 DataTime=0.415 Loss=1.179 Prec@1=69.945 Prec@5=89.612 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=01:15 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.638 DataTime=0.415 Loss=1.179 Prec@1=69.945 Prec@5=89.612 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=01:15 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.638 DataTime=0.415 Loss=1.179 Prec@1=69.945 Prec@5=89.612 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=01:16 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.630 DataTime=0.406 Loss=1.180 Prec@1=69.997 Prec@5=89.600 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=01:16 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.630 DataTime=0.406 Loss=1.180 Prec@1=69.997 Prec@5=89.600 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:16 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.630 DataTime=0.406 Loss=1.180 Prec@1=69.997 Prec@5=89.600 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:17 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.626 DataTime=0.401 Loss=1.181 Prec@1=69.996 Prec@5=89.626 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=01:17 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.626 DataTime=0.401 Loss=1.181 Prec@1=69.996 Prec@5=89.626 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=01:17 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.626 DataTime=0.401 Loss=1.181 Prec@1=69.996 Prec@5=89.626 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=01:18 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.623 DataTime=0.399 Loss=1.186 Prec@1=69.866 Prec@5=89.554 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=01:18 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.623 DataTime=0.399 Loss=1.186 Prec@1=69.866 Prec@5=89.554 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:18 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.623 DataTime=0.399 Loss=1.186 Prec@1=69.866 Prec@5=89.554 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:19 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.622 DataTime=0.397 Loss=1.185 Prec@1=69.881 Prec@5=89.576 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:19 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.622 DataTime=0.397 Loss=1.185 Prec@1=69.881 Prec@5=89.576 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=01:19 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.622 DataTime=0.397 Loss=1.185 Prec@1=69.881 Prec@5=89.576 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=01:20 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.621 DataTime=0.397 Loss=1.187 Prec@1=69.887 Prec@5=89.528 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=01:20 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.621 DataTime=0.397 Loss=1.187 Prec@1=69.887 Prec@5=89.528 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:20 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.621 DataTime=0.397 Loss=1.187 Prec@1=69.887 Prec@5=89.528 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:21 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.396 Loss=1.188 Prec@1=69.863 Prec@5=89.517 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:21 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.396 Loss=1.188 Prec@1=69.863 Prec@5=89.517 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:21 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.396 Loss=1.188 Prec@1=69.863 Prec@5=89.517 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:22 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.395 Loss=1.190 Prec@1=69.823 Prec@5=89.500 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:22 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.395 Loss=1.190 Prec@1=69.823 Prec@5=89.500 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:22 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.620 DataTime=0.395 Loss=1.190 Prec@1=69.823 Prec@5=89.500 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:24 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.191 Prec@1=69.810 Prec@5=89.492 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:24 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.191 Prec@1=69.810 Prec@5=89.492 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:24 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.191 Prec@1=69.810 Prec@5=89.492 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:25 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.192 Prec@1=69.802 Prec@5=89.477 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=01:25 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.192 Prec@1=69.802 Prec@5=89.477 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=01:25 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.619 DataTime=0.394 Loss=1.192 Prec@1=69.802 Prec@5=89.477 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=01:26 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.194 Prec@1=69.783 Prec@5=89.445 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=01:26 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.194 Prec@1=69.783 Prec@5=89.445 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:26 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.194 Prec@1=69.783 Prec@5=89.445 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:27 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.765 Prec@5=89.433 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=01:27 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.765 Prec@5=89.433 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:27 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.765 Prec@5=89.433 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:28 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.748 Prec@5=89.426 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:28 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.748 Prec@5=89.426 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:28 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.618 DataTime=0.393 Loss=1.195 Prec@1=69.748 Prec@5=89.426 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:29 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.393 Loss=1.196 Prec@1=69.733 Prec@5=89.420 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:29 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.393 Loss=1.196 Prec@1=69.733 Prec@5=89.420 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:29 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.393 Loss=1.196 Prec@1=69.733 Prec@5=89.420 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:30 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.197 Prec@1=69.699 Prec@5=89.396 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=01:30 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.197 Prec@1=69.699 Prec@5=89.396 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:30 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.197 Prec@1=69.699 Prec@5=89.396 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:31 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.198 Prec@1=69.681 Prec@5=89.386 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:31 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.198 Prec@1=69.681 Prec@5=89.386 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=01:31 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.198 Prec@1=69.681 Prec@5=89.386 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=01:32 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.200 Prec@1=69.662 Prec@5=89.372 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=01:32 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.200 Prec@1=69.662 Prec@5=89.372 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:32 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.200 Prec@1=69.662 Prec@5=89.372 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:33 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.201 Prec@1=69.627 Prec@5=89.355 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:33 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.201 Prec@1=69.627 Prec@5=89.355 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:33 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.617 DataTime=0.392 Loss=1.201 Prec@1=69.627 Prec@5=89.355 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:34 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.392 Loss=1.202 Prec@1=69.611 Prec@5=89.344 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:34 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.392 Loss=1.202 Prec@1=69.611 Prec@5=89.344 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=01:34 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.392 Loss=1.202 Prec@1=69.611 Prec@5=89.344 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=01:35 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.203 Prec@1=69.598 Prec@5=89.319 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=01:35 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.203 Prec@1=69.598 Prec@5=89.319 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:35 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.203 Prec@1=69.598 Prec@5=89.319 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:36 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.204 Prec@1=69.583 Prec@5=89.301 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=01:36 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.204 Prec@1=69.583 Prec@5=89.301 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=01:36 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.204 Prec@1=69.583 Prec@5=89.301 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=01:37 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.569 Prec@5=89.284 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=01:37 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.569 Prec@5=89.284 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=01:37 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.569 Prec@5=89.284 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=01:38 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.555 Prec@5=89.271 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=01:38 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.555 Prec@5=89.271 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=01:38 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.205 Prec@1=69.555 Prec@5=89.271 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=01:39 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.206 Prec@1=69.537 Prec@5=89.258 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=01:39 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.206 Prec@1=69.537 Prec@5=89.258 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=01:39 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.206 Prec@1=69.537 Prec@5=89.258 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=01:39 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.207 Prec@1=69.537 Prec@5=89.258 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=01:39 IST=> training   100.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.616 DataTime=0.391 Loss=1.207 Prec@1=69.537 Prec@5=89.258 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=01:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST=> validation 0.00% of 1x98...Epoch=80/150 LR=0.04582 Time=6.148 Loss=1.361 Prec@1=67.578 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST=> validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.148 Loss=1.361 Prec@1=67.578 Prec@5=86.523 rate=4369.37 Hz, eta=0:00:00, total=0:00:00, wall=01:39 IST** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.148 Loss=1.361 Prec@1=67.578 Prec@5=86.523 rate=4369.37 Hz, eta=0:00:00, total=0:00:00, wall=01:40 IST** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=0.398 Loss=1.320 Prec@1=67.034 Prec@5=88.264 rate=4369.37 Hz, eta=0:00:00, total=0:00:00, wall=01:40 IST** validation 100.00% of 1x98...Epoch=80/150 LR=0.04582 Time=0.398 Loss=1.320 Prec@1=67.034 Prec@5=88.264 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=01:40 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:40 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:40 IST=> training   0.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=5.828 DataTime=5.568 Loss=1.182 Prec@1=68.359 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=01:40 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=5.828 DataTime=5.568 Loss=1.182 Prec@1=68.359 Prec@5=89.844 rate=8436.62 Hz, eta=0:00:00, total=0:00:00, wall=01:40 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=5.828 DataTime=5.568 Loss=1.182 Prec@1=68.359 Prec@5=89.844 rate=8436.62 Hz, eta=0:00:00, total=0:00:00, wall=01:41 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.662 DataTime=0.436 Loss=1.171 Prec@1=70.581 Prec@5=89.780 rate=8436.62 Hz, eta=0:00:00, total=0:00:00, wall=01:41 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.662 DataTime=0.436 Loss=1.171 Prec@1=70.581 Prec@5=89.780 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=01:41 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.662 DataTime=0.436 Loss=1.171 Prec@1=70.581 Prec@5=89.780 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=01:42 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.638 DataTime=0.413 Loss=1.169 Prec@1=70.418 Prec@5=89.725 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=01:42 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.638 DataTime=0.413 Loss=1.169 Prec@1=70.418 Prec@5=89.725 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:42 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.638 DataTime=0.413 Loss=1.169 Prec@1=70.418 Prec@5=89.725 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:43 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.630 DataTime=0.405 Loss=1.170 Prec@1=70.409 Prec@5=89.683 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=01:43 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.630 DataTime=0.405 Loss=1.170 Prec@1=70.409 Prec@5=89.683 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=01:43 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.630 DataTime=0.405 Loss=1.170 Prec@1=70.409 Prec@5=89.683 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=01:44 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.626 DataTime=0.401 Loss=1.171 Prec@1=70.308 Prec@5=89.677 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=01:44 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.626 DataTime=0.401 Loss=1.171 Prec@1=70.308 Prec@5=89.677 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=01:44 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.626 DataTime=0.401 Loss=1.171 Prec@1=70.308 Prec@5=89.677 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=01:45 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.624 DataTime=0.398 Loss=1.171 Prec@1=70.294 Prec@5=89.673 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=01:45 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.624 DataTime=0.398 Loss=1.171 Prec@1=70.294 Prec@5=89.673 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:45 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.624 DataTime=0.398 Loss=1.171 Prec@1=70.294 Prec@5=89.673 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:46 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.622 DataTime=0.397 Loss=1.175 Prec@1=70.206 Prec@5=89.643 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=01:46 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.622 DataTime=0.397 Loss=1.175 Prec@1=70.206 Prec@5=89.643 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:46 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.622 DataTime=0.397 Loss=1.175 Prec@1=70.206 Prec@5=89.643 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:47 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.621 DataTime=0.396 Loss=1.177 Prec@1=70.184 Prec@5=89.626 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=01:47 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.621 DataTime=0.396 Loss=1.177 Prec@1=70.184 Prec@5=89.626 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:47 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.621 DataTime=0.396 Loss=1.177 Prec@1=70.184 Prec@5=89.626 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:48 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.395 Loss=1.178 Prec@1=70.130 Prec@5=89.640 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=01:48 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.395 Loss=1.178 Prec@1=70.130 Prec@5=89.640 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:48 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.395 Loss=1.178 Prec@1=70.130 Prec@5=89.640 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:49 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.394 Loss=1.178 Prec@1=70.130 Prec@5=89.614 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=01:49 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.394 Loss=1.178 Prec@1=70.130 Prec@5=89.614 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:49 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.620 DataTime=0.394 Loss=1.178 Prec@1=70.130 Prec@5=89.614 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:50 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.178 Prec@1=70.145 Prec@5=89.609 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=01:50 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.178 Prec@1=70.145 Prec@5=89.609 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:50 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.178 Prec@1=70.145 Prec@5=89.609 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:51 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.180 Prec@1=70.096 Prec@5=89.585 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=01:51 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.180 Prec@1=70.096 Prec@5=89.585 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:51 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.619 DataTime=0.394 Loss=1.180 Prec@1=70.096 Prec@5=89.585 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:52 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.181 Prec@1=70.067 Prec@5=89.577 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:52 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.181 Prec@1=70.067 Prec@5=89.577 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=01:52 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.181 Prec@1=70.067 Prec@5=89.577 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=01:53 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=70.011 Prec@5=89.532 rate=1.63 Hz, eta=0:13:18, total=0:12:17, wall=01:53 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=70.011 Prec@5=89.532 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:53 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=70.011 Prec@5=89.532 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:54 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=69.997 Prec@5=89.539 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:54 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=69.997 Prec@5=89.539 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:54 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.618 DataTime=0.393 Loss=1.184 Prec@1=69.997 Prec@5=89.539 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:55 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.393 Loss=1.187 Prec@1=69.950 Prec@5=89.505 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=01:55 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.393 Loss=1.187 Prec@1=69.950 Prec@5=89.505 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:55 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.393 Loss=1.187 Prec@1=69.950 Prec@5=89.505 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:56 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.188 Prec@1=69.917 Prec@5=89.500 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=01:56 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.188 Prec@1=69.917 Prec@5=89.500 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:56 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.188 Prec@1=69.917 Prec@5=89.500 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:57 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.189 Prec@1=69.877 Prec@5=89.484 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:57 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.189 Prec@1=69.877 Prec@5=89.484 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:57 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.189 Prec@1=69.877 Prec@5=89.484 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:58 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.190 Prec@1=69.848 Prec@5=89.468 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=01:58 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.190 Prec@1=69.848 Prec@5=89.468 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:58 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.190 Prec@1=69.848 Prec@5=89.468 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:59 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.191 Prec@1=69.838 Prec@5=89.465 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=01:59 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.191 Prec@1=69.838 Prec@5=89.465 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=01:59 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.617 DataTime=0.392 Loss=1.191 Prec@1=69.838 Prec@5=89.465 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:00 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.809 Prec@5=89.445 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:00 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.809 Prec@5=89.445 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:00 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.809 Prec@5=89.445 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:01 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.791 Prec@5=89.433 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=02:01 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.791 Prec@5=89.433 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:01 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.193 Prec@1=69.791 Prec@5=89.433 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:02 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.195 Prec@1=69.752 Prec@5=89.411 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:02 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.195 Prec@1=69.752 Prec@5=89.411 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:02 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.195 Prec@1=69.752 Prec@5=89.411 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:03 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.196 Prec@1=69.737 Prec@5=89.398 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:03 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.196 Prec@1=69.737 Prec@5=89.398 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:03 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.196 Prec@1=69.737 Prec@5=89.398 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:04 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.197 Prec@1=69.709 Prec@5=89.378 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=02:04 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.197 Prec@1=69.709 Prec@5=89.378 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:04 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.197 Prec@1=69.709 Prec@5=89.378 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:05 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.198 Prec@1=69.698 Prec@5=89.376 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.198 Prec@1=69.698 Prec@5=89.376 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.198 Prec@1=69.698 Prec@5=89.376 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.198 Prec@1=69.699 Prec@5=89.377 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:05 IST=> training   100.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.616 DataTime=0.392 Loss=1.198 Prec@1=69.699 Prec@5=89.377 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=02:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 0.00% of 1x98...Epoch=81/150 LR=0.04477 Time=7.668 Loss=1.376 Prec@1=66.797 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=7.668 Loss=1.376 Prec@1=66.797 Prec@5=86.523 rate=5906.47 Hz, eta=0:00:00, total=0:00:00, wall=02:05 IST** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=7.668 Loss=1.376 Prec@1=66.797 Prec@5=86.523 rate=5906.47 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=0.406 Loss=1.372 Prec@1=65.928 Prec@5=87.552 rate=5906.47 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST** validation 100.00% of 1x98...Epoch=81/150 LR=0.04477 Time=0.406 Loss=1.372 Prec@1=65.928 Prec@5=87.552 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=02:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.926 DataTime=5.671 Loss=1.231 Prec@1=71.484 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.926 DataTime=5.671 Loss=1.231 Prec@1=71.484 Prec@5=88.281 rate=6561.81 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.926 DataTime=5.671 Loss=1.231 Prec@1=71.484 Prec@5=88.281 rate=6561.81 Hz, eta=0:00:00, total=0:00:00, wall=02:07 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.662 DataTime=0.438 Loss=1.160 Prec@1=70.508 Prec@5=89.991 rate=6561.81 Hz, eta=0:00:00, total=0:00:00, wall=02:07 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.662 DataTime=0.438 Loss=1.160 Prec@1=70.508 Prec@5=89.991 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=02:07 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.662 DataTime=0.438 Loss=1.160 Prec@1=70.508 Prec@5=89.991 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=02:08 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.638 DataTime=0.414 Loss=1.159 Prec@1=70.561 Prec@5=89.944 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=02:08 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.638 DataTime=0.414 Loss=1.159 Prec@1=70.561 Prec@5=89.944 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:08 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.638 DataTime=0.414 Loss=1.159 Prec@1=70.561 Prec@5=89.944 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:09 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.630 DataTime=0.406 Loss=1.159 Prec@1=70.573 Prec@5=89.931 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:09 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.630 DataTime=0.406 Loss=1.159 Prec@1=70.573 Prec@5=89.931 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:09 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.630 DataTime=0.406 Loss=1.159 Prec@1=70.573 Prec@5=89.931 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:10 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.626 DataTime=0.401 Loss=1.159 Prec@1=70.580 Prec@5=89.929 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:10 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.626 DataTime=0.401 Loss=1.159 Prec@1=70.580 Prec@5=89.929 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:10 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.626 DataTime=0.401 Loss=1.159 Prec@1=70.580 Prec@5=89.929 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:11 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.624 DataTime=0.399 Loss=1.163 Prec@1=70.510 Prec@5=89.878 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:11 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.624 DataTime=0.399 Loss=1.163 Prec@1=70.510 Prec@5=89.878 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:11 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.624 DataTime=0.399 Loss=1.163 Prec@1=70.510 Prec@5=89.878 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:12 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.622 DataTime=0.397 Loss=1.165 Prec@1=70.462 Prec@5=89.839 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:12 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.622 DataTime=0.397 Loss=1.165 Prec@1=70.462 Prec@5=89.839 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:12 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.622 DataTime=0.397 Loss=1.165 Prec@1=70.462 Prec@5=89.839 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:13 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.621 DataTime=0.396 Loss=1.168 Prec@1=70.437 Prec@5=89.778 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=02:13 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.621 DataTime=0.396 Loss=1.168 Prec@1=70.437 Prec@5=89.778 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:13 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.621 DataTime=0.396 Loss=1.168 Prec@1=70.437 Prec@5=89.778 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:14 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.168 Prec@1=70.435 Prec@5=89.758 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:14 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.168 Prec@1=70.435 Prec@5=89.758 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:14 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.168 Prec@1=70.435 Prec@5=89.758 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:15 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.171 Prec@1=70.365 Prec@5=89.744 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=02:15 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.171 Prec@1=70.365 Prec@5=89.744 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:15 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.620 DataTime=0.395 Loss=1.171 Prec@1=70.365 Prec@5=89.744 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:16 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.395 Loss=1.172 Prec@1=70.350 Prec@5=89.727 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:16 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.395 Loss=1.172 Prec@1=70.350 Prec@5=89.727 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:16 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.395 Loss=1.172 Prec@1=70.350 Prec@5=89.727 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:17 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.394 Loss=1.173 Prec@1=70.313 Prec@5=89.704 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:17 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.394 Loss=1.173 Prec@1=70.313 Prec@5=89.704 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=02:17 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.619 DataTime=0.394 Loss=1.173 Prec@1=70.313 Prec@5=89.704 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=02:18 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.176 Prec@1=70.267 Prec@5=89.671 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=02:18 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.176 Prec@1=70.267 Prec@5=89.671 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:18 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.176 Prec@1=70.267 Prec@5=89.671 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:19 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.178 Prec@1=70.213 Prec@5=89.654 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:19 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.178 Prec@1=70.213 Prec@5=89.654 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:19 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.394 Loss=1.178 Prec@1=70.213 Prec@5=89.654 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:20 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.200 Prec@5=89.645 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:20 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.200 Prec@5=89.645 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:20 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.200 Prec@5=89.645 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:21 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.181 Prec@5=89.635 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:21 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.181 Prec@5=89.635 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:21 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.618 DataTime=0.393 Loss=1.179 Prec@1=70.181 Prec@5=89.635 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:22 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.181 Prec@1=70.154 Prec@5=89.611 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:22 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.181 Prec@1=70.154 Prec@5=89.611 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:22 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.181 Prec@1=70.154 Prec@5=89.611 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:23 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.182 Prec@1=70.108 Prec@5=89.583 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:23 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.182 Prec@1=70.108 Prec@5=89.583 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=02:23 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.182 Prec@1=70.108 Prec@5=89.583 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=02:24 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.183 Prec@1=70.101 Prec@5=89.574 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=02:24 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.183 Prec@1=70.101 Prec@5=89.574 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:24 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.393 Loss=1.183 Prec@1=70.101 Prec@5=89.574 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:26 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.077 Prec@5=89.560 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:26 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.077 Prec@5=89.560 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=02:26 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.077 Prec@5=89.560 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=02:27 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.049 Prec@5=89.554 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=02:27 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.049 Prec@5=89.554 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:27 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.184 Prec@1=70.049 Prec@5=89.554 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:28 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.185 Prec@1=70.021 Prec@5=89.546 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:28 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.185 Prec@1=70.021 Prec@5=89.546 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:28 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.185 Prec@1=70.021 Prec@5=89.546 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:29 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.987 Prec@5=89.529 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:29 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.987 Prec@5=89.529 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:29 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.987 Prec@5=89.529 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:30 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.964 Prec@5=89.516 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:30 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.964 Prec@5=89.516 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:30 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.617 DataTime=0.392 Loss=1.187 Prec@1=69.964 Prec@5=89.516 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:31 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.188 Prec@1=69.953 Prec@5=89.508 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:31 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.188 Prec@1=69.953 Prec@5=89.508 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:31 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.188 Prec@1=69.953 Prec@5=89.508 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:32 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.189 Prec@1=69.926 Prec@5=89.491 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:32 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.189 Prec@1=69.926 Prec@5=89.491 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:32 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.189 Prec@1=69.926 Prec@5=89.491 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:32 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.189 Prec@1=69.926 Prec@5=89.490 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:32 IST=> training   100.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.616 DataTime=0.392 Loss=1.189 Prec@1=69.926 Prec@5=89.490 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=02:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> validation 0.00% of 1x98...Epoch=82/150 LR=0.04373 Time=5.663 Loss=1.472 Prec@1=62.891 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=5.663 Loss=1.472 Prec@1=62.891 Prec@5=86.523 rate=4681.41 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=5.663 Loss=1.472 Prec@1=62.891 Prec@5=86.523 rate=4681.41 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=0.397 Loss=1.337 Prec@1=66.748 Prec@5=88.068 rate=4681.41 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST** validation 100.00% of 1x98...Epoch=82/150 LR=0.04373 Time=0.397 Loss=1.337 Prec@1=66.748 Prec@5=88.068 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=02:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> training   0.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.523 DataTime=5.114 Loss=1.235 Prec@1=71.094 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=02:32 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.523 DataTime=5.114 Loss=1.235 Prec@1=71.094 Prec@5=88.281 rate=6664.13 Hz, eta=0:00:00, total=0:00:00, wall=02:32 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.523 DataTime=5.114 Loss=1.235 Prec@1=71.094 Prec@5=88.281 rate=6664.13 Hz, eta=0:00:00, total=0:00:00, wall=02:33 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.658 DataTime=0.431 Loss=1.160 Prec@1=70.682 Prec@5=89.875 rate=6664.13 Hz, eta=0:00:00, total=0:00:00, wall=02:33 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.658 DataTime=0.431 Loss=1.160 Prec@1=70.682 Prec@5=89.875 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=02:33 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.658 DataTime=0.431 Loss=1.160 Prec@1=70.682 Prec@5=89.875 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=02:35 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.636 DataTime=0.410 Loss=1.153 Prec@1=70.765 Prec@5=89.921 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=02:35 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.636 DataTime=0.410 Loss=1.153 Prec@1=70.765 Prec@5=89.921 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:35 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.636 DataTime=0.410 Loss=1.153 Prec@1=70.765 Prec@5=89.921 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:36 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.629 DataTime=0.403 Loss=1.152 Prec@1=70.774 Prec@5=89.894 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=02:36 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.629 DataTime=0.403 Loss=1.152 Prec@1=70.774 Prec@5=89.894 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:36 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.629 DataTime=0.403 Loss=1.152 Prec@1=70.774 Prec@5=89.894 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:37 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.625 DataTime=0.400 Loss=1.159 Prec@1=70.599 Prec@5=89.823 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=02:37 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.625 DataTime=0.400 Loss=1.159 Prec@1=70.599 Prec@5=89.823 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=02:37 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.625 DataTime=0.400 Loss=1.159 Prec@1=70.599 Prec@5=89.823 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=02:38 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.623 DataTime=0.398 Loss=1.158 Prec@1=70.633 Prec@5=89.839 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=02:38 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.623 DataTime=0.398 Loss=1.158 Prec@1=70.633 Prec@5=89.839 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:38 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.623 DataTime=0.398 Loss=1.158 Prec@1=70.633 Prec@5=89.839 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:39 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.622 DataTime=0.397 Loss=1.159 Prec@1=70.615 Prec@5=89.828 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:39 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.622 DataTime=0.397 Loss=1.159 Prec@1=70.615 Prec@5=89.828 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:39 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.622 DataTime=0.397 Loss=1.159 Prec@1=70.615 Prec@5=89.828 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:40 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.621 DataTime=0.396 Loss=1.161 Prec@1=70.588 Prec@5=89.796 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=02:40 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.621 DataTime=0.396 Loss=1.161 Prec@1=70.588 Prec@5=89.796 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:40 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.621 DataTime=0.396 Loss=1.161 Prec@1=70.588 Prec@5=89.796 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:41 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.620 DataTime=0.395 Loss=1.162 Prec@1=70.546 Prec@5=89.789 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:41 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.620 DataTime=0.395 Loss=1.162 Prec@1=70.546 Prec@5=89.789 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:41 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.620 DataTime=0.395 Loss=1.162 Prec@1=70.546 Prec@5=89.789 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:42 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.164 Prec@1=70.514 Prec@5=89.780 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:42 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.164 Prec@1=70.514 Prec@5=89.780 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:42 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.164 Prec@1=70.514 Prec@5=89.780 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:43 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.166 Prec@1=70.481 Prec@5=89.761 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=02:43 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.166 Prec@1=70.481 Prec@5=89.761 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:43 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.619 DataTime=0.394 Loss=1.166 Prec@1=70.481 Prec@5=89.761 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:44 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.394 Loss=1.168 Prec@1=70.427 Prec@5=89.732 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=02:44 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.394 Loss=1.168 Prec@1=70.427 Prec@5=89.732 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:44 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.394 Loss=1.168 Prec@1=70.427 Prec@5=89.732 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:45 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.170 Prec@1=70.403 Prec@5=89.704 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:45 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.170 Prec@1=70.403 Prec@5=89.704 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:45 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.170 Prec@1=70.403 Prec@5=89.704 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:46 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.171 Prec@1=70.379 Prec@5=89.704 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:46 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.171 Prec@1=70.379 Prec@5=89.704 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:46 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.618 DataTime=0.393 Loss=1.171 Prec@1=70.379 Prec@5=89.704 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:47 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.172 Prec@1=70.359 Prec@5=89.684 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=02:47 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.172 Prec@1=70.359 Prec@5=89.684 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:47 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.172 Prec@1=70.359 Prec@5=89.684 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:48 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.173 Prec@1=70.321 Prec@5=89.674 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:48 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.173 Prec@1=70.321 Prec@5=89.674 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:48 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.393 Loss=1.173 Prec@1=70.321 Prec@5=89.674 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:49 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.175 Prec@1=70.280 Prec@5=89.656 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=02:49 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.175 Prec@1=70.280 Prec@5=89.656 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:49 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.175 Prec@1=70.280 Prec@5=89.656 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:50 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.177 Prec@1=70.244 Prec@5=89.633 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:50 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.177 Prec@1=70.244 Prec@5=89.633 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:50 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.177 Prec@1=70.244 Prec@5=89.633 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:51 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.179 Prec@1=70.204 Prec@5=89.600 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=02:51 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.179 Prec@1=70.204 Prec@5=89.600 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:51 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.179 Prec@1=70.204 Prec@5=89.600 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:52 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.179 Prec@5=89.584 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=02:52 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.179 Prec@5=89.584 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:52 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.179 Prec@5=89.584 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:53 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.176 Prec@5=89.575 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:53 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.176 Prec@5=89.575 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:53 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.617 DataTime=0.392 Loss=1.180 Prec@1=70.176 Prec@5=89.575 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:54 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.181 Prec@1=70.156 Prec@5=89.563 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=02:54 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.181 Prec@1=70.156 Prec@5=89.563 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:54 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.181 Prec@1=70.156 Prec@5=89.563 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:55 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.182 Prec@1=70.121 Prec@5=89.554 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=02:55 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.182 Prec@1=70.121 Prec@5=89.554 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:55 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.182 Prec@1=70.121 Prec@5=89.554 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:56 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.183 Prec@1=70.092 Prec@5=89.545 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=02:56 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.183 Prec@1=70.092 Prec@5=89.545 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:56 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.183 Prec@1=70.092 Prec@5=89.545 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:57 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.082 Prec@5=89.532 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=02:57 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.082 Prec@5=89.532 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:57 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.082 Prec@5=89.532 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:58 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.074 Prec@5=89.528 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=02:58 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.074 Prec@5=89.528 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:58 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.074 Prec@5=89.528 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:58 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.074 Prec@5=89.528 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=02:58 IST=> training   100.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.616 DataTime=0.392 Loss=1.184 Prec@1=70.074 Prec@5=89.528 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=02:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:58 IST=> validation 0.00% of 1x98...Epoch=83/150 LR=0.04270 Time=7.257 Loss=1.196 Prec@1=69.727 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=02:58 IST=> validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=7.257 Loss=1.196 Prec@1=69.727 Prec@5=89.844 rate=6618.57 Hz, eta=0:00:00, total=0:00:00, wall=02:58 IST** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=7.257 Loss=1.196 Prec@1=69.727 Prec@5=89.844 rate=6618.57 Hz, eta=0:00:00, total=0:00:00, wall=02:59 IST** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=0.413 Loss=1.335 Prec@1=66.858 Prec@5=88.078 rate=6618.57 Hz, eta=0:00:00, total=0:00:00, wall=02:59 IST** validation 100.00% of 1x98...Epoch=83/150 LR=0.04270 Time=0.413 Loss=1.335 Prec@1=66.858 Prec@5=88.078 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=02:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:59 IST=> training   0.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.484 DataTime=5.189 Loss=1.099 Prec@1=73.047 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=02:59 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.484 DataTime=5.189 Loss=1.099 Prec@1=73.047 Prec@5=89.844 rate=3828.73 Hz, eta=0:00:00, total=0:00:00, wall=02:59 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.484 DataTime=5.189 Loss=1.099 Prec@1=73.047 Prec@5=89.844 rate=3828.73 Hz, eta=0:00:00, total=0:00:00, wall=03:00 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.659 DataTime=0.435 Loss=1.136 Prec@1=70.995 Prec@5=90.147 rate=3828.73 Hz, eta=0:00:00, total=0:00:00, wall=03:00 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.659 DataTime=0.435 Loss=1.136 Prec@1=70.995 Prec@5=90.147 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:00 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.659 DataTime=0.435 Loss=1.136 Prec@1=70.995 Prec@5=90.147 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:01 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.637 DataTime=0.412 Loss=1.142 Prec@1=70.878 Prec@5=90.129 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:01 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.637 DataTime=0.412 Loss=1.142 Prec@1=70.878 Prec@5=90.129 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=03:01 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.637 DataTime=0.412 Loss=1.142 Prec@1=70.878 Prec@5=90.129 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=03:02 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.629 DataTime=0.405 Loss=1.149 Prec@1=70.749 Prec@5=89.981 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=03:02 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.629 DataTime=0.405 Loss=1.149 Prec@1=70.749 Prec@5=89.981 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:02 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.629 DataTime=0.405 Loss=1.149 Prec@1=70.749 Prec@5=89.981 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:03 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.626 DataTime=0.401 Loss=1.152 Prec@1=70.731 Prec@5=89.929 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:03 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.626 DataTime=0.401 Loss=1.152 Prec@1=70.731 Prec@5=89.929 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:03 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.626 DataTime=0.401 Loss=1.152 Prec@1=70.731 Prec@5=89.929 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:04 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.623 DataTime=0.398 Loss=1.153 Prec@1=70.682 Prec@5=89.911 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:04 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.623 DataTime=0.398 Loss=1.153 Prec@1=70.682 Prec@5=89.911 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:04 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.623 DataTime=0.398 Loss=1.153 Prec@1=70.682 Prec@5=89.911 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:05 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.622 DataTime=0.397 Loss=1.155 Prec@1=70.643 Prec@5=89.888 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:05 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.622 DataTime=0.397 Loss=1.155 Prec@1=70.643 Prec@5=89.888 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:05 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.622 DataTime=0.397 Loss=1.155 Prec@1=70.643 Prec@5=89.888 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:06 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.621 DataTime=0.396 Loss=1.158 Prec@1=70.584 Prec@5=89.850 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:06 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.621 DataTime=0.396 Loss=1.158 Prec@1=70.584 Prec@5=89.850 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:06 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.621 DataTime=0.396 Loss=1.158 Prec@1=70.584 Prec@5=89.850 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:07 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.620 DataTime=0.395 Loss=1.160 Prec@1=70.520 Prec@5=89.824 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:07 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.620 DataTime=0.395 Loss=1.160 Prec@1=70.520 Prec@5=89.824 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:07 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.620 DataTime=0.395 Loss=1.160 Prec@1=70.520 Prec@5=89.824 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:08 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.161 Prec@1=70.490 Prec@5=89.812 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:08 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.161 Prec@1=70.490 Prec@5=89.812 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:08 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.161 Prec@1=70.490 Prec@5=89.812 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:09 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.162 Prec@1=70.472 Prec@5=89.797 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:09 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.162 Prec@1=70.472 Prec@5=89.797 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=03:09 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.619 DataTime=0.394 Loss=1.162 Prec@1=70.472 Prec@5=89.797 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=03:10 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.163 Prec@1=70.441 Prec@5=89.789 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=03:10 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.163 Prec@1=70.441 Prec@5=89.789 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:10 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.163 Prec@1=70.441 Prec@5=89.789 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:11 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.164 Prec@1=70.442 Prec@5=89.785 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:11 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.164 Prec@1=70.442 Prec@5=89.785 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:11 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.164 Prec@1=70.442 Prec@5=89.785 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:12 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.165 Prec@1=70.399 Prec@5=89.765 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:12 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.165 Prec@1=70.399 Prec@5=89.765 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:12 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.393 Loss=1.165 Prec@1=70.399 Prec@5=89.765 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:13 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.392 Loss=1.167 Prec@1=70.358 Prec@5=89.738 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:13 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.392 Loss=1.167 Prec@1=70.358 Prec@5=89.738 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:13 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.618 DataTime=0.392 Loss=1.167 Prec@1=70.358 Prec@5=89.738 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:14 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.168 Prec@1=70.333 Prec@5=89.731 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:14 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.168 Prec@1=70.333 Prec@5=89.731 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:14 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.168 Prec@1=70.333 Prec@5=89.731 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:15 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.169 Prec@1=70.335 Prec@5=89.721 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:15 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.169 Prec@1=70.335 Prec@5=89.721 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:15 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.169 Prec@1=70.335 Prec@5=89.721 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:16 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.326 Prec@5=89.708 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:16 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.326 Prec@5=89.708 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:16 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.326 Prec@5=89.708 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:17 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.333 Prec@5=89.710 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:17 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.333 Prec@5=89.710 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=03:17 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.170 Prec@1=70.333 Prec@5=89.710 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=03:18 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.172 Prec@1=70.293 Prec@5=89.683 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=03:18 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.172 Prec@1=70.293 Prec@5=89.683 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:18 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.172 Prec@1=70.293 Prec@5=89.683 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:19 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.173 Prec@1=70.263 Prec@5=89.669 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:19 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.173 Prec@1=70.263 Prec@5=89.669 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:19 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.392 Loss=1.173 Prec@1=70.263 Prec@5=89.669 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:20 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.391 Loss=1.173 Prec@1=70.248 Prec@5=89.659 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:20 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.391 Loss=1.173 Prec@1=70.248 Prec@5=89.659 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=03:20 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.617 DataTime=0.391 Loss=1.173 Prec@1=70.248 Prec@5=89.659 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=03:21 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.232 Prec@5=89.642 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=03:21 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.232 Prec@5=89.642 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:21 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.232 Prec@5=89.642 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:22 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.218 Prec@5=89.639 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:22 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.218 Prec@5=89.639 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=03:22 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.175 Prec@1=70.218 Prec@5=89.639 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=03:23 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.176 Prec@1=70.195 Prec@5=89.622 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=03:23 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.176 Prec@1=70.195 Prec@5=89.622 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:23 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.176 Prec@1=70.195 Prec@5=89.622 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:24 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.177 Prec@1=70.179 Prec@5=89.611 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:24 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.177 Prec@1=70.179 Prec@5=89.611 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=03:24 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.177 Prec@1=70.179 Prec@5=89.611 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=03:24 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.177 Prec@1=70.178 Prec@5=89.611 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=03:24 IST=> training   100.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.616 DataTime=0.391 Loss=1.177 Prec@1=70.178 Prec@5=89.611 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=03:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> validation 0.00% of 1x98...Epoch=84/150 LR=0.04166 Time=7.193 Loss=1.314 Prec@1=67.773 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=7.193 Loss=1.314 Prec@1=67.773 Prec@5=87.305 rate=4947.85 Hz, eta=0:00:00, total=0:00:00, wall=03:25 IST** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=7.193 Loss=1.314 Prec@1=67.773 Prec@5=87.305 rate=4947.85 Hz, eta=0:00:00, total=0:00:00, wall=03:25 IST** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=0.413 Loss=1.310 Prec@1=67.626 Prec@5=88.332 rate=4947.85 Hz, eta=0:00:00, total=0:00:00, wall=03:25 IST** validation 100.00% of 1x98...Epoch=84/150 LR=0.04166 Time=0.413 Loss=1.310 Prec@1=67.626 Prec@5=88.332 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=03:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> training   0.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.425 DataTime=5.152 Loss=1.159 Prec@1=69.727 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=03:25 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.425 DataTime=5.152 Loss=1.159 Prec@1=69.727 Prec@5=89.453 rate=5847.10 Hz, eta=0:00:00, total=0:00:00, wall=03:25 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.425 DataTime=5.152 Loss=1.159 Prec@1=69.727 Prec@5=89.453 rate=5847.10 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.658 DataTime=0.432 Loss=1.130 Prec@1=71.303 Prec@5=90.080 rate=5847.10 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.658 DataTime=0.432 Loss=1.130 Prec@1=71.303 Prec@5=90.080 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=03:26 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.658 DataTime=0.432 Loss=1.130 Prec@1=71.303 Prec@5=90.080 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=03:27 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.636 DataTime=0.410 Loss=1.129 Prec@1=71.171 Prec@5=90.200 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=03:27 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.636 DataTime=0.410 Loss=1.129 Prec@1=71.171 Prec@5=90.200 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:27 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.636 DataTime=0.410 Loss=1.129 Prec@1=71.171 Prec@5=90.200 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:28 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.629 DataTime=0.404 Loss=1.133 Prec@1=71.089 Prec@5=90.201 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:28 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.629 DataTime=0.404 Loss=1.133 Prec@1=71.089 Prec@5=90.201 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:28 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.629 DataTime=0.404 Loss=1.133 Prec@1=71.089 Prec@5=90.201 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:29 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.625 DataTime=0.400 Loss=1.135 Prec@1=71.039 Prec@5=90.164 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:29 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.625 DataTime=0.400 Loss=1.135 Prec@1=71.039 Prec@5=90.164 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:29 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.625 DataTime=0.400 Loss=1.135 Prec@1=71.039 Prec@5=90.164 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:30 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.623 DataTime=0.398 Loss=1.138 Prec@1=71.024 Prec@5=90.114 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:30 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.623 DataTime=0.398 Loss=1.138 Prec@1=71.024 Prec@5=90.114 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=03:30 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.623 DataTime=0.398 Loss=1.138 Prec@1=71.024 Prec@5=90.114 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=03:31 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.622 DataTime=0.397 Loss=1.140 Prec@1=70.945 Prec@5=90.074 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=03:31 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.622 DataTime=0.397 Loss=1.140 Prec@1=70.945 Prec@5=90.074 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:31 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.622 DataTime=0.397 Loss=1.140 Prec@1=70.945 Prec@5=90.074 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:32 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.621 DataTime=0.395 Loss=1.142 Prec@1=70.934 Prec@5=90.053 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:32 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.621 DataTime=0.395 Loss=1.142 Prec@1=70.934 Prec@5=90.053 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:32 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.621 DataTime=0.395 Loss=1.142 Prec@1=70.934 Prec@5=90.053 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:34 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.620 DataTime=0.394 Loss=1.144 Prec@1=70.909 Prec@5=90.047 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:34 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.620 DataTime=0.394 Loss=1.144 Prec@1=70.909 Prec@5=90.047 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:34 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.620 DataTime=0.394 Loss=1.144 Prec@1=70.909 Prec@5=90.047 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:35 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.146 Prec@1=70.878 Prec@5=90.019 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=03:35 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.146 Prec@1=70.878 Prec@5=90.019 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:35 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.146 Prec@1=70.878 Prec@5=90.019 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:36 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.148 Prec@1=70.807 Prec@5=90.001 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=03:36 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.148 Prec@1=70.807 Prec@5=90.001 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=03:36 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.619 DataTime=0.394 Loss=1.148 Prec@1=70.807 Prec@5=90.001 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=03:37 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.776 Prec@5=89.977 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=03:37 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.776 Prec@5=89.977 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:37 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.776 Prec@5=89.977 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:38 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.151 Prec@1=70.738 Prec@5=89.967 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=03:38 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.151 Prec@1=70.738 Prec@5=89.967 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:38 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.151 Prec@1=70.738 Prec@5=89.967 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:39 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.700 Prec@5=89.952 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:39 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.700 Prec@5=89.952 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:39 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.700 Prec@5=89.952 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:40 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.392 Loss=1.154 Prec@1=70.639 Prec@5=89.929 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=03:40 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.392 Loss=1.154 Prec@1=70.639 Prec@5=89.929 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:40 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.618 DataTime=0.392 Loss=1.154 Prec@1=70.639 Prec@5=89.929 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:41 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.155 Prec@1=70.634 Prec@5=89.913 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=03:41 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.155 Prec@1=70.634 Prec@5=89.913 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:41 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.155 Prec@1=70.634 Prec@5=89.913 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:42 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.156 Prec@1=70.636 Prec@5=89.901 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=03:42 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.156 Prec@1=70.636 Prec@5=89.901 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:42 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.156 Prec@1=70.636 Prec@5=89.901 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:43 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.583 Prec@5=89.888 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:43 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.583 Prec@5=89.888 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:43 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.583 Prec@5=89.888 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:44 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.563 Prec@5=89.864 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=03:44 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.563 Prec@5=89.864 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=03:44 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.563 Prec@5=89.864 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=03:45 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.538 Prec@5=89.853 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=03:45 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.538 Prec@5=89.853 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:45 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.538 Prec@5=89.853 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:46 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.162 Prec@1=70.494 Prec@5=89.829 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=03:46 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.162 Prec@1=70.494 Prec@5=89.829 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:46 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.392 Loss=1.162 Prec@1=70.494 Prec@5=89.829 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:47 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.163 Prec@1=70.468 Prec@5=89.824 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=03:47 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.163 Prec@1=70.468 Prec@5=89.824 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=03:47 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.163 Prec@1=70.468 Prec@5=89.824 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=03:48 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.164 Prec@1=70.432 Prec@5=89.806 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=03:48 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.164 Prec@1=70.432 Prec@5=89.806 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:48 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.617 DataTime=0.391 Loss=1.164 Prec@1=70.432 Prec@5=89.806 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:49 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.165 Prec@1=70.411 Prec@5=89.793 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=03:49 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.165 Prec@1=70.411 Prec@5=89.793 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=03:49 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.165 Prec@1=70.411 Prec@5=89.793 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=03:50 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.166 Prec@1=70.403 Prec@5=89.780 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=03:50 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.166 Prec@1=70.403 Prec@5=89.780 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:50 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.166 Prec@1=70.403 Prec@5=89.780 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:51 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.167 Prec@1=70.387 Prec@5=89.768 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=03:51 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.167 Prec@1=70.387 Prec@5=89.768 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=03:51 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.167 Prec@1=70.387 Prec@5=89.768 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=03:51 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.167 Prec@1=70.386 Prec@5=89.766 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=03:51 IST=> training   100.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.616 DataTime=0.391 Loss=1.167 Prec@1=70.386 Prec@5=89.766 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=03:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST=> validation 0.00% of 1x98...Epoch=85/150 LR=0.04063 Time=6.774 Loss=1.386 Prec@1=67.383 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST=> validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.774 Loss=1.386 Prec@1=67.383 Prec@5=87.891 rate=1508.10 Hz, eta=0:00:00, total=0:00:00, wall=03:51 IST** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.774 Loss=1.386 Prec@1=67.383 Prec@5=87.891 rate=1508.10 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=0.409 Loss=1.302 Prec@1=67.520 Prec@5=88.454 rate=1508.10 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST** validation 100.00% of 1x98...Epoch=85/150 LR=0.04063 Time=0.409 Loss=1.302 Prec@1=67.520 Prec@5=88.454 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=03:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST=> training   0.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.802 DataTime=5.435 Loss=1.075 Prec@1=73.242 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.802 DataTime=5.435 Loss=1.075 Prec@1=73.242 Prec@5=90.430 rate=5811.86 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.802 DataTime=5.435 Loss=1.075 Prec@1=73.242 Prec@5=90.430 rate=5811.86 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.661 DataTime=0.436 Loss=1.141 Prec@1=71.113 Prec@5=90.056 rate=5811.86 Hz, eta=0:00:00, total=0:00:00, wall=03:53 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.661 DataTime=0.436 Loss=1.141 Prec@1=71.113 Prec@5=90.056 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:53 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.661 DataTime=0.436 Loss=1.141 Prec@1=71.113 Prec@5=90.056 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:54 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.638 DataTime=0.413 Loss=1.137 Prec@1=71.112 Prec@5=90.095 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=03:54 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.638 DataTime=0.413 Loss=1.137 Prec@1=71.112 Prec@5=90.095 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:54 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.638 DataTime=0.413 Loss=1.137 Prec@1=71.112 Prec@5=90.095 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:55 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.630 DataTime=0.406 Loss=1.136 Prec@1=71.088 Prec@5=90.118 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=03:55 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.630 DataTime=0.406 Loss=1.136 Prec@1=71.088 Prec@5=90.118 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:55 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.630 DataTime=0.406 Loss=1.136 Prec@1=71.088 Prec@5=90.118 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:56 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.626 DataTime=0.402 Loss=1.136 Prec@1=71.035 Prec@5=90.151 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=03:56 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.626 DataTime=0.402 Loss=1.136 Prec@1=71.035 Prec@5=90.151 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:56 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.626 DataTime=0.402 Loss=1.136 Prec@1=71.035 Prec@5=90.151 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:57 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.624 DataTime=0.399 Loss=1.138 Prec@1=70.973 Prec@5=90.140 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=03:57 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.624 DataTime=0.399 Loss=1.138 Prec@1=70.973 Prec@5=90.140 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:57 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.624 DataTime=0.399 Loss=1.138 Prec@1=70.973 Prec@5=90.140 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:58 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.622 DataTime=0.398 Loss=1.140 Prec@1=70.912 Prec@5=90.111 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=03:58 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.622 DataTime=0.398 Loss=1.140 Prec@1=70.912 Prec@5=90.111 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:58 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.622 DataTime=0.398 Loss=1.140 Prec@1=70.912 Prec@5=90.111 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:59 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.621 DataTime=0.397 Loss=1.142 Prec@1=70.900 Prec@5=90.081 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=03:59 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.621 DataTime=0.397 Loss=1.142 Prec@1=70.900 Prec@5=90.081 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=03:59 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.621 DataTime=0.397 Loss=1.142 Prec@1=70.900 Prec@5=90.081 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:00 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.396 Loss=1.144 Prec@1=70.840 Prec@5=90.050 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:00 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.396 Loss=1.144 Prec@1=70.840 Prec@5=90.050 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:00 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.396 Loss=1.144 Prec@1=70.840 Prec@5=90.050 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:01 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.395 Loss=1.146 Prec@1=70.794 Prec@5=90.023 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:01 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.395 Loss=1.146 Prec@1=70.794 Prec@5=90.023 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:01 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.620 DataTime=0.395 Loss=1.146 Prec@1=70.794 Prec@5=90.023 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:02 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.395 Loss=1.147 Prec@1=70.769 Prec@5=90.008 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:02 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.395 Loss=1.147 Prec@1=70.769 Prec@5=90.008 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:02 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.395 Loss=1.147 Prec@1=70.769 Prec@5=90.008 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:03 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.394 Loss=1.147 Prec@1=70.747 Prec@5=90.021 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:03 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.394 Loss=1.147 Prec@1=70.747 Prec@5=90.021 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:03 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.619 DataTime=0.394 Loss=1.147 Prec@1=70.747 Prec@5=90.021 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:04 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.394 Loss=1.148 Prec@1=70.735 Prec@5=90.000 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:04 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.394 Loss=1.148 Prec@1=70.735 Prec@5=90.000 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:04 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.394 Loss=1.148 Prec@1=70.735 Prec@5=90.000 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:05 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.724 Prec@5=89.985 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:05 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.724 Prec@5=89.985 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:05 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.149 Prec@1=70.724 Prec@5=89.985 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:06 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.653 Prec@5=89.955 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:06 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.653 Prec@5=89.955 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:06 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.152 Prec@1=70.653 Prec@5=89.955 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:07 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.153 Prec@1=70.640 Prec@5=89.941 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:07 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.153 Prec@1=70.640 Prec@5=89.941 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:07 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.618 DataTime=0.393 Loss=1.153 Prec@1=70.640 Prec@5=89.941 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:08 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.155 Prec@1=70.595 Prec@5=89.917 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:08 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.155 Prec@1=70.595 Prec@5=89.917 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:08 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.155 Prec@1=70.595 Prec@5=89.917 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:09 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.156 Prec@1=70.578 Prec@5=89.900 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:09 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.156 Prec@1=70.578 Prec@5=89.900 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:09 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.393 Loss=1.156 Prec@1=70.578 Prec@5=89.900 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:10 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.157 Prec@1=70.565 Prec@5=89.881 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:10 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.157 Prec@1=70.565 Prec@5=89.881 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:10 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.157 Prec@1=70.565 Prec@5=89.881 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:11 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.542 Prec@5=89.862 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:11 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.542 Prec@5=89.862 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:11 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.158 Prec@1=70.542 Prec@5=89.862 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:12 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.519 Prec@5=89.858 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:12 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.519 Prec@5=89.858 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:12 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.519 Prec@5=89.858 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:13 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.512 Prec@5=89.853 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:13 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.512 Prec@5=89.853 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:13 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.617 DataTime=0.392 Loss=1.160 Prec@1=70.512 Prec@5=89.853 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:14 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.161 Prec@1=70.492 Prec@5=89.836 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:14 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.161 Prec@1=70.492 Prec@5=89.836 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:14 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.161 Prec@1=70.492 Prec@5=89.836 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:15 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.483 Prec@5=89.830 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:15 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.483 Prec@5=89.830 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:15 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.483 Prec@5=89.830 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:16 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.487 Prec@5=89.829 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:16 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.487 Prec@5=89.829 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:16 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.487 Prec@5=89.829 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:17 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.498 Prec@5=89.826 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:17 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.498 Prec@5=89.826 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:17 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.498 Prec@5=89.826 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:17 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.498 Prec@5=89.825 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:17 IST=> training   100.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.616 DataTime=0.392 Loss=1.162 Prec@1=70.498 Prec@5=89.825 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=04:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST=> validation 0.00% of 1x98...Epoch=86/150 LR=0.03960 Time=6.294 Loss=1.200 Prec@1=69.336 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=04:17 IST=> validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=6.294 Loss=1.200 Prec@1=69.336 Prec@5=89.648 rate=3498.23 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=6.294 Loss=1.200 Prec@1=69.336 Prec@5=89.648 rate=3498.23 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=0.400 Loss=1.287 Prec@1=68.198 Prec@5=88.636 rate=3498.23 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST** validation 100.00% of 1x98...Epoch=86/150 LR=0.03960 Time=0.400 Loss=1.287 Prec@1=68.198 Prec@5=88.636 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=04:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.237 DataTime=4.938 Loss=1.214 Prec@1=70.898 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.237 DataTime=4.938 Loss=1.214 Prec@1=70.898 Prec@5=88.867 rate=2411.66 Hz, eta=0:00:01, total=0:00:00, wall=04:18 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=5.237 DataTime=4.938 Loss=1.214 Prec@1=70.898 Prec@5=88.867 rate=2411.66 Hz, eta=0:00:01, total=0:00:00, wall=04:19 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.658 DataTime=0.433 Loss=1.122 Prec@1=71.384 Prec@5=90.254 rate=2411.66 Hz, eta=0:00:01, total=0:00:00, wall=04:19 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.658 DataTime=0.433 Loss=1.122 Prec@1=71.384 Prec@5=90.254 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=04:19 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.658 DataTime=0.433 Loss=1.122 Prec@1=71.384 Prec@5=90.254 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=04:20 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.636 DataTime=0.410 Loss=1.119 Prec@1=71.299 Prec@5=90.343 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=04:20 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.636 DataTime=0.410 Loss=1.119 Prec@1=71.299 Prec@5=90.343 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=04:20 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.636 DataTime=0.410 Loss=1.119 Prec@1=71.299 Prec@5=90.343 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=04:21 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.629 DataTime=0.403 Loss=1.122 Prec@1=71.255 Prec@5=90.328 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=04:21 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.629 DataTime=0.403 Loss=1.122 Prec@1=71.255 Prec@5=90.328 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:21 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.629 DataTime=0.403 Loss=1.122 Prec@1=71.255 Prec@5=90.328 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:22 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.625 DataTime=0.400 Loss=1.125 Prec@1=71.239 Prec@5=90.252 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:22 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.625 DataTime=0.400 Loss=1.125 Prec@1=71.239 Prec@5=90.252 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:22 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.625 DataTime=0.400 Loss=1.125 Prec@1=71.239 Prec@5=90.252 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:23 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.623 DataTime=0.398 Loss=1.128 Prec@1=71.197 Prec@5=90.227 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:23 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.623 DataTime=0.398 Loss=1.128 Prec@1=71.197 Prec@5=90.227 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:23 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.623 DataTime=0.398 Loss=1.128 Prec@1=71.197 Prec@5=90.227 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:24 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.622 DataTime=0.396 Loss=1.131 Prec@1=71.184 Prec@5=90.216 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:24 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.622 DataTime=0.396 Loss=1.131 Prec@1=71.184 Prec@5=90.216 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:24 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.622 DataTime=0.396 Loss=1.131 Prec@1=71.184 Prec@5=90.216 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:25 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.621 DataTime=0.396 Loss=1.133 Prec@1=71.131 Prec@5=90.193 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:25 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.621 DataTime=0.396 Loss=1.133 Prec@1=71.131 Prec@5=90.193 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:25 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.621 DataTime=0.396 Loss=1.133 Prec@1=71.131 Prec@5=90.193 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:26 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.620 DataTime=0.395 Loss=1.133 Prec@1=71.147 Prec@5=90.189 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:26 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.620 DataTime=0.395 Loss=1.133 Prec@1=71.147 Prec@5=90.189 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:26 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.620 DataTime=0.395 Loss=1.133 Prec@1=71.147 Prec@5=90.189 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:27 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.121 Prec@5=90.186 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:27 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.121 Prec@5=90.186 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:27 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.121 Prec@5=90.186 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:28 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.118 Prec@5=90.191 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:28 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.118 Prec@5=90.191 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:28 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.619 DataTime=0.394 Loss=1.133 Prec@1=71.118 Prec@5=90.191 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:29 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.394 Loss=1.135 Prec@1=71.069 Prec@5=90.161 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:29 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.394 Loss=1.135 Prec@1=71.069 Prec@5=90.161 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:29 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.394 Loss=1.135 Prec@1=71.069 Prec@5=90.161 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:30 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.393 Loss=1.136 Prec@1=71.048 Prec@5=90.147 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:30 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.393 Loss=1.136 Prec@1=71.048 Prec@5=90.147 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:30 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.618 DataTime=0.393 Loss=1.136 Prec@1=71.048 Prec@5=90.147 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:31 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.138 Prec@1=71.028 Prec@5=90.142 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:31 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.138 Prec@1=71.028 Prec@5=90.142 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:31 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.138 Prec@1=71.028 Prec@5=90.142 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:32 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.139 Prec@1=70.991 Prec@5=90.137 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:32 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.139 Prec@1=70.991 Prec@5=90.137 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:32 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.393 Loss=1.139 Prec@1=70.991 Prec@5=90.137 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:34 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.140 Prec@1=70.977 Prec@5=90.117 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:34 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.140 Prec@1=70.977 Prec@5=90.117 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:34 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.140 Prec@1=70.977 Prec@5=90.117 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:35 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.141 Prec@1=70.955 Prec@5=90.107 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=04:35 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.141 Prec@1=70.955 Prec@5=90.107 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:35 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.141 Prec@1=70.955 Prec@5=90.107 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:36 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.142 Prec@1=70.920 Prec@5=90.093 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:36 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.142 Prec@1=70.920 Prec@5=90.093 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=04:36 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.142 Prec@1=70.920 Prec@5=90.093 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=04:37 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.144 Prec@1=70.889 Prec@5=90.070 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=04:37 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.144 Prec@1=70.889 Prec@5=90.070 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:37 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.617 DataTime=0.392 Loss=1.144 Prec@1=70.889 Prec@5=90.070 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:38 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.145 Prec@1=70.854 Prec@5=90.046 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:38 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.145 Prec@1=70.854 Prec@5=90.046 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:38 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.145 Prec@1=70.854 Prec@5=90.046 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:39 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.147 Prec@1=70.821 Prec@5=90.029 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:39 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.147 Prec@1=70.821 Prec@5=90.029 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:39 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.147 Prec@1=70.821 Prec@5=90.029 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:40 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.148 Prec@1=70.794 Prec@5=90.008 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:40 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.148 Prec@1=70.794 Prec@5=90.008 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:40 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.392 Loss=1.148 Prec@1=70.794 Prec@5=90.008 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:41 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.149 Prec@1=70.784 Prec@5=89.995 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:41 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.149 Prec@1=70.784 Prec@5=89.995 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:41 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.149 Prec@1=70.784 Prec@5=89.995 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:42 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.150 Prec@1=70.755 Prec@5=89.979 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:42 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.150 Prec@1=70.755 Prec@5=89.979 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:42 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.150 Prec@1=70.755 Prec@5=89.979 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:43 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.717 Prec@5=89.955 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:43 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.717 Prec@5=89.955 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:43 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.717 Prec@5=89.955 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:44 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.706 Prec@5=89.944 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:44 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.706 Prec@5=89.944 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:44 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.706 Prec@5=89.944 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:44 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.704 Prec@5=89.943 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:44 IST=> training   100.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.616 DataTime=0.391 Loss=1.152 Prec@1=70.704 Prec@5=89.943 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=04:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST=> validation 0.00% of 1x98...Epoch=87/150 LR=0.03858 Time=5.989 Loss=1.326 Prec@1=65.820 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST=> validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=5.989 Loss=1.326 Prec@1=65.820 Prec@5=87.305 rate=3232.33 Hz, eta=0:00:00, total=0:00:00, wall=04:44 IST** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=5.989 Loss=1.326 Prec@1=65.820 Prec@5=87.305 rate=3232.33 Hz, eta=0:00:00, total=0:00:00, wall=04:44 IST** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=0.404 Loss=1.302 Prec@1=67.358 Prec@5=88.416 rate=3232.33 Hz, eta=0:00:00, total=0:00:00, wall=04:44 IST** validation 100.00% of 1x98...Epoch=87/150 LR=0.03858 Time=0.404 Loss=1.302 Prec@1=67.358 Prec@5=88.416 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=04:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST=> training   0.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.917 DataTime=4.619 Loss=1.259 Prec@1=70.117 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.917 DataTime=4.619 Loss=1.259 Prec@1=70.117 Prec@5=88.672 rate=7208.14 Hz, eta=0:00:00, total=0:00:00, wall=04:45 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=4.917 DataTime=4.619 Loss=1.259 Prec@1=70.117 Prec@5=88.672 rate=7208.14 Hz, eta=0:00:00, total=0:00:00, wall=04:46 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.656 DataTime=0.430 Loss=1.116 Prec@1=71.552 Prec@5=90.443 rate=7208.14 Hz, eta=0:00:00, total=0:00:00, wall=04:46 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.656 DataTime=0.430 Loss=1.116 Prec@1=71.552 Prec@5=90.443 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=04:46 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.656 DataTime=0.430 Loss=1.116 Prec@1=71.552 Prec@5=90.443 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=04:47 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.635 DataTime=0.410 Loss=1.119 Prec@1=71.377 Prec@5=90.368 rate=1.64 Hz, eta=0:24:20, total=0:01:01, wall=04:47 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.635 DataTime=0.410 Loss=1.119 Prec@1=71.377 Prec@5=90.368 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:47 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.635 DataTime=0.410 Loss=1.119 Prec@1=71.377 Prec@5=90.368 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:48 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.628 DataTime=0.403 Loss=1.121 Prec@1=71.377 Prec@5=90.340 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:48 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.628 DataTime=0.403 Loss=1.121 Prec@1=71.377 Prec@5=90.340 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:48 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.628 DataTime=0.403 Loss=1.121 Prec@1=71.377 Prec@5=90.340 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:49 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.624 DataTime=0.400 Loss=1.122 Prec@1=71.410 Prec@5=90.326 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:49 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.624 DataTime=0.400 Loss=1.122 Prec@1=71.410 Prec@5=90.326 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:49 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.624 DataTime=0.400 Loss=1.122 Prec@1=71.410 Prec@5=90.326 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:50 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.622 DataTime=0.398 Loss=1.120 Prec@1=71.460 Prec@5=90.362 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=04:50 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.622 DataTime=0.398 Loss=1.120 Prec@1=71.460 Prec@5=90.362 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:50 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.622 DataTime=0.398 Loss=1.120 Prec@1=71.460 Prec@5=90.362 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:51 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.621 DataTime=0.397 Loss=1.123 Prec@1=71.389 Prec@5=90.325 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:51 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.621 DataTime=0.397 Loss=1.123 Prec@1=71.389 Prec@5=90.325 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:51 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.621 DataTime=0.397 Loss=1.123 Prec@1=71.389 Prec@5=90.325 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:52 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.620 DataTime=0.396 Loss=1.125 Prec@1=71.355 Prec@5=90.314 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:52 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.620 DataTime=0.396 Loss=1.125 Prec@1=71.355 Prec@5=90.314 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:52 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.620 DataTime=0.396 Loss=1.125 Prec@1=71.355 Prec@5=90.314 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:53 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.395 Loss=1.125 Prec@1=71.356 Prec@5=90.312 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:53 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.395 Loss=1.125 Prec@1=71.356 Prec@5=90.312 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:53 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.395 Loss=1.125 Prec@1=71.356 Prec@5=90.312 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:54 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.394 Loss=1.126 Prec@1=71.340 Prec@5=90.319 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:54 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.394 Loss=1.126 Prec@1=71.340 Prec@5=90.319 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:54 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.619 DataTime=0.394 Loss=1.126 Prec@1=71.340 Prec@5=90.319 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:55 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.394 Loss=1.126 Prec@1=71.324 Prec@5=90.309 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:55 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.394 Loss=1.126 Prec@1=71.324 Prec@5=90.309 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:55 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.394 Loss=1.126 Prec@1=71.324 Prec@5=90.309 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:56 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.128 Prec@1=71.276 Prec@5=90.272 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:56 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.128 Prec@1=71.276 Prec@5=90.272 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:56 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.128 Prec@1=71.276 Prec@5=90.272 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:57 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.129 Prec@1=71.231 Prec@5=90.248 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:57 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.129 Prec@1=71.231 Prec@5=90.248 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:57 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.129 Prec@1=71.231 Prec@5=90.248 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:58 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.131 Prec@1=71.195 Prec@5=90.221 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:58 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.131 Prec@1=71.195 Prec@5=90.221 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:58 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.618 DataTime=0.393 Loss=1.131 Prec@1=71.195 Prec@5=90.221 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:59 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.132 Prec@1=71.162 Prec@5=90.210 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:59 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.132 Prec@1=71.162 Prec@5=90.210 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:59 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.132 Prec@1=71.162 Prec@5=90.210 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:00 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.133 Prec@1=71.135 Prec@5=90.191 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:00 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.133 Prec@1=71.135 Prec@5=90.191 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:00 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.393 Loss=1.133 Prec@1=71.135 Prec@5=90.191 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:01 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.135 Prec@1=71.109 Prec@5=90.177 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:01 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.135 Prec@1=71.109 Prec@5=90.177 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:01 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.135 Prec@1=71.109 Prec@5=90.177 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:02 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.136 Prec@1=71.072 Prec@5=90.161 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:02 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.136 Prec@1=71.072 Prec@5=90.161 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:02 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.136 Prec@1=71.072 Prec@5=90.161 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:03 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.137 Prec@1=71.040 Prec@5=90.144 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:03 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.137 Prec@1=71.040 Prec@5=90.144 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:03 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.137 Prec@1=71.040 Prec@5=90.144 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:04 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.138 Prec@1=71.035 Prec@5=90.125 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:04 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.138 Prec@1=71.035 Prec@5=90.125 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:04 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.617 DataTime=0.392 Loss=1.138 Prec@1=71.035 Prec@5=90.125 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:05 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.139 Prec@1=71.002 Prec@5=90.111 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:05 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.139 Prec@1=71.002 Prec@5=90.111 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:05 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.139 Prec@1=71.002 Prec@5=90.111 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:06 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.140 Prec@1=70.979 Prec@5=90.102 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:06 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.140 Prec@1=70.979 Prec@5=90.102 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:06 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.140 Prec@1=70.979 Prec@5=90.102 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:07 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.141 Prec@1=70.949 Prec@5=90.085 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:07 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.141 Prec@1=70.949 Prec@5=90.085 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:07 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.141 Prec@1=70.949 Prec@5=90.085 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:08 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.142 Prec@1=70.931 Prec@5=90.068 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:08 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.142 Prec@1=70.931 Prec@5=90.068 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:08 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.392 Loss=1.142 Prec@1=70.931 Prec@5=90.068 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:09 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.923 Prec@5=90.062 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:09 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.923 Prec@5=90.062 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:09 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.923 Prec@5=90.062 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:10 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.917 Prec@5=90.049 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:10 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.917 Prec@5=90.049 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:10 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.917 Prec@5=90.049 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:10 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.917 Prec@5=90.049 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:10 IST=> training   100.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.616 DataTime=0.391 Loss=1.143 Prec@1=70.917 Prec@5=90.049 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=05:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> validation 0.00% of 1x98...Epoch=88/150 LR=0.03757 Time=5.660 Loss=1.342 Prec@1=69.336 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=5.660 Loss=1.342 Prec@1=69.336 Prec@5=87.109 rate=2083.41 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=5.660 Loss=1.342 Prec@1=69.336 Prec@5=87.109 rate=2083.41 Hz, eta=0:00:00, total=0:00:00, wall=05:11 IST** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=0.396 Loss=1.293 Prec@1=68.204 Prec@5=88.576 rate=2083.41 Hz, eta=0:00:00, total=0:00:00, wall=05:11 IST** validation 100.00% of 1x98...Epoch=88/150 LR=0.03757 Time=0.396 Loss=1.293 Prec@1=68.204 Prec@5=88.576 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=05:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:11 IST=> training   0.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.802 DataTime=5.505 Loss=1.021 Prec@1=73.828 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=05:11 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.802 DataTime=5.505 Loss=1.021 Prec@1=73.828 Prec@5=91.797 rate=7196.93 Hz, eta=0:00:00, total=0:00:00, wall=05:11 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.802 DataTime=5.505 Loss=1.021 Prec@1=73.828 Prec@5=91.797 rate=7196.93 Hz, eta=0:00:00, total=0:00:00, wall=05:12 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.662 DataTime=0.437 Loss=1.097 Prec@1=72.169 Prec@5=90.563 rate=7196.93 Hz, eta=0:00:00, total=0:00:00, wall=05:12 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.662 DataTime=0.437 Loss=1.097 Prec@1=72.169 Prec@5=90.563 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=05:12 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.662 DataTime=0.437 Loss=1.097 Prec@1=72.169 Prec@5=90.563 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=05:13 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.638 DataTime=0.414 Loss=1.103 Prec@1=71.893 Prec@5=90.559 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=05:13 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.638 DataTime=0.414 Loss=1.103 Prec@1=71.893 Prec@5=90.559 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=05:13 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.638 DataTime=0.414 Loss=1.103 Prec@1=71.893 Prec@5=90.559 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=05:14 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.629 DataTime=0.405 Loss=1.105 Prec@1=71.797 Prec@5=90.559 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=05:14 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.629 DataTime=0.405 Loss=1.105 Prec@1=71.797 Prec@5=90.559 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:14 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.629 DataTime=0.405 Loss=1.105 Prec@1=71.797 Prec@5=90.559 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:15 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.625 DataTime=0.401 Loss=1.110 Prec@1=71.689 Prec@5=90.496 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:15 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.625 DataTime=0.401 Loss=1.110 Prec@1=71.689 Prec@5=90.496 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:15 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.625 DataTime=0.401 Loss=1.110 Prec@1=71.689 Prec@5=90.496 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:16 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.623 DataTime=0.398 Loss=1.111 Prec@1=71.688 Prec@5=90.463 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:16 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.623 DataTime=0.398 Loss=1.111 Prec@1=71.688 Prec@5=90.463 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:16 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.623 DataTime=0.398 Loss=1.111 Prec@1=71.688 Prec@5=90.463 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:17 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.621 DataTime=0.397 Loss=1.112 Prec@1=71.630 Prec@5=90.464 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:17 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.621 DataTime=0.397 Loss=1.112 Prec@1=71.630 Prec@5=90.464 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:17 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.621 DataTime=0.397 Loss=1.112 Prec@1=71.630 Prec@5=90.464 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:18 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.396 Loss=1.113 Prec@1=71.582 Prec@5=90.444 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:18 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.396 Loss=1.113 Prec@1=71.582 Prec@5=90.444 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=05:18 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.396 Loss=1.113 Prec@1=71.582 Prec@5=90.444 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=05:19 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.395 Loss=1.114 Prec@1=71.533 Prec@5=90.436 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=05:19 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.395 Loss=1.114 Prec@1=71.533 Prec@5=90.436 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:19 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.620 DataTime=0.395 Loss=1.114 Prec@1=71.533 Prec@5=90.436 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:20 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.395 Loss=1.113 Prec@1=71.559 Prec@5=90.441 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:20 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.395 Loss=1.113 Prec@1=71.559 Prec@5=90.441 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:20 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.395 Loss=1.113 Prec@1=71.559 Prec@5=90.441 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:21 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.394 Loss=1.116 Prec@1=71.497 Prec@5=90.413 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:21 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.394 Loss=1.116 Prec@1=71.497 Prec@5=90.413 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:21 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.619 DataTime=0.394 Loss=1.116 Prec@1=71.497 Prec@5=90.413 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:22 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.394 Loss=1.118 Prec@1=71.442 Prec@5=90.396 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:22 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.394 Loss=1.118 Prec@1=71.442 Prec@5=90.396 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=05:22 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.394 Loss=1.118 Prec@1=71.442 Prec@5=90.396 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=05:23 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.120 Prec@1=71.380 Prec@5=90.363 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=05:23 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.120 Prec@1=71.380 Prec@5=90.363 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:23 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.120 Prec@1=71.380 Prec@5=90.363 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:24 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.122 Prec@1=71.345 Prec@5=90.340 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:24 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.122 Prec@1=71.345 Prec@5=90.340 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:24 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.618 DataTime=0.393 Loss=1.122 Prec@1=71.345 Prec@5=90.340 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:25 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.329 Prec@5=90.321 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:25 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.329 Prec@5=90.321 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=05:25 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.329 Prec@5=90.321 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=05:26 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.314 Prec@5=90.309 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=05:26 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.314 Prec@5=90.309 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:26 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.124 Prec@1=71.314 Prec@5=90.309 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:27 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.126 Prec@1=71.284 Prec@5=90.294 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:27 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.126 Prec@1=71.284 Prec@5=90.294 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:27 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.393 Loss=1.126 Prec@1=71.284 Prec@5=90.294 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:28 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.127 Prec@1=71.242 Prec@5=90.266 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:28 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.127 Prec@1=71.242 Prec@5=90.266 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=05:28 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.127 Prec@1=71.242 Prec@5=90.266 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=05:29 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.129 Prec@1=71.213 Prec@5=90.239 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=05:29 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.129 Prec@1=71.213 Prec@5=90.239 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:29 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.129 Prec@1=71.213 Prec@5=90.239 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:30 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.175 Prec@5=90.221 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:30 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.175 Prec@5=90.221 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:30 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.175 Prec@5=90.221 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:31 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.155 Prec@5=90.222 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:31 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.155 Prec@5=90.222 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:31 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.131 Prec@1=71.155 Prec@5=90.222 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:33 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.133 Prec@1=71.124 Prec@5=90.208 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:33 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.133 Prec@1=71.124 Prec@5=90.208 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:33 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.133 Prec@1=71.124 Prec@5=90.208 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:34 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.106 Prec@5=90.187 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:34 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.106 Prec@5=90.187 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:34 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.106 Prec@5=90.187 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:35 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.088 Prec@5=90.172 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:35 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.088 Prec@5=90.172 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:35 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.617 DataTime=0.392 Loss=1.134 Prec@1=71.088 Prec@5=90.172 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:36 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.135 Prec@1=71.066 Prec@5=90.157 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:36 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.135 Prec@1=71.066 Prec@5=90.157 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:36 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.135 Prec@1=71.066 Prec@5=90.157 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:37 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.136 Prec@1=71.043 Prec@5=90.147 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:37 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.136 Prec@1=71.043 Prec@5=90.147 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:37 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.136 Prec@1=71.043 Prec@5=90.147 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:37 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.136 Prec@1=71.041 Prec@5=90.147 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:37 IST=> training   100.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.616 DataTime=0.392 Loss=1.136 Prec@1=71.041 Prec@5=90.147 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=05:37 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> validation 0.00% of 1x98...Epoch=89/150 LR=0.03655 Time=7.205 Loss=1.168 Prec@1=71.875 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=7.205 Loss=1.168 Prec@1=71.875 Prec@5=89.453 rate=3928.49 Hz, eta=0:00:00, total=0:00:00, wall=05:37 IST** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=7.205 Loss=1.168 Prec@1=71.875 Prec@5=89.453 rate=3928.49 Hz, eta=0:00:00, total=0:00:00, wall=05:37 IST** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=0.403 Loss=1.310 Prec@1=67.764 Prec@5=88.446 rate=3928.49 Hz, eta=0:00:00, total=0:00:00, wall=05:37 IST** validation 100.00% of 1x98...Epoch=89/150 LR=0.03655 Time=0.403 Loss=1.310 Prec@1=67.764 Prec@5=88.446 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=05:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> training   0.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=6.106 DataTime=5.794 Loss=1.178 Prec@1=67.773 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=05:37 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=6.106 DataTime=5.794 Loss=1.178 Prec@1=67.773 Prec@5=89.453 rate=7136.99 Hz, eta=0:00:00, total=0:00:00, wall=05:37 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=6.106 DataTime=5.794 Loss=1.178 Prec@1=67.773 Prec@5=89.453 rate=7136.99 Hz, eta=0:00:00, total=0:00:00, wall=05:38 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.667 DataTime=0.439 Loss=1.098 Prec@1=71.958 Prec@5=90.631 rate=7136.99 Hz, eta=0:00:00, total=0:00:00, wall=05:38 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.667 DataTime=0.439 Loss=1.098 Prec@1=71.958 Prec@5=90.631 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=05:38 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.667 DataTime=0.439 Loss=1.098 Prec@1=71.958 Prec@5=90.631 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=05:39 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.640 DataTime=0.414 Loss=1.094 Prec@1=72.034 Prec@5=90.646 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=05:39 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.640 DataTime=0.414 Loss=1.094 Prec@1=72.034 Prec@5=90.646 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=05:39 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.640 DataTime=0.414 Loss=1.094 Prec@1=72.034 Prec@5=90.646 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=05:40 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.630 DataTime=0.405 Loss=1.096 Prec@1=71.931 Prec@5=90.610 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=05:40 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.630 DataTime=0.405 Loss=1.096 Prec@1=71.931 Prec@5=90.610 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:40 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.630 DataTime=0.405 Loss=1.096 Prec@1=71.931 Prec@5=90.610 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:41 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.626 DataTime=0.402 Loss=1.096 Prec@1=71.953 Prec@5=90.591 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=05:41 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.626 DataTime=0.402 Loss=1.096 Prec@1=71.953 Prec@5=90.591 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:41 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.626 DataTime=0.402 Loss=1.096 Prec@1=71.953 Prec@5=90.591 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:43 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.624 DataTime=0.399 Loss=1.100 Prec@1=71.886 Prec@5=90.529 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=05:43 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.624 DataTime=0.399 Loss=1.100 Prec@1=71.886 Prec@5=90.529 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:43 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.624 DataTime=0.399 Loss=1.100 Prec@1=71.886 Prec@5=90.529 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:44 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.622 DataTime=0.398 Loss=1.103 Prec@1=71.784 Prec@5=90.496 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=05:44 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.622 DataTime=0.398 Loss=1.103 Prec@1=71.784 Prec@5=90.496 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:44 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.622 DataTime=0.398 Loss=1.103 Prec@1=71.784 Prec@5=90.496 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:45 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.621 DataTime=0.397 Loss=1.105 Prec@1=71.766 Prec@5=90.470 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=05:45 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.621 DataTime=0.397 Loss=1.105 Prec@1=71.766 Prec@5=90.470 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=05:45 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.621 DataTime=0.397 Loss=1.105 Prec@1=71.766 Prec@5=90.470 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=05:46 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.620 DataTime=0.396 Loss=1.107 Prec@1=71.747 Prec@5=90.440 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=05:46 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.620 DataTime=0.396 Loss=1.107 Prec@1=71.747 Prec@5=90.440 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:46 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.620 DataTime=0.396 Loss=1.107 Prec@1=71.747 Prec@5=90.440 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:47 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.109 Prec@1=71.721 Prec@5=90.421 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=05:47 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.109 Prec@1=71.721 Prec@5=90.421 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:47 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.109 Prec@1=71.721 Prec@5=90.421 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:48 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.111 Prec@1=71.650 Prec@5=90.415 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=05:48 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.111 Prec@1=71.650 Prec@5=90.415 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:48 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.619 DataTime=0.395 Loss=1.111 Prec@1=71.650 Prec@5=90.415 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:49 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.112 Prec@1=71.634 Prec@5=90.402 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=05:49 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.112 Prec@1=71.634 Prec@5=90.402 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=05:49 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.112 Prec@1=71.634 Prec@5=90.402 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=05:50 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.113 Prec@1=71.603 Prec@5=90.392 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=05:50 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.113 Prec@1=71.603 Prec@5=90.392 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=05:50 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.618 DataTime=0.394 Loss=1.113 Prec@1=71.603 Prec@5=90.392 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=05:51 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.394 Loss=1.115 Prec@1=71.580 Prec@5=90.366 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=05:51 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.394 Loss=1.115 Prec@1=71.580 Prec@5=90.366 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:51 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.394 Loss=1.115 Prec@1=71.580 Prec@5=90.366 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:52 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.117 Prec@1=71.543 Prec@5=90.339 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=05:52 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.117 Prec@1=71.543 Prec@5=90.339 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=05:52 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.117 Prec@1=71.543 Prec@5=90.339 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=05:53 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.118 Prec@1=71.517 Prec@5=90.317 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=05:53 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.118 Prec@1=71.517 Prec@5=90.317 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:53 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.118 Prec@1=71.517 Prec@5=90.317 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:54 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.120 Prec@1=71.481 Prec@5=90.301 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=05:54 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.120 Prec@1=71.481 Prec@5=90.301 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=05:54 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.120 Prec@1=71.481 Prec@5=90.301 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=05:55 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.121 Prec@1=71.452 Prec@5=90.277 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=05:55 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.121 Prec@1=71.452 Prec@5=90.277 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=05:55 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.393 Loss=1.121 Prec@1=71.452 Prec@5=90.277 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=05:56 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.392 Loss=1.122 Prec@1=71.423 Prec@5=90.269 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=05:56 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.392 Loss=1.122 Prec@1=71.423 Prec@5=90.269 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=05:56 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.617 DataTime=0.392 Loss=1.122 Prec@1=71.423 Prec@5=90.269 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=05:57 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.123 Prec@1=71.400 Prec@5=90.258 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=05:57 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.123 Prec@1=71.400 Prec@5=90.258 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=05:57 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.123 Prec@1=71.400 Prec@5=90.258 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=05:58 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.124 Prec@1=71.382 Prec@5=90.249 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=05:58 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.124 Prec@1=71.382 Prec@5=90.249 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=05:58 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.124 Prec@1=71.382 Prec@5=90.249 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=05:59 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.362 Prec@5=90.237 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=05:59 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.362 Prec@5=90.237 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=05:59 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.362 Prec@5=90.237 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=06:00 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.345 Prec@5=90.236 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=06:00 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.345 Prec@5=90.236 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:00 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.125 Prec@1=71.345 Prec@5=90.236 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:01 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.126 Prec@1=71.328 Prec@5=90.232 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=06:01 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.126 Prec@1=71.328 Prec@5=90.232 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:01 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.126 Prec@1=71.328 Prec@5=90.232 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:02 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.303 Prec@5=90.224 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=06:02 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.303 Prec@5=90.224 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=06:02 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.303 Prec@5=90.224 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=06:03 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.292 Prec@5=90.215 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=06:03 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.292 Prec@5=90.215 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:03 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.292 Prec@5=90.215 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:03 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.291 Prec@5=90.215 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=06:03 IST=> training   100.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.616 DataTime=0.392 Loss=1.127 Prec@1=71.291 Prec@5=90.215 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=06:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST=> validation 0.00% of 1x98...Epoch=90/150 LR=0.03555 Time=6.222 Loss=1.312 Prec@1=67.383 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=06:03 IST=> validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=6.222 Loss=1.312 Prec@1=67.383 Prec@5=89.648 rate=6563.66 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=6.222 Loss=1.312 Prec@1=67.383 Prec@5=89.648 rate=6563.66 Hz, eta=0:00:00, total=0:00:00, wall=06:04 IST** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=0.402 Loss=1.296 Prec@1=67.904 Prec@5=88.570 rate=6563.66 Hz, eta=0:00:00, total=0:00:00, wall=06:04 IST** validation 100.00% of 1x98...Epoch=90/150 LR=0.03555 Time=0.402 Loss=1.296 Prec@1=67.904 Prec@5=88.570 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=06:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:04 IST=> training   0.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.819 DataTime=5.537 Loss=1.143 Prec@1=72.852 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=06:04 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.819 DataTime=5.537 Loss=1.143 Prec@1=72.852 Prec@5=89.258 rate=7789.98 Hz, eta=0:00:00, total=0:00:00, wall=06:04 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.819 DataTime=5.537 Loss=1.143 Prec@1=72.852 Prec@5=89.258 rate=7789.98 Hz, eta=0:00:00, total=0:00:00, wall=06:05 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.664 DataTime=0.436 Loss=1.110 Prec@1=71.774 Prec@5=90.472 rate=7789.98 Hz, eta=0:00:00, total=0:00:00, wall=06:05 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.664 DataTime=0.436 Loss=1.110 Prec@1=71.774 Prec@5=90.472 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:05 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.664 DataTime=0.436 Loss=1.110 Prec@1=71.774 Prec@5=90.472 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:06 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.641 DataTime=0.414 Loss=1.091 Prec@1=72.068 Prec@5=90.716 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:06 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.641 DataTime=0.414 Loss=1.091 Prec@1=72.068 Prec@5=90.716 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=06:06 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.641 DataTime=0.414 Loss=1.091 Prec@1=72.068 Prec@5=90.716 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=06:07 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.634 DataTime=0.407 Loss=1.094 Prec@1=71.985 Prec@5=90.704 rate=1.63 Hz, eta=0:23:28, total=0:02:03, wall=06:07 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.634 DataTime=0.407 Loss=1.094 Prec@1=71.985 Prec@5=90.704 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=06:07 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.634 DataTime=0.407 Loss=1.094 Prec@1=71.985 Prec@5=90.704 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=06:08 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.629 DataTime=0.403 Loss=1.095 Prec@1=71.919 Prec@5=90.681 rate=1.63 Hz, eta=0:22:32, total=0:03:04, wall=06:08 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.629 DataTime=0.403 Loss=1.095 Prec@1=71.919 Prec@5=90.681 rate=1.63 Hz, eta=0:21:31, total=0:04:06, wall=06:08 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.629 DataTime=0.403 Loss=1.095 Prec@1=71.919 Prec@5=90.681 rate=1.63 Hz, eta=0:21:31, total=0:04:06, wall=06:09 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.626 DataTime=0.400 Loss=1.095 Prec@1=71.917 Prec@5=90.698 rate=1.63 Hz, eta=0:21:31, total=0:04:06, wall=06:09 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.626 DataTime=0.400 Loss=1.095 Prec@1=71.917 Prec@5=90.698 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=06:09 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.626 DataTime=0.400 Loss=1.095 Prec@1=71.917 Prec@5=90.698 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=06:10 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.624 DataTime=0.399 Loss=1.095 Prec@1=71.953 Prec@5=90.678 rate=1.63 Hz, eta=0:20:29, total=0:05:07, wall=06:10 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.624 DataTime=0.399 Loss=1.095 Prec@1=71.953 Prec@5=90.678 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=06:10 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.624 DataTime=0.399 Loss=1.095 Prec@1=71.953 Prec@5=90.678 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=06:11 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.623 DataTime=0.397 Loss=1.099 Prec@1=71.910 Prec@5=90.614 rate=1.63 Hz, eta=0:19:28, total=0:06:09, wall=06:11 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.623 DataTime=0.397 Loss=1.099 Prec@1=71.910 Prec@5=90.614 rate=1.63 Hz, eta=0:18:27, total=0:07:10, wall=06:11 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.623 DataTime=0.397 Loss=1.099 Prec@1=71.910 Prec@5=90.614 rate=1.63 Hz, eta=0:18:27, total=0:07:10, wall=06:12 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.622 DataTime=0.397 Loss=1.100 Prec@1=71.891 Prec@5=90.593 rate=1.63 Hz, eta=0:18:27, total=0:07:10, wall=06:12 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.622 DataTime=0.397 Loss=1.100 Prec@1=71.891 Prec@5=90.593 rate=1.63 Hz, eta=0:17:26, total=0:08:12, wall=06:12 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.622 DataTime=0.397 Loss=1.100 Prec@1=71.891 Prec@5=90.593 rate=1.63 Hz, eta=0:17:26, total=0:08:12, wall=06:13 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.621 DataTime=0.396 Loss=1.101 Prec@1=71.863 Prec@5=90.571 rate=1.63 Hz, eta=0:17:26, total=0:08:12, wall=06:13 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.621 DataTime=0.396 Loss=1.101 Prec@1=71.863 Prec@5=90.571 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=06:13 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.621 DataTime=0.396 Loss=1.101 Prec@1=71.863 Prec@5=90.571 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=06:14 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.102 Prec@1=71.830 Prec@5=90.556 rate=1.63 Hz, eta=0:16:24, total=0:09:13, wall=06:14 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.102 Prec@1=71.830 Prec@5=90.556 rate=1.63 Hz, eta=0:15:23, total=0:10:15, wall=06:14 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.102 Prec@1=71.830 Prec@5=90.556 rate=1.63 Hz, eta=0:15:23, total=0:10:15, wall=06:15 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.103 Prec@1=71.782 Prec@5=90.548 rate=1.63 Hz, eta=0:15:23, total=0:10:15, wall=06:15 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.103 Prec@1=71.782 Prec@5=90.548 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=06:15 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.620 DataTime=0.395 Loss=1.103 Prec@1=71.782 Prec@5=90.548 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=06:16 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.395 Loss=1.104 Prec@1=71.765 Prec@5=90.545 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=06:16 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.395 Loss=1.104 Prec@1=71.765 Prec@5=90.545 rate=1.63 Hz, eta=0:13:20, total=0:12:17, wall=06:16 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.395 Loss=1.104 Prec@1=71.765 Prec@5=90.545 rate=1.63 Hz, eta=0:13:20, total=0:12:17, wall=06:17 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.106 Prec@1=71.734 Prec@5=90.527 rate=1.63 Hz, eta=0:13:20, total=0:12:17, wall=06:17 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.106 Prec@1=71.734 Prec@5=90.527 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=06:17 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.106 Prec@1=71.734 Prec@5=90.527 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=06:18 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.107 Prec@1=71.713 Prec@5=90.517 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=06:18 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.107 Prec@1=71.713 Prec@5=90.517 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=06:18 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.619 DataTime=0.394 Loss=1.107 Prec@1=71.713 Prec@5=90.517 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=06:19 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.394 Loss=1.109 Prec@1=71.680 Prec@5=90.497 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=06:19 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.394 Loss=1.109 Prec@1=71.680 Prec@5=90.497 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=06:19 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.394 Loss=1.109 Prec@1=71.680 Prec@5=90.497 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=06:20 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.110 Prec@1=71.643 Prec@5=90.464 rate=1.63 Hz, eta=0:10:15, total=0:15:22, wall=06:20 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.110 Prec@1=71.643 Prec@5=90.464 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=06:20 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.110 Prec@1=71.643 Prec@5=90.464 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=06:21 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.112 Prec@1=71.624 Prec@5=90.449 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=06:21 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.112 Prec@1=71.624 Prec@5=90.449 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:21 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.618 DataTime=0.393 Loss=1.112 Prec@1=71.624 Prec@5=90.449 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:22 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.113 Prec@1=71.592 Prec@5=90.432 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:22 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.113 Prec@1=71.592 Prec@5=90.432 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=06:22 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.113 Prec@1=71.592 Prec@5=90.432 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=06:23 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.115 Prec@1=71.563 Prec@5=90.422 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=06:23 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.115 Prec@1=71.563 Prec@5=90.422 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:23 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.393 Loss=1.115 Prec@1=71.563 Prec@5=90.422 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:24 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.116 Prec@1=71.517 Prec@5=90.400 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:24 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.116 Prec@1=71.517 Prec@5=90.400 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:24 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.116 Prec@1=71.517 Prec@5=90.400 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:25 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.117 Prec@1=71.491 Prec@5=90.388 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:25 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.117 Prec@1=71.491 Prec@5=90.388 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=06:25 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.117 Prec@1=71.491 Prec@5=90.388 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=06:26 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.118 Prec@1=71.465 Prec@5=90.378 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=06:26 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.118 Prec@1=71.465 Prec@5=90.378 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:26 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.617 DataTime=0.392 Loss=1.118 Prec@1=71.465 Prec@5=90.378 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:27 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.119 Prec@1=71.449 Prec@5=90.365 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:27 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.119 Prec@1=71.449 Prec@5=90.365 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:27 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.119 Prec@1=71.449 Prec@5=90.365 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:28 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.422 Prec@5=90.351 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:28 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.422 Prec@5=90.351 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:28 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.422 Prec@5=90.351 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:29 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.409 Prec@5=90.339 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:29 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.409 Prec@5=90.339 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:29 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.409 Prec@5=90.339 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:29 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.407 Prec@5=90.338 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:29 IST=> training   100.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.616 DataTime=0.392 Loss=1.121 Prec@1=71.407 Prec@5=90.338 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=06:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> validation 0.00% of 1x98...Epoch=91/150 LR=0.03455 Time=7.216 Loss=1.228 Prec@1=67.383 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=7.216 Loss=1.228 Prec@1=67.383 Prec@5=88.867 rate=5648.76 Hz, eta=0:00:00, total=0:00:00, wall=06:30 IST** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=7.216 Loss=1.228 Prec@1=67.383 Prec@5=88.867 rate=5648.76 Hz, eta=0:00:00, total=0:00:00, wall=06:30 IST** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=0.402 Loss=1.305 Prec@1=67.802 Prec@5=88.516 rate=5648.76 Hz, eta=0:00:00, total=0:00:00, wall=06:30 IST** validation 100.00% of 1x98...Epoch=91/150 LR=0.03455 Time=0.402 Loss=1.305 Prec@1=67.802 Prec@5=88.516 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=06:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> training   0.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=6.057 DataTime=5.786 Loss=1.133 Prec@1=70.117 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=06:30 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=6.057 DataTime=5.786 Loss=1.133 Prec@1=70.117 Prec@5=91.016 rate=8167.13 Hz, eta=0:00:00, total=0:00:00, wall=06:30 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=6.057 DataTime=5.786 Loss=1.133 Prec@1=70.117 Prec@5=91.016 rate=8167.13 Hz, eta=0:00:00, total=0:00:00, wall=06:31 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.669 DataTime=0.440 Loss=1.081 Prec@1=72.364 Prec@5=90.927 rate=8167.13 Hz, eta=0:00:00, total=0:00:00, wall=06:31 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.669 DataTime=0.440 Loss=1.081 Prec@1=72.364 Prec@5=90.927 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=06:31 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.669 DataTime=0.440 Loss=1.081 Prec@1=72.364 Prec@5=90.927 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=06:32 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.642 DataTime=0.415 Loss=1.075 Prec@1=72.466 Prec@5=90.991 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=06:32 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.642 DataTime=0.415 Loss=1.075 Prec@1=72.466 Prec@5=90.991 rate=1.63 Hz, eta=0:23:27, total=0:02:02, wall=06:32 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.642 DataTime=0.415 Loss=1.075 Prec@1=72.466 Prec@5=90.991 rate=1.63 Hz, eta=0:23:27, total=0:02:02, wall=06:33 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.633 DataTime=0.407 Loss=1.078 Prec@1=72.382 Prec@5=90.935 rate=1.63 Hz, eta=0:23:27, total=0:02:02, wall=06:33 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.633 DataTime=0.407 Loss=1.078 Prec@1=72.382 Prec@5=90.935 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=06:33 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.633 DataTime=0.407 Loss=1.078 Prec@1=72.382 Prec@5=90.935 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=06:34 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.628 DataTime=0.403 Loss=1.080 Prec@1=72.373 Prec@5=90.910 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=06:34 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.628 DataTime=0.403 Loss=1.080 Prec@1=72.373 Prec@5=90.910 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=06:34 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.628 DataTime=0.403 Loss=1.080 Prec@1=72.373 Prec@5=90.910 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=06:35 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.625 DataTime=0.400 Loss=1.086 Prec@1=72.231 Prec@5=90.768 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=06:35 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.625 DataTime=0.400 Loss=1.086 Prec@1=72.231 Prec@5=90.768 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=06:35 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.625 DataTime=0.400 Loss=1.086 Prec@1=72.231 Prec@5=90.768 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=06:36 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.623 DataTime=0.398 Loss=1.087 Prec@1=72.158 Prec@5=90.752 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=06:36 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.623 DataTime=0.398 Loss=1.087 Prec@1=72.158 Prec@5=90.752 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:36 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.623 DataTime=0.398 Loss=1.087 Prec@1=72.158 Prec@5=90.752 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:37 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.622 DataTime=0.397 Loss=1.090 Prec@1=72.111 Prec@5=90.701 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:37 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.622 DataTime=0.397 Loss=1.090 Prec@1=72.111 Prec@5=90.701 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:37 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.622 DataTime=0.397 Loss=1.090 Prec@1=72.111 Prec@5=90.701 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:38 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.621 DataTime=0.396 Loss=1.092 Prec@1=72.073 Prec@5=90.666 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:38 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.621 DataTime=0.396 Loss=1.092 Prec@1=72.073 Prec@5=90.666 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:38 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.621 DataTime=0.396 Loss=1.092 Prec@1=72.073 Prec@5=90.666 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:39 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.095 Prec@1=72.027 Prec@5=90.628 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=06:39 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.095 Prec@1=72.027 Prec@5=90.628 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:39 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.095 Prec@1=72.027 Prec@5=90.628 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:40 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.096 Prec@1=72.017 Prec@5=90.614 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:40 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.096 Prec@1=72.017 Prec@5=90.614 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:40 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.620 DataTime=0.395 Loss=1.096 Prec@1=72.017 Prec@5=90.614 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:41 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.097 Prec@1=71.995 Prec@5=90.594 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:41 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.097 Prec@1=71.995 Prec@5=90.594 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:41 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.097 Prec@1=71.995 Prec@5=90.594 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:42 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.098 Prec@1=71.973 Prec@5=90.575 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:42 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.098 Prec@1=71.973 Prec@5=90.575 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:42 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.619 DataTime=0.394 Loss=1.098 Prec@1=71.973 Prec@5=90.575 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:44 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.100 Prec@1=71.925 Prec@5=90.554 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:44 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.100 Prec@1=71.925 Prec@5=90.554 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:44 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.100 Prec@1=71.925 Prec@5=90.554 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:45 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.102 Prec@1=71.888 Prec@5=90.540 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:45 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.102 Prec@1=71.888 Prec@5=90.540 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:45 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.394 Loss=1.102 Prec@1=71.888 Prec@5=90.540 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:46 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.103 Prec@1=71.853 Prec@5=90.522 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:46 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.103 Prec@1=71.853 Prec@5=90.522 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:46 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.103 Prec@1=71.853 Prec@5=90.522 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:47 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.104 Prec@1=71.849 Prec@5=90.510 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:47 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.104 Prec@1=71.849 Prec@5=90.510 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:47 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.618 DataTime=0.393 Loss=1.104 Prec@1=71.849 Prec@5=90.510 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:48 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.841 Prec@5=90.512 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:48 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.841 Prec@5=90.512 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:48 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.841 Prec@5=90.512 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:49 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.829 Prec@5=90.511 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:49 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.829 Prec@5=90.511 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:49 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.104 Prec@1=71.829 Prec@5=90.511 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:50 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.106 Prec@1=71.806 Prec@5=90.492 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:50 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.106 Prec@1=71.806 Prec@5=90.492 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:50 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.393 Loss=1.106 Prec@1=71.806 Prec@5=90.492 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:51 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.107 Prec@1=71.774 Prec@5=90.478 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=06:51 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.107 Prec@1=71.774 Prec@5=90.478 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:51 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.107 Prec@1=71.774 Prec@5=90.478 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:52 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.108 Prec@1=71.754 Prec@5=90.466 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:52 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.108 Prec@1=71.754 Prec@5=90.466 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:52 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.108 Prec@1=71.754 Prec@5=90.466 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:53 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.109 Prec@1=71.738 Prec@5=90.458 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:53 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.109 Prec@1=71.738 Prec@5=90.458 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:53 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.109 Prec@1=71.738 Prec@5=90.458 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:54 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.110 Prec@1=71.720 Prec@5=90.453 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:54 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.110 Prec@1=71.720 Prec@5=90.453 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:54 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.110 Prec@1=71.720 Prec@5=90.453 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:55 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.111 Prec@1=71.693 Prec@5=90.437 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=06:55 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.111 Prec@1=71.693 Prec@5=90.437 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:55 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.617 DataTime=0.392 Loss=1.111 Prec@1=71.693 Prec@5=90.437 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:56 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.616 DataTime=0.392 Loss=1.112 Prec@1=71.669 Prec@5=90.421 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:56 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.616 DataTime=0.392 Loss=1.112 Prec@1=71.669 Prec@5=90.421 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:56 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.616 DataTime=0.392 Loss=1.112 Prec@1=71.669 Prec@5=90.421 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:56 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.616 DataTime=0.392 Loss=1.112 Prec@1=71.667 Prec@5=90.421 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:56 IST=> training   100.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.616 DataTime=0.392 Loss=1.112 Prec@1=71.667 Prec@5=90.421 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=06:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST=> validation 0.00% of 1x98...Epoch=92/150 LR=0.03356 Time=7.260 Loss=1.209 Prec@1=69.531 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST=> validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=7.260 Loss=1.209 Prec@1=69.531 Prec@5=89.844 rate=7011.10 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=7.260 Loss=1.209 Prec@1=69.531 Prec@5=89.844 rate=7011.10 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=0.410 Loss=1.278 Prec@1=68.506 Prec@5=88.748 rate=7011.10 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST** validation 100.00% of 1x98...Epoch=92/150 LR=0.03356 Time=0.410 Loss=1.278 Prec@1=68.506 Prec@5=88.748 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=06:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST=> training   0.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.019 DataTime=4.704 Loss=1.245 Prec@1=67.578 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=06:57 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.019 DataTime=4.704 Loss=1.245 Prec@1=67.578 Prec@5=89.258 rate=4108.06 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.019 DataTime=4.704 Loss=1.245 Prec@1=67.578 Prec@5=89.258 rate=4108.06 Hz, eta=0:00:00, total=0:00:00, wall=06:58 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.657 DataTime=0.428 Loss=1.067 Prec@1=72.674 Prec@5=91.018 rate=4108.06 Hz, eta=0:00:00, total=0:00:00, wall=06:58 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.657 DataTime=0.428 Loss=1.067 Prec@1=72.674 Prec@5=91.018 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=06:58 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.657 DataTime=0.428 Loss=1.067 Prec@1=72.674 Prec@5=91.018 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=06:59 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.636 DataTime=0.409 Loss=1.074 Prec@1=72.441 Prec@5=90.930 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=06:59 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.636 DataTime=0.409 Loss=1.074 Prec@1=72.441 Prec@5=90.930 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=06:59 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.636 DataTime=0.409 Loss=1.074 Prec@1=72.441 Prec@5=90.930 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=07:00 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.629 DataTime=0.403 Loss=1.075 Prec@1=72.383 Prec@5=90.910 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=07:00 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.629 DataTime=0.403 Loss=1.075 Prec@1=72.383 Prec@5=90.910 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=07:00 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.629 DataTime=0.403 Loss=1.075 Prec@1=72.383 Prec@5=90.910 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=07:01 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.625 DataTime=0.400 Loss=1.075 Prec@1=72.374 Prec@5=90.876 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=07:01 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.625 DataTime=0.400 Loss=1.075 Prec@1=72.374 Prec@5=90.876 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=07:01 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.625 DataTime=0.400 Loss=1.075 Prec@1=72.374 Prec@5=90.876 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=07:02 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.623 DataTime=0.398 Loss=1.079 Prec@1=72.321 Prec@5=90.808 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=07:02 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.623 DataTime=0.398 Loss=1.079 Prec@1=72.321 Prec@5=90.808 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=07:02 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.623 DataTime=0.398 Loss=1.079 Prec@1=72.321 Prec@5=90.808 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=07:03 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.622 DataTime=0.396 Loss=1.080 Prec@1=72.312 Prec@5=90.800 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=07:03 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.622 DataTime=0.396 Loss=1.080 Prec@1=72.312 Prec@5=90.800 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:03 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.622 DataTime=0.396 Loss=1.080 Prec@1=72.312 Prec@5=90.800 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:04 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.621 DataTime=0.395 Loss=1.082 Prec@1=72.311 Prec@5=90.776 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:04 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.621 DataTime=0.395 Loss=1.082 Prec@1=72.311 Prec@5=90.776 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:04 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.621 DataTime=0.395 Loss=1.082 Prec@1=72.311 Prec@5=90.776 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:05 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.620 DataTime=0.394 Loss=1.085 Prec@1=72.231 Prec@5=90.734 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:05 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.620 DataTime=0.394 Loss=1.085 Prec@1=72.231 Prec@5=90.734 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:05 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.620 DataTime=0.394 Loss=1.085 Prec@1=72.231 Prec@5=90.734 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:06 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.394 Loss=1.085 Prec@1=72.210 Prec@5=90.744 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:06 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.394 Loss=1.085 Prec@1=72.210 Prec@5=90.744 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=07:06 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.394 Loss=1.085 Prec@1=72.210 Prec@5=90.744 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=07:07 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.393 Loss=1.086 Prec@1=72.198 Prec@5=90.741 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=07:07 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.393 Loss=1.086 Prec@1=72.198 Prec@5=90.741 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=07:07 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.619 DataTime=0.393 Loss=1.086 Prec@1=72.198 Prec@5=90.741 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=07:08 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.088 Prec@1=72.139 Prec@5=90.709 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=07:08 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.088 Prec@1=72.139 Prec@5=90.709 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:08 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.088 Prec@1=72.139 Prec@5=90.709 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:09 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.090 Prec@1=72.124 Prec@5=90.685 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:09 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.090 Prec@1=72.124 Prec@5=90.685 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=07:09 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.090 Prec@1=72.124 Prec@5=90.685 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=07:10 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.091 Prec@1=72.118 Prec@5=90.672 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=07:10 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.091 Prec@1=72.118 Prec@5=90.672 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=07:10 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.393 Loss=1.091 Prec@1=72.118 Prec@5=90.672 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=07:11 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.392 Loss=1.091 Prec@1=72.118 Prec@5=90.673 rate=1.63 Hz, eta=0:12:18, total=0:13:18, wall=07:11 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.392 Loss=1.091 Prec@1=72.118 Prec@5=90.673 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=07:11 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.618 DataTime=0.392 Loss=1.091 Prec@1=72.118 Prec@5=90.673 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=07:12 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.082 Prec@5=90.652 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=07:12 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.082 Prec@5=90.652 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=07:12 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.082 Prec@5=90.652 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=07:13 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.052 Prec@5=90.644 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=07:13 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.052 Prec@5=90.644 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=07:13 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.052 Prec@5=90.644 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=07:14 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.095 Prec@1=72.025 Prec@5=90.624 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=07:14 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.095 Prec@1=72.025 Prec@5=90.624 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=07:14 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.095 Prec@1=72.025 Prec@5=90.624 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=07:15 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.096 Prec@1=71.988 Prec@5=90.615 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=07:15 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.096 Prec@1=71.988 Prec@5=90.615 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=07:15 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.096 Prec@1=71.988 Prec@5=90.615 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=07:16 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.097 Prec@1=71.961 Prec@5=90.599 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=07:16 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.097 Prec@1=71.961 Prec@5=90.599 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=07:16 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.097 Prec@1=71.961 Prec@5=90.599 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=07:17 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.098 Prec@1=71.937 Prec@5=90.581 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=07:17 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.098 Prec@1=71.937 Prec@5=90.581 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:17 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.392 Loss=1.098 Prec@1=71.937 Prec@5=90.581 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:18 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.391 Loss=1.099 Prec@1=71.912 Prec@5=90.573 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:18 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.391 Loss=1.099 Prec@1=71.912 Prec@5=90.573 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=07:18 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.617 DataTime=0.391 Loss=1.099 Prec@1=71.912 Prec@5=90.573 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=07:19 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.100 Prec@1=71.888 Prec@5=90.559 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=07:19 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.100 Prec@1=71.888 Prec@5=90.559 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:19 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.100 Prec@1=71.888 Prec@5=90.559 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:20 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.852 Prec@5=90.542 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:20 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.852 Prec@5=90.542 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=07:20 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.852 Prec@5=90.542 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=07:21 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.838 Prec@5=90.532 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=07:21 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.838 Prec@5=90.532 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:21 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.102 Prec@1=71.838 Prec@5=90.532 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:22 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.103 Prec@1=71.824 Prec@5=90.536 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.103 Prec@1=71.824 Prec@5=90.536 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.103 Prec@1=71.824 Prec@5=90.536 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.103 Prec@1=71.823 Prec@5=90.536 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=07:22 IST=> training   100.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.616 DataTime=0.391 Loss=1.103 Prec@1=71.823 Prec@5=90.536 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=07:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 0.00% of 1x98...Epoch=93/150 LR=0.03257 Time=6.895 Loss=1.321 Prec@1=68.555 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=6.895 Loss=1.321 Prec@1=68.555 Prec@5=88.086 rate=5591.40 Hz, eta=0:00:00, total=0:00:00, wall=07:22 IST** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=6.895 Loss=1.321 Prec@1=68.555 Prec@5=88.086 rate=5591.40 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=0.404 Loss=1.275 Prec@1=68.388 Prec@5=88.938 rate=5591.40 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST** validation 100.00% of 1x98...Epoch=93/150 LR=0.03257 Time=0.404 Loss=1.275 Prec@1=68.388 Prec@5=88.938 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=07:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.929 DataTime=5.439 Loss=1.059 Prec@1=70.898 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.929 DataTime=5.439 Loss=1.059 Prec@1=70.898 Prec@5=91.406 rate=8911.94 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.929 DataTime=5.439 Loss=1.059 Prec@1=70.898 Prec@5=91.406 rate=8911.94 Hz, eta=0:00:00, total=0:00:00, wall=07:24 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.663 DataTime=0.436 Loss=1.074 Prec@1=72.399 Prec@5=90.828 rate=8911.94 Hz, eta=0:00:00, total=0:00:00, wall=07:24 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.663 DataTime=0.436 Loss=1.074 Prec@1=72.399 Prec@5=90.828 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=07:24 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.663 DataTime=0.436 Loss=1.074 Prec@1=72.399 Prec@5=90.828 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=07:25 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.639 DataTime=0.413 Loss=1.071 Prec@1=72.484 Prec@5=90.925 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=07:25 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.639 DataTime=0.413 Loss=1.071 Prec@1=72.484 Prec@5=90.925 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:25 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.639 DataTime=0.413 Loss=1.071 Prec@1=72.484 Prec@5=90.925 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:26 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.631 DataTime=0.405 Loss=1.071 Prec@1=72.543 Prec@5=90.908 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=07:26 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.631 DataTime=0.405 Loss=1.071 Prec@1=72.543 Prec@5=90.908 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:26 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.631 DataTime=0.405 Loss=1.071 Prec@1=72.543 Prec@5=90.908 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:27 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.627 DataTime=0.401 Loss=1.072 Prec@1=72.503 Prec@5=90.901 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:27 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.627 DataTime=0.401 Loss=1.072 Prec@1=72.503 Prec@5=90.901 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:27 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.627 DataTime=0.401 Loss=1.072 Prec@1=72.503 Prec@5=90.901 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:28 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.624 DataTime=0.399 Loss=1.075 Prec@1=72.422 Prec@5=90.898 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:28 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.624 DataTime=0.399 Loss=1.075 Prec@1=72.422 Prec@5=90.898 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=07:28 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.624 DataTime=0.399 Loss=1.075 Prec@1=72.422 Prec@5=90.898 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=07:29 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.622 DataTime=0.397 Loss=1.075 Prec@1=72.373 Prec@5=90.867 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=07:29 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.622 DataTime=0.397 Loss=1.075 Prec@1=72.373 Prec@5=90.867 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=07:29 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.622 DataTime=0.397 Loss=1.075 Prec@1=72.373 Prec@5=90.867 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=07:30 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.621 DataTime=0.396 Loss=1.077 Prec@1=72.364 Prec@5=90.836 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=07:30 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.621 DataTime=0.396 Loss=1.077 Prec@1=72.364 Prec@5=90.836 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=07:30 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.621 DataTime=0.396 Loss=1.077 Prec@1=72.364 Prec@5=90.836 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=07:31 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.396 Loss=1.079 Prec@1=72.335 Prec@5=90.795 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=07:31 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.396 Loss=1.079 Prec@1=72.335 Prec@5=90.795 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=07:31 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.396 Loss=1.079 Prec@1=72.335 Prec@5=90.795 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=07:32 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.395 Loss=1.081 Prec@1=72.313 Prec@5=90.777 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=07:32 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.395 Loss=1.081 Prec@1=72.313 Prec@5=90.777 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:32 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.620 DataTime=0.395 Loss=1.081 Prec@1=72.313 Prec@5=90.777 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:33 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.081 Prec@1=72.299 Prec@5=90.771 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:33 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.081 Prec@1=72.299 Prec@5=90.771 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:33 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.081 Prec@1=72.299 Prec@5=90.771 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:34 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.084 Prec@1=72.250 Prec@5=90.750 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:34 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.084 Prec@1=72.250 Prec@5=90.750 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=07:34 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.619 DataTime=0.394 Loss=1.084 Prec@1=72.250 Prec@5=90.750 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=07:35 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.086 Prec@1=72.200 Prec@5=90.735 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=07:35 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.086 Prec@1=72.200 Prec@5=90.735 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:35 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.086 Prec@1=72.200 Prec@5=90.735 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:36 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.191 Prec@5=90.736 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:36 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.191 Prec@5=90.736 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=07:36 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.191 Prec@5=90.736 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=07:37 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.167 Prec@5=90.728 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=07:37 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.167 Prec@5=90.728 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=07:37 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.618 DataTime=0.393 Loss=1.087 Prec@1=72.167 Prec@5=90.728 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=07:38 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.393 Loss=1.089 Prec@1=72.138 Prec@5=90.699 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=07:38 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.393 Loss=1.089 Prec@1=72.138 Prec@5=90.699 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:38 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.393 Loss=1.089 Prec@1=72.138 Prec@5=90.699 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:39 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.109 Prec@5=90.677 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:39 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.109 Prec@5=90.677 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:39 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.109 Prec@5=90.677 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:40 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.112 Prec@5=90.675 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:40 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.112 Prec@5=90.675 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:40 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.090 Prec@1=72.112 Prec@5=90.675 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:41 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.091 Prec@1=72.092 Prec@5=90.671 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:41 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.091 Prec@1=72.092 Prec@5=90.671 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:41 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.091 Prec@1=72.092 Prec@5=90.671 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:43 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.061 Prec@5=90.666 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:43 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.061 Prec@5=90.666 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:43 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.092 Prec@1=72.061 Prec@5=90.666 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:44 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.045 Prec@5=90.658 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:44 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.045 Prec@5=90.658 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:44 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.093 Prec@1=72.045 Prec@5=90.658 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:45 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.032 Prec@5=90.646 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=07:45 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.032 Prec@5=90.646 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:45 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.032 Prec@5=90.646 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:46 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.018 Prec@5=90.638 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:46 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.018 Prec@5=90.638 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:46 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.617 DataTime=0.392 Loss=1.094 Prec@1=72.018 Prec@5=90.638 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:47 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.392 Loss=1.095 Prec@1=71.999 Prec@5=90.627 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=07:47 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.392 Loss=1.095 Prec@1=71.999 Prec@5=90.627 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=07:47 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.392 Loss=1.095 Prec@1=71.999 Prec@5=90.627 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=07:48 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.095 Prec@1=71.998 Prec@5=90.626 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=07:48 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.095 Prec@1=71.998 Prec@5=90.626 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:48 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.095 Prec@1=71.998 Prec@5=90.626 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:49 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.096 Prec@1=71.987 Prec@5=90.618 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=07:49 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.096 Prec@1=71.987 Prec@5=90.618 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=07:49 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.096 Prec@1=71.987 Prec@5=90.618 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=07:49 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.096 Prec@1=71.987 Prec@5=90.617 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=07:49 IST=> training   100.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.616 DataTime=0.391 Loss=1.096 Prec@1=71.987 Prec@5=90.617 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=07:49 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> validation 0.00% of 1x98...Epoch=94/150 LR=0.03159 Time=7.045 Loss=1.325 Prec@1=66.992 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=7.045 Loss=1.325 Prec@1=66.992 Prec@5=89.453 rate=5544.99 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=7.045 Loss=1.325 Prec@1=66.992 Prec@5=89.453 rate=5544.99 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=0.406 Loss=1.267 Prec@1=68.548 Prec@5=89.056 rate=5544.99 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST** validation 100.00% of 1x98...Epoch=94/150 LR=0.03159 Time=0.406 Loss=1.267 Prec@1=68.548 Prec@5=89.056 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=07:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> training   0.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.022 DataTime=4.618 Loss=0.999 Prec@1=74.023 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=07:49 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.022 DataTime=4.618 Loss=0.999 Prec@1=74.023 Prec@5=91.797 rate=2722.43 Hz, eta=0:00:00, total=0:00:00, wall=07:49 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.022 DataTime=4.618 Loss=0.999 Prec@1=74.023 Prec@5=91.797 rate=2722.43 Hz, eta=0:00:00, total=0:00:00, wall=07:51 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.656 DataTime=0.429 Loss=1.046 Prec@1=72.977 Prec@5=91.199 rate=2722.43 Hz, eta=0:00:00, total=0:00:00, wall=07:51 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.656 DataTime=0.429 Loss=1.046 Prec@1=72.977 Prec@5=91.199 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=07:51 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.656 DataTime=0.429 Loss=1.046 Prec@1=72.977 Prec@5=91.199 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=07:52 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.635 DataTime=0.409 Loss=1.057 Prec@1=72.808 Prec@5=91.051 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=07:52 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.635 DataTime=0.409 Loss=1.057 Prec@1=72.808 Prec@5=91.051 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=07:52 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.635 DataTime=0.409 Loss=1.057 Prec@1=72.808 Prec@5=91.051 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=07:53 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.628 DataTime=0.403 Loss=1.062 Prec@1=72.730 Prec@5=91.017 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=07:53 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.628 DataTime=0.403 Loss=1.062 Prec@1=72.730 Prec@5=91.017 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:53 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.628 DataTime=0.403 Loss=1.062 Prec@1=72.730 Prec@5=91.017 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:54 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.624 DataTime=0.400 Loss=1.064 Prec@1=72.753 Prec@5=90.988 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=07:54 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.624 DataTime=0.400 Loss=1.064 Prec@1=72.753 Prec@5=90.988 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:54 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.624 DataTime=0.400 Loss=1.064 Prec@1=72.753 Prec@5=90.988 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:55 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.623 DataTime=0.398 Loss=1.064 Prec@1=72.713 Prec@5=90.963 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=07:55 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.623 DataTime=0.398 Loss=1.064 Prec@1=72.713 Prec@5=90.963 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=07:55 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.623 DataTime=0.398 Loss=1.064 Prec@1=72.713 Prec@5=90.963 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=07:56 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.397 Loss=1.065 Prec@1=72.697 Prec@5=90.972 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=07:56 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.397 Loss=1.065 Prec@1=72.697 Prec@5=90.972 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:56 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.397 Loss=1.065 Prec@1=72.697 Prec@5=90.972 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:57 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.396 Loss=1.065 Prec@1=72.669 Prec@5=90.955 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=07:57 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.396 Loss=1.065 Prec@1=72.669 Prec@5=90.955 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:57 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.621 DataTime=0.396 Loss=1.065 Prec@1=72.669 Prec@5=90.955 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:58 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.620 DataTime=0.395 Loss=1.067 Prec@1=72.590 Prec@5=90.935 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=07:58 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.620 DataTime=0.395 Loss=1.067 Prec@1=72.590 Prec@5=90.935 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:58 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.620 DataTime=0.395 Loss=1.067 Prec@1=72.590 Prec@5=90.935 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:59 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.619 DataTime=0.394 Loss=1.069 Prec@1=72.563 Prec@5=90.922 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=07:59 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.619 DataTime=0.394 Loss=1.069 Prec@1=72.563 Prec@5=90.922 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:59 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.619 DataTime=0.394 Loss=1.069 Prec@1=72.563 Prec@5=90.922 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=08:00 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.071 Prec@1=72.522 Prec@5=90.894 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=08:00 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.071 Prec@1=72.522 Prec@5=90.894 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=08:00 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.071 Prec@1=72.522 Prec@5=90.894 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=08:01 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.072 Prec@1=72.516 Prec@5=90.877 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=08:01 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.072 Prec@1=72.516 Prec@5=90.877 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:01 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.394 Loss=1.072 Prec@1=72.516 Prec@5=90.877 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:02 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.073 Prec@1=72.489 Prec@5=90.860 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:02 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.073 Prec@1=72.489 Prec@5=90.860 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=08:02 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.073 Prec@1=72.489 Prec@5=90.860 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=08:03 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.075 Prec@1=72.448 Prec@5=90.836 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=08:03 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.075 Prec@1=72.448 Prec@5=90.836 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:03 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.618 DataTime=0.393 Loss=1.075 Prec@1=72.448 Prec@5=90.836 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:04 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.076 Prec@1=72.419 Prec@5=90.837 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:04 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.076 Prec@1=72.419 Prec@5=90.837 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=08:04 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.076 Prec@1=72.419 Prec@5=90.837 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=08:05 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.078 Prec@1=72.376 Prec@5=90.817 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=08:05 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.078 Prec@1=72.376 Prec@5=90.817 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=08:05 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.078 Prec@1=72.376 Prec@5=90.817 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=08:06 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.079 Prec@1=72.343 Prec@5=90.802 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=08:06 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.079 Prec@1=72.343 Prec@5=90.802 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:06 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.079 Prec@1=72.343 Prec@5=90.802 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:07 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.080 Prec@1=72.331 Prec@5=90.797 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:07 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.080 Prec@1=72.331 Prec@5=90.797 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=08:07 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.393 Loss=1.080 Prec@1=72.331 Prec@5=90.797 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=08:08 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.081 Prec@1=72.313 Prec@5=90.782 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=08:08 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.081 Prec@1=72.313 Prec@5=90.782 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=08:08 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.081 Prec@1=72.313 Prec@5=90.782 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=08:09 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.083 Prec@1=72.275 Prec@5=90.758 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=08:09 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.083 Prec@1=72.275 Prec@5=90.758 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=08:09 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.617 DataTime=0.392 Loss=1.083 Prec@1=72.275 Prec@5=90.758 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=08:10 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.084 Prec@1=72.241 Prec@5=90.740 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=08:10 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.084 Prec@1=72.241 Prec@5=90.740 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=08:10 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.084 Prec@1=72.241 Prec@5=90.740 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=08:11 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.211 Prec@5=90.732 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=08:11 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.211 Prec@5=90.732 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=08:11 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.211 Prec@5=90.732 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=08:12 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.205 Prec@5=90.735 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=08:12 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.205 Prec@5=90.735 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=08:12 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.085 Prec@1=72.205 Prec@5=90.735 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=08:13 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.086 Prec@1=72.186 Prec@5=90.728 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=08:13 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.086 Prec@1=72.186 Prec@5=90.728 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=08:13 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.086 Prec@1=72.186 Prec@5=90.728 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=08:14 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.087 Prec@1=72.165 Prec@5=90.712 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=08:14 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.087 Prec@1=72.165 Prec@5=90.712 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=08:14 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.087 Prec@1=72.165 Prec@5=90.712 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=08:15 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.088 Prec@1=72.154 Prec@5=90.706 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.088 Prec@1=72.154 Prec@5=90.706 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.088 Prec@1=72.154 Prec@5=90.706 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.088 Prec@1=72.152 Prec@5=90.706 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=08:15 IST=> training   100.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.616 DataTime=0.392 Loss=1.088 Prec@1=72.152 Prec@5=90.706 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=08:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 0.00% of 1x98...Epoch=95/150 LR=0.03062 Time=6.927 Loss=1.179 Prec@1=72.070 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.927 Loss=1.179 Prec@1=72.070 Prec@5=92.188 rate=6584.54 Hz, eta=0:00:00, total=0:00:00, wall=08:15 IST** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.927 Loss=1.179 Prec@1=72.070 Prec@5=92.188 rate=6584.54 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=0.409 Loss=1.234 Prec@1=69.144 Prec@5=89.326 rate=6584.54 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST** validation 100.00% of 1x98...Epoch=95/150 LR=0.03062 Time=0.409 Loss=1.234 Prec@1=69.144 Prec@5=89.326 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=08:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.043 DataTime=4.755 Loss=1.063 Prec@1=71.094 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.043 DataTime=4.755 Loss=1.063 Prec@1=71.094 Prec@5=91.602 rate=6771.07 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.043 DataTime=4.755 Loss=1.063 Prec@1=71.094 Prec@5=91.602 rate=6771.07 Hz, eta=0:00:00, total=0:00:00, wall=08:17 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.654 DataTime=0.430 Loss=1.033 Prec@1=73.441 Prec@5=91.414 rate=6771.07 Hz, eta=0:00:00, total=0:00:00, wall=08:17 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.654 DataTime=0.430 Loss=1.033 Prec@1=73.441 Prec@5=91.414 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=08:17 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.654 DataTime=0.430 Loss=1.033 Prec@1=73.441 Prec@5=91.414 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=08:18 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.633 DataTime=0.409 Loss=1.034 Prec@1=73.511 Prec@5=91.375 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=08:18 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.633 DataTime=0.409 Loss=1.034 Prec@1=73.511 Prec@5=91.375 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=08:18 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.633 DataTime=0.409 Loss=1.034 Prec@1=73.511 Prec@5=91.375 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=08:19 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.626 DataTime=0.403 Loss=1.040 Prec@1=73.329 Prec@5=91.360 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=08:19 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.626 DataTime=0.403 Loss=1.040 Prec@1=73.329 Prec@5=91.360 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=08:19 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.626 DataTime=0.403 Loss=1.040 Prec@1=73.329 Prec@5=91.360 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=08:20 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.623 DataTime=0.399 Loss=1.038 Prec@1=73.331 Prec@5=91.357 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=08:20 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.623 DataTime=0.399 Loss=1.038 Prec@1=73.331 Prec@5=91.357 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=08:20 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.623 DataTime=0.399 Loss=1.038 Prec@1=73.331 Prec@5=91.357 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=08:21 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.621 DataTime=0.397 Loss=1.044 Prec@1=73.172 Prec@5=91.286 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=08:21 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.621 DataTime=0.397 Loss=1.044 Prec@1=73.172 Prec@5=91.286 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=08:21 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.621 DataTime=0.397 Loss=1.044 Prec@1=73.172 Prec@5=91.286 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=08:22 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.620 DataTime=0.396 Loss=1.050 Prec@1=73.048 Prec@5=91.190 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=08:22 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.620 DataTime=0.396 Loss=1.050 Prec@1=73.048 Prec@5=91.190 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=08:22 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.620 DataTime=0.396 Loss=1.050 Prec@1=73.048 Prec@5=91.190 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=08:23 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.051 Prec@1=73.035 Prec@5=91.177 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=08:23 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.051 Prec@1=73.035 Prec@5=91.177 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:23 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.051 Prec@1=73.035 Prec@5=91.177 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:24 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.962 Prec@5=91.128 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:24 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.962 Prec@5=91.128 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:24 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.962 Prec@5=91.128 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:25 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.057 Prec@1=72.886 Prec@5=91.080 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:25 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.057 Prec@1=72.886 Prec@5=91.080 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:25 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.057 Prec@1=72.886 Prec@5=91.080 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:26 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.058 Prec@1=72.849 Prec@5=91.089 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:26 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.058 Prec@1=72.849 Prec@5=91.089 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:26 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.394 Loss=1.058 Prec@1=72.849 Prec@5=91.089 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:27 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.393 Loss=1.059 Prec@1=72.799 Prec@5=91.079 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:27 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.393 Loss=1.059 Prec@1=72.799 Prec@5=91.079 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:27 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.618 DataTime=0.393 Loss=1.059 Prec@1=72.799 Prec@5=91.079 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:28 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.061 Prec@1=72.746 Prec@5=91.069 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=08:28 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.061 Prec@1=72.746 Prec@5=91.069 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:28 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.061 Prec@1=72.746 Prec@5=91.069 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:29 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.063 Prec@1=72.702 Prec@5=91.032 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:29 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.063 Prec@1=72.702 Prec@5=91.032 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=08:29 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.063 Prec@1=72.702 Prec@5=91.032 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=08:30 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.065 Prec@1=72.669 Prec@5=91.010 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=08:30 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.065 Prec@1=72.669 Prec@5=91.010 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:30 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.065 Prec@1=72.669 Prec@5=91.010 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:31 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.066 Prec@1=72.657 Prec@5=90.987 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:31 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.066 Prec@1=72.657 Prec@5=90.987 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:31 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.393 Loss=1.066 Prec@1=72.657 Prec@5=90.987 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:32 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.067 Prec@1=72.628 Prec@5=90.975 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:32 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.067 Prec@1=72.628 Prec@5=90.975 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:32 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.067 Prec@1=72.628 Prec@5=90.975 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:33 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.069 Prec@1=72.585 Prec@5=90.955 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=08:33 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.069 Prec@1=72.585 Prec@5=90.955 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=08:33 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.617 DataTime=0.392 Loss=1.069 Prec@1=72.585 Prec@5=90.955 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=08:34 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.070 Prec@1=72.564 Prec@5=90.945 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=08:34 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.070 Prec@1=72.564 Prec@5=90.945 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=08:34 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.070 Prec@1=72.564 Prec@5=90.945 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=08:35 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.072 Prec@1=72.522 Prec@5=90.925 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=08:35 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.072 Prec@1=72.522 Prec@5=90.925 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:35 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.072 Prec@1=72.522 Prec@5=90.925 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:36 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.073 Prec@1=72.491 Prec@5=90.913 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=08:36 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.073 Prec@1=72.491 Prec@5=90.913 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=08:36 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.073 Prec@1=72.491 Prec@5=90.913 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=08:37 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.074 Prec@1=72.468 Prec@5=90.901 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=08:37 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.074 Prec@1=72.468 Prec@5=90.901 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=08:37 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.074 Prec@1=72.468 Prec@5=90.901 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=08:38 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.075 Prec@1=72.445 Prec@5=90.883 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=08:38 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.075 Prec@1=72.445 Prec@5=90.883 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:38 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.075 Prec@1=72.445 Prec@5=90.883 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:39 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.076 Prec@1=72.419 Prec@5=90.862 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=08:39 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.076 Prec@1=72.419 Prec@5=90.862 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:39 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.076 Prec@1=72.419 Prec@5=90.862 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:40 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.077 Prec@1=72.390 Prec@5=90.849 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:40 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.077 Prec@1=72.390 Prec@5=90.849 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:40 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.392 Loss=1.077 Prec@1=72.390 Prec@5=90.849 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:42 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.391 Loss=1.078 Prec@1=72.371 Prec@5=90.841 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=08:42 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.391 Loss=1.078 Prec@1=72.371 Prec@5=90.841 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:42 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.391 Loss=1.078 Prec@1=72.371 Prec@5=90.841 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:42 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.391 Loss=1.078 Prec@1=72.371 Prec@5=90.840 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:42 IST=> training   100.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.616 DataTime=0.391 Loss=1.078 Prec@1=72.371 Prec@5=90.840 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=08:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> validation 0.00% of 1x98...Epoch=96/150 LR=0.02966 Time=6.665 Loss=1.278 Prec@1=67.773 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=6.665 Loss=1.278 Prec@1=67.773 Prec@5=89.062 rate=5194.99 Hz, eta=0:00:00, total=0:00:00, wall=08:42 IST** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=6.665 Loss=1.278 Prec@1=67.773 Prec@5=89.062 rate=5194.99 Hz, eta=0:00:00, total=0:00:00, wall=08:42 IST** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=0.398 Loss=1.280 Prec@1=68.402 Prec@5=88.794 rate=5194.99 Hz, eta=0:00:00, total=0:00:00, wall=08:42 IST** validation 100.00% of 1x98...Epoch=96/150 LR=0.02966 Time=0.398 Loss=1.280 Prec@1=68.402 Prec@5=88.794 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=08:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> training   0.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.646 DataTime=5.366 Loss=0.972 Prec@1=74.414 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=08:42 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.646 DataTime=5.366 Loss=0.972 Prec@1=74.414 Prec@5=91.992 rate=7345.65 Hz, eta=0:00:00, total=0:00:00, wall=08:42 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.646 DataTime=5.366 Loss=0.972 Prec@1=74.414 Prec@5=91.992 rate=7345.65 Hz, eta=0:00:00, total=0:00:00, wall=08:43 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.662 DataTime=0.436 Loss=1.037 Prec@1=73.277 Prec@5=91.319 rate=7345.65 Hz, eta=0:00:00, total=0:00:00, wall=08:43 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.662 DataTime=0.436 Loss=1.037 Prec@1=73.277 Prec@5=91.319 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=08:43 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.662 DataTime=0.436 Loss=1.037 Prec@1=73.277 Prec@5=91.319 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=08:44 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.638 DataTime=0.413 Loss=1.039 Prec@1=73.243 Prec@5=91.238 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=08:44 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.638 DataTime=0.413 Loss=1.039 Prec@1=73.243 Prec@5=91.238 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=08:44 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.638 DataTime=0.413 Loss=1.039 Prec@1=73.243 Prec@5=91.238 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=08:45 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.630 DataTime=0.405 Loss=1.043 Prec@1=73.190 Prec@5=91.204 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=08:45 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.630 DataTime=0.405 Loss=1.043 Prec@1=73.190 Prec@5=91.204 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=08:45 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.630 DataTime=0.405 Loss=1.043 Prec@1=73.190 Prec@5=91.204 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=08:46 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.625 DataTime=0.401 Loss=1.044 Prec@1=73.168 Prec@5=91.234 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=08:46 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.625 DataTime=0.401 Loss=1.044 Prec@1=73.168 Prec@5=91.234 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=08:46 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.625 DataTime=0.401 Loss=1.044 Prec@1=73.168 Prec@5=91.234 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=08:47 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.623 DataTime=0.399 Loss=1.046 Prec@1=73.109 Prec@5=91.202 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=08:47 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.623 DataTime=0.399 Loss=1.046 Prec@1=73.109 Prec@5=91.202 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=08:47 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.623 DataTime=0.399 Loss=1.046 Prec@1=73.109 Prec@5=91.202 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=08:48 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.622 DataTime=0.398 Loss=1.047 Prec@1=73.087 Prec@5=91.192 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=08:48 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.622 DataTime=0.398 Loss=1.047 Prec@1=73.087 Prec@5=91.192 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:48 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.622 DataTime=0.398 Loss=1.047 Prec@1=73.087 Prec@5=91.192 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:49 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.621 DataTime=0.396 Loss=1.049 Prec@1=73.062 Prec@5=91.161 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=08:49 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.621 DataTime=0.396 Loss=1.049 Prec@1=73.062 Prec@5=91.161 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:49 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.621 DataTime=0.396 Loss=1.049 Prec@1=73.062 Prec@5=91.161 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:50 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.620 DataTime=0.396 Loss=1.051 Prec@1=73.027 Prec@5=91.121 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=08:50 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.620 DataTime=0.396 Loss=1.051 Prec@1=73.027 Prec@5=91.121 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:50 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.620 DataTime=0.396 Loss=1.051 Prec@1=73.027 Prec@5=91.121 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:52 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.967 Prec@5=91.115 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:52 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.967 Prec@5=91.115 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:52 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.395 Loss=1.054 Prec@1=72.967 Prec@5=91.115 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:53 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.054 Prec@1=72.955 Prec@5=91.104 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=08:53 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.054 Prec@1=72.955 Prec@5=91.104 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:53 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.054 Prec@1=72.955 Prec@5=91.104 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:54 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.056 Prec@1=72.908 Prec@5=91.086 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:54 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.056 Prec@1=72.908 Prec@5=91.086 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:54 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.619 DataTime=0.394 Loss=1.056 Prec@1=72.908 Prec@5=91.086 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:55 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.394 Loss=1.056 Prec@1=72.905 Prec@5=91.080 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=08:55 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.394 Loss=1.056 Prec@1=72.905 Prec@5=91.080 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:55 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.394 Loss=1.056 Prec@1=72.905 Prec@5=91.080 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:56 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.393 Loss=1.057 Prec@1=72.874 Prec@5=91.071 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=08:56 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.393 Loss=1.057 Prec@1=72.874 Prec@5=91.071 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:56 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.618 DataTime=0.393 Loss=1.057 Prec@1=72.874 Prec@5=91.071 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:57 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.059 Prec@1=72.856 Prec@5=91.046 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=08:57 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.059 Prec@1=72.856 Prec@5=91.046 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:57 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.059 Prec@1=72.856 Prec@5=91.046 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:58 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.060 Prec@1=72.846 Prec@5=91.027 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=08:58 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.060 Prec@1=72.846 Prec@5=91.027 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:58 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.393 Loss=1.060 Prec@1=72.846 Prec@5=91.027 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:59 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.061 Prec@1=72.805 Prec@5=91.008 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=08:59 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.061 Prec@1=72.805 Prec@5=91.008 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=08:59 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.061 Prec@1=72.805 Prec@5=91.008 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:00 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.062 Prec@1=72.765 Prec@5=90.998 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=09:00 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.062 Prec@1=72.765 Prec@5=90.998 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=09:00 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.062 Prec@1=72.765 Prec@5=90.998 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=09:01 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.063 Prec@1=72.731 Prec@5=90.987 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=09:01 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.063 Prec@1=72.731 Prec@5=90.987 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:01 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.617 DataTime=0.392 Loss=1.063 Prec@1=72.731 Prec@5=90.987 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:02 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.064 Prec@1=72.710 Prec@5=90.976 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:02 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.064 Prec@1=72.710 Prec@5=90.976 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:02 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.064 Prec@1=72.710 Prec@5=90.976 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:03 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.677 Prec@5=90.960 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:03 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.677 Prec@5=90.960 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:03 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.677 Prec@5=90.960 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:04 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.664 Prec@5=90.950 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:04 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.664 Prec@5=90.950 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:04 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.066 Prec@1=72.664 Prec@5=90.950 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:05 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.067 Prec@1=72.640 Prec@5=90.942 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:05 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.067 Prec@1=72.640 Prec@5=90.942 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:05 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.392 Loss=1.067 Prec@1=72.640 Prec@5=90.942 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:06 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.068 Prec@1=72.620 Prec@5=90.935 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:06 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.068 Prec@1=72.620 Prec@5=90.935 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:06 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.068 Prec@1=72.620 Prec@5=90.935 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:07 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.069 Prec@1=72.604 Prec@5=90.916 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:07 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.069 Prec@1=72.604 Prec@5=90.916 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:07 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.069 Prec@1=72.604 Prec@5=90.916 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:08 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.070 Prec@1=72.577 Prec@5=90.896 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:08 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.070 Prec@1=72.577 Prec@5=90.896 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:08 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.070 Prec@1=72.577 Prec@5=90.896 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:08 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.070 Prec@1=72.578 Prec@5=90.897 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:08 IST=> training   100.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.616 DataTime=0.391 Loss=1.070 Prec@1=72.578 Prec@5=90.897 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=09:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST=> validation 0.00% of 1x98...Epoch=97/150 LR=0.02871 Time=6.503 Loss=1.237 Prec@1=67.969 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST=> validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=6.503 Loss=1.237 Prec@1=67.969 Prec@5=89.062 rate=6288.20 Hz, eta=0:00:00, total=0:00:00, wall=09:08 IST** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=6.503 Loss=1.237 Prec@1=67.969 Prec@5=89.062 rate=6288.20 Hz, eta=0:00:00, total=0:00:00, wall=09:09 IST** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=0.405 Loss=1.235 Prec@1=69.410 Prec@5=89.534 rate=6288.20 Hz, eta=0:00:00, total=0:00:00, wall=09:09 IST** validation 100.00% of 1x98...Epoch=97/150 LR=0.02871 Time=0.405 Loss=1.235 Prec@1=69.410 Prec@5=89.534 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=09:09 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST=> training   0.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=6.050 DataTime=5.760 Loss=1.110 Prec@1=73.242 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=6.050 DataTime=5.760 Loss=1.110 Prec@1=73.242 Prec@5=89.453 rate=7518.51 Hz, eta=0:00:00, total=0:00:00, wall=09:09 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=6.050 DataTime=5.760 Loss=1.110 Prec@1=73.242 Prec@5=89.453 rate=7518.51 Hz, eta=0:00:00, total=0:00:00, wall=09:10 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.666 DataTime=0.440 Loss=1.037 Prec@1=73.080 Prec@5=91.364 rate=7518.51 Hz, eta=0:00:00, total=0:00:00, wall=09:10 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.666 DataTime=0.440 Loss=1.037 Prec@1=73.080 Prec@5=91.364 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=09:10 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.666 DataTime=0.440 Loss=1.037 Prec@1=73.080 Prec@5=91.364 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=09:11 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.640 DataTime=0.416 Loss=1.032 Prec@1=73.304 Prec@5=91.397 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=09:11 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.640 DataTime=0.416 Loss=1.032 Prec@1=73.304 Prec@5=91.397 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=09:11 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.640 DataTime=0.416 Loss=1.032 Prec@1=73.304 Prec@5=91.397 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=09:12 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.632 DataTime=0.407 Loss=1.035 Prec@1=73.282 Prec@5=91.415 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=09:12 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.632 DataTime=0.407 Loss=1.035 Prec@1=73.282 Prec@5=91.415 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=09:12 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.632 DataTime=0.407 Loss=1.035 Prec@1=73.282 Prec@5=91.415 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=09:13 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.627 DataTime=0.403 Loss=1.037 Prec@1=73.248 Prec@5=91.376 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=09:13 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.627 DataTime=0.403 Loss=1.037 Prec@1=73.248 Prec@5=91.376 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=09:13 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.627 DataTime=0.403 Loss=1.037 Prec@1=73.248 Prec@5=91.376 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=09:14 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.625 DataTime=0.400 Loss=1.038 Prec@1=73.231 Prec@5=91.341 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=09:14 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.625 DataTime=0.400 Loss=1.038 Prec@1=73.231 Prec@5=91.341 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=09:14 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.625 DataTime=0.400 Loss=1.038 Prec@1=73.231 Prec@5=91.341 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=09:15 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.623 DataTime=0.398 Loss=1.038 Prec@1=73.268 Prec@5=91.309 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=09:15 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.623 DataTime=0.398 Loss=1.038 Prec@1=73.268 Prec@5=91.309 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:15 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.623 DataTime=0.398 Loss=1.038 Prec@1=73.268 Prec@5=91.309 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:16 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.622 DataTime=0.397 Loss=1.040 Prec@1=73.239 Prec@5=91.294 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=09:16 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.622 DataTime=0.397 Loss=1.040 Prec@1=73.239 Prec@5=91.294 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=09:16 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.622 DataTime=0.397 Loss=1.040 Prec@1=73.239 Prec@5=91.294 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=09:17 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.621 DataTime=0.396 Loss=1.040 Prec@1=73.214 Prec@5=91.293 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=09:17 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.621 DataTime=0.396 Loss=1.040 Prec@1=73.214 Prec@5=91.293 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:17 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.621 DataTime=0.396 Loss=1.040 Prec@1=73.214 Prec@5=91.293 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:18 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.620 DataTime=0.395 Loss=1.043 Prec@1=73.160 Prec@5=91.255 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=09:18 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.620 DataTime=0.395 Loss=1.043 Prec@1=73.160 Prec@5=91.255 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:18 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.620 DataTime=0.395 Loss=1.043 Prec@1=73.160 Prec@5=91.255 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:19 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.395 Loss=1.045 Prec@1=73.104 Prec@5=91.235 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=09:19 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.395 Loss=1.045 Prec@1=73.104 Prec@5=91.235 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:19 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.395 Loss=1.045 Prec@1=73.104 Prec@5=91.235 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:20 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.394 Loss=1.046 Prec@1=73.069 Prec@5=91.210 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=09:20 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.394 Loss=1.046 Prec@1=73.069 Prec@5=91.210 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=09:20 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.619 DataTime=0.394 Loss=1.046 Prec@1=73.069 Prec@5=91.210 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=09:21 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.048 Prec@1=73.029 Prec@5=91.192 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=09:21 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.048 Prec@1=73.029 Prec@5=91.192 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:21 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.048 Prec@1=73.029 Prec@5=91.192 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:22 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.049 Prec@1=73.002 Prec@5=91.187 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:22 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.049 Prec@1=73.002 Prec@5=91.187 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:22 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.394 Loss=1.049 Prec@1=73.002 Prec@5=91.187 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:23 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.393 Loss=1.050 Prec@1=72.998 Prec@5=91.179 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:23 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.393 Loss=1.050 Prec@1=72.998 Prec@5=91.179 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=09:23 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.618 DataTime=0.393 Loss=1.050 Prec@1=72.998 Prec@5=91.179 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=09:24 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.051 Prec@1=72.973 Prec@5=91.159 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=09:24 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.051 Prec@1=72.973 Prec@5=91.159 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:24 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.051 Prec@1=72.973 Prec@5=91.159 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:25 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.052 Prec@1=72.956 Prec@5=91.143 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:25 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.052 Prec@1=72.956 Prec@5=91.143 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:25 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.052 Prec@1=72.956 Prec@5=91.143 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:26 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.053 Prec@1=72.953 Prec@5=91.141 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:26 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.053 Prec@1=72.953 Prec@5=91.141 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:26 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.393 Loss=1.053 Prec@1=72.953 Prec@5=91.141 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:27 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.054 Prec@1=72.924 Prec@5=91.125 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:27 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.054 Prec@1=72.924 Prec@5=91.125 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=09:27 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.054 Prec@1=72.924 Prec@5=91.125 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=09:28 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.055 Prec@1=72.898 Prec@5=91.107 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=09:28 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.055 Prec@1=72.898 Prec@5=91.107 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:28 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.055 Prec@1=72.898 Prec@5=91.107 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:29 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.056 Prec@1=72.884 Prec@5=91.100 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:29 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.056 Prec@1=72.884 Prec@5=91.100 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=09:29 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.617 DataTime=0.392 Loss=1.056 Prec@1=72.884 Prec@5=91.100 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=09:30 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.057 Prec@1=72.863 Prec@5=91.093 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=09:30 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.057 Prec@1=72.863 Prec@5=91.093 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:30 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.057 Prec@1=72.863 Prec@5=91.093 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:31 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.836 Prec@5=91.084 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=09:31 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.836 Prec@5=91.084 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:31 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.836 Prec@5=91.084 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:32 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.814 Prec@5=91.082 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:32 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.814 Prec@5=91.082 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:32 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.058 Prec@1=72.814 Prec@5=91.082 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:33 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.060 Prec@1=72.790 Prec@5=91.068 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:33 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.060 Prec@1=72.790 Prec@5=91.068 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:33 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.060 Prec@1=72.790 Prec@5=91.068 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:34 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.061 Prec@1=72.775 Prec@5=91.052 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=09:34 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.061 Prec@1=72.775 Prec@5=91.052 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:34 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.061 Prec@1=72.775 Prec@5=91.052 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:34 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.061 Prec@1=72.773 Prec@5=91.053 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=09:34 IST=> training   100.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.616 DataTime=0.392 Loss=1.061 Prec@1=72.773 Prec@5=91.053 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=09:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST=> validation 0.00% of 1x98...Epoch=98/150 LR=0.02777 Time=7.650 Loss=1.248 Prec@1=67.383 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST=> validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=7.650 Loss=1.248 Prec@1=67.383 Prec@5=89.648 rate=3829.32 Hz, eta=0:00:00, total=0:00:00, wall=09:34 IST** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=7.650 Loss=1.248 Prec@1=67.383 Prec@5=89.648 rate=3829.32 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=0.404 Loss=1.251 Prec@1=68.838 Prec@5=89.282 rate=3829.32 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST** validation 100.00% of 1x98...Epoch=98/150 LR=0.02777 Time=0.404 Loss=1.251 Prec@1=68.838 Prec@5=89.282 rate=3.07 Hz, eta=0:00:00, total=0:00:31, wall=09:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> training   0.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.595 DataTime=4.302 Loss=0.941 Prec@1=73.828 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.595 DataTime=4.302 Loss=0.941 Prec@1=73.828 Prec@5=92.578 rate=4015.82 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=4.595 DataTime=4.302 Loss=0.941 Prec@1=73.828 Prec@5=92.578 rate=4015.82 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.657 DataTime=0.429 Loss=1.025 Prec@1=73.745 Prec@5=91.418 rate=4015.82 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.657 DataTime=0.429 Loss=1.025 Prec@1=73.745 Prec@5=91.418 rate=1.63 Hz, eta=0:24:30, total=0:01:01, wall=09:36 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.657 DataTime=0.429 Loss=1.025 Prec@1=73.745 Prec@5=91.418 rate=1.63 Hz, eta=0:24:30, total=0:01:01, wall=09:37 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.636 DataTime=0.410 Loss=1.029 Prec@1=73.594 Prec@5=91.423 rate=1.63 Hz, eta=0:24:30, total=0:01:01, wall=09:37 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.636 DataTime=0.410 Loss=1.029 Prec@1=73.594 Prec@5=91.423 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=09:37 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.636 DataTime=0.410 Loss=1.029 Prec@1=73.594 Prec@5=91.423 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=09:38 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.628 DataTime=0.402 Loss=1.030 Prec@1=73.562 Prec@5=91.407 rate=1.63 Hz, eta=0:23:31, total=0:02:03, wall=09:38 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.628 DataTime=0.402 Loss=1.030 Prec@1=73.562 Prec@5=91.407 rate=1.63 Hz, eta=0:22:30, total=0:03:04, wall=09:38 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.628 DataTime=0.402 Loss=1.030 Prec@1=73.562 Prec@5=91.407 rate=1.63 Hz, eta=0:22:30, total=0:03:04, wall=09:39 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.624 DataTime=0.399 Loss=1.032 Prec@1=73.472 Prec@5=91.375 rate=1.63 Hz, eta=0:22:30, total=0:03:04, wall=09:39 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.624 DataTime=0.399 Loss=1.032 Prec@1=73.472 Prec@5=91.375 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=09:39 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.624 DataTime=0.399 Loss=1.032 Prec@1=73.472 Prec@5=91.375 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=09:40 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.622 DataTime=0.397 Loss=1.034 Prec@1=73.404 Prec@5=91.346 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=09:40 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.622 DataTime=0.397 Loss=1.034 Prec@1=73.404 Prec@5=91.346 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=09:40 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.622 DataTime=0.397 Loss=1.034 Prec@1=73.404 Prec@5=91.346 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=09:41 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.621 DataTime=0.396 Loss=1.036 Prec@1=73.389 Prec@5=91.336 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=09:41 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.621 DataTime=0.396 Loss=1.036 Prec@1=73.389 Prec@5=91.336 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=09:41 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.621 DataTime=0.396 Loss=1.036 Prec@1=73.389 Prec@5=91.336 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=09:42 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.620 DataTime=0.395 Loss=1.038 Prec@1=73.366 Prec@5=91.298 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=09:42 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.620 DataTime=0.395 Loss=1.038 Prec@1=73.366 Prec@5=91.298 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=09:42 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.620 DataTime=0.395 Loss=1.038 Prec@1=73.366 Prec@5=91.298 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=09:43 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.037 Prec@1=73.356 Prec@5=91.313 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=09:43 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.037 Prec@1=73.356 Prec@5=91.313 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=09:43 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.037 Prec@1=73.356 Prec@5=91.313 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=09:44 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.039 Prec@1=73.318 Prec@5=91.323 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=09:44 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.039 Prec@1=73.318 Prec@5=91.323 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=09:44 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.619 DataTime=0.394 Loss=1.039 Prec@1=73.318 Prec@5=91.323 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=09:45 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.394 Loss=1.039 Prec@1=73.301 Prec@5=91.322 rate=1.63 Hz, eta=0:16:23, total=0:09:12, wall=09:45 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.394 Loss=1.039 Prec@1=73.301 Prec@5=91.322 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=09:45 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.394 Loss=1.039 Prec@1=73.301 Prec@5=91.322 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=09:46 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.393 Loss=1.040 Prec@1=73.280 Prec@5=91.316 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=09:46 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.393 Loss=1.040 Prec@1=73.280 Prec@5=91.316 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=09:46 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.618 DataTime=0.393 Loss=1.040 Prec@1=73.280 Prec@5=91.316 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=09:47 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.040 Prec@1=73.264 Prec@5=91.306 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=09:47 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.040 Prec@1=73.264 Prec@5=91.306 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:47 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.040 Prec@1=73.264 Prec@5=91.306 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:48 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.042 Prec@1=73.218 Prec@5=91.277 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=09:48 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.042 Prec@1=73.218 Prec@5=91.277 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:48 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.393 Loss=1.042 Prec@1=73.218 Prec@5=91.277 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:49 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.043 Prec@1=73.189 Prec@5=91.254 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=09:49 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.043 Prec@1=73.189 Prec@5=91.254 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=09:49 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.043 Prec@1=73.189 Prec@5=91.254 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=09:50 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.044 Prec@1=73.164 Prec@5=91.257 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=09:50 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.044 Prec@1=73.164 Prec@5=91.257 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:50 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.617 DataTime=0.392 Loss=1.044 Prec@1=73.164 Prec@5=91.257 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:51 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.045 Prec@1=73.140 Prec@5=91.236 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=09:51 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.045 Prec@1=73.140 Prec@5=91.236 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:51 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.045 Prec@1=73.140 Prec@5=91.236 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:53 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.046 Prec@1=73.111 Prec@5=91.226 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=09:53 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.046 Prec@1=73.111 Prec@5=91.226 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:53 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.046 Prec@1=73.111 Prec@5=91.226 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:54 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.047 Prec@1=73.097 Prec@5=91.214 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=09:54 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.047 Prec@1=73.097 Prec@5=91.214 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:54 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.392 Loss=1.047 Prec@1=73.097 Prec@5=91.214 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:55 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.048 Prec@1=73.085 Prec@5=91.203 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=09:55 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.048 Prec@1=73.085 Prec@5=91.203 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:55 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.048 Prec@1=73.085 Prec@5=91.203 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:56 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.049 Prec@1=73.069 Prec@5=91.192 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=09:56 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.049 Prec@1=73.069 Prec@5=91.192 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:56 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.049 Prec@1=73.069 Prec@5=91.192 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:57 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.049 Prec@1=73.052 Prec@5=91.186 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=09:57 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.049 Prec@1=73.052 Prec@5=91.186 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:57 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.049 Prec@1=73.052 Prec@5=91.186 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:58 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.050 Prec@1=73.018 Prec@5=91.173 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=09:58 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.050 Prec@1=73.018 Prec@5=91.173 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:58 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.616 DataTime=0.391 Loss=1.050 Prec@1=73.018 Prec@5=91.173 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:59 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.051 Prec@1=72.990 Prec@5=91.172 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=09:59 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.051 Prec@1=72.990 Prec@5=91.172 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=09:59 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.051 Prec@1=72.990 Prec@5=91.172 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:00 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.052 Prec@1=72.967 Prec@5=91.161 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:00 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.052 Prec@1=72.967 Prec@5=91.161 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:00 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.052 Prec@1=72.967 Prec@5=91.161 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:01 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.053 Prec@1=72.953 Prec@5=91.155 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:01 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.053 Prec@1=72.953 Prec@5=91.155 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:01 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.053 Prec@1=72.953 Prec@5=91.155 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:01 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.053 Prec@1=72.952 Prec@5=91.155 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=10:01 IST=> training   100.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.615 DataTime=0.391 Loss=1.053 Prec@1=72.952 Prec@5=91.155 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=10:01 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST=> validation 0.00% of 1x98...Epoch=99/150 LR=0.02684 Time=5.703 Loss=1.327 Prec@1=69.922 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=10:01 IST=> validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=5.703 Loss=1.327 Prec@1=69.922 Prec@5=86.914 rate=3044.15 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=5.703 Loss=1.327 Prec@1=69.922 Prec@5=86.914 rate=3044.15 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=0.405 Loss=1.230 Prec@1=69.132 Prec@5=89.548 rate=3044.15 Hz, eta=0:00:00, total=0:00:00, wall=10:01 IST** validation 100.00% of 1x98...Epoch=99/150 LR=0.02684 Time=0.405 Loss=1.230 Prec@1=69.132 Prec@5=89.548 rate=2.88 Hz, eta=0:00:00, total=0:00:34, wall=10:01 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST=> training   0.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.635 DataTime=5.316 Loss=0.863 Prec@1=76.562 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.635 DataTime=5.316 Loss=0.863 Prec@1=76.562 Prec@5=92.773 rate=5113.91 Hz, eta=0:00:00, total=0:00:00, wall=10:02 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.635 DataTime=5.316 Loss=0.863 Prec@1=76.562 Prec@5=92.773 rate=5113.91 Hz, eta=0:00:00, total=0:00:00, wall=10:03 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.663 DataTime=0.436 Loss=1.016 Prec@1=73.695 Prec@5=91.727 rate=5113.91 Hz, eta=0:00:00, total=0:00:00, wall=10:03 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.663 DataTime=0.436 Loss=1.016 Prec@1=73.695 Prec@5=91.727 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=10:03 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.663 DataTime=0.436 Loss=1.016 Prec@1=73.695 Prec@5=91.727 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=10:04 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.639 DataTime=0.413 Loss=1.018 Prec@1=73.762 Prec@5=91.623 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=10:04 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.639 DataTime=0.413 Loss=1.018 Prec@1=73.762 Prec@5=91.623 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=10:04 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.639 DataTime=0.413 Loss=1.018 Prec@1=73.762 Prec@5=91.623 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=10:05 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.630 DataTime=0.405 Loss=1.021 Prec@1=73.741 Prec@5=91.544 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=10:05 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.630 DataTime=0.405 Loss=1.021 Prec@1=73.741 Prec@5=91.544 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:05 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.630 DataTime=0.405 Loss=1.021 Prec@1=73.741 Prec@5=91.544 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:06 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.626 DataTime=0.401 Loss=1.023 Prec@1=73.632 Prec@5=91.523 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:06 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.626 DataTime=0.401 Loss=1.023 Prec@1=73.632 Prec@5=91.523 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=10:06 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.626 DataTime=0.401 Loss=1.023 Prec@1=73.632 Prec@5=91.523 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=10:07 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.623 DataTime=0.398 Loss=1.026 Prec@1=73.574 Prec@5=91.485 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=10:07 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.623 DataTime=0.398 Loss=1.026 Prec@1=73.574 Prec@5=91.485 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=10:07 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.623 DataTime=0.398 Loss=1.026 Prec@1=73.574 Prec@5=91.485 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=10:08 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.622 DataTime=0.397 Loss=1.024 Prec@1=73.618 Prec@5=91.498 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=10:08 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.622 DataTime=0.397 Loss=1.024 Prec@1=73.618 Prec@5=91.498 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=10:08 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.622 DataTime=0.397 Loss=1.024 Prec@1=73.618 Prec@5=91.498 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=10:09 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.620 DataTime=0.396 Loss=1.027 Prec@1=73.569 Prec@5=91.466 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=10:09 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.620 DataTime=0.396 Loss=1.027 Prec@1=73.569 Prec@5=91.466 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:09 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.620 DataTime=0.396 Loss=1.027 Prec@1=73.569 Prec@5=91.466 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:10 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.395 Loss=1.030 Prec@1=73.517 Prec@5=91.426 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:10 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.395 Loss=1.030 Prec@1=73.517 Prec@5=91.426 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:10 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.395 Loss=1.030 Prec@1=73.517 Prec@5=91.426 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:11 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.394 Loss=1.030 Prec@1=73.502 Prec@5=91.413 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:11 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.394 Loss=1.030 Prec@1=73.502 Prec@5=91.413 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:11 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.619 DataTime=0.394 Loss=1.030 Prec@1=73.502 Prec@5=91.413 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:12 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.394 Loss=1.032 Prec@1=73.439 Prec@5=91.397 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=10:12 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.394 Loss=1.032 Prec@1=73.439 Prec@5=91.397 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:12 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.394 Loss=1.032 Prec@1=73.439 Prec@5=91.397 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:13 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.032 Prec@1=73.418 Prec@5=91.406 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:13 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.032 Prec@1=73.418 Prec@5=91.406 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:13 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.032 Prec@1=73.418 Prec@5=91.406 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:14 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.033 Prec@1=73.399 Prec@5=91.401 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=10:14 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.033 Prec@1=73.399 Prec@5=91.401 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:14 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.618 DataTime=0.393 Loss=1.033 Prec@1=73.399 Prec@5=91.401 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:15 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.034 Prec@1=73.378 Prec@5=91.383 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:15 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.034 Prec@1=73.378 Prec@5=91.383 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:15 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.034 Prec@1=73.378 Prec@5=91.383 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:16 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.035 Prec@1=73.351 Prec@5=91.377 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=10:16 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.035 Prec@1=73.351 Prec@5=91.377 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:16 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.393 Loss=1.035 Prec@1=73.351 Prec@5=91.377 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:17 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.392 Loss=1.036 Prec@1=73.334 Prec@5=91.365 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=10:17 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.392 Loss=1.036 Prec@1=73.334 Prec@5=91.365 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=10:17 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.617 DataTime=0.392 Loss=1.036 Prec@1=73.334 Prec@5=91.365 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=10:18 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.320 Prec@5=91.364 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=10:18 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.320 Prec@5=91.364 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:18 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.320 Prec@5=91.364 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:19 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.327 Prec@5=91.353 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=10:19 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.327 Prec@5=91.353 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:19 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.036 Prec@1=73.327 Prec@5=91.353 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:20 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.298 Prec@5=91.334 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=10:20 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.298 Prec@5=91.334 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:20 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.298 Prec@5=91.334 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:21 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.285 Prec@5=91.326 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=10:21 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.285 Prec@5=91.326 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:21 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.285 Prec@5=91.326 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:22 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.290 Prec@5=91.325 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=10:22 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.290 Prec@5=91.325 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:22 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.038 Prec@1=73.290 Prec@5=91.325 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:23 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.275 Prec@5=91.319 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=10:23 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.275 Prec@5=91.319 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:23 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.039 Prec@1=73.275 Prec@5=91.319 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:24 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.040 Prec@1=73.248 Prec@5=91.312 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=10:24 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.040 Prec@1=73.248 Prec@5=91.312 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:24 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.392 Loss=1.040 Prec@1=73.248 Prec@5=91.312 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:25 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.041 Prec@1=73.240 Prec@5=91.306 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=10:25 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.041 Prec@1=73.240 Prec@5=91.306 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:25 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.041 Prec@1=73.240 Prec@5=91.306 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:26 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.042 Prec@1=73.200 Prec@5=91.294 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=10:26 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.042 Prec@1=73.200 Prec@5=91.294 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:26 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.042 Prec@1=73.200 Prec@5=91.294 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:27 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.043 Prec@1=73.181 Prec@5=91.285 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=10:27 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.043 Prec@1=73.181 Prec@5=91.285 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:27 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.043 Prec@1=73.181 Prec@5=91.285 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:27 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.043 Prec@1=73.180 Prec@5=91.284 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=10:27 IST=> training   100.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.616 DataTime=0.391 Loss=1.043 Prec@1=73.180 Prec@5=91.284 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=10:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST=> validation 0.00% of 1x98...Epoch=100/150 LR=0.02591 Time=6.745 Loss=1.315 Prec@1=67.383 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST=> validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=6.745 Loss=1.315 Prec@1=67.383 Prec@5=87.695 rate=3260.78 Hz, eta=0:00:00, total=0:00:00, wall=10:27 IST** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=6.745 Loss=1.315 Prec@1=67.383 Prec@5=87.695 rate=3260.78 Hz, eta=0:00:00, total=0:00:00, wall=10:28 IST** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=0.409 Loss=1.241 Prec@1=69.294 Prec@5=89.234 rate=3260.78 Hz, eta=0:00:00, total=0:00:00, wall=10:28 IST** validation 100.00% of 1x98...Epoch=100/150 LR=0.02591 Time=0.409 Loss=1.241 Prec@1=69.294 Prec@5=89.234 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=10:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> training   0.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.795 DataTime=5.492 Loss=0.932 Prec@1=75.977 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.795 DataTime=5.492 Loss=0.932 Prec@1=75.977 Prec@5=93.359 rate=7807.68 Hz, eta=0:00:00, total=0:00:00, wall=10:28 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.795 DataTime=5.492 Loss=0.932 Prec@1=75.977 Prec@5=93.359 rate=7807.68 Hz, eta=0:00:00, total=0:00:00, wall=10:29 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.661 DataTime=0.436 Loss=1.022 Prec@1=73.617 Prec@5=91.418 rate=7807.68 Hz, eta=0:00:00, total=0:00:00, wall=10:29 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.661 DataTime=0.436 Loss=1.022 Prec@1=73.617 Prec@5=91.418 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=10:29 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.661 DataTime=0.436 Loss=1.022 Prec@1=73.617 Prec@5=91.418 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=10:30 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.637 DataTime=0.412 Loss=1.011 Prec@1=73.974 Prec@5=91.608 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=10:30 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.637 DataTime=0.412 Loss=1.011 Prec@1=73.974 Prec@5=91.608 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=10:30 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.637 DataTime=0.412 Loss=1.011 Prec@1=73.974 Prec@5=91.608 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=10:31 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.629 DataTime=0.404 Loss=1.007 Prec@1=74.034 Prec@5=91.672 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=10:31 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.629 DataTime=0.404 Loss=1.007 Prec@1=74.034 Prec@5=91.672 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=10:31 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.629 DataTime=0.404 Loss=1.007 Prec@1=74.034 Prec@5=91.672 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=10:32 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.626 DataTime=0.401 Loss=1.004 Prec@1=74.072 Prec@5=91.706 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=10:32 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.626 DataTime=0.401 Loss=1.004 Prec@1=74.072 Prec@5=91.706 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=10:32 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.626 DataTime=0.401 Loss=1.004 Prec@1=74.072 Prec@5=91.706 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=10:33 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.623 DataTime=0.398 Loss=1.006 Prec@1=74.032 Prec@5=91.677 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=10:33 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.623 DataTime=0.398 Loss=1.006 Prec@1=74.032 Prec@5=91.677 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:33 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.623 DataTime=0.398 Loss=1.006 Prec@1=74.032 Prec@5=91.677 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:34 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.622 DataTime=0.397 Loss=1.008 Prec@1=73.968 Prec@5=91.650 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:34 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.622 DataTime=0.397 Loss=1.008 Prec@1=73.968 Prec@5=91.650 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:34 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.622 DataTime=0.397 Loss=1.008 Prec@1=73.968 Prec@5=91.650 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:35 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.621 DataTime=0.396 Loss=1.010 Prec@1=73.915 Prec@5=91.626 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=10:35 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.621 DataTime=0.396 Loss=1.010 Prec@1=73.915 Prec@5=91.626 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:35 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.621 DataTime=0.396 Loss=1.010 Prec@1=73.915 Prec@5=91.626 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:36 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.013 Prec@1=73.862 Prec@5=91.608 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=10:36 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.013 Prec@1=73.862 Prec@5=91.608 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:36 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.013 Prec@1=73.862 Prec@5=91.608 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:37 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.014 Prec@1=73.846 Prec@5=91.599 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=10:37 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.014 Prec@1=73.846 Prec@5=91.599 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=10:37 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.620 DataTime=0.395 Loss=1.014 Prec@1=73.846 Prec@5=91.599 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=10:38 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.619 DataTime=0.394 Loss=1.016 Prec@1=73.798 Prec@5=91.588 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=10:38 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.619 DataTime=0.394 Loss=1.016 Prec@1=73.798 Prec@5=91.588 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:38 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.619 DataTime=0.394 Loss=1.016 Prec@1=73.798 Prec@5=91.588 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:39 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.394 Loss=1.018 Prec@1=73.765 Prec@5=91.562 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=10:39 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.394 Loss=1.018 Prec@1=73.765 Prec@5=91.562 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=10:39 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.394 Loss=1.018 Prec@1=73.765 Prec@5=91.562 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=10:40 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.018 Prec@1=73.754 Prec@5=91.564 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=10:40 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.018 Prec@1=73.754 Prec@5=91.564 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:40 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.018 Prec@1=73.754 Prec@5=91.564 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:41 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.020 Prec@1=73.702 Prec@5=91.538 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=10:41 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.020 Prec@1=73.702 Prec@5=91.538 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=10:41 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.020 Prec@1=73.702 Prec@5=91.538 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=10:42 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.022 Prec@1=73.649 Prec@5=91.521 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=10:42 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.022 Prec@1=73.649 Prec@5=91.521 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=10:42 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.618 DataTime=0.393 Loss=1.022 Prec@1=73.649 Prec@5=91.521 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=10:43 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.393 Loss=1.023 Prec@1=73.625 Prec@5=91.501 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=10:43 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.393 Loss=1.023 Prec@1=73.625 Prec@5=91.501 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:43 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.393 Loss=1.023 Prec@1=73.625 Prec@5=91.501 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:44 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.024 Prec@1=73.608 Prec@5=91.491 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=10:44 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.024 Prec@1=73.608 Prec@5=91.491 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:44 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.024 Prec@1=73.608 Prec@5=91.491 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:45 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.566 Prec@5=91.477 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=10:45 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.566 Prec@5=91.477 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:45 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.566 Prec@5=91.477 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:46 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.556 Prec@5=91.471 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=10:46 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.556 Prec@5=91.471 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:46 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.026 Prec@1=73.556 Prec@5=91.471 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:47 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.027 Prec@1=73.546 Prec@5=91.465 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=10:47 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.027 Prec@1=73.546 Prec@5=91.465 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:47 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.617 DataTime=0.392 Loss=1.027 Prec@1=73.546 Prec@5=91.465 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:48 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.028 Prec@1=73.522 Prec@5=91.455 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=10:48 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.028 Prec@1=73.522 Prec@5=91.455 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=10:48 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.028 Prec@1=73.522 Prec@5=91.455 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=10:49 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.029 Prec@1=73.508 Prec@5=91.440 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=10:49 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.029 Prec@1=73.508 Prec@5=91.440 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=10:49 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.029 Prec@1=73.508 Prec@5=91.440 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=10:50 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.030 Prec@1=73.465 Prec@5=91.421 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=10:50 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.030 Prec@1=73.465 Prec@5=91.421 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:50 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.030 Prec@1=73.465 Prec@5=91.421 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:51 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.031 Prec@1=73.449 Prec@5=91.414 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=10:51 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.031 Prec@1=73.449 Prec@5=91.414 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=10:51 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.031 Prec@1=73.449 Prec@5=91.414 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=10:52 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.032 Prec@1=73.419 Prec@5=91.395 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=10:52 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.032 Prec@1=73.419 Prec@5=91.395 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=10:52 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.032 Prec@1=73.419 Prec@5=91.395 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=10:53 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.033 Prec@1=73.403 Prec@5=91.382 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=10:53 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.033 Prec@1=73.403 Prec@5=91.382 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=10:53 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.392 Loss=1.033 Prec@1=73.403 Prec@5=91.382 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=10:54 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.391 Loss=1.033 Prec@1=73.402 Prec@5=91.382 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=10:54 IST=> training   100.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.616 DataTime=0.391 Loss=1.033 Prec@1=73.402 Prec@5=91.382 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=10:54 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> validation 0.00% of 1x98...Epoch=101/150 LR=0.02500 Time=6.321 Loss=1.426 Prec@1=64.453 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=6.321 Loss=1.426 Prec@1=64.453 Prec@5=87.500 rate=2802.82 Hz, eta=0:00:00, total=0:00:00, wall=10:54 IST** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=6.321 Loss=1.426 Prec@1=64.453 Prec@5=87.500 rate=2802.82 Hz, eta=0:00:00, total=0:00:00, wall=10:54 IST** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=0.404 Loss=1.244 Prec@1=69.174 Prec@5=89.160 rate=2802.82 Hz, eta=0:00:00, total=0:00:00, wall=10:54 IST** validation 100.00% of 1x98...Epoch=101/150 LR=0.02500 Time=0.404 Loss=1.244 Prec@1=69.174 Prec@5=89.160 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=10:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> training   0.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.841 DataTime=4.535 Loss=0.986 Prec@1=73.047 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=10:54 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.841 DataTime=4.535 Loss=0.986 Prec@1=73.047 Prec@5=92.188 rate=6303.54 Hz, eta=0:00:00, total=0:00:00, wall=10:54 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=4.841 DataTime=4.535 Loss=0.986 Prec@1=73.047 Prec@5=92.188 rate=6303.54 Hz, eta=0:00:00, total=0:00:00, wall=10:55 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.652 DataTime=0.428 Loss=0.988 Prec@1=74.285 Prec@5=91.921 rate=6303.54 Hz, eta=0:00:00, total=0:00:00, wall=10:55 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.652 DataTime=0.428 Loss=0.988 Prec@1=74.285 Prec@5=91.921 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=10:55 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.652 DataTime=0.428 Loss=0.988 Prec@1=74.285 Prec@5=91.921 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=10:56 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.634 DataTime=0.409 Loss=0.992 Prec@1=74.234 Prec@5=91.844 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=10:56 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.634 DataTime=0.409 Loss=0.992 Prec@1=74.234 Prec@5=91.844 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:56 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.634 DataTime=0.409 Loss=0.992 Prec@1=74.234 Prec@5=91.844 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:57 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.627 DataTime=0.402 Loss=1.000 Prec@1=74.095 Prec@5=91.715 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=10:57 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.627 DataTime=0.402 Loss=1.000 Prec@1=74.095 Prec@5=91.715 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:57 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.627 DataTime=0.402 Loss=1.000 Prec@1=74.095 Prec@5=91.715 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:58 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.624 DataTime=0.400 Loss=1.002 Prec@1=74.110 Prec@5=91.719 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=10:58 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.624 DataTime=0.400 Loss=1.002 Prec@1=74.110 Prec@5=91.719 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:58 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.624 DataTime=0.400 Loss=1.002 Prec@1=74.110 Prec@5=91.719 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:59 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.621 DataTime=0.398 Loss=1.004 Prec@1=74.085 Prec@5=91.715 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=10:59 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.621 DataTime=0.398 Loss=1.004 Prec@1=74.085 Prec@5=91.715 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=10:59 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.621 DataTime=0.398 Loss=1.004 Prec@1=74.085 Prec@5=91.715 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=11:00 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.620 DataTime=0.397 Loss=1.004 Prec@1=74.098 Prec@5=91.717 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=11:00 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.620 DataTime=0.397 Loss=1.004 Prec@1=74.098 Prec@5=91.717 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:00 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.620 DataTime=0.397 Loss=1.004 Prec@1=74.098 Prec@5=91.717 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:01 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.396 Loss=1.006 Prec@1=74.072 Prec@5=91.689 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:01 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.396 Loss=1.006 Prec@1=74.072 Prec@5=91.689 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:01 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.396 Loss=1.006 Prec@1=74.072 Prec@5=91.689 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:02 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.395 Loss=1.006 Prec@1=74.018 Prec@5=91.681 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:02 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.395 Loss=1.006 Prec@1=74.018 Prec@5=91.681 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:02 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.619 DataTime=0.395 Loss=1.006 Prec@1=74.018 Prec@5=91.681 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:04 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.395 Loss=1.008 Prec@1=74.010 Prec@5=91.654 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:04 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.395 Loss=1.008 Prec@1=74.010 Prec@5=91.654 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=11:04 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.395 Loss=1.008 Prec@1=74.010 Prec@5=91.654 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=11:05 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.394 Loss=1.009 Prec@1=73.987 Prec@5=91.651 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=11:05 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.394 Loss=1.009 Prec@1=73.987 Prec@5=91.651 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:05 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.618 DataTime=0.394 Loss=1.009 Prec@1=73.987 Prec@5=91.651 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:06 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.010 Prec@1=73.947 Prec@5=91.631 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:06 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.010 Prec@1=73.947 Prec@5=91.631 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:06 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.010 Prec@1=73.947 Prec@5=91.631 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:07 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.011 Prec@1=73.931 Prec@5=91.610 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:07 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.011 Prec@1=73.931 Prec@5=91.610 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:07 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.394 Loss=1.011 Prec@1=73.931 Prec@5=91.610 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:08 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.908 Prec@5=91.593 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:08 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.908 Prec@5=91.593 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=11:08 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.908 Prec@5=91.593 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=11:09 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.014 Prec@1=73.878 Prec@5=91.584 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=11:09 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.014 Prec@1=73.878 Prec@5=91.584 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=11:09 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.014 Prec@1=73.878 Prec@5=91.584 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=11:10 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.015 Prec@1=73.847 Prec@5=91.566 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=11:10 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.015 Prec@1=73.847 Prec@5=91.566 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:10 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.015 Prec@1=73.847 Prec@5=91.566 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:11 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.015 Prec@1=73.836 Prec@5=91.557 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:11 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.015 Prec@1=73.836 Prec@5=91.557 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:11 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.015 Prec@1=73.836 Prec@5=91.557 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:12 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.016 Prec@1=73.809 Prec@5=91.548 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:12 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.016 Prec@1=73.809 Prec@5=91.548 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:12 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.617 DataTime=0.393 Loss=1.016 Prec@1=73.809 Prec@5=91.548 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:13 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.017 Prec@1=73.779 Prec@5=91.537 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:13 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.017 Prec@1=73.779 Prec@5=91.537 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:13 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.393 Loss=1.017 Prec@1=73.779 Prec@5=91.537 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:14 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.019 Prec@1=73.740 Prec@5=91.523 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:14 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.019 Prec@1=73.740 Prec@5=91.523 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:14 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.019 Prec@1=73.740 Prec@5=91.523 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:15 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.020 Prec@1=73.715 Prec@5=91.503 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:15 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.020 Prec@1=73.715 Prec@5=91.503 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=11:15 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.020 Prec@1=73.715 Prec@5=91.503 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=11:16 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.021 Prec@1=73.684 Prec@5=91.489 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=11:16 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.021 Prec@1=73.684 Prec@5=91.489 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:16 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.021 Prec@1=73.684 Prec@5=91.489 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:17 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.022 Prec@1=73.661 Prec@5=91.472 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:17 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.022 Prec@1=73.661 Prec@5=91.472 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=11:17 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.022 Prec@1=73.661 Prec@5=91.472 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=11:18 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.023 Prec@1=73.654 Prec@5=91.466 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=11:18 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.023 Prec@1=73.654 Prec@5=91.466 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=11:18 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.023 Prec@1=73.654 Prec@5=91.466 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=11:19 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.024 Prec@1=73.636 Prec@5=91.456 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=11:19 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.024 Prec@1=73.636 Prec@5=91.456 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:19 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.024 Prec@1=73.636 Prec@5=91.456 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:20 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.025 Prec@1=73.612 Prec@5=91.441 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:20 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.025 Prec@1=73.612 Prec@5=91.441 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:20 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.025 Prec@1=73.612 Prec@5=91.441 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:20 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.025 Prec@1=73.611 Prec@5=91.441 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:20 IST=> training   100.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.616 DataTime=0.392 Loss=1.025 Prec@1=73.611 Prec@5=91.441 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=11:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST=> validation 0.00% of 1x98...Epoch=102/150 LR=0.02410 Time=6.434 Loss=1.148 Prec@1=70.703 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=11:20 IST=> validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=6.434 Loss=1.148 Prec@1=70.703 Prec@5=90.430 rate=3340.83 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=6.434 Loss=1.148 Prec@1=70.703 Prec@5=90.430 rate=3340.83 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=0.411 Loss=1.229 Prec@1=69.626 Prec@5=89.504 rate=3340.83 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST** validation 100.00% of 1x98...Epoch=102/150 LR=0.02410 Time=0.411 Loss=1.229 Prec@1=69.626 Prec@5=89.504 rate=2.90 Hz, eta=0:00:00, total=0:00:33, wall=11:21 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> training   0.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=5.764 DataTime=5.473 Loss=1.147 Prec@1=69.922 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=11:21 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=5.764 DataTime=5.473 Loss=1.147 Prec@1=69.922 Prec@5=90.430 rate=9894.14 Hz, eta=0:00:00, total=0:00:00, wall=11:21 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=5.764 DataTime=5.473 Loss=1.147 Prec@1=69.922 Prec@5=90.430 rate=9894.14 Hz, eta=0:00:00, total=0:00:00, wall=11:22 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.659 DataTime=0.437 Loss=0.991 Prec@1=74.631 Prec@5=91.803 rate=9894.14 Hz, eta=0:00:00, total=0:00:00, wall=11:22 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.659 DataTime=0.437 Loss=0.991 Prec@1=74.631 Prec@5=91.803 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:22 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.659 DataTime=0.437 Loss=0.991 Prec@1=74.631 Prec@5=91.803 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:23 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.638 DataTime=0.414 Loss=0.991 Prec@1=74.505 Prec@5=91.828 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=11:23 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.638 DataTime=0.414 Loss=0.991 Prec@1=74.505 Prec@5=91.828 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=11:23 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.638 DataTime=0.414 Loss=0.991 Prec@1=74.505 Prec@5=91.828 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=11:24 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.630 DataTime=0.406 Loss=0.990 Prec@1=74.498 Prec@5=91.860 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=11:24 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.630 DataTime=0.406 Loss=0.990 Prec@1=74.498 Prec@5=91.860 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=11:24 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.630 DataTime=0.406 Loss=0.990 Prec@1=74.498 Prec@5=91.860 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=11:25 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.626 DataTime=0.402 Loss=0.993 Prec@1=74.397 Prec@5=91.839 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=11:25 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.626 DataTime=0.402 Loss=0.993 Prec@1=74.397 Prec@5=91.839 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=11:25 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.626 DataTime=0.402 Loss=0.993 Prec@1=74.397 Prec@5=91.839 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=11:26 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.624 DataTime=0.400 Loss=0.995 Prec@1=74.351 Prec@5=91.834 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=11:26 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.624 DataTime=0.400 Loss=0.995 Prec@1=74.351 Prec@5=91.834 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=11:26 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.624 DataTime=0.400 Loss=0.995 Prec@1=74.351 Prec@5=91.834 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=11:27 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.622 DataTime=0.398 Loss=0.995 Prec@1=74.329 Prec@5=91.827 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=11:27 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.622 DataTime=0.398 Loss=0.995 Prec@1=74.329 Prec@5=91.827 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=11:27 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.622 DataTime=0.398 Loss=0.995 Prec@1=74.329 Prec@5=91.827 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=11:28 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.621 DataTime=0.397 Loss=0.997 Prec@1=74.262 Prec@5=91.797 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=11:28 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.621 DataTime=0.397 Loss=0.997 Prec@1=74.262 Prec@5=91.797 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=11:28 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.621 DataTime=0.397 Loss=0.997 Prec@1=74.262 Prec@5=91.797 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=11:29 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=0.998 Prec@1=74.265 Prec@5=91.786 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=11:29 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=0.998 Prec@1=74.265 Prec@5=91.786 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=11:29 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=0.998 Prec@1=74.265 Prec@5=91.786 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=11:30 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=1.000 Prec@1=74.229 Prec@5=91.767 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=11:30 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=1.000 Prec@1=74.229 Prec@5=91.767 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:30 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.620 DataTime=0.396 Loss=1.000 Prec@1=74.229 Prec@5=91.767 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:31 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.001 Prec@1=74.175 Prec@5=91.744 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:31 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.001 Prec@1=74.175 Prec@5=91.744 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:31 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.001 Prec@1=74.175 Prec@5=91.744 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:32 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.003 Prec@1=74.145 Prec@5=91.729 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:32 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.003 Prec@1=74.145 Prec@5=91.729 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=11:32 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.619 DataTime=0.395 Loss=1.003 Prec@1=74.145 Prec@5=91.729 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=11:33 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.004 Prec@1=74.118 Prec@5=91.721 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=11:33 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.004 Prec@1=74.118 Prec@5=91.721 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:33 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.004 Prec@1=74.118 Prec@5=91.721 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:34 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.005 Prec@1=74.079 Prec@5=91.707 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:34 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.005 Prec@1=74.079 Prec@5=91.707 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=11:34 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.005 Prec@1=74.079 Prec@5=91.707 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=11:35 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.006 Prec@1=74.057 Prec@5=91.678 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=11:35 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.006 Prec@1=74.057 Prec@5=91.678 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=11:35 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.618 DataTime=0.394 Loss=1.006 Prec@1=74.057 Prec@5=91.678 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=11:36 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.008 Prec@1=74.021 Prec@5=91.663 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=11:36 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.008 Prec@1=74.021 Prec@5=91.663 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:36 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.008 Prec@1=74.021 Prec@5=91.663 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:37 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.977 Prec@5=91.642 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=11:37 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.977 Prec@5=91.642 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:37 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.977 Prec@5=91.642 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:38 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.971 Prec@5=91.633 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=11:38 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.971 Prec@5=91.633 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:38 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.010 Prec@1=73.971 Prec@5=91.633 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:39 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.955 Prec@5=91.623 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=11:39 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.955 Prec@5=91.623 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:39 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.955 Prec@5=91.623 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:40 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.930 Prec@5=91.621 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=11:40 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.930 Prec@5=91.621 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:40 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.011 Prec@1=73.930 Prec@5=91.621 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:41 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.916 Prec@5=91.610 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=11:41 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.916 Prec@5=91.610 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=11:41 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.012 Prec@1=73.916 Prec@5=91.610 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=11:42 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.013 Prec@1=73.899 Prec@5=91.596 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=11:42 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.013 Prec@1=73.899 Prec@5=91.596 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:42 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.617 DataTime=0.393 Loss=1.013 Prec@1=73.899 Prec@5=91.596 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:43 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.393 Loss=1.014 Prec@1=73.885 Prec@5=91.585 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=11:43 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.393 Loss=1.014 Prec@1=73.885 Prec@5=91.585 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=11:43 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.393 Loss=1.014 Prec@1=73.885 Prec@5=91.585 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=11:44 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.847 Prec@5=91.574 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=11:44 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.847 Prec@5=91.574 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=11:44 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.847 Prec@5=91.574 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=11:45 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.843 Prec@5=91.572 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=11:45 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.843 Prec@5=91.572 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:45 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.015 Prec@1=73.843 Prec@5=91.572 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:46 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.016 Prec@1=73.830 Prec@5=91.563 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=11:46 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.016 Prec@1=73.830 Prec@5=91.563 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:46 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.016 Prec@1=73.830 Prec@5=91.563 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:46 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.016 Prec@1=73.829 Prec@5=91.562 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=11:46 IST=> training   100.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.616 DataTime=0.392 Loss=1.016 Prec@1=73.829 Prec@5=91.562 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=11:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:46 IST=> validation 0.00% of 1x98...Epoch=103/150 LR=0.02321 Time=7.093 Loss=1.060 Prec@1=73.047 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=11:46 IST=> validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.093 Loss=1.060 Prec@1=73.047 Prec@5=92.188 rate=6256.29 Hz, eta=0:00:00, total=0:00:00, wall=11:46 IST** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.093 Loss=1.060 Prec@1=73.047 Prec@5=92.188 rate=6256.29 Hz, eta=0:00:00, total=0:00:00, wall=11:47 IST** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=0.397 Loss=1.211 Prec@1=70.052 Prec@5=89.798 rate=6256.29 Hz, eta=0:00:00, total=0:00:00, wall=11:47 IST** validation 100.00% of 1x98...Epoch=103/150 LR=0.02321 Time=0.397 Loss=1.211 Prec@1=70.052 Prec@5=89.798 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=11:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:47 IST=> training   0.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.437 DataTime=5.159 Loss=0.961 Prec@1=74.609 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=11:47 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.437 DataTime=5.159 Loss=0.961 Prec@1=74.609 Prec@5=93.555 rate=4623.53 Hz, eta=0:00:00, total=0:00:00, wall=11:47 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.437 DataTime=5.159 Loss=0.961 Prec@1=74.609 Prec@5=93.555 rate=4623.53 Hz, eta=0:00:00, total=0:00:00, wall=11:48 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.658 DataTime=0.433 Loss=0.976 Prec@1=74.981 Prec@5=92.060 rate=4623.53 Hz, eta=0:00:00, total=0:00:00, wall=11:48 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.658 DataTime=0.433 Loss=0.976 Prec@1=74.981 Prec@5=92.060 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=11:48 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.658 DataTime=0.433 Loss=0.976 Prec@1=74.981 Prec@5=92.060 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=11:49 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.635 DataTime=0.411 Loss=0.980 Prec@1=74.710 Prec@5=92.041 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=11:49 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.635 DataTime=0.411 Loss=0.980 Prec@1=74.710 Prec@5=92.041 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=11:49 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.635 DataTime=0.411 Loss=0.980 Prec@1=74.710 Prec@5=92.041 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=11:50 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.627 DataTime=0.403 Loss=0.979 Prec@1=74.757 Prec@5=92.062 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=11:50 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.627 DataTime=0.403 Loss=0.979 Prec@1=74.757 Prec@5=92.062 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=11:50 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.627 DataTime=0.403 Loss=0.979 Prec@1=74.757 Prec@5=92.062 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=11:51 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.624 DataTime=0.400 Loss=0.985 Prec@1=74.656 Prec@5=91.947 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=11:51 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.624 DataTime=0.400 Loss=0.985 Prec@1=74.656 Prec@5=91.947 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=11:51 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.624 DataTime=0.400 Loss=0.985 Prec@1=74.656 Prec@5=91.947 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=11:52 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.623 DataTime=0.398 Loss=0.986 Prec@1=74.576 Prec@5=91.948 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=11:52 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.623 DataTime=0.398 Loss=0.986 Prec@1=74.576 Prec@5=91.948 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=11:52 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.623 DataTime=0.398 Loss=0.986 Prec@1=74.576 Prec@5=91.948 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=11:53 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.621 DataTime=0.397 Loss=0.985 Prec@1=74.601 Prec@5=91.933 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=11:53 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.621 DataTime=0.397 Loss=0.985 Prec@1=74.601 Prec@5=91.933 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:53 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.621 DataTime=0.397 Loss=0.985 Prec@1=74.601 Prec@5=91.933 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:54 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.396 Loss=0.988 Prec@1=74.526 Prec@5=91.911 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=11:54 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.396 Loss=0.988 Prec@1=74.526 Prec@5=91.911 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:54 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.396 Loss=0.988 Prec@1=74.526 Prec@5=91.911 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:55 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.395 Loss=0.990 Prec@1=74.499 Prec@5=91.887 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=11:55 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.395 Loss=0.990 Prec@1=74.499 Prec@5=91.887 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:55 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.620 DataTime=0.395 Loss=0.990 Prec@1=74.499 Prec@5=91.887 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:56 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.619 DataTime=0.395 Loss=0.989 Prec@1=74.500 Prec@5=91.883 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=11:56 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.619 DataTime=0.395 Loss=0.989 Prec@1=74.500 Prec@5=91.883 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:56 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.619 DataTime=0.395 Loss=0.989 Prec@1=74.500 Prec@5=91.883 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:57 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.394 Loss=0.992 Prec@1=74.428 Prec@5=91.857 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=11:57 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.394 Loss=0.992 Prec@1=74.428 Prec@5=91.857 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:57 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.394 Loss=0.992 Prec@1=74.428 Prec@5=91.857 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:58 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.370 Prec@5=91.838 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=11:58 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.370 Prec@5=91.838 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:58 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.370 Prec@5=91.838 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:59 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.354 Prec@5=91.828 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=11:59 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.354 Prec@5=91.828 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=11:59 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.618 DataTime=0.393 Loss=0.994 Prec@1=74.354 Prec@5=91.828 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:00 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.994 Prec@1=74.345 Prec@5=91.834 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:00 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.994 Prec@1=74.345 Prec@5=91.834 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:00 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.994 Prec@1=74.345 Prec@5=91.834 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:01 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.995 Prec@1=74.303 Prec@5=91.818 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=12:01 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.995 Prec@1=74.303 Prec@5=91.818 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=12:01 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.995 Prec@1=74.303 Prec@5=91.818 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=12:03 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.997 Prec@1=74.259 Prec@5=91.799 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=12:03 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.997 Prec@1=74.259 Prec@5=91.799 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:03 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.617 DataTime=0.393 Loss=0.997 Prec@1=74.259 Prec@5=91.799 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:04 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=0.999 Prec@1=74.238 Prec@5=91.786 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:04 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=0.999 Prec@1=74.238 Prec@5=91.786 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=12:04 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=0.999 Prec@1=74.238 Prec@5=91.786 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=12:05 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.000 Prec@1=74.205 Prec@5=91.768 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=12:05 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.000 Prec@1=74.205 Prec@5=91.768 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=12:05 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.000 Prec@1=74.205 Prec@5=91.768 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=12:06 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.001 Prec@1=74.194 Prec@5=91.746 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=12:06 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.001 Prec@1=74.194 Prec@5=91.746 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=12:06 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.001 Prec@1=74.194 Prec@5=91.746 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=12:07 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.002 Prec@1=74.174 Prec@5=91.733 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=12:07 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.002 Prec@1=74.174 Prec@5=91.733 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=12:07 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.002 Prec@1=74.174 Prec@5=91.733 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=12:08 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.140 Prec@5=91.725 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=12:08 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.140 Prec@5=91.725 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=12:08 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.140 Prec@5=91.725 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=12:09 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.113 Prec@5=91.726 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=12:09 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.113 Prec@5=91.726 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=12:09 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.003 Prec@1=74.113 Prec@5=91.726 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=12:10 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.077 Prec@5=91.713 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=12:10 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.077 Prec@5=91.713 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=12:10 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.077 Prec@5=91.713 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=12:11 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.060 Prec@5=91.699 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=12:11 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.060 Prec@5=91.699 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:11 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.392 Loss=1.005 Prec@1=74.060 Prec@5=91.699 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:12 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.006 Prec@1=74.041 Prec@5=91.684 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=12:12 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.006 Prec@1=74.041 Prec@5=91.684 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=12:12 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.006 Prec@1=74.041 Prec@5=91.684 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=12:13 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.007 Prec@1=74.016 Prec@5=91.677 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=12:13 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.007 Prec@1=74.016 Prec@5=91.677 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:13 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.007 Prec@1=74.016 Prec@5=91.677 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:13 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.008 Prec@1=74.015 Prec@5=91.676 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=12:13 IST=> training   100.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.616 DataTime=0.391 Loss=1.008 Prec@1=74.015 Prec@5=91.676 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=12:13 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:13 IST=> validation 0.00% of 1x98...Epoch=104/150 LR=0.02233 Time=6.934 Loss=1.063 Prec@1=73.047 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=12:13 IST=> validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=6.934 Loss=1.063 Prec@1=73.047 Prec@5=90.820 rate=2497.18 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=6.934 Loss=1.063 Prec@1=73.047 Prec@5=90.820 rate=2497.18 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=0.407 Loss=1.189 Prec@1=70.334 Prec@5=89.846 rate=2497.18 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST** validation 100.00% of 1x98...Epoch=104/150 LR=0.02233 Time=0.407 Loss=1.189 Prec@1=70.334 Prec@5=89.846 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=12:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:14 IST=> training   0.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.802 DataTime=4.498 Loss=0.949 Prec@1=75.391 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=12:14 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.802 DataTime=4.498 Loss=0.949 Prec@1=75.391 Prec@5=92.578 rate=6227.51 Hz, eta=0:00:00, total=0:00:00, wall=12:14 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=4.802 DataTime=4.498 Loss=0.949 Prec@1=75.391 Prec@5=92.578 rate=6227.51 Hz, eta=0:00:00, total=0:00:00, wall=12:15 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.652 DataTime=0.425 Loss=0.980 Prec@1=74.433 Prec@5=91.967 rate=6227.51 Hz, eta=0:00:00, total=0:00:00, wall=12:15 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.652 DataTime=0.425 Loss=0.980 Prec@1=74.433 Prec@5=91.967 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:15 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.652 DataTime=0.425 Loss=0.980 Prec@1=74.433 Prec@5=91.967 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:16 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.634 DataTime=0.408 Loss=0.975 Prec@1=74.720 Prec@5=92.019 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=12:16 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.634 DataTime=0.408 Loss=0.975 Prec@1=74.720 Prec@5=92.019 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=12:16 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.634 DataTime=0.408 Loss=0.975 Prec@1=74.720 Prec@5=92.019 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=12:17 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.628 DataTime=0.402 Loss=0.971 Prec@1=74.860 Prec@5=92.073 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=12:17 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.628 DataTime=0.402 Loss=0.971 Prec@1=74.860 Prec@5=92.073 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=12:17 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.628 DataTime=0.402 Loss=0.971 Prec@1=74.860 Prec@5=92.073 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=12:18 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.624 DataTime=0.399 Loss=0.973 Prec@1=74.784 Prec@5=92.038 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=12:18 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.624 DataTime=0.399 Loss=0.973 Prec@1=74.784 Prec@5=92.038 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=12:18 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.624 DataTime=0.399 Loss=0.973 Prec@1=74.784 Prec@5=92.038 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=12:19 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.622 DataTime=0.397 Loss=0.974 Prec@1=74.786 Prec@5=92.008 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=12:19 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.622 DataTime=0.397 Loss=0.974 Prec@1=74.786 Prec@5=92.008 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=12:19 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.622 DataTime=0.397 Loss=0.974 Prec@1=74.786 Prec@5=92.008 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=12:20 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.621 DataTime=0.396 Loss=0.976 Prec@1=74.742 Prec@5=91.987 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=12:20 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.621 DataTime=0.396 Loss=0.976 Prec@1=74.742 Prec@5=91.987 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=12:20 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.621 DataTime=0.396 Loss=0.976 Prec@1=74.742 Prec@5=91.987 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=12:21 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.396 Loss=0.978 Prec@1=74.674 Prec@5=91.973 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=12:21 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.396 Loss=0.978 Prec@1=74.674 Prec@5=91.973 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=12:21 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.396 Loss=0.978 Prec@1=74.674 Prec@5=91.973 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=12:22 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.395 Loss=0.981 Prec@1=74.595 Prec@5=91.944 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=12:22 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.395 Loss=0.981 Prec@1=74.595 Prec@5=91.944 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=12:22 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.620 DataTime=0.395 Loss=0.981 Prec@1=74.595 Prec@5=91.944 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=12:23 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.981 Prec@1=74.581 Prec@5=91.946 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=12:23 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.981 Prec@1=74.581 Prec@5=91.946 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=12:23 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.981 Prec@1=74.581 Prec@5=91.946 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=12:24 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.984 Prec@1=74.538 Prec@5=91.926 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=12:24 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.984 Prec@1=74.538 Prec@5=91.926 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=12:24 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.619 DataTime=0.395 Loss=0.984 Prec@1=74.538 Prec@5=91.926 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=12:25 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.985 Prec@1=74.500 Prec@5=91.908 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=12:25 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.985 Prec@1=74.500 Prec@5=91.908 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=12:25 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.985 Prec@1=74.500 Prec@5=91.908 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=12:26 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.986 Prec@1=74.472 Prec@5=91.892 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=12:26 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.986 Prec@1=74.472 Prec@5=91.892 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=12:26 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.986 Prec@1=74.472 Prec@5=91.892 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=12:27 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.988 Prec@1=74.424 Prec@5=91.869 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=12:27 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.988 Prec@1=74.424 Prec@5=91.869 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:27 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.618 DataTime=0.394 Loss=0.988 Prec@1=74.424 Prec@5=91.869 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:28 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.437 Prec@5=91.879 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:28 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.437 Prec@5=91.879 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:28 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.437 Prec@5=91.879 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:29 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.424 Prec@5=91.879 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:29 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.424 Prec@5=91.879 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=12:29 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.988 Prec@1=74.424 Prec@5=91.879 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=12:30 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.383 Prec@5=91.858 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=12:30 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.383 Prec@5=91.858 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:30 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.383 Prec@5=91.858 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:31 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.367 Prec@5=91.846 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:31 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.367 Prec@5=91.846 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:31 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.617 DataTime=0.393 Loss=0.990 Prec@1=74.367 Prec@5=91.846 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:32 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.992 Prec@1=74.335 Prec@5=91.826 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:32 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.992 Prec@1=74.335 Prec@5=91.826 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:32 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.992 Prec@1=74.335 Prec@5=91.826 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:33 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.993 Prec@1=74.312 Prec@5=91.825 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:33 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.993 Prec@1=74.312 Prec@5=91.825 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:33 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.993 Prec@1=74.312 Prec@5=91.825 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:34 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.994 Prec@1=74.285 Prec@5=91.817 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:34 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.994 Prec@1=74.285 Prec@5=91.817 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:34 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.393 Loss=0.994 Prec@1=74.285 Prec@5=91.817 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:35 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.279 Prec@5=91.813 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=12:35 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.279 Prec@5=91.813 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:35 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.279 Prec@5=91.813 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:36 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.273 Prec@5=91.801 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=12:36 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.273 Prec@5=91.801 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:36 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.995 Prec@1=74.273 Prec@5=91.801 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:37 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.996 Prec@1=74.264 Prec@5=91.795 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=12:37 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.996 Prec@1=74.264 Prec@5=91.795 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:37 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.996 Prec@1=74.264 Prec@5=91.795 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:38 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.997 Prec@1=74.240 Prec@5=91.790 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=12:38 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.997 Prec@1=74.240 Prec@5=91.790 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:38 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.997 Prec@1=74.240 Prec@5=91.790 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:39 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.998 Prec@1=74.211 Prec@5=91.774 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=12:39 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.998 Prec@1=74.211 Prec@5=91.774 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:39 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.998 Prec@1=74.211 Prec@5=91.774 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:39 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.998 Prec@1=74.210 Prec@5=91.774 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=12:39 IST=> training   100.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.616 DataTime=0.392 Loss=0.998 Prec@1=74.210 Prec@5=91.774 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=12:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:39 IST=> validation 0.00% of 1x98...Epoch=105/150 LR=0.02146 Time=6.501 Loss=1.203 Prec@1=70.898 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=12:39 IST=> validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=6.501 Loss=1.203 Prec@1=70.898 Prec@5=88.477 rate=7364.64 Hz, eta=0:00:00, total=0:00:00, wall=12:39 IST** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=6.501 Loss=1.203 Prec@1=70.898 Prec@5=88.477 rate=7364.64 Hz, eta=0:00:00, total=0:00:00, wall=12:40 IST** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=0.408 Loss=1.201 Prec@1=70.160 Prec@5=89.900 rate=7364.64 Hz, eta=0:00:00, total=0:00:00, wall=12:40 IST** validation 100.00% of 1x98...Epoch=105/150 LR=0.02146 Time=0.408 Loss=1.201 Prec@1=70.160 Prec@5=89.900 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=12:40 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:40 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:40 IST=> training   0.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.495 DataTime=5.075 Loss=0.969 Prec@1=75.586 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=12:40 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.495 DataTime=5.075 Loss=0.969 Prec@1=75.586 Prec@5=91.992 rate=4871.54 Hz, eta=0:00:00, total=0:00:00, wall=12:40 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.495 DataTime=5.075 Loss=0.969 Prec@1=75.586 Prec@5=91.992 rate=4871.54 Hz, eta=0:00:00, total=0:00:00, wall=12:41 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.662 DataTime=0.432 Loss=0.950 Prec@1=75.244 Prec@5=92.315 rate=4871.54 Hz, eta=0:00:00, total=0:00:00, wall=12:41 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.662 DataTime=0.432 Loss=0.950 Prec@1=75.244 Prec@5=92.315 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=12:41 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.662 DataTime=0.432 Loss=0.950 Prec@1=75.244 Prec@5=92.315 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=12:42 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.638 DataTime=0.411 Loss=0.960 Prec@1=75.044 Prec@5=92.138 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=12:42 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.638 DataTime=0.411 Loss=0.960 Prec@1=75.044 Prec@5=92.138 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=12:42 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.638 DataTime=0.411 Loss=0.960 Prec@1=75.044 Prec@5=92.138 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=12:43 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.630 DataTime=0.404 Loss=0.963 Prec@1=75.033 Prec@5=92.147 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=12:43 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.630 DataTime=0.404 Loss=0.963 Prec@1=75.033 Prec@5=92.147 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=12:43 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.630 DataTime=0.404 Loss=0.963 Prec@1=75.033 Prec@5=92.147 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=12:44 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.625 DataTime=0.400 Loss=0.965 Prec@1=74.980 Prec@5=92.136 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=12:44 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.625 DataTime=0.400 Loss=0.965 Prec@1=74.980 Prec@5=92.136 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=12:44 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.625 DataTime=0.400 Loss=0.965 Prec@1=74.980 Prec@5=92.136 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=12:45 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.623 DataTime=0.398 Loss=0.964 Prec@1=75.005 Prec@5=92.139 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=12:45 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.623 DataTime=0.398 Loss=0.964 Prec@1=75.005 Prec@5=92.139 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=12:45 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.623 DataTime=0.398 Loss=0.964 Prec@1=75.005 Prec@5=92.139 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=12:46 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.621 DataTime=0.397 Loss=0.965 Prec@1=74.989 Prec@5=92.134 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=12:46 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.621 DataTime=0.397 Loss=0.965 Prec@1=74.989 Prec@5=92.134 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=12:46 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.621 DataTime=0.397 Loss=0.965 Prec@1=74.989 Prec@5=92.134 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=12:47 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.620 DataTime=0.396 Loss=0.968 Prec@1=74.940 Prec@5=92.077 rate=1.63 Hz, eta=0:19:24, total=0:06:08, wall=12:47 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.620 DataTime=0.396 Loss=0.968 Prec@1=74.940 Prec@5=92.077 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:47 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.620 DataTime=0.396 Loss=0.968 Prec@1=74.940 Prec@5=92.077 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:48 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.971 Prec@1=74.885 Prec@5=92.062 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=12:48 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.971 Prec@1=74.885 Prec@5=92.062 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:48 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.971 Prec@1=74.885 Prec@5=92.062 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:49 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.972 Prec@1=74.841 Prec@5=92.055 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=12:49 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.972 Prec@1=74.841 Prec@5=92.055 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:49 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.972 Prec@1=74.841 Prec@5=92.055 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:50 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.973 Prec@1=74.803 Prec@5=92.061 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=12:50 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.973 Prec@1=74.803 Prec@5=92.061 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:50 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.619 DataTime=0.395 Loss=0.973 Prec@1=74.803 Prec@5=92.061 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:51 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.763 Prec@5=92.024 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=12:51 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.763 Prec@5=92.024 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:51 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.763 Prec@5=92.024 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:52 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.749 Prec@5=92.025 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=12:52 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.749 Prec@5=92.025 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:52 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.394 Loss=0.975 Prec@1=74.749 Prec@5=92.025 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:53 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.393 Loss=0.977 Prec@1=74.715 Prec@5=92.020 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=12:53 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.393 Loss=0.977 Prec@1=74.715 Prec@5=92.020 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:53 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.618 DataTime=0.393 Loss=0.977 Prec@1=74.715 Prec@5=92.020 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:54 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.682 Prec@5=92.018 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=12:54 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.682 Prec@5=92.018 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:54 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.682 Prec@5=92.018 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:55 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.672 Prec@5=92.009 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=12:55 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.672 Prec@5=92.009 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:55 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.978 Prec@1=74.672 Prec@5=92.009 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:56 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.633 Prec@5=91.995 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=12:56 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.633 Prec@5=91.995 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:56 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.633 Prec@5=91.995 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:57 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.605 Prec@5=91.992 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=12:57 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.605 Prec@5=91.992 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:57 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.980 Prec@1=74.605 Prec@5=91.992 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:58 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.981 Prec@1=74.591 Prec@5=91.989 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=12:58 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.981 Prec@1=74.591 Prec@5=91.989 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:58 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.617 DataTime=0.393 Loss=0.981 Prec@1=74.591 Prec@5=91.989 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:59 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.981 Prec@1=74.579 Prec@5=91.983 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=12:59 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.981 Prec@1=74.579 Prec@5=91.983 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=12:59 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.981 Prec@1=74.579 Prec@5=91.983 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:00 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.556 Prec@5=91.966 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:00 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.556 Prec@5=91.966 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:00 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.556 Prec@5=91.966 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:01 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.544 Prec@5=91.959 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=13:01 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.544 Prec@5=91.959 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:01 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.983 Prec@1=74.544 Prec@5=91.959 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:02 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.984 Prec@1=74.520 Prec@5=91.949 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:02 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.984 Prec@1=74.520 Prec@5=91.949 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:02 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.984 Prec@1=74.520 Prec@5=91.949 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:04 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.986 Prec@1=74.488 Prec@5=91.923 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:04 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.986 Prec@1=74.488 Prec@5=91.923 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=13:04 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.986 Prec@1=74.488 Prec@5=91.923 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=13:05 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.987 Prec@1=74.478 Prec@5=91.916 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=13:05 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.987 Prec@1=74.478 Prec@5=91.916 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:05 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.987 Prec@1=74.478 Prec@5=91.916 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:06 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.988 Prec@1=74.453 Prec@5=91.899 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:06 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.988 Prec@1=74.453 Prec@5=91.899 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=13:06 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.988 Prec@1=74.453 Prec@5=91.899 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=13:06 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.988 Prec@1=74.453 Prec@5=91.899 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=13:06 IST=> training   100.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.616 DataTime=0.392 Loss=0.988 Prec@1=74.453 Prec@5=91.899 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=13:06 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> validation 0.00% of 1x98...Epoch=106/150 LR=0.02061 Time=6.502 Loss=1.197 Prec@1=70.898 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=6.502 Loss=1.197 Prec@1=70.898 Prec@5=91.016 rate=2975.33 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=6.502 Loss=1.197 Prec@1=70.898 Prec@5=91.016 rate=2975.33 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=0.407 Loss=1.216 Prec@1=69.824 Prec@5=89.714 rate=2975.33 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST** validation 100.00% of 1x98...Epoch=106/150 LR=0.02061 Time=0.407 Loss=1.216 Prec@1=69.824 Prec@5=89.714 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=13:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> training   0.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.488 DataTime=5.204 Loss=1.021 Prec@1=73.438 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=13:06 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.488 DataTime=5.204 Loss=1.021 Prec@1=73.438 Prec@5=90.820 rate=5082.57 Hz, eta=0:00:00, total=0:00:00, wall=13:06 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=5.488 DataTime=5.204 Loss=1.021 Prec@1=73.438 Prec@5=90.820 rate=5082.57 Hz, eta=0:00:00, total=0:00:00, wall=13:07 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.661 DataTime=0.435 Loss=0.946 Prec@1=75.389 Prec@5=92.321 rate=5082.57 Hz, eta=0:00:00, total=0:00:00, wall=13:07 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.661 DataTime=0.435 Loss=0.946 Prec@1=75.389 Prec@5=92.321 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=13:07 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.661 DataTime=0.435 Loss=0.946 Prec@1=75.389 Prec@5=92.321 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=13:08 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.638 DataTime=0.413 Loss=0.943 Prec@1=75.461 Prec@5=92.367 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=13:08 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.638 DataTime=0.413 Loss=0.943 Prec@1=75.461 Prec@5=92.367 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=13:08 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.638 DataTime=0.413 Loss=0.943 Prec@1=75.461 Prec@5=92.367 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=13:09 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.630 DataTime=0.406 Loss=0.947 Prec@1=75.297 Prec@5=92.378 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=13:09 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.630 DataTime=0.406 Loss=0.947 Prec@1=75.297 Prec@5=92.378 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=13:09 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.630 DataTime=0.406 Loss=0.947 Prec@1=75.297 Prec@5=92.378 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=13:10 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.626 DataTime=0.402 Loss=0.952 Prec@1=75.219 Prec@5=92.285 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=13:10 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.626 DataTime=0.402 Loss=0.952 Prec@1=75.219 Prec@5=92.285 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:10 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.626 DataTime=0.402 Loss=0.952 Prec@1=75.219 Prec@5=92.285 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:11 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.624 DataTime=0.399 Loss=0.951 Prec@1=75.271 Prec@5=92.293 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=13:11 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.624 DataTime=0.399 Loss=0.951 Prec@1=75.271 Prec@5=92.293 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=13:11 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.624 DataTime=0.399 Loss=0.951 Prec@1=75.271 Prec@5=92.293 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=13:13 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.622 DataTime=0.398 Loss=0.954 Prec@1=75.207 Prec@5=92.276 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=13:13 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.622 DataTime=0.398 Loss=0.954 Prec@1=75.207 Prec@5=92.276 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=13:13 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.622 DataTime=0.398 Loss=0.954 Prec@1=75.207 Prec@5=92.276 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=13:14 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.621 DataTime=0.397 Loss=0.956 Prec@1=75.171 Prec@5=92.242 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=13:14 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.621 DataTime=0.397 Loss=0.956 Prec@1=75.171 Prec@5=92.242 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:14 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.621 DataTime=0.397 Loss=0.956 Prec@1=75.171 Prec@5=92.242 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:15 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.620 DataTime=0.396 Loss=0.958 Prec@1=75.147 Prec@5=92.227 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=13:15 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.620 DataTime=0.396 Loss=0.958 Prec@1=75.147 Prec@5=92.227 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:15 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.620 DataTime=0.396 Loss=0.958 Prec@1=75.147 Prec@5=92.227 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:16 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.960 Prec@1=75.087 Prec@5=92.191 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=13:16 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.960 Prec@1=75.087 Prec@5=92.191 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:16 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.960 Prec@1=75.087 Prec@5=92.191 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:17 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.962 Prec@1=75.054 Prec@5=92.181 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=13:17 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.962 Prec@1=75.054 Prec@5=92.181 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:17 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.619 DataTime=0.395 Loss=0.962 Prec@1=75.054 Prec@5=92.181 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:18 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.964 Prec@1=75.001 Prec@5=92.148 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=13:18 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.964 Prec@1=75.001 Prec@5=92.148 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:18 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.964 Prec@1=75.001 Prec@5=92.148 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:19 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.965 Prec@1=74.950 Prec@5=92.140 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=13:19 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.965 Prec@1=74.950 Prec@5=92.140 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:19 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.965 Prec@1=74.950 Prec@5=92.140 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:20 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.967 Prec@1=74.912 Prec@5=92.123 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=13:20 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.967 Prec@1=74.912 Prec@5=92.123 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:20 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.618 DataTime=0.394 Loss=0.967 Prec@1=74.912 Prec@5=92.123 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:21 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.969 Prec@1=74.883 Prec@5=92.098 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=13:21 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.969 Prec@1=74.883 Prec@5=92.098 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:21 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.969 Prec@1=74.883 Prec@5=92.098 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:22 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.971 Prec@1=74.843 Prec@5=92.077 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=13:22 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.971 Prec@1=74.843 Prec@5=92.077 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=13:22 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.971 Prec@1=74.843 Prec@5=92.077 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=13:23 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.972 Prec@1=74.813 Prec@5=92.060 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=13:23 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.972 Prec@1=74.813 Prec@5=92.060 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:23 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.972 Prec@1=74.813 Prec@5=92.060 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:24 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.793 Prec@5=92.054 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=13:24 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.793 Prec@5=92.054 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=13:24 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.793 Prec@5=92.054 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=13:25 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.788 Prec@5=92.060 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=13:25 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.788 Prec@5=92.060 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=13:25 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.617 DataTime=0.393 Loss=0.973 Prec@1=74.788 Prec@5=92.060 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=13:26 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.745 Prec@5=92.051 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=13:26 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.745 Prec@5=92.051 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:26 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.745 Prec@5=92.051 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:27 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.723 Prec@5=92.050 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=13:27 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.723 Prec@5=92.050 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=13:27 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.974 Prec@1=74.723 Prec@5=92.050 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=13:28 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.975 Prec@1=74.709 Prec@5=92.039 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=13:28 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.975 Prec@1=74.709 Prec@5=92.039 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:28 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.975 Prec@1=74.709 Prec@5=92.039 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:29 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.976 Prec@1=74.688 Prec@5=92.037 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=13:29 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.976 Prec@1=74.688 Prec@5=92.037 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:29 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.976 Prec@1=74.688 Prec@5=92.037 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:30 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.659 Prec@5=92.025 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=13:30 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.659 Prec@5=92.025 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=13:30 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.659 Prec@5=92.025 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=13:31 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.642 Prec@5=92.016 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=13:31 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.642 Prec@5=92.016 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:31 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.977 Prec@1=74.642 Prec@5=92.016 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:32 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.978 Prec@1=74.632 Prec@5=92.015 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=13:32 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.978 Prec@1=74.632 Prec@5=92.015 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:32 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.978 Prec@1=74.632 Prec@5=92.015 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:32 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.978 Prec@1=74.631 Prec@5=92.015 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=13:32 IST=> training   100.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.616 DataTime=0.392 Loss=0.978 Prec@1=74.631 Prec@5=92.015 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=13:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> validation 0.00% of 1x98...Epoch=107/150 LR=0.01977 Time=5.838 Loss=1.294 Prec@1=68.164 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=5.838 Loss=1.294 Prec@1=68.164 Prec@5=88.086 rate=3209.20 Hz, eta=0:00:00, total=0:00:00, wall=13:32 IST** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=5.838 Loss=1.294 Prec@1=68.164 Prec@5=88.086 rate=3209.20 Hz, eta=0:00:00, total=0:00:00, wall=13:33 IST** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=0.410 Loss=1.186 Prec@1=70.150 Prec@5=89.856 rate=3209.20 Hz, eta=0:00:00, total=0:00:00, wall=13:33 IST** validation 100.00% of 1x98...Epoch=107/150 LR=0.01977 Time=0.410 Loss=1.186 Prec@1=70.150 Prec@5=89.856 rate=2.85 Hz, eta=0:00:00, total=0:00:34, wall=13:33 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:33 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:33 IST=> training   0.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.349 DataTime=4.972 Loss=1.029 Prec@1=72.070 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=13:33 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.349 DataTime=4.972 Loss=1.029 Prec@1=72.070 Prec@5=90.625 rate=6893.08 Hz, eta=0:00:00, total=0:00:00, wall=13:33 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=5.349 DataTime=4.972 Loss=1.029 Prec@1=72.070 Prec@5=90.625 rate=6893.08 Hz, eta=0:00:00, total=0:00:00, wall=13:34 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.656 DataTime=0.432 Loss=0.943 Prec@1=75.286 Prec@5=92.358 rate=6893.08 Hz, eta=0:00:00, total=0:00:00, wall=13:34 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.656 DataTime=0.432 Loss=0.943 Prec@1=75.286 Prec@5=92.358 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=13:34 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.656 DataTime=0.432 Loss=0.943 Prec@1=75.286 Prec@5=92.358 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=13:35 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.635 DataTime=0.410 Loss=0.940 Prec@1=75.338 Prec@5=92.362 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=13:35 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.635 DataTime=0.410 Loss=0.940 Prec@1=75.338 Prec@5=92.362 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=13:35 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.635 DataTime=0.410 Loss=0.940 Prec@1=75.338 Prec@5=92.362 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=13:36 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.627 DataTime=0.403 Loss=0.940 Prec@1=75.354 Prec@5=92.381 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=13:36 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.627 DataTime=0.403 Loss=0.940 Prec@1=75.354 Prec@5=92.381 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=13:36 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.627 DataTime=0.403 Loss=0.940 Prec@1=75.354 Prec@5=92.381 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=13:37 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.623 DataTime=0.399 Loss=0.942 Prec@1=75.381 Prec@5=92.341 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=13:37 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.623 DataTime=0.399 Loss=0.942 Prec@1=75.381 Prec@5=92.341 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=13:37 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.623 DataTime=0.399 Loss=0.942 Prec@1=75.381 Prec@5=92.341 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=13:38 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.621 DataTime=0.397 Loss=0.945 Prec@1=75.316 Prec@5=92.337 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=13:38 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.621 DataTime=0.397 Loss=0.945 Prec@1=75.316 Prec@5=92.337 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=13:38 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.621 DataTime=0.397 Loss=0.945 Prec@1=75.316 Prec@5=92.337 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=13:39 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.620 DataTime=0.396 Loss=0.948 Prec@1=75.286 Prec@5=92.298 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=13:39 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.620 DataTime=0.396 Loss=0.948 Prec@1=75.286 Prec@5=92.298 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=13:39 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.620 DataTime=0.396 Loss=0.948 Prec@1=75.286 Prec@5=92.298 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=13:40 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.619 DataTime=0.395 Loss=0.950 Prec@1=75.258 Prec@5=92.297 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=13:40 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.619 DataTime=0.395 Loss=0.950 Prec@1=75.258 Prec@5=92.297 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=13:40 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.619 DataTime=0.395 Loss=0.950 Prec@1=75.258 Prec@5=92.297 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=13:41 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.950 Prec@1=75.299 Prec@5=92.287 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=13:41 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.950 Prec@1=75.299 Prec@5=92.287 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=13:41 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.950 Prec@1=75.299 Prec@5=92.287 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=13:42 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.951 Prec@1=75.273 Prec@5=92.284 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=13:42 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.951 Prec@1=75.273 Prec@5=92.284 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=13:42 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.618 DataTime=0.394 Loss=0.951 Prec@1=75.273 Prec@5=92.284 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=13:43 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.953 Prec@1=75.237 Prec@5=92.272 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=13:43 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.953 Prec@1=75.237 Prec@5=92.272 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=13:43 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.953 Prec@1=75.237 Prec@5=92.272 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=13:44 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.955 Prec@1=75.192 Prec@5=92.254 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=13:44 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.955 Prec@1=75.192 Prec@5=92.254 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=13:44 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.393 Loss=0.955 Prec@1=75.192 Prec@5=92.254 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=13:45 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.392 Loss=0.957 Prec@1=75.160 Prec@5=92.236 rate=1.63 Hz, eta=0:14:18, total=0:11:13, wall=13:45 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.392 Loss=0.957 Prec@1=75.160 Prec@5=92.236 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=13:45 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.617 DataTime=0.392 Loss=0.957 Prec@1=75.160 Prec@5=92.236 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=13:46 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.129 Prec@5=92.228 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=13:46 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.129 Prec@5=92.228 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=13:46 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.129 Prec@5=92.228 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=13:47 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.113 Prec@5=92.224 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=13:47 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.113 Prec@5=92.224 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=13:47 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.113 Prec@5=92.224 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=13:48 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.093 Prec@5=92.217 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=13:48 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.093 Prec@5=92.217 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=13:48 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.093 Prec@5=92.217 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=13:49 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.961 Prec@1=75.082 Prec@5=92.202 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=13:49 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.961 Prec@1=75.082 Prec@5=92.202 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=13:49 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.961 Prec@1=75.082 Prec@5=92.202 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=13:50 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.079 Prec@5=92.188 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=13:50 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.079 Prec@5=92.188 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=13:50 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.079 Prec@5=92.188 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=13:51 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.962 Prec@1=75.043 Prec@5=92.181 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=13:51 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.962 Prec@1=75.043 Prec@5=92.181 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=13:51 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.392 Loss=0.962 Prec@1=75.043 Prec@5=92.181 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=13:52 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.036 Prec@5=92.184 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=13:52 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.036 Prec@5=92.184 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=13:52 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.962 Prec@1=75.036 Prec@5=92.184 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=13:53 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.963 Prec@1=75.016 Prec@5=92.172 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=13:53 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.963 Prec@1=75.016 Prec@5=92.172 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=13:53 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.616 DataTime=0.391 Loss=0.963 Prec@1=75.016 Prec@5=92.172 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=13:54 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.964 Prec@1=75.007 Prec@5=92.159 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=13:54 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.964 Prec@1=75.007 Prec@5=92.159 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=13:54 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.964 Prec@1=75.007 Prec@5=92.159 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=13:55 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.982 Prec@5=92.145 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=13:55 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.982 Prec@5=92.145 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=13:55 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.982 Prec@5=92.145 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=13:56 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.972 Prec@5=92.136 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=13:56 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.972 Prec@5=92.136 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=13:56 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.965 Prec@1=74.972 Prec@5=92.136 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=13:57 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.966 Prec@1=74.951 Prec@5=92.128 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=13:57 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.966 Prec@1=74.951 Prec@5=92.128 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=13:57 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.966 Prec@1=74.951 Prec@5=92.128 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=13:58 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.967 Prec@1=74.940 Prec@5=92.121 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=13:58 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.967 Prec@1=74.940 Prec@5=92.121 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=13:58 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.967 Prec@1=74.940 Prec@5=92.121 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=13:58 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.967 Prec@1=74.938 Prec@5=92.120 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=13:58 IST=> training   100.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.615 DataTime=0.391 Loss=0.967 Prec@1=74.938 Prec@5=92.120 rate=1.63 Hz, eta=0:00:00, total=0:25:33, wall=13:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:58 IST=> validation 0.00% of 1x98...Epoch=108/150 LR=0.01894 Time=6.725 Loss=1.070 Prec@1=70.898 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=13:58 IST=> validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.725 Loss=1.070 Prec@1=70.898 Prec@5=91.211 rate=2107.84 Hz, eta=0:00:00, total=0:00:00, wall=13:58 IST** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.725 Loss=1.070 Prec@1=70.898 Prec@5=91.211 rate=2107.84 Hz, eta=0:00:00, total=0:00:00, wall=13:59 IST** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=0.415 Loss=1.181 Prec@1=70.630 Prec@5=89.958 rate=2107.84 Hz, eta=0:00:00, total=0:00:00, wall=13:59 IST** validation 100.00% of 1x98...Epoch=108/150 LR=0.01894 Time=0.415 Loss=1.181 Prec@1=70.630 Prec@5=89.958 rate=2.88 Hz, eta=0:00:00, total=0:00:33, wall=13:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:59 IST=> training   0.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.220 DataTime=4.950 Loss=0.943 Prec@1=73.438 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=13:59 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.220 DataTime=4.950 Loss=0.943 Prec@1=73.438 Prec@5=91.602 rate=6652.47 Hz, eta=0:00:00, total=0:00:00, wall=13:59 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.220 DataTime=4.950 Loss=0.943 Prec@1=73.438 Prec@5=91.602 rate=6652.47 Hz, eta=0:00:00, total=0:00:00, wall=14:00 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.657 DataTime=0.432 Loss=0.931 Prec@1=75.714 Prec@5=92.570 rate=6652.47 Hz, eta=0:00:00, total=0:00:00, wall=14:00 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.657 DataTime=0.432 Loss=0.931 Prec@1=75.714 Prec@5=92.570 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=14:00 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.657 DataTime=0.432 Loss=0.931 Prec@1=75.714 Prec@5=92.570 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=14:01 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.634 DataTime=0.410 Loss=0.925 Prec@1=75.888 Prec@5=92.653 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=14:01 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.634 DataTime=0.410 Loss=0.925 Prec@1=75.888 Prec@5=92.653 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=14:01 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.634 DataTime=0.410 Loss=0.925 Prec@1=75.888 Prec@5=92.653 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=14:02 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.627 DataTime=0.403 Loss=0.931 Prec@1=75.722 Prec@5=92.582 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=14:02 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.627 DataTime=0.403 Loss=0.931 Prec@1=75.722 Prec@5=92.582 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=14:02 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.627 DataTime=0.403 Loss=0.931 Prec@1=75.722 Prec@5=92.582 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=14:03 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.624 DataTime=0.400 Loss=0.934 Prec@1=75.658 Prec@5=92.561 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=14:03 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.624 DataTime=0.400 Loss=0.934 Prec@1=75.658 Prec@5=92.561 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:03 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.624 DataTime=0.400 Loss=0.934 Prec@1=75.658 Prec@5=92.561 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:04 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.622 DataTime=0.398 Loss=0.934 Prec@1=75.628 Prec@5=92.561 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:04 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.622 DataTime=0.398 Loss=0.934 Prec@1=75.628 Prec@5=92.561 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=14:04 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.622 DataTime=0.398 Loss=0.934 Prec@1=75.628 Prec@5=92.561 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=14:05 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.621 DataTime=0.397 Loss=0.937 Prec@1=75.566 Prec@5=92.501 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=14:05 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.621 DataTime=0.397 Loss=0.937 Prec@1=75.566 Prec@5=92.501 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:05 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.621 DataTime=0.397 Loss=0.937 Prec@1=75.566 Prec@5=92.501 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:06 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.620 DataTime=0.396 Loss=0.940 Prec@1=75.524 Prec@5=92.450 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:06 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.620 DataTime=0.396 Loss=0.940 Prec@1=75.524 Prec@5=92.450 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:06 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.620 DataTime=0.396 Loss=0.940 Prec@1=75.524 Prec@5=92.450 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:07 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.395 Loss=0.941 Prec@1=75.496 Prec@5=92.442 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:07 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.395 Loss=0.941 Prec@1=75.496 Prec@5=92.442 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:07 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.395 Loss=0.941 Prec@1=75.496 Prec@5=92.442 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:08 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.394 Loss=0.944 Prec@1=75.437 Prec@5=92.396 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=14:08 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.394 Loss=0.944 Prec@1=75.437 Prec@5=92.396 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=14:08 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.619 DataTime=0.394 Loss=0.944 Prec@1=75.437 Prec@5=92.396 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=14:09 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.945 Prec@1=75.427 Prec@5=92.388 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=14:09 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.945 Prec@1=75.427 Prec@5=92.388 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:09 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.945 Prec@1=75.427 Prec@5=92.388 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:10 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.946 Prec@1=75.403 Prec@5=92.376 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=14:10 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.946 Prec@1=75.403 Prec@5=92.376 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:10 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.618 DataTime=0.394 Loss=0.946 Prec@1=75.403 Prec@5=92.376 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:11 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.948 Prec@1=75.365 Prec@5=92.355 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:11 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.948 Prec@1=75.365 Prec@5=92.355 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:11 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.948 Prec@1=75.365 Prec@5=92.355 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:12 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.343 Prec@5=92.358 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:12 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.343 Prec@5=92.358 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:12 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.343 Prec@5=92.358 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:14 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.324 Prec@5=92.353 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=14:14 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.324 Prec@5=92.353 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:14 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.393 Loss=0.949 Prec@1=75.324 Prec@5=92.353 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:15 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.392 Loss=0.950 Prec@1=75.304 Prec@5=92.330 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=14:15 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.392 Loss=0.950 Prec@1=75.304 Prec@5=92.330 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:15 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.617 DataTime=0.392 Loss=0.950 Prec@1=75.304 Prec@5=92.330 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:16 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.951 Prec@1=75.284 Prec@5=92.308 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=14:16 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.951 Prec@1=75.284 Prec@5=92.308 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=14:16 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.951 Prec@1=75.284 Prec@5=92.308 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=14:17 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.952 Prec@1=75.259 Prec@5=92.301 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=14:17 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.952 Prec@1=75.259 Prec@5=92.301 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=14:17 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.952 Prec@1=75.259 Prec@5=92.301 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=14:18 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.954 Prec@1=75.222 Prec@5=92.280 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=14:18 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.954 Prec@1=75.222 Prec@5=92.280 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:18 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.954 Prec@1=75.222 Prec@5=92.280 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:19 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.212 Prec@5=92.272 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=14:19 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.212 Prec@5=92.272 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:19 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.212 Prec@5=92.272 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:20 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.198 Prec@5=92.260 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=14:20 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.198 Prec@5=92.260 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:20 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.955 Prec@1=75.198 Prec@5=92.260 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:21 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.956 Prec@1=75.194 Prec@5=92.264 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=14:21 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.956 Prec@1=75.194 Prec@5=92.264 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:21 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.956 Prec@1=75.194 Prec@5=92.264 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:22 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.957 Prec@1=75.159 Prec@5=92.248 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=14:22 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.957 Prec@1=75.159 Prec@5=92.248 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:22 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.957 Prec@1=75.159 Prec@5=92.248 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:23 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.958 Prec@1=75.139 Prec@5=92.232 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=14:23 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.958 Prec@1=75.139 Prec@5=92.232 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:23 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.958 Prec@1=75.139 Prec@5=92.232 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:24 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.127 Prec@5=92.216 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=14:24 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.127 Prec@5=92.216 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=14:24 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.959 Prec@1=75.127 Prec@5=92.216 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=14:25 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.098 Prec@5=92.207 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=14:25 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.098 Prec@5=92.207 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:25 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.098 Prec@5=92.207 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:25 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.098 Prec@5=92.207 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=14:25 IST=> training   100.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.616 DataTime=0.392 Loss=0.960 Prec@1=75.098 Prec@5=92.207 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=14:25 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:25 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:25 IST=> validation 0.00% of 1x98...Epoch=109/150 LR=0.01813 Time=6.997 Loss=1.083 Prec@1=73.438 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=14:25 IST=> validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.997 Loss=1.083 Prec@1=73.438 Prec@5=90.234 rate=5346.54 Hz, eta=0:00:00, total=0:00:00, wall=14:25 IST** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.997 Loss=1.083 Prec@1=73.438 Prec@5=90.234 rate=5346.54 Hz, eta=0:00:00, total=0:00:00, wall=14:25 IST** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=0.401 Loss=1.201 Prec@1=70.418 Prec@5=89.972 rate=5346.54 Hz, eta=0:00:00, total=0:00:00, wall=14:25 IST** validation 100.00% of 1x98...Epoch=109/150 LR=0.01813 Time=0.401 Loss=1.201 Prec@1=70.418 Prec@5=89.972 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=14:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> training   0.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.403 DataTime=5.100 Loss=0.891 Prec@1=77.539 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=14:26 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.403 DataTime=5.100 Loss=0.891 Prec@1=77.539 Prec@5=91.992 rate=5495.65 Hz, eta=0:00:00, total=0:00:00, wall=14:26 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.403 DataTime=5.100 Loss=0.891 Prec@1=77.539 Prec@5=91.992 rate=5495.65 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.657 DataTime=0.432 Loss=0.916 Prec@1=76.257 Prec@5=92.586 rate=5495.65 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.657 DataTime=0.432 Loss=0.916 Prec@1=76.257 Prec@5=92.586 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=14:27 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.657 DataTime=0.432 Loss=0.916 Prec@1=76.257 Prec@5=92.586 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=14:28 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.635 DataTime=0.411 Loss=0.923 Prec@1=75.947 Prec@5=92.546 rate=1.66 Hz, eta=0:24:10, total=0:01:00, wall=14:28 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.635 DataTime=0.411 Loss=0.923 Prec@1=75.947 Prec@5=92.546 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:28 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.635 DataTime=0.411 Loss=0.923 Prec@1=75.947 Prec@5=92.546 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:29 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.628 DataTime=0.404 Loss=0.926 Prec@1=75.923 Prec@5=92.528 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:29 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.628 DataTime=0.404 Loss=0.926 Prec@1=75.923 Prec@5=92.528 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=14:29 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.628 DataTime=0.404 Loss=0.926 Prec@1=75.923 Prec@5=92.528 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=14:30 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.625 DataTime=0.400 Loss=0.929 Prec@1=75.792 Prec@5=92.524 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=14:30 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.625 DataTime=0.400 Loss=0.929 Prec@1=75.792 Prec@5=92.524 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:30 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.625 DataTime=0.400 Loss=0.929 Prec@1=75.792 Prec@5=92.524 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:31 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.623 DataTime=0.398 Loss=0.930 Prec@1=75.704 Prec@5=92.499 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=14:31 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.623 DataTime=0.398 Loss=0.930 Prec@1=75.704 Prec@5=92.499 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=14:31 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.623 DataTime=0.398 Loss=0.930 Prec@1=75.704 Prec@5=92.499 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=14:32 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.621 DataTime=0.397 Loss=0.932 Prec@1=75.660 Prec@5=92.489 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=14:32 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.621 DataTime=0.397 Loss=0.932 Prec@1=75.660 Prec@5=92.489 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:32 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.621 DataTime=0.397 Loss=0.932 Prec@1=75.660 Prec@5=92.489 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:33 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.396 Loss=0.932 Prec@1=75.652 Prec@5=92.517 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=14:33 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.396 Loss=0.932 Prec@1=75.652 Prec@5=92.517 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:33 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.396 Loss=0.932 Prec@1=75.652 Prec@5=92.517 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:34 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.395 Loss=0.932 Prec@1=75.659 Prec@5=92.506 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:34 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.395 Loss=0.932 Prec@1=75.659 Prec@5=92.506 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:34 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.620 DataTime=0.395 Loss=0.932 Prec@1=75.659 Prec@5=92.506 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:35 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.395 Loss=0.932 Prec@1=75.662 Prec@5=92.500 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=14:35 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.395 Loss=0.932 Prec@1=75.662 Prec@5=92.500 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:35 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.395 Loss=0.932 Prec@1=75.662 Prec@5=92.500 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:36 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.394 Loss=0.933 Prec@1=75.652 Prec@5=92.514 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=14:36 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.394 Loss=0.933 Prec@1=75.652 Prec@5=92.514 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:36 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.619 DataTime=0.394 Loss=0.933 Prec@1=75.652 Prec@5=92.514 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.934 Prec@1=75.637 Prec@5=92.502 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=14:37 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.934 Prec@1=75.637 Prec@5=92.502 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:37 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.934 Prec@1=75.637 Prec@5=92.502 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:38 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.936 Prec@1=75.592 Prec@5=92.486 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=14:38 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.936 Prec@1=75.592 Prec@5=92.486 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:38 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.394 Loss=0.936 Prec@1=75.592 Prec@5=92.486 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:39 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.936 Prec@1=75.591 Prec@5=92.483 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=14:39 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.936 Prec@1=75.591 Prec@5=92.483 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:39 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.936 Prec@1=75.591 Prec@5=92.483 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:40 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.937 Prec@1=75.569 Prec@5=92.474 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=14:40 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.937 Prec@1=75.569 Prec@5=92.474 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:40 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.937 Prec@1=75.569 Prec@5=92.474 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:41 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.939 Prec@1=75.545 Prec@5=92.459 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=14:41 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.939 Prec@1=75.545 Prec@5=92.459 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=14:41 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.618 DataTime=0.393 Loss=0.939 Prec@1=75.545 Prec@5=92.459 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=14:42 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.939 Prec@1=75.548 Prec@5=92.449 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=14:42 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.939 Prec@1=75.548 Prec@5=92.449 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:42 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.939 Prec@1=75.548 Prec@5=92.449 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:43 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.942 Prec@1=75.496 Prec@5=92.415 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=14:43 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.942 Prec@1=75.496 Prec@5=92.415 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=14:43 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.393 Loss=0.942 Prec@1=75.496 Prec@5=92.415 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=14:44 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.942 Prec@1=75.484 Prec@5=92.413 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=14:44 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.942 Prec@1=75.484 Prec@5=92.413 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=14:44 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.942 Prec@1=75.484 Prec@5=92.413 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=14:45 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.943 Prec@1=75.455 Prec@5=92.405 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=14:45 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.943 Prec@1=75.455 Prec@5=92.405 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=14:45 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.943 Prec@1=75.455 Prec@5=92.405 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=14:46 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.944 Prec@1=75.424 Prec@5=92.387 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=14:46 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.944 Prec@1=75.424 Prec@5=92.387 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:46 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.944 Prec@1=75.424 Prec@5=92.387 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:47 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.393 Prec@5=92.376 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=14:47 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.393 Prec@5=92.376 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=14:47 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.393 Prec@5=92.376 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=14:48 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.373 Prec@5=92.363 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=14:48 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.373 Prec@5=92.363 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=14:48 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.946 Prec@1=75.373 Prec@5=92.363 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=14:49 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.947 Prec@1=75.362 Prec@5=92.361 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=14:49 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.947 Prec@1=75.362 Prec@5=92.361 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=14:49 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.947 Prec@1=75.362 Prec@5=92.361 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=14:50 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.948 Prec@1=75.343 Prec@5=92.350 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=14:50 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.948 Prec@1=75.343 Prec@5=92.350 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=14:50 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.617 DataTime=0.392 Loss=0.948 Prec@1=75.343 Prec@5=92.350 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=14:51 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.616 DataTime=0.392 Loss=0.949 Prec@1=75.309 Prec@5=92.336 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=14:51 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.616 DataTime=0.392 Loss=0.949 Prec@1=75.309 Prec@5=92.336 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=14:51 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.616 DataTime=0.392 Loss=0.949 Prec@1=75.309 Prec@5=92.336 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=14:51 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.616 DataTime=0.392 Loss=0.949 Prec@1=75.308 Prec@5=92.336 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=14:51 IST=> training   100.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.616 DataTime=0.392 Loss=0.949 Prec@1=75.308 Prec@5=92.336 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=14:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST=> validation 0.00% of 1x98...Epoch=110/150 LR=0.01733 Time=6.745 Loss=1.171 Prec@1=68.750 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST=> validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=6.745 Loss=1.171 Prec@1=68.750 Prec@5=90.820 rate=6353.36 Hz, eta=0:00:00, total=0:00:00, wall=14:51 IST** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=6.745 Loss=1.171 Prec@1=68.750 Prec@5=90.820 rate=6353.36 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=0.403 Loss=1.159 Prec@1=71.158 Prec@5=90.334 rate=6353.36 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST** validation 100.00% of 1x98...Epoch=110/150 LR=0.01733 Time=0.403 Loss=1.159 Prec@1=71.158 Prec@5=90.334 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=14:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> training   0.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=5.209 DataTime=4.917 Loss=0.903 Prec@1=75.195 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=5.209 DataTime=4.917 Loss=0.903 Prec@1=75.195 Prec@5=92.773 rate=7040.02 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=5.209 DataTime=4.917 Loss=0.903 Prec@1=75.195 Prec@5=92.773 rate=7040.02 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.656 DataTime=0.430 Loss=0.895 Prec@1=76.767 Prec@5=92.971 rate=7040.02 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.656 DataTime=0.430 Loss=0.895 Prec@1=76.767 Prec@5=92.971 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=14:53 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.656 DataTime=0.430 Loss=0.895 Prec@1=76.767 Prec@5=92.971 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=14:54 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.634 DataTime=0.410 Loss=0.906 Prec@1=76.452 Prec@5=92.808 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=14:54 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.634 DataTime=0.410 Loss=0.906 Prec@1=76.452 Prec@5=92.808 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:54 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.634 DataTime=0.410 Loss=0.906 Prec@1=76.452 Prec@5=92.808 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:55 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.627 DataTime=0.403 Loss=0.911 Prec@1=76.267 Prec@5=92.720 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=14:55 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.627 DataTime=0.403 Loss=0.911 Prec@1=76.267 Prec@5=92.720 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=14:55 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.627 DataTime=0.403 Loss=0.911 Prec@1=76.267 Prec@5=92.720 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=14:56 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.624 DataTime=0.399 Loss=0.914 Prec@1=76.190 Prec@5=92.683 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=14:56 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.624 DataTime=0.399 Loss=0.914 Prec@1=76.190 Prec@5=92.683 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:56 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.624 DataTime=0.399 Loss=0.914 Prec@1=76.190 Prec@5=92.683 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:57 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.621 DataTime=0.397 Loss=0.916 Prec@1=76.147 Prec@5=92.651 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=14:57 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.621 DataTime=0.397 Loss=0.916 Prec@1=76.147 Prec@5=92.651 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=14:57 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.621 DataTime=0.397 Loss=0.916 Prec@1=76.147 Prec@5=92.651 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=14:58 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.396 Loss=0.919 Prec@1=76.094 Prec@5=92.635 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=14:58 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.396 Loss=0.919 Prec@1=76.094 Prec@5=92.635 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=14:58 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.396 Loss=0.919 Prec@1=76.094 Prec@5=92.635 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=14:59 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.395 Loss=0.920 Prec@1=76.052 Prec@5=92.628 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=14:59 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.395 Loss=0.920 Prec@1=76.052 Prec@5=92.628 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=14:59 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.620 DataTime=0.395 Loss=0.920 Prec@1=76.052 Prec@5=92.628 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:00 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.395 Loss=0.923 Prec@1=75.984 Prec@5=92.606 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:00 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.395 Loss=0.923 Prec@1=75.984 Prec@5=92.606 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:00 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.395 Loss=0.923 Prec@1=75.984 Prec@5=92.606 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:01 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.394 Loss=0.924 Prec@1=75.912 Prec@5=92.585 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:01 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.394 Loss=0.924 Prec@1=75.912 Prec@5=92.585 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:01 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.619 DataTime=0.394 Loss=0.924 Prec@1=75.912 Prec@5=92.585 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:02 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.394 Loss=0.926 Prec@1=75.865 Prec@5=92.570 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:02 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.394 Loss=0.926 Prec@1=75.865 Prec@5=92.570 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:02 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.394 Loss=0.926 Prec@1=75.865 Prec@5=92.570 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:03 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.834 Prec@5=92.563 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:03 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.834 Prec@5=92.563 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:03 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.834 Prec@5=92.563 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:04 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.829 Prec@5=92.561 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:04 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.829 Prec@5=92.561 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:04 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.618 DataTime=0.393 Loss=0.928 Prec@1=75.829 Prec@5=92.561 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:05 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.930 Prec@1=75.795 Prec@5=92.551 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:05 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.930 Prec@1=75.795 Prec@5=92.551 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=15:05 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.930 Prec@1=75.795 Prec@5=92.551 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=15:06 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.931 Prec@1=75.776 Prec@5=92.535 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=15:06 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.931 Prec@1=75.776 Prec@5=92.535 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:06 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.931 Prec@1=75.776 Prec@5=92.535 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:07 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.932 Prec@1=75.745 Prec@5=92.516 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=15:07 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.932 Prec@1=75.745 Prec@5=92.516 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:07 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.393 Loss=0.932 Prec@1=75.745 Prec@5=92.516 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:08 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.392 Loss=0.933 Prec@1=75.712 Prec@5=92.510 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:08 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.392 Loss=0.933 Prec@1=75.712 Prec@5=92.510 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:08 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.617 DataTime=0.392 Loss=0.933 Prec@1=75.712 Prec@5=92.510 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:09 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.934 Prec@1=75.695 Prec@5=92.501 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=15:09 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.934 Prec@1=75.695 Prec@5=92.501 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=15:09 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.934 Prec@1=75.695 Prec@5=92.501 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=15:10 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.935 Prec@1=75.665 Prec@5=92.498 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=15:10 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.935 Prec@1=75.665 Prec@5=92.498 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:10 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.935 Prec@1=75.665 Prec@5=92.498 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:11 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.936 Prec@1=75.638 Prec@5=92.481 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:11 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.936 Prec@1=75.638 Prec@5=92.481 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:11 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.936 Prec@1=75.638 Prec@5=92.481 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:12 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.937 Prec@1=75.614 Prec@5=92.474 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=15:12 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.937 Prec@1=75.614 Prec@5=92.474 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=15:12 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.937 Prec@1=75.614 Prec@5=92.474 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=15:13 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.593 Prec@5=92.461 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=15:13 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.593 Prec@5=92.461 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:13 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.593 Prec@5=92.461 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:15 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.568 Prec@5=92.457 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=15:15 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.568 Prec@5=92.457 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:15 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.938 Prec@1=75.568 Prec@5=92.457 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:16 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.552 Prec@5=92.447 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=15:16 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.552 Prec@5=92.447 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:16 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.552 Prec@5=92.447 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:17 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.543 Prec@5=92.442 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=15:17 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.543 Prec@5=92.442 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:17 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.392 Loss=0.939 Prec@1=75.543 Prec@5=92.442 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:18 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.391 Loss=0.940 Prec@1=75.535 Prec@5=92.438 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=15:18 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.391 Loss=0.940 Prec@1=75.535 Prec@5=92.438 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:18 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.391 Loss=0.940 Prec@1=75.535 Prec@5=92.438 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:18 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.391 Loss=0.940 Prec@1=75.533 Prec@5=92.437 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=15:18 IST=> training   100.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.616 DataTime=0.391 Loss=0.940 Prec@1=75.533 Prec@5=92.437 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=15:18 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> validation 0.00% of 1x98...Epoch=111/150 LR=0.01654 Time=6.999 Loss=0.989 Prec@1=73.828 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=6.999 Loss=0.989 Prec@1=73.828 Prec@5=93.555 rate=6207.48 Hz, eta=0:00:00, total=0:00:00, wall=15:18 IST** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=6.999 Loss=0.989 Prec@1=73.828 Prec@5=93.555 rate=6207.48 Hz, eta=0:00:00, total=0:00:00, wall=15:18 IST** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=0.408 Loss=1.162 Prec@1=71.136 Prec@5=90.290 rate=6207.48 Hz, eta=0:00:00, total=0:00:00, wall=15:18 IST** validation 100.00% of 1x98...Epoch=111/150 LR=0.01654 Time=0.408 Loss=1.162 Prec@1=71.136 Prec@5=90.290 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=15:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> training   0.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.728 DataTime=5.434 Loss=0.891 Prec@1=76.367 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=15:18 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.728 DataTime=5.434 Loss=0.891 Prec@1=76.367 Prec@5=93.164 rate=7447.13 Hz, eta=0:00:00, total=0:00:00, wall=15:18 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.728 DataTime=5.434 Loss=0.891 Prec@1=76.367 Prec@5=93.164 rate=7447.13 Hz, eta=0:00:00, total=0:00:00, wall=15:19 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.659 DataTime=0.434 Loss=0.894 Prec@1=76.686 Prec@5=93.042 rate=7447.13 Hz, eta=0:00:00, total=0:00:00, wall=15:19 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.659 DataTime=0.434 Loss=0.894 Prec@1=76.686 Prec@5=93.042 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=15:19 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.659 DataTime=0.434 Loss=0.894 Prec@1=76.686 Prec@5=93.042 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=15:20 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.637 DataTime=0.413 Loss=0.899 Prec@1=76.525 Prec@5=92.953 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=15:20 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.637 DataTime=0.413 Loss=0.899 Prec@1=76.525 Prec@5=92.953 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:20 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.637 DataTime=0.413 Loss=0.899 Prec@1=76.525 Prec@5=92.953 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:21 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.630 DataTime=0.405 Loss=0.902 Prec@1=76.466 Prec@5=92.917 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=15:21 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.630 DataTime=0.405 Loss=0.902 Prec@1=76.466 Prec@5=92.917 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=15:21 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.630 DataTime=0.405 Loss=0.902 Prec@1=76.466 Prec@5=92.917 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=15:22 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.625 DataTime=0.400 Loss=0.905 Prec@1=76.376 Prec@5=92.860 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=15:22 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.625 DataTime=0.400 Loss=0.905 Prec@1=76.376 Prec@5=92.860 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=15:22 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.625 DataTime=0.400 Loss=0.905 Prec@1=76.376 Prec@5=92.860 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=15:24 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.623 DataTime=0.398 Loss=0.910 Prec@1=76.258 Prec@5=92.786 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=15:24 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.623 DataTime=0.398 Loss=0.910 Prec@1=76.258 Prec@5=92.786 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:24 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.623 DataTime=0.398 Loss=0.910 Prec@1=76.258 Prec@5=92.786 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:25 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.622 DataTime=0.397 Loss=0.911 Prec@1=76.225 Prec@5=92.777 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:25 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.622 DataTime=0.397 Loss=0.911 Prec@1=76.225 Prec@5=92.777 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:25 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.622 DataTime=0.397 Loss=0.911 Prec@1=76.225 Prec@5=92.777 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:26 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.621 DataTime=0.396 Loss=0.913 Prec@1=76.170 Prec@5=92.742 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=15:26 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.621 DataTime=0.396 Loss=0.913 Prec@1=76.170 Prec@5=92.742 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:26 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.621 DataTime=0.396 Loss=0.913 Prec@1=76.170 Prec@5=92.742 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:27 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.620 DataTime=0.395 Loss=0.914 Prec@1=76.167 Prec@5=92.745 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:27 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.620 DataTime=0.395 Loss=0.914 Prec@1=76.167 Prec@5=92.745 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:27 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.620 DataTime=0.395 Loss=0.914 Prec@1=76.167 Prec@5=92.745 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:28 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.619 DataTime=0.395 Loss=0.914 Prec@1=76.150 Prec@5=92.750 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:28 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.619 DataTime=0.395 Loss=0.914 Prec@1=76.150 Prec@5=92.750 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:28 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.619 DataTime=0.395 Loss=0.914 Prec@1=76.150 Prec@5=92.750 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:29 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.916 Prec@1=76.116 Prec@5=92.726 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:29 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.916 Prec@1=76.116 Prec@5=92.726 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:29 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.916 Prec@1=76.116 Prec@5=92.726 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:30 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.917 Prec@1=76.094 Prec@5=92.698 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=15:30 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.917 Prec@1=76.094 Prec@5=92.698 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:30 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.394 Loss=0.917 Prec@1=76.094 Prec@5=92.698 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:31 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.393 Loss=0.917 Prec@1=76.094 Prec@5=92.685 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=15:31 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.393 Loss=0.917 Prec@1=76.094 Prec@5=92.685 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=15:31 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.618 DataTime=0.393 Loss=0.917 Prec@1=76.094 Prec@5=92.685 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=15:32 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.920 Prec@1=76.034 Prec@5=92.657 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=15:32 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.920 Prec@1=76.034 Prec@5=92.657 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:32 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.920 Prec@1=76.034 Prec@5=92.657 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:33 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=76.004 Prec@5=92.654 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:33 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=76.004 Prec@5=92.654 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:33 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=76.004 Prec@5=92.654 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:34 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=75.998 Prec@5=92.652 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=15:34 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=75.998 Prec@5=92.652 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:34 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.921 Prec@1=75.998 Prec@5=92.652 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:35 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.922 Prec@1=75.982 Prec@5=92.637 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=15:35 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.922 Prec@1=75.982 Prec@5=92.637 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:35 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.617 DataTime=0.393 Loss=0.922 Prec@1=75.982 Prec@5=92.637 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:36 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.923 Prec@1=75.941 Prec@5=92.616 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=15:36 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.923 Prec@1=75.941 Prec@5=92.616 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:36 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.923 Prec@1=75.941 Prec@5=92.616 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:37 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.924 Prec@1=75.910 Prec@5=92.607 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=15:37 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.924 Prec@1=75.910 Prec@5=92.607 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:37 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.924 Prec@1=75.910 Prec@5=92.607 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:38 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.925 Prec@1=75.896 Prec@5=92.601 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=15:38 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.925 Prec@1=75.896 Prec@5=92.601 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:38 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.925 Prec@1=75.896 Prec@5=92.601 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:39 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.926 Prec@1=75.886 Prec@5=92.585 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=15:39 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.926 Prec@1=75.886 Prec@5=92.585 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:39 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.926 Prec@1=75.886 Prec@5=92.585 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:40 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.860 Prec@5=92.577 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=15:40 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.860 Prec@5=92.577 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:40 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.860 Prec@5=92.577 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:41 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.856 Prec@5=92.570 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=15:41 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.856 Prec@5=92.570 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:41 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.927 Prec@1=75.856 Prec@5=92.570 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:42 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.928 Prec@1=75.835 Prec@5=92.564 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=15:42 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.928 Prec@1=75.835 Prec@5=92.564 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:42 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.392 Loss=0.928 Prec@1=75.835 Prec@5=92.564 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:43 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.391 Loss=0.929 Prec@1=75.807 Prec@5=92.556 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=15:43 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.391 Loss=0.929 Prec@1=75.807 Prec@5=92.556 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:43 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.616 DataTime=0.391 Loss=0.929 Prec@1=75.807 Prec@5=92.556 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.615 DataTime=0.391 Loss=0.929 Prec@1=75.797 Prec@5=92.551 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.615 DataTime=0.391 Loss=0.929 Prec@1=75.797 Prec@5=92.551 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.615 DataTime=0.391 Loss=0.929 Prec@1=75.797 Prec@5=92.551 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.615 DataTime=0.391 Loss=0.930 Prec@1=75.796 Prec@5=92.550 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=15:44 IST=> training   100.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.615 DataTime=0.391 Loss=0.930 Prec@1=75.796 Prec@5=92.550 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=15:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 0.00% of 1x98...Epoch=112/150 LR=0.01577 Time=6.867 Loss=1.124 Prec@1=72.656 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=6.867 Loss=1.124 Prec@1=72.656 Prec@5=91.406 rate=3965.09 Hz, eta=0:00:00, total=0:00:00, wall=15:44 IST** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=6.867 Loss=1.124 Prec@1=72.656 Prec@5=91.406 rate=3965.09 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=0.400 Loss=1.165 Prec@1=71.218 Prec@5=90.380 rate=3965.09 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST** validation 100.00% of 1x98...Epoch=112/150 LR=0.01577 Time=0.400 Loss=1.165 Prec@1=71.218 Prec@5=90.380 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=15:45 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.378 DataTime=5.082 Loss=0.892 Prec@1=77.734 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.378 DataTime=5.082 Loss=0.892 Prec@1=77.734 Prec@5=93.750 rate=7624.51 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.378 DataTime=5.082 Loss=0.892 Prec@1=77.734 Prec@5=93.750 rate=7624.51 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.657 DataTime=0.432 Loss=0.899 Prec@1=76.650 Prec@5=92.835 rate=7624.51 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.657 DataTime=0.432 Loss=0.899 Prec@1=76.650 Prec@5=92.835 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=15:46 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.657 DataTime=0.432 Loss=0.899 Prec@1=76.650 Prec@5=92.835 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=15:47 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.634 DataTime=0.410 Loss=0.899 Prec@1=76.456 Prec@5=92.819 rate=1.66 Hz, eta=0:24:11, total=0:01:01, wall=15:47 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.634 DataTime=0.410 Loss=0.899 Prec@1=76.456 Prec@5=92.819 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=15:47 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.634 DataTime=0.410 Loss=0.899 Prec@1=76.456 Prec@5=92.819 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=15:48 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.627 DataTime=0.404 Loss=0.900 Prec@1=76.482 Prec@5=92.822 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=15:48 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.627 DataTime=0.404 Loss=0.900 Prec@1=76.482 Prec@5=92.822 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=15:48 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.627 DataTime=0.404 Loss=0.900 Prec@1=76.482 Prec@5=92.822 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=15:49 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.624 DataTime=0.400 Loss=0.901 Prec@1=76.428 Prec@5=92.809 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=15:49 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.624 DataTime=0.400 Loss=0.901 Prec@1=76.428 Prec@5=92.809 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=15:49 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.624 DataTime=0.400 Loss=0.901 Prec@1=76.428 Prec@5=92.809 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=15:50 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.622 DataTime=0.398 Loss=0.901 Prec@1=76.427 Prec@5=92.851 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=15:50 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.622 DataTime=0.398 Loss=0.901 Prec@1=76.427 Prec@5=92.851 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:50 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.622 DataTime=0.398 Loss=0.901 Prec@1=76.427 Prec@5=92.851 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:51 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.901 Prec@1=76.394 Prec@5=92.875 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=15:51 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.901 Prec@1=76.394 Prec@5=92.875 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=15:51 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.901 Prec@1=76.394 Prec@5=92.875 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=15:52 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.902 Prec@1=76.356 Prec@5=92.868 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=15:52 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.902 Prec@1=76.356 Prec@5=92.868 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:52 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.620 DataTime=0.396 Loss=0.902 Prec@1=76.356 Prec@5=92.868 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:53 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.395 Loss=0.904 Prec@1=76.332 Prec@5=92.834 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=15:53 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.395 Loss=0.904 Prec@1=76.332 Prec@5=92.834 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:53 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.395 Loss=0.904 Prec@1=76.332 Prec@5=92.834 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:54 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.904 Prec@1=76.344 Prec@5=92.823 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=15:54 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.904 Prec@1=76.344 Prec@5=92.823 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:54 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.904 Prec@1=76.344 Prec@5=92.823 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:55 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.906 Prec@1=76.303 Prec@5=92.795 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=15:55 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.906 Prec@1=76.303 Prec@5=92.795 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=15:55 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.619 DataTime=0.394 Loss=0.906 Prec@1=76.303 Prec@5=92.795 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=15:56 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.394 Loss=0.907 Prec@1=76.270 Prec@5=92.792 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=15:56 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.394 Loss=0.907 Prec@1=76.270 Prec@5=92.792 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:56 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.394 Loss=0.907 Prec@1=76.270 Prec@5=92.792 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:57 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.393 Loss=0.910 Prec@1=76.215 Prec@5=92.774 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=15:57 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.393 Loss=0.910 Prec@1=76.215 Prec@5=92.774 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:57 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.618 DataTime=0.393 Loss=0.910 Prec@1=76.215 Prec@5=92.774 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:58 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.199 Prec@5=92.764 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=15:58 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.199 Prec@5=92.764 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:58 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.199 Prec@5=92.764 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:59 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.191 Prec@5=92.777 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=15:59 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.191 Prec@5=92.777 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=15:59 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.910 Prec@1=76.191 Prec@5=92.777 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=16:00 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.912 Prec@1=76.155 Prec@5=92.757 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=16:00 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.912 Prec@1=76.155 Prec@5=92.757 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:00 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.617 DataTime=0.393 Loss=0.912 Prec@1=76.155 Prec@5=92.757 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:01 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.393 Loss=0.912 Prec@1=76.156 Prec@5=92.746 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=16:01 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.393 Loss=0.912 Prec@1=76.156 Prec@5=92.746 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:01 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.393 Loss=0.912 Prec@1=76.156 Prec@5=92.746 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:02 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.913 Prec@1=76.154 Prec@5=92.737 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=16:02 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.913 Prec@1=76.154 Prec@5=92.737 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=16:02 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.913 Prec@1=76.154 Prec@5=92.737 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=16:03 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.147 Prec@5=92.720 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=16:03 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.147 Prec@5=92.720 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:03 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.147 Prec@5=92.720 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:04 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.141 Prec@5=92.708 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=16:04 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.141 Prec@5=92.708 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=16:04 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.914 Prec@1=76.141 Prec@5=92.708 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=16:05 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.915 Prec@1=76.130 Prec@5=92.702 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=16:05 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.915 Prec@1=76.130 Prec@5=92.702 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:05 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.915 Prec@1=76.130 Prec@5=92.702 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:06 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.916 Prec@1=76.112 Prec@5=92.694 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=16:06 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.916 Prec@1=76.112 Prec@5=92.694 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:06 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.916 Prec@1=76.112 Prec@5=92.694 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:07 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.917 Prec@1=76.087 Prec@5=92.687 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=16:07 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.917 Prec@1=76.087 Prec@5=92.687 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:07 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.917 Prec@1=76.087 Prec@5=92.687 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:08 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.065 Prec@5=92.682 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=16:08 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.065 Prec@5=92.682 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:08 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.065 Prec@5=92.682 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:09 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.048 Prec@5=92.680 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=16:09 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.048 Prec@5=92.680 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:09 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.918 Prec@1=76.048 Prec@5=92.680 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:10 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.919 Prec@1=76.029 Prec@5=92.675 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=16:10 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.919 Prec@1=76.029 Prec@5=92.675 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:10 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.919 Prec@1=76.029 Prec@5=92.675 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:10 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.919 Prec@1=76.028 Prec@5=92.674 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=16:10 IST=> training   100.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.616 DataTime=0.392 Loss=0.919 Prec@1=76.028 Prec@5=92.674 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=16:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST=> validation 0.00% of 1x98...Epoch=113/150 LR=0.01502 Time=7.088 Loss=1.068 Prec@1=72.656 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=16:10 IST=> validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=7.088 Loss=1.068 Prec@1=72.656 Prec@5=90.625 rate=6558.80 Hz, eta=0:00:00, total=0:00:00, wall=16:10 IST** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=7.088 Loss=1.068 Prec@1=72.656 Prec@5=90.625 rate=6558.80 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=0.408 Loss=1.161 Prec@1=71.346 Prec@5=90.484 rate=6558.80 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST** validation 100.00% of 1x98...Epoch=113/150 LR=0.01502 Time=0.408 Loss=1.161 Prec@1=71.346 Prec@5=90.484 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=16:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST=> training   0.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.877 DataTime=4.435 Loss=1.029 Prec@1=73.438 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=16:11 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.877 DataTime=4.435 Loss=1.029 Prec@1=73.438 Prec@5=91.992 rate=3265.30 Hz, eta=0:00:00, total=0:00:00, wall=16:11 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=4.877 DataTime=4.435 Loss=1.029 Prec@1=73.438 Prec@5=91.992 rate=3265.30 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.653 DataTime=0.426 Loss=0.900 Prec@1=76.472 Prec@5=92.938 rate=3265.30 Hz, eta=0:00:00, total=0:00:00, wall=16:12 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.653 DataTime=0.426 Loss=0.900 Prec@1=76.472 Prec@5=92.938 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=16:12 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.653 DataTime=0.426 Loss=0.900 Prec@1=76.472 Prec@5=92.938 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=16:13 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.634 DataTime=0.409 Loss=0.894 Prec@1=76.673 Prec@5=92.996 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=16:13 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.634 DataTime=0.409 Loss=0.894 Prec@1=76.673 Prec@5=92.996 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:13 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.634 DataTime=0.409 Loss=0.894 Prec@1=76.673 Prec@5=92.996 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:14 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.627 DataTime=0.402 Loss=0.893 Prec@1=76.642 Prec@5=92.978 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=16:14 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.627 DataTime=0.402 Loss=0.893 Prec@1=76.642 Prec@5=92.978 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:14 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.627 DataTime=0.402 Loss=0.893 Prec@1=76.642 Prec@5=92.978 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:15 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.624 DataTime=0.399 Loss=0.898 Prec@1=76.535 Prec@5=92.898 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=16:15 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.624 DataTime=0.399 Loss=0.898 Prec@1=76.535 Prec@5=92.898 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=16:15 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.624 DataTime=0.399 Loss=0.898 Prec@1=76.535 Prec@5=92.898 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=16:16 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.623 DataTime=0.397 Loss=0.897 Prec@1=76.543 Prec@5=92.918 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=16:16 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.623 DataTime=0.397 Loss=0.897 Prec@1=76.543 Prec@5=92.918 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=16:16 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.623 DataTime=0.397 Loss=0.897 Prec@1=76.543 Prec@5=92.918 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=16:17 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.621 DataTime=0.396 Loss=0.897 Prec@1=76.537 Prec@5=92.917 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=16:17 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.621 DataTime=0.396 Loss=0.897 Prec@1=76.537 Prec@5=92.917 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:17 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.621 DataTime=0.396 Loss=0.897 Prec@1=76.537 Prec@5=92.917 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:18 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.900 Prec@1=76.501 Prec@5=92.884 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=16:18 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.900 Prec@1=76.501 Prec@5=92.884 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:18 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.900 Prec@1=76.501 Prec@5=92.884 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:19 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.478 Prec@5=92.867 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=16:19 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.478 Prec@5=92.867 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:19 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.478 Prec@5=92.867 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:20 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.472 Prec@5=92.860 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=16:20 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.472 Prec@5=92.860 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:20 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.620 DataTime=0.395 Loss=0.901 Prec@1=76.472 Prec@5=92.860 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:21 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.901 Prec@1=76.473 Prec@5=92.866 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=16:21 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.901 Prec@1=76.473 Prec@5=92.866 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:21 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.901 Prec@1=76.473 Prec@5=92.866 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:22 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.902 Prec@1=76.449 Prec@5=92.854 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=16:22 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.902 Prec@1=76.449 Prec@5=92.854 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=16:22 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.902 Prec@1=76.449 Prec@5=92.854 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=16:23 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.903 Prec@1=76.430 Prec@5=92.840 rate=1.63 Hz, eta=0:14:21, total=0:11:16, wall=16:23 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.903 Prec@1=76.430 Prec@5=92.840 rate=1.63 Hz, eta=0:13:20, total=0:12:18, wall=16:23 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.619 DataTime=0.394 Loss=0.903 Prec@1=76.430 Prec@5=92.840 rate=1.63 Hz, eta=0:13:20, total=0:12:18, wall=16:25 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.404 Prec@5=92.821 rate=1.63 Hz, eta=0:13:20, total=0:12:18, wall=16:25 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.404 Prec@5=92.821 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:25 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.404 Prec@5=92.821 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:26 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.398 Prec@5=92.830 rate=1.63 Hz, eta=0:12:18, total=0:13:19, wall=16:26 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.398 Prec@5=92.830 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=16:26 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.904 Prec@1=76.398 Prec@5=92.830 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=16:27 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.905 Prec@1=76.377 Prec@5=92.818 rate=1.63 Hz, eta=0:11:17, total=0:14:20, wall=16:27 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.905 Prec@1=76.377 Prec@5=92.818 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:27 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.618 DataTime=0.393 Loss=0.905 Prec@1=76.377 Prec@5=92.818 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:28 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.393 Loss=0.905 Prec@1=76.390 Prec@5=92.816 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:28 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.393 Loss=0.905 Prec@1=76.390 Prec@5=92.816 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:28 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.393 Loss=0.905 Prec@1=76.390 Prec@5=92.816 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:29 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.905 Prec@1=76.371 Prec@5=92.826 rate=1.63 Hz, eta=0:09:14, total=0:16:23, wall=16:29 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.905 Prec@1=76.371 Prec@5=92.826 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:29 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.905 Prec@1=76.371 Prec@5=92.826 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:30 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.812 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:30 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.812 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:30 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.812 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:31 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.810 rate=1.63 Hz, eta=0:07:11, total=0:18:26, wall=16:31 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.810 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:31 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.350 Prec@5=92.810 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:32 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.352 Prec@5=92.813 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:32 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.352 Prec@5=92.813 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=16:32 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.906 Prec@1=76.352 Prec@5=92.813 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=16:33 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.907 Prec@1=76.336 Prec@5=92.801 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=16:33 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.907 Prec@1=76.336 Prec@5=92.801 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:33 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.907 Prec@1=76.336 Prec@5=92.801 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:34 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.908 Prec@1=76.317 Prec@5=92.797 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:34 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.908 Prec@1=76.317 Prec@5=92.797 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=16:34 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.617 DataTime=0.392 Loss=0.908 Prec@1=76.317 Prec@5=92.797 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=16:35 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.289 Prec@5=92.784 rate=1.63 Hz, eta=0:03:05, total=0:22:32, wall=16:35 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.289 Prec@5=92.784 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=16:35 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.289 Prec@5=92.784 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=16:36 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.280 Prec@5=92.775 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=16:36 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.280 Prec@5=92.775 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:36 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.909 Prec@1=76.280 Prec@5=92.775 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:37 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.910 Prec@1=76.259 Prec@5=92.775 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=16:37 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.910 Prec@1=76.259 Prec@5=92.775 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=16:37 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.910 Prec@1=76.259 Prec@5=92.775 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=16:37 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.910 Prec@1=76.260 Prec@5=92.776 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=16:37 IST=> training   100.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.616 DataTime=0.392 Loss=0.910 Prec@1=76.260 Prec@5=92.776 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=16:37 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:37 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:37 IST=> validation 0.00% of 1x98...Epoch=114/150 LR=0.01428 Time=6.708 Loss=1.154 Prec@1=71.094 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=16:37 IST=> validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=6.708 Loss=1.154 Prec@1=71.094 Prec@5=89.844 rate=5293.33 Hz, eta=0:00:00, total=0:00:00, wall=16:37 IST** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=6.708 Loss=1.154 Prec@1=71.094 Prec@5=89.844 rate=5293.33 Hz, eta=0:00:00, total=0:00:00, wall=16:37 IST** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=0.407 Loss=1.157 Prec@1=71.290 Prec@5=90.520 rate=5293.33 Hz, eta=0:00:00, total=0:00:00, wall=16:37 IST** validation 100.00% of 1x98...Epoch=114/150 LR=0.01428 Time=0.407 Loss=1.157 Prec@1=71.290 Prec@5=90.520 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=16:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> training   0.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=6.162 DataTime=5.852 Loss=0.870 Prec@1=77.734 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=16:38 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=6.162 DataTime=5.852 Loss=0.870 Prec@1=77.734 Prec@5=93.750 rate=8147.10 Hz, eta=0:00:00, total=0:00:00, wall=16:38 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=6.162 DataTime=5.852 Loss=0.870 Prec@1=77.734 Prec@5=93.750 rate=8147.10 Hz, eta=0:00:00, total=0:00:00, wall=16:39 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.666 DataTime=0.439 Loss=0.871 Prec@1=77.059 Prec@5=93.139 rate=8147.10 Hz, eta=0:00:00, total=0:00:00, wall=16:39 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.666 DataTime=0.439 Loss=0.871 Prec@1=77.059 Prec@5=93.139 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=16:39 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.666 DataTime=0.439 Loss=0.871 Prec@1=77.059 Prec@5=93.139 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=16:40 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.641 DataTime=0.415 Loss=0.874 Prec@1=77.115 Prec@5=93.168 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=16:40 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.641 DataTime=0.415 Loss=0.874 Prec@1=77.115 Prec@5=93.168 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:40 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.641 DataTime=0.415 Loss=0.874 Prec@1=77.115 Prec@5=93.168 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:41 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.631 DataTime=0.407 Loss=0.881 Prec@1=76.954 Prec@5=93.054 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=16:41 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.631 DataTime=0.407 Loss=0.881 Prec@1=76.954 Prec@5=93.054 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=16:41 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.631 DataTime=0.407 Loss=0.881 Prec@1=76.954 Prec@5=93.054 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=16:42 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.626 DataTime=0.402 Loss=0.883 Prec@1=76.919 Prec@5=93.053 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=16:42 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.626 DataTime=0.402 Loss=0.883 Prec@1=76.919 Prec@5=93.053 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=16:42 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.626 DataTime=0.402 Loss=0.883 Prec@1=76.919 Prec@5=93.053 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=16:43 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.624 DataTime=0.399 Loss=0.883 Prec@1=76.893 Prec@5=93.068 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=16:43 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.624 DataTime=0.399 Loss=0.883 Prec@1=76.893 Prec@5=93.068 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=16:43 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.624 DataTime=0.399 Loss=0.883 Prec@1=76.893 Prec@5=93.068 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=16:44 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.622 DataTime=0.398 Loss=0.886 Prec@1=76.787 Prec@5=93.046 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=16:44 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.622 DataTime=0.398 Loss=0.886 Prec@1=76.787 Prec@5=93.046 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=16:44 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.622 DataTime=0.398 Loss=0.886 Prec@1=76.787 Prec@5=93.046 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=16:45 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.397 Loss=0.885 Prec@1=76.803 Prec@5=93.054 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=16:45 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.397 Loss=0.885 Prec@1=76.803 Prec@5=93.054 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:45 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.397 Loss=0.885 Prec@1=76.803 Prec@5=93.054 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:46 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.396 Loss=0.888 Prec@1=76.785 Prec@5=93.009 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=16:46 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.396 Loss=0.888 Prec@1=76.785 Prec@5=93.009 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:46 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.621 DataTime=0.396 Loss=0.888 Prec@1=76.785 Prec@5=93.009 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:47 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.396 Loss=0.887 Prec@1=76.800 Prec@5=93.030 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=16:47 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.396 Loss=0.887 Prec@1=76.800 Prec@5=93.030 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:47 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.396 Loss=0.887 Prec@1=76.800 Prec@5=93.030 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:48 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.395 Loss=0.889 Prec@1=76.763 Prec@5=93.008 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=16:48 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.395 Loss=0.889 Prec@1=76.763 Prec@5=93.008 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:48 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.620 DataTime=0.395 Loss=0.889 Prec@1=76.763 Prec@5=93.008 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:49 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.395 Loss=0.889 Prec@1=76.733 Prec@5=93.009 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=16:49 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.395 Loss=0.889 Prec@1=76.733 Prec@5=93.009 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:49 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.395 Loss=0.889 Prec@1=76.733 Prec@5=93.009 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:50 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.889 Prec@1=76.733 Prec@5=93.022 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=16:50 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.889 Prec@1=76.733 Prec@5=93.022 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:50 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.889 Prec@1=76.733 Prec@5=93.022 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.890 Prec@1=76.727 Prec@5=93.015 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=16:51 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.890 Prec@1=76.727 Prec@5=93.015 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:51 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.619 DataTime=0.394 Loss=0.890 Prec@1=76.727 Prec@5=93.015 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:52 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.889 Prec@1=76.727 Prec@5=93.019 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=16:52 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.889 Prec@1=76.727 Prec@5=93.019 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:52 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.889 Prec@1=76.727 Prec@5=93.019 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:53 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.891 Prec@1=76.694 Prec@5=92.997 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=16:53 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.891 Prec@1=76.694 Prec@5=92.997 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:53 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.394 Loss=0.891 Prec@1=76.694 Prec@5=92.997 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:54 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.649 Prec@5=92.984 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=16:54 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.649 Prec@5=92.984 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=16:54 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.649 Prec@5=92.984 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=16:55 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.648 Prec@5=92.980 rate=1.63 Hz, eta=0:09:13, total=0:16:23, wall=16:55 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.648 Prec@5=92.980 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:55 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.618 DataTime=0.393 Loss=0.893 Prec@1=76.648 Prec@5=92.980 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:56 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.894 Prec@1=76.628 Prec@5=92.968 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=16:56 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.894 Prec@1=76.628 Prec@5=92.968 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=16:56 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.894 Prec@1=76.628 Prec@5=92.968 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=16:57 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.895 Prec@1=76.622 Prec@5=92.959 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=16:57 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.895 Prec@1=76.622 Prec@5=92.959 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:57 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.393 Loss=0.895 Prec@1=76.622 Prec@5=92.959 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:58 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.896 Prec@1=76.606 Prec@5=92.953 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=16:58 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.896 Prec@1=76.606 Prec@5=92.953 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:58 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.896 Prec@1=76.606 Prec@5=92.953 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:59 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.597 Prec@5=92.940 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=16:59 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.597 Prec@5=92.940 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=16:59 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.597 Prec@5=92.940 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:00 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.578 Prec@5=92.938 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=17:00 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.578 Prec@5=92.938 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:00 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.578 Prec@5=92.938 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:01 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.571 Prec@5=92.934 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=17:01 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.571 Prec@5=92.934 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:01 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.897 Prec@1=76.571 Prec@5=92.934 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:02 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.898 Prec@1=76.554 Prec@5=92.923 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=17:02 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.898 Prec@1=76.554 Prec@5=92.923 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:02 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.898 Prec@1=76.554 Prec@5=92.923 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:03 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.899 Prec@1=76.532 Prec@5=92.913 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=17:03 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.899 Prec@1=76.532 Prec@5=92.913 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:03 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.899 Prec@1=76.532 Prec@5=92.913 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:03 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.899 Prec@1=76.530 Prec@5=92.912 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=17:03 IST=> training   100.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.617 DataTime=0.392 Loss=0.899 Prec@1=76.530 Prec@5=92.912 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=17:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:03 IST=> validation 0.00% of 1x98...Epoch=115/150 LR=0.01355 Time=6.424 Loss=1.255 Prec@1=68.945 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=17:03 IST=> validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.424 Loss=1.255 Prec@1=68.945 Prec@5=88.477 rate=4174.91 Hz, eta=0:00:00, total=0:00:00, wall=17:03 IST** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.424 Loss=1.255 Prec@1=68.945 Prec@5=88.477 rate=4174.91 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=0.408 Loss=1.141 Prec@1=71.550 Prec@5=90.824 rate=4174.91 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST** validation 100.00% of 1x98...Epoch=115/150 LR=0.01355 Time=0.408 Loss=1.141 Prec@1=71.550 Prec@5=90.824 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=17:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST=> training   0.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.007 DataTime=4.755 Loss=0.797 Prec@1=77.344 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.007 DataTime=4.755 Loss=0.797 Prec@1=77.344 Prec@5=93.750 rate=5131.36 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=5.007 DataTime=4.755 Loss=0.797 Prec@1=77.344 Prec@5=93.750 rate=5131.36 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.657 DataTime=0.429 Loss=0.876 Prec@1=77.017 Prec@5=93.160 rate=5131.36 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.657 DataTime=0.429 Loss=0.876 Prec@1=77.017 Prec@5=93.160 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:05 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.657 DataTime=0.429 Loss=0.876 Prec@1=77.017 Prec@5=93.160 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:06 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.636 DataTime=0.409 Loss=0.876 Prec@1=77.122 Prec@5=93.161 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=17:06 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.636 DataTime=0.409 Loss=0.876 Prec@1=77.122 Prec@5=93.161 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:06 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.636 DataTime=0.409 Loss=0.876 Prec@1=77.122 Prec@5=93.161 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:07 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.628 DataTime=0.403 Loss=0.871 Prec@1=77.204 Prec@5=93.187 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=17:07 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.628 DataTime=0.403 Loss=0.871 Prec@1=77.204 Prec@5=93.187 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:07 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.628 DataTime=0.403 Loss=0.871 Prec@1=77.204 Prec@5=93.187 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:08 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.624 DataTime=0.399 Loss=0.873 Prec@1=77.153 Prec@5=93.171 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=17:08 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.624 DataTime=0.399 Loss=0.873 Prec@1=77.153 Prec@5=93.171 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=17:08 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.624 DataTime=0.399 Loss=0.873 Prec@1=77.153 Prec@5=93.171 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=17:09 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.622 DataTime=0.397 Loss=0.876 Prec@1=77.086 Prec@5=93.139 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=17:09 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.622 DataTime=0.397 Loss=0.876 Prec@1=77.086 Prec@5=93.139 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=17:09 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.622 DataTime=0.397 Loss=0.876 Prec@1=77.086 Prec@5=93.139 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=17:10 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.621 DataTime=0.396 Loss=0.875 Prec@1=77.124 Prec@5=93.156 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=17:10 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.621 DataTime=0.396 Loss=0.875 Prec@1=77.124 Prec@5=93.156 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:10 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.621 DataTime=0.396 Loss=0.875 Prec@1=77.124 Prec@5=93.156 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:11 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.395 Loss=0.875 Prec@1=77.096 Prec@5=93.153 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=17:11 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.395 Loss=0.875 Prec@1=77.096 Prec@5=93.153 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:11 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.395 Loss=0.875 Prec@1=77.096 Prec@5=93.153 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:12 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.394 Loss=0.876 Prec@1=77.088 Prec@5=93.148 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=17:12 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.394 Loss=0.876 Prec@1=77.088 Prec@5=93.148 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:12 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.619 DataTime=0.394 Loss=0.876 Prec@1=77.088 Prec@5=93.148 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:13 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.394 Loss=0.877 Prec@1=77.048 Prec@5=93.148 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:13 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.394 Loss=0.877 Prec@1=77.048 Prec@5=93.148 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:13 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.394 Loss=0.877 Prec@1=77.048 Prec@5=93.148 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:14 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.878 Prec@1=77.020 Prec@5=93.133 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=17:14 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.878 Prec@1=77.020 Prec@5=93.133 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:14 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.878 Prec@1=77.020 Prec@5=93.133 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:15 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.880 Prec@1=76.993 Prec@5=93.118 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=17:15 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.880 Prec@1=76.993 Prec@5=93.118 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:15 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.618 DataTime=0.393 Loss=0.880 Prec@1=76.993 Prec@5=93.118 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:16 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.881 Prec@1=76.993 Prec@5=93.102 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=17:16 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.881 Prec@1=76.993 Prec@5=93.102 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:16 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.881 Prec@1=76.993 Prec@5=93.102 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:17 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.880 Prec@1=76.988 Prec@5=93.111 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=17:17 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.880 Prec@1=76.988 Prec@5=93.111 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:17 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.393 Loss=0.880 Prec@1=76.988 Prec@5=93.111 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:18 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.953 Prec@5=93.110 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=17:18 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.953 Prec@5=93.110 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:18 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.953 Prec@5=93.110 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:19 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.950 Prec@5=93.108 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=17:19 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.950 Prec@5=93.108 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:19 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.881 Prec@1=76.950 Prec@5=93.108 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:20 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.882 Prec@1=76.912 Prec@5=93.093 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=17:20 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.882 Prec@1=76.912 Prec@5=93.093 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:20 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.882 Prec@1=76.912 Prec@5=93.093 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:21 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.076 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=17:21 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.076 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:21 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.617 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.076 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:22 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.066 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=17:22 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.066 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:22 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.884 Prec@1=76.892 Prec@5=93.066 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.869 Prec@5=93.058 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=17:24 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.869 Prec@5=93.058 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:24 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.869 Prec@5=93.058 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:25 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.859 Prec@5=93.049 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=17:25 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.859 Prec@5=93.049 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:25 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.885 Prec@1=76.859 Prec@5=93.049 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:26 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.886 Prec@1=76.837 Prec@5=93.045 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=17:26 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.886 Prec@1=76.837 Prec@5=93.045 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:26 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.886 Prec@1=76.837 Prec@5=93.045 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:27 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.887 Prec@1=76.812 Prec@5=93.033 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=17:27 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.887 Prec@1=76.812 Prec@5=93.033 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=17:27 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.887 Prec@1=76.812 Prec@5=93.033 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=17:28 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.888 Prec@1=76.790 Prec@5=93.024 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=17:28 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.888 Prec@1=76.790 Prec@5=93.024 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:28 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.888 Prec@1=76.790 Prec@5=93.024 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:29 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.889 Prec@1=76.767 Prec@5=93.006 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=17:29 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.889 Prec@1=76.767 Prec@5=93.006 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:29 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.616 DataTime=0.392 Loss=0.889 Prec@1=76.767 Prec@5=93.006 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:30 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.615 DataTime=0.391 Loss=0.890 Prec@1=76.731 Prec@5=92.991 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=17:30 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.615 DataTime=0.391 Loss=0.890 Prec@1=76.731 Prec@5=92.991 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=17:30 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.615 DataTime=0.391 Loss=0.890 Prec@1=76.731 Prec@5=92.991 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=17:30 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.615 DataTime=0.391 Loss=0.890 Prec@1=76.728 Prec@5=92.990 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=17:30 IST=> training   100.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.615 DataTime=0.391 Loss=0.890 Prec@1=76.728 Prec@5=92.990 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=17:30 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> validation 0.00% of 1x98...Epoch=116/150 LR=0.01284 Time=6.755 Loss=1.143 Prec@1=71.875 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=6.755 Loss=1.143 Prec@1=71.875 Prec@5=88.672 rate=3396.54 Hz, eta=0:00:00, total=0:00:00, wall=17:30 IST** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=6.755 Loss=1.143 Prec@1=71.875 Prec@5=88.672 rate=3396.54 Hz, eta=0:00:00, total=0:00:00, wall=17:30 IST** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=0.409 Loss=1.159 Prec@1=71.208 Prec@5=90.386 rate=3396.54 Hz, eta=0:00:00, total=0:00:00, wall=17:30 IST** validation 100.00% of 1x98...Epoch=116/150 LR=0.01284 Time=0.409 Loss=1.159 Prec@1=71.208 Prec@5=90.386 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=17:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> training   0.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.968 DataTime=5.687 Loss=0.840 Prec@1=76.562 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.968 DataTime=5.687 Loss=0.840 Prec@1=76.562 Prec@5=93.750 rate=7109.94 Hz, eta=0:00:00, total=0:00:00, wall=17:30 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.968 DataTime=5.687 Loss=0.840 Prec@1=76.562 Prec@5=93.750 rate=7109.94 Hz, eta=0:00:00, total=0:00:00, wall=17:31 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.663 DataTime=0.440 Loss=0.853 Prec@1=77.527 Prec@5=93.487 rate=7109.94 Hz, eta=0:00:00, total=0:00:00, wall=17:31 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.663 DataTime=0.440 Loss=0.853 Prec@1=77.527 Prec@5=93.487 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:31 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.663 DataTime=0.440 Loss=0.853 Prec@1=77.527 Prec@5=93.487 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:33 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.639 DataTime=0.414 Loss=0.854 Prec@1=77.513 Prec@5=93.465 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=17:33 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.639 DataTime=0.414 Loss=0.854 Prec@1=77.513 Prec@5=93.465 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:33 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.639 DataTime=0.414 Loss=0.854 Prec@1=77.513 Prec@5=93.465 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:34 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.630 DataTime=0.406 Loss=0.858 Prec@1=77.496 Prec@5=93.402 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=17:34 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.630 DataTime=0.406 Loss=0.858 Prec@1=77.496 Prec@5=93.402 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=17:34 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.630 DataTime=0.406 Loss=0.858 Prec@1=77.496 Prec@5=93.402 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=17:35 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.626 DataTime=0.401 Loss=0.861 Prec@1=77.452 Prec@5=93.344 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=17:35 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.626 DataTime=0.401 Loss=0.861 Prec@1=77.452 Prec@5=93.344 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=17:35 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.626 DataTime=0.401 Loss=0.861 Prec@1=77.452 Prec@5=93.344 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=17:36 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.623 DataTime=0.399 Loss=0.863 Prec@1=77.389 Prec@5=93.334 rate=1.64 Hz, eta=0:21:24, total=0:04:04, wall=17:36 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.623 DataTime=0.399 Loss=0.863 Prec@1=77.389 Prec@5=93.334 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=17:36 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.623 DataTime=0.399 Loss=0.863 Prec@1=77.389 Prec@5=93.334 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=17:37 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.622 DataTime=0.398 Loss=0.865 Prec@1=77.317 Prec@5=93.314 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=17:37 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.622 DataTime=0.398 Loss=0.865 Prec@1=77.317 Prec@5=93.314 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=17:37 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.622 DataTime=0.398 Loss=0.865 Prec@1=77.317 Prec@5=93.314 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=17:38 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.621 DataTime=0.396 Loss=0.866 Prec@1=77.294 Prec@5=93.305 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=17:38 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.621 DataTime=0.396 Loss=0.866 Prec@1=77.294 Prec@5=93.305 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=17:38 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.621 DataTime=0.396 Loss=0.866 Prec@1=77.294 Prec@5=93.305 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=17:39 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.620 DataTime=0.396 Loss=0.866 Prec@1=77.302 Prec@5=93.285 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=17:39 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.620 DataTime=0.396 Loss=0.866 Prec@1=77.302 Prec@5=93.285 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:39 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.620 DataTime=0.396 Loss=0.866 Prec@1=77.302 Prec@5=93.285 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:40 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.619 DataTime=0.395 Loss=0.867 Prec@1=77.283 Prec@5=93.277 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=17:40 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.619 DataTime=0.395 Loss=0.867 Prec@1=77.283 Prec@5=93.277 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=17:40 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.619 DataTime=0.395 Loss=0.867 Prec@1=77.283 Prec@5=93.277 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=17:41 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.395 Loss=0.869 Prec@1=77.239 Prec@5=93.254 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=17:41 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.395 Loss=0.869 Prec@1=77.239 Prec@5=93.254 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=17:41 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.395 Loss=0.869 Prec@1=77.239 Prec@5=93.254 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=17:42 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.394 Loss=0.870 Prec@1=77.223 Prec@5=93.243 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=17:42 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.394 Loss=0.870 Prec@1=77.223 Prec@5=93.243 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=17:42 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.618 DataTime=0.394 Loss=0.870 Prec@1=77.223 Prec@5=93.243 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=17:43 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.394 Loss=0.870 Prec@1=77.201 Prec@5=93.240 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=17:43 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.394 Loss=0.870 Prec@1=77.201 Prec@5=93.240 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=17:43 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.394 Loss=0.870 Prec@1=77.201 Prec@5=93.240 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=17:44 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.870 Prec@1=77.205 Prec@5=93.243 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=17:44 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.870 Prec@1=77.205 Prec@5=93.243 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=17:44 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.870 Prec@1=77.205 Prec@5=93.243 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=17:45 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.871 Prec@1=77.159 Prec@5=93.224 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=17:45 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.871 Prec@1=77.159 Prec@5=93.224 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=17:45 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.617 DataTime=0.393 Loss=0.871 Prec@1=77.159 Prec@5=93.224 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=17:46 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.393 Loss=0.872 Prec@1=77.121 Prec@5=93.222 rate=1.63 Hz, eta=0:11:14, total=0:14:18, wall=17:46 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.393 Loss=0.872 Prec@1=77.121 Prec@5=93.222 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=17:46 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.393 Loss=0.872 Prec@1=77.121 Prec@5=93.222 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=17:47 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.873 Prec@1=77.109 Prec@5=93.219 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=17:47 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.873 Prec@1=77.109 Prec@5=93.219 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=17:47 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.873 Prec@1=77.109 Prec@5=93.219 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=17:48 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.874 Prec@1=77.078 Prec@5=93.211 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=17:48 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.874 Prec@1=77.078 Prec@5=93.211 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=17:48 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.874 Prec@1=77.078 Prec@5=93.211 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=17:49 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.875 Prec@1=77.053 Prec@5=93.194 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=17:49 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.875 Prec@1=77.053 Prec@5=93.194 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=17:49 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.875 Prec@1=77.053 Prec@5=93.194 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=17:50 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.876 Prec@1=77.033 Prec@5=93.187 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=17:50 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.876 Prec@1=77.033 Prec@5=93.187 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=17:50 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.876 Prec@1=77.033 Prec@5=93.187 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=17:51 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.014 Prec@5=93.167 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=17:51 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.014 Prec@5=93.167 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=17:51 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.014 Prec@5=93.167 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=17:52 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.011 Prec@5=93.162 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=17:52 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.011 Prec@5=93.162 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=17:52 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.616 DataTime=0.392 Loss=0.877 Prec@1=77.011 Prec@5=93.162 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=17:53 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.877 Prec@1=76.993 Prec@5=93.161 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=17:53 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.877 Prec@1=76.993 Prec@5=93.161 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=17:53 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.877 Prec@1=76.993 Prec@5=93.161 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=17:54 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.878 Prec@1=76.971 Prec@5=93.153 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=17:54 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.878 Prec@1=76.971 Prec@5=93.153 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=17:54 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.392 Loss=0.878 Prec@1=76.971 Prec@5=93.153 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=17:55 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.879 Prec@1=76.945 Prec@5=93.135 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=17:55 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.879 Prec@1=76.945 Prec@5=93.135 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=17:55 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.879 Prec@1=76.945 Prec@5=93.135 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=17:56 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.880 Prec@1=76.931 Prec@5=93.126 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=17:56 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.880 Prec@1=76.931 Prec@5=93.126 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=17:56 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.880 Prec@1=76.931 Prec@5=93.126 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=17:56 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.880 Prec@1=76.931 Prec@5=93.126 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=17:56 IST=> training   100.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.615 DataTime=0.391 Loss=0.880 Prec@1=76.931 Prec@5=93.126 rate=1.63 Hz, eta=0:00:00, total=0:25:33, wall=17:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:56 IST=> validation 0.00% of 1x98...Epoch=117/150 LR=0.01215 Time=6.222 Loss=1.116 Prec@1=71.680 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=17:56 IST=> validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=6.222 Loss=1.116 Prec@1=71.680 Prec@5=89.844 rate=4213.31 Hz, eta=0:00:00, total=0:00:00, wall=17:56 IST** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=6.222 Loss=1.116 Prec@1=71.680 Prec@5=89.844 rate=4213.31 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=0.403 Loss=1.127 Prec@1=71.884 Prec@5=90.782 rate=4213.31 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST** validation 100.00% of 1x98...Epoch=117/150 LR=0.01215 Time=0.403 Loss=1.127 Prec@1=71.884 Prec@5=90.782 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=17:57 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> training   0.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=5.168 DataTime=4.826 Loss=0.863 Prec@1=76.367 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=5.168 DataTime=4.826 Loss=0.863 Prec@1=76.367 Prec@5=93.750 rate=4015.02 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=5.168 DataTime=4.826 Loss=0.863 Prec@1=76.367 Prec@5=93.750 rate=4015.02 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.653 DataTime=0.428 Loss=0.851 Prec@1=77.601 Prec@5=93.441 rate=4015.02 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.653 DataTime=0.428 Loss=0.851 Prec@1=77.601 Prec@5=93.441 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=17:58 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.653 DataTime=0.428 Loss=0.851 Prec@1=77.601 Prec@5=93.441 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.634 DataTime=0.408 Loss=0.856 Prec@1=77.580 Prec@5=93.372 rate=1.66 Hz, eta=0:24:08, total=0:01:00, wall=17:59 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.634 DataTime=0.408 Loss=0.856 Prec@1=77.580 Prec@5=93.372 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=17:59 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.634 DataTime=0.408 Loss=0.856 Prec@1=77.580 Prec@5=93.372 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=18:00 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.626 DataTime=0.402 Loss=0.854 Prec@1=77.634 Prec@5=93.400 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=18:00 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.626 DataTime=0.402 Loss=0.854 Prec@1=77.634 Prec@5=93.400 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=18:00 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.626 DataTime=0.402 Loss=0.854 Prec@1=77.634 Prec@5=93.400 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=18:01 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.623 DataTime=0.399 Loss=0.854 Prec@1=77.686 Prec@5=93.392 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=18:01 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.623 DataTime=0.399 Loss=0.854 Prec@1=77.686 Prec@5=93.392 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=18:01 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.623 DataTime=0.399 Loss=0.854 Prec@1=77.686 Prec@5=93.392 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=18:02 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.621 DataTime=0.397 Loss=0.853 Prec@1=77.698 Prec@5=93.393 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=18:02 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.621 DataTime=0.397 Loss=0.853 Prec@1=77.698 Prec@5=93.393 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:02 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.621 DataTime=0.397 Loss=0.853 Prec@1=77.698 Prec@5=93.393 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:03 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.619 DataTime=0.396 Loss=0.852 Prec@1=77.703 Prec@5=93.398 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:03 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.619 DataTime=0.396 Loss=0.852 Prec@1=77.703 Prec@5=93.398 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=18:03 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.619 DataTime=0.396 Loss=0.852 Prec@1=77.703 Prec@5=93.398 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=18:04 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.395 Loss=0.853 Prec@1=77.664 Prec@5=93.386 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=18:04 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.395 Loss=0.853 Prec@1=77.664 Prec@5=93.386 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=18:04 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.395 Loss=0.853 Prec@1=77.664 Prec@5=93.386 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=18:05 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.855 Prec@1=77.600 Prec@5=93.349 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=18:05 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.855 Prec@1=77.600 Prec@5=93.349 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=18:05 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.855 Prec@1=77.600 Prec@5=93.349 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=18:06 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.857 Prec@1=77.554 Prec@5=93.326 rate=1.63 Hz, eta=0:17:21, total=0:08:09, wall=18:06 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.857 Prec@1=77.554 Prec@5=93.326 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=18:06 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.618 DataTime=0.394 Loss=0.857 Prec@1=77.554 Prec@5=93.326 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=18:07 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.859 Prec@1=77.500 Prec@5=93.307 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=18:07 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.859 Prec@1=77.500 Prec@5=93.307 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=18:07 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.859 Prec@1=77.500 Prec@5=93.307 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=18:08 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.861 Prec@1=77.465 Prec@5=93.292 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=18:08 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.861 Prec@1=77.465 Prec@5=93.292 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=18:08 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.861 Prec@1=77.465 Prec@5=93.292 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=18:09 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.860 Prec@1=77.465 Prec@5=93.299 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=18:09 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.860 Prec@1=77.465 Prec@5=93.299 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=18:09 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.617 DataTime=0.393 Loss=0.860 Prec@1=77.465 Prec@5=93.299 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=18:10 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.393 Loss=0.861 Prec@1=77.434 Prec@5=93.300 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=18:10 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.393 Loss=0.861 Prec@1=77.434 Prec@5=93.300 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=18:10 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.393 Loss=0.861 Prec@1=77.434 Prec@5=93.300 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=18:11 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.862 Prec@1=77.419 Prec@5=93.296 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=18:11 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.862 Prec@1=77.419 Prec@5=93.296 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:11 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.862 Prec@1=77.419 Prec@5=93.296 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:12 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.396 Prec@5=93.292 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:12 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.396 Prec@5=93.292 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=18:12 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.396 Prec@5=93.292 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=18:13 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.377 Prec@5=93.286 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=18:13 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.377 Prec@5=93.286 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=18:13 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.863 Prec@1=77.377 Prec@5=93.286 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=18:14 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.864 Prec@1=77.372 Prec@5=93.284 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=18:14 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.864 Prec@1=77.372 Prec@5=93.284 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=18:14 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.864 Prec@1=77.372 Prec@5=93.284 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=18:15 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.865 Prec@1=77.342 Prec@5=93.268 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=18:15 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.865 Prec@1=77.342 Prec@5=93.268 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=18:15 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.392 Loss=0.865 Prec@1=77.342 Prec@5=93.268 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=18:16 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.391 Loss=0.865 Prec@1=77.330 Prec@5=93.264 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=18:16 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.391 Loss=0.865 Prec@1=77.330 Prec@5=93.264 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=18:16 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.616 DataTime=0.391 Loss=0.865 Prec@1=77.330 Prec@5=93.264 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=18:17 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.310 Prec@5=93.256 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=18:17 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.310 Prec@5=93.256 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=18:17 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.310 Prec@5=93.256 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=18:18 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.288 Prec@5=93.252 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=18:18 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.288 Prec@5=93.252 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=18:18 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.866 Prec@1=77.288 Prec@5=93.252 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=18:19 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.867 Prec@1=77.262 Prec@5=93.242 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=18:19 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.867 Prec@1=77.262 Prec@5=93.242 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=18:19 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.867 Prec@1=77.262 Prec@5=93.242 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=18:20 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.241 Prec@5=93.240 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=18:20 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.241 Prec@5=93.240 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=18:20 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.241 Prec@5=93.240 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=18:21 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.230 Prec@5=93.238 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=18:21 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.230 Prec@5=93.238 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=18:21 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.868 Prec@1=77.230 Prec@5=93.238 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=18:22 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.869 Prec@1=77.208 Prec@5=93.230 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=18:22 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.869 Prec@1=77.208 Prec@5=93.230 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=18:22 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.869 Prec@1=77.208 Prec@5=93.230 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=18:22 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.869 Prec@1=77.207 Prec@5=93.229 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=18:22 IST=> training   100.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.615 DataTime=0.391 Loss=0.869 Prec@1=77.207 Prec@5=93.229 rate=1.63 Hz, eta=0:00:00, total=0:25:33, wall=18:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> validation 0.00% of 1x98...Epoch=118/150 LR=0.01147 Time=6.577 Loss=1.082 Prec@1=73.828 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.577 Loss=1.082 Prec@1=73.828 Prec@5=91.992 rate=3870.01 Hz, eta=0:00:00, total=0:00:00, wall=18:23 IST** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.577 Loss=1.082 Prec@1=73.828 Prec@5=91.992 rate=3870.01 Hz, eta=0:00:00, total=0:00:00, wall=18:23 IST** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=0.414 Loss=1.129 Prec@1=71.762 Prec@5=90.932 rate=3870.01 Hz, eta=0:00:00, total=0:00:00, wall=18:23 IST** validation 100.00% of 1x98...Epoch=118/150 LR=0.01147 Time=0.414 Loss=1.129 Prec@1=71.762 Prec@5=90.932 rate=2.88 Hz, eta=0:00:00, total=0:00:34, wall=18:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> training   0.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.719 DataTime=4.425 Loss=0.797 Prec@1=79.883 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=18:23 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.719 DataTime=4.425 Loss=0.797 Prec@1=79.883 Prec@5=94.141 rate=4869.07 Hz, eta=0:00:00, total=0:00:00, wall=18:23 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.719 DataTime=4.425 Loss=0.797 Prec@1=79.883 Prec@5=94.141 rate=4869.07 Hz, eta=0:00:00, total=0:00:00, wall=18:24 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.653 DataTime=0.427 Loss=0.833 Prec@1=78.001 Prec@5=93.663 rate=4869.07 Hz, eta=0:00:00, total=0:00:00, wall=18:24 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.653 DataTime=0.427 Loss=0.833 Prec@1=78.001 Prec@5=93.663 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:24 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.653 DataTime=0.427 Loss=0.833 Prec@1=78.001 Prec@5=93.663 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:25 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.634 DataTime=0.408 Loss=0.834 Prec@1=78.019 Prec@5=93.600 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=18:25 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.634 DataTime=0.408 Loss=0.834 Prec@1=78.019 Prec@5=93.600 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:25 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.634 DataTime=0.408 Loss=0.834 Prec@1=78.019 Prec@5=93.600 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:26 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.626 DataTime=0.401 Loss=0.842 Prec@1=77.847 Prec@5=93.519 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=18:26 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.626 DataTime=0.401 Loss=0.842 Prec@1=77.847 Prec@5=93.519 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:26 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.626 DataTime=0.401 Loss=0.842 Prec@1=77.847 Prec@5=93.519 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:27 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.623 DataTime=0.399 Loss=0.841 Prec@1=77.841 Prec@5=93.527 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=18:27 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.623 DataTime=0.399 Loss=0.841 Prec@1=77.841 Prec@5=93.527 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=18:27 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.623 DataTime=0.399 Loss=0.841 Prec@1=77.841 Prec@5=93.527 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=18:28 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.621 DataTime=0.397 Loss=0.843 Prec@1=77.787 Prec@5=93.516 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=18:28 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.621 DataTime=0.397 Loss=0.843 Prec@1=77.787 Prec@5=93.516 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=18:28 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.621 DataTime=0.397 Loss=0.843 Prec@1=77.787 Prec@5=93.516 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=18:29 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.620 DataTime=0.396 Loss=0.843 Prec@1=77.772 Prec@5=93.519 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=18:29 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.620 DataTime=0.396 Loss=0.843 Prec@1=77.772 Prec@5=93.519 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=18:29 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.620 DataTime=0.396 Loss=0.843 Prec@1=77.772 Prec@5=93.519 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=18:30 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.619 DataTime=0.395 Loss=0.845 Prec@1=77.767 Prec@5=93.495 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=18:30 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.619 DataTime=0.395 Loss=0.845 Prec@1=77.767 Prec@5=93.495 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=18:30 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.619 DataTime=0.395 Loss=0.845 Prec@1=77.767 Prec@5=93.495 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=18:31 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.847 Prec@1=77.722 Prec@5=93.465 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=18:31 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.847 Prec@1=77.722 Prec@5=93.465 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:31 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.847 Prec@1=77.722 Prec@5=93.465 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:32 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.848 Prec@1=77.711 Prec@5=93.453 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=18:32 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.848 Prec@1=77.711 Prec@5=93.453 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=18:32 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.618 DataTime=0.394 Loss=0.848 Prec@1=77.711 Prec@5=93.453 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=18:33 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.850 Prec@1=77.666 Prec@5=93.421 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=18:33 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.850 Prec@1=77.666 Prec@5=93.421 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:33 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.850 Prec@1=77.666 Prec@5=93.421 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:34 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.647 Prec@5=93.416 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=18:34 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.647 Prec@5=93.416 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:34 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.647 Prec@5=93.416 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:35 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.637 Prec@5=93.423 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=18:35 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.637 Prec@5=93.423 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:35 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.851 Prec@1=77.637 Prec@5=93.423 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:37 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.852 Prec@1=77.615 Prec@5=93.404 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=18:37 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.852 Prec@1=77.615 Prec@5=93.404 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=18:37 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.617 DataTime=0.393 Loss=0.852 Prec@1=77.615 Prec@5=93.404 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=18:38 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.853 Prec@1=77.608 Prec@5=93.397 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=18:38 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.853 Prec@1=77.608 Prec@5=93.397 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:38 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.853 Prec@1=77.608 Prec@5=93.397 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:39 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.854 Prec@1=77.582 Prec@5=93.373 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=18:39 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.854 Prec@1=77.582 Prec@5=93.373 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:39 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.854 Prec@1=77.582 Prec@5=93.373 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:40 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.561 Prec@5=93.361 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=18:40 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.561 Prec@5=93.361 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=18:40 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.561 Prec@5=93.361 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=18:41 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.549 Prec@5=93.360 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=18:41 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.549 Prec@5=93.360 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=18:41 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.855 Prec@1=77.549 Prec@5=93.360 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=18:42 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.520 Prec@5=93.352 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=18:42 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.520 Prec@5=93.352 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=18:42 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.520 Prec@5=93.352 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=18:43 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.511 Prec@5=93.363 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=18:43 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.511 Prec@5=93.363 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:43 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.392 Loss=0.856 Prec@1=77.511 Prec@5=93.363 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:44 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.518 Prec@5=93.359 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=18:44 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.518 Prec@5=93.359 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=18:44 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.518 Prec@5=93.359 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=18:45 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.517 Prec@5=93.356 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=18:45 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.517 Prec@5=93.356 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:45 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.517 Prec@5=93.356 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:46 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.511 Prec@5=93.349 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=18:46 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.511 Prec@5=93.349 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:46 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.857 Prec@1=77.511 Prec@5=93.349 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:47 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.858 Prec@1=77.491 Prec@5=93.346 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=18:47 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.858 Prec@1=77.491 Prec@5=93.346 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:47 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.858 Prec@1=77.491 Prec@5=93.346 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:48 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.859 Prec@1=77.467 Prec@5=93.337 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=18:48 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.859 Prec@1=77.467 Prec@5=93.337 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:48 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.859 Prec@1=77.467 Prec@5=93.337 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:49 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.860 Prec@1=77.452 Prec@5=93.329 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=18:49 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.860 Prec@1=77.452 Prec@5=93.329 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:49 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.616 DataTime=0.391 Loss=0.860 Prec@1=77.452 Prec@5=93.329 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:49 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.615 DataTime=0.391 Loss=0.860 Prec@1=77.453 Prec@5=93.329 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=18:49 IST=> training   100.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.615 DataTime=0.391 Loss=0.860 Prec@1=77.453 Prec@5=93.329 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=18:49 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:49 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:49 IST=> validation 0.00% of 1x98...Epoch=119/150 LR=0.01082 Time=7.173 Loss=1.153 Prec@1=70.703 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=18:49 IST=> validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=7.173 Loss=1.153 Prec@1=70.703 Prec@5=90.820 rate=5262.27 Hz, eta=0:00:00, total=0:00:00, wall=18:49 IST** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=7.173 Loss=1.153 Prec@1=70.703 Prec@5=90.820 rate=5262.27 Hz, eta=0:00:00, total=0:00:00, wall=18:49 IST** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=0.404 Loss=1.133 Prec@1=72.090 Prec@5=90.930 rate=5262.27 Hz, eta=0:00:00, total=0:00:00, wall=18:49 IST** validation 100.00% of 1x98...Epoch=119/150 LR=0.01082 Time=0.404 Loss=1.133 Prec@1=72.090 Prec@5=90.930 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=18:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> training   0.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=6.118 DataTime=5.898 Loss=0.738 Prec@1=80.273 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=18:50 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=6.118 DataTime=5.898 Loss=0.738 Prec@1=80.273 Prec@5=94.922 rate=4617.87 Hz, eta=0:00:00, total=0:00:00, wall=18:50 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=6.118 DataTime=5.898 Loss=0.738 Prec@1=80.273 Prec@5=94.922 rate=4617.87 Hz, eta=0:00:00, total=0:00:00, wall=18:51 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.663 DataTime=0.439 Loss=0.823 Prec@1=78.245 Prec@5=93.725 rate=4617.87 Hz, eta=0:00:00, total=0:00:00, wall=18:51 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.663 DataTime=0.439 Loss=0.823 Prec@1=78.245 Prec@5=93.725 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=18:51 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.663 DataTime=0.439 Loss=0.823 Prec@1=78.245 Prec@5=93.725 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=18:52 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.638 DataTime=0.413 Loss=0.831 Prec@1=78.161 Prec@5=93.555 rate=1.66 Hz, eta=0:24:06, total=0:01:00, wall=18:52 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.638 DataTime=0.413 Loss=0.831 Prec@1=78.161 Prec@5=93.555 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=18:52 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.638 DataTime=0.413 Loss=0.831 Prec@1=78.161 Prec@5=93.555 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=18:53 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.629 DataTime=0.405 Loss=0.832 Prec@1=78.170 Prec@5=93.576 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=18:53 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.629 DataTime=0.405 Loss=0.832 Prec@1=78.170 Prec@5=93.576 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=18:53 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.629 DataTime=0.405 Loss=0.832 Prec@1=78.170 Prec@5=93.576 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=18:54 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.624 DataTime=0.401 Loss=0.831 Prec@1=78.164 Prec@5=93.571 rate=1.64 Hz, eta=0:22:19, total=0:03:03, wall=18:54 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.624 DataTime=0.401 Loss=0.831 Prec@1=78.164 Prec@5=93.571 rate=1.64 Hz, eta=0:21:20, total=0:04:04, wall=18:54 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.624 DataTime=0.401 Loss=0.831 Prec@1=78.164 Prec@5=93.571 rate=1.64 Hz, eta=0:21:20, total=0:04:04, wall=18:55 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.622 DataTime=0.399 Loss=0.836 Prec@1=78.027 Prec@5=93.525 rate=1.64 Hz, eta=0:21:20, total=0:04:04, wall=18:55 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.622 DataTime=0.399 Loss=0.836 Prec@1=78.027 Prec@5=93.525 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:55 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.622 DataTime=0.399 Loss=0.836 Prec@1=78.027 Prec@5=93.525 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:56 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.621 DataTime=0.397 Loss=0.836 Prec@1=78.007 Prec@5=93.532 rate=1.64 Hz, eta=0:20:21, total=0:05:05, wall=18:56 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.621 DataTime=0.397 Loss=0.836 Prec@1=78.007 Prec@5=93.532 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=18:56 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.621 DataTime=0.397 Loss=0.836 Prec@1=78.007 Prec@5=93.532 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=18:57 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.620 DataTime=0.396 Loss=0.837 Prec@1=77.973 Prec@5=93.559 rate=1.64 Hz, eta=0:19:21, total=0:06:06, wall=18:57 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.620 DataTime=0.396 Loss=0.837 Prec@1=77.973 Prec@5=93.559 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=18:57 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.620 DataTime=0.396 Loss=0.837 Prec@1=77.973 Prec@5=93.559 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=18:58 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.619 DataTime=0.395 Loss=0.838 Prec@1=77.929 Prec@5=93.531 rate=1.64 Hz, eta=0:18:20, total=0:07:08, wall=18:58 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.619 DataTime=0.395 Loss=0.838 Prec@1=77.929 Prec@5=93.531 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=18:58 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.619 DataTime=0.395 Loss=0.838 Prec@1=77.929 Prec@5=93.531 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=18:59 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.395 Loss=0.838 Prec@1=77.955 Prec@5=93.541 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=18:59 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.395 Loss=0.838 Prec@1=77.955 Prec@5=93.541 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=18:59 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.395 Loss=0.838 Prec@1=77.955 Prec@5=93.541 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=19:00 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.945 Prec@5=93.534 rate=1.64 Hz, eta=0:16:19, total=0:09:10, wall=19:00 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.945 Prec@5=93.534 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=19:00 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.945 Prec@5=93.534 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=19:01 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.936 Prec@5=93.541 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=19:01 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.936 Prec@5=93.541 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=19:01 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.618 DataTime=0.394 Loss=0.838 Prec@1=77.936 Prec@5=93.541 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=19:02 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.839 Prec@1=77.939 Prec@5=93.524 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=19:02 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.839 Prec@1=77.939 Prec@5=93.524 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=19:02 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.839 Prec@1=77.939 Prec@5=93.524 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=19:03 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.840 Prec@1=77.928 Prec@5=93.514 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=19:03 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.840 Prec@1=77.928 Prec@5=93.514 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=19:03 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.840 Prec@1=77.928 Prec@5=93.514 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=19:04 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.841 Prec@1=77.900 Prec@5=93.497 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=19:04 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.841 Prec@1=77.900 Prec@5=93.497 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=19:04 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.617 DataTime=0.393 Loss=0.841 Prec@1=77.900 Prec@5=93.497 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=19:05 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.842 Prec@1=77.867 Prec@5=93.488 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=19:05 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.842 Prec@1=77.867 Prec@5=93.488 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=19:05 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.842 Prec@1=77.867 Prec@5=93.488 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.843 Prec@1=77.846 Prec@5=93.483 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=19:06 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.843 Prec@1=77.846 Prec@5=93.483 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=19:06 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.843 Prec@1=77.846 Prec@5=93.483 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=19:07 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.845 Prec@1=77.821 Prec@5=93.469 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=19:07 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.845 Prec@1=77.821 Prec@5=93.469 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=19:07 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.845 Prec@1=77.821 Prec@5=93.469 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=19:08 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.795 Prec@5=93.460 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=19:08 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.795 Prec@5=93.460 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=19:08 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.795 Prec@5=93.460 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=19:09 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.769 Prec@5=93.464 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=19:09 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.769 Prec@5=93.464 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=19:09 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.846 Prec@1=77.769 Prec@5=93.464 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=19:10 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.761 Prec@5=93.453 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=19:10 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.761 Prec@5=93.453 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=19:10 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.761 Prec@5=93.453 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=19:11 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.756 Prec@5=93.455 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=19:11 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.756 Prec@5=93.455 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=19:11 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.847 Prec@1=77.756 Prec@5=93.455 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=19:12 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.738 Prec@5=93.445 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=19:12 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.738 Prec@5=93.445 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=19:12 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.738 Prec@5=93.445 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=19:13 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.717 Prec@5=93.436 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=19:13 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.717 Prec@5=93.436 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=19:13 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.848 Prec@1=77.717 Prec@5=93.436 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=19:14 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.849 Prec@1=77.686 Prec@5=93.426 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=19:14 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.849 Prec@1=77.686 Prec@5=93.426 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:14 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.392 Loss=0.849 Prec@1=77.686 Prec@5=93.426 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:15 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.391 Loss=0.850 Prec@1=77.686 Prec@5=93.418 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=19:15 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.391 Loss=0.850 Prec@1=77.686 Prec@5=93.418 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:15 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.391 Loss=0.850 Prec@1=77.686 Prec@5=93.418 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:15 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.391 Loss=0.850 Prec@1=77.685 Prec@5=93.418 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=19:15 IST=> training   100.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.616 DataTime=0.391 Loss=0.850 Prec@1=77.685 Prec@5=93.418 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=19:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:15 IST=> validation 0.00% of 1x98...Epoch=120/150 LR=0.01017 Time=7.222 Loss=1.134 Prec@1=73.438 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=19:15 IST=> validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=7.222 Loss=1.134 Prec@1=73.438 Prec@5=91.211 rate=7112.67 Hz, eta=0:00:00, total=0:00:00, wall=19:15 IST** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=7.222 Loss=1.134 Prec@1=73.438 Prec@5=91.211 rate=7112.67 Hz, eta=0:00:00, total=0:00:00, wall=19:16 IST** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=0.400 Loss=1.121 Prec@1=72.246 Prec@5=91.088 rate=7112.67 Hz, eta=0:00:00, total=0:00:00, wall=19:16 IST** validation 100.00% of 1x98...Epoch=120/150 LR=0.01017 Time=0.400 Loss=1.121 Prec@1=72.246 Prec@5=91.088 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=19:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:16 IST=> training   0.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.296 DataTime=5.013 Loss=0.800 Prec@1=78.711 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=19:16 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.296 DataTime=5.013 Loss=0.800 Prec@1=78.711 Prec@5=93.164 rate=6714.75 Hz, eta=0:00:00, total=0:00:00, wall=19:16 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.296 DataTime=5.013 Loss=0.800 Prec@1=78.711 Prec@5=93.164 rate=6714.75 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.661 DataTime=0.437 Loss=0.823 Prec@1=78.465 Prec@5=93.659 rate=6714.75 Hz, eta=0:00:00, total=0:00:00, wall=19:17 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.661 DataTime=0.437 Loss=0.823 Prec@1=78.465 Prec@5=93.659 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=19:17 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.661 DataTime=0.437 Loss=0.823 Prec@1=78.465 Prec@5=93.659 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=19:18 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.637 DataTime=0.413 Loss=0.822 Prec@1=78.395 Prec@5=93.720 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=19:18 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.637 DataTime=0.413 Loss=0.822 Prec@1=78.395 Prec@5=93.720 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:18 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.637 DataTime=0.413 Loss=0.822 Prec@1=78.395 Prec@5=93.720 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:19 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.629 DataTime=0.405 Loss=0.822 Prec@1=78.396 Prec@5=93.734 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=19:19 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.629 DataTime=0.405 Loss=0.822 Prec@1=78.396 Prec@5=93.734 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=19:19 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.629 DataTime=0.405 Loss=0.822 Prec@1=78.396 Prec@5=93.734 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=19:20 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.626 DataTime=0.401 Loss=0.825 Prec@1=78.258 Prec@5=93.701 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=19:20 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.626 DataTime=0.401 Loss=0.825 Prec@1=78.258 Prec@5=93.701 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=19:20 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.626 DataTime=0.401 Loss=0.825 Prec@1=78.258 Prec@5=93.701 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=19:21 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.623 DataTime=0.399 Loss=0.825 Prec@1=78.245 Prec@5=93.694 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=19:21 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.623 DataTime=0.399 Loss=0.825 Prec@1=78.245 Prec@5=93.694 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=19:21 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.623 DataTime=0.399 Loss=0.825 Prec@1=78.245 Prec@5=93.694 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=19:22 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.621 DataTime=0.397 Loss=0.827 Prec@1=78.196 Prec@5=93.684 rate=1.63 Hz, eta=0:20:26, total=0:05:07, wall=19:22 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.621 DataTime=0.397 Loss=0.827 Prec@1=78.196 Prec@5=93.684 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:22 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.621 DataTime=0.397 Loss=0.827 Prec@1=78.196 Prec@5=93.684 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:23 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.620 DataTime=0.396 Loss=0.829 Prec@1=78.144 Prec@5=93.651 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=19:23 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.620 DataTime=0.396 Loss=0.829 Prec@1=78.144 Prec@5=93.651 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:23 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.620 DataTime=0.396 Loss=0.829 Prec@1=78.144 Prec@5=93.651 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:24 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.395 Loss=0.831 Prec@1=78.125 Prec@5=93.637 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=19:24 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.395 Loss=0.831 Prec@1=78.125 Prec@5=93.637 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=19:24 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.395 Loss=0.831 Prec@1=78.125 Prec@5=93.637 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=19:25 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.394 Loss=0.833 Prec@1=78.087 Prec@5=93.628 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=19:25 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.394 Loss=0.833 Prec@1=78.087 Prec@5=93.628 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:25 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.619 DataTime=0.394 Loss=0.833 Prec@1=78.087 Prec@5=93.628 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:26 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.394 Loss=0.832 Prec@1=78.084 Prec@5=93.636 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=19:26 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.394 Loss=0.832 Prec@1=78.084 Prec@5=93.636 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:26 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.394 Loss=0.832 Prec@1=78.084 Prec@5=93.636 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:27 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.080 Prec@5=93.624 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:27 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.080 Prec@5=93.624 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=19:27 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.080 Prec@5=93.624 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=19:28 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.060 Prec@5=93.624 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=19:28 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.060 Prec@5=93.624 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:28 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.618 DataTime=0.393 Loss=0.833 Prec@1=78.060 Prec@5=93.624 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:29 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.833 Prec@1=78.066 Prec@5=93.618 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:29 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.833 Prec@1=78.066 Prec@5=93.618 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=19:29 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.833 Prec@1=78.066 Prec@5=93.618 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=19:30 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.834 Prec@1=78.051 Prec@5=93.602 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=19:30 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.834 Prec@1=78.051 Prec@5=93.602 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:30 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.393 Loss=0.834 Prec@1=78.051 Prec@5=93.602 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:31 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.835 Prec@1=78.028 Prec@5=93.584 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:31 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.835 Prec@1=78.028 Prec@5=93.584 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:31 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.835 Prec@1=78.028 Prec@5=93.584 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:32 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.017 Prec@5=93.579 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:32 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.017 Prec@5=93.579 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:32 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.017 Prec@5=93.579 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:33 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.004 Prec@5=93.569 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=19:33 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.004 Prec@5=93.569 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=19:33 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.836 Prec@1=78.004 Prec@5=93.569 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=19:34 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.837 Prec@1=77.997 Prec@5=93.572 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=19:34 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.837 Prec@1=77.997 Prec@5=93.572 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:34 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.617 DataTime=0.392 Loss=0.837 Prec@1=77.997 Prec@5=93.572 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:35 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.978 Prec@5=93.568 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=19:35 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.978 Prec@5=93.568 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:35 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.978 Prec@5=93.568 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:37 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.967 Prec@5=93.559 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=19:37 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.967 Prec@5=93.559 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:37 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.392 Loss=0.838 Prec@1=77.967 Prec@5=93.559 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.961 Prec@5=93.560 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=19:38 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.961 Prec@5=93.560 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:38 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.961 Prec@5=93.560 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:39 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.950 Prec@5=93.557 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=19:39 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.950 Prec@5=93.557 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:39 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.839 Prec@1=77.950 Prec@5=93.557 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:40 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.931 Prec@5=93.547 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=19:40 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.931 Prec@5=93.547 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:40 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.931 Prec@5=93.547 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:41 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.911 Prec@5=93.542 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=19:41 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.911 Prec@5=93.542 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:41 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.840 Prec@1=77.911 Prec@5=93.542 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:42 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.841 Prec@1=77.899 Prec@5=93.535 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=19:42 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.841 Prec@1=77.899 Prec@5=93.535 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:42 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.841 Prec@1=77.899 Prec@5=93.535 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:42 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.841 Prec@1=77.899 Prec@5=93.535 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=19:42 IST=> training   100.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.616 DataTime=0.391 Loss=0.841 Prec@1=77.899 Prec@5=93.535 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=19:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> validation 0.00% of 1x98...Epoch=121/150 LR=0.00955 Time=7.247 Loss=1.042 Prec@1=76.367 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=7.247 Loss=1.042 Prec@1=76.367 Prec@5=91.211 rate=2977.23 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=7.247 Loss=1.042 Prec@1=76.367 Prec@5=91.211 rate=2977.23 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=0.415 Loss=1.109 Prec@1=72.332 Prec@5=91.060 rate=2977.23 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST** validation 100.00% of 1x98...Epoch=121/150 LR=0.00955 Time=0.415 Loss=1.109 Prec@1=72.332 Prec@5=91.060 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=19:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> training   0.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.777 DataTime=5.510 Loss=0.677 Prec@1=82.031 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.777 DataTime=5.510 Loss=0.677 Prec@1=82.031 Prec@5=95.117 rate=9463.70 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.777 DataTime=5.510 Loss=0.677 Prec@1=82.031 Prec@5=95.117 rate=9463.70 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.662 DataTime=0.438 Loss=0.814 Prec@1=78.581 Prec@5=93.837 rate=9463.70 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.662 DataTime=0.438 Loss=0.814 Prec@1=78.581 Prec@5=93.837 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=19:44 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.662 DataTime=0.438 Loss=0.814 Prec@1=78.581 Prec@5=93.837 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=19:45 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.636 DataTime=0.413 Loss=0.810 Prec@1=78.712 Prec@5=93.844 rate=1.65 Hz, eta=0:24:12, total=0:01:01, wall=19:45 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.636 DataTime=0.413 Loss=0.810 Prec@1=78.712 Prec@5=93.844 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=19:45 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.636 DataTime=0.413 Loss=0.810 Prec@1=78.712 Prec@5=93.844 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=19:46 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.628 DataTime=0.404 Loss=0.811 Prec@1=78.680 Prec@5=93.891 rate=1.65 Hz, eta=0:23:19, total=0:02:02, wall=19:46 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.628 DataTime=0.404 Loss=0.811 Prec@1=78.680 Prec@5=93.891 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=19:46 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.628 DataTime=0.404 Loss=0.811 Prec@1=78.680 Prec@5=93.891 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=19:47 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.625 DataTime=0.401 Loss=0.812 Prec@1=78.644 Prec@5=93.889 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=19:47 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.625 DataTime=0.401 Loss=0.812 Prec@1=78.644 Prec@5=93.889 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=19:47 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.625 DataTime=0.401 Loss=0.812 Prec@1=78.644 Prec@5=93.889 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=19:48 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.623 DataTime=0.399 Loss=0.815 Prec@1=78.559 Prec@5=93.840 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=19:48 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.623 DataTime=0.399 Loss=0.815 Prec@1=78.559 Prec@5=93.840 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:48 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.623 DataTime=0.399 Loss=0.815 Prec@1=78.559 Prec@5=93.840 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:49 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.621 DataTime=0.397 Loss=0.816 Prec@1=78.516 Prec@5=93.822 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=19:49 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.621 DataTime=0.397 Loss=0.816 Prec@1=78.516 Prec@5=93.822 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=19:49 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.621 DataTime=0.397 Loss=0.816 Prec@1=78.516 Prec@5=93.822 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=19:50 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.396 Loss=0.817 Prec@1=78.452 Prec@5=93.796 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=19:50 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.396 Loss=0.817 Prec@1=78.452 Prec@5=93.796 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:50 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.396 Loss=0.817 Prec@1=78.452 Prec@5=93.796 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:51 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.395 Loss=0.819 Prec@1=78.432 Prec@5=93.797 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=19:51 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.395 Loss=0.819 Prec@1=78.432 Prec@5=93.797 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:51 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.620 DataTime=0.395 Loss=0.819 Prec@1=78.432 Prec@5=93.797 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:52 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.619 DataTime=0.394 Loss=0.818 Prec@1=78.419 Prec@5=93.792 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=19:52 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.619 DataTime=0.394 Loss=0.818 Prec@1=78.419 Prec@5=93.792 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=19:52 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.619 DataTime=0.394 Loss=0.818 Prec@1=78.419 Prec@5=93.792 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=19:53 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.819 Prec@1=78.414 Prec@5=93.773 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=19:53 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.819 Prec@1=78.414 Prec@5=93.773 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:53 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.819 Prec@1=78.414 Prec@5=93.773 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:54 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.820 Prec@1=78.386 Prec@5=93.757 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=19:54 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.820 Prec@1=78.386 Prec@5=93.757 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:54 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.394 Loss=0.820 Prec@1=78.386 Prec@5=93.757 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:55 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.393 Loss=0.820 Prec@1=78.360 Prec@5=93.766 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=19:55 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.393 Loss=0.820 Prec@1=78.360 Prec@5=93.766 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:55 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.618 DataTime=0.393 Loss=0.820 Prec@1=78.360 Prec@5=93.766 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:56 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.822 Prec@1=78.337 Prec@5=93.759 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=19:56 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.822 Prec@1=78.337 Prec@5=93.759 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:56 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.822 Prec@1=78.337 Prec@5=93.759 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:57 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.313 Prec@5=93.734 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=19:57 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.313 Prec@5=93.734 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:57 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.313 Prec@5=93.734 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:58 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.306 Prec@5=93.736 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=19:58 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.306 Prec@5=93.736 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:58 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.823 Prec@1=78.306 Prec@5=93.736 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:59 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.824 Prec@1=78.293 Prec@5=93.723 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=19:59 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.824 Prec@1=78.293 Prec@5=93.723 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=19:59 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.393 Loss=0.824 Prec@1=78.293 Prec@5=93.723 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:00 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.824 Prec@1=78.270 Prec@5=93.716 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:00 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.824 Prec@1=78.270 Prec@5=93.716 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:00 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.824 Prec@1=78.270 Prec@5=93.716 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:01 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.825 Prec@1=78.253 Prec@5=93.707 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:01 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.825 Prec@1=78.253 Prec@5=93.707 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:01 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.825 Prec@1=78.253 Prec@5=93.707 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:02 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.826 Prec@1=78.219 Prec@5=93.693 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:02 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.826 Prec@1=78.219 Prec@5=93.693 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:02 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.617 DataTime=0.392 Loss=0.826 Prec@1=78.219 Prec@5=93.693 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:03 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.208 Prec@5=93.687 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=20:03 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.208 Prec@5=93.687 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=20:03 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.208 Prec@5=93.687 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=20:04 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.192 Prec@5=93.681 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=20:04 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.192 Prec@5=93.681 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:04 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.827 Prec@1=78.192 Prec@5=93.681 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:05 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.193 Prec@5=93.675 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=20:05 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.193 Prec@5=93.675 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:05 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.193 Prec@5=93.675 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:06 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.187 Prec@5=93.673 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=20:06 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.187 Prec@5=93.673 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=20:06 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.187 Prec@5=93.673 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=20:07 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.164 Prec@5=93.672 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=20:07 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.164 Prec@5=93.672 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=20:07 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.828 Prec@1=78.164 Prec@5=93.672 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=20:08 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.829 Prec@1=78.147 Prec@5=93.664 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=20:08 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.829 Prec@1=78.147 Prec@5=93.664 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:08 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.829 Prec@1=78.147 Prec@5=93.664 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:08 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.829 Prec@1=78.147 Prec@5=93.664 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=20:08 IST=> training   100.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.616 DataTime=0.392 Loss=0.829 Prec@1=78.147 Prec@5=93.664 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=20:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:08 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:08 IST=> validation 0.00% of 1x98...Epoch=122/150 LR=0.00894 Time=6.282 Loss=1.325 Prec@1=67.969 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=20:08 IST=> validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=6.282 Loss=1.325 Prec@1=67.969 Prec@5=90.039 rate=3485.16 Hz, eta=0:00:00, total=0:00:00, wall=20:08 IST** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=6.282 Loss=1.325 Prec@1=67.969 Prec@5=90.039 rate=3485.16 Hz, eta=0:00:00, total=0:00:00, wall=20:09 IST** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=0.406 Loss=1.118 Prec@1=72.272 Prec@5=91.092 rate=3485.16 Hz, eta=0:00:00, total=0:00:00, wall=20:09 IST** validation 100.00% of 1x98...Epoch=122/150 LR=0.00894 Time=0.406 Loss=1.118 Prec@1=72.272 Prec@5=91.092 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=20:09 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:09 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:09 IST=> training   0.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.828 DataTime=5.559 Loss=0.725 Prec@1=81.445 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=20:09 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.828 DataTime=5.559 Loss=0.725 Prec@1=81.445 Prec@5=95.117 rate=7535.17 Hz, eta=0:00:00, total=0:00:00, wall=20:09 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.828 DataTime=5.559 Loss=0.725 Prec@1=81.445 Prec@5=95.117 rate=7535.17 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.664 DataTime=0.438 Loss=0.821 Prec@1=78.365 Prec@5=93.783 rate=7535.17 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.664 DataTime=0.438 Loss=0.821 Prec@1=78.365 Prec@5=93.783 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=20:10 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.664 DataTime=0.438 Loss=0.821 Prec@1=78.365 Prec@5=93.783 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=20:11 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.639 DataTime=0.414 Loss=0.811 Prec@1=78.611 Prec@5=93.885 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=20:11 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.639 DataTime=0.414 Loss=0.811 Prec@1=78.611 Prec@5=93.885 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=20:11 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.639 DataTime=0.414 Loss=0.811 Prec@1=78.611 Prec@5=93.885 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=20:12 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.630 DataTime=0.405 Loss=0.809 Prec@1=78.666 Prec@5=93.904 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=20:12 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.630 DataTime=0.405 Loss=0.809 Prec@1=78.666 Prec@5=93.904 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:12 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.630 DataTime=0.405 Loss=0.809 Prec@1=78.666 Prec@5=93.904 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.626 DataTime=0.401 Loss=0.810 Prec@1=78.646 Prec@5=93.902 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:13 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.626 DataTime=0.401 Loss=0.810 Prec@1=78.646 Prec@5=93.902 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:13 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.626 DataTime=0.401 Loss=0.810 Prec@1=78.646 Prec@5=93.902 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:14 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.623 DataTime=0.399 Loss=0.807 Prec@1=78.711 Prec@5=93.919 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:14 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.623 DataTime=0.399 Loss=0.807 Prec@1=78.711 Prec@5=93.919 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:14 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.623 DataTime=0.399 Loss=0.807 Prec@1=78.711 Prec@5=93.919 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:15 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.622 DataTime=0.397 Loss=0.809 Prec@1=78.684 Prec@5=93.900 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:15 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.622 DataTime=0.397 Loss=0.809 Prec@1=78.684 Prec@5=93.900 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:15 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.622 DataTime=0.397 Loss=0.809 Prec@1=78.684 Prec@5=93.900 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:16 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.621 DataTime=0.396 Loss=0.809 Prec@1=78.694 Prec@5=93.901 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:16 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.621 DataTime=0.396 Loss=0.809 Prec@1=78.694 Prec@5=93.901 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:16 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.621 DataTime=0.396 Loss=0.809 Prec@1=78.694 Prec@5=93.901 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:17 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.620 DataTime=0.395 Loss=0.809 Prec@1=78.696 Prec@5=93.899 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:17 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.620 DataTime=0.395 Loss=0.809 Prec@1=78.696 Prec@5=93.899 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:17 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.620 DataTime=0.395 Loss=0.809 Prec@1=78.696 Prec@5=93.899 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:18 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.395 Loss=0.809 Prec@1=78.677 Prec@5=93.885 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=20:18 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.395 Loss=0.809 Prec@1=78.677 Prec@5=93.885 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:18 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.395 Loss=0.809 Prec@1=78.677 Prec@5=93.885 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:19 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.394 Loss=0.810 Prec@1=78.663 Prec@5=93.865 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=20:19 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.394 Loss=0.810 Prec@1=78.663 Prec@5=93.865 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:19 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.619 DataTime=0.394 Loss=0.810 Prec@1=78.663 Prec@5=93.865 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:20 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.394 Loss=0.811 Prec@1=78.648 Prec@5=93.856 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=20:20 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.394 Loss=0.811 Prec@1=78.648 Prec@5=93.856 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:20 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.394 Loss=0.811 Prec@1=78.648 Prec@5=93.856 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:21 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.393 Loss=0.812 Prec@1=78.658 Prec@5=93.850 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:21 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.393 Loss=0.812 Prec@1=78.658 Prec@5=93.850 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:21 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.618 DataTime=0.393 Loss=0.812 Prec@1=78.658 Prec@5=93.850 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:22 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.813 Prec@1=78.610 Prec@5=93.836 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:22 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.813 Prec@1=78.610 Prec@5=93.836 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:22 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.813 Prec@1=78.610 Prec@5=93.836 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:23 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.814 Prec@1=78.577 Prec@5=93.828 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:23 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.814 Prec@1=78.577 Prec@5=93.828 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:23 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.814 Prec@1=78.577 Prec@5=93.828 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:24 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.562 Prec@5=93.820 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:24 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.562 Prec@5=93.820 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:24 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.562 Prec@5=93.820 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:25 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.544 Prec@5=93.811 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:25 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.544 Prec@5=93.811 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:25 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.393 Loss=0.815 Prec@1=78.544 Prec@5=93.811 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:26 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.392 Loss=0.816 Prec@1=78.516 Prec@5=93.805 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=20:26 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.392 Loss=0.816 Prec@1=78.516 Prec@5=93.805 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:26 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.617 DataTime=0.392 Loss=0.816 Prec@1=78.516 Prec@5=93.805 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:27 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.485 Prec@5=93.794 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=20:27 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.485 Prec@5=93.794 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:27 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.485 Prec@5=93.794 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:28 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.490 Prec@5=93.787 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:28 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.490 Prec@5=93.787 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:28 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.817 Prec@1=78.490 Prec@5=93.787 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:29 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.484 Prec@5=93.783 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:29 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.484 Prec@5=93.783 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:29 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.484 Prec@5=93.783 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:30 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.471 Prec@5=93.774 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=20:30 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.471 Prec@5=93.774 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:30 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.818 Prec@1=78.471 Prec@5=93.774 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:31 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.466 Prec@5=93.768 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:31 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.466 Prec@5=93.768 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:31 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.466 Prec@5=93.768 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:32 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.453 Prec@5=93.757 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:32 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.453 Prec@5=93.757 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:32 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.453 Prec@5=93.757 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:33 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.448 Prec@5=93.762 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=20:33 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.448 Prec@5=93.762 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:33 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.392 Loss=0.819 Prec@1=78.448 Prec@5=93.762 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:34 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.391 Loss=0.820 Prec@1=78.437 Prec@5=93.756 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=20:34 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.391 Loss=0.820 Prec@1=78.437 Prec@5=93.756 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:34 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.616 DataTime=0.391 Loss=0.820 Prec@1=78.437 Prec@5=93.756 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:34 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.615 DataTime=0.391 Loss=0.820 Prec@1=78.437 Prec@5=93.757 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=20:34 IST=> training   100.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.615 DataTime=0.391 Loss=0.820 Prec@1=78.437 Prec@5=93.757 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=20:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> validation 0.00% of 1x98...Epoch=123/150 LR=0.00835 Time=7.310 Loss=1.175 Prec@1=70.703 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=7.310 Loss=1.175 Prec@1=70.703 Prec@5=89.453 rate=3177.45 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=7.310 Loss=1.175 Prec@1=70.703 Prec@5=89.453 rate=3177.45 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=0.406 Loss=1.117 Prec@1=72.622 Prec@5=91.122 rate=3177.45 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST** validation 100.00% of 1x98...Epoch=123/150 LR=0.00835 Time=0.406 Loss=1.117 Prec@1=72.622 Prec@5=91.122 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=20:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> training   0.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=6.014 DataTime=5.722 Loss=0.786 Prec@1=79.297 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=6.014 DataTime=5.722 Loss=0.786 Prec@1=79.297 Prec@5=93.945 rate=4543.10 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=6.014 DataTime=5.722 Loss=0.786 Prec@1=79.297 Prec@5=93.945 rate=4543.10 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.662 DataTime=0.436 Loss=0.811 Prec@1=78.641 Prec@5=93.889 rate=4543.10 Hz, eta=0:00:00, total=0:00:00, wall=20:36 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.662 DataTime=0.436 Loss=0.811 Prec@1=78.641 Prec@5=93.889 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=20:36 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.662 DataTime=0.436 Loss=0.811 Prec@1=78.641 Prec@5=93.889 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=20:37 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.638 DataTime=0.413 Loss=0.809 Prec@1=78.571 Prec@5=93.877 rate=1.66 Hz, eta=0:24:07, total=0:01:00, wall=20:37 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.638 DataTime=0.413 Loss=0.809 Prec@1=78.571 Prec@5=93.877 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=20:37 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.638 DataTime=0.413 Loss=0.809 Prec@1=78.571 Prec@5=93.877 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=20:38 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.631 DataTime=0.405 Loss=0.806 Prec@1=78.668 Prec@5=93.906 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=20:38 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.631 DataTime=0.405 Loss=0.806 Prec@1=78.668 Prec@5=93.906 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:38 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.631 DataTime=0.405 Loss=0.806 Prec@1=78.668 Prec@5=93.906 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:39 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.626 DataTime=0.401 Loss=0.804 Prec@1=78.739 Prec@5=93.894 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=20:39 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.626 DataTime=0.401 Loss=0.804 Prec@1=78.739 Prec@5=93.894 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:39 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.626 DataTime=0.401 Loss=0.804 Prec@1=78.739 Prec@5=93.894 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:40 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.624 DataTime=0.399 Loss=0.804 Prec@1=78.737 Prec@5=93.912 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=20:40 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.624 DataTime=0.399 Loss=0.804 Prec@1=78.737 Prec@5=93.912 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:40 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.624 DataTime=0.399 Loss=0.804 Prec@1=78.737 Prec@5=93.912 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:41 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.622 DataTime=0.397 Loss=0.804 Prec@1=78.763 Prec@5=93.901 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=20:41 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.622 DataTime=0.397 Loss=0.804 Prec@1=78.763 Prec@5=93.901 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:41 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.622 DataTime=0.397 Loss=0.804 Prec@1=78.763 Prec@5=93.901 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:42 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.621 DataTime=0.396 Loss=0.803 Prec@1=78.767 Prec@5=93.898 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=20:42 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.621 DataTime=0.396 Loss=0.803 Prec@1=78.767 Prec@5=93.898 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:42 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.621 DataTime=0.396 Loss=0.803 Prec@1=78.767 Prec@5=93.898 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:43 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.620 DataTime=0.395 Loss=0.802 Prec@1=78.810 Prec@5=93.929 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=20:43 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.620 DataTime=0.395 Loss=0.802 Prec@1=78.810 Prec@5=93.929 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:43 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.620 DataTime=0.395 Loss=0.802 Prec@1=78.810 Prec@5=93.929 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:45 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.619 DataTime=0.394 Loss=0.802 Prec@1=78.805 Prec@5=93.913 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=20:45 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.619 DataTime=0.394 Loss=0.802 Prec@1=78.805 Prec@5=93.913 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:45 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.619 DataTime=0.394 Loss=0.802 Prec@1=78.805 Prec@5=93.913 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.816 Prec@5=93.906 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=20:46 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.816 Prec@5=93.906 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=20:46 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.816 Prec@5=93.906 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=20:47 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.824 Prec@5=93.907 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=20:47 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.824 Prec@5=93.907 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:47 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.394 Loss=0.802 Prec@1=78.824 Prec@5=93.907 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:48 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.825 Prec@5=93.916 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=20:48 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.825 Prec@5=93.916 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:48 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.825 Prec@5=93.916 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:49 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.836 Prec@5=93.918 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=20:49 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.836 Prec@5=93.918 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:49 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.618 DataTime=0.393 Loss=0.802 Prec@1=78.836 Prec@5=93.918 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:50 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.393 Loss=0.803 Prec@1=78.819 Prec@5=93.909 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=20:50 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.393 Loss=0.803 Prec@1=78.819 Prec@5=93.909 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:50 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.393 Loss=0.803 Prec@1=78.819 Prec@5=93.909 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:51 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.812 Prec@5=93.900 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=20:51 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.812 Prec@5=93.900 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:51 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.812 Prec@5=93.900 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:52 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.800 Prec@5=93.896 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=20:52 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.800 Prec@5=93.896 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:52 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.800 Prec@5=93.896 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:53 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.786 Prec@5=93.898 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=20:53 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.786 Prec@5=93.898 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:53 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.617 DataTime=0.392 Loss=0.804 Prec@1=78.786 Prec@5=93.898 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:54 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.804 Prec@1=78.782 Prec@5=93.900 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=20:54 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.804 Prec@1=78.782 Prec@5=93.900 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:54 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.804 Prec@1=78.782 Prec@5=93.900 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:55 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.805 Prec@1=78.775 Prec@5=93.895 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=20:55 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.805 Prec@1=78.775 Prec@5=93.895 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:55 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.392 Loss=0.805 Prec@1=78.775 Prec@5=93.895 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:56 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.759 Prec@5=93.897 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=20:56 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.759 Prec@5=93.897 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:56 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.759 Prec@5=93.897 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:57 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.740 Prec@5=93.892 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=20:57 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.740 Prec@5=93.892 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:57 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.805 Prec@1=78.740 Prec@5=93.892 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:58 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.883 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=20:58 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.883 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:58 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.883 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:59 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.882 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=20:59 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.882 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=20:59 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.807 Prec@1=78.720 Prec@5=93.882 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=21:00 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.694 Prec@5=93.871 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=21:00 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.694 Prec@5=93.871 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:00 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.694 Prec@5=93.871 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:01 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.678 Prec@5=93.869 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:01 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.678 Prec@5=93.869 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:01 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.678 Prec@5=93.869 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:01 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.678 Prec@5=93.869 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:01 IST=> training   100.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.616 DataTime=0.391 Loss=0.808 Prec@1=78.678 Prec@5=93.869 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=21:01 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST=> validation 0.00% of 1x98...Epoch=124/150 LR=0.00778 Time=7.516 Loss=1.197 Prec@1=73.047 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=21:01 IST=> validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=7.516 Loss=1.197 Prec@1=73.047 Prec@5=88.672 rate=4668.42 Hz, eta=0:00:00, total=0:00:00, wall=21:01 IST** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=7.516 Loss=1.197 Prec@1=73.047 Prec@5=88.672 rate=4668.42 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=0.412 Loss=1.103 Prec@1=72.776 Prec@5=91.150 rate=4668.42 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST** validation 100.00% of 1x98...Epoch=124/150 LR=0.00778 Time=0.412 Loss=1.103 Prec@1=72.776 Prec@5=91.150 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=21:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> training   0.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.209 DataTime=4.780 Loss=0.745 Prec@1=82.031 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=21:02 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.209 DataTime=4.780 Loss=0.745 Prec@1=82.031 Prec@5=94.922 rate=7068.54 Hz, eta=0:00:00, total=0:00:00, wall=21:02 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.209 DataTime=4.780 Loss=0.745 Prec@1=82.031 Prec@5=94.922 rate=7068.54 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.657 DataTime=0.430 Loss=0.786 Prec@1=79.368 Prec@5=94.114 rate=7068.54 Hz, eta=0:00:00, total=0:00:00, wall=21:03 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.657 DataTime=0.430 Loss=0.786 Prec@1=79.368 Prec@5=94.114 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=21:03 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.657 DataTime=0.430 Loss=0.786 Prec@1=79.368 Prec@5=94.114 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=21:04 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.634 DataTime=0.408 Loss=0.788 Prec@1=79.217 Prec@5=94.061 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=21:04 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.634 DataTime=0.408 Loss=0.788 Prec@1=79.217 Prec@5=94.061 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:04 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.634 DataTime=0.408 Loss=0.788 Prec@1=79.217 Prec@5=94.061 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:05 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.627 DataTime=0.402 Loss=0.786 Prec@1=79.244 Prec@5=94.098 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:05 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.627 DataTime=0.402 Loss=0.786 Prec@1=79.244 Prec@5=94.098 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:05 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.627 DataTime=0.402 Loss=0.786 Prec@1=79.244 Prec@5=94.098 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:06 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.624 DataTime=0.398 Loss=0.784 Prec@1=79.280 Prec@5=94.138 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=21:06 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.624 DataTime=0.398 Loss=0.784 Prec@1=79.280 Prec@5=94.138 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=21:06 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.624 DataTime=0.398 Loss=0.784 Prec@1=79.280 Prec@5=94.138 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=21:07 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.621 DataTime=0.396 Loss=0.784 Prec@1=79.276 Prec@5=94.141 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=21:07 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.621 DataTime=0.396 Loss=0.784 Prec@1=79.276 Prec@5=94.141 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:07 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.621 DataTime=0.396 Loss=0.784 Prec@1=79.276 Prec@5=94.141 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:08 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.620 DataTime=0.395 Loss=0.786 Prec@1=79.246 Prec@5=94.120 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=21:08 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.620 DataTime=0.395 Loss=0.786 Prec@1=79.246 Prec@5=94.120 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=21:08 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.620 DataTime=0.395 Loss=0.786 Prec@1=79.246 Prec@5=94.120 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=21:09 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.619 DataTime=0.395 Loss=0.786 Prec@1=79.227 Prec@5=94.105 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=21:09 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.619 DataTime=0.395 Loss=0.786 Prec@1=79.227 Prec@5=94.105 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=21:09 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.619 DataTime=0.395 Loss=0.786 Prec@1=79.227 Prec@5=94.105 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=21:10 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.394 Loss=0.789 Prec@1=79.136 Prec@5=94.076 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=21:10 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.394 Loss=0.789 Prec@1=79.136 Prec@5=94.076 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:10 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.394 Loss=0.789 Prec@1=79.136 Prec@5=94.076 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:11 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.393 Loss=0.790 Prec@1=79.116 Prec@5=94.068 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:11 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.393 Loss=0.790 Prec@1=79.116 Prec@5=94.068 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:11 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.618 DataTime=0.393 Loss=0.790 Prec@1=79.116 Prec@5=94.068 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:12 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.791 Prec@1=79.128 Prec@5=94.072 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:12 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.791 Prec@1=79.128 Prec@5=94.072 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=21:12 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.791 Prec@1=79.128 Prec@5=94.072 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=21:13 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.090 Prec@5=94.052 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=21:13 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.090 Prec@5=94.052 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:13 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.090 Prec@5=94.052 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:14 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.056 Prec@5=94.045 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:14 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.056 Prec@5=94.045 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:14 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.617 DataTime=0.393 Loss=0.793 Prec@1=79.056 Prec@5=94.045 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:15 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.049 Prec@5=94.046 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:15 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.049 Prec@5=94.046 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:15 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.049 Prec@5=94.046 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:16 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.036 Prec@5=94.049 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:16 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.036 Prec@5=94.049 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:16 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.793 Prec@1=79.036 Prec@5=94.049 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:17 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=79.020 Prec@5=94.039 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:17 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=79.020 Prec@5=94.039 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:17 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=79.020 Prec@5=94.039 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:18 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.994 Prec@5=94.038 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:18 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.994 Prec@5=94.038 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=21:18 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.994 Prec@5=94.038 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=21:19 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.991 Prec@5=94.038 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=21:19 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.991 Prec@5=94.038 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:19 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.794 Prec@1=78.991 Prec@5=94.038 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.795 Prec@1=78.978 Prec@5=94.024 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:20 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.795 Prec@1=78.978 Prec@5=94.024 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=21:20 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.795 Prec@1=78.978 Prec@5=94.024 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=21:21 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.970 Prec@5=94.014 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=21:21 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.970 Prec@5=94.014 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:21 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.970 Prec@5=94.014 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:22 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.954 Prec@5=94.011 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:22 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.954 Prec@5=94.011 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:22 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.616 DataTime=0.392 Loss=0.796 Prec@1=78.954 Prec@5=94.011 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:23 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.797 Prec@1=78.935 Prec@5=94.001 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:23 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.797 Prec@1=78.935 Prec@5=94.001 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=21:23 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.797 Prec@1=78.935 Prec@5=94.001 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=21:24 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.911 Prec@5=93.995 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=21:24 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.911 Prec@5=93.995 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:24 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.911 Prec@5=93.995 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:25 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.901 Prec@5=93.986 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:25 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.901 Prec@5=93.986 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=21:25 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.798 Prec@1=78.901 Prec@5=93.986 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=21:26 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.799 Prec@1=78.893 Prec@5=93.973 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=21:26 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.799 Prec@1=78.893 Prec@5=93.973 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=21:26 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.799 Prec@1=78.893 Prec@5=93.973 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=21:27 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.800 Prec@1=78.880 Prec@5=93.970 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=21:27 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.800 Prec@1=78.880 Prec@5=93.970 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:27 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.800 Prec@1=78.880 Prec@5=93.970 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:27 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.800 Prec@1=78.880 Prec@5=93.970 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:27 IST=> training   100.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.615 DataTime=0.391 Loss=0.800 Prec@1=78.880 Prec@5=93.970 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=21:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:27 IST=> validation 0.00% of 1x98...Epoch=125/150 LR=0.00723 Time=6.655 Loss=1.202 Prec@1=70.508 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=21:27 IST=> validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.655 Loss=1.202 Prec@1=70.508 Prec@5=91.016 rate=4246.09 Hz, eta=0:00:00, total=0:00:00, wall=21:27 IST** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.655 Loss=1.202 Prec@1=70.508 Prec@5=91.016 rate=4246.09 Hz, eta=0:00:00, total=0:00:00, wall=21:28 IST** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=0.406 Loss=1.114 Prec@1=72.596 Prec@5=91.174 rate=4246.09 Hz, eta=0:00:00, total=0:00:00, wall=21:28 IST** validation 100.00% of 1x98...Epoch=125/150 LR=0.00723 Time=0.406 Loss=1.114 Prec@1=72.596 Prec@5=91.174 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=21:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:28 IST=> training   0.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.204 DataTime=5.967 Loss=0.748 Prec@1=80.859 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=21:28 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.204 DataTime=5.967 Loss=0.748 Prec@1=80.859 Prec@5=94.531 rate=7930.65 Hz, eta=0:00:00, total=0:00:00, wall=21:28 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.204 DataTime=5.967 Loss=0.748 Prec@1=80.859 Prec@5=94.531 rate=7930.65 Hz, eta=0:00:00, total=0:00:00, wall=21:29 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.668 DataTime=0.443 Loss=0.778 Prec@1=79.382 Prec@5=94.175 rate=7930.65 Hz, eta=0:00:00, total=0:00:00, wall=21:29 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.668 DataTime=0.443 Loss=0.778 Prec@1=79.382 Prec@5=94.175 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=21:29 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.668 DataTime=0.443 Loss=0.778 Prec@1=79.382 Prec@5=94.175 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=21:30 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.642 DataTime=0.417 Loss=0.779 Prec@1=79.400 Prec@5=94.118 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=21:30 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.642 DataTime=0.417 Loss=0.779 Prec@1=79.400 Prec@5=94.118 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=21:30 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.642 DataTime=0.417 Loss=0.779 Prec@1=79.400 Prec@5=94.118 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=21:31 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.631 DataTime=0.407 Loss=0.778 Prec@1=79.434 Prec@5=94.172 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=21:31 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.631 DataTime=0.407 Loss=0.778 Prec@1=79.434 Prec@5=94.172 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:31 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.631 DataTime=0.407 Loss=0.778 Prec@1=79.434 Prec@5=94.172 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:32 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.627 DataTime=0.402 Loss=0.778 Prec@1=79.380 Prec@5=94.186 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=21:32 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.627 DataTime=0.402 Loss=0.778 Prec@1=79.380 Prec@5=94.186 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:32 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.627 DataTime=0.402 Loss=0.778 Prec@1=79.380 Prec@5=94.186 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:33 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.624 DataTime=0.400 Loss=0.779 Prec@1=79.390 Prec@5=94.177 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=21:33 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.624 DataTime=0.400 Loss=0.779 Prec@1=79.390 Prec@5=94.177 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=21:33 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.624 DataTime=0.400 Loss=0.779 Prec@1=79.390 Prec@5=94.177 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=21:34 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.622 DataTime=0.398 Loss=0.780 Prec@1=79.356 Prec@5=94.157 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=21:34 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.622 DataTime=0.398 Loss=0.780 Prec@1=79.356 Prec@5=94.157 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:34 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.622 DataTime=0.398 Loss=0.780 Prec@1=79.356 Prec@5=94.157 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:35 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.621 DataTime=0.397 Loss=0.780 Prec@1=79.353 Prec@5=94.148 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=21:35 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.621 DataTime=0.397 Loss=0.780 Prec@1=79.353 Prec@5=94.148 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=21:35 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.621 DataTime=0.397 Loss=0.780 Prec@1=79.353 Prec@5=94.148 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=21:36 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.620 DataTime=0.396 Loss=0.781 Prec@1=79.344 Prec@5=94.137 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=21:36 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.620 DataTime=0.396 Loss=0.781 Prec@1=79.344 Prec@5=94.137 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:36 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.620 DataTime=0.396 Loss=0.781 Prec@1=79.344 Prec@5=94.137 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:37 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.349 Prec@5=94.127 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=21:37 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.349 Prec@5=94.127 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:37 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.349 Prec@5=94.127 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:38 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.363 Prec@5=94.124 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=21:38 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.363 Prec@5=94.124 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=21:38 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.619 DataTime=0.395 Loss=0.781 Prec@1=79.363 Prec@5=94.124 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=21:39 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.781 Prec@1=79.365 Prec@5=94.129 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=21:39 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.781 Prec@1=79.365 Prec@5=94.129 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:39 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.781 Prec@1=79.365 Prec@5=94.129 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:40 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.782 Prec@1=79.341 Prec@5=94.120 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=21:40 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.782 Prec@1=79.341 Prec@5=94.120 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:40 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.618 DataTime=0.394 Loss=0.782 Prec@1=79.341 Prec@5=94.120 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:41 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.783 Prec@1=79.331 Prec@5=94.117 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=21:41 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.783 Prec@1=79.331 Prec@5=94.117 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:41 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.783 Prec@1=79.331 Prec@5=94.117 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:42 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.784 Prec@1=79.300 Prec@5=94.105 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=21:42 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.784 Prec@1=79.300 Prec@5=94.105 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:42 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.784 Prec@1=79.300 Prec@5=94.105 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:43 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.785 Prec@1=79.278 Prec@5=94.090 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=21:43 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.785 Prec@1=79.278 Prec@5=94.090 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:43 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.785 Prec@1=79.278 Prec@5=94.090 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:44 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.268 Prec@5=94.079 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=21:44 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.268 Prec@5=94.079 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=21:44 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.268 Prec@5=94.079 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=21:45 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.269 Prec@5=94.078 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=21:45 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.269 Prec@5=94.078 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:45 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.269 Prec@5=94.078 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:47 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.257 Prec@5=94.070 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=21:47 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.257 Prec@5=94.070 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:47 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.617 DataTime=0.393 Loss=0.786 Prec@1=79.257 Prec@5=94.070 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:48 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.787 Prec@1=79.240 Prec@5=94.065 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=21:48 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.787 Prec@1=79.240 Prec@5=94.065 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:48 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.787 Prec@1=79.240 Prec@5=94.065 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:49 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.220 Prec@5=94.063 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=21:49 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.220 Prec@5=94.063 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:49 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.220 Prec@5=94.063 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:50 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.197 Prec@5=94.053 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=21:50 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.197 Prec@5=94.053 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:50 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.788 Prec@1=79.197 Prec@5=94.053 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:51 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.190 Prec@5=94.055 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=21:51 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.190 Prec@5=94.055 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:51 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.190 Prec@5=94.055 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.177 Prec@5=94.055 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=21:52 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.177 Prec@5=94.055 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:52 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.177 Prec@5=94.055 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:53 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.179 Prec@5=94.051 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=21:53 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.179 Prec@5=94.051 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:53 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.179 Prec@5=94.051 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:54 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.172 Prec@5=94.048 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.172 Prec@5=94.048 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.172 Prec@5=94.048 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.170 Prec@5=94.047 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=21:54 IST=> training   100.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.616 DataTime=0.392 Loss=0.789 Prec@1=79.170 Prec@5=94.047 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=21:54 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 0.00% of 1x98...Epoch=126/150 LR=0.00670 Time=7.331 Loss=1.120 Prec@1=71.680 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=7.331 Loss=1.120 Prec@1=71.680 Prec@5=91.406 rate=7285.50 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=7.331 Loss=1.120 Prec@1=71.680 Prec@5=91.406 rate=7285.50 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=0.404 Loss=1.089 Prec@1=72.982 Prec@5=91.370 rate=7285.50 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 100.00% of 1x98...Epoch=126/150 LR=0.00670 Time=0.404 Loss=1.089 Prec@1=72.982 Prec@5=91.370 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=21:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> training   0.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=5.131 DataTime=4.826 Loss=0.708 Prec@1=81.836 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=21:55 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=5.131 DataTime=4.826 Loss=0.708 Prec@1=81.836 Prec@5=95.312 rate=7093.26 Hz, eta=0:00:00, total=0:00:00, wall=21:55 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=5.131 DataTime=4.826 Loss=0.708 Prec@1=81.836 Prec@5=95.312 rate=7093.26 Hz, eta=0:00:00, total=0:00:00, wall=21:56 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.652 DataTime=0.428 Loss=0.761 Prec@1=80.154 Prec@5=94.361 rate=7093.26 Hz, eta=0:00:00, total=0:00:00, wall=21:56 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.652 DataTime=0.428 Loss=0.761 Prec@1=80.154 Prec@5=94.361 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=21:56 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.652 DataTime=0.428 Loss=0.761 Prec@1=80.154 Prec@5=94.361 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=21:57 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.634 DataTime=0.409 Loss=0.766 Prec@1=79.872 Prec@5=94.266 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=21:57 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.634 DataTime=0.409 Loss=0.766 Prec@1=79.872 Prec@5=94.266 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:57 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.634 DataTime=0.409 Loss=0.766 Prec@1=79.872 Prec@5=94.266 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:58 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.626 DataTime=0.402 Loss=0.766 Prec@1=79.887 Prec@5=94.289 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=21:58 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.626 DataTime=0.402 Loss=0.766 Prec@1=79.887 Prec@5=94.289 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=21:58 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.626 DataTime=0.402 Loss=0.766 Prec@1=79.887 Prec@5=94.289 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=21:59 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.623 DataTime=0.399 Loss=0.766 Prec@1=79.881 Prec@5=94.284 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=21:59 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.623 DataTime=0.399 Loss=0.766 Prec@1=79.881 Prec@5=94.284 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=21:59 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.623 DataTime=0.399 Loss=0.766 Prec@1=79.881 Prec@5=94.284 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=22:00 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.621 DataTime=0.397 Loss=0.768 Prec@1=79.840 Prec@5=94.269 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=22:00 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.621 DataTime=0.397 Loss=0.768 Prec@1=79.840 Prec@5=94.269 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:00 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.621 DataTime=0.397 Loss=0.768 Prec@1=79.840 Prec@5=94.269 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:01 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.620 DataTime=0.396 Loss=0.770 Prec@1=79.761 Prec@5=94.240 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:01 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.620 DataTime=0.396 Loss=0.770 Prec@1=79.761 Prec@5=94.240 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:01 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.620 DataTime=0.396 Loss=0.770 Prec@1=79.761 Prec@5=94.240 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:02 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.395 Loss=0.770 Prec@1=79.736 Prec@5=94.248 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=22:02 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.395 Loss=0.770 Prec@1=79.736 Prec@5=94.248 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:02 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.395 Loss=0.770 Prec@1=79.736 Prec@5=94.248 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:03 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.394 Loss=0.771 Prec@1=79.673 Prec@5=94.251 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=22:03 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.394 Loss=0.771 Prec@1=79.673 Prec@5=94.251 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:03 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.619 DataTime=0.394 Loss=0.771 Prec@1=79.673 Prec@5=94.251 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:04 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.618 DataTime=0.394 Loss=0.772 Prec@1=79.642 Prec@5=94.243 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=22:04 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.618 DataTime=0.394 Loss=0.772 Prec@1=79.642 Prec@5=94.243 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:04 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.618 DataTime=0.394 Loss=0.772 Prec@1=79.642 Prec@5=94.243 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:05 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.626 Prec@5=94.239 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:05 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.626 Prec@5=94.239 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:05 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.626 Prec@5=94.239 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:06 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.628 Prec@5=94.232 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=22:06 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.628 Prec@5=94.232 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:06 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.772 Prec@1=79.628 Prec@5=94.232 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:07 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.598 Prec@5=94.228 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:07 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.598 Prec@5=94.228 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:07 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.598 Prec@5=94.228 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:08 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.583 Prec@5=94.221 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=22:08 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.583 Prec@5=94.221 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:08 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.617 DataTime=0.393 Loss=0.774 Prec@1=79.583 Prec@5=94.221 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:09 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.575 Prec@5=94.218 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=22:09 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.575 Prec@5=94.218 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:09 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.575 Prec@5=94.218 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:10 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.569 Prec@5=94.215 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=22:10 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.569 Prec@5=94.215 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=22:10 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.775 Prec@1=79.569 Prec@5=94.215 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=22:11 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.776 Prec@1=79.555 Prec@5=94.203 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=22:11 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.776 Prec@1=79.555 Prec@5=94.203 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:11 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.776 Prec@1=79.555 Prec@5=94.203 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:12 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.777 Prec@1=79.532 Prec@5=94.192 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=22:12 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.777 Prec@1=79.532 Prec@5=94.192 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:12 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.777 Prec@1=79.532 Prec@5=94.192 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:13 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.478 Prec@5=94.182 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=22:13 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.478 Prec@5=94.182 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=22:13 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.478 Prec@5=94.182 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=22:14 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.465 Prec@5=94.185 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=22:14 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.465 Prec@5=94.185 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=22:14 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.778 Prec@1=79.465 Prec@5=94.185 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=22:15 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.779 Prec@1=79.437 Prec@5=94.169 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=22:15 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.779 Prec@1=79.437 Prec@5=94.169 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=22:15 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.616 DataTime=0.392 Loss=0.779 Prec@1=79.437 Prec@5=94.169 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=22:16 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.417 Prec@5=94.158 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=22:16 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.417 Prec@5=94.158 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=22:16 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.417 Prec@5=94.158 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=22:17 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.399 Prec@5=94.155 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=22:17 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.399 Prec@5=94.155 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:17 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.780 Prec@1=79.399 Prec@5=94.155 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:18 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.373 Prec@5=94.143 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=22:18 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.373 Prec@5=94.143 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=22:18 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.373 Prec@5=94.143 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=22:19 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.369 Prec@5=94.138 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=22:19 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.369 Prec@5=94.138 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=22:19 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.392 Loss=0.781 Prec@1=79.369 Prec@5=94.138 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=22:20 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.391 Loss=0.781 Prec@1=79.380 Prec@5=94.150 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=22:20 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.391 Loss=0.781 Prec@1=79.380 Prec@5=94.150 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=22:20 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.391 Loss=0.781 Prec@1=79.380 Prec@5=94.150 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=22:20 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.391 Loss=0.781 Prec@1=79.379 Prec@5=94.150 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=22:20 IST=> training   100.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.615 DataTime=0.391 Loss=0.781 Prec@1=79.379 Prec@5=94.150 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=22:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:20 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:20 IST=> validation 0.00% of 1x98...Epoch=127/150 LR=0.00618 Time=6.625 Loss=1.161 Prec@1=72.461 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=22:20 IST=> validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.625 Loss=1.161 Prec@1=72.461 Prec@5=91.211 rate=5119.73 Hz, eta=0:00:00, total=0:00:00, wall=22:20 IST** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.625 Loss=1.161 Prec@1=72.461 Prec@5=91.211 rate=5119.73 Hz, eta=0:00:00, total=0:00:00, wall=22:21 IST** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=0.403 Loss=1.088 Prec@1=73.162 Prec@5=91.450 rate=5119.73 Hz, eta=0:00:00, total=0:00:00, wall=22:21 IST** validation 100.00% of 1x98...Epoch=127/150 LR=0.00618 Time=0.403 Loss=1.088 Prec@1=73.162 Prec@5=91.450 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=22:21 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:21 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:21 IST=> training   0.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.258 DataTime=4.953 Loss=0.838 Prec@1=78.320 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=22:21 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.258 DataTime=4.953 Loss=0.838 Prec@1=78.320 Prec@5=93.164 rate=4494.22 Hz, eta=0:00:00, total=0:00:00, wall=22:21 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.258 DataTime=4.953 Loss=0.838 Prec@1=78.320 Prec@5=93.164 rate=4494.22 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.655 DataTime=0.432 Loss=0.758 Prec@1=79.898 Prec@5=94.388 rate=4494.22 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.655 DataTime=0.432 Loss=0.758 Prec@1=79.898 Prec@5=94.388 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:22 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.655 DataTime=0.432 Loss=0.758 Prec@1=79.898 Prec@5=94.388 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:23 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.633 DataTime=0.411 Loss=0.756 Prec@1=79.968 Prec@5=94.417 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:23 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.633 DataTime=0.411 Loss=0.756 Prec@1=79.968 Prec@5=94.417 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=22:23 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.633 DataTime=0.411 Loss=0.756 Prec@1=79.968 Prec@5=94.417 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=22:24 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.626 DataTime=0.403 Loss=0.756 Prec@1=80.002 Prec@5=94.405 rate=1.65 Hz, eta=0:23:18, total=0:02:02, wall=22:24 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.626 DataTime=0.403 Loss=0.756 Prec@1=80.002 Prec@5=94.405 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=22:24 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.626 DataTime=0.403 Loss=0.756 Prec@1=80.002 Prec@5=94.405 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=22:25 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.623 DataTime=0.399 Loss=0.755 Prec@1=80.046 Prec@5=94.401 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=22:25 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.623 DataTime=0.399 Loss=0.755 Prec@1=80.046 Prec@5=94.401 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=22:25 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.623 DataTime=0.399 Loss=0.755 Prec@1=80.046 Prec@5=94.401 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=22:26 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.621 DataTime=0.398 Loss=0.757 Prec@1=80.001 Prec@5=94.412 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=22:26 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.621 DataTime=0.398 Loss=0.757 Prec@1=80.001 Prec@5=94.412 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:26 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.621 DataTime=0.398 Loss=0.757 Prec@1=80.001 Prec@5=94.412 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.620 DataTime=0.396 Loss=0.759 Prec@1=79.939 Prec@5=94.370 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=22:27 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.620 DataTime=0.396 Loss=0.759 Prec@1=79.939 Prec@5=94.370 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=22:27 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.620 DataTime=0.396 Loss=0.759 Prec@1=79.939 Prec@5=94.370 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=22:28 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.619 DataTime=0.395 Loss=0.760 Prec@1=79.890 Prec@5=94.359 rate=1.64 Hz, eta=0:19:21, total=0:06:07, wall=22:28 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.619 DataTime=0.395 Loss=0.760 Prec@1=79.890 Prec@5=94.359 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:28 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.619 DataTime=0.395 Loss=0.760 Prec@1=79.890 Prec@5=94.359 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:29 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.395 Loss=0.760 Prec@1=79.893 Prec@5=94.372 rate=1.64 Hz, eta=0:18:21, total=0:07:08, wall=22:29 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.395 Loss=0.760 Prec@1=79.893 Prec@5=94.372 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:29 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.395 Loss=0.760 Prec@1=79.893 Prec@5=94.372 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:30 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.394 Loss=0.760 Prec@1=79.895 Prec@5=94.370 rate=1.64 Hz, eta=0:17:20, total=0:08:09, wall=22:30 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.394 Loss=0.760 Prec@1=79.895 Prec@5=94.370 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:30 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.618 DataTime=0.394 Loss=0.760 Prec@1=79.895 Prec@5=94.370 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:31 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.394 Loss=0.761 Prec@1=79.849 Prec@5=94.365 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=22:31 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.394 Loss=0.761 Prec@1=79.849 Prec@5=94.365 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=22:31 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.394 Loss=0.761 Prec@1=79.849 Prec@5=94.365 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=22:32 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.393 Loss=0.762 Prec@1=79.819 Prec@5=94.369 rate=1.63 Hz, eta=0:15:18, total=0:10:12, wall=22:32 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.393 Loss=0.762 Prec@1=79.819 Prec@5=94.369 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=22:32 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.617 DataTime=0.393 Loss=0.762 Prec@1=79.819 Prec@5=94.369 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=22:33 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.816 Prec@5=94.360 rate=1.63 Hz, eta=0:14:17, total=0:11:13, wall=22:33 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.816 Prec@5=94.360 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=22:33 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.816 Prec@5=94.360 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=22:34 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.828 Prec@5=94.353 rate=1.63 Hz, eta=0:13:16, total=0:12:15, wall=22:34 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.828 Prec@5=94.353 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=22:34 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.393 Loss=0.762 Prec@1=79.828 Prec@5=94.353 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=22:35 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.763 Prec@1=79.812 Prec@5=94.340 rate=1.63 Hz, eta=0:12:15, total=0:13:16, wall=22:35 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.763 Prec@1=79.812 Prec@5=94.340 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=22:35 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.763 Prec@1=79.812 Prec@5=94.340 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=22:36 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.764 Prec@1=79.795 Prec@5=94.346 rate=1.63 Hz, eta=0:11:14, total=0:14:17, wall=22:36 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.764 Prec@1=79.795 Prec@5=94.346 rate=1.63 Hz, eta=0:10:13, total=0:15:18, wall=22:36 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.764 Prec@1=79.795 Prec@5=94.346 rate=1.63 Hz, eta=0:10:13, total=0:15:18, wall=22:37 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.765 Prec@1=79.763 Prec@5=94.331 rate=1.63 Hz, eta=0:10:13, total=0:15:18, wall=22:37 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.765 Prec@1=79.763 Prec@5=94.331 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=22:37 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.616 DataTime=0.392 Loss=0.765 Prec@1=79.763 Prec@5=94.331 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=22:38 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.746 Prec@5=94.317 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=22:38 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.746 Prec@5=94.317 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=22:38 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.746 Prec@5=94.317 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=22:39 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.730 Prec@5=94.307 rate=1.63 Hz, eta=0:08:11, total=0:17:21, wall=22:39 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.730 Prec@5=94.307 rate=1.63 Hz, eta=0:07:09, total=0:18:22, wall=22:39 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.392 Loss=0.766 Prec@1=79.730 Prec@5=94.307 rate=1.63 Hz, eta=0:07:09, total=0:18:22, wall=22:40 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.708 Prec@5=94.307 rate=1.63 Hz, eta=0:07:09, total=0:18:22, wall=22:40 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.708 Prec@5=94.307 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=22:40 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.708 Prec@5=94.307 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=22:41 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.680 Prec@5=94.297 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=22:41 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.680 Prec@5=94.297 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=22:41 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.767 Prec@1=79.680 Prec@5=94.297 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=22:42 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.665 Prec@5=94.295 rate=1.63 Hz, eta=0:05:07, total=0:20:25, wall=22:42 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.665 Prec@5=94.295 rate=1.63 Hz, eta=0:04:06, total=0:21:26, wall=22:42 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.665 Prec@5=94.295 rate=1.63 Hz, eta=0:04:06, total=0:21:26, wall=22:43 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.654 Prec@5=94.292 rate=1.63 Hz, eta=0:04:06, total=0:21:26, wall=22:43 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.654 Prec@5=94.292 rate=1.63 Hz, eta=0:03:04, total=0:22:28, wall=22:43 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.654 Prec@5=94.292 rate=1.63 Hz, eta=0:03:04, total=0:22:28, wall=22:44 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.650 Prec@5=94.297 rate=1.63 Hz, eta=0:03:04, total=0:22:28, wall=22:44 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.650 Prec@5=94.297 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=22:44 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.650 Prec@5=94.297 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=22:45 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.641 Prec@5=94.294 rate=1.63 Hz, eta=0:02:03, total=0:23:29, wall=22:45 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.641 Prec@5=94.294 rate=1.63 Hz, eta=0:01:02, total=0:24:30, wall=22:45 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.768 Prec@1=79.641 Prec@5=94.294 rate=1.63 Hz, eta=0:01:02, total=0:24:30, wall=22:46 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.769 Prec@1=79.635 Prec@5=94.289 rate=1.63 Hz, eta=0:01:02, total=0:24:30, wall=22:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.769 Prec@1=79.635 Prec@5=94.289 rate=1.63 Hz, eta=0:00:01, total=0:25:31, wall=22:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.769 Prec@1=79.635 Prec@5=94.289 rate=1.63 Hz, eta=0:00:01, total=0:25:31, wall=22:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.769 Prec@1=79.636 Prec@5=94.289 rate=1.63 Hz, eta=0:00:01, total=0:25:31, wall=22:46 IST=> training   100.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.615 DataTime=0.391 Loss=0.769 Prec@1=79.636 Prec@5=94.289 rate=1.63 Hz, eta=0:00:00, total=0:25:32, wall=22:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> validation 0.00% of 1x98...Epoch=128/150 LR=0.00569 Time=6.958 Loss=1.106 Prec@1=72.461 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.958 Loss=1.106 Prec@1=72.461 Prec@5=90.820 rate=2160.13 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.958 Loss=1.106 Prec@1=72.461 Prec@5=90.820 rate=2160.13 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=0.406 Loss=1.093 Prec@1=73.030 Prec@5=91.404 rate=2160.13 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST** validation 100.00% of 1x98...Epoch=128/150 LR=0.00569 Time=0.406 Loss=1.093 Prec@1=73.030 Prec@5=91.404 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=22:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> training   0.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.429 DataTime=5.131 Loss=0.767 Prec@1=80.273 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.429 DataTime=5.131 Loss=0.767 Prec@1=80.273 Prec@5=93.945 rate=4379.18 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.429 DataTime=5.131 Loss=0.767 Prec@1=80.273 Prec@5=93.945 rate=4379.18 Hz, eta=0:00:00, total=0:00:00, wall=22:48 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.658 DataTime=0.433 Loss=0.743 Prec@1=80.161 Prec@5=94.659 rate=4379.18 Hz, eta=0:00:00, total=0:00:00, wall=22:48 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.658 DataTime=0.433 Loss=0.743 Prec@1=80.161 Prec@5=94.659 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:48 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.658 DataTime=0.433 Loss=0.743 Prec@1=80.161 Prec@5=94.659 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:49 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.638 DataTime=0.413 Loss=0.744 Prec@1=80.199 Prec@5=94.616 rate=1.66 Hz, eta=0:24:09, total=0:01:00, wall=22:49 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.638 DataTime=0.413 Loss=0.744 Prec@1=80.199 Prec@5=94.616 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=22:49 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.638 DataTime=0.413 Loss=0.744 Prec@1=80.199 Prec@5=94.616 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=22:50 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.629 DataTime=0.405 Loss=0.748 Prec@1=80.085 Prec@5=94.529 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=22:50 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.629 DataTime=0.405 Loss=0.748 Prec@1=80.085 Prec@5=94.529 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=22:50 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.629 DataTime=0.405 Loss=0.748 Prec@1=80.085 Prec@5=94.529 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=22:51 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.625 DataTime=0.401 Loss=0.748 Prec@1=80.119 Prec@5=94.532 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=22:51 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.625 DataTime=0.401 Loss=0.748 Prec@1=80.119 Prec@5=94.532 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=22:51 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.625 DataTime=0.401 Loss=0.748 Prec@1=80.119 Prec@5=94.532 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=22:52 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.623 DataTime=0.399 Loss=0.751 Prec@1=80.043 Prec@5=94.504 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=22:52 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.623 DataTime=0.399 Loss=0.751 Prec@1=80.043 Prec@5=94.504 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=22:52 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.623 DataTime=0.399 Loss=0.751 Prec@1=80.043 Prec@5=94.504 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=22:53 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.621 DataTime=0.398 Loss=0.750 Prec@1=80.046 Prec@5=94.513 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=22:53 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.621 DataTime=0.398 Loss=0.750 Prec@1=80.046 Prec@5=94.513 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=22:53 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.621 DataTime=0.398 Loss=0.750 Prec@1=80.046 Prec@5=94.513 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=22:54 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.620 DataTime=0.397 Loss=0.752 Prec@1=79.981 Prec@5=94.496 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=22:54 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.620 DataTime=0.397 Loss=0.752 Prec@1=79.981 Prec@5=94.496 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=22:54 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.620 DataTime=0.397 Loss=0.752 Prec@1=79.981 Prec@5=94.496 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=22:55 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.396 Loss=0.751 Prec@1=80.020 Prec@5=94.502 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=22:55 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.396 Loss=0.751 Prec@1=80.020 Prec@5=94.502 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=22:55 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.396 Loss=0.751 Prec@1=80.020 Prec@5=94.502 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=22:56 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.395 Loss=0.752 Prec@1=80.029 Prec@5=94.490 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=22:56 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.395 Loss=0.752 Prec@1=80.029 Prec@5=94.490 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=22:56 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.619 DataTime=0.395 Loss=0.752 Prec@1=80.029 Prec@5=94.490 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=22:57 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.753 Prec@1=80.014 Prec@5=94.470 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=22:57 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.753 Prec@1=80.014 Prec@5=94.470 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:57 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.753 Prec@1=80.014 Prec@5=94.470 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:58 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.755 Prec@1=79.993 Prec@5=94.444 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=22:58 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.755 Prec@1=79.993 Prec@5=94.444 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=22:58 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.618 DataTime=0.394 Loss=0.755 Prec@1=79.993 Prec@5=94.444 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.985 Prec@5=94.431 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:00 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.985 Prec@5=94.431 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:00 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.985 Prec@5=94.431 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:01 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.970 Prec@5=94.432 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:01 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.970 Prec@5=94.432 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:01 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.755 Prec@1=79.970 Prec@5=94.432 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:02 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.956 Prec@5=94.412 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:02 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.956 Prec@5=94.412 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:02 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.956 Prec@5=94.412 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:03 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.950 Prec@5=94.404 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:03 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.950 Prec@5=94.404 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:03 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.617 DataTime=0.393 Loss=0.756 Prec@1=79.950 Prec@5=94.404 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:04 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.393 Loss=0.756 Prec@1=79.938 Prec@5=94.399 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:04 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.393 Loss=0.756 Prec@1=79.938 Prec@5=94.399 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:04 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.393 Loss=0.756 Prec@1=79.938 Prec@5=94.399 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:05 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.757 Prec@1=79.929 Prec@5=94.387 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=23:05 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.757 Prec@1=79.929 Prec@5=94.387 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=23:05 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.757 Prec@1=79.929 Prec@5=94.387 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=23:06 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.758 Prec@1=79.907 Prec@5=94.375 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=23:06 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.758 Prec@1=79.907 Prec@5=94.375 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=23:06 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.758 Prec@1=79.907 Prec@5=94.375 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=23:07 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.898 Prec@5=94.368 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=23:07 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.898 Prec@5=94.368 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:07 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.898 Prec@5=94.368 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:08 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.894 Prec@5=94.365 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:08 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.894 Prec@5=94.365 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=23:08 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.759 Prec@1=79.894 Prec@5=94.365 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=23:09 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.872 Prec@5=94.360 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=23:09 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.872 Prec@5=94.360 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:09 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.872 Prec@5=94.360 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:10 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.871 Prec@5=94.359 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:10 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.871 Prec@5=94.359 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=23:10 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.760 Prec@1=79.871 Prec@5=94.359 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=23:11 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.875 Prec@5=94.356 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=23:11 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.875 Prec@5=94.356 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=23:11 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.875 Prec@5=94.356 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=23:12 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.863 Prec@5=94.349 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=23:12 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.863 Prec@5=94.349 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=23:12 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.616 DataTime=0.392 Loss=0.761 Prec@1=79.863 Prec@5=94.349 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=23:13 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.615 DataTime=0.392 Loss=0.762 Prec@1=79.844 Prec@5=94.339 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=23:13 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.615 DataTime=0.392 Loss=0.762 Prec@1=79.844 Prec@5=94.339 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=23:13 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.615 DataTime=0.392 Loss=0.762 Prec@1=79.844 Prec@5=94.339 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=23:13 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.615 DataTime=0.392 Loss=0.762 Prec@1=79.844 Prec@5=94.340 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=23:13 IST=> training   100.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.615 DataTime=0.392 Loss=0.762 Prec@1=79.844 Prec@5=94.340 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=23:13 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:13 IST=> validation 0.00% of 1x98...Epoch=129/150 LR=0.00521 Time=6.492 Loss=1.143 Prec@1=72.461 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=23:13 IST=> validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=6.492 Loss=1.143 Prec@1=72.461 Prec@5=91.406 rate=4851.33 Hz, eta=0:00:00, total=0:00:00, wall=23:13 IST** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=6.492 Loss=1.143 Prec@1=72.461 Prec@5=91.406 rate=4851.33 Hz, eta=0:00:00, total=0:00:00, wall=23:13 IST** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=0.410 Loss=1.089 Prec@1=73.214 Prec@5=91.496 rate=4851.33 Hz, eta=0:00:00, total=0:00:00, wall=23:13 IST** validation 100.00% of 1x98...Epoch=129/150 LR=0.00521 Time=0.410 Loss=1.089 Prec@1=73.214 Prec@5=91.496 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=23:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:14 IST=> training   0.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.866 DataTime=5.585 Loss=0.701 Prec@1=79.688 Prec@5=95.703 rate=0 Hz, eta=?, total=0:00:00, wall=23:14 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.866 DataTime=5.585 Loss=0.701 Prec@1=79.688 Prec@5=95.703 rate=5402.45 Hz, eta=0:00:00, total=0:00:00, wall=23:14 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=5.866 DataTime=5.585 Loss=0.701 Prec@1=79.688 Prec@5=95.703 rate=5402.45 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.663 DataTime=0.438 Loss=0.740 Prec@1=80.326 Prec@5=94.626 rate=5402.45 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.663 DataTime=0.438 Loss=0.740 Prec@1=80.326 Prec@5=94.626 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:15 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.663 DataTime=0.438 Loss=0.740 Prec@1=80.326 Prec@5=94.626 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.638 DataTime=0.413 Loss=0.746 Prec@1=80.190 Prec@5=94.535 rate=1.65 Hz, eta=0:24:14, total=0:01:01, wall=23:16 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.638 DataTime=0.413 Loss=0.746 Prec@1=80.190 Prec@5=94.535 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=23:16 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.638 DataTime=0.413 Loss=0.746 Prec@1=80.190 Prec@5=94.535 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=23:17 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.630 DataTime=0.405 Loss=0.748 Prec@1=80.146 Prec@5=94.512 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=23:17 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.630 DataTime=0.405 Loss=0.748 Prec@1=80.146 Prec@5=94.512 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=23:17 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.630 DataTime=0.405 Loss=0.748 Prec@1=80.146 Prec@5=94.512 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=23:18 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.626 DataTime=0.401 Loss=0.745 Prec@1=80.284 Prec@5=94.561 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=23:18 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.626 DataTime=0.401 Loss=0.745 Prec@1=80.284 Prec@5=94.561 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=23:18 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.626 DataTime=0.401 Loss=0.745 Prec@1=80.284 Prec@5=94.561 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=23:19 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.623 DataTime=0.398 Loss=0.746 Prec@1=80.227 Prec@5=94.527 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=23:19 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.623 DataTime=0.398 Loss=0.746 Prec@1=80.227 Prec@5=94.527 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=23:19 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.623 DataTime=0.398 Loss=0.746 Prec@1=80.227 Prec@5=94.527 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=23:20 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.621 DataTime=0.397 Loss=0.748 Prec@1=80.179 Prec@5=94.518 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=23:20 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.621 DataTime=0.397 Loss=0.748 Prec@1=80.179 Prec@5=94.518 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=23:20 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.621 DataTime=0.397 Loss=0.748 Prec@1=80.179 Prec@5=94.518 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=23:21 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.620 DataTime=0.395 Loss=0.748 Prec@1=80.176 Prec@5=94.502 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=23:21 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.620 DataTime=0.395 Loss=0.748 Prec@1=80.176 Prec@5=94.502 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=23:21 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.620 DataTime=0.395 Loss=0.748 Prec@1=80.176 Prec@5=94.502 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=23:22 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.395 Loss=0.748 Prec@1=80.171 Prec@5=94.495 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=23:22 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.395 Loss=0.748 Prec@1=80.171 Prec@5=94.495 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:22 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.395 Loss=0.748 Prec@1=80.171 Prec@5=94.495 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:23 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.394 Loss=0.749 Prec@1=80.157 Prec@5=94.490 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=23:23 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.394 Loss=0.749 Prec@1=80.157 Prec@5=94.490 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:23 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.619 DataTime=0.394 Loss=0.749 Prec@1=80.157 Prec@5=94.490 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:24 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.394 Loss=0.749 Prec@1=80.152 Prec@5=94.482 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=23:24 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.394 Loss=0.749 Prec@1=80.152 Prec@5=94.482 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=23:24 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.394 Loss=0.749 Prec@1=80.152 Prec@5=94.482 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=23:25 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.164 Prec@5=94.462 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=23:25 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.164 Prec@5=94.462 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:25 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.164 Prec@5=94.462 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:26 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.160 Prec@5=94.456 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=23:26 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.160 Prec@5=94.456 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:26 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.618 DataTime=0.393 Loss=0.750 Prec@1=80.160 Prec@5=94.456 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:27 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.151 Prec@5=94.462 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=23:27 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.151 Prec@5=94.462 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:27 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.151 Prec@5=94.462 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:28 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.142 Prec@5=94.452 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=23:28 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.142 Prec@5=94.452 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:28 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.393 Loss=0.750 Prec@1=80.142 Prec@5=94.452 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:29 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.122 Prec@5=94.443 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=23:29 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.122 Prec@5=94.443 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:29 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.122 Prec@5=94.443 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:30 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.109 Prec@5=94.434 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=23:30 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.109 Prec@5=94.434 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:30 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.751 Prec@1=80.109 Prec@5=94.434 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:31 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.097 Prec@5=94.426 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=23:31 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.097 Prec@5=94.426 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:31 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.097 Prec@5=94.426 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:32 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.105 Prec@5=94.433 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=23:32 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.105 Prec@5=94.433 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:32 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.617 DataTime=0.392 Loss=0.752 Prec@1=80.105 Prec@5=94.433 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:33 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.099 Prec@5=94.439 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=23:33 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.099 Prec@5=94.439 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:33 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.099 Prec@5=94.439 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:34 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.088 Prec@5=94.434 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=23:34 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.088 Prec@5=94.434 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:34 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.752 Prec@1=80.088 Prec@5=94.434 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:35 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.069 Prec@5=94.426 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=23:35 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.069 Prec@5=94.426 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:35 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.069 Prec@5=94.426 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:36 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.063 Prec@5=94.430 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=23:36 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.063 Prec@5=94.430 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:36 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.753 Prec@1=80.063 Prec@5=94.430 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:37 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.754 Prec@1=80.047 Prec@5=94.427 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=23:37 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.754 Prec@1=80.047 Prec@5=94.427 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:37 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.392 Loss=0.754 Prec@1=80.047 Prec@5=94.427 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:38 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.041 Prec@5=94.423 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=23:38 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.041 Prec@5=94.423 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:38 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.041 Prec@5=94.423 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:39 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.034 Prec@5=94.416 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=23:39 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.034 Prec@5=94.416 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:39 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.034 Prec@5=94.416 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:39 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.034 Prec@5=94.416 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=23:39 IST=> training   100.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.616 DataTime=0.391 Loss=0.754 Prec@1=80.034 Prec@5=94.416 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=23:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:39 IST=> validation 0.00% of 1x98...Epoch=130/150 LR=0.00476 Time=6.253 Loss=1.013 Prec@1=73.633 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=23:39 IST=> validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=6.253 Loss=1.013 Prec@1=73.633 Prec@5=90.820 rate=4297.23 Hz, eta=0:00:00, total=0:00:00, wall=23:39 IST** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=6.253 Loss=1.013 Prec@1=73.633 Prec@5=90.820 rate=4297.23 Hz, eta=0:00:00, total=0:00:00, wall=23:40 IST** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=0.406 Loss=1.097 Prec@1=73.198 Prec@5=91.386 rate=4297.23 Hz, eta=0:00:00, total=0:00:00, wall=23:40 IST** validation 100.00% of 1x98...Epoch=130/150 LR=0.00476 Time=0.406 Loss=1.097 Prec@1=73.198 Prec@5=91.386 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=23:40 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:40 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:40 IST=> training   0.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.337 DataTime=5.059 Loss=0.865 Prec@1=76.953 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=23:40 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.337 DataTime=5.059 Loss=0.865 Prec@1=76.953 Prec@5=93.359 rate=4260.74 Hz, eta=0:00:00, total=0:00:00, wall=23:40 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.337 DataTime=5.059 Loss=0.865 Prec@1=76.953 Prec@5=93.359 rate=4260.74 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.659 DataTime=0.434 Loss=0.729 Prec@1=80.587 Prec@5=94.696 rate=4260.74 Hz, eta=0:00:00, total=0:00:00, wall=23:41 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.659 DataTime=0.434 Loss=0.729 Prec@1=80.587 Prec@5=94.696 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=23:41 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.659 DataTime=0.434 Loss=0.729 Prec@1=80.587 Prec@5=94.696 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=23:42 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.637 DataTime=0.412 Loss=0.732 Prec@1=80.541 Prec@5=94.637 rate=1.65 Hz, eta=0:24:16, total=0:01:01, wall=23:42 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.637 DataTime=0.412 Loss=0.732 Prec@1=80.541 Prec@5=94.637 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:42 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.637 DataTime=0.412 Loss=0.732 Prec@1=80.541 Prec@5=94.637 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:43 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.629 DataTime=0.404 Loss=0.736 Prec@1=80.483 Prec@5=94.609 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=23:43 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.629 DataTime=0.404 Loss=0.736 Prec@1=80.483 Prec@5=94.609 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=23:43 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.629 DataTime=0.404 Loss=0.736 Prec@1=80.483 Prec@5=94.609 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=23:44 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.625 DataTime=0.400 Loss=0.735 Prec@1=80.483 Prec@5=94.645 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=23:44 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.625 DataTime=0.400 Loss=0.735 Prec@1=80.483 Prec@5=94.645 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=23:44 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.625 DataTime=0.400 Loss=0.735 Prec@1=80.483 Prec@5=94.645 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=23:45 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.623 DataTime=0.398 Loss=0.734 Prec@1=80.532 Prec@5=94.661 rate=1.64 Hz, eta=0:21:25, total=0:04:05, wall=23:45 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.623 DataTime=0.398 Loss=0.734 Prec@1=80.532 Prec@5=94.661 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=23:45 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.623 DataTime=0.398 Loss=0.734 Prec@1=80.532 Prec@5=94.661 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=23:46 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.621 DataTime=0.397 Loss=0.734 Prec@1=80.532 Prec@5=94.678 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=23:46 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.621 DataTime=0.397 Loss=0.734 Prec@1=80.532 Prec@5=94.678 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=23:46 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.621 DataTime=0.397 Loss=0.734 Prec@1=80.532 Prec@5=94.678 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=23:47 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.396 Loss=0.734 Prec@1=80.511 Prec@5=94.688 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=23:47 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.396 Loss=0.734 Prec@1=80.511 Prec@5=94.688 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:47 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.396 Loss=0.734 Prec@1=80.511 Prec@5=94.688 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:48 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.395 Loss=0.734 Prec@1=80.512 Prec@5=94.695 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=23:48 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.395 Loss=0.734 Prec@1=80.512 Prec@5=94.695 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=23:48 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.620 DataTime=0.395 Loss=0.734 Prec@1=80.512 Prec@5=94.695 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=23:49 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.695 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=23:49 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.695 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:49 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.695 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:50 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.676 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=23:50 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.676 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=23:50 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.619 DataTime=0.394 Loss=0.733 Prec@1=80.536 Prec@5=94.676 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=23:51 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.734 Prec@1=80.505 Prec@5=94.665 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=23:51 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.734 Prec@1=80.505 Prec@5=94.665 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:51 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.734 Prec@1=80.505 Prec@5=94.665 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:52 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.478 Prec@5=94.651 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=23:52 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.478 Prec@5=94.651 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:52 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.478 Prec@5=94.651 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:53 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.463 Prec@5=94.648 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=23:53 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.463 Prec@5=94.648 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:53 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.618 DataTime=0.393 Loss=0.736 Prec@1=80.463 Prec@5=94.648 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:54 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.393 Loss=0.736 Prec@1=80.446 Prec@5=94.645 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=23:54 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.393 Loss=0.736 Prec@1=80.446 Prec@5=94.645 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:54 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.393 Loss=0.736 Prec@1=80.446 Prec@5=94.645 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:55 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.435 Prec@5=94.636 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=23:55 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.435 Prec@5=94.636 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:55 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.435 Prec@5=94.636 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:56 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.421 Prec@5=94.621 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=23:56 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.421 Prec@5=94.621 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:56 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.737 Prec@1=80.421 Prec@5=94.621 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:57 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.618 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=23:57 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.618 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:57 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.618 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:58 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.397 Prec@5=94.620 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=23:58 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.397 Prec@5=94.620 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=23:58 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.397 Prec@5=94.620 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:00 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.619 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:00 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.619 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:00 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.617 DataTime=0.392 Loss=0.738 Prec@1=80.395 Prec@5=94.619 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:01 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.392 Loss=0.739 Prec@1=80.385 Prec@5=94.614 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:01 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.392 Loss=0.739 Prec@1=80.385 Prec@5=94.614 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:01 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.392 Loss=0.739 Prec@1=80.385 Prec@5=94.614 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:02 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.740 Prec@1=80.368 Prec@5=94.602 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:02 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.740 Prec@1=80.368 Prec@5=94.602 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:02 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.740 Prec@1=80.368 Prec@5=94.602 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:03 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.349 Prec@5=94.595 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:03 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.349 Prec@5=94.595 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:03 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.349 Prec@5=94.595 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:04 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.332 Prec@5=94.586 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:04 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.332 Prec@5=94.586 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:04 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.741 Prec@1=80.332 Prec@5=94.586 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:05 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.327 Prec@5=94.588 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:05 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.327 Prec@5=94.588 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:05 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.327 Prec@5=94.588 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.316 Prec@5=94.584 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:06 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.316 Prec@5=94.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:06 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.316 Prec@5=94.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:06 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.316 Prec@5=94.584 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:06 IST=> training   100.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.616 DataTime=0.391 Loss=0.742 Prec@1=80.316 Prec@5=94.584 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=00:06 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST=> validation 0.00% of 1x98...Epoch=131/150 LR=0.00432 Time=7.409 Loss=1.253 Prec@1=70.312 Prec@5=89.648 rate=0 Hz, eta=?, total=0:00:00, wall=00:06 IST=> validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=7.409 Loss=1.253 Prec@1=70.312 Prec@5=89.648 rate=3705.89 Hz, eta=0:00:00, total=0:00:00, wall=00:06 IST** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=7.409 Loss=1.253 Prec@1=70.312 Prec@5=89.648 rate=3705.89 Hz, eta=0:00:00, total=0:00:00, wall=00:06 IST** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=0.412 Loss=1.088 Prec@1=73.404 Prec@5=91.604 rate=3705.89 Hz, eta=0:00:00, total=0:00:00, wall=00:06 IST** validation 100.00% of 1x98...Epoch=131/150 LR=0.00432 Time=0.412 Loss=1.088 Prec@1=73.404 Prec@5=91.604 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=00:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=6.177 DataTime=5.938 Loss=0.698 Prec@1=80.273 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=6.177 DataTime=5.938 Loss=0.698 Prec@1=80.273 Prec@5=95.117 rate=5452.21 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=6.177 DataTime=5.938 Loss=0.698 Prec@1=80.273 Prec@5=95.117 rate=5452.21 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.665 DataTime=0.442 Loss=0.730 Prec@1=80.681 Prec@5=94.732 rate=5452.21 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.665 DataTime=0.442 Loss=0.730 Prec@1=80.681 Prec@5=94.732 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=00:08 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.665 DataTime=0.442 Loss=0.730 Prec@1=80.681 Prec@5=94.732 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=00:09 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.640 DataTime=0.415 Loss=0.728 Prec@1=80.799 Prec@5=94.676 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=00:09 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.640 DataTime=0.415 Loss=0.728 Prec@1=80.799 Prec@5=94.676 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=00:09 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.640 DataTime=0.415 Loss=0.728 Prec@1=80.799 Prec@5=94.676 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=00:10 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.631 DataTime=0.406 Loss=0.728 Prec@1=80.765 Prec@5=94.703 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=00:10 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.631 DataTime=0.406 Loss=0.728 Prec@1=80.765 Prec@5=94.703 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=00:10 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.631 DataTime=0.406 Loss=0.728 Prec@1=80.765 Prec@5=94.703 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=00:11 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.627 DataTime=0.401 Loss=0.728 Prec@1=80.723 Prec@5=94.694 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=00:11 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.627 DataTime=0.401 Loss=0.728 Prec@1=80.723 Prec@5=94.694 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=00:11 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.627 DataTime=0.401 Loss=0.728 Prec@1=80.723 Prec@5=94.694 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=00:12 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.625 DataTime=0.399 Loss=0.728 Prec@1=80.697 Prec@5=94.680 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=00:12 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.625 DataTime=0.399 Loss=0.728 Prec@1=80.697 Prec@5=94.680 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=00:12 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.625 DataTime=0.399 Loss=0.728 Prec@1=80.697 Prec@5=94.680 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=00:13 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.623 DataTime=0.397 Loss=0.728 Prec@1=80.720 Prec@5=94.676 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=00:13 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.623 DataTime=0.397 Loss=0.728 Prec@1=80.720 Prec@5=94.676 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=00:13 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.623 DataTime=0.397 Loss=0.728 Prec@1=80.720 Prec@5=94.676 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=00:14 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.622 DataTime=0.396 Loss=0.726 Prec@1=80.761 Prec@5=94.693 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=00:14 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.622 DataTime=0.396 Loss=0.726 Prec@1=80.761 Prec@5=94.693 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=00:14 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.622 DataTime=0.396 Loss=0.726 Prec@1=80.761 Prec@5=94.693 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=00:15 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.621 DataTime=0.395 Loss=0.728 Prec@1=80.734 Prec@5=94.688 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=00:15 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.621 DataTime=0.395 Loss=0.728 Prec@1=80.734 Prec@5=94.688 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=00:15 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.621 DataTime=0.395 Loss=0.728 Prec@1=80.734 Prec@5=94.688 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=00:16 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.726 Prec@1=80.769 Prec@5=94.695 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=00:16 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.726 Prec@1=80.769 Prec@5=94.695 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:16 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.726 Prec@1=80.769 Prec@5=94.695 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:17 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.728 Prec@1=80.726 Prec@5=94.680 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:17 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.728 Prec@1=80.726 Prec@5=94.680 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:17 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.620 DataTime=0.394 Loss=0.728 Prec@1=80.726 Prec@5=94.680 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:18 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.394 Loss=0.728 Prec@1=80.698 Prec@5=94.676 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=00:18 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.394 Loss=0.728 Prec@1=80.698 Prec@5=94.676 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:18 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.394 Loss=0.728 Prec@1=80.698 Prec@5=94.676 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:19 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.393 Loss=0.729 Prec@1=80.671 Prec@5=94.668 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:19 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.393 Loss=0.729 Prec@1=80.671 Prec@5=94.668 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:19 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.619 DataTime=0.393 Loss=0.729 Prec@1=80.671 Prec@5=94.668 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:20 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.729 Prec@1=80.667 Prec@5=94.664 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:20 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.729 Prec@1=80.667 Prec@5=94.664 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:20 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.729 Prec@1=80.667 Prec@5=94.664 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:21 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.648 Prec@5=94.664 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:21 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.648 Prec@5=94.664 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:21 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.648 Prec@5=94.664 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:22 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.629 Prec@5=94.664 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:22 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.629 Prec@5=94.664 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:22 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.393 Loss=0.730 Prec@1=80.629 Prec@5=94.664 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.392 Loss=0.730 Prec@1=80.620 Prec@5=94.660 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:23 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.392 Loss=0.730 Prec@1=80.620 Prec@5=94.660 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:23 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.618 DataTime=0.392 Loss=0.730 Prec@1=80.620 Prec@5=94.660 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:24 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.594 Prec@5=94.658 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:24 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.594 Prec@5=94.658 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:24 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.594 Prec@5=94.658 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:25 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.582 Prec@5=94.657 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:25 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.582 Prec@5=94.657 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:25 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.731 Prec@1=80.582 Prec@5=94.657 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:26 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.583 Prec@5=94.653 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:26 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.583 Prec@5=94.653 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:26 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.583 Prec@5=94.653 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:27 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.576 Prec@5=94.654 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=00:27 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.576 Prec@5=94.654 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:27 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.576 Prec@5=94.654 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:28 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.571 Prec@5=94.654 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:28 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.571 Prec@5=94.654 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:28 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.571 Prec@5=94.654 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:29 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.557 Prec@5=94.652 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:29 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.557 Prec@5=94.652 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:29 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.732 Prec@1=80.557 Prec@5=94.652 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:30 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.733 Prec@1=80.542 Prec@5=94.647 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:30 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.733 Prec@1=80.542 Prec@5=94.647 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:30 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.733 Prec@1=80.542 Prec@5=94.647 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:31 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.734 Prec@1=80.513 Prec@5=94.639 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:31 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.734 Prec@1=80.513 Prec@5=94.639 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:31 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.617 DataTime=0.392 Loss=0.734 Prec@1=80.513 Prec@5=94.639 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:32 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.616 DataTime=0.391 Loss=0.734 Prec@1=80.505 Prec@5=94.643 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:32 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.616 DataTime=0.391 Loss=0.734 Prec@1=80.505 Prec@5=94.643 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:32 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.616 DataTime=0.391 Loss=0.734 Prec@1=80.505 Prec@5=94.643 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:32 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.616 DataTime=0.391 Loss=0.734 Prec@1=80.504 Prec@5=94.643 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:32 IST=> training   100.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.616 DataTime=0.391 Loss=0.734 Prec@1=80.504 Prec@5=94.643 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=00:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:32 IST=> validation 0.00% of 1x98...Epoch=132/150 LR=0.00391 Time=6.496 Loss=1.213 Prec@1=70.312 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=00:32 IST=> validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=6.496 Loss=1.213 Prec@1=70.312 Prec@5=89.453 rate=6312.22 Hz, eta=0:00:00, total=0:00:00, wall=00:32 IST** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=6.496 Loss=1.213 Prec@1=70.312 Prec@5=89.453 rate=6312.22 Hz, eta=0:00:00, total=0:00:00, wall=00:33 IST** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=0.402 Loss=1.079 Prec@1=73.416 Prec@5=91.692 rate=6312.22 Hz, eta=0:00:00, total=0:00:00, wall=00:33 IST** validation 100.00% of 1x98...Epoch=132/150 LR=0.00391 Time=0.402 Loss=1.079 Prec@1=73.416 Prec@5=91.692 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=00:33 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:33 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:33 IST=> training   0.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=6.234 DataTime=5.995 Loss=0.669 Prec@1=81.641 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=00:33 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=6.234 DataTime=5.995 Loss=0.669 Prec@1=81.641 Prec@5=95.312 rate=9843.88 Hz, eta=0:00:00, total=0:00:00, wall=00:33 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=6.234 DataTime=5.995 Loss=0.669 Prec@1=81.641 Prec@5=95.312 rate=9843.88 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.668 DataTime=0.445 Loss=0.709 Prec@1=81.221 Prec@5=94.957 rate=9843.88 Hz, eta=0:00:00, total=0:00:00, wall=00:34 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.668 DataTime=0.445 Loss=0.709 Prec@1=81.221 Prec@5=94.957 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=00:34 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.668 DataTime=0.445 Loss=0.709 Prec@1=81.221 Prec@5=94.957 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=00:35 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.640 DataTime=0.417 Loss=0.714 Prec@1=81.129 Prec@5=94.812 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=00:35 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.640 DataTime=0.417 Loss=0.714 Prec@1=81.129 Prec@5=94.812 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=00:35 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.640 DataTime=0.417 Loss=0.714 Prec@1=81.129 Prec@5=94.812 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=00:36 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.630 DataTime=0.407 Loss=0.718 Prec@1=80.998 Prec@5=94.756 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=00:36 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.630 DataTime=0.407 Loss=0.718 Prec@1=80.998 Prec@5=94.756 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=00:36 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.630 DataTime=0.407 Loss=0.718 Prec@1=80.998 Prec@5=94.756 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=00:37 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.626 DataTime=0.403 Loss=0.721 Prec@1=80.914 Prec@5=94.738 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=00:37 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.626 DataTime=0.403 Loss=0.721 Prec@1=80.914 Prec@5=94.738 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=00:37 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.626 DataTime=0.403 Loss=0.721 Prec@1=80.914 Prec@5=94.738 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=00:38 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.624 DataTime=0.400 Loss=0.720 Prec@1=80.931 Prec@5=94.744 rate=1.64 Hz, eta=0:21:23, total=0:04:04, wall=00:38 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.624 DataTime=0.400 Loss=0.720 Prec@1=80.931 Prec@5=94.744 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=00:38 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.624 DataTime=0.400 Loss=0.720 Prec@1=80.931 Prec@5=94.744 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=00:39 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.622 DataTime=0.398 Loss=0.720 Prec@1=80.905 Prec@5=94.731 rate=1.64 Hz, eta=0:20:24, total=0:05:06, wall=00:39 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.622 DataTime=0.398 Loss=0.720 Prec@1=80.905 Prec@5=94.731 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=00:39 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.622 DataTime=0.398 Loss=0.720 Prec@1=80.905 Prec@5=94.731 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=00:40 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.397 Loss=0.721 Prec@1=80.875 Prec@5=94.722 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=00:40 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.397 Loss=0.721 Prec@1=80.875 Prec@5=94.722 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=00:40 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.397 Loss=0.721 Prec@1=80.875 Prec@5=94.722 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=00:41 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.396 Loss=0.722 Prec@1=80.858 Prec@5=94.709 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=00:41 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.396 Loss=0.722 Prec@1=80.858 Prec@5=94.709 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=00:41 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.621 DataTime=0.396 Loss=0.722 Prec@1=80.858 Prec@5=94.709 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=00:42 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.620 DataTime=0.396 Loss=0.722 Prec@1=80.863 Prec@5=94.703 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=00:42 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.620 DataTime=0.396 Loss=0.722 Prec@1=80.863 Prec@5=94.703 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:42 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.620 DataTime=0.396 Loss=0.722 Prec@1=80.863 Prec@5=94.703 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:43 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.395 Loss=0.723 Prec@1=80.855 Prec@5=94.690 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=00:43 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.395 Loss=0.723 Prec@1=80.855 Prec@5=94.690 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=00:43 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.395 Loss=0.723 Prec@1=80.855 Prec@5=94.690 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=00:44 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.868 Prec@5=94.692 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=00:44 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.868 Prec@5=94.692 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:44 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.868 Prec@5=94.692 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:45 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.856 Prec@5=94.704 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=00:45 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.856 Prec@5=94.704 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:45 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.619 DataTime=0.394 Loss=0.722 Prec@1=80.856 Prec@5=94.704 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:46 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.394 Loss=0.722 Prec@1=80.860 Prec@5=94.708 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=00:46 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.394 Loss=0.722 Prec@1=80.860 Prec@5=94.708 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:46 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.394 Loss=0.722 Prec@1=80.860 Prec@5=94.708 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:47 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.723 Prec@1=80.830 Prec@5=94.702 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=00:47 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.723 Prec@1=80.830 Prec@5=94.702 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:47 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.723 Prec@1=80.830 Prec@5=94.702 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:48 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.833 Prec@5=94.710 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=00:48 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.833 Prec@5=94.710 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:48 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.833 Prec@5=94.710 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:49 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.815 Prec@5=94.718 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=00:49 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.815 Prec@5=94.718 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:49 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.618 DataTime=0.393 Loss=0.722 Prec@1=80.815 Prec@5=94.718 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:50 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.723 Prec@1=80.806 Prec@5=94.722 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=00:50 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.723 Prec@1=80.806 Prec@5=94.722 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:50 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.723 Prec@1=80.806 Prec@5=94.722 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:51 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.724 Prec@1=80.781 Prec@5=94.710 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=00:51 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.724 Prec@1=80.781 Prec@5=94.710 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:51 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.393 Loss=0.724 Prec@1=80.781 Prec@5=94.710 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:52 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.724 Prec@1=80.768 Prec@5=94.705 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=00:52 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.724 Prec@1=80.768 Prec@5=94.705 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:52 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.724 Prec@1=80.768 Prec@5=94.705 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:53 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.756 Prec@5=94.701 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=00:53 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.756 Prec@5=94.701 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:53 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.756 Prec@5=94.701 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:54 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.758 Prec@5=94.702 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=00:54 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.758 Prec@5=94.702 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:54 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.758 Prec@5=94.702 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:55 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.746 Prec@5=94.700 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=00:55 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.746 Prec@5=94.700 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:55 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.746 Prec@5=94.700 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:57 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.740 Prec@5=94.699 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=00:57 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.740 Prec@5=94.699 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:57 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.725 Prec@1=80.740 Prec@5=94.699 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:58 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=00:58 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:58 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.617 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:59 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.616 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=00:59 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.616 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:59 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.616 DataTime=0.392 Loss=0.726 Prec@1=80.734 Prec@5=94.695 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:59 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.616 DataTime=0.392 Loss=0.726 Prec@1=80.732 Prec@5=94.694 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=00:59 IST=> training   100.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.616 DataTime=0.392 Loss=0.726 Prec@1=80.732 Prec@5=94.694 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=00:59 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> validation 0.00% of 1x98...Epoch=133/150 LR=0.00351 Time=6.453 Loss=1.134 Prec@1=73.438 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=6.453 Loss=1.134 Prec@1=73.438 Prec@5=90.039 rate=2933.78 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=6.453 Loss=1.134 Prec@1=73.438 Prec@5=90.039 rate=2933.78 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=0.409 Loss=1.076 Prec@1=73.658 Prec@5=91.726 rate=2933.78 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST** validation 100.00% of 1x98...Epoch=133/150 LR=0.00351 Time=0.409 Loss=1.076 Prec@1=73.658 Prec@5=91.726 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=00:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.229 DataTime=4.916 Loss=0.664 Prec@1=82.617 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.229 DataTime=4.916 Loss=0.664 Prec@1=82.617 Prec@5=95.898 rate=2354.94 Hz, eta=0:00:01, total=0:00:00, wall=00:59 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.229 DataTime=4.916 Loss=0.664 Prec@1=82.617 Prec@5=95.898 rate=2354.94 Hz, eta=0:00:01, total=0:00:00, wall=01:00 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.659 DataTime=0.431 Loss=0.711 Prec@1=81.188 Prec@5=94.866 rate=2354.94 Hz, eta=0:00:01, total=0:00:00, wall=01:00 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.659 DataTime=0.431 Loss=0.711 Prec@1=81.188 Prec@5=94.866 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=01:00 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.659 DataTime=0.431 Loss=0.711 Prec@1=81.188 Prec@5=94.866 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=01:01 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.637 DataTime=0.410 Loss=0.708 Prec@1=81.320 Prec@5=94.897 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=01:01 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.637 DataTime=0.410 Loss=0.708 Prec@1=81.320 Prec@5=94.897 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=01:01 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.637 DataTime=0.410 Loss=0.708 Prec@1=81.320 Prec@5=94.897 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=01:02 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.630 DataTime=0.403 Loss=0.713 Prec@1=81.155 Prec@5=94.810 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=01:02 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.630 DataTime=0.403 Loss=0.713 Prec@1=81.155 Prec@5=94.810 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=01:02 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.630 DataTime=0.403 Loss=0.713 Prec@1=81.155 Prec@5=94.810 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=01:04 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.626 DataTime=0.400 Loss=0.712 Prec@1=81.153 Prec@5=94.824 rate=1.63 Hz, eta=0:22:29, total=0:03:04, wall=01:04 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.626 DataTime=0.400 Loss=0.712 Prec@1=81.153 Prec@5=94.824 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=01:04 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.626 DataTime=0.400 Loss=0.712 Prec@1=81.153 Prec@5=94.824 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=01:05 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.624 DataTime=0.398 Loss=0.713 Prec@1=81.104 Prec@5=94.803 rate=1.63 Hz, eta=0:21:28, total=0:04:05, wall=01:05 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.624 DataTime=0.398 Loss=0.713 Prec@1=81.104 Prec@5=94.803 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:05 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.624 DataTime=0.398 Loss=0.713 Prec@1=81.104 Prec@5=94.803 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:06 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.622 DataTime=0.396 Loss=0.713 Prec@1=81.096 Prec@5=94.815 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:06 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.622 DataTime=0.396 Loss=0.713 Prec@1=81.096 Prec@5=94.815 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:06 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.622 DataTime=0.396 Loss=0.713 Prec@1=81.096 Prec@5=94.815 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:07 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.621 DataTime=0.396 Loss=0.712 Prec@1=81.101 Prec@5=94.832 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:07 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.621 DataTime=0.396 Loss=0.712 Prec@1=81.101 Prec@5=94.832 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=01:07 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.621 DataTime=0.396 Loss=0.712 Prec@1=81.101 Prec@5=94.832 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=01:08 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.395 Loss=0.712 Prec@1=81.092 Prec@5=94.826 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=01:08 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.395 Loss=0.712 Prec@1=81.092 Prec@5=94.826 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:08 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.395 Loss=0.712 Prec@1=81.092 Prec@5=94.826 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:09 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.394 Loss=0.712 Prec@1=81.091 Prec@5=94.838 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:09 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.394 Loss=0.712 Prec@1=81.091 Prec@5=94.838 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:09 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.620 DataTime=0.394 Loss=0.712 Prec@1=81.091 Prec@5=94.838 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:10 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.394 Loss=0.712 Prec@1=81.070 Prec@5=94.836 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:10 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.394 Loss=0.712 Prec@1=81.070 Prec@5=94.836 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:10 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.394 Loss=0.712 Prec@1=81.070 Prec@5=94.836 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:11 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.393 Loss=0.712 Prec@1=81.052 Prec@5=94.831 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:11 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.393 Loss=0.712 Prec@1=81.052 Prec@5=94.831 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:11 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.619 DataTime=0.393 Loss=0.712 Prec@1=81.052 Prec@5=94.831 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:12 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.033 Prec@5=94.835 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:12 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.033 Prec@5=94.835 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:12 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.033 Prec@5=94.835 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:13 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.056 Prec@5=94.846 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:13 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.056 Prec@5=94.846 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:13 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.712 Prec@1=81.056 Prec@5=94.846 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.713 Prec@1=81.033 Prec@5=94.839 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:14 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.713 Prec@1=81.033 Prec@5=94.839 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:14 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.618 DataTime=0.393 Loss=0.713 Prec@1=81.033 Prec@5=94.839 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:15 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.015 Prec@5=94.828 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:15 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.015 Prec@5=94.828 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:15 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.015 Prec@5=94.828 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:16 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.011 Prec@5=94.827 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:16 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.011 Prec@5=94.827 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:16 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.713 Prec@1=81.011 Prec@5=94.827 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:17 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.997 Prec@5=94.816 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:17 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.997 Prec@5=94.816 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:17 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.997 Prec@5=94.816 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:18 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.982 Prec@5=94.811 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:18 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.982 Prec@5=94.811 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:18 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.982 Prec@5=94.811 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:19 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.715 Prec@1=80.982 Prec@5=94.806 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:19 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.715 Prec@1=80.982 Prec@5=94.806 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:19 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.715 Prec@1=80.982 Prec@5=94.806 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:20 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.989 Prec@5=94.817 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:20 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.989 Prec@5=94.817 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:20 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.392 Loss=0.714 Prec@1=80.989 Prec@5=94.817 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:21 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.814 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=01:21 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.814 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:21 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.617 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.814 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:22 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.974 Prec@5=94.812 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:22 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.974 Prec@5=94.812 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:22 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.974 Prec@5=94.812 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:23 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.813 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:23 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.813 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:23 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.970 Prec@5=94.813 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:24 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.958 Prec@5=94.810 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=01:24 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.958 Prec@5=94.810 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:24 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.715 Prec@1=80.958 Prec@5=94.810 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:25 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.716 Prec@1=80.943 Prec@5=94.801 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:25 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.716 Prec@1=80.943 Prec@5=94.801 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:25 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.716 Prec@1=80.943 Prec@5=94.801 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:25 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.716 Prec@1=80.942 Prec@5=94.801 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:25 IST=> training   100.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.616 DataTime=0.391 Loss=0.716 Prec@1=80.942 Prec@5=94.801 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=01:25 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST=> validation 0.00% of 1x98...Epoch=134/150 LR=0.00314 Time=6.505 Loss=1.050 Prec@1=72.656 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST=> validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.505 Loss=1.050 Prec@1=72.656 Prec@5=93.359 rate=3986.53 Hz, eta=0:00:00, total=0:00:00, wall=01:25 IST** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.505 Loss=1.050 Prec@1=72.656 Prec@5=93.359 rate=3986.53 Hz, eta=0:00:00, total=0:00:00, wall=01:26 IST** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=0.403 Loss=1.077 Prec@1=73.566 Prec@5=91.690 rate=3986.53 Hz, eta=0:00:00, total=0:00:00, wall=01:26 IST** validation 100.00% of 1x98...Epoch=134/150 LR=0.00314 Time=0.403 Loss=1.077 Prec@1=73.566 Prec@5=91.690 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=01:26 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:26 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:26 IST=> training   0.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.804 DataTime=4.501 Loss=0.657 Prec@1=79.883 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=01:26 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.804 DataTime=4.501 Loss=0.657 Prec@1=79.883 Prec@5=95.312 rate=3696.72 Hz, eta=0:00:00, total=0:00:00, wall=01:26 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.804 DataTime=4.501 Loss=0.657 Prec@1=79.883 Prec@5=95.312 rate=3696.72 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.656 DataTime=0.429 Loss=0.716 Prec@1=81.028 Prec@5=94.785 rate=3696.72 Hz, eta=0:00:00, total=0:00:00, wall=01:27 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.656 DataTime=0.429 Loss=0.716 Prec@1=81.028 Prec@5=94.785 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=01:27 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.656 DataTime=0.429 Loss=0.716 Prec@1=81.028 Prec@5=94.785 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=01:28 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.635 DataTime=0.409 Loss=0.711 Prec@1=81.150 Prec@5=94.823 rate=1.64 Hz, eta=0:24:22, total=0:01:01, wall=01:28 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.635 DataTime=0.409 Loss=0.711 Prec@1=81.150 Prec@5=94.823 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=01:28 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.635 DataTime=0.409 Loss=0.711 Prec@1=81.150 Prec@5=94.823 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=01:29 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.628 DataTime=0.402 Loss=0.712 Prec@1=81.118 Prec@5=94.793 rate=1.64 Hz, eta=0:23:27, total=0:02:02, wall=01:29 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.628 DataTime=0.402 Loss=0.712 Prec@1=81.118 Prec@5=94.793 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=01:29 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.628 DataTime=0.402 Loss=0.712 Prec@1=81.118 Prec@5=94.793 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=01:30 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.625 DataTime=0.399 Loss=0.712 Prec@1=81.090 Prec@5=94.778 rate=1.63 Hz, eta=0:22:28, total=0:03:04, wall=01:30 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.625 DataTime=0.399 Loss=0.712 Prec@1=81.090 Prec@5=94.778 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=01:30 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.625 DataTime=0.399 Loss=0.712 Prec@1=81.090 Prec@5=94.778 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.623 DataTime=0.398 Loss=0.711 Prec@1=81.088 Prec@5=94.810 rate=1.63 Hz, eta=0:21:29, total=0:04:05, wall=01:31 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.623 DataTime=0.398 Loss=0.711 Prec@1=81.088 Prec@5=94.810 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:31 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.623 DataTime=0.398 Loss=0.711 Prec@1=81.088 Prec@5=94.810 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:32 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.621 DataTime=0.396 Loss=0.710 Prec@1=81.105 Prec@5=94.826 rate=1.63 Hz, eta=0:20:27, total=0:05:07, wall=01:32 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.621 DataTime=0.396 Loss=0.710 Prec@1=81.105 Prec@5=94.826 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:32 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.621 DataTime=0.396 Loss=0.710 Prec@1=81.105 Prec@5=94.826 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:33 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.710 Prec@1=81.116 Prec@5=94.821 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=01:33 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.710 Prec@1=81.116 Prec@5=94.821 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:33 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.710 Prec@1=81.116 Prec@5=94.821 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:34 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.709 Prec@1=81.131 Prec@5=94.846 rate=1.63 Hz, eta=0:18:25, total=0:07:10, wall=01:34 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.709 Prec@1=81.131 Prec@5=94.846 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:34 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.620 DataTime=0.395 Loss=0.709 Prec@1=81.131 Prec@5=94.846 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:35 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.134 Prec@5=94.847 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=01:35 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.134 Prec@5=94.847 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:35 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.134 Prec@5=94.847 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:36 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.128 Prec@5=94.844 rate=1.63 Hz, eta=0:16:23, total=0:09:13, wall=01:36 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.128 Prec@5=94.844 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:36 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.619 DataTime=0.394 Loss=0.709 Prec@1=81.128 Prec@5=94.844 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:37 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.117 Prec@5=94.831 rate=1.63 Hz, eta=0:15:22, total=0:10:14, wall=01:37 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.117 Prec@5=94.831 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:37 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.117 Prec@5=94.831 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:38 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.110 Prec@5=94.833 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=01:38 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.110 Prec@5=94.833 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:38 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.710 Prec@1=81.110 Prec@5=94.833 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:39 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.709 Prec@1=81.125 Prec@5=94.839 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=01:39 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.709 Prec@1=81.125 Prec@5=94.839 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:39 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.618 DataTime=0.393 Loss=0.709 Prec@1=81.125 Prec@5=94.839 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:40 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.709 Prec@1=81.127 Prec@5=94.839 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=01:40 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.709 Prec@1=81.127 Prec@5=94.839 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:40 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.709 Prec@1=81.127 Prec@5=94.839 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:41 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.708 Prec@1=81.147 Prec@5=94.845 rate=1.63 Hz, eta=0:11:16, total=0:14:20, wall=01:41 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.708 Prec@1=81.147 Prec@5=94.845 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:41 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.708 Prec@1=81.147 Prec@5=94.845 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:42 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.126 Prec@5=94.836 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=01:42 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.126 Prec@5=94.836 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:42 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.126 Prec@5=94.836 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:43 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.118 Prec@5=94.827 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=01:43 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.118 Prec@5=94.827 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:43 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.118 Prec@5=94.827 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:44 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.128 Prec@5=94.824 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=01:44 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.128 Prec@5=94.824 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:44 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.128 Prec@5=94.824 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:45 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=01:45 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:45 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.710 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:46 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.108 Prec@5=94.819 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=01:46 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.108 Prec@5=94.819 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=01:46 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.108 Prec@5=94.819 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=01:47 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:05:08, total=0:20:29, wall=01:47 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:47 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.617 DataTime=0.392 Loss=0.711 Prec@1=81.109 Prec@5=94.823 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:48 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.710 Prec@1=81.108 Prec@5=94.826 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=01:48 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.710 Prec@1=81.108 Prec@5=94.826 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:48 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.710 Prec@1=81.108 Prec@5=94.826 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:49 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.711 Prec@1=81.093 Prec@5=94.818 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=01:49 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.711 Prec@1=81.093 Prec@5=94.818 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:49 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.392 Loss=0.711 Prec@1=81.093 Prec@5=94.818 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:50 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.081 Prec@5=94.816 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=01:50 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.081 Prec@5=94.816 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:50 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.081 Prec@5=94.816 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:51 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.079 Prec@5=94.812 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=01:51 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.079 Prec@5=94.812 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:51 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.079 Prec@5=94.812 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:51 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.078 Prec@5=94.812 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=01:51 IST=> training   100.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.616 DataTime=0.391 Loss=0.712 Prec@1=81.078 Prec@5=94.812 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=01:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> validation 0.00% of 1x98...Epoch=135/150 LR=0.00278 Time=6.555 Loss=1.109 Prec@1=72.852 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=6.555 Loss=1.109 Prec@1=72.852 Prec@5=91.797 rate=4161.81 Hz, eta=0:00:00, total=0:00:00, wall=01:52 IST** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=6.555 Loss=1.109 Prec@1=72.852 Prec@5=91.797 rate=4161.81 Hz, eta=0:00:00, total=0:00:00, wall=01:52 IST** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=0.403 Loss=1.081 Prec@1=73.656 Prec@5=91.726 rate=4161.81 Hz, eta=0:00:00, total=0:00:00, wall=01:52 IST** validation 100.00% of 1x98...Epoch=135/150 LR=0.00278 Time=0.403 Loss=1.081 Prec@1=73.656 Prec@5=91.726 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=01:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> training   0.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.874 DataTime=5.591 Loss=0.779 Prec@1=80.078 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=01:52 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.874 DataTime=5.591 Loss=0.779 Prec@1=80.078 Prec@5=92.969 rate=8242.26 Hz, eta=0:00:00, total=0:00:00, wall=01:52 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.874 DataTime=5.591 Loss=0.779 Prec@1=80.078 Prec@5=92.969 rate=8242.26 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.664 DataTime=0.439 Loss=0.703 Prec@1=81.401 Prec@5=94.887 rate=8242.26 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.664 DataTime=0.439 Loss=0.703 Prec@1=81.401 Prec@5=94.887 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=01:53 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.664 DataTime=0.439 Loss=0.703 Prec@1=81.401 Prec@5=94.887 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=01:54 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.638 DataTime=0.413 Loss=0.699 Prec@1=81.438 Prec@5=94.976 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=01:54 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.638 DataTime=0.413 Loss=0.699 Prec@1=81.438 Prec@5=94.976 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=01:54 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.638 DataTime=0.413 Loss=0.699 Prec@1=81.438 Prec@5=94.976 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=01:55 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.629 DataTime=0.405 Loss=0.699 Prec@1=81.474 Prec@5=94.961 rate=1.64 Hz, eta=0:23:21, total=0:02:02, wall=01:55 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.629 DataTime=0.405 Loss=0.699 Prec@1=81.474 Prec@5=94.961 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=01:55 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.629 DataTime=0.405 Loss=0.699 Prec@1=81.474 Prec@5=94.961 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=01:56 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.625 DataTime=0.401 Loss=0.700 Prec@1=81.420 Prec@5=94.930 rate=1.64 Hz, eta=0:22:21, total=0:03:03, wall=01:56 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.625 DataTime=0.401 Loss=0.700 Prec@1=81.420 Prec@5=94.930 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=01:56 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.625 DataTime=0.401 Loss=0.700 Prec@1=81.420 Prec@5=94.930 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=01:57 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.623 DataTime=0.399 Loss=0.701 Prec@1=81.412 Prec@5=94.916 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=01:57 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.623 DataTime=0.399 Loss=0.701 Prec@1=81.412 Prec@5=94.916 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=01:57 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.623 DataTime=0.399 Loss=0.701 Prec@1=81.412 Prec@5=94.916 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=01:58 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.621 DataTime=0.397 Loss=0.700 Prec@1=81.431 Prec@5=94.920 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=01:58 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.621 DataTime=0.397 Loss=0.700 Prec@1=81.431 Prec@5=94.920 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=01:58 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.621 DataTime=0.397 Loss=0.700 Prec@1=81.431 Prec@5=94.920 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=01:59 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.396 Loss=0.700 Prec@1=81.447 Prec@5=94.933 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=01:59 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.396 Loss=0.700 Prec@1=81.447 Prec@5=94.933 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=01:59 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.396 Loss=0.700 Prec@1=81.447 Prec@5=94.933 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=02:00 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.395 Loss=0.699 Prec@1=81.448 Prec@5=94.938 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=02:00 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.395 Loss=0.699 Prec@1=81.448 Prec@5=94.938 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=02:00 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.620 DataTime=0.395 Loss=0.699 Prec@1=81.448 Prec@5=94.938 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=02:01 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.619 DataTime=0.394 Loss=0.699 Prec@1=81.435 Prec@5=94.948 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=02:01 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.619 DataTime=0.394 Loss=0.699 Prec@1=81.435 Prec@5=94.948 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:01 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.619 DataTime=0.394 Loss=0.699 Prec@1=81.435 Prec@5=94.948 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:02 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.394 Loss=0.700 Prec@1=81.421 Prec@5=94.926 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:02 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.394 Loss=0.700 Prec@1=81.421 Prec@5=94.926 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=02:02 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.394 Loss=0.700 Prec@1=81.421 Prec@5=94.926 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=02:03 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.700 Prec@1=81.419 Prec@5=94.924 rate=1.63 Hz, eta=0:15:19, total=0:10:13, wall=02:03 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.700 Prec@1=81.419 Prec@5=94.924 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:03 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.700 Prec@1=81.419 Prec@5=94.924 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:04 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.701 Prec@1=81.409 Prec@5=94.920 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:04 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.701 Prec@1=81.409 Prec@5=94.920 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:04 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.618 DataTime=0.393 Loss=0.701 Prec@1=81.409 Prec@5=94.920 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:06 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.914 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:06 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.914 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=02:06 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.914 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=02:07 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.912 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=02:07 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.912 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=02:07 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.393 Loss=0.702 Prec@1=81.381 Prec@5=94.912 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=02:08 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.392 Loss=0.702 Prec@1=81.385 Prec@5=94.911 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=02:08 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.392 Loss=0.702 Prec@1=81.385 Prec@5=94.911 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=02:08 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.617 DataTime=0.392 Loss=0.702 Prec@1=81.385 Prec@5=94.911 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=02:09 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.386 Prec@5=94.916 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=02:09 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.386 Prec@5=94.916 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=02:09 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.386 Prec@5=94.916 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=02:10 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.361 Prec@5=94.913 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=02:10 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.361 Prec@5=94.913 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=02:10 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.702 Prec@1=81.361 Prec@5=94.913 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=02:11 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.337 Prec@5=94.907 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=02:11 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.337 Prec@5=94.907 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=02:11 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.337 Prec@5=94.907 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=02:12 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.333 Prec@5=94.903 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=02:12 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.333 Prec@5=94.903 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=02:12 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.333 Prec@5=94.903 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=02:13 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.312 Prec@5=94.899 rate=1.63 Hz, eta=0:06:08, total=0:19:25, wall=02:13 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.312 Prec@5=94.899 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=02:13 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.312 Prec@5=94.899 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=02:14 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.293 Prec@5=94.902 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=02:14 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.293 Prec@5=94.902 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=02:14 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.293 Prec@5=94.902 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=02:15 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.286 Prec@5=94.905 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=02:15 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.286 Prec@5=94.905 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=02:15 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.392 Loss=0.703 Prec@1=81.286 Prec@5=94.905 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=02:16 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.298 Prec@5=94.910 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=02:16 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.298 Prec@5=94.910 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=02:16 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.298 Prec@5=94.910 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=02:17 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.284 Prec@5=94.909 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=02:17 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.284 Prec@5=94.909 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:17 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.703 Prec@1=81.284 Prec@5=94.909 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:18 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.704 Prec@1=81.280 Prec@5=94.904 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:18 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.704 Prec@1=81.280 Prec@5=94.904 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=02:18 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.704 Prec@1=81.280 Prec@5=94.904 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=02:18 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.704 Prec@1=81.279 Prec@5=94.904 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=02:18 IST=> training   100.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.616 DataTime=0.391 Loss=0.704 Prec@1=81.279 Prec@5=94.904 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=02:18 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST=> validation 0.00% of 1x98...Epoch=136/150 LR=0.00245 Time=7.277 Loss=0.895 Prec@1=77.344 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST=> validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=7.277 Loss=0.895 Prec@1=77.344 Prec@5=93.359 rate=7566.12 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=7.277 Loss=0.895 Prec@1=77.344 Prec@5=93.359 rate=7566.12 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=0.412 Loss=1.070 Prec@1=73.826 Prec@5=91.798 rate=7566.12 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST** validation 100.00% of 1x98...Epoch=136/150 LR=0.00245 Time=0.412 Loss=1.070 Prec@1=73.826 Prec@5=91.798 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=02:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> training   0.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.933 DataTime=4.615 Loss=0.690 Prec@1=82.617 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=02:19 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.933 DataTime=4.615 Loss=0.690 Prec@1=82.617 Prec@5=94.727 rate=4005.96 Hz, eta=0:00:00, total=0:00:00, wall=02:19 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=4.933 DataTime=4.615 Loss=0.690 Prec@1=82.617 Prec@5=94.727 rate=4005.96 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.653 DataTime=0.427 Loss=0.694 Prec@1=81.478 Prec@5=94.906 rate=4005.96 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.653 DataTime=0.427 Loss=0.694 Prec@1=81.478 Prec@5=94.906 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=02:20 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.653 DataTime=0.427 Loss=0.694 Prec@1=81.478 Prec@5=94.906 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.634 DataTime=0.408 Loss=0.689 Prec@1=81.611 Prec@5=95.020 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=02:21 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.634 DataTime=0.408 Loss=0.689 Prec@1=81.611 Prec@5=95.020 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:21 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.634 DataTime=0.408 Loss=0.689 Prec@1=81.611 Prec@5=95.020 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:22 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.627 DataTime=0.402 Loss=0.688 Prec@1=81.664 Prec@5=95.032 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:22 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.627 DataTime=0.402 Loss=0.688 Prec@1=81.664 Prec@5=95.032 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=02:22 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.627 DataTime=0.402 Loss=0.688 Prec@1=81.664 Prec@5=95.032 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=02:23 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.624 DataTime=0.399 Loss=0.689 Prec@1=81.671 Prec@5=95.021 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=02:23 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.624 DataTime=0.399 Loss=0.689 Prec@1=81.671 Prec@5=95.021 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:23 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.624 DataTime=0.399 Loss=0.689 Prec@1=81.671 Prec@5=95.021 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:24 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.622 DataTime=0.397 Loss=0.688 Prec@1=81.685 Prec@5=95.017 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=02:24 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.622 DataTime=0.397 Loss=0.688 Prec@1=81.685 Prec@5=95.017 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:24 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.622 DataTime=0.397 Loss=0.688 Prec@1=81.685 Prec@5=95.017 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:25 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.396 Loss=0.689 Prec@1=81.665 Prec@5=95.023 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=02:25 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.396 Loss=0.689 Prec@1=81.665 Prec@5=95.023 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=02:25 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.396 Loss=0.689 Prec@1=81.665 Prec@5=95.023 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=02:26 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.395 Loss=0.689 Prec@1=81.660 Prec@5=95.011 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=02:26 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.395 Loss=0.689 Prec@1=81.660 Prec@5=95.011 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:26 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.620 DataTime=0.395 Loss=0.689 Prec@1=81.660 Prec@5=95.011 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:27 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.619 DataTime=0.394 Loss=0.690 Prec@1=81.641 Prec@5=95.004 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=02:27 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.619 DataTime=0.394 Loss=0.690 Prec@1=81.641 Prec@5=95.004 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:27 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.619 DataTime=0.394 Loss=0.690 Prec@1=81.641 Prec@5=95.004 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:28 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.394 Loss=0.690 Prec@1=81.645 Prec@5=95.019 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=02:28 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.394 Loss=0.690 Prec@1=81.645 Prec@5=95.019 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:28 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.394 Loss=0.690 Prec@1=81.645 Prec@5=95.019 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:29 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.646 Prec@5=95.016 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=02:29 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.646 Prec@5=95.016 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:29 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.646 Prec@5=95.016 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:30 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.610 Prec@5=95.021 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:30 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.610 Prec@5=95.021 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:30 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.618 DataTime=0.393 Loss=0.690 Prec@1=81.610 Prec@5=95.021 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:31 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.393 Loss=0.691 Prec@1=81.581 Prec@5=95.011 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=02:31 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.393 Loss=0.691 Prec@1=81.581 Prec@5=95.011 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:31 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.393 Loss=0.691 Prec@1=81.581 Prec@5=95.011 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:32 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.691 Prec@1=81.579 Prec@5=95.004 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=02:32 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.691 Prec@1=81.579 Prec@5=95.004 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=02:32 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.691 Prec@1=81.579 Prec@5=95.004 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=02:33 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.577 Prec@5=95.003 rate=1.63 Hz, eta=0:12:17, total=0:13:17, wall=02:33 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.577 Prec@5=95.003 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:33 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.577 Prec@5=95.003 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:34 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.561 Prec@5=94.994 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=02:34 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.561 Prec@5=94.994 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=02:34 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.692 Prec@1=81.561 Prec@5=94.994 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=02:35 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.693 Prec@1=81.546 Prec@5=94.993 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=02:35 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.693 Prec@1=81.546 Prec@5=94.993 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:35 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.617 DataTime=0.392 Loss=0.693 Prec@1=81.546 Prec@5=94.993 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:36 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.693 Prec@1=81.548 Prec@5=94.991 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=02:36 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.693 Prec@1=81.548 Prec@5=94.991 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=02:36 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.693 Prec@1=81.548 Prec@5=94.991 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=02:37 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.546 Prec@5=94.987 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=02:37 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.546 Prec@5=94.987 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:37 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.546 Prec@5=94.987 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:38 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.548 Prec@5=94.992 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=02:38 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.548 Prec@5=94.992 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:38 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.392 Loss=0.694 Prec@1=81.548 Prec@5=94.992 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:39 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.553 Prec@5=94.998 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=02:39 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.553 Prec@5=94.998 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:39 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.553 Prec@5=94.998 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:40 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.549 Prec@5=94.999 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=02:40 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.549 Prec@5=94.999 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:40 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.549 Prec@5=94.999 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:41 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.538 Prec@5=94.997 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=02:41 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.538 Prec@5=94.997 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:41 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.538 Prec@5=94.997 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:42 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.529 Prec@5=94.989 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=02:42 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.529 Prec@5=94.989 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:42 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.694 Prec@1=81.529 Prec@5=94.989 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:43 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.695 Prec@1=81.508 Prec@5=94.981 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=02:43 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.695 Prec@1=81.508 Prec@5=94.981 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:43 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.616 DataTime=0.391 Loss=0.695 Prec@1=81.508 Prec@5=94.981 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:44 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.615 DataTime=0.391 Loss=0.695 Prec@1=81.509 Prec@5=94.979 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=02:44 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.615 DataTime=0.391 Loss=0.695 Prec@1=81.509 Prec@5=94.979 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:44 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.615 DataTime=0.391 Loss=0.695 Prec@1=81.509 Prec@5=94.979 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:44 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.615 DataTime=0.391 Loss=0.695 Prec@1=81.509 Prec@5=94.979 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=02:44 IST=> training   100.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.615 DataTime=0.391 Loss=0.695 Prec@1=81.509 Prec@5=94.979 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=02:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:44 IST=> validation 0.00% of 1x98...Epoch=137/150 LR=0.00213 Time=5.684 Loss=1.059 Prec@1=75.195 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=02:44 IST=> validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=5.684 Loss=1.059 Prec@1=75.195 Prec@5=92.383 rate=4923.08 Hz, eta=0:00:00, total=0:00:00, wall=02:44 IST** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=5.684 Loss=1.059 Prec@1=75.195 Prec@5=92.383 rate=4923.08 Hz, eta=0:00:00, total=0:00:00, wall=02:45 IST** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=0.403 Loss=1.072 Prec@1=73.784 Prec@5=91.852 rate=4923.08 Hz, eta=0:00:00, total=0:00:00, wall=02:45 IST** validation 100.00% of 1x98...Epoch=137/150 LR=0.00213 Time=0.403 Loss=1.072 Prec@1=73.784 Prec@5=91.852 rate=2.90 Hz, eta=0:00:00, total=0:00:33, wall=02:45 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:45 IST=> training   0.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.595 DataTime=5.217 Loss=0.829 Prec@1=77.930 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=02:45 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.595 DataTime=5.217 Loss=0.829 Prec@1=77.930 Prec@5=92.188 rate=2724.23 Hz, eta=0:00:00, total=0:00:00, wall=02:45 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.595 DataTime=5.217 Loss=0.829 Prec@1=77.930 Prec@5=92.188 rate=2724.23 Hz, eta=0:00:00, total=0:00:00, wall=02:46 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.661 DataTime=0.435 Loss=0.691 Prec@1=81.710 Prec@5=94.978 rate=2724.23 Hz, eta=0:00:00, total=0:00:00, wall=02:46 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.661 DataTime=0.435 Loss=0.691 Prec@1=81.710 Prec@5=94.978 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=02:46 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.661 DataTime=0.435 Loss=0.691 Prec@1=81.710 Prec@5=94.978 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=02:47 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.637 DataTime=0.413 Loss=0.684 Prec@1=81.891 Prec@5=95.077 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=02:47 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.637 DataTime=0.413 Loss=0.684 Prec@1=81.891 Prec@5=95.077 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:47 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.637 DataTime=0.413 Loss=0.684 Prec@1=81.891 Prec@5=95.077 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:48 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.629 DataTime=0.405 Loss=0.685 Prec@1=81.868 Prec@5=95.067 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=02:48 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.629 DataTime=0.405 Loss=0.685 Prec@1=81.868 Prec@5=95.067 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=02:48 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.629 DataTime=0.405 Loss=0.685 Prec@1=81.868 Prec@5=95.067 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=02:49 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.625 DataTime=0.401 Loss=0.687 Prec@1=81.789 Prec@5=95.062 rate=1.64 Hz, eta=0:22:23, total=0:03:03, wall=02:49 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.625 DataTime=0.401 Loss=0.687 Prec@1=81.789 Prec@5=95.062 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=02:49 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.625 DataTime=0.401 Loss=0.687 Prec@1=81.789 Prec@5=95.062 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=02:50 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.622 DataTime=0.399 Loss=0.684 Prec@1=81.859 Prec@5=95.079 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=02:50 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.622 DataTime=0.399 Loss=0.684 Prec@1=81.859 Prec@5=95.079 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=02:50 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.622 DataTime=0.399 Loss=0.684 Prec@1=81.859 Prec@5=95.079 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=02:51 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.621 DataTime=0.397 Loss=0.684 Prec@1=81.833 Prec@5=95.102 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=02:51 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.621 DataTime=0.397 Loss=0.684 Prec@1=81.833 Prec@5=95.102 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=02:51 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.621 DataTime=0.397 Loss=0.684 Prec@1=81.833 Prec@5=95.102 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=02:52 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.620 DataTime=0.396 Loss=0.686 Prec@1=81.777 Prec@5=95.079 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=02:52 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.620 DataTime=0.396 Loss=0.686 Prec@1=81.777 Prec@5=95.079 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=02:52 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.620 DataTime=0.396 Loss=0.686 Prec@1=81.777 Prec@5=95.079 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=02:53 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.687 Prec@1=81.740 Prec@5=95.061 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=02:53 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.687 Prec@1=81.740 Prec@5=95.061 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=02:53 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.687 Prec@1=81.740 Prec@5=95.061 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=02:54 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.686 Prec@1=81.752 Prec@5=95.082 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=02:54 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.686 Prec@1=81.752 Prec@5=95.082 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:54 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.619 DataTime=0.395 Loss=0.686 Prec@1=81.752 Prec@5=95.082 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:55 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.685 Prec@1=81.778 Prec@5=95.092 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=02:55 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.685 Prec@1=81.778 Prec@5=95.092 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:55 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.685 Prec@1=81.778 Prec@5=95.092 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:56 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.686 Prec@1=81.766 Prec@5=95.077 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=02:56 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.686 Prec@1=81.766 Prec@5=95.077 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:56 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.618 DataTime=0.394 Loss=0.686 Prec@1=81.766 Prec@5=95.077 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:57 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.070 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=02:57 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.070 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:57 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.070 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:58 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.751 Prec@5=95.068 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=02:58 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.751 Prec@5=95.068 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=02:58 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.751 Prec@5=95.068 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=02:59 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.755 Prec@5=95.073 rate=1.63 Hz, eta=0:12:16, total=0:13:16, wall=02:59 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.755 Prec@5=95.073 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=02:59 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.617 DataTime=0.393 Loss=0.686 Prec@1=81.755 Prec@5=95.073 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:00 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.069 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:00 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.069 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=03:00 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.393 Loss=0.686 Prec@1=81.759 Prec@5=95.069 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=03:01 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.751 Prec@5=95.069 rate=1.63 Hz, eta=0:10:13, total=0:15:19, wall=03:01 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.751 Prec@5=95.069 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=03:01 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.751 Prec@5=95.069 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=03:02 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.070 rate=1.63 Hz, eta=0:09:12, total=0:16:20, wall=03:02 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.070 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:02 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.070 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:03 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.078 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=03:03 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.078 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:03 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.740 Prec@5=95.078 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:04 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.730 Prec@5=95.072 rate=1.63 Hz, eta=0:07:10, total=0:18:23, wall=03:04 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.730 Prec@5=95.072 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=03:04 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.730 Prec@5=95.072 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=03:05 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.722 Prec@5=95.074 rate=1.63 Hz, eta=0:06:08, total=0:19:24, wall=03:05 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.722 Prec@5=95.074 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:05 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.616 DataTime=0.392 Loss=0.687 Prec@1=81.722 Prec@5=95.074 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:06 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.392 Loss=0.687 Prec@1=81.720 Prec@5=95.077 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=03:06 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.392 Loss=0.687 Prec@1=81.720 Prec@5=95.077 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:06 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.392 Loss=0.687 Prec@1=81.720 Prec@5=95.077 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:07 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.081 rate=1.63 Hz, eta=0:04:06, total=0:21:27, wall=03:07 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.081 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=03:07 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.081 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=03:09 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.726 Prec@5=95.079 rate=1.63 Hz, eta=0:03:05, total=0:22:28, wall=03:09 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.726 Prec@5=95.079 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:09 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.726 Prec@5=95.079 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.077 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:10 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.077 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=03:10 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.721 Prec@5=95.077 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=03:11 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.720 Prec@5=95.071 rate=1.63 Hz, eta=0:01:02, total=0:24:31, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.720 Prec@5=95.071 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.720 Prec@5=95.071 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.720 Prec@5=95.072 rate=1.63 Hz, eta=0:00:01, total=0:25:32, wall=03:11 IST=> training   100.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.615 DataTime=0.391 Loss=0.687 Prec@1=81.720 Prec@5=95.072 rate=1.63 Hz, eta=0:00:00, total=0:25:33, wall=03:11 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 0.00% of 1x98...Epoch=138/150 LR=0.00184 Time=5.572 Loss=1.008 Prec@1=74.805 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=5.572 Loss=1.008 Prec@1=74.805 Prec@5=92.773 rate=4038.74 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=5.572 Loss=1.008 Prec@1=74.805 Prec@5=92.773 rate=4038.74 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=0.399 Loss=1.067 Prec@1=73.912 Prec@5=91.838 rate=4038.74 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST** validation 100.00% of 1x98...Epoch=138/150 LR=0.00184 Time=0.399 Loss=1.067 Prec@1=73.912 Prec@5=91.838 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=03:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> training   0.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.272 DataTime=4.983 Loss=0.702 Prec@1=79.688 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.272 DataTime=4.983 Loss=0.702 Prec@1=79.688 Prec@5=95.312 rate=6696.18 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.272 DataTime=4.983 Loss=0.702 Prec@1=79.688 Prec@5=95.312 rate=6696.18 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.661 DataTime=0.437 Loss=0.671 Prec@1=82.184 Prec@5=95.266 rate=6696.18 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.661 DataTime=0.437 Loss=0.671 Prec@1=82.184 Prec@5=95.266 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=03:12 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.661 DataTime=0.437 Loss=0.671 Prec@1=82.184 Prec@5=95.266 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=03:13 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.638 DataTime=0.414 Loss=0.677 Prec@1=82.006 Prec@5=95.186 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=03:13 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.638 DataTime=0.414 Loss=0.677 Prec@1=82.006 Prec@5=95.186 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=03:13 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.638 DataTime=0.414 Loss=0.677 Prec@1=82.006 Prec@5=95.186 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=03:14 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.629 DataTime=0.406 Loss=0.679 Prec@1=81.914 Prec@5=95.166 rate=1.63 Hz, eta=0:23:28, total=0:02:02, wall=03:14 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.629 DataTime=0.406 Loss=0.679 Prec@1=81.914 Prec@5=95.166 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=03:14 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.629 DataTime=0.406 Loss=0.679 Prec@1=81.914 Prec@5=95.166 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=03:15 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.625 DataTime=0.402 Loss=0.678 Prec@1=81.890 Prec@5=95.181 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=03:15 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.625 DataTime=0.402 Loss=0.678 Prec@1=81.890 Prec@5=95.181 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=03:15 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.625 DataTime=0.402 Loss=0.678 Prec@1=81.890 Prec@5=95.181 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=03:16 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.622 DataTime=0.399 Loss=0.678 Prec@1=81.873 Prec@5=95.202 rate=1.63 Hz, eta=0:21:25, total=0:04:05, wall=03:16 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.622 DataTime=0.399 Loss=0.678 Prec@1=81.873 Prec@5=95.202 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=03:16 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.622 DataTime=0.399 Loss=0.678 Prec@1=81.873 Prec@5=95.202 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=03:18 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.621 DataTime=0.398 Loss=0.679 Prec@1=81.869 Prec@5=95.186 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=03:18 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.621 DataTime=0.398 Loss=0.679 Prec@1=81.869 Prec@5=95.186 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=03:18 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.621 DataTime=0.398 Loss=0.679 Prec@1=81.869 Prec@5=95.186 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=03:19 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.620 DataTime=0.397 Loss=0.679 Prec@1=81.865 Prec@5=95.166 rate=1.63 Hz, eta=0:19:24, total=0:06:07, wall=03:19 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.620 DataTime=0.397 Loss=0.679 Prec@1=81.865 Prec@5=95.166 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=03:19 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.620 DataTime=0.397 Loss=0.679 Prec@1=81.865 Prec@5=95.166 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=03:20 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.619 DataTime=0.396 Loss=0.679 Prec@1=81.894 Prec@5=95.169 rate=1.63 Hz, eta=0:18:23, total=0:07:09, wall=03:20 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.619 DataTime=0.396 Loss=0.679 Prec@1=81.894 Prec@5=95.169 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:20 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.619 DataTime=0.396 Loss=0.679 Prec@1=81.894 Prec@5=95.169 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:21 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.395 Loss=0.679 Prec@1=81.885 Prec@5=95.154 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:21 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.395 Loss=0.679 Prec@1=81.885 Prec@5=95.154 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=03:21 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.395 Loss=0.679 Prec@1=81.885 Prec@5=95.154 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=03:22 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.394 Loss=0.678 Prec@1=81.892 Prec@5=95.159 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=03:22 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.394 Loss=0.678 Prec@1=81.892 Prec@5=95.159 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:22 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.618 DataTime=0.394 Loss=0.678 Prec@1=81.892 Prec@5=95.159 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:23 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.880 Prec@5=95.149 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:23 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.880 Prec@5=95.149 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=03:23 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.880 Prec@5=95.149 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=03:24 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.857 Prec@5=95.147 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=03:24 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.857 Prec@5=95.147 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=03:24 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.394 Loss=0.679 Prec@1=81.857 Prec@5=95.147 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=03:25 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.678 Prec@1=81.885 Prec@5=95.162 rate=1.63 Hz, eta=0:13:17, total=0:12:16, wall=03:25 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.678 Prec@1=81.885 Prec@5=95.162 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:25 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.678 Prec@1=81.885 Prec@5=95.162 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:26 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.876 Prec@5=95.163 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:26 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.876 Prec@5=95.163 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:26 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.876 Prec@5=95.163 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:27 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.867 Prec@5=95.164 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=03:27 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.867 Prec@5=95.164 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:27 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.679 Prec@1=81.867 Prec@5=95.164 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.680 Prec@1=81.857 Prec@5=95.161 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:28 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.680 Prec@1=81.857 Prec@5=95.161 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:28 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.617 DataTime=0.393 Loss=0.680 Prec@1=81.857 Prec@5=95.161 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:29 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.850 Prec@5=95.161 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=03:29 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.850 Prec@5=95.161 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:29 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.850 Prec@5=95.161 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:30 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.847 Prec@5=95.155 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:30 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.847 Prec@5=95.155 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:30 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.680 Prec@1=81.847 Prec@5=95.155 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:31 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.845 Prec@5=95.149 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:31 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.845 Prec@5=95.149 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=03:31 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.845 Prec@5=95.149 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=03:32 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.142 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=03:32 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.142 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:32 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.142 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:33 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.822 Prec@5=95.139 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:33 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.822 Prec@5=95.139 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:33 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.822 Prec@5=95.139 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:34 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.141 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=03:34 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.141 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:34 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.616 DataTime=0.392 Loss=0.681 Prec@1=81.828 Prec@5=95.141 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:35 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.821 Prec@5=95.136 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=03:35 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.821 Prec@5=95.136 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:35 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.821 Prec@5=95.136 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:36 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.823 Prec@5=95.137 rate=1.63 Hz, eta=0:02:03, total=0:23:30, wall=03:36 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.823 Prec@5=95.137 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:36 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.681 Prec@1=81.823 Prec@5=95.137 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:37 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.682 Prec@1=81.819 Prec@5=95.132 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=03:37 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.682 Prec@1=81.819 Prec@5=95.132 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:37 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.682 Prec@1=81.819 Prec@5=95.132 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:37 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.682 Prec@1=81.819 Prec@5=95.133 rate=1.63 Hz, eta=0:00:01, total=0:25:33, wall=03:37 IST=> training   100.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.615 DataTime=0.392 Loss=0.682 Prec@1=81.819 Prec@5=95.133 rate=1.63 Hz, eta=0:00:00, total=0:25:34, wall=03:37 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:37 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:37 IST=> validation 0.00% of 1x98...Epoch=139/150 LR=0.00157 Time=6.906 Loss=1.039 Prec@1=72.852 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=03:37 IST=> validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=6.906 Loss=1.039 Prec@1=72.852 Prec@5=92.578 rate=4976.78 Hz, eta=0:00:00, total=0:00:00, wall=03:37 IST** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=6.906 Loss=1.039 Prec@1=72.852 Prec@5=92.578 rate=4976.78 Hz, eta=0:00:00, total=0:00:00, wall=03:38 IST** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=0.402 Loss=1.069 Prec@1=73.968 Prec@5=91.792 rate=4976.78 Hz, eta=0:00:00, total=0:00:00, wall=03:38 IST** validation 100.00% of 1x98...Epoch=139/150 LR=0.00157 Time=0.402 Loss=1.069 Prec@1=73.968 Prec@5=91.792 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=03:38 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:38 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:38 IST=> training   0.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=5.511 DataTime=5.233 Loss=0.643 Prec@1=84.570 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=03:38 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=5.511 DataTime=5.233 Loss=0.643 Prec@1=84.570 Prec@5=94.141 rate=7640.01 Hz, eta=0:00:00, total=0:00:00, wall=03:38 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=5.511 DataTime=5.233 Loss=0.643 Prec@1=84.570 Prec@5=94.141 rate=7640.01 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.656 DataTime=0.434 Loss=0.676 Prec@1=82.018 Prec@5=95.224 rate=7640.01 Hz, eta=0:00:00, total=0:00:00, wall=03:39 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.656 DataTime=0.434 Loss=0.676 Prec@1=82.018 Prec@5=95.224 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=03:39 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.656 DataTime=0.434 Loss=0.676 Prec@1=82.018 Prec@5=95.224 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=03:40 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.635 DataTime=0.412 Loss=0.677 Prec@1=81.975 Prec@5=95.193 rate=1.66 Hz, eta=0:24:05, total=0:01:00, wall=03:40 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.635 DataTime=0.412 Loss=0.677 Prec@1=81.975 Prec@5=95.193 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=03:40 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.635 DataTime=0.412 Loss=0.677 Prec@1=81.975 Prec@5=95.193 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=03:41 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.628 DataTime=0.404 Loss=0.673 Prec@1=82.056 Prec@5=95.242 rate=1.64 Hz, eta=0:23:19, total=0:02:02, wall=03:41 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.628 DataTime=0.404 Loss=0.673 Prec@1=82.056 Prec@5=95.242 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=03:41 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.628 DataTime=0.404 Loss=0.673 Prec@1=82.056 Prec@5=95.242 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=03:42 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.624 DataTime=0.400 Loss=0.673 Prec@1=82.047 Prec@5=95.243 rate=1.64 Hz, eta=0:22:22, total=0:03:03, wall=03:42 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.624 DataTime=0.400 Loss=0.673 Prec@1=82.047 Prec@5=95.243 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=03:42 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.624 DataTime=0.400 Loss=0.673 Prec@1=82.047 Prec@5=95.243 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=03:43 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.622 DataTime=0.398 Loss=0.672 Prec@1=82.094 Prec@5=95.250 rate=1.64 Hz, eta=0:21:22, total=0:04:04, wall=03:43 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.622 DataTime=0.398 Loss=0.672 Prec@1=82.094 Prec@5=95.250 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=03:43 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.622 DataTime=0.398 Loss=0.672 Prec@1=82.094 Prec@5=95.250 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=03:44 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.063 Prec@5=95.222 rate=1.64 Hz, eta=0:20:23, total=0:05:06, wall=03:44 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.063 Prec@5=95.222 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=03:44 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.063 Prec@5=95.222 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.620 DataTime=0.396 Loss=0.675 Prec@1=82.011 Prec@5=95.186 rate=1.64 Hz, eta=0:19:23, total=0:06:07, wall=03:45 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.620 DataTime=0.396 Loss=0.675 Prec@1=82.011 Prec@5=95.186 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=03:45 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.620 DataTime=0.396 Loss=0.675 Prec@1=82.011 Prec@5=95.186 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=03:46 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.675 Prec@1=82.004 Prec@5=95.187 rate=1.63 Hz, eta=0:18:22, total=0:07:09, wall=03:46 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.675 Prec@1=82.004 Prec@5=95.187 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:46 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.675 Prec@1=82.004 Prec@5=95.187 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:47 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.676 Prec@1=82.003 Prec@5=95.174 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=03:47 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.676 Prec@1=82.003 Prec@5=95.174 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:47 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.619 DataTime=0.395 Loss=0.676 Prec@1=82.003 Prec@5=95.174 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:48 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.021 Prec@5=95.178 rate=1.63 Hz, eta=0:16:21, total=0:09:12, wall=03:48 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.021 Prec@5=95.178 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:48 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.021 Prec@5=95.178 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:49 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.038 Prec@5=95.177 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=03:49 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.038 Prec@5=95.177 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:49 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.038 Prec@5=95.177 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:50 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.018 Prec@5=95.181 rate=1.63 Hz, eta=0:14:19, total=0:11:14, wall=03:50 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.018 Prec@5=95.181 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:50 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.618 DataTime=0.394 Loss=0.675 Prec@1=82.018 Prec@5=95.181 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:51 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.675 Prec@1=82.049 Prec@5=95.190 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=03:51 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.675 Prec@1=82.049 Prec@5=95.190 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:51 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.675 Prec@1=82.049 Prec@5=95.190 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:52 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.018 Prec@5=95.174 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=03:52 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.018 Prec@5=95.174 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:52 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.018 Prec@5=95.174 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:53 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.010 Prec@5=95.164 rate=1.63 Hz, eta=0:11:15, total=0:14:19, wall=03:53 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.010 Prec@5=95.164 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:53 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.393 Loss=0.676 Prec@1=82.010 Prec@5=95.164 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:54 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.013 Prec@5=95.151 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=03:54 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.013 Prec@5=95.151 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:54 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.013 Prec@5=95.151 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:55 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.014 Prec@5=95.149 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=03:55 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.014 Prec@5=95.149 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:55 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.676 Prec@1=82.014 Prec@5=95.149 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:56 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.677 Prec@1=81.998 Prec@5=95.141 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=03:56 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.677 Prec@1=81.998 Prec@5=95.141 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:56 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.617 DataTime=0.392 Loss=0.677 Prec@1=81.998 Prec@5=95.141 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:57 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.995 Prec@5=95.137 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=03:57 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.995 Prec@5=95.137 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:57 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.995 Prec@5=95.137 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:58 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.990 Prec@5=95.130 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=03:58 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.990 Prec@5=95.130 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:58 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.990 Prec@5=95.130 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:59 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.983 Prec@5=95.132 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=03:59 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.983 Prec@5=95.132 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=03:59 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.983 Prec@5=95.132 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:00 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.985 Prec@5=95.136 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:00 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.985 Prec@5=95.136 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:00 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.677 Prec@1=81.985 Prec@5=95.136 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:01 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.975 Prec@5=95.137 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=04:01 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.975 Prec@5=95.137 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:01 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.975 Prec@5=95.137 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:02 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.971 Prec@5=95.137 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=04:02 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.971 Prec@5=95.137 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:02 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.971 Prec@5=95.137 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:03 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.967 Prec@5=95.139 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.967 Prec@5=95.139 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.967 Prec@5=95.139 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.967 Prec@5=95.139 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:03 IST=> training   100.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.616 DataTime=0.392 Loss=0.678 Prec@1=81.967 Prec@5=95.139 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=04:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 0.00% of 1x98...Epoch=140/150 LR=0.00132 Time=6.836 Loss=1.115 Prec@1=70.312 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.836 Loss=1.115 Prec@1=70.312 Prec@5=91.797 rate=6754.84 Hz, eta=0:00:00, total=0:00:00, wall=04:03 IST** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.836 Loss=1.115 Prec@1=70.312 Prec@5=91.797 rate=6754.84 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=0.404 Loss=1.063 Prec@1=73.874 Prec@5=91.912 rate=6754.84 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST** validation 100.00% of 1x98...Epoch=140/150 LR=0.00132 Time=0.404 Loss=1.063 Prec@1=73.874 Prec@5=91.912 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=04:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.665 DataTime=4.373 Loss=0.623 Prec@1=81.836 Prec@5=96.484 rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.665 DataTime=4.373 Loss=0.623 Prec@1=81.836 Prec@5=96.484 rate=2770.58 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=4.665 DataTime=4.373 Loss=0.623 Prec@1=81.836 Prec@5=96.484 rate=2770.58 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.653 DataTime=0.427 Loss=0.676 Prec@1=82.230 Prec@5=95.080 rate=2770.58 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.653 DataTime=0.427 Loss=0.676 Prec@1=82.230 Prec@5=95.080 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=04:05 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.653 DataTime=0.427 Loss=0.676 Prec@1=82.230 Prec@5=95.080 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=04:06 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.634 DataTime=0.409 Loss=0.676 Prec@1=82.099 Prec@5=95.124 rate=1.65 Hz, eta=0:24:19, total=0:01:01, wall=04:06 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.634 DataTime=0.409 Loss=0.676 Prec@1=82.099 Prec@5=95.124 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:06 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.634 DataTime=0.409 Loss=0.676 Prec@1=82.099 Prec@5=95.124 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:07 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.627 DataTime=0.403 Loss=0.675 Prec@1=82.046 Prec@5=95.106 rate=1.64 Hz, eta=0:23:26, total=0:02:02, wall=04:07 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.627 DataTime=0.403 Loss=0.675 Prec@1=82.046 Prec@5=95.106 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=04:07 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.627 DataTime=0.403 Loss=0.675 Prec@1=82.046 Prec@5=95.106 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=04:08 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.624 DataTime=0.399 Loss=0.673 Prec@1=82.098 Prec@5=95.163 rate=1.63 Hz, eta=0:22:27, total=0:03:04, wall=04:08 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.624 DataTime=0.399 Loss=0.673 Prec@1=82.098 Prec@5=95.163 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:08 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.624 DataTime=0.399 Loss=0.673 Prec@1=82.098 Prec@5=95.163 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:09 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.105 Prec@5=95.166 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:09 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.105 Prec@5=95.166 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=04:09 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.621 DataTime=0.397 Loss=0.673 Prec@1=82.105 Prec@5=95.166 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=04:10 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.620 DataTime=0.396 Loss=0.673 Prec@1=82.097 Prec@5=95.161 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=04:10 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.620 DataTime=0.396 Loss=0.673 Prec@1=82.097 Prec@5=95.161 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:10 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.620 DataTime=0.396 Loss=0.673 Prec@1=82.097 Prec@5=95.161 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:11 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.395 Loss=0.673 Prec@1=82.091 Prec@5=95.172 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=04:11 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.395 Loss=0.673 Prec@1=82.091 Prec@5=95.172 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:11 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.395 Loss=0.673 Prec@1=82.091 Prec@5=95.172 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:12 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.394 Loss=0.674 Prec@1=82.071 Prec@5=95.170 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:12 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.394 Loss=0.674 Prec@1=82.071 Prec@5=95.170 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:12 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.619 DataTime=0.394 Loss=0.674 Prec@1=82.071 Prec@5=95.170 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:13 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.082 Prec@5=95.187 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=04:13 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.082 Prec@5=95.187 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:13 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.082 Prec@5=95.187 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:14 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.078 Prec@5=95.189 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:14 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.078 Prec@5=95.189 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:14 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.394 Loss=0.674 Prec@1=82.078 Prec@5=95.189 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:15 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.393 Loss=0.673 Prec@1=82.075 Prec@5=95.207 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=04:15 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.393 Loss=0.673 Prec@1=82.075 Prec@5=95.207 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:15 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.618 DataTime=0.393 Loss=0.673 Prec@1=82.075 Prec@5=95.207 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:16 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.673 Prec@1=82.085 Prec@5=95.213 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:16 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.673 Prec@1=82.085 Prec@5=95.213 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:16 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.673 Prec@1=82.085 Prec@5=95.213 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:17 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.081 Prec@5=95.212 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=04:17 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.081 Prec@5=95.212 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:17 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.081 Prec@5=95.212 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:18 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.068 Prec@5=95.213 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:18 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.068 Prec@5=95.213 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:18 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.393 Loss=0.672 Prec@1=82.068 Prec@5=95.213 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:20 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.673 Prec@1=82.058 Prec@5=95.206 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:20 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.673 Prec@1=82.058 Prec@5=95.206 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:20 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.673 Prec@1=82.058 Prec@5=95.206 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:21 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.081 Prec@5=95.224 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=04:21 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.081 Prec@5=95.224 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:21 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.081 Prec@5=95.224 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:22 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.097 Prec@5=95.219 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:22 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.097 Prec@5=95.219 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:22 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.617 DataTime=0.392 Loss=0.671 Prec@1=82.097 Prec@5=95.219 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:23 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.094 Prec@5=95.215 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:23 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.094 Prec@5=95.215 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:23 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.094 Prec@5=95.215 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:24 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.088 Prec@5=95.215 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:24 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.088 Prec@5=95.215 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:24 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.088 Prec@5=95.215 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:25 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.083 Prec@5=95.217 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=04:25 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.083 Prec@5=95.217 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:25 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.083 Prec@5=95.217 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:26 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.080 Prec@5=95.223 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:26 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.080 Prec@5=95.223 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:26 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.080 Prec@5=95.223 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:27 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.076 Prec@5=95.223 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=04:27 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.076 Prec@5=95.223 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:27 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.672 Prec@1=82.076 Prec@5=95.223 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:28 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.078 Prec@5=95.224 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:28 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.078 Prec@5=95.224 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:28 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.392 Loss=0.671 Prec@1=82.078 Prec@5=95.224 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:29 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.671 Prec@1=82.076 Prec@5=95.226 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:29 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.671 Prec@1=82.076 Prec@5=95.226 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:29 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.671 Prec@1=82.076 Prec@5=95.226 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:30 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.672 Prec@1=82.068 Prec@5=95.222 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:30 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.672 Prec@1=82.068 Prec@5=95.222 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:30 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.672 Prec@1=82.068 Prec@5=95.222 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:30 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.672 Prec@1=82.067 Prec@5=95.222 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:30 IST=> training   100.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.616 DataTime=0.391 Loss=0.672 Prec@1=82.067 Prec@5=95.222 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=04:30 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST=> validation 0.00% of 1x98...Epoch=141/150 LR=0.00109 Time=7.397 Loss=1.046 Prec@1=72.656 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST=> validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=7.397 Loss=1.046 Prec@1=72.656 Prec@5=91.992 rate=4827.86 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=7.397 Loss=1.046 Prec@1=72.656 Prec@5=91.992 rate=4827.86 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=0.413 Loss=1.063 Prec@1=74.132 Prec@5=91.848 rate=4827.86 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST** validation 100.00% of 1x98...Epoch=141/150 LR=0.00109 Time=0.413 Loss=1.063 Prec@1=74.132 Prec@5=91.848 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=04:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> training   0.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.916 DataTime=5.633 Loss=0.766 Prec@1=80.273 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=04:31 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.916 DataTime=5.633 Loss=0.766 Prec@1=80.273 Prec@5=93.750 rate=7497.88 Hz, eta=0:00:00, total=0:00:00, wall=04:31 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.916 DataTime=5.633 Loss=0.766 Prec@1=80.273 Prec@5=93.750 rate=7497.88 Hz, eta=0:00:00, total=0:00:00, wall=04:32 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.663 DataTime=0.436 Loss=0.665 Prec@1=82.211 Prec@5=95.359 rate=7497.88 Hz, eta=0:00:00, total=0:00:00, wall=04:32 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.663 DataTime=0.436 Loss=0.665 Prec@1=82.211 Prec@5=95.359 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=04:32 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.663 DataTime=0.436 Loss=0.665 Prec@1=82.211 Prec@5=95.359 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=04:33 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.639 DataTime=0.413 Loss=0.667 Prec@1=82.234 Prec@5=95.300 rate=1.66 Hz, eta=0:24:10, total=0:01:01, wall=04:33 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.639 DataTime=0.413 Loss=0.667 Prec@1=82.234 Prec@5=95.300 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=04:33 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.639 DataTime=0.413 Loss=0.667 Prec@1=82.234 Prec@5=95.300 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=04:34 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.631 DataTime=0.406 Loss=0.666 Prec@1=82.263 Prec@5=95.331 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=04:34 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.631 DataTime=0.406 Loss=0.666 Prec@1=82.263 Prec@5=95.331 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:34 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.631 DataTime=0.406 Loss=0.666 Prec@1=82.263 Prec@5=95.331 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.627 DataTime=0.402 Loss=0.667 Prec@1=82.257 Prec@5=95.283 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=04:35 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.627 DataTime=0.402 Loss=0.667 Prec@1=82.257 Prec@5=95.283 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:35 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.627 DataTime=0.402 Loss=0.667 Prec@1=82.257 Prec@5=95.283 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:36 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.624 DataTime=0.399 Loss=0.668 Prec@1=82.240 Prec@5=95.285 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=04:36 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.624 DataTime=0.399 Loss=0.668 Prec@1=82.240 Prec@5=95.285 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:36 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.624 DataTime=0.399 Loss=0.668 Prec@1=82.240 Prec@5=95.285 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:37 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.623 DataTime=0.398 Loss=0.666 Prec@1=82.252 Prec@5=95.305 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=04:37 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.623 DataTime=0.398 Loss=0.666 Prec@1=82.252 Prec@5=95.305 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=04:37 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.623 DataTime=0.398 Loss=0.666 Prec@1=82.252 Prec@5=95.305 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=04:38 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.622 DataTime=0.397 Loss=0.666 Prec@1=82.228 Prec@5=95.304 rate=1.63 Hz, eta=0:19:26, total=0:06:08, wall=04:38 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.622 DataTime=0.397 Loss=0.666 Prec@1=82.228 Prec@5=95.304 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:38 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.622 DataTime=0.397 Loss=0.666 Prec@1=82.228 Prec@5=95.304 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:39 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.621 DataTime=0.396 Loss=0.667 Prec@1=82.209 Prec@5=95.290 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=04:39 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.621 DataTime=0.396 Loss=0.667 Prec@1=82.209 Prec@5=95.290 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=04:39 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.621 DataTime=0.396 Loss=0.667 Prec@1=82.209 Prec@5=95.290 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=04:40 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.620 DataTime=0.395 Loss=0.665 Prec@1=82.259 Prec@5=95.305 rate=1.63 Hz, eta=0:17:24, total=0:08:11, wall=04:40 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.620 DataTime=0.395 Loss=0.665 Prec@1=82.259 Prec@5=95.305 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:40 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.620 DataTime=0.395 Loss=0.665 Prec@1=82.259 Prec@5=95.305 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:41 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.395 Loss=0.667 Prec@1=82.233 Prec@5=95.292 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=04:41 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.395 Loss=0.667 Prec@1=82.233 Prec@5=95.292 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=04:41 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.395 Loss=0.667 Prec@1=82.233 Prec@5=95.292 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=04:42 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.232 Prec@5=95.281 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=04:42 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.232 Prec@5=95.281 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:42 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.232 Prec@5=95.281 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:43 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.227 Prec@5=95.268 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=04:43 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.227 Prec@5=95.268 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:43 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.619 DataTime=0.394 Loss=0.667 Prec@1=82.227 Prec@5=95.268 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:44 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.394 Loss=0.667 Prec@1=82.241 Prec@5=95.275 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=04:44 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.394 Loss=0.667 Prec@1=82.241 Prec@5=95.275 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:44 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.394 Loss=0.667 Prec@1=82.241 Prec@5=95.275 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:45 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.668 Prec@1=82.228 Prec@5=95.271 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=04:45 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.668 Prec@1=82.228 Prec@5=95.271 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:45 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.668 Prec@1=82.228 Prec@5=95.271 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:46 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.242 Prec@5=95.274 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=04:46 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.242 Prec@5=95.274 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=04:46 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.242 Prec@5=95.274 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=04:47 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.227 Prec@5=95.265 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=04:47 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.227 Prec@5=95.265 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:47 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.618 DataTime=0.393 Loss=0.667 Prec@1=82.227 Prec@5=95.265 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:48 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.393 Loss=0.667 Prec@1=82.216 Prec@5=95.265 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=04:48 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.393 Loss=0.667 Prec@1=82.216 Prec@5=95.265 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:48 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.393 Loss=0.667 Prec@1=82.216 Prec@5=95.265 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:49 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.208 Prec@5=95.260 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=04:49 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.208 Prec@5=95.260 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:49 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.208 Prec@5=95.260 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:50 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.193 Prec@5=95.256 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=04:50 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.193 Prec@5=95.256 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=04:50 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.193 Prec@5=95.256 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=04:51 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.204 Prec@5=95.256 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=04:51 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.204 Prec@5=95.256 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:51 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.204 Prec@5=95.256 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:52 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.203 Prec@5=95.256 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=04:52 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.203 Prec@5=95.256 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:52 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.203 Prec@5=95.256 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:53 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.191 Prec@5=95.249 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=04:53 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.191 Prec@5=95.249 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:53 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.191 Prec@5=95.249 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:54 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.182 Prec@5=95.253 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=04:54 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.182 Prec@5=95.253 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:54 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.668 Prec@1=82.182 Prec@5=95.253 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:55 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.669 Prec@1=82.180 Prec@5=95.254 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=04:55 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.669 Prec@1=82.180 Prec@5=95.254 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:55 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.617 DataTime=0.392 Loss=0.669 Prec@1=82.180 Prec@5=95.254 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:56 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.616 DataTime=0.392 Loss=0.668 Prec@1=82.183 Prec@5=95.261 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=04:56 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.616 DataTime=0.392 Loss=0.668 Prec@1=82.183 Prec@5=95.261 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:56 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.616 DataTime=0.392 Loss=0.668 Prec@1=82.183 Prec@5=95.261 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:56 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.616 DataTime=0.392 Loss=0.668 Prec@1=82.182 Prec@5=95.261 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=04:56 IST=> training   100.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.616 DataTime=0.392 Loss=0.668 Prec@1=82.182 Prec@5=95.261 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=04:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:56 IST=> validation 0.00% of 1x98...Epoch=142/150 LR=0.00089 Time=7.061 Loss=1.017 Prec@1=76.562 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=04:56 IST=> validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=7.061 Loss=1.017 Prec@1=76.562 Prec@5=91.602 rate=5383.46 Hz, eta=0:00:00, total=0:00:00, wall=04:56 IST** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=7.061 Loss=1.017 Prec@1=76.562 Prec@5=91.602 rate=5383.46 Hz, eta=0:00:00, total=0:00:00, wall=04:57 IST** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=0.402 Loss=1.063 Prec@1=74.104 Prec@5=91.928 rate=5383.46 Hz, eta=0:00:00, total=0:00:00, wall=04:57 IST** validation 100.00% of 1x98...Epoch=142/150 LR=0.00089 Time=0.402 Loss=1.063 Prec@1=74.104 Prec@5=91.928 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=04:57 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:57 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:57 IST=> training   0.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=5.793 DataTime=5.507 Loss=0.588 Prec@1=86.133 Prec@5=96.094 rate=0 Hz, eta=?, total=0:00:00, wall=04:57 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=5.793 DataTime=5.507 Loss=0.588 Prec@1=86.133 Prec@5=96.094 rate=6930.11 Hz, eta=0:00:00, total=0:00:00, wall=04:57 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=5.793 DataTime=5.507 Loss=0.588 Prec@1=86.133 Prec@5=96.094 rate=6930.11 Hz, eta=0:00:00, total=0:00:00, wall=04:58 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.663 DataTime=0.436 Loss=0.664 Prec@1=82.368 Prec@5=95.351 rate=6930.11 Hz, eta=0:00:00, total=0:00:00, wall=04:58 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.663 DataTime=0.436 Loss=0.664 Prec@1=82.368 Prec@5=95.351 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=04:58 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.663 DataTime=0.436 Loss=0.664 Prec@1=82.368 Prec@5=95.351 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=04:59 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.639 DataTime=0.413 Loss=0.658 Prec@1=82.514 Prec@5=95.366 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=04:59 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.639 DataTime=0.413 Loss=0.658 Prec@1=82.514 Prec@5=95.366 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=04:59 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.639 DataTime=0.413 Loss=0.658 Prec@1=82.514 Prec@5=95.366 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=05:00 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.631 DataTime=0.405 Loss=0.660 Prec@1=82.429 Prec@5=95.364 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=05:00 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.631 DataTime=0.405 Loss=0.660 Prec@1=82.429 Prec@5=95.364 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:00 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.631 DataTime=0.405 Loss=0.660 Prec@1=82.429 Prec@5=95.364 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:01 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.627 DataTime=0.401 Loss=0.661 Prec@1=82.401 Prec@5=95.348 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:01 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.627 DataTime=0.401 Loss=0.661 Prec@1=82.401 Prec@5=95.348 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:01 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.627 DataTime=0.401 Loss=0.661 Prec@1=82.401 Prec@5=95.348 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:02 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.624 DataTime=0.399 Loss=0.660 Prec@1=82.450 Prec@5=95.344 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:02 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.624 DataTime=0.399 Loss=0.660 Prec@1=82.450 Prec@5=95.344 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:02 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.624 DataTime=0.399 Loss=0.660 Prec@1=82.450 Prec@5=95.344 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:03 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.623 DataTime=0.398 Loss=0.661 Prec@1=82.405 Prec@5=95.325 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:03 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.623 DataTime=0.398 Loss=0.661 Prec@1=82.405 Prec@5=95.325 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:03 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.623 DataTime=0.398 Loss=0.661 Prec@1=82.405 Prec@5=95.325 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:04 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.397 Loss=0.662 Prec@1=82.400 Prec@5=95.299 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:04 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.397 Loss=0.662 Prec@1=82.400 Prec@5=95.299 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:04 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.397 Loss=0.662 Prec@1=82.400 Prec@5=95.299 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:05 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.396 Loss=0.663 Prec@1=82.365 Prec@5=95.306 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:05 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.396 Loss=0.663 Prec@1=82.365 Prec@5=95.306 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:05 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.621 DataTime=0.396 Loss=0.663 Prec@1=82.365 Prec@5=95.306 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:06 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.620 DataTime=0.395 Loss=0.663 Prec@1=82.344 Prec@5=95.292 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:06 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.620 DataTime=0.395 Loss=0.663 Prec@1=82.344 Prec@5=95.292 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:06 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.620 DataTime=0.395 Loss=0.663 Prec@1=82.344 Prec@5=95.292 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:07 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.343 Prec@5=95.287 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:07 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.343 Prec@5=95.287 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:07 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.343 Prec@5=95.287 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:08 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.361 Prec@5=95.288 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:08 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.361 Prec@5=95.288 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:08 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.619 DataTime=0.394 Loss=0.663 Prec@1=82.361 Prec@5=95.288 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:09 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.327 Prec@5=95.284 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:09 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.327 Prec@5=95.284 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:09 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.327 Prec@5=95.284 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:10 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.319 Prec@5=95.282 rate=1.63 Hz, eta=0:13:19, total=0:12:17, wall=05:10 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.319 Prec@5=95.282 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:10 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.319 Prec@5=95.282 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:11 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.316 Prec@5=95.286 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:11 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.316 Prec@5=95.286 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:11 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.393 Loss=0.664 Prec@1=82.316 Prec@5=95.286 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:12 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.392 Loss=0.664 Prec@1=82.305 Prec@5=95.285 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:12 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.392 Loss=0.664 Prec@1=82.305 Prec@5=95.285 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:12 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.618 DataTime=0.392 Loss=0.664 Prec@1=82.305 Prec@5=95.285 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:13 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.303 Prec@5=95.288 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:13 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.303 Prec@5=95.288 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:13 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.303 Prec@5=95.288 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:14 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.294 Prec@5=95.288 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:14 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.294 Prec@5=95.288 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:14 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.294 Prec@5=95.288 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:15 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.299 Prec@5=95.293 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:15 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.299 Prec@5=95.293 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:15 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.664 Prec@1=82.299 Prec@5=95.293 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:16 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.293 Prec@5=95.287 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=05:16 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.293 Prec@5=95.287 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:16 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.293 Prec@5=95.287 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:18 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.287 Prec@5=95.289 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:18 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.287 Prec@5=95.289 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:18 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.287 Prec@5=95.289 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:19 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.284 Prec@5=95.290 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:19 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.284 Prec@5=95.290 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:19 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.284 Prec@5=95.290 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:20 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.272 Prec@5=95.287 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:20 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.272 Prec@5=95.287 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:20 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.392 Loss=0.665 Prec@1=82.272 Prec@5=95.287 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:21 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.391 Loss=0.665 Prec@1=82.280 Prec@5=95.294 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:21 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.391 Loss=0.665 Prec@1=82.280 Prec@5=95.294 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:21 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.617 DataTime=0.391 Loss=0.665 Prec@1=82.280 Prec@5=95.294 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:22 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.282 Prec@5=95.292 rate=1.63 Hz, eta=0:02:04, total=0:23:32, wall=05:22 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.282 Prec@5=95.292 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:22 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.282 Prec@5=95.292 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:23 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.268 Prec@5=95.288 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:23 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.268 Prec@5=95.288 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:23 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.268 Prec@5=95.288 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:23 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.267 Prec@5=95.288 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=05:23 IST=> training   100.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.616 DataTime=0.391 Loss=0.665 Prec@1=82.267 Prec@5=95.288 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=05:23 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> validation 0.00% of 1x98...Epoch=143/150 LR=0.00070 Time=7.148 Loss=1.098 Prec@1=75.195 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.148 Loss=1.098 Prec@1=75.195 Prec@5=92.969 rate=2975.19 Hz, eta=0:00:00, total=0:00:00, wall=05:23 IST** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.148 Loss=1.098 Prec@1=75.195 Prec@5=92.969 rate=2975.19 Hz, eta=0:00:00, total=0:00:00, wall=05:23 IST** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=0.405 Loss=1.063 Prec@1=74.102 Prec@5=91.906 rate=2975.19 Hz, eta=0:00:00, total=0:00:00, wall=05:23 IST** validation 100.00% of 1x98...Epoch=143/150 LR=0.00070 Time=0.405 Loss=1.063 Prec@1=74.102 Prec@5=91.906 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=05:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> training   0.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.676 DataTime=5.381 Loss=0.558 Prec@1=83.203 Prec@5=96.680 rate=0 Hz, eta=?, total=0:00:00, wall=05:23 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.676 DataTime=5.381 Loss=0.558 Prec@1=83.203 Prec@5=96.680 rate=7529.44 Hz, eta=0:00:00, total=0:00:00, wall=05:23 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.676 DataTime=5.381 Loss=0.558 Prec@1=83.203 Prec@5=96.680 rate=7529.44 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.662 DataTime=0.435 Loss=0.660 Prec@1=82.495 Prec@5=95.183 rate=7529.44 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.662 DataTime=0.435 Loss=0.660 Prec@1=82.495 Prec@5=95.183 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=05:24 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.662 DataTime=0.435 Loss=0.660 Prec@1=82.495 Prec@5=95.183 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=05:25 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.638 DataTime=0.413 Loss=0.660 Prec@1=82.449 Prec@5=95.260 rate=1.65 Hz, eta=0:24:15, total=0:01:01, wall=05:25 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.638 DataTime=0.413 Loss=0.660 Prec@1=82.449 Prec@5=95.260 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=05:25 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.638 DataTime=0.413 Loss=0.660 Prec@1=82.449 Prec@5=95.260 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=05:27 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.630 DataTime=0.405 Loss=0.659 Prec@1=82.488 Prec@5=95.321 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=05:27 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.630 DataTime=0.405 Loss=0.659 Prec@1=82.488 Prec@5=95.321 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:27 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.630 DataTime=0.405 Loss=0.659 Prec@1=82.488 Prec@5=95.321 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:28 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.626 DataTime=0.401 Loss=0.660 Prec@1=82.476 Prec@5=95.305 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=05:28 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.626 DataTime=0.401 Loss=0.660 Prec@1=82.476 Prec@5=95.305 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:28 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.626 DataTime=0.401 Loss=0.660 Prec@1=82.476 Prec@5=95.305 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:29 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.624 DataTime=0.399 Loss=0.661 Prec@1=82.453 Prec@5=95.286 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:29 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.624 DataTime=0.399 Loss=0.661 Prec@1=82.453 Prec@5=95.286 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:29 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.624 DataTime=0.399 Loss=0.661 Prec@1=82.453 Prec@5=95.286 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:30 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.622 DataTime=0.397 Loss=0.661 Prec@1=82.421 Prec@5=95.299 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:30 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.622 DataTime=0.397 Loss=0.661 Prec@1=82.421 Prec@5=95.299 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:30 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.622 DataTime=0.397 Loss=0.661 Prec@1=82.421 Prec@5=95.299 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:31 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.621 DataTime=0.396 Loss=0.662 Prec@1=82.393 Prec@5=95.290 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:31 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.621 DataTime=0.396 Loss=0.662 Prec@1=82.393 Prec@5=95.290 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:31 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.621 DataTime=0.396 Loss=0.662 Prec@1=82.393 Prec@5=95.290 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:32 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.396 Loss=0.661 Prec@1=82.419 Prec@5=95.309 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:32 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.396 Loss=0.661 Prec@1=82.419 Prec@5=95.309 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:32 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.396 Loss=0.661 Prec@1=82.419 Prec@5=95.309 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:33 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.395 Loss=0.660 Prec@1=82.417 Prec@5=95.328 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:33 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.395 Loss=0.660 Prec@1=82.417 Prec@5=95.328 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:33 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.620 DataTime=0.395 Loss=0.660 Prec@1=82.417 Prec@5=95.328 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:34 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.414 Prec@5=95.321 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:34 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.414 Prec@5=95.321 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:34 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.414 Prec@5=95.321 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:35 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.437 Prec@5=95.329 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=05:35 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.437 Prec@5=95.329 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:35 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.619 DataTime=0.394 Loss=0.660 Prec@1=82.437 Prec@5=95.329 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:36 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.394 Loss=0.660 Prec@1=82.439 Prec@5=95.321 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=05:36 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.394 Loss=0.660 Prec@1=82.439 Prec@5=95.321 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:36 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.394 Loss=0.660 Prec@1=82.439 Prec@5=95.321 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:37 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=05:37 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:37 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:38 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.433 Prec@5=95.292 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=05:38 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.433 Prec@5=95.292 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:38 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.433 Prec@5=95.292 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:39 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.437 Prec@5=95.295 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=05:39 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.437 Prec@5=95.295 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:39 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.618 DataTime=0.393 Loss=0.661 Prec@1=82.437 Prec@5=95.295 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:40 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.393 Loss=0.661 Prec@1=82.426 Prec@5=95.301 rate=1.63 Hz, eta=0:10:15, total=0:15:21, wall=05:40 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.393 Loss=0.661 Prec@1=82.426 Prec@5=95.301 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:40 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.393 Loss=0.661 Prec@1=82.426 Prec@5=95.301 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:41 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=05:41 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:41 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.429 Prec@5=95.305 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:42 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.425 Prec@5=95.307 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=05:42 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.425 Prec@5=95.307 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=05:42 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.425 Prec@5=95.307 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.423 Prec@5=95.312 rate=1.63 Hz, eta=0:07:11, total=0:18:25, wall=05:43 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.423 Prec@5=95.312 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:43 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.423 Prec@5=95.312 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:44 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.414 Prec@5=95.313 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=05:44 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.414 Prec@5=95.313 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:44 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.414 Prec@5=95.313 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:45 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.421 Prec@5=95.318 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=05:45 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.421 Prec@5=95.318 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:45 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.421 Prec@5=95.318 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:46 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.416 Prec@5=95.320 rate=1.63 Hz, eta=0:04:06, total=0:21:30, wall=05:46 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.416 Prec@5=95.320 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:46 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.416 Prec@5=95.320 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:47 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.408 Prec@5=95.319 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=05:47 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.408 Prec@5=95.319 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:47 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.408 Prec@5=95.319 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:48 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.405 Prec@5=95.315 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=05:48 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.405 Prec@5=95.315 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:48 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.617 DataTime=0.392 Loss=0.661 Prec@1=82.405 Prec@5=95.315 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:49 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.616 DataTime=0.391 Loss=0.661 Prec@1=82.403 Prec@5=95.312 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=05:49 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.616 DataTime=0.391 Loss=0.661 Prec@1=82.403 Prec@5=95.312 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=05:49 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.616 DataTime=0.391 Loss=0.661 Prec@1=82.403 Prec@5=95.312 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=05:49 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.616 DataTime=0.391 Loss=0.661 Prec@1=82.402 Prec@5=95.311 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=05:49 IST=> training   100.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.616 DataTime=0.391 Loss=0.661 Prec@1=82.402 Prec@5=95.311 rate=1.63 Hz, eta=0:00:00, total=0:25:37, wall=05:49 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:49 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:49 IST=> validation 0.00% of 1x98...Epoch=144/150 LR=0.00054 Time=7.087 Loss=0.955 Prec@1=75.781 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=05:49 IST=> validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=7.087 Loss=0.955 Prec@1=75.781 Prec@5=91.797 rate=4171.39 Hz, eta=0:00:00, total=0:00:00, wall=05:49 IST** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=7.087 Loss=0.955 Prec@1=75.781 Prec@5=91.797 rate=4171.39 Hz, eta=0:00:00, total=0:00:00, wall=05:50 IST** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=0.414 Loss=1.062 Prec@1=74.098 Prec@5=91.890 rate=4171.39 Hz, eta=0:00:00, total=0:00:00, wall=05:50 IST** validation 100.00% of 1x98...Epoch=144/150 LR=0.00054 Time=0.414 Loss=1.062 Prec@1=74.098 Prec@5=91.890 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=05:50 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:50 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:50 IST=> training   0.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.698 DataTime=5.401 Loss=0.608 Prec@1=82.812 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=05:50 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.698 DataTime=5.401 Loss=0.608 Prec@1=82.812 Prec@5=95.312 rate=7844.19 Hz, eta=0:00:00, total=0:00:00, wall=05:50 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.698 DataTime=5.401 Loss=0.608 Prec@1=82.812 Prec@5=95.312 rate=7844.19 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.661 DataTime=0.436 Loss=0.651 Prec@1=82.468 Prec@5=95.405 rate=7844.19 Hz, eta=0:00:00, total=0:00:00, wall=05:51 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.661 DataTime=0.436 Loss=0.651 Prec@1=82.468 Prec@5=95.405 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=05:51 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.661 DataTime=0.436 Loss=0.651 Prec@1=82.468 Prec@5=95.405 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=05:52 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.638 DataTime=0.413 Loss=0.654 Prec@1=82.572 Prec@5=95.390 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=05:52 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.638 DataTime=0.413 Loss=0.654 Prec@1=82.572 Prec@5=95.390 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:52 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.638 DataTime=0.413 Loss=0.654 Prec@1=82.572 Prec@5=95.390 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:53 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.630 DataTime=0.405 Loss=0.655 Prec@1=82.491 Prec@5=95.405 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=05:53 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.630 DataTime=0.405 Loss=0.655 Prec@1=82.491 Prec@5=95.405 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:53 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.630 DataTime=0.405 Loss=0.655 Prec@1=82.491 Prec@5=95.405 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:54 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.626 DataTime=0.401 Loss=0.659 Prec@1=82.435 Prec@5=95.334 rate=1.64 Hz, eta=0:22:25, total=0:03:03, wall=05:54 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.626 DataTime=0.401 Loss=0.659 Prec@1=82.435 Prec@5=95.334 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:54 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.626 DataTime=0.401 Loss=0.659 Prec@1=82.435 Prec@5=95.334 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:55 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.624 DataTime=0.399 Loss=0.657 Prec@1=82.477 Prec@5=95.357 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=05:55 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.624 DataTime=0.399 Loss=0.657 Prec@1=82.477 Prec@5=95.357 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:55 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.624 DataTime=0.399 Loss=0.657 Prec@1=82.477 Prec@5=95.357 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:56 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.622 DataTime=0.397 Loss=0.656 Prec@1=82.496 Prec@5=95.372 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=05:56 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.622 DataTime=0.397 Loss=0.656 Prec@1=82.496 Prec@5=95.372 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:56 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.622 DataTime=0.397 Loss=0.656 Prec@1=82.496 Prec@5=95.372 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:57 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.621 DataTime=0.396 Loss=0.656 Prec@1=82.512 Prec@5=95.360 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=05:57 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.621 DataTime=0.396 Loss=0.656 Prec@1=82.512 Prec@5=95.360 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:57 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.621 DataTime=0.396 Loss=0.656 Prec@1=82.512 Prec@5=95.360 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:58 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.483 Prec@5=95.364 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=05:58 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.483 Prec@5=95.364 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:58 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.483 Prec@5=95.364 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:59 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.465 Prec@5=95.369 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=05:59 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.465 Prec@5=95.369 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=05:59 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.620 DataTime=0.395 Loss=0.657 Prec@1=82.465 Prec@5=95.369 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:00 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.657 Prec@1=82.440 Prec@5=95.364 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:00 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.657 Prec@1=82.440 Prec@5=95.364 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:00 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.657 Prec@1=82.440 Prec@5=95.364 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:01 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.656 Prec@1=82.458 Prec@5=95.366 rate=1.63 Hz, eta=0:15:21, total=0:10:14, wall=06:01 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.656 Prec@1=82.458 Prec@5=95.366 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:01 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.619 DataTime=0.394 Loss=0.656 Prec@1=82.458 Prec@5=95.366 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:02 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.465 Prec@5=95.363 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=06:02 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.465 Prec@5=95.363 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:02 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.465 Prec@5=95.363 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:03 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.473 Prec@5=95.353 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:03 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.473 Prec@5=95.353 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:03 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.473 Prec@5=95.353 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:04 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.452 Prec@5=95.343 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:04 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.452 Prec@5=95.343 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:04 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.452 Prec@5=95.343 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:05 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.451 Prec@5=95.341 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:05 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.451 Prec@5=95.341 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:05 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.618 DataTime=0.393 Loss=0.657 Prec@1=82.451 Prec@5=95.341 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:06 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.459 Prec@5=95.342 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:06 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.459 Prec@5=95.342 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:06 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.459 Prec@5=95.342 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:07 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.452 Prec@5=95.336 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:07 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.452 Prec@5=95.336 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:07 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.452 Prec@5=95.336 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:08 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.341 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:08 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.341 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:08 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.341 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:09 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.458 Prec@5=95.344 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:09 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.458 Prec@5=95.344 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:09 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.458 Prec@5=95.344 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:10 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.345 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:10 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.345 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:10 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.467 Prec@5=95.345 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:11 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.483 Prec@5=95.346 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:11 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.483 Prec@5=95.346 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:11 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.483 Prec@5=95.346 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:12 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.481 Prec@5=95.345 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:12 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.481 Prec@5=95.345 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:12 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.481 Prec@5=95.345 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:13 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.487 Prec@5=95.347 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:13 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.487 Prec@5=95.347 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=06:13 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.487 Prec@5=95.347 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=06:14 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.488 Prec@5=95.343 rate=1.63 Hz, eta=0:02:04, total=0:23:33, wall=06:14 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.488 Prec@5=95.343 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:14 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.617 DataTime=0.392 Loss=0.657 Prec@1=82.488 Prec@5=95.343 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:15 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.616 DataTime=0.391 Loss=0.657 Prec@1=82.492 Prec@5=95.343 rate=1.63 Hz, eta=0:01:02, total=0:24:34, wall=06:15 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.616 DataTime=0.391 Loss=0.657 Prec@1=82.492 Prec@5=95.343 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=06:15 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.616 DataTime=0.391 Loss=0.657 Prec@1=82.492 Prec@5=95.343 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=06:15 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.616 DataTime=0.391 Loss=0.657 Prec@1=82.492 Prec@5=95.343 rate=1.63 Hz, eta=0:00:01, total=0:25:36, wall=06:15 IST=> training   100.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.616 DataTime=0.391 Loss=0.657 Prec@1=82.492 Prec@5=95.343 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=06:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> validation 0.00% of 1x98...Epoch=145/150 LR=0.00039 Time=7.061 Loss=1.197 Prec@1=73.438 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=7.061 Loss=1.197 Prec@1=73.438 Prec@5=89.258 rate=6022.03 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=7.061 Loss=1.197 Prec@1=73.438 Prec@5=89.258 rate=6022.03 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=0.402 Loss=1.064 Prec@1=74.034 Prec@5=91.964 rate=6022.03 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST** validation 100.00% of 1x98...Epoch=145/150 LR=0.00039 Time=0.402 Loss=1.064 Prec@1=74.034 Prec@5=91.964 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=06:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> training   0.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.925 DataTime=5.632 Loss=0.647 Prec@1=82.227 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=06:16 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.925 DataTime=5.632 Loss=0.647 Prec@1=82.227 Prec@5=95.898 rate=7609.02 Hz, eta=0:00:00, total=0:00:00, wall=06:16 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.925 DataTime=5.632 Loss=0.647 Prec@1=82.227 Prec@5=95.898 rate=7609.02 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.664 DataTime=0.439 Loss=0.657 Prec@1=82.563 Prec@5=95.496 rate=7609.02 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.664 DataTime=0.439 Loss=0.657 Prec@1=82.563 Prec@5=95.496 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=06:17 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.664 DataTime=0.439 Loss=0.657 Prec@1=82.563 Prec@5=95.496 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=06:18 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.639 DataTime=0.413 Loss=0.654 Prec@1=82.741 Prec@5=95.445 rate=1.65 Hz, eta=0:24:13, total=0:01:01, wall=06:18 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.639 DataTime=0.413 Loss=0.654 Prec@1=82.741 Prec@5=95.445 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=06:18 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.639 DataTime=0.413 Loss=0.654 Prec@1=82.741 Prec@5=95.445 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=06:19 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.631 DataTime=0.405 Loss=0.652 Prec@1=82.642 Prec@5=95.435 rate=1.64 Hz, eta=0:23:23, total=0:02:02, wall=06:19 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.631 DataTime=0.405 Loss=0.652 Prec@1=82.642 Prec@5=95.435 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=06:19 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.631 DataTime=0.405 Loss=0.652 Prec@1=82.642 Prec@5=95.435 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=06:20 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.627 DataTime=0.401 Loss=0.654 Prec@1=82.570 Prec@5=95.403 rate=1.64 Hz, eta=0:22:26, total=0:03:04, wall=06:20 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.627 DataTime=0.401 Loss=0.654 Prec@1=82.570 Prec@5=95.403 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=06:20 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.627 DataTime=0.401 Loss=0.654 Prec@1=82.570 Prec@5=95.403 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=06:21 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.624 DataTime=0.399 Loss=0.653 Prec@1=82.620 Prec@5=95.399 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=06:21 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.624 DataTime=0.399 Loss=0.653 Prec@1=82.620 Prec@5=95.399 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=06:21 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.624 DataTime=0.399 Loss=0.653 Prec@1=82.620 Prec@5=95.399 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=06:22 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.623 DataTime=0.397 Loss=0.654 Prec@1=82.606 Prec@5=95.399 rate=1.63 Hz, eta=0:20:26, total=0:05:06, wall=06:22 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.623 DataTime=0.397 Loss=0.654 Prec@1=82.606 Prec@5=95.399 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:22 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.623 DataTime=0.397 Loss=0.654 Prec@1=82.606 Prec@5=95.399 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:23 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.622 DataTime=0.396 Loss=0.653 Prec@1=82.601 Prec@5=95.415 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=06:23 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.622 DataTime=0.396 Loss=0.653 Prec@1=82.601 Prec@5=95.415 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:23 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.622 DataTime=0.396 Loss=0.653 Prec@1=82.601 Prec@5=95.415 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:24 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.654 Prec@1=82.596 Prec@5=95.411 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=06:24 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.654 Prec@1=82.596 Prec@5=95.411 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=06:24 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.654 Prec@1=82.596 Prec@5=95.411 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=06:26 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.655 Prec@1=82.560 Prec@5=95.394 rate=1.63 Hz, eta=0:17:23, total=0:08:10, wall=06:26 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.655 Prec@1=82.560 Prec@5=95.394 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:26 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.620 DataTime=0.395 Loss=0.655 Prec@1=82.560 Prec@5=95.394 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:27 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.394 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=06:27 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.394 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=06:27 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.394 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=06:28 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.393 Loss=0.656 Prec@1=82.523 Prec@5=95.361 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=06:28 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.393 Loss=0.656 Prec@1=82.523 Prec@5=95.361 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=06:28 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.619 DataTime=0.393 Loss=0.656 Prec@1=82.523 Prec@5=95.361 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=06:29 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.515 Prec@5=95.356 rate=1.63 Hz, eta=0:14:19, total=0:11:15, wall=06:29 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.515 Prec@5=95.356 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:29 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.656 Prec@1=82.515 Prec@5=95.356 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:30 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.551 Prec@5=95.380 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=06:30 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.551 Prec@5=95.380 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:30 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.551 Prec@5=95.380 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.545 Prec@5=95.375 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=06:31 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.545 Prec@5=95.375 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:31 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.393 Loss=0.655 Prec@1=82.545 Prec@5=95.375 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:32 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.381 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=06:32 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.381 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:32 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.618 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.381 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:33 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.552 Prec@5=95.380 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=06:33 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.552 Prec@5=95.380 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:33 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.552 Prec@5=95.380 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:34 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.386 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=06:34 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.386 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:34 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.386 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:35 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.565 Prec@5=95.389 rate=1.63 Hz, eta=0:08:12, total=0:17:24, wall=06:35 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.565 Prec@5=95.389 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:35 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.565 Prec@5=95.389 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:36 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.566 Prec@5=95.390 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=06:36 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.566 Prec@5=95.390 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:36 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.566 Prec@5=95.390 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:37 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.390 rate=1.63 Hz, eta=0:06:09, total=0:19:27, wall=06:37 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.390 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:37 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.558 Prec@5=95.390 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:38 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.382 rate=1.63 Hz, eta=0:05:08, total=0:20:28, wall=06:38 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.382 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:38 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.392 Loss=0.655 Prec@1=82.554 Prec@5=95.382 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:39 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.391 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=06:39 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.391 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:39 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.617 DataTime=0.391 Loss=0.655 Prec@1=82.550 Prec@5=95.383 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:40 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.538 Prec@5=95.383 rate=1.63 Hz, eta=0:03:05, total=0:22:31, wall=06:40 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.538 Prec@5=95.383 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=06:40 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.538 Prec@5=95.383 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=06:41 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.530 Prec@5=95.375 rate=1.63 Hz, eta=0:02:03, total=0:23:32, wall=06:41 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.530 Prec@5=95.375 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:41 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.530 Prec@5=95.375 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:42 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.525 Prec@5=95.369 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=06:42 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.525 Prec@5=95.369 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:42 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.525 Prec@5=95.369 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:42 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.525 Prec@5=95.369 rate=1.63 Hz, eta=0:00:01, total=0:25:35, wall=06:42 IST=> training   100.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.616 DataTime=0.391 Loss=0.655 Prec@1=82.525 Prec@5=95.369 rate=1.63 Hz, eta=0:00:00, total=0:25:36, wall=06:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST=> validation 0.00% of 1x98...Epoch=146/150 LR=0.00027 Time=5.719 Loss=1.257 Prec@1=72.266 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST=> validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=5.719 Loss=1.257 Prec@1=72.266 Prec@5=90.039 rate=6309.07 Hz, eta=0:00:00, total=0:00:00, wall=06:42 IST** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=5.719 Loss=1.257 Prec@1=72.266 Prec@5=90.039 rate=6309.07 Hz, eta=0:00:00, total=0:00:00, wall=06:43 IST** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=0.401 Loss=1.063 Prec@1=74.160 Prec@5=91.940 rate=6309.07 Hz, eta=0:00:00, total=0:00:00, wall=06:43 IST** validation 100.00% of 1x98...Epoch=146/150 LR=0.00027 Time=0.401 Loss=1.063 Prec@1=74.160 Prec@5=91.940 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=06:43 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST=> training   0.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=5.477 DataTime=5.173 Loss=0.630 Prec@1=84.766 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=5.477 DataTime=5.173 Loss=0.630 Prec@1=84.766 Prec@5=94.922 rate=2546.43 Hz, eta=0:00:00, total=0:00:00, wall=06:43 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=5.477 DataTime=5.173 Loss=0.630 Prec@1=84.766 Prec@5=94.922 rate=2546.43 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.661 DataTime=0.435 Loss=0.644 Prec@1=82.913 Prec@5=95.506 rate=2546.43 Hz, eta=0:00:00, total=0:00:00, wall=06:44 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.661 DataTime=0.435 Loss=0.644 Prec@1=82.913 Prec@5=95.506 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:44 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.661 DataTime=0.435 Loss=0.644 Prec@1=82.913 Prec@5=95.506 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:45 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.636 DataTime=0.410 Loss=0.648 Prec@1=82.831 Prec@5=95.422 rate=1.65 Hz, eta=0:24:17, total=0:01:01, wall=06:45 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.636 DataTime=0.410 Loss=0.648 Prec@1=82.831 Prec@5=95.422 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=06:45 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.636 DataTime=0.410 Loss=0.648 Prec@1=82.831 Prec@5=95.422 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=06:46 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.627 DataTime=0.402 Loss=0.648 Prec@1=82.744 Prec@5=95.452 rate=1.64 Hz, eta=0:23:20, total=0:02:02, wall=06:46 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.627 DataTime=0.402 Loss=0.648 Prec@1=82.744 Prec@5=95.452 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=06:46 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.627 DataTime=0.402 Loss=0.648 Prec@1=82.744 Prec@5=95.452 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=06:47 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.623 DataTime=0.399 Loss=0.648 Prec@1=82.763 Prec@5=95.458 rate=1.64 Hz, eta=0:22:20, total=0:03:03, wall=06:47 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.623 DataTime=0.399 Loss=0.648 Prec@1=82.763 Prec@5=95.458 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=06:47 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.623 DataTime=0.399 Loss=0.648 Prec@1=82.763 Prec@5=95.458 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=06:48 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.622 DataTime=0.397 Loss=0.648 Prec@1=82.743 Prec@5=95.426 rate=1.64 Hz, eta=0:21:21, total=0:04:04, wall=06:48 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.622 DataTime=0.397 Loss=0.648 Prec@1=82.743 Prec@5=95.426 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=06:48 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.622 DataTime=0.397 Loss=0.648 Prec@1=82.743 Prec@5=95.426 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=06:49 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.676 Prec@5=95.415 rate=1.64 Hz, eta=0:20:22, total=0:05:05, wall=06:49 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.676 Prec@5=95.415 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:49 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.676 Prec@5=95.415 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:50 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.395 Loss=0.651 Prec@1=82.669 Prec@5=95.422 rate=1.64 Hz, eta=0:19:22, total=0:06:07, wall=06:50 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.395 Loss=0.651 Prec@1=82.669 Prec@5=95.422 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=06:50 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.620 DataTime=0.395 Loss=0.651 Prec@1=82.669 Prec@5=95.422 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.619 DataTime=0.394 Loss=0.652 Prec@1=82.631 Prec@5=95.401 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=06:51 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.619 DataTime=0.394 Loss=0.652 Prec@1=82.631 Prec@5=95.401 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:51 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.619 DataTime=0.394 Loss=0.652 Prec@1=82.631 Prec@5=95.401 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:52 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.632 Prec@5=95.392 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=06:52 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.632 Prec@5=95.392 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:52 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.632 Prec@5=95.392 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:53 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.628 Prec@5=95.399 rate=1.63 Hz, eta=0:16:20, total=0:09:11, wall=06:53 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.628 Prec@5=95.399 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:53 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.628 Prec@5=95.399 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:54 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.653 Prec@1=82.623 Prec@5=95.401 rate=1.63 Hz, eta=0:15:19, total=0:10:12, wall=06:54 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.653 Prec@1=82.623 Prec@5=95.401 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:54 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.653 Prec@1=82.623 Prec@5=95.401 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:55 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.654 Prec@1=82.600 Prec@5=95.388 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=06:55 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.654 Prec@1=82.600 Prec@5=95.388 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:55 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.393 Loss=0.654 Prec@1=82.600 Prec@5=95.388 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:56 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.581 Prec@5=95.379 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=06:56 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.581 Prec@5=95.379 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:56 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.581 Prec@5=95.379 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:57 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.596 Prec@5=95.388 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=06:57 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.596 Prec@5=95.388 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=06:57 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.596 Prec@5=95.388 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=06:58 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.589 Prec@5=95.385 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=06:58 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.589 Prec@5=95.385 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=06:58 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.589 Prec@5=95.385 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=06:59 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.591 Prec@5=95.384 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=06:59 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.591 Prec@5=95.384 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=06:59 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.591 Prec@5=95.384 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=07:00 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.586 Prec@5=95.380 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=07:00 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.586 Prec@5=95.380 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:00 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.586 Prec@5=95.380 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:01 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.378 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=07:01 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.378 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:01 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.378 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:02 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.577 Prec@5=95.377 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:02 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.577 Prec@5=95.377 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=07:02 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.577 Prec@5=95.377 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=07:03 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.381 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=07:03 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.381 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:03 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.381 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:04 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.376 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=07:04 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.376 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:04 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.584 Prec@5=95.376 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:05 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.574 Prec@5=95.374 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:05 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.574 Prec@5=95.374 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:05 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.574 Prec@5=95.374 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:06 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.581 Prec@5=95.371 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:06 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.581 Prec@5=95.371 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:06 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.581 Prec@5=95.371 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:07 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.369 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:07 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.369 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=07:07 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.582 Prec@5=95.369 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=07:08 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.374 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=07:08 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:08 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:08 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.579 Prec@5=95.374 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:08 IST=> training   100.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.579 Prec@5=95.374 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=07:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> validation 0.00% of 1x98...Epoch=147/150 LR=0.00018 Time=7.407 Loss=0.949 Prec@1=75.391 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=7.407 Loss=0.949 Prec@1=75.391 Prec@5=93.945 rate=5562.82 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=7.407 Loss=0.949 Prec@1=75.391 Prec@5=93.945 rate=5562.82 Hz, eta=0:00:00, total=0:00:00, wall=07:09 IST** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=0.411 Loss=1.062 Prec@1=74.098 Prec@5=91.970 rate=5562.82 Hz, eta=0:00:00, total=0:00:00, wall=07:09 IST** validation 100.00% of 1x98...Epoch=147/150 LR=0.00018 Time=0.411 Loss=1.062 Prec@1=74.098 Prec@5=91.970 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=07:09 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:09 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:09 IST=> training   0.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.094 DataTime=4.777 Loss=0.590 Prec@1=83.984 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=07:09 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.094 DataTime=4.777 Loss=0.590 Prec@1=83.984 Prec@5=95.117 rate=6733.10 Hz, eta=0:00:00, total=0:00:00, wall=07:09 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.094 DataTime=4.777 Loss=0.590 Prec@1=83.984 Prec@5=95.117 rate=6733.10 Hz, eta=0:00:00, total=0:00:00, wall=07:10 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.659 DataTime=0.429 Loss=0.656 Prec@1=82.621 Prec@5=95.311 rate=6733.10 Hz, eta=0:00:00, total=0:00:00, wall=07:10 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.659 DataTime=0.429 Loss=0.656 Prec@1=82.621 Prec@5=95.311 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=07:10 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.659 DataTime=0.429 Loss=0.656 Prec@1=82.621 Prec@5=95.311 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=07:11 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.635 DataTime=0.409 Loss=0.651 Prec@1=82.663 Prec@5=95.415 rate=1.64 Hz, eta=0:24:21, total=0:01:01, wall=07:11 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.635 DataTime=0.409 Loss=0.651 Prec@1=82.663 Prec@5=95.415 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=07:11 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.635 DataTime=0.409 Loss=0.651 Prec@1=82.663 Prec@5=95.415 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=07:12 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.628 DataTime=0.403 Loss=0.655 Prec@1=82.605 Prec@5=95.351 rate=1.64 Hz, eta=0:23:24, total=0:02:02, wall=07:12 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.628 DataTime=0.403 Loss=0.655 Prec@1=82.605 Prec@5=95.351 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=07:12 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.628 DataTime=0.403 Loss=0.655 Prec@1=82.605 Prec@5=95.351 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=07:13 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.625 DataTime=0.399 Loss=0.657 Prec@1=82.562 Prec@5=95.322 rate=1.63 Hz, eta=0:22:26, total=0:03:04, wall=07:13 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.625 DataTime=0.399 Loss=0.657 Prec@1=82.562 Prec@5=95.322 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=07:13 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.625 DataTime=0.399 Loss=0.657 Prec@1=82.562 Prec@5=95.322 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=07:14 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.622 DataTime=0.397 Loss=0.657 Prec@1=82.560 Prec@5=95.304 rate=1.63 Hz, eta=0:21:27, total=0:04:05, wall=07:14 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.622 DataTime=0.397 Loss=0.657 Prec@1=82.560 Prec@5=95.304 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=07:14 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.622 DataTime=0.397 Loss=0.657 Prec@1=82.560 Prec@5=95.304 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=07:15 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.396 Loss=0.657 Prec@1=82.522 Prec@5=95.305 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=07:15 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.396 Loss=0.657 Prec@1=82.522 Prec@5=95.305 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=07:15 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.396 Loss=0.657 Prec@1=82.522 Prec@5=95.305 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=07:16 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.395 Loss=0.656 Prec@1=82.529 Prec@5=95.318 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=07:16 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.395 Loss=0.656 Prec@1=82.529 Prec@5=95.318 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=07:16 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.621 DataTime=0.395 Loss=0.656 Prec@1=82.529 Prec@5=95.318 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=07:17 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.620 DataTime=0.394 Loss=0.655 Prec@1=82.555 Prec@5=95.332 rate=1.63 Hz, eta=0:18:25, total=0:07:09, wall=07:17 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.620 DataTime=0.394 Loss=0.655 Prec@1=82.555 Prec@5=95.332 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=07:17 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.620 DataTime=0.394 Loss=0.655 Prec@1=82.555 Prec@5=95.332 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=07:18 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.549 Prec@5=95.353 rate=1.63 Hz, eta=0:17:23, total=0:08:11, wall=07:18 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.549 Prec@5=95.353 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:18 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.549 Prec@5=95.353 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:19 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.580 Prec@5=95.367 rate=1.63 Hz, eta=0:16:22, total=0:09:12, wall=07:19 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.580 Prec@5=95.367 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:19 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.580 Prec@5=95.367 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:20 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.593 Prec@5=95.370 rate=1.63 Hz, eta=0:15:21, total=0:10:13, wall=07:20 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.593 Prec@5=95.370 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:20 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.593 Prec@5=95.370 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:21 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.654 Prec@1=82.581 Prec@5=95.367 rate=1.63 Hz, eta=0:14:20, total=0:11:15, wall=07:21 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.654 Prec@1=82.581 Prec@5=95.367 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:21 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.393 Loss=0.654 Prec@1=82.581 Prec@5=95.367 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:22 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=0.654 Prec@1=82.578 Prec@5=95.369 rate=1.63 Hz, eta=0:13:18, total=0:12:16, wall=07:22 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=0.654 Prec@1=82.578 Prec@5=95.369 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=07:22 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.618 DataTime=0.392 Loss=0.654 Prec@1=82.578 Prec@5=95.369 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=07:23 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.594 Prec@5=95.375 rate=1.63 Hz, eta=0:12:17, total=0:13:18, wall=07:23 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.594 Prec@5=95.375 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=07:23 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.594 Prec@5=95.375 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=07:24 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.653 Prec@1=82.608 Prec@5=95.381 rate=1.63 Hz, eta=0:11:16, total=0:14:19, wall=07:24 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.653 Prec@1=82.608 Prec@5=95.381 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=07:24 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.653 Prec@1=82.608 Prec@5=95.381 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=07:25 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.598 Prec@5=95.369 rate=1.63 Hz, eta=0:10:14, total=0:15:21, wall=07:25 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.598 Prec@5=95.369 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:25 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.598 Prec@5=95.369 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:27 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.585 Prec@5=95.370 rate=1.63 Hz, eta=0:09:13, total=0:16:22, wall=07:27 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.585 Prec@5=95.370 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:27 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.585 Prec@5=95.370 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:28 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.575 Prec@5=95.370 rate=1.63 Hz, eta=0:08:12, total=0:17:23, wall=07:28 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.575 Prec@5=95.370 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:28 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.617 DataTime=0.392 Loss=0.654 Prec@1=82.575 Prec@5=95.370 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:29 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.583 Prec@5=95.364 rate=1.63 Hz, eta=0:07:10, total=0:18:25, wall=07:29 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.583 Prec@5=95.364 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:29 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.392 Loss=0.654 Prec@1=82.583 Prec@5=95.364 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:30 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.361 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:30 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.361 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=07:30 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.361 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=07:31 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.370 rate=1.63 Hz, eta=0:05:08, total=0:20:27, wall=07:31 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.370 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:31 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.587 Prec@5=95.370 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:32 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.653 Prec@1=82.592 Prec@5=95.379 rate=1.63 Hz, eta=0:04:06, total=0:21:29, wall=07:32 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.653 Prec@1=82.592 Prec@5=95.379 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:32 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.653 Prec@1=82.592 Prec@5=95.379 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:33 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.590 Prec@5=95.379 rate=1.63 Hz, eta=0:03:05, total=0:22:30, wall=07:33 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.590 Prec@5=95.379 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:33 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.590 Prec@5=95.379 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:34 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.588 Prec@5=95.370 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:34 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.588 Prec@5=95.370 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:34 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.588 Prec@5=95.370 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:35 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.372 rate=1.63 Hz, eta=0:01:02, total=0:24:33, wall=07:35 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.372 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:35 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.580 Prec@5=95.372 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:35 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.578 Prec@5=95.371 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=07:35 IST=> training   100.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.616 DataTime=0.391 Loss=0.654 Prec@1=82.578 Prec@5=95.371 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=07:35 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST=> validation 0.00% of 1x98...Epoch=148/150 LR=0.00010 Time=7.097 Loss=1.032 Prec@1=73.047 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST=> validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=7.097 Loss=1.032 Prec@1=73.047 Prec@5=92.383 rate=5294.03 Hz, eta=0:00:00, total=0:00:00, wall=07:35 IST** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=7.097 Loss=1.032 Prec@1=73.047 Prec@5=92.383 rate=5294.03 Hz, eta=0:00:00, total=0:00:00, wall=07:35 IST** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=0.419 Loss=1.062 Prec@1=74.110 Prec@5=91.982 rate=5294.03 Hz, eta=0:00:00, total=0:00:00, wall=07:35 IST** validation 100.00% of 1x98...Epoch=148/150 LR=0.00010 Time=0.419 Loss=1.062 Prec@1=74.110 Prec@5=91.982 rate=2.89 Hz, eta=0:00:00, total=0:00:33, wall=07:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> training   0.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.856 DataTime=5.568 Loss=0.556 Prec@1=86.133 Prec@5=96.680 rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.856 DataTime=5.568 Loss=0.556 Prec@1=86.133 Prec@5=96.680 rate=6294.14 Hz, eta=0:00:00, total=0:00:00, wall=07:36 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=5.856 DataTime=5.568 Loss=0.556 Prec@1=86.133 Prec@5=96.680 rate=6294.14 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.662 DataTime=0.438 Loss=0.655 Prec@1=82.534 Prec@5=95.347 rate=6294.14 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.662 DataTime=0.438 Loss=0.655 Prec@1=82.534 Prec@5=95.347 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=07:37 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.662 DataTime=0.438 Loss=0.655 Prec@1=82.534 Prec@5=95.347 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=07:38 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.638 DataTime=0.413 Loss=0.648 Prec@1=82.629 Prec@5=95.515 rate=1.65 Hz, eta=0:24:11, total=0:01:01, wall=07:38 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.638 DataTime=0.413 Loss=0.648 Prec@1=82.629 Prec@5=95.515 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=07:38 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.638 DataTime=0.413 Loss=0.648 Prec@1=82.629 Prec@5=95.515 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.630 DataTime=0.405 Loss=0.652 Prec@1=82.500 Prec@5=95.503 rate=1.64 Hz, eta=0:23:22, total=0:02:02, wall=07:39 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.630 DataTime=0.405 Loss=0.652 Prec@1=82.500 Prec@5=95.503 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:39 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.630 DataTime=0.405 Loss=0.652 Prec@1=82.500 Prec@5=95.503 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:40 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.626 DataTime=0.401 Loss=0.653 Prec@1=82.490 Prec@5=95.470 rate=1.64 Hz, eta=0:22:24, total=0:03:03, wall=07:40 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.626 DataTime=0.401 Loss=0.653 Prec@1=82.490 Prec@5=95.470 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:40 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.626 DataTime=0.401 Loss=0.653 Prec@1=82.490 Prec@5=95.470 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:41 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.623 DataTime=0.399 Loss=0.655 Prec@1=82.451 Prec@5=95.435 rate=1.64 Hz, eta=0:21:24, total=0:04:05, wall=07:41 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.623 DataTime=0.399 Loss=0.655 Prec@1=82.451 Prec@5=95.435 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:41 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.623 DataTime=0.399 Loss=0.655 Prec@1=82.451 Prec@5=95.435 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:42 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.621 DataTime=0.397 Loss=0.656 Prec@1=82.431 Prec@5=95.396 rate=1.63 Hz, eta=0:20:24, total=0:05:06, wall=07:42 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.621 DataTime=0.397 Loss=0.656 Prec@1=82.431 Prec@5=95.396 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=07:42 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.621 DataTime=0.397 Loss=0.656 Prec@1=82.431 Prec@5=95.396 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=07:43 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.620 DataTime=0.396 Loss=0.656 Prec@1=82.459 Prec@5=95.384 rate=1.63 Hz, eta=0:19:23, total=0:06:07, wall=07:43 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.620 DataTime=0.396 Loss=0.656 Prec@1=82.459 Prec@5=95.384 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=07:43 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.620 DataTime=0.396 Loss=0.656 Prec@1=82.459 Prec@5=95.384 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=07:44 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.504 Prec@5=95.403 rate=1.63 Hz, eta=0:18:22, total=0:07:08, wall=07:44 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.504 Prec@5=95.403 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:44 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.504 Prec@5=95.403 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:45 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.528 Prec@5=95.400 rate=1.63 Hz, eta=0:17:21, total=0:08:10, wall=07:45 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.528 Prec@5=95.400 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:45 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.395 Loss=0.654 Prec@1=82.528 Prec@5=95.400 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:46 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.542 Prec@5=95.400 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=07:46 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.542 Prec@5=95.400 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=07:46 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.619 DataTime=0.394 Loss=0.654 Prec@1=82.542 Prec@5=95.400 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=07:47 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.568 Prec@5=95.416 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=07:47 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.568 Prec@5=95.416 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:47 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.568 Prec@5=95.416 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:48 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.562 Prec@5=95.404 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=07:48 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.562 Prec@5=95.404 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:48 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.618 DataTime=0.393 Loss=0.653 Prec@1=82.562 Prec@5=95.404 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:49 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.588 Prec@5=95.408 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=07:49 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.588 Prec@5=95.408 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=07:49 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.588 Prec@5=95.408 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=07:50 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.582 Prec@5=95.411 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=07:50 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.582 Prec@5=95.411 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=07:50 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.393 Loss=0.652 Prec@1=82.582 Prec@5=95.411 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=07:51 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.604 Prec@5=95.417 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=07:51 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.604 Prec@5=95.417 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:51 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.604 Prec@5=95.417 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:52 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.605 Prec@5=95.420 rate=1.63 Hz, eta=0:10:14, total=0:15:20, wall=07:52 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.605 Prec@5=95.420 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=07:52 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.605 Prec@5=95.420 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=07:53 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.596 Prec@5=95.418 rate=1.63 Hz, eta=0:09:13, total=0:16:21, wall=07:53 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.596 Prec@5=95.418 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=07:53 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.596 Prec@5=95.418 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=07:54 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.615 Prec@5=95.418 rate=1.63 Hz, eta=0:08:11, total=0:17:23, wall=07:54 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.615 Prec@5=95.418 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:54 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.615 Prec@5=95.418 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:55 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.629 Prec@5=95.418 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=07:55 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.629 Prec@5=95.418 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:55 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.629 Prec@5=95.418 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:56 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.617 Prec@5=95.418 rate=1.63 Hz, eta=0:06:09, total=0:19:26, wall=07:56 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.617 Prec@5=95.418 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:56 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.617 Prec@5=95.418 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:57 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.618 Prec@5=95.422 rate=1.63 Hz, eta=0:05:07, total=0:20:27, wall=07:57 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.618 Prec@5=95.422 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:57 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.618 Prec@5=95.422 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.621 Prec@5=95.422 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=07:58 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.621 Prec@5=95.422 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:58 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.621 Prec@5=95.422 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:59 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.628 Prec@5=95.426 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=07:59 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.628 Prec@5=95.426 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=07:59 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.628 Prec@5=95.426 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:00 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.641 Prec@5=95.434 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:00 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.641 Prec@5=95.434 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:00 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.641 Prec@5=95.434 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:01 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.650 Prec@5=95.429 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:01 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.650 Prec@5=95.429 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:01 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.650 Prec@5=95.429 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:01 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.650 Prec@5=95.429 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:01 IST=> training   100.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.650 Prec@5=95.429 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=08:01 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> validation 0.00% of 1x98...Epoch=149/150 LR=0.00004 Time=7.487 Loss=1.211 Prec@1=73.438 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=7.487 Loss=1.211 Prec@1=73.438 Prec@5=90.625 rate=5928.59 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=7.487 Loss=1.211 Prec@1=73.438 Prec@5=90.625 rate=5928.59 Hz, eta=0:00:00, total=0:00:00, wall=08:02 IST** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=0.414 Loss=1.061 Prec@1=74.154 Prec@5=91.992 rate=5928.59 Hz, eta=0:00:00, total=0:00:00, wall=08:02 IST** validation 100.00% of 1x98...Epoch=149/150 LR=0.00004 Time=0.414 Loss=1.061 Prec@1=74.154 Prec@5=91.992 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=08:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:02 IST=> training   0.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=5.141 DataTime=4.852 Loss=0.671 Prec@1=82.031 Prec@5=95.703 rate=0 Hz, eta=?, total=0:00:00, wall=08:02 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=5.141 DataTime=4.852 Loss=0.671 Prec@1=82.031 Prec@5=95.703 rate=4046.42 Hz, eta=0:00:00, total=0:00:00, wall=08:02 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=5.141 DataTime=4.852 Loss=0.671 Prec@1=82.031 Prec@5=95.703 rate=4046.42 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.658 DataTime=0.431 Loss=0.656 Prec@1=82.575 Prec@5=95.363 rate=4046.42 Hz, eta=0:00:00, total=0:00:00, wall=08:03 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.658 DataTime=0.431 Loss=0.656 Prec@1=82.575 Prec@5=95.363 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:03 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.658 DataTime=0.431 Loss=0.656 Prec@1=82.575 Prec@5=95.363 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:04 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.636 DataTime=0.411 Loss=0.655 Prec@1=82.591 Prec@5=95.364 rate=1.65 Hz, eta=0:24:18, total=0:01:01, wall=08:04 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.636 DataTime=0.411 Loss=0.655 Prec@1=82.591 Prec@5=95.364 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=08:04 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.636 DataTime=0.411 Loss=0.655 Prec@1=82.591 Prec@5=95.364 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=08:05 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.628 DataTime=0.404 Loss=0.654 Prec@1=82.604 Prec@5=95.349 rate=1.64 Hz, eta=0:23:25, total=0:02:02, wall=08:05 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.628 DataTime=0.404 Loss=0.654 Prec@1=82.604 Prec@5=95.349 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=08:05 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.628 DataTime=0.404 Loss=0.654 Prec@1=82.604 Prec@5=95.349 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=08:06 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.625 DataTime=0.400 Loss=0.652 Prec@1=82.625 Prec@5=95.371 rate=1.64 Hz, eta=0:22:26, total=0:03:03, wall=08:06 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.625 DataTime=0.400 Loss=0.652 Prec@1=82.625 Prec@5=95.371 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:06 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.625 DataTime=0.400 Loss=0.652 Prec@1=82.625 Prec@5=95.371 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:07 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.623 DataTime=0.398 Loss=0.652 Prec@1=82.632 Prec@5=95.382 rate=1.63 Hz, eta=0:21:26, total=0:04:05, wall=08:07 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.623 DataTime=0.398 Loss=0.652 Prec@1=82.632 Prec@5=95.382 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:07 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.623 DataTime=0.398 Loss=0.652 Prec@1=82.632 Prec@5=95.382 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:08 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.621 DataTime=0.397 Loss=0.652 Prec@1=82.625 Prec@5=95.367 rate=1.63 Hz, eta=0:20:25, total=0:05:06, wall=08:08 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.621 DataTime=0.397 Loss=0.652 Prec@1=82.625 Prec@5=95.367 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=08:08 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.621 DataTime=0.397 Loss=0.652 Prec@1=82.625 Prec@5=95.367 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=08:09 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.669 Prec@5=95.374 rate=1.63 Hz, eta=0:19:25, total=0:06:08, wall=08:09 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.669 Prec@5=95.374 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=08:09 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.620 DataTime=0.396 Loss=0.651 Prec@1=82.669 Prec@5=95.374 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=08:10 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.619 DataTime=0.395 Loss=0.651 Prec@1=82.659 Prec@5=95.377 rate=1.63 Hz, eta=0:18:24, total=0:07:09, wall=08:10 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.619 DataTime=0.395 Loss=0.651 Prec@1=82.659 Prec@5=95.377 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:10 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.619 DataTime=0.395 Loss=0.651 Prec@1=82.659 Prec@5=95.377 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:11 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.394 Loss=0.651 Prec@1=82.654 Prec@5=95.376 rate=1.63 Hz, eta=0:17:22, total=0:08:10, wall=08:11 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.394 Loss=0.651 Prec@1=82.654 Prec@5=95.376 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:11 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.394 Loss=0.651 Prec@1=82.654 Prec@5=95.376 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:12 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.393 Loss=0.652 Prec@1=82.643 Prec@5=95.378 rate=1.63 Hz, eta=0:16:21, total=0:09:11, wall=08:12 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.393 Loss=0.652 Prec@1=82.643 Prec@5=95.378 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:12 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.618 DataTime=0.393 Loss=0.652 Prec@1=82.643 Prec@5=95.378 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:13 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.659 Prec@5=95.390 rate=1.63 Hz, eta=0:15:20, total=0:10:13, wall=08:13 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.659 Prec@5=95.390 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=08:13 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.659 Prec@5=95.390 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=08:14 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.651 Prec@5=95.390 rate=1.63 Hz, eta=0:14:18, total=0:11:14, wall=08:14 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.651 Prec@5=95.390 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:14 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.393 Loss=0.651 Prec@1=82.651 Prec@5=95.390 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:15 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.650 Prec@1=82.665 Prec@5=95.398 rate=1.63 Hz, eta=0:13:17, total=0:12:15, wall=08:15 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.650 Prec@1=82.665 Prec@5=95.398 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:15 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.650 Prec@1=82.665 Prec@5=95.398 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:16 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.668 Prec@5=95.399 rate=1.63 Hz, eta=0:12:16, total=0:13:17, wall=08:16 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.668 Prec@5=95.399 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:16 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.617 DataTime=0.392 Loss=0.651 Prec@1=82.668 Prec@5=95.399 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:17 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.650 Prec@1=82.669 Prec@5=95.405 rate=1.63 Hz, eta=0:11:15, total=0:14:18, wall=08:17 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.650 Prec@1=82.669 Prec@5=95.405 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:17 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.650 Prec@1=82.669 Prec@5=95.405 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:18 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.670 Prec@5=95.404 rate=1.63 Hz, eta=0:10:14, total=0:15:19, wall=08:18 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.670 Prec@5=95.404 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:18 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.670 Prec@5=95.404 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:19 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.665 Prec@5=95.399 rate=1.63 Hz, eta=0:09:12, total=0:16:21, wall=08:19 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.665 Prec@5=95.399 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:19 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.665 Prec@5=95.399 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:20 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.664 Prec@5=95.393 rate=1.63 Hz, eta=0:08:11, total=0:17:22, wall=08:20 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.664 Prec@5=95.393 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:20 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.664 Prec@5=95.393 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:21 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.675 Prec@5=95.397 rate=1.63 Hz, eta=0:07:10, total=0:18:24, wall=08:21 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.675 Prec@5=95.397 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:21 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.392 Loss=0.651 Prec@1=82.675 Prec@5=95.397 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:22 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.685 Prec@5=95.399 rate=1.63 Hz, eta=0:06:09, total=0:19:25, wall=08:22 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.685 Prec@5=95.399 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:22 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.651 Prec@1=82.685 Prec@5=95.399 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:23 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.677 Prec@5=95.401 rate=1.63 Hz, eta=0:05:07, total=0:20:26, wall=08:23 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.677 Prec@5=95.401 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:23 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.677 Prec@5=95.401 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:24 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.691 Prec@5=95.404 rate=1.63 Hz, eta=0:04:06, total=0:21:28, wall=08:24 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.691 Prec@5=95.404 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:24 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.691 Prec@5=95.404 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:25 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.689 Prec@5=95.403 rate=1.63 Hz, eta=0:03:05, total=0:22:29, wall=08:25 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.689 Prec@5=95.403 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:25 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.689 Prec@5=95.403 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:26 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.687 Prec@5=95.406 rate=1.63 Hz, eta=0:02:03, total=0:23:31, wall=08:26 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.687 Prec@5=95.406 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:26 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.687 Prec@5=95.406 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:27 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.681 Prec@5=95.397 rate=1.63 Hz, eta=0:01:02, total=0:24:32, wall=08:27 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.681 Prec@5=95.397 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:27 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.615 DataTime=0.391 Loss=0.650 Prec@1=82.681 Prec@5=95.397 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:28 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.680 Prec@5=95.396 rate=1.63 Hz, eta=0:00:01, total=0:25:34, wall=08:28 IST=> training   100.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.616 DataTime=0.391 Loss=0.650 Prec@1=82.680 Prec@5=95.396 rate=1.63 Hz, eta=0:00:00, total=0:25:35, wall=08:28 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:28 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:28 IST=> validation 0.00% of 1x98...Epoch=150/150 LR=0.00001 Time=5.574 Loss=1.082 Prec@1=74.023 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=08:28 IST=> validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=5.574 Loss=1.082 Prec@1=74.023 Prec@5=90.820 rate=5155.25 Hz, eta=0:00:00, total=0:00:00, wall=08:28 IST** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=5.574 Loss=1.082 Prec@1=74.023 Prec@5=90.820 rate=5155.25 Hz, eta=0:00:00, total=0:00:00, wall=08:28 IST** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=0.403 Loss=1.061 Prec@1=74.124 Prec@5=91.996 rate=5155.25 Hz, eta=0:00:00, total=0:00:00, wall=08:28 IST** validation 100.00% of 1x98...Epoch=150/150 LR=0.00001 Time=0.403 Loss=1.061 Prec@1=74.124 Prec@5=91.996 rate=2.89 Hz, eta=0:00:00, total=0:00:33, wall=08:28 IST
[39m