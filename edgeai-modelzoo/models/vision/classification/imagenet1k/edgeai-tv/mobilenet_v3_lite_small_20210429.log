[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_small_x1', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training', 'date': '2021-04-29_23-14-40', 'workers': 12, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fbd513c6908>, 'epochs': 150, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
batch_size: 512
best_prec1: -1
beta: 0.999
bias_calibration: True
bias_decay: None
bitwidth_activations: 8
bitwidth_weights: 8
constrain_bias: None
count_flops: True
data_augument: inception
data_path: ./data/datasets/image_folder_classification
dataset_config: {}
dataset_name: image_folder_classification
date: 2021-04-29_23-14-40
dist_backend: gloo
dist_url: tcp://224.66.41.62:23456
distributed: False
epoch_size: 0
epoch_size_val: 0
epochs: 150
evaluate_start: True
freeze_bn: False
histogram_range: True
image_mean: (123.675, 116.28, 103.53)
image_scale: (0.017125, 0.017507, 0.017429)
img_crop: 224
img_resize: 256
input_channel_reverse: False
iter_size: 1
logger: <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fbd513c6908>
lr: 0.1
lr_calib: 0.05
lr_clips: None
milestones: (30, 60, 90)
model: None
model_config: {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}
model_name: mobilenetv3_lite_small_x1
momentum: 0.9
multi_color_modes: None
multistep_gamma: 0.1
num_inputs: 1
opset_version: 11
optimizer: sgd
parallel_model: True
per_channel_q: False
phase: training
polystep_power: 1.0
pretrained: None
print_freq: 100
print_model: False
quantize: False
rand_scale: (0.2, 1.0)
rand_seed: 1
resume: None
run_soon: True
save_mod_files: False
save_onnx: True
save_path: None
scheduler: cosine
shuffle: True
shuffle_val: True
start_epoch: 0
step_size: 1
stop_epoch: 150
total_batch_size: 512
transforms: None
warmup_epochs: 5
warmup_factor: 0.001
weight_decay: 4e-05
workers: 12
world_size: 1
=> resize resolution: 256
=> crop resolution  : 224
=> creating model 'mobilenetv3_lite_small_x1'
=> Resize = 256, Crop = 224, GFLOPs = 0.108875648, GMACs = 0.054437824
MobileNetV3Lite(
  (features): Sequential(
    (0): ConvBNActivation(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)
          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (2): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)
          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (3): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)
          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (4): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (5): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (6): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (7): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (8): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (9): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (10): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (11): InvertedResidual(
      (block): Sequential(
        (0): ConvBNActivation(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): ConvBNActivation(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (2): ConvBNActivation(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): Identity()
        )
      )
    )
    (12): ConvBNActivation(
      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Linear(in_features=576, out_features=1024, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=True)
    (3): Linear(in_features=1024, out_features=1000, bias=True)
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'strides': None, 'num_tiles_x': 1, 'num_tiles_y': 1, 'en_make_divisible_by8': True, 'enable_fp16': False, 'num_classes': 1000}, 'dataset_config': {}, 'input_channel_reverse': False, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv3_lite_small_x1', 'model': None, 'dataset_name': 'image_folder_classification', 'transforms': None, 'save_path': None, 'phase': 'training', 'date': '2021-04-29_23-14-40', 'workers': 12, 'logger': <pytorch_jacinto_ai.xnn.utils.logger.TeeLogger object at 0x7fbd513c6908>, 'epochs': 150, 'warmup_epochs': 5, 'warmup_factor': 0.001, 'epoch_size': 0, 'epoch_size_val': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'lr_clips': None, 'lr_calib': 0.05, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': None, 'shuffle': True, 'shuffle_val': True, 'rand_seed': 1, 'print_freq': 100, 'resume': None, 'evaluate_start': True, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': (30, 60, 90), 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'save_onnx': True, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': (123.675, 116.28, 103.53), 'image_scale': (0.017125, 0.017507, 0.017429), 'parallel_model': True, 'quantize': False, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'bias_calibration': True, 'per_channel_q': False, 'constrain_bias': None, 'freeze_bn': False, 'save_mod_files': False, 'opset_version': 11, 'best_prec1': -1, 'num_inputs': 1, 'distributed': False}
=> optimizer type   : sgd
=> learning rate    : 0.1
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 150
[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 0.00% of 1x98...Epoch=1/150 LR=0.00010 Time=14.172 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=14.172 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=2797.64 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=14.172 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=2797.64 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=0.426 Loss=6.908 Prec@1=0.112 Prec@5=0.494 rate=2797.64 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST** validation 100.00% of 1x98...Epoch=1/150 LR=0.00010 Time=0.426 Loss=6.908 Prec@1=0.112 Prec@5=0.494 rate=3.56 Hz, eta=0:00:00, total=0:00:27, wall=23:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=5.693 DataTime=4.623 Loss=6.909 Prec@1=0.195 Prec@5=0.586 rate=0 Hz, eta=?, total=0:00:00, wall=23:15 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=5.693 DataTime=4.623 Loss=6.909 Prec@1=0.195 Prec@5=0.586 rate=5926.07 Hz, eta=0:00:00, total=0:00:00, wall=23:15 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=5.693 DataTime=4.623 Loss=6.909 Prec@1=0.195 Prec@5=0.586 rate=5926.07 Hz, eta=0:00:00, total=0:00:00, wall=23:16 IST=> training   0.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.378 DataTime=0.276 Loss=6.908 Prec@1=0.106 Prec@5=0.514 rate=5926.07 Hz, eta=0:00:00, total=0:00:00, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.378 DataTime=0.276 Loss=6.908 Prec@1=0.106 Prec@5=0.514 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.378 DataTime=0.276 Loss=6.908 Prec@1=0.106 Prec@5=0.514 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=23:16 IST=> training   4.04% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.359 DataTime=0.259 Loss=6.908 Prec@1=0.107 Prec@5=0.524 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=23:16 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.359 DataTime=0.259 Loss=6.908 Prec@1=0.107 Prec@5=0.524 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=23:16 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.359 DataTime=0.259 Loss=6.908 Prec@1=0.107 Prec@5=0.524 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=23:17 IST=> training   8.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=23:17 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:17 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:17 IST=> training   12.03% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.345 DataTime=0.246 Loss=6.908 Prec@1=0.104 Prec@5=0.514 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:17 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.345 DataTime=0.246 Loss=6.908 Prec@1=0.104 Prec@5=0.514 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=23:17 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.345 DataTime=0.246 Loss=6.908 Prec@1=0.104 Prec@5=0.514 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=23:18 IST=> training   16.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.245 Loss=6.908 Prec@1=0.108 Prec@5=0.512 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=23:18 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.245 Loss=6.908 Prec@1=0.108 Prec@5=0.512 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=23:18 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.245 Loss=6.908 Prec@1=0.108 Prec@5=0.512 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=23:19 IST=> training   20.02% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.244 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=23:19 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.244 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=23:19 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.346 DataTime=0.244 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=23:19 IST=> training   24.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.344 DataTime=0.242 Loss=6.908 Prec@1=0.113 Prec@5=0.517 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=23:19 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.344 DataTime=0.242 Loss=6.908 Prec@1=0.113 Prec@5=0.517 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=23:19 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.344 DataTime=0.242 Loss=6.908 Prec@1=0.113 Prec@5=0.517 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=23:20 IST=> training   28.01% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.516 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=23:20 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.516 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=23:20 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.516 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=23:20 IST=> training   32.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.515 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=23:20 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.515 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:20 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.240 Loss=6.908 Prec@1=0.113 Prec@5=0.515 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:21 IST=> training   36.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:21 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=23:21 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.111 Prec@5=0.515 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=23:21 IST=> training   39.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.112 Prec@5=0.517 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=23:21 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.112 Prec@5=0.517 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=23:21 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.112 Prec@5=0.517 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=23:22 IST=> training   43.99% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.113 Prec@5=0.519 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=23:22 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.113 Prec@5=0.519 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=23:22 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.238 Loss=6.908 Prec@1=0.113 Prec@5=0.519 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=23:23 IST=> training   47.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.111 Prec@5=0.518 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=23:23 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.111 Prec@5=0.518 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=23:23 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.111 Prec@5=0.518 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=23:23 IST=> training   51.98% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.113 Prec@5=0.520 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=23:23 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.113 Prec@5=0.520 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=23:23 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.113 Prec@5=0.520 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=23:24 IST=> training   55.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.111 Prec@5=0.517 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=23:24 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.111 Prec@5=0.517 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=23:24 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.111 Prec@5=0.517 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=23:24 IST=> training   59.97% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.514 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=23:24 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.514 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:24 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.514 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:25 IST=> training   63.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.235 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:25 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.235 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:25 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.235 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:25 IST=> training   67.96% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:25 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=23:25 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.110 Prec@5=0.516 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=23:26 IST=> training   71.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.517 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=23:26 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.517 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:26 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.517 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:26 IST=> training   75.95% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.518 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:26 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.518 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:26 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.338 DataTime=0.236 Loss=6.908 Prec@1=0.109 Prec@5=0.518 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:27 IST=> training   79.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:27 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=23:27 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=23:28 IST=> training   83.94% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.109 Prec@5=0.521 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=23:28 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.109 Prec@5=0.521 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:28 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.109 Prec@5=0.521 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:28 IST=> training   87.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.520 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:28 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.520 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=23:28 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.520 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=23:29 IST=> training   91.93% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.239 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=23:29 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.239 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=23:29 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.341 DataTime=0.239 Loss=6.908 Prec@1=0.107 Prec@5=0.520 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=23:29 IST=> training   95.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=23:29 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=23:29 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.339 DataTime=0.238 Loss=6.908 Prec@1=0.108 Prec@5=0.519 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=23:29 IST=> training   99.92% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.520 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=23:29 IST=> training   100.00% of 1x2503...Epoch=1/150 LR=0.00010 Time=0.340 DataTime=0.237 Loss=6.908 Prec@1=0.108 Prec@5=0.520 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=23:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:29 IST=> validation 0.00% of 1x98...Epoch=1/150 LR=0.00010 Time=5.785 Loss=6.907 Prec@1=0.000 Prec@5=0.195 rate=0 Hz, eta=?, total=0:00:00, wall=23:29 IST=> validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=5.785 Loss=6.907 Prec@1=0.000 Prec@5=0.195 rate=6587.44 Hz, eta=0:00:00, total=0:00:00, wall=23:29 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=5.785 Loss=6.907 Prec@1=0.000 Prec@5=0.195 rate=6587.44 Hz, eta=0:00:00, total=0:00:00, wall=23:30 IST** validation 1.02% of 1x98...Epoch=1/150 LR=0.00010 Time=0.389 Loss=6.907 Prec@1=0.106 Prec@5=0.518 rate=6587.44 Hz, eta=0:00:00, total=0:00:00, wall=23:30 IST** validation 100.00% of 1x98...Epoch=1/150 LR=0.00010 Time=0.389 Loss=6.907 Prec@1=0.106 Prec@5=0.518 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=23:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:30 IST=> training   0.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=5.677 DataTime=5.359 Loss=6.908 Prec@1=0.000 Prec@5=0.586 rate=0 Hz, eta=?, total=0:00:00, wall=23:30 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=5.677 DataTime=5.359 Loss=6.908 Prec@1=0.000 Prec@5=0.586 rate=9005.28 Hz, eta=0:00:00, total=0:00:00, wall=23:30 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=5.677 DataTime=5.359 Loss=6.908 Prec@1=0.000 Prec@5=0.586 rate=9005.28 Hz, eta=0:00:00, total=0:00:00, wall=23:31 IST=> training   0.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.387 DataTime=0.279 Loss=6.903 Prec@1=0.114 Prec@5=0.586 rate=9005.28 Hz, eta=0:00:00, total=0:00:00, wall=23:31 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.387 DataTime=0.279 Loss=6.903 Prec@1=0.114 Prec@5=0.586 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=23:31 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.387 DataTime=0.279 Loss=6.903 Prec@1=0.114 Prec@5=0.586 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=23:31 IST=> training   4.04% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.357 DataTime=0.247 Loss=6.883 Prec@1=0.150 Prec@5=0.706 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=23:31 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.357 DataTime=0.247 Loss=6.883 Prec@1=0.150 Prec@5=0.706 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=23:31 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.357 DataTime=0.247 Loss=6.883 Prec@1=0.150 Prec@5=0.706 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=23:32 IST=> training   8.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.238 Loss=6.853 Prec@1=0.161 Prec@5=0.794 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=23:32 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.238 Loss=6.853 Prec@1=0.161 Prec@5=0.794 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=23:32 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.238 Loss=6.853 Prec@1=0.161 Prec@5=0.794 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=23:32 IST=> training   12.03% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.239 Loss=6.812 Prec@1=0.186 Prec@5=0.866 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=23:32 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.239 Loss=6.812 Prec@1=0.186 Prec@5=0.866 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=23:32 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.349 DataTime=0.239 Loss=6.812 Prec@1=0.186 Prec@5=0.866 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=23:33 IST=> training   16.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.344 DataTime=0.235 Loss=6.774 Prec@1=0.199 Prec@5=0.958 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=23:33 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.344 DataTime=0.235 Loss=6.774 Prec@1=0.199 Prec@5=0.958 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=23:33 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.344 DataTime=0.235 Loss=6.774 Prec@1=0.199 Prec@5=0.958 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=23:33 IST=> training   20.02% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.731 Prec@1=0.234 Prec@5=1.122 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=23:33 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.731 Prec@1=0.234 Prec@5=1.122 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=23:33 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.731 Prec@1=0.234 Prec@5=1.122 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=23:34 IST=> training   24.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.685 Prec@1=0.293 Prec@5=1.361 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=23:34 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.685 Prec@1=0.293 Prec@5=1.361 rate=3.00 Hz, eta=0:09:59, total=0:03:53, wall=23:34 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.341 DataTime=0.234 Loss=6.685 Prec@1=0.293 Prec@5=1.361 rate=3.00 Hz, eta=0:09:59, total=0:03:53, wall=23:35 IST=> training   28.01% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.234 Loss=6.635 Prec@1=0.369 Prec@5=1.659 rate=3.00 Hz, eta=0:09:59, total=0:03:53, wall=23:35 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.234 Loss=6.635 Prec@1=0.369 Prec@5=1.659 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=23:35 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.234 Loss=6.635 Prec@1=0.369 Prec@5=1.659 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=23:35 IST=> training   32.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.586 Prec@1=0.438 Prec@5=1.947 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=23:35 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.586 Prec@1=0.438 Prec@5=1.947 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=23:35 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.586 Prec@1=0.438 Prec@5=1.947 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=23:36 IST=> training   36.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.233 Loss=6.541 Prec@1=0.510 Prec@5=2.242 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=23:36 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.233 Loss=6.541 Prec@1=0.510 Prec@5=2.242 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:36 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.233 Loss=6.541 Prec@1=0.510 Prec@5=2.242 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:36 IST=> training   39.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.495 Prec@1=0.601 Prec@5=2.587 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:36 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.495 Prec@1=0.601 Prec@5=2.587 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=23:36 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.495 Prec@1=0.601 Prec@5=2.587 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=23:37 IST=> training   43.99% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.450 Prec@1=0.684 Prec@5=2.921 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=23:37 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.450 Prec@1=0.684 Prec@5=2.921 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=23:37 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.450 Prec@1=0.684 Prec@5=2.921 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=23:37 IST=> training   47.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.235 Loss=6.405 Prec@1=0.791 Prec@5=3.291 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=23:37 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.235 Loss=6.405 Prec@1=0.791 Prec@5=3.291 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=23:37 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.340 DataTime=0.235 Loss=6.405 Prec@1=0.791 Prec@5=3.291 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=23:38 IST=> training   51.98% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.360 Prec@1=0.897 Prec@5=3.672 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=23:38 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.360 Prec@1=0.897 Prec@5=3.672 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=23:38 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.360 Prec@1=0.897 Prec@5=3.672 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=23:38 IST=> training   55.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.318 Prec@1=1.020 Prec@5=4.085 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=23:38 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.318 Prec@1=1.020 Prec@5=4.085 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=23:38 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.318 Prec@1=1.020 Prec@5=4.085 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=23:39 IST=> training   59.97% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.277 Prec@1=1.143 Prec@5=4.491 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=23:39 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.277 Prec@1=1.143 Prec@5=4.491 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=23:39 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.233 Loss=6.277 Prec@1=1.143 Prec@5=4.491 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=23:40 IST=> training   63.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.238 Prec@1=1.268 Prec@5=4.907 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=23:40 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.238 Prec@1=1.268 Prec@5=4.907 rate=2.98 Hz, eta=0:04:28, total=0:09:29, wall=23:40 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.238 Prec@1=1.268 Prec@5=4.907 rate=2.98 Hz, eta=0:04:28, total=0:09:29, wall=23:40 IST=> training   67.96% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.200 Prec@1=1.405 Prec@5=5.322 rate=2.98 Hz, eta=0:04:28, total=0:09:29, wall=23:40 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.200 Prec@1=1.405 Prec@5=5.322 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:40 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.200 Prec@1=1.405 Prec@5=5.322 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:41 IST=> training   71.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.163 Prec@1=1.529 Prec@5=5.720 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:41 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.163 Prec@1=1.529 Prec@5=5.720 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:41 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.163 Prec@1=1.529 Prec@5=5.720 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:41 IST=> training   75.95% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.129 Prec@1=1.660 Prec@5=6.115 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:41 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.129 Prec@1=1.660 Prec@5=6.115 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:41 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.129 Prec@1=1.660 Prec@5=6.115 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:42 IST=> training   79.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.095 Prec@1=1.791 Prec@5=6.510 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=23:42 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.095 Prec@1=1.791 Prec@5=6.510 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=23:42 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.339 DataTime=0.232 Loss=6.095 Prec@1=1.791 Prec@5=6.510 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=23:42 IST=> training   83.94% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.062 Prec@1=1.926 Prec@5=6.915 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=23:42 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.062 Prec@1=1.926 Prec@5=6.915 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=23:42 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.232 Loss=6.062 Prec@1=1.926 Prec@5=6.915 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=23:43 IST=> training   87.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.231 Loss=6.030 Prec@1=2.060 Prec@5=7.314 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=23:43 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.231 Loss=6.030 Prec@1=2.060 Prec@5=7.314 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=23:43 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.338 DataTime=0.231 Loss=6.030 Prec@1=2.060 Prec@5=7.314 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=23:43 IST=> training   91.93% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.337 DataTime=0.230 Loss=5.999 Prec@1=2.203 Prec@5=7.701 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=23:43 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.337 DataTime=0.230 Loss=5.999 Prec@1=2.203 Prec@5=7.701 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:43 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.337 DataTime=0.230 Loss=5.999 Prec@1=2.203 Prec@5=7.701 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:44 IST=> training   95.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.336 DataTime=0.230 Loss=5.969 Prec@1=2.339 Prec@5=8.089 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:44 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.336 DataTime=0.230 Loss=5.969 Prec@1=2.339 Prec@5=8.089 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:44 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.336 DataTime=0.230 Loss=5.969 Prec@1=2.339 Prec@5=8.089 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:44 IST=> training   99.92% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.336 DataTime=0.230 Loss=5.968 Prec@1=2.341 Prec@5=8.093 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:44 IST=> training   100.00% of 1x2503...Epoch=2/150 LR=0.02000 Time=0.336 DataTime=0.230 Loss=5.968 Prec@1=2.341 Prec@5=8.093 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=23:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:44 IST=> validation 0.00% of 1x98...Epoch=2/150 LR=0.02000 Time=6.410 Loss=5.353 Prec@1=5.078 Prec@5=16.016 rate=0 Hz, eta=?, total=0:00:00, wall=23:44 IST=> validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=6.410 Loss=5.353 Prec@1=5.078 Prec@5=16.016 rate=5770.11 Hz, eta=0:00:00, total=0:00:00, wall=23:44 IST** validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=6.410 Loss=5.353 Prec@1=5.078 Prec@5=16.016 rate=5770.11 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST** validation 1.02% of 1x98...Epoch=2/150 LR=0.02000 Time=0.396 Loss=5.324 Prec@1=5.106 Prec@5=15.960 rate=5770.11 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST** validation 100.00% of 1x98...Epoch=2/150 LR=0.02000 Time=0.396 Loss=5.324 Prec@1=5.106 Prec@5=15.960 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=23:45 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST=> training   0.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=4.418 DataTime=4.263 Loss=5.296 Prec@1=6.055 Prec@5=15.430 rate=0 Hz, eta=?, total=0:00:00, wall=23:45 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=4.418 DataTime=4.263 Loss=5.296 Prec@1=6.055 Prec@5=15.430 rate=7316.73 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=4.418 DataTime=4.263 Loss=5.296 Prec@1=6.055 Prec@5=15.430 rate=7316.73 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST=> training   0.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.369 DataTime=0.269 Loss=5.299 Prec@1=5.229 Prec@5=16.426 rate=7316.73 Hz, eta=0:00:00, total=0:00:00, wall=23:45 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.369 DataTime=0.269 Loss=5.299 Prec@1=5.229 Prec@5=16.426 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=23:45 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.369 DataTime=0.269 Loss=5.299 Prec@1=5.229 Prec@5=16.426 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=23:46 IST=> training   4.04% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.347 DataTime=0.247 Loss=5.267 Prec@1=5.467 Prec@5=17.017 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=23:46 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.347 DataTime=0.247 Loss=5.267 Prec@1=5.467 Prec@5=17.017 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=23:46 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.347 DataTime=0.247 Loss=5.267 Prec@1=5.467 Prec@5=17.017 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=23:46 IST=> training   8.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.344 DataTime=0.243 Loss=5.239 Prec@1=5.732 Prec@5=17.476 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=23:46 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.344 DataTime=0.243 Loss=5.239 Prec@1=5.732 Prec@5=17.476 rate=3.04 Hz, eta=0:12:04, total=0:01:38, wall=23:46 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.344 DataTime=0.243 Loss=5.239 Prec@1=5.732 Prec@5=17.476 rate=3.04 Hz, eta=0:12:04, total=0:01:38, wall=23:47 IST=> training   12.03% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.238 Loss=5.214 Prec@1=5.976 Prec@5=18.020 rate=3.04 Hz, eta=0:12:04, total=0:01:38, wall=23:47 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.238 Loss=5.214 Prec@1=5.976 Prec@5=18.020 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=23:47 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.238 Loss=5.214 Prec@1=5.976 Prec@5=18.020 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=23:48 IST=> training   16.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.343 DataTime=0.241 Loss=5.184 Prec@1=6.197 Prec@5=18.615 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=23:48 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.343 DataTime=0.241 Loss=5.184 Prec@1=6.197 Prec@5=18.615 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=23:48 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.343 DataTime=0.241 Loss=5.184 Prec@1=6.197 Prec@5=18.615 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=23:48 IST=> training   20.02% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.341 DataTime=0.239 Loss=5.157 Prec@1=6.430 Prec@5=19.137 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=23:48 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.341 DataTime=0.239 Loss=5.157 Prec@1=6.430 Prec@5=19.137 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=23:48 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.341 DataTime=0.239 Loss=5.157 Prec@1=6.430 Prec@5=19.137 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=23:49 IST=> training   24.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.129 Prec@1=6.647 Prec@5=19.621 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=23:49 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.129 Prec@1=6.647 Prec@5=19.621 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=23:49 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.129 Prec@1=6.647 Prec@5=19.621 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=23:49 IST=> training   28.01% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.103 Prec@1=6.907 Prec@5=20.161 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=23:49 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.103 Prec@1=6.907 Prec@5=20.161 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:49 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.103 Prec@1=6.907 Prec@5=20.161 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:50 IST=> training   32.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.077 Prec@1=7.119 Prec@5=20.623 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:50 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.077 Prec@1=7.119 Prec@5=20.623 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:50 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.077 Prec@1=7.119 Prec@5=20.623 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:50 IST=> training   36.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.236 Loss=5.051 Prec@1=7.360 Prec@5=21.146 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=23:50 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.236 Loss=5.051 Prec@1=7.360 Prec@5=21.146 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:50 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.236 Loss=5.051 Prec@1=7.360 Prec@5=21.146 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:51 IST=> training   39.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.027 Prec@1=7.601 Prec@5=21.619 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:51 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.027 Prec@1=7.601 Prec@5=21.619 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=23:51 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.340 DataTime=0.237 Loss=5.027 Prec@1=7.601 Prec@5=21.619 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=23:52 IST=> training   43.99% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=5.003 Prec@1=7.843 Prec@5=22.061 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=23:52 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=5.003 Prec@1=7.843 Prec@5=22.061 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=23:52 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=5.003 Prec@1=7.843 Prec@5=22.061 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=23:52 IST=> training   47.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.979 Prec@1=8.075 Prec@5=22.537 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=23:52 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.979 Prec@1=8.075 Prec@5=22.537 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=23:52 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.979 Prec@1=8.075 Prec@5=22.537 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=23:53 IST=> training   51.98% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=4.956 Prec@1=8.291 Prec@5=22.967 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=23:53 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=4.956 Prec@1=8.291 Prec@5=22.967 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=23:53 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.339 DataTime=0.235 Loss=4.956 Prec@1=8.291 Prec@5=22.967 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=23:53 IST=> training   55.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.933 Prec@1=8.519 Prec@5=23.404 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=23:53 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.933 Prec@1=8.519 Prec@5=23.404 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=23:53 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.234 Loss=4.933 Prec@1=8.519 Prec@5=23.404 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=23:54 IST=> training   59.97% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.912 Prec@1=8.746 Prec@5=23.824 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=23:54 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.912 Prec@1=8.746 Prec@5=23.824 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=23:54 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.912 Prec@1=8.746 Prec@5=23.824 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=23:54 IST=> training   63.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.235 Loss=4.890 Prec@1=8.972 Prec@5=24.255 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=23:54 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.235 Loss=4.890 Prec@1=8.972 Prec@5=24.255 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=23:54 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.338 DataTime=0.235 Loss=4.890 Prec@1=8.972 Prec@5=24.255 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=23:55 IST=> training   67.96% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.869 Prec@1=9.185 Prec@5=24.660 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=23:55 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.869 Prec@1=9.185 Prec@5=24.660 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:55 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.869 Prec@1=9.185 Prec@5=24.660 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:55 IST=> training   71.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.849 Prec@1=9.394 Prec@5=25.052 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=23:55 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.849 Prec@1=9.394 Prec@5=25.052 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=23:55 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.849 Prec@1=9.394 Prec@5=25.052 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=23:56 IST=> training   75.95% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.830 Prec@1=9.601 Prec@5=25.431 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=23:56 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.830 Prec@1=9.601 Prec@5=25.431 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=23:56 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.830 Prec@1=9.601 Prec@5=25.431 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=23:57 IST=> training   79.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.811 Prec@1=9.817 Prec@5=25.812 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=23:57 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.811 Prec@1=9.817 Prec@5=25.812 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=23:57 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.234 Loss=4.811 Prec@1=9.817 Prec@5=25.812 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=23:57 IST=> training   83.94% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.793 Prec@1=10.025 Prec@5=26.174 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=23:57 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.793 Prec@1=10.025 Prec@5=26.174 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=23:57 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.793 Prec@1=10.025 Prec@5=26.174 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=23:58 IST=> training   87.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.775 Prec@1=10.209 Prec@5=26.524 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=23:58 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.775 Prec@1=10.209 Prec@5=26.524 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=23:58 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.775 Prec@1=10.209 Prec@5=26.524 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=23:58 IST=> training   91.93% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.757 Prec@1=10.403 Prec@5=26.884 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=23:58 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.757 Prec@1=10.403 Prec@5=26.884 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:58 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.337 DataTime=0.233 Loss=4.757 Prec@1=10.403 Prec@5=26.884 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:59 IST=> training   95.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.336 DataTime=0.232 Loss=4.740 Prec@1=10.585 Prec@5=27.224 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=23:59 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.336 DataTime=0.232 Loss=4.740 Prec@1=10.585 Prec@5=27.224 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:59 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.336 DataTime=0.232 Loss=4.740 Prec@1=10.585 Prec@5=27.224 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:59 IST=> training   99.92% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.336 DataTime=0.232 Loss=4.740 Prec@1=10.587 Prec@5=27.227 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:59 IST=> training   100.00% of 1x2503...Epoch=3/150 LR=0.04000 Time=0.336 DataTime=0.232 Loss=4.740 Prec@1=10.587 Prec@5=27.227 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=23:59 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:59 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:59 IST=> validation 0.00% of 1x98...Epoch=3/150 LR=0.04000 Time=7.467 Loss=4.467 Prec@1=12.891 Prec@5=32.031 rate=0 Hz, eta=?, total=0:00:00, wall=23:59 IST=> validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=7.467 Loss=4.467 Prec@1=12.891 Prec@5=32.031 rate=6222.35 Hz, eta=0:00:00, total=0:00:00, wall=23:59 IST** validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=7.467 Loss=4.467 Prec@1=12.891 Prec@5=32.031 rate=6222.35 Hz, eta=0:00:00, total=0:00:00, wall=23:59 IST** validation 1.02% of 1x98...Epoch=3/150 LR=0.04000 Time=0.408 Loss=4.470 Prec@1=13.640 Prec@5=32.876 rate=6222.35 Hz, eta=0:00:00, total=0:00:00, wall=23:59 IST** validation 100.00% of 1x98...Epoch=3/150 LR=0.04000 Time=0.408 Loss=4.470 Prec@1=13.640 Prec@5=32.876 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=23:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST=> training   0.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.209 DataTime=5.059 Loss=4.296 Prec@1=14.258 Prec@5=39.062 rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.209 DataTime=5.059 Loss=4.296 Prec@1=14.258 Prec@5=39.062 rate=1895.95 Hz, eta=0:00:01, total=0:00:00, wall=00:00 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=5.209 DataTime=5.059 Loss=4.296 Prec@1=14.258 Prec@5=39.062 rate=1895.95 Hz, eta=0:00:01, total=0:00:00, wall=00:00 IST=> training   0.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.384 DataTime=0.292 Loss=4.366 Prec@1=14.343 Prec@5=34.563 rate=1895.95 Hz, eta=0:00:01, total=0:00:00, wall=00:00 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.384 DataTime=0.292 Loss=4.366 Prec@1=14.343 Prec@5=34.563 rate=3.01 Hz, eta=0:13:16, total=0:00:33, wall=00:00 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.384 DataTime=0.292 Loss=4.366 Prec@1=14.343 Prec@5=34.563 rate=3.01 Hz, eta=0:13:16, total=0:00:33, wall=00:01 IST=> training   4.04% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.358 DataTime=0.267 Loss=4.364 Prec@1=14.463 Prec@5=34.525 rate=3.01 Hz, eta=0:13:16, total=0:00:33, wall=00:01 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.358 DataTime=0.267 Loss=4.364 Prec@1=14.463 Prec@5=34.525 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=00:01 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.358 DataTime=0.267 Loss=4.364 Prec@1=14.463 Prec@5=34.525 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=00:01 IST=> training   8.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.352 DataTime=0.257 Loss=4.342 Prec@1=14.815 Prec@5=34.994 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=00:01 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.352 DataTime=0.257 Loss=4.342 Prec@1=14.815 Prec@5=34.994 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=00:01 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.352 DataTime=0.257 Loss=4.342 Prec@1=14.815 Prec@5=34.994 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=00:02 IST=> training   12.03% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.349 DataTime=0.253 Loss=4.329 Prec@1=15.030 Prec@5=35.306 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=00:02 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.349 DataTime=0.253 Loss=4.329 Prec@1=15.030 Prec@5=35.306 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=00:02 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.349 DataTime=0.253 Loss=4.329 Prec@1=15.030 Prec@5=35.306 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=00:02 IST=> training   16.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.347 DataTime=0.249 Loss=4.316 Prec@1=15.175 Prec@5=35.541 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=00:02 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.347 DataTime=0.249 Loss=4.316 Prec@1=15.175 Prec@5=35.541 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:02 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.347 DataTime=0.249 Loss=4.316 Prec@1=15.175 Prec@5=35.541 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:03 IST=> training   20.02% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.346 DataTime=0.247 Loss=4.304 Prec@1=15.358 Prec@5=35.826 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:03 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.346 DataTime=0.247 Loss=4.304 Prec@1=15.358 Prec@5=35.826 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=00:03 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.346 DataTime=0.247 Loss=4.304 Prec@1=15.358 Prec@5=35.826 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=00:03 IST=> training   24.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.345 DataTime=0.246 Loss=4.290 Prec@1=15.571 Prec@5=36.109 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=00:03 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.345 DataTime=0.246 Loss=4.290 Prec@1=15.571 Prec@5=36.109 rate=2.96 Hz, eta=0:10:09, total=0:03:56, wall=00:03 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.345 DataTime=0.246 Loss=4.290 Prec@1=15.571 Prec@5=36.109 rate=2.96 Hz, eta=0:10:09, total=0:03:56, wall=00:04 IST=> training   28.01% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.343 DataTime=0.243 Loss=4.275 Prec@1=15.783 Prec@5=36.436 rate=2.96 Hz, eta=0:10:09, total=0:03:56, wall=00:04 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.343 DataTime=0.243 Loss=4.275 Prec@1=15.783 Prec@5=36.436 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:04 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.343 DataTime=0.243 Loss=4.275 Prec@1=15.783 Prec@5=36.436 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:05 IST=> training   32.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.260 Prec@1=15.966 Prec@5=36.735 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:05 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.260 Prec@1=15.966 Prec@5=36.735 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:05 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.260 Prec@1=15.966 Prec@5=36.735 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:05 IST=> training   36.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.247 Prec@1=16.162 Prec@5=36.990 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:05 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.247 Prec@1=16.162 Prec@5=36.990 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:05 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.241 Loss=4.247 Prec@1=16.162 Prec@5=36.990 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:06 IST=> training   39.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.240 Loss=4.233 Prec@1=16.356 Prec@5=37.281 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:06 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.240 Loss=4.233 Prec@1=16.356 Prec@5=37.281 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:06 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.240 Loss=4.233 Prec@1=16.356 Prec@5=37.281 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:06 IST=> training   43.99% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.240 Loss=4.219 Prec@1=16.520 Prec@5=37.558 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:06 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.240 Loss=4.219 Prec@1=16.520 Prec@5=37.558 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:06 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.342 DataTime=0.240 Loss=4.219 Prec@1=16.520 Prec@5=37.558 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:07 IST=> training   47.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.239 Loss=4.207 Prec@1=16.674 Prec@5=37.815 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:07 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.239 Loss=4.207 Prec@1=16.674 Prec@5=37.815 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=00:07 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.239 Loss=4.207 Prec@1=16.674 Prec@5=37.815 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=00:07 IST=> training   51.98% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.238 Loss=4.193 Prec@1=16.871 Prec@5=38.100 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=00:07 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.238 Loss=4.193 Prec@1=16.871 Prec@5=38.100 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=00:07 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.341 DataTime=0.238 Loss=4.193 Prec@1=16.871 Prec@5=38.100 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=00:08 IST=> training   55.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.182 Prec@1=17.017 Prec@5=38.339 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=00:08 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.182 Prec@1=17.017 Prec@5=38.339 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:08 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.182 Prec@1=17.017 Prec@5=38.339 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:09 IST=> training   59.97% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.239 Loss=4.170 Prec@1=17.201 Prec@5=38.579 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:09 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.239 Loss=4.170 Prec@1=17.201 Prec@5=38.579 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=00:09 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.239 Loss=4.170 Prec@1=17.201 Prec@5=38.579 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=00:09 IST=> training   63.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.159 Prec@1=17.349 Prec@5=38.796 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=00:09 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.159 Prec@1=17.349 Prec@5=38.796 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:09 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.159 Prec@1=17.349 Prec@5=38.796 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:10 IST=> training   67.96% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.148 Prec@1=17.517 Prec@5=39.017 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:10 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.148 Prec@1=17.517 Prec@5=39.017 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:10 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.148 Prec@1=17.517 Prec@5=39.017 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:10 IST=> training   71.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.136 Prec@1=17.679 Prec@5=39.255 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:10 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.136 Prec@1=17.679 Prec@5=39.255 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:10 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.136 Prec@1=17.679 Prec@5=39.255 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:11 IST=> training   75.95% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.124 Prec@1=17.822 Prec@5=39.476 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:11 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.124 Prec@1=17.822 Prec@5=39.476 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:11 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.124 Prec@1=17.822 Prec@5=39.476 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:11 IST=> training   79.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.113 Prec@1=17.979 Prec@5=39.703 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:11 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.113 Prec@1=17.979 Prec@5=39.703 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:11 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.340 DataTime=0.238 Loss=4.113 Prec@1=17.979 Prec@5=39.703 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:12 IST=> training   83.94% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.102 Prec@1=18.144 Prec@5=39.920 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:12 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.102 Prec@1=18.144 Prec@5=39.920 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=00:12 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.102 Prec@1=18.144 Prec@5=39.920 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=00:12 IST=> training   87.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.091 Prec@1=18.286 Prec@5=40.144 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=00:12 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.091 Prec@1=18.286 Prec@5=40.144 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:12 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.237 Loss=4.091 Prec@1=18.286 Prec@5=40.144 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:13 IST=> training   91.93% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.238 Loss=4.081 Prec@1=18.421 Prec@5=40.338 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:13 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.238 Loss=4.081 Prec@1=18.421 Prec@5=40.338 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:13 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.339 DataTime=0.238 Loss=4.081 Prec@1=18.421 Prec@5=40.338 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:14 IST=> training   95.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.338 DataTime=0.237 Loss=4.070 Prec@1=18.572 Prec@5=40.543 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:14 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.338 DataTime=0.237 Loss=4.070 Prec@1=18.572 Prec@5=40.543 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=00:14 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.338 DataTime=0.237 Loss=4.070 Prec@1=18.572 Prec@5=40.543 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=00:14 IST=> training   99.92% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.338 DataTime=0.237 Loss=4.070 Prec@1=18.574 Prec@5=40.545 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=00:14 IST=> training   100.00% of 1x2503...Epoch=4/150 LR=0.06000 Time=0.338 DataTime=0.237 Loss=4.070 Prec@1=18.574 Prec@5=40.545 rate=2.98 Hz, eta=0:00:00, total=0:14:01, wall=00:14 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> validation 0.00% of 1x98...Epoch=4/150 LR=0.06000 Time=5.706 Loss=3.751 Prec@1=25.391 Prec@5=46.680 rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=5.706 Loss=3.751 Prec@1=25.391 Prec@5=46.680 rate=2668.50 Hz, eta=0:00:00, total=0:00:00, wall=00:14 IST** validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=5.706 Loss=3.751 Prec@1=25.391 Prec@5=46.680 rate=2668.50 Hz, eta=0:00:00, total=0:00:00, wall=00:14 IST** validation 1.02% of 1x98...Epoch=4/150 LR=0.06000 Time=0.389 Loss=3.909 Prec@1=20.142 Prec@5=43.626 rate=2668.50 Hz, eta=0:00:00, total=0:00:00, wall=00:14 IST** validation 100.00% of 1x98...Epoch=4/150 LR=0.06000 Time=0.389 Loss=3.909 Prec@1=20.142 Prec@5=43.626 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=00:14 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> training   0.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=5.259 DataTime=5.022 Loss=3.586 Prec@1=23.633 Prec@5=53.516 rate=0 Hz, eta=?, total=0:00:00, wall=00:14 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=5.259 DataTime=5.022 Loss=3.586 Prec@1=23.633 Prec@5=53.516 rate=4625.41 Hz, eta=0:00:00, total=0:00:00, wall=00:14 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=5.259 DataTime=5.022 Loss=3.586 Prec@1=23.633 Prec@5=53.516 rate=4625.41 Hz, eta=0:00:00, total=0:00:00, wall=00:15 IST=> training   0.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.378 DataTime=0.278 Loss=3.833 Prec@1=21.792 Prec@5=45.144 rate=4625.41 Hz, eta=0:00:00, total=0:00:00, wall=00:15 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.378 DataTime=0.278 Loss=3.833 Prec@1=21.792 Prec@5=45.144 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=00:15 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.378 DataTime=0.278 Loss=3.833 Prec@1=21.792 Prec@5=45.144 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=00:15 IST=> training   4.04% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.362 DataTime=0.263 Loss=3.829 Prec@1=21.851 Prec@5=45.139 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=00:15 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.362 DataTime=0.263 Loss=3.829 Prec@1=21.851 Prec@5=45.139 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=00:15 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.362 DataTime=0.263 Loss=3.829 Prec@1=21.851 Prec@5=45.139 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=00:16 IST=> training   8.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.251 Loss=3.831 Prec@1=21.783 Prec@5=45.107 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=00:16 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.251 Loss=3.831 Prec@1=21.783 Prec@5=45.107 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=00:16 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.251 Loss=3.831 Prec@1=21.783 Prec@5=45.107 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=00:17 IST=> training   12.03% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.250 Loss=3.825 Prec@1=21.879 Prec@5=45.271 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=00:17 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.250 Loss=3.825 Prec@1=21.879 Prec@5=45.271 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=00:17 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.351 DataTime=0.250 Loss=3.825 Prec@1=21.879 Prec@5=45.271 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=00:17 IST=> training   16.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.347 DataTime=0.246 Loss=3.821 Prec@1=21.935 Prec@5=45.367 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=00:17 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.347 DataTime=0.246 Loss=3.821 Prec@1=21.935 Prec@5=45.367 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:17 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.347 DataTime=0.246 Loss=3.821 Prec@1=21.935 Prec@5=45.367 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:18 IST=> training   20.02% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.343 DataTime=0.242 Loss=3.814 Prec@1=22.038 Prec@5=45.518 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:18 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.343 DataTime=0.242 Loss=3.814 Prec@1=22.038 Prec@5=45.518 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=00:18 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.343 DataTime=0.242 Loss=3.814 Prec@1=22.038 Prec@5=45.518 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=00:18 IST=> training   24.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.345 DataTime=0.244 Loss=3.804 Prec@1=22.188 Prec@5=45.737 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=00:18 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.345 DataTime=0.244 Loss=3.804 Prec@1=22.188 Prec@5=45.737 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=00:18 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.345 DataTime=0.244 Loss=3.804 Prec@1=22.188 Prec@5=45.737 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=00:19 IST=> training   28.01% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.344 DataTime=0.242 Loss=3.797 Prec@1=22.310 Prec@5=45.898 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=00:19 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.344 DataTime=0.242 Loss=3.797 Prec@1=22.310 Prec@5=45.898 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:19 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.344 DataTime=0.242 Loss=3.797 Prec@1=22.310 Prec@5=45.898 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:19 IST=> training   32.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.240 Loss=3.788 Prec@1=22.441 Prec@5=46.097 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:19 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.240 Loss=3.788 Prec@1=22.441 Prec@5=46.097 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:19 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.240 Loss=3.788 Prec@1=22.441 Prec@5=46.097 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:20 IST=> training   36.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.780 Prec@1=22.586 Prec@5=46.258 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:20 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.780 Prec@1=22.586 Prec@5=46.258 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:20 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.780 Prec@1=22.586 Prec@5=46.258 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:20 IST=> training   39.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.772 Prec@1=22.703 Prec@5=46.421 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=00:20 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.772 Prec@1=22.703 Prec@5=46.421 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:20 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.772 Prec@1=22.703 Prec@5=46.421 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:21 IST=> training   43.99% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.765 Prec@1=22.821 Prec@5=46.565 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:21 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.765 Prec@1=22.821 Prec@5=46.565 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:21 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.765 Prec@1=22.821 Prec@5=46.565 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:22 IST=> training   47.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.758 Prec@1=22.931 Prec@5=46.690 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:22 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.758 Prec@1=22.931 Prec@5=46.690 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=00:22 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.342 DataTime=0.240 Loss=3.758 Prec@1=22.931 Prec@5=46.690 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=00:22 IST=> training   51.98% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.748 Prec@1=23.065 Prec@5=46.869 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=00:22 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.748 Prec@1=23.065 Prec@5=46.869 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:22 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.748 Prec@1=23.065 Prec@5=46.869 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:23 IST=> training   55.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.238 Loss=3.740 Prec@1=23.189 Prec@5=47.033 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.238 Loss=3.740 Prec@1=23.189 Prec@5=47.033 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.238 Loss=3.740 Prec@1=23.189 Prec@5=47.033 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:23 IST=> training   59.97% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.733 Prec@1=23.313 Prec@5=47.186 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:23 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.733 Prec@1=23.313 Prec@5=47.186 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=00:23 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.239 Loss=3.733 Prec@1=23.313 Prec@5=47.186 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=00:24 IST=> training   63.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.725 Prec@1=23.429 Prec@5=47.336 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=00:24 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.725 Prec@1=23.429 Prec@5=47.336 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=00:24 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.725 Prec@1=23.429 Prec@5=47.336 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=00:24 IST=> training   67.96% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.718 Prec@1=23.542 Prec@5=47.486 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=00:24 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.718 Prec@1=23.542 Prec@5=47.486 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=00:24 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.718 Prec@1=23.542 Prec@5=47.486 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=00:25 IST=> training   71.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.710 Prec@1=23.663 Prec@5=47.634 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=00:25 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.710 Prec@1=23.663 Prec@5=47.634 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=00:25 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.710 Prec@1=23.663 Prec@5=47.634 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=00:26 IST=> training   75.95% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.703 Prec@1=23.770 Prec@5=47.774 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=00:26 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.703 Prec@1=23.770 Prec@5=47.774 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:26 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.703 Prec@1=23.770 Prec@5=47.774 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:26 IST=> training   79.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.696 Prec@1=23.877 Prec@5=47.911 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:26 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.696 Prec@1=23.877 Prec@5=47.911 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:26 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.696 Prec@1=23.877 Prec@5=47.911 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:27 IST=> training   83.94% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.689 Prec@1=23.983 Prec@5=48.042 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:27 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.689 Prec@1=23.983 Prec@5=48.042 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=00:27 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.689 Prec@1=23.983 Prec@5=48.042 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=00:27 IST=> training   87.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.682 Prec@1=24.095 Prec@5=48.169 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=00:27 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.682 Prec@1=24.095 Prec@5=48.169 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=00:27 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.238 Loss=3.682 Prec@1=24.095 Prec@5=48.169 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=00:28 IST=> training   91.93% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.237 Loss=3.675 Prec@1=24.205 Prec@5=48.300 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=00:28 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.237 Loss=3.675 Prec@1=24.205 Prec@5=48.300 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=00:28 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.341 DataTime=0.237 Loss=3.675 Prec@1=24.205 Prec@5=48.300 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=00:28 IST=> training   95.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.669 Prec@1=24.291 Prec@5=48.417 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=00:28 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.669 Prec@1=24.291 Prec@5=48.417 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=00:28 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.237 Loss=3.669 Prec@1=24.291 Prec@5=48.417 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=00:28 IST=> training   99.92% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.236 Loss=3.669 Prec@1=24.291 Prec@5=48.418 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=00:28 IST=> training   100.00% of 1x2503...Epoch=5/150 LR=0.08000 Time=0.340 DataTime=0.236 Loss=3.669 Prec@1=24.291 Prec@5=48.418 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=00:28 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> validation 0.00% of 1x98...Epoch=5/150 LR=0.08000 Time=5.668 Loss=3.731 Prec@1=23.438 Prec@5=48.438 rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=5.668 Loss=3.731 Prec@1=23.438 Prec@5=48.438 rate=5592.34 Hz, eta=0:00:00, total=0:00:00, wall=00:29 IST** validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=5.668 Loss=3.731 Prec@1=23.438 Prec@5=48.438 rate=5592.34 Hz, eta=0:00:00, total=0:00:00, wall=00:29 IST** validation 1.02% of 1x98...Epoch=5/150 LR=0.08000 Time=0.386 Loss=3.569 Prec@1=25.750 Prec@5=50.292 rate=5592.34 Hz, eta=0:00:00, total=0:00:00, wall=00:29 IST** validation 100.00% of 1x98...Epoch=5/150 LR=0.08000 Time=0.386 Loss=3.569 Prec@1=25.750 Prec@5=50.292 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=00:29 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> training   0.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.881 DataTime=4.713 Loss=3.443 Prec@1=27.930 Prec@5=52.930 rate=0 Hz, eta=?, total=0:00:00, wall=00:29 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.881 DataTime=4.713 Loss=3.443 Prec@1=27.930 Prec@5=52.930 rate=4733.39 Hz, eta=0:00:00, total=0:00:00, wall=00:29 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=4.881 DataTime=4.713 Loss=3.443 Prec@1=27.930 Prec@5=52.930 rate=4733.39 Hz, eta=0:00:00, total=0:00:00, wall=00:30 IST=> training   0.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.378 DataTime=0.285 Loss=3.542 Prec@1=26.127 Prec@5=51.091 rate=4733.39 Hz, eta=0:00:00, total=0:00:00, wall=00:30 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.378 DataTime=0.285 Loss=3.542 Prec@1=26.127 Prec@5=51.091 rate=3.04 Hz, eta=0:13:11, total=0:00:33, wall=00:30 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.378 DataTime=0.285 Loss=3.542 Prec@1=26.127 Prec@5=51.091 rate=3.04 Hz, eta=0:13:11, total=0:00:33, wall=00:30 IST=> training   4.04% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.357 DataTime=0.263 Loss=3.530 Prec@1=26.498 Prec@5=51.304 rate=3.04 Hz, eta=0:13:11, total=0:00:33, wall=00:30 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.357 DataTime=0.263 Loss=3.530 Prec@1=26.498 Prec@5=51.304 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=00:30 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.357 DataTime=0.263 Loss=3.530 Prec@1=26.498 Prec@5=51.304 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=00:31 IST=> training   8.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.353 DataTime=0.255 Loss=3.525 Prec@1=26.468 Prec@5=51.336 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=00:31 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.353 DataTime=0.255 Loss=3.525 Prec@1=26.468 Prec@5=51.336 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=00:31 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.353 DataTime=0.255 Loss=3.525 Prec@1=26.468 Prec@5=51.336 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=00:31 IST=> training   12.03% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.349 DataTime=0.249 Loss=3.519 Prec@1=26.561 Prec@5=51.409 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=00:31 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.349 DataTime=0.249 Loss=3.519 Prec@1=26.561 Prec@5=51.409 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=00:31 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.349 DataTime=0.249 Loss=3.519 Prec@1=26.561 Prec@5=51.409 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=00:32 IST=> training   16.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.516 Prec@1=26.628 Prec@5=51.443 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=00:32 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.516 Prec@1=26.628 Prec@5=51.443 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:32 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.516 Prec@1=26.628 Prec@5=51.443 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:33 IST=> training   20.02% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.513 Prec@1=26.651 Prec@5=51.491 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:33 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.513 Prec@1=26.651 Prec@5=51.491 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:33 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.345 DataTime=0.243 Loss=3.513 Prec@1=26.651 Prec@5=51.491 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:33 IST=> training   24.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.344 DataTime=0.241 Loss=3.509 Prec@1=26.693 Prec@5=51.565 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:33 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.344 DataTime=0.241 Loss=3.509 Prec@1=26.693 Prec@5=51.565 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=00:33 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.344 DataTime=0.241 Loss=3.509 Prec@1=26.693 Prec@5=51.565 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=00:34 IST=> training   28.01% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.342 DataTime=0.239 Loss=3.504 Prec@1=26.788 Prec@5=51.661 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=00:34 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.342 DataTime=0.239 Loss=3.504 Prec@1=26.788 Prec@5=51.661 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:34 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.342 DataTime=0.239 Loss=3.504 Prec@1=26.788 Prec@5=51.661 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:34 IST=> training   32.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.499 Prec@1=26.873 Prec@5=51.769 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:34 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.499 Prec@1=26.873 Prec@5=51.769 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:34 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.499 Prec@1=26.873 Prec@5=51.769 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:35 IST=> training   36.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.495 Prec@1=26.919 Prec@5=51.820 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:35 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.495 Prec@1=26.919 Prec@5=51.820 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:35 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.495 Prec@1=26.919 Prec@5=51.820 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:35 IST=> training   39.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.239 Loss=3.490 Prec@1=27.019 Prec@5=51.912 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:35 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.239 Loss=3.490 Prec@1=27.019 Prec@5=51.912 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:35 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.239 Loss=3.490 Prec@1=27.019 Prec@5=51.912 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:36 IST=> training   43.99% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.238 Loss=3.485 Prec@1=27.096 Prec@5=52.003 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=00:36 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.238 Loss=3.485 Prec@1=27.096 Prec@5=52.003 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:36 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.238 Loss=3.485 Prec@1=27.096 Prec@5=52.003 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:36 IST=> training   47.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.480 Prec@1=27.146 Prec@5=52.073 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:36 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.480 Prec@1=27.146 Prec@5=52.073 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:36 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.480 Prec@1=27.146 Prec@5=52.073 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:37 IST=> training   51.98% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.474 Prec@1=27.251 Prec@5=52.200 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:37 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.474 Prec@1=27.251 Prec@5=52.200 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:37 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.474 Prec@1=27.251 Prec@5=52.200 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:38 IST=> training   55.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.469 Prec@1=27.317 Prec@5=52.288 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:38 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.469 Prec@1=27.317 Prec@5=52.288 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=00:38 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.341 DataTime=0.238 Loss=3.469 Prec@1=27.317 Prec@5=52.288 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=00:38 IST=> training   59.97% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.465 Prec@1=27.393 Prec@5=52.376 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=00:38 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.465 Prec@1=27.393 Prec@5=52.376 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=00:38 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.237 Loss=3.465 Prec@1=27.393 Prec@5=52.376 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=00:39 IST=> training   63.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.460 Prec@1=27.464 Prec@5=52.466 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=00:39 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.460 Prec@1=27.464 Prec@5=52.466 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=00:39 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.460 Prec@1=27.464 Prec@5=52.466 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=00:39 IST=> training   67.96% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.236 Loss=3.455 Prec@1=27.546 Prec@5=52.570 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=00:39 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.236 Loss=3.455 Prec@1=27.546 Prec@5=52.570 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:39 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.236 Loss=3.455 Prec@1=27.546 Prec@5=52.570 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:40 IST=> training   71.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.450 Prec@1=27.632 Prec@5=52.668 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:40 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.450 Prec@1=27.632 Prec@5=52.668 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:40 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.450 Prec@1=27.632 Prec@5=52.668 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:40 IST=> training   75.95% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.445 Prec@1=27.734 Prec@5=52.763 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:40 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.445 Prec@1=27.734 Prec@5=52.763 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:40 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.445 Prec@1=27.734 Prec@5=52.763 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:41 IST=> training   79.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.440 Prec@1=27.812 Prec@5=52.863 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:41 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.440 Prec@1=27.812 Prec@5=52.863 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:41 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.440 Prec@1=27.812 Prec@5=52.863 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:42 IST=> training   83.94% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.435 Prec@1=27.889 Prec@5=52.943 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:42 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.435 Prec@1=27.889 Prec@5=52.943 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=00:42 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.236 Loss=3.435 Prec@1=27.889 Prec@5=52.943 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=00:42 IST=> training   87.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.431 Prec@1=27.956 Prec@5=53.022 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=00:42 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.431 Prec@1=27.956 Prec@5=53.022 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:42 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.431 Prec@1=27.956 Prec@5=53.022 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:43 IST=> training   91.93% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.235 Loss=3.426 Prec@1=28.037 Prec@5=53.114 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:43 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.235 Loss=3.426 Prec@1=28.037 Prec@5=53.114 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:43 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.340 DataTime=0.235 Loss=3.426 Prec@1=28.037 Prec@5=53.114 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:43 IST=> training   95.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.421 Prec@1=28.116 Prec@5=53.198 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:43 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.421 Prec@1=28.116 Prec@5=53.198 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:43 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.421 Prec@1=28.116 Prec@5=53.198 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:43 IST=> training   99.92% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.421 Prec@1=28.118 Prec@5=53.200 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:43 IST=> training   100.00% of 1x2503...Epoch=6/150 LR=0.10000 Time=0.339 DataTime=0.235 Loss=3.421 Prec@1=28.118 Prec@5=53.200 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:43 IST=> validation 0.00% of 1x98...Epoch=6/150 LR=0.10000 Time=6.856 Loss=3.441 Prec@1=27.148 Prec@5=53.906 rate=0 Hz, eta=?, total=0:00:00, wall=00:43 IST=> validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=6.856 Loss=3.441 Prec@1=27.148 Prec@5=53.906 rate=6221.97 Hz, eta=0:00:00, total=0:00:00, wall=00:43 IST** validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=6.856 Loss=3.441 Prec@1=27.148 Prec@5=53.906 rate=6221.97 Hz, eta=0:00:00, total=0:00:00, wall=00:44 IST** validation 1.02% of 1x98...Epoch=6/150 LR=0.10000 Time=0.403 Loss=3.469 Prec@1=27.546 Prec@5=52.224 rate=6221.97 Hz, eta=0:00:00, total=0:00:00, wall=00:44 IST** validation 100.00% of 1x98...Epoch=6/150 LR=0.10000 Time=0.403 Loss=3.469 Prec@1=27.546 Prec@5=52.224 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=00:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:44 IST=> training   0.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=6.112 DataTime=6.013 Loss=3.450 Prec@1=27.539 Prec@5=51.953 rate=0 Hz, eta=?, total=0:00:00, wall=00:44 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=6.112 DataTime=6.013 Loss=3.450 Prec@1=27.539 Prec@5=51.953 rate=7206.74 Hz, eta=0:00:00, total=0:00:00, wall=00:44 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=6.112 DataTime=6.013 Loss=3.450 Prec@1=27.539 Prec@5=51.953 rate=7206.74 Hz, eta=0:00:00, total=0:00:00, wall=00:45 IST=> training   0.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.390 DataTime=0.294 Loss=3.276 Prec@1=30.204 Prec@5=55.786 rate=7206.74 Hz, eta=0:00:00, total=0:00:00, wall=00:45 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.390 DataTime=0.294 Loss=3.276 Prec@1=30.204 Prec@5=55.786 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=00:45 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.390 DataTime=0.294 Loss=3.276 Prec@1=30.204 Prec@5=55.786 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=00:45 IST=> training   4.04% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.358 DataTime=0.260 Loss=3.272 Prec@1=30.350 Prec@5=56.047 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=00:45 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.358 DataTime=0.260 Loss=3.272 Prec@1=30.350 Prec@5=56.047 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=00:45 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.358 DataTime=0.260 Loss=3.272 Prec@1=30.350 Prec@5=56.047 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=00:46 IST=> training   8.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.357 DataTime=0.257 Loss=3.268 Prec@1=30.496 Prec@5=56.129 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=00:46 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.357 DataTime=0.257 Loss=3.268 Prec@1=30.496 Prec@5=56.129 rate=2.97 Hz, eta=0:12:21, total=0:01:41, wall=00:46 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.357 DataTime=0.257 Loss=3.268 Prec@1=30.496 Prec@5=56.129 rate=2.97 Hz, eta=0:12:21, total=0:01:41, wall=00:46 IST=> training   12.03% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.350 DataTime=0.248 Loss=3.261 Prec@1=30.573 Prec@5=56.196 rate=2.97 Hz, eta=0:12:21, total=0:01:41, wall=00:46 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.350 DataTime=0.248 Loss=3.261 Prec@1=30.573 Prec@5=56.196 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:46 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.350 DataTime=0.248 Loss=3.261 Prec@1=30.573 Prec@5=56.196 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:47 IST=> training   16.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.259 Prec@1=30.640 Prec@5=56.255 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:47 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.259 Prec@1=30.640 Prec@5=56.255 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:47 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.259 Prec@1=30.640 Prec@5=56.255 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:47 IST=> training   20.02% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.255 Prec@1=30.735 Prec@5=56.329 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=00:47 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.255 Prec@1=30.735 Prec@5=56.329 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=00:47 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.347 DataTime=0.245 Loss=3.255 Prec@1=30.735 Prec@5=56.329 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=00:48 IST=> training   24.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.344 DataTime=0.242 Loss=3.253 Prec@1=30.796 Prec@5=56.352 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=00:48 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.344 DataTime=0.242 Loss=3.253 Prec@1=30.796 Prec@5=56.352 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:48 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.344 DataTime=0.242 Loss=3.253 Prec@1=30.796 Prec@5=56.352 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:49 IST=> training   28.01% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.252 Prec@1=30.809 Prec@5=56.382 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:49 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.252 Prec@1=30.809 Prec@5=56.382 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:49 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.252 Prec@1=30.809 Prec@5=56.382 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:49 IST=> training   32.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.240 Loss=3.248 Prec@1=30.854 Prec@5=56.429 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=00:49 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.240 Loss=3.248 Prec@1=30.854 Prec@5=56.429 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:49 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.240 Loss=3.248 Prec@1=30.854 Prec@5=56.429 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:50 IST=> training   36.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.244 Prec@1=30.901 Prec@5=56.525 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:50 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.244 Prec@1=30.901 Prec@5=56.525 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:50 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.343 DataTime=0.240 Loss=3.244 Prec@1=30.901 Prec@5=56.525 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:50 IST=> training   39.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.239 Loss=3.238 Prec@1=30.993 Prec@5=56.643 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:50 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.239 Loss=3.238 Prec@1=30.993 Prec@5=56.643 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:50 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.342 DataTime=0.239 Loss=3.238 Prec@1=30.993 Prec@5=56.643 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:51 IST=> training   43.99% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.235 Prec@1=31.059 Prec@5=56.696 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:51 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.235 Prec@1=31.059 Prec@5=56.696 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=00:51 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.235 Prec@1=31.059 Prec@5=56.696 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=00:51 IST=> training   47.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.238 Loss=3.232 Prec@1=31.134 Prec@5=56.757 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=00:51 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.238 Loss=3.232 Prec@1=31.134 Prec@5=56.757 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:51 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.238 Loss=3.232 Prec@1=31.134 Prec@5=56.757 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:52 IST=> training   51.98% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.229 Prec@1=31.179 Prec@5=56.813 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=00:52 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.229 Prec@1=31.179 Prec@5=56.813 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:52 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.341 DataTime=0.237 Loss=3.229 Prec@1=31.179 Prec@5=56.813 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:52 IST=> training   55.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.226 Prec@1=31.249 Prec@5=56.869 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=00:52 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.226 Prec@1=31.249 Prec@5=56.869 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=00:52 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.226 Prec@1=31.249 Prec@5=56.869 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=00:53 IST=> training   59.97% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.223 Prec@1=31.300 Prec@5=56.926 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=00:53 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.223 Prec@1=31.300 Prec@5=56.926 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:53 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.223 Prec@1=31.300 Prec@5=56.926 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:54 IST=> training   63.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.219 Prec@1=31.355 Prec@5=56.991 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:54 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.219 Prec@1=31.355 Prec@5=56.991 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=00:54 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.219 Prec@1=31.355 Prec@5=56.991 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=00:54 IST=> training   67.96% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.217 Prec@1=31.400 Prec@5=57.039 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=00:54 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.217 Prec@1=31.400 Prec@5=57.039 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:54 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.217 Prec@1=31.400 Prec@5=57.039 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:55 IST=> training   71.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.213 Prec@1=31.456 Prec@5=57.111 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:55 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.213 Prec@1=31.456 Prec@5=57.111 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:55 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.236 Loss=3.213 Prec@1=31.456 Prec@5=57.111 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:55 IST=> training   75.95% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.210 Prec@1=31.508 Prec@5=57.161 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:55 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.210 Prec@1=31.508 Prec@5=57.161 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:55 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.210 Prec@1=31.508 Prec@5=57.161 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:56 IST=> training   79.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.208 Prec@1=31.556 Prec@5=57.189 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=00:56 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.208 Prec@1=31.556 Prec@5=57.189 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:56 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.208 Prec@1=31.556 Prec@5=57.189 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:56 IST=> training   83.94% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.205 Prec@1=31.594 Prec@5=57.243 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=00:56 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.205 Prec@1=31.594 Prec@5=57.243 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=00:56 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.340 DataTime=0.235 Loss=3.205 Prec@1=31.594 Prec@5=57.243 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=00:57 IST=> training   87.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.203 Prec@1=31.639 Prec@5=57.285 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=00:57 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.203 Prec@1=31.639 Prec@5=57.285 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:57 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.203 Prec@1=31.639 Prec@5=57.285 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:57 IST=> training   91.93% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.200 Prec@1=31.676 Prec@5=57.337 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=00:57 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.200 Prec@1=31.676 Prec@5=57.337 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:57 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.200 Prec@1=31.676 Prec@5=57.337 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:58 IST=> training   95.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.198 Prec@1=31.705 Prec@5=57.374 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:58 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.198 Prec@1=31.705 Prec@5=57.374 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:58 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.198 Prec@1=31.705 Prec@5=57.374 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:58 IST=> training   99.92% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.198 Prec@1=31.705 Prec@5=57.375 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:58 IST=> training   100.00% of 1x2503...Epoch=7/150 LR=0.09961 Time=0.339 DataTime=0.234 Loss=3.198 Prec@1=31.705 Prec@5=57.375 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:58 IST=> validation 0.00% of 1x98...Epoch=7/150 LR=0.09961 Time=6.109 Loss=3.851 Prec@1=25.977 Prec@5=46.875 rate=0 Hz, eta=?, total=0:00:00, wall=00:58 IST=> validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=6.109 Loss=3.851 Prec@1=25.977 Prec@5=46.875 rate=3826.51 Hz, eta=0:00:00, total=0:00:00, wall=00:58 IST** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=6.109 Loss=3.851 Prec@1=25.977 Prec@5=46.875 rate=3826.51 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST** validation 1.02% of 1x98...Epoch=7/150 LR=0.09961 Time=0.390 Loss=3.738 Prec@1=24.200 Prec@5=47.622 rate=3826.51 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST** validation 100.00% of 1x98...Epoch=7/150 LR=0.09961 Time=0.390 Loss=3.738 Prec@1=24.200 Prec@5=47.622 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=00:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.841 DataTime=4.683 Loss=3.106 Prec@1=35.352 Prec@5=57.812 rate=0 Hz, eta=?, total=0:00:00, wall=00:59 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.841 DataTime=4.683 Loss=3.106 Prec@1=35.352 Prec@5=57.812 rate=2514.41 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=4.841 DataTime=4.683 Loss=3.106 Prec@1=35.352 Prec@5=57.812 rate=2514.41 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST=> training   0.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.374 DataTime=0.279 Loss=3.080 Prec@1=33.611 Prec@5=59.723 rate=2514.41 Hz, eta=0:00:00, total=0:00:00, wall=00:59 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.374 DataTime=0.279 Loss=3.080 Prec@1=33.611 Prec@5=59.723 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=00:59 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.374 DataTime=0.279 Loss=3.080 Prec@1=33.611 Prec@5=59.723 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:00 IST=> training   4.04% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.352 DataTime=0.256 Loss=3.076 Prec@1=33.780 Prec@5=59.667 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:00 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.352 DataTime=0.256 Loss=3.076 Prec@1=33.780 Prec@5=59.667 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=01:00 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.352 DataTime=0.256 Loss=3.076 Prec@1=33.780 Prec@5=59.667 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=01:00 IST=> training   8.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.349 DataTime=0.252 Loss=3.073 Prec@1=33.762 Prec@5=59.703 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=01:00 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.349 DataTime=0.252 Loss=3.073 Prec@1=33.762 Prec@5=59.703 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=01:00 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.349 DataTime=0.252 Loss=3.073 Prec@1=33.762 Prec@5=59.703 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=01:01 IST=> training   12.03% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.346 DataTime=0.248 Loss=3.071 Prec@1=33.714 Prec@5=59.712 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=01:01 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.346 DataTime=0.248 Loss=3.071 Prec@1=33.714 Prec@5=59.712 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=01:01 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.346 DataTime=0.248 Loss=3.071 Prec@1=33.714 Prec@5=59.712 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=01:02 IST=> training   16.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.343 DataTime=0.245 Loss=3.072 Prec@1=33.713 Prec@5=59.724 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=01:02 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.343 DataTime=0.245 Loss=3.072 Prec@1=33.713 Prec@5=59.724 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=01:02 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.343 DataTime=0.245 Loss=3.072 Prec@1=33.713 Prec@5=59.724 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=01:02 IST=> training   20.02% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.242 Loss=3.071 Prec@1=33.733 Prec@5=59.755 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=01:02 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.242 Loss=3.071 Prec@1=33.733 Prec@5=59.755 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=01:02 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.242 Loss=3.071 Prec@1=33.733 Prec@5=59.755 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=01:03 IST=> training   24.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.342 DataTime=0.242 Loss=3.071 Prec@1=33.742 Prec@5=59.727 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=01:03 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.342 DataTime=0.242 Loss=3.071 Prec@1=33.742 Prec@5=59.727 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=01:03 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.342 DataTime=0.242 Loss=3.071 Prec@1=33.742 Prec@5=59.727 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=01:03 IST=> training   28.01% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.240 Loss=3.070 Prec@1=33.721 Prec@5=59.685 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=01:03 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.240 Loss=3.070 Prec@1=33.721 Prec@5=59.685 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=01:03 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.240 Loss=3.070 Prec@1=33.721 Prec@5=59.685 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=01:04 IST=> training   32.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.239 Loss=3.069 Prec@1=33.745 Prec@5=59.671 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=01:04 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.239 Loss=3.069 Prec@1=33.745 Prec@5=59.671 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=01:04 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.239 Loss=3.069 Prec@1=33.745 Prec@5=59.671 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=01:04 IST=> training   36.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.239 Loss=3.068 Prec@1=33.767 Prec@5=59.711 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=01:04 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.239 Loss=3.068 Prec@1=33.767 Prec@5=59.711 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=01:04 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.341 DataTime=0.239 Loss=3.068 Prec@1=33.767 Prec@5=59.711 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=01:05 IST=> training   39.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.068 Prec@1=33.785 Prec@5=59.716 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=01:05 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.068 Prec@1=33.785 Prec@5=59.716 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=01:05 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.068 Prec@1=33.785 Prec@5=59.716 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=01:06 IST=> training   43.99% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.066 Prec@1=33.837 Prec@5=59.731 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=01:06 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.066 Prec@1=33.837 Prec@5=59.731 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:06 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.237 Loss=3.066 Prec@1=33.837 Prec@5=59.731 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:06 IST=> training   47.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.237 Loss=3.066 Prec@1=33.851 Prec@5=59.749 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:06 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.237 Loss=3.066 Prec@1=33.851 Prec@5=59.749 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:06 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.237 Loss=3.066 Prec@1=33.851 Prec@5=59.749 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:07 IST=> training   51.98% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.063 Prec@1=33.884 Prec@5=59.801 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:07 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.063 Prec@1=33.884 Prec@5=59.801 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:07 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.063 Prec@1=33.884 Prec@5=59.801 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:07 IST=> training   55.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.062 Prec@1=33.899 Prec@5=59.822 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:07 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.062 Prec@1=33.899 Prec@5=59.822 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=01:07 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.236 Loss=3.062 Prec@1=33.899 Prec@5=59.822 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=01:08 IST=> training   59.97% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.236 Loss=3.061 Prec@1=33.930 Prec@5=59.855 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=01:08 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.236 Loss=3.061 Prec@1=33.930 Prec@5=59.855 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=01:08 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.340 DataTime=0.236 Loss=3.061 Prec@1=33.930 Prec@5=59.855 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=01:08 IST=> training   63.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.059 Prec@1=33.951 Prec@5=59.884 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=01:08 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.059 Prec@1=33.951 Prec@5=59.884 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:08 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.059 Prec@1=33.951 Prec@5=59.884 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:09 IST=> training   67.96% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.058 Prec@1=33.988 Prec@5=59.907 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:09 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.058 Prec@1=33.988 Prec@5=59.907 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:09 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.058 Prec@1=33.988 Prec@5=59.907 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:09 IST=> training   71.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.056 Prec@1=34.011 Prec@5=59.936 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:09 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.056 Prec@1=34.011 Prec@5=59.936 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:09 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.056 Prec@1=34.011 Prec@5=59.936 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:10 IST=> training   75.95% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.054 Prec@1=34.059 Prec@5=59.964 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:10 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.054 Prec@1=34.059 Prec@5=59.964 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=01:10 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.235 Loss=3.054 Prec@1=34.059 Prec@5=59.964 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=01:11 IST=> training   79.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.053 Prec@1=34.079 Prec@5=59.988 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=01:11 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.053 Prec@1=34.079 Prec@5=59.988 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:11 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.053 Prec@1=34.079 Prec@5=59.988 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:11 IST=> training   83.94% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.051 Prec@1=34.125 Prec@5=60.022 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:11 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.051 Prec@1=34.125 Prec@5=60.022 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:11 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.339 DataTime=0.234 Loss=3.051 Prec@1=34.125 Prec@5=60.022 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:12 IST=> training   87.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.049 Prec@1=34.150 Prec@5=60.049 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:12 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.049 Prec@1=34.150 Prec@5=60.049 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:12 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.049 Prec@1=34.150 Prec@5=60.049 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:12 IST=> training   91.93% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.047 Prec@1=34.193 Prec@5=60.096 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:12 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.047 Prec@1=34.193 Prec@5=60.096 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=01:12 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.047 Prec@1=34.193 Prec@5=60.096 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=01:13 IST=> training   95.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.045 Prec@1=34.233 Prec@5=60.128 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=01:13 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.045 Prec@1=34.233 Prec@5=60.128 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=01:13 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.045 Prec@1=34.233 Prec@5=60.128 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=01:13 IST=> training   99.92% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.045 Prec@1=34.234 Prec@5=60.128 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=01:13 IST=> training   100.00% of 1x2503...Epoch=8/150 LR=0.09946 Time=0.338 DataTime=0.234 Loss=3.045 Prec@1=34.234 Prec@5=60.128 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=01:13 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 0.00% of 1x98...Epoch=8/150 LR=0.09946 Time=6.558 Loss=3.160 Prec@1=33.789 Prec@5=59.766 rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST=> validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=6.558 Loss=3.160 Prec@1=33.789 Prec@5=59.766 rate=3274.79 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=6.558 Loss=3.160 Prec@1=33.789 Prec@5=59.766 rate=3274.79 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 1.02% of 1x98...Epoch=8/150 LR=0.09946 Time=0.390 Loss=3.117 Prec@1=32.542 Prec@5=58.760 rate=3274.79 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST** validation 100.00% of 1x98...Epoch=8/150 LR=0.09946 Time=0.390 Loss=3.117 Prec@1=32.542 Prec@5=58.760 rate=3.10 Hz, eta=0:00:00, total=0:00:31, wall=01:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST=> training   0.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.139 DataTime=4.976 Loss=3.028 Prec@1=32.227 Prec@5=62.109 rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.139 DataTime=4.976 Loss=3.028 Prec@1=32.227 Prec@5=62.109 rate=1961.63 Hz, eta=0:00:01, total=0:00:00, wall=01:14 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=5.139 DataTime=4.976 Loss=3.028 Prec@1=32.227 Prec@5=62.109 rate=1961.63 Hz, eta=0:00:01, total=0:00:00, wall=01:14 IST=> training   0.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.378 DataTime=0.278 Loss=2.957 Prec@1=35.709 Prec@5=61.775 rate=1961.63 Hz, eta=0:00:01, total=0:00:00, wall=01:14 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.378 DataTime=0.278 Loss=2.957 Prec@1=35.709 Prec@5=61.775 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=01:14 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.378 DataTime=0.278 Loss=2.957 Prec@1=35.709 Prec@5=61.775 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=01:15 IST=> training   4.04% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.360 DataTime=0.258 Loss=2.960 Prec@1=35.568 Prec@5=61.696 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=01:15 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.360 DataTime=0.258 Loss=2.960 Prec@1=35.568 Prec@5=61.696 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=01:15 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.360 DataTime=0.258 Loss=2.960 Prec@1=35.568 Prec@5=61.696 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=01:15 IST=> training   8.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.354 DataTime=0.249 Loss=2.960 Prec@1=35.672 Prec@5=61.720 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=01:15 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.354 DataTime=0.249 Loss=2.960 Prec@1=35.672 Prec@5=61.720 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=01:15 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.354 DataTime=0.249 Loss=2.960 Prec@1=35.672 Prec@5=61.720 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=01:16 IST=> training   12.03% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.353 DataTime=0.247 Loss=2.961 Prec@1=35.668 Prec@5=61.681 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=01:16 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.353 DataTime=0.247 Loss=2.961 Prec@1=35.668 Prec@5=61.681 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=01:16 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.353 DataTime=0.247 Loss=2.961 Prec@1=35.668 Prec@5=61.681 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=01:16 IST=> training   16.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.346 DataTime=0.240 Loss=2.958 Prec@1=35.703 Prec@5=61.708 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=01:16 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.346 DataTime=0.240 Loss=2.958 Prec@1=35.703 Prec@5=61.708 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=01:16 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.346 DataTime=0.240 Loss=2.958 Prec@1=35.703 Prec@5=61.708 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=01:17 IST=> training   20.02% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.959 Prec@1=35.738 Prec@5=61.693 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=01:17 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.959 Prec@1=35.738 Prec@5=61.693 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=01:17 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.959 Prec@1=35.738 Prec@5=61.693 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=01:18 IST=> training   24.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.960 Prec@1=35.763 Prec@5=61.681 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=01:18 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.960 Prec@1=35.763 Prec@5=61.681 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:18 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.345 DataTime=0.240 Loss=2.960 Prec@1=35.763 Prec@5=61.681 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:18 IST=> training   28.01% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.962 Prec@1=35.744 Prec@5=61.640 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:18 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.962 Prec@1=35.744 Prec@5=61.640 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=01:18 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.962 Prec@1=35.744 Prec@5=61.640 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=01:19 IST=> training   32.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.960 Prec@1=35.768 Prec@5=61.687 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=01:19 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.960 Prec@1=35.768 Prec@5=61.687 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=01:19 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.238 Loss=2.960 Prec@1=35.768 Prec@5=61.687 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=01:19 IST=> training   36.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.237 Loss=2.958 Prec@1=35.796 Prec@5=61.714 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=01:19 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.237 Loss=2.958 Prec@1=35.796 Prec@5=61.714 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=01:19 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.344 DataTime=0.237 Loss=2.958 Prec@1=35.796 Prec@5=61.714 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=01:20 IST=> training   39.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.957 Prec@1=35.795 Prec@5=61.722 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=01:20 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.957 Prec@1=35.795 Prec@5=61.722 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=01:20 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.957 Prec@1=35.795 Prec@5=61.722 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=01:20 IST=> training   43.99% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.955 Prec@1=35.816 Prec@5=61.755 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=01:20 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.955 Prec@1=35.816 Prec@5=61.755 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:20 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.955 Prec@1=35.816 Prec@5=61.755 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:21 IST=> training   47.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.954 Prec@1=35.853 Prec@5=61.784 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:21 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.954 Prec@1=35.853 Prec@5=61.784 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=01:21 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.342 DataTime=0.236 Loss=2.954 Prec@1=35.853 Prec@5=61.784 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=01:21 IST=> training   51.98% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.952 Prec@1=35.881 Prec@5=61.823 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=01:21 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.952 Prec@1=35.881 Prec@5=61.823 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=01:21 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.952 Prec@1=35.881 Prec@5=61.823 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=01:22 IST=> training   55.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.950 Prec@1=35.920 Prec@5=61.851 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=01:22 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.950 Prec@1=35.920 Prec@5=61.851 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=01:22 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.950 Prec@1=35.920 Prec@5=61.851 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=01:23 IST=> training   59.97% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.948 Prec@1=35.948 Prec@5=61.896 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=01:23 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.948 Prec@1=35.948 Prec@5=61.896 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=01:23 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.341 DataTime=0.235 Loss=2.948 Prec@1=35.948 Prec@5=61.896 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=01:23 IST=> training   63.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.945 Prec@1=35.972 Prec@5=61.933 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=01:23 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.945 Prec@1=35.972 Prec@5=61.933 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:23 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.945 Prec@1=35.972 Prec@5=61.933 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:24 IST=> training   67.96% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.945 Prec@1=35.988 Prec@5=61.951 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:24 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.945 Prec@1=35.988 Prec@5=61.951 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:24 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.945 Prec@1=35.988 Prec@5=61.951 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:24 IST=> training   71.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.943 Prec@1=36.026 Prec@5=61.973 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:24 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.943 Prec@1=36.026 Prec@5=61.973 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=01:24 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.943 Prec@1=36.026 Prec@5=61.973 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=01:25 IST=> training   75.95% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.941 Prec@1=36.056 Prec@5=62.012 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=01:25 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.941 Prec@1=36.056 Prec@5=62.012 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=01:25 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.235 Loss=2.941 Prec@1=36.056 Prec@5=62.012 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=01:25 IST=> training   79.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.939 Prec@1=36.082 Prec@5=62.051 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=01:25 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.939 Prec@1=36.082 Prec@5=62.051 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:25 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.939 Prec@1=36.082 Prec@5=62.051 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:26 IST=> training   83.94% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.938 Prec@1=36.101 Prec@5=62.083 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:26 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.938 Prec@1=36.101 Prec@5=62.083 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:26 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.340 DataTime=0.234 Loss=2.938 Prec@1=36.101 Prec@5=62.083 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:27 IST=> training   87.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.936 Prec@1=36.125 Prec@5=62.115 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:27 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.936 Prec@1=36.125 Prec@5=62.115 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=01:27 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.234 Loss=2.936 Prec@1=36.125 Prec@5=62.115 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=01:27 IST=> training   91.93% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.935 Prec@1=36.145 Prec@5=62.133 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=01:27 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.935 Prec@1=36.145 Prec@5=62.133 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:27 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.935 Prec@1=36.145 Prec@5=62.133 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:28 IST=> training   95.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.934 Prec@1=36.171 Prec@5=62.146 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:28 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.934 Prec@1=36.171 Prec@5=62.146 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=01:28 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.339 DataTime=0.233 Loss=2.934 Prec@1=36.171 Prec@5=62.146 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=01:28 IST=> training   99.92% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.338 DataTime=0.233 Loss=2.934 Prec@1=36.170 Prec@5=62.146 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=01:28 IST=> training   100.00% of 1x2503...Epoch=9/150 LR=0.09930 Time=0.338 DataTime=0.233 Loss=2.934 Prec@1=36.170 Prec@5=62.146 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=01:28 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> validation 0.00% of 1x98...Epoch=9/150 LR=0.09930 Time=6.274 Loss=2.943 Prec@1=34.570 Prec@5=62.695 rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.274 Loss=2.943 Prec@1=34.570 Prec@5=62.695 rate=4324.53 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=6.274 Loss=2.943 Prec@1=34.570 Prec@5=62.695 rate=4324.53 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST** validation 1.02% of 1x98...Epoch=9/150 LR=0.09930 Time=0.402 Loss=3.001 Prec@1=34.692 Prec@5=60.898 rate=4324.53 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST** validation 100.00% of 1x98...Epoch=9/150 LR=0.09930 Time=0.402 Loss=3.001 Prec@1=34.692 Prec@5=60.898 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=01:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> training   0.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=6.183 DataTime=6.093 Loss=2.814 Prec@1=41.406 Prec@5=65.039 rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=6.183 DataTime=6.093 Loss=2.814 Prec@1=41.406 Prec@5=65.039 rate=5411.87 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=6.183 DataTime=6.093 Loss=2.814 Prec@1=41.406 Prec@5=65.039 rate=5411.87 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST=> training   0.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.387 DataTime=0.295 Loss=2.861 Prec@1=37.403 Prec@5=63.359 rate=5411.87 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.387 DataTime=0.295 Loss=2.861 Prec@1=37.403 Prec@5=63.359 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:29 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.387 DataTime=0.295 Loss=2.861 Prec@1=37.403 Prec@5=63.359 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:30 IST=> training   4.04% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.357 DataTime=0.262 Loss=2.862 Prec@1=37.211 Prec@5=63.442 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:30 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.357 DataTime=0.262 Loss=2.862 Prec@1=37.211 Prec@5=63.442 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:30 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.357 DataTime=0.262 Loss=2.862 Prec@1=37.211 Prec@5=63.442 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:30 IST=> training   8.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.349 DataTime=0.250 Loss=2.862 Prec@1=37.340 Prec@5=63.395 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:30 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.349 DataTime=0.250 Loss=2.862 Prec@1=37.340 Prec@5=63.395 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=01:30 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.349 DataTime=0.250 Loss=2.862 Prec@1=37.340 Prec@5=63.395 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=01:31 IST=> training   12.03% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.345 DataTime=0.243 Loss=2.863 Prec@1=37.318 Prec@5=63.344 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.345 DataTime=0.243 Loss=2.863 Prec@1=37.318 Prec@5=63.344 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.345 DataTime=0.243 Loss=2.863 Prec@1=37.318 Prec@5=63.344 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=01:31 IST=> training   16.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.862 Prec@1=37.377 Prec@5=63.411 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=01:31 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.862 Prec@1=37.377 Prec@5=63.411 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=01:31 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.862 Prec@1=37.377 Prec@5=63.411 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=01:32 IST=> training   20.02% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.865 Prec@1=37.340 Prec@5=63.345 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=01:32 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.865 Prec@1=37.340 Prec@5=63.345 rate=3.04 Hz, eta=0:10:26, total=0:03:17, wall=01:32 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.865 Prec@1=37.340 Prec@5=63.345 rate=3.04 Hz, eta=0:10:26, total=0:03:17, wall=01:32 IST=> training   24.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.341 DataTime=0.239 Loss=2.863 Prec@1=37.334 Prec@5=63.375 rate=3.04 Hz, eta=0:10:26, total=0:03:17, wall=01:32 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.341 DataTime=0.239 Loss=2.863 Prec@1=37.334 Prec@5=63.375 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=01:32 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.341 DataTime=0.239 Loss=2.863 Prec@1=37.334 Prec@5=63.375 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=01:33 IST=> training   28.01% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.861 Prec@1=37.356 Prec@5=63.405 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=01:33 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.861 Prec@1=37.356 Prec@5=63.405 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=01:33 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.238 Loss=2.861 Prec@1=37.356 Prec@5=63.405 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=01:33 IST=> training   32.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.237 Loss=2.861 Prec@1=37.365 Prec@5=63.433 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=01:33 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.237 Loss=2.861 Prec@1=37.365 Prec@5=63.433 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=01:33 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.237 Loss=2.861 Prec@1=37.365 Prec@5=63.433 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=01:34 IST=> training   36.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.237 Loss=2.860 Prec@1=37.387 Prec@5=63.473 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=01:34 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.237 Loss=2.860 Prec@1=37.387 Prec@5=63.473 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=01:34 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.340 DataTime=0.237 Loss=2.860 Prec@1=37.387 Prec@5=63.473 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=01:35 IST=> training   39.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.236 Loss=2.860 Prec@1=37.393 Prec@5=63.463 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=01:35 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.236 Loss=2.860 Prec@1=37.393 Prec@5=63.463 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=01:35 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.339 DataTime=0.236 Loss=2.860 Prec@1=37.393 Prec@5=63.463 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=01:35 IST=> training   43.99% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.235 Loss=2.859 Prec@1=37.421 Prec@5=63.473 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=01:35 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.235 Loss=2.859 Prec@1=37.421 Prec@5=63.473 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=01:35 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.235 Loss=2.859 Prec@1=37.421 Prec@5=63.473 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=01:36 IST=> training   47.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.236 Loss=2.858 Prec@1=37.440 Prec@5=63.494 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=01:36 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.236 Loss=2.858 Prec@1=37.440 Prec@5=63.494 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=01:36 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.338 DataTime=0.236 Loss=2.858 Prec@1=37.440 Prec@5=63.494 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=01:36 IST=> training   51.98% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.857 Prec@1=37.450 Prec@5=63.505 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=01:36 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.857 Prec@1=37.450 Prec@5=63.505 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=01:36 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.857 Prec@1=37.450 Prec@5=63.505 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=01:37 IST=> training   55.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.481 Prec@5=63.523 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=01:37 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.481 Prec@5=63.523 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=01:37 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.481 Prec@5=63.523 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=01:37 IST=> training   59.97% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.496 Prec@5=63.528 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=01:37 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.496 Prec@5=63.528 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=01:37 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.496 Prec@5=63.528 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=01:38 IST=> training   63.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.498 Prec@5=63.524 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=01:38 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.498 Prec@5=63.524 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=01:38 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.856 Prec@1=37.498 Prec@5=63.524 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=01:38 IST=> training   67.96% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.855 Prec@1=37.508 Prec@5=63.545 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=01:38 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.855 Prec@1=37.508 Prec@5=63.545 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=01:38 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.855 Prec@1=37.508 Prec@5=63.545 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=01:39 IST=> training   71.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.854 Prec@1=37.527 Prec@5=63.563 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=01:39 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.854 Prec@1=37.527 Prec@5=63.563 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:39 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.234 Loss=2.854 Prec@1=37.527 Prec@5=63.563 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:40 IST=> training   75.95% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.854 Prec@1=37.540 Prec@5=63.589 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:40 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.854 Prec@1=37.540 Prec@5=63.589 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=01:40 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.854 Prec@1=37.540 Prec@5=63.589 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=01:40 IST=> training   79.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.233 Loss=2.853 Prec@1=37.549 Prec@5=63.607 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=01:40 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.233 Loss=2.853 Prec@1=37.549 Prec@5=63.607 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=01:40 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.233 Loss=2.853 Prec@1=37.549 Prec@5=63.607 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=01:41 IST=> training   83.94% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.852 Prec@1=37.561 Prec@5=63.624 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=01:41 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.852 Prec@1=37.561 Prec@5=63.624 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:41 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.852 Prec@1=37.561 Prec@5=63.624 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:41 IST=> training   87.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.851 Prec@1=37.575 Prec@5=63.630 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:41 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.851 Prec@1=37.575 Prec@5=63.630 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:41 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.337 DataTime=0.233 Loss=2.851 Prec@1=37.575 Prec@5=63.630 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:42 IST=> training   91.93% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.850 Prec@1=37.606 Prec@5=63.655 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:42 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.850 Prec@1=37.606 Prec@5=63.655 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=01:42 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.850 Prec@1=37.606 Prec@5=63.655 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=01:42 IST=> training   95.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.848 Prec@1=37.631 Prec@5=63.683 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=01:42 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.848 Prec@1=37.631 Prec@5=63.683 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=01:42 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.848 Prec@1=37.631 Prec@5=63.683 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=01:42 IST=> training   99.92% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.849 Prec@1=37.631 Prec@5=63.683 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=01:42 IST=> training   100.00% of 1x2503...Epoch=10/150 LR=0.09911 Time=0.336 DataTime=0.232 Loss=2.849 Prec@1=37.631 Prec@5=63.683 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=01:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:42 IST=> validation 0.00% of 1x98...Epoch=10/150 LR=0.09911 Time=6.508 Loss=2.908 Prec@1=32.812 Prec@5=62.891 rate=0 Hz, eta=?, total=0:00:00, wall=01:42 IST=> validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=6.508 Loss=2.908 Prec@1=32.812 Prec@5=62.891 rate=6834.43 Hz, eta=0:00:00, total=0:00:00, wall=01:42 IST** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=6.508 Loss=2.908 Prec@1=32.812 Prec@5=62.891 rate=6834.43 Hz, eta=0:00:00, total=0:00:00, wall=01:43 IST** validation 1.02% of 1x98...Epoch=10/150 LR=0.09911 Time=0.398 Loss=2.950 Prec@1=35.634 Prec@5=61.870 rate=6834.43 Hz, eta=0:00:00, total=0:00:00, wall=01:43 IST** validation 100.00% of 1x98...Epoch=10/150 LR=0.09911 Time=0.398 Loss=2.950 Prec@1=35.634 Prec@5=61.870 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=01:43 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:43 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:43 IST=> training   0.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.741 DataTime=5.612 Loss=2.890 Prec@1=36.914 Prec@5=63.672 rate=0 Hz, eta=?, total=0:00:00, wall=01:43 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.741 DataTime=5.612 Loss=2.890 Prec@1=36.914 Prec@5=63.672 rate=7197.61 Hz, eta=0:00:00, total=0:00:00, wall=01:43 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=5.741 DataTime=5.612 Loss=2.890 Prec@1=36.914 Prec@5=63.672 rate=7197.61 Hz, eta=0:00:00, total=0:00:00, wall=01:44 IST=> training   0.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.382 DataTime=0.288 Loss=2.792 Prec@1=38.575 Prec@5=64.598 rate=7197.61 Hz, eta=0:00:00, total=0:00:00, wall=01:44 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.382 DataTime=0.288 Loss=2.792 Prec@1=38.575 Prec@5=64.598 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:44 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.382 DataTime=0.288 Loss=2.792 Prec@1=38.575 Prec@5=64.598 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:44 IST=> training   4.04% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.353 DataTime=0.257 Loss=2.788 Prec@1=38.496 Prec@5=64.656 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:44 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.353 DataTime=0.257 Loss=2.788 Prec@1=38.496 Prec@5=64.656 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=01:44 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.353 DataTime=0.257 Loss=2.788 Prec@1=38.496 Prec@5=64.656 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=01:45 IST=> training   8.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.350 DataTime=0.253 Loss=2.791 Prec@1=38.350 Prec@5=64.598 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=01:45 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.350 DataTime=0.253 Loss=2.791 Prec@1=38.350 Prec@5=64.598 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=01:45 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.350 DataTime=0.253 Loss=2.791 Prec@1=38.350 Prec@5=64.598 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=01:45 IST=> training   12.03% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.245 Loss=2.785 Prec@1=38.519 Prec@5=64.719 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=01:45 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.245 Loss=2.785 Prec@1=38.519 Prec@5=64.719 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=01:45 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.245 Loss=2.785 Prec@1=38.519 Prec@5=64.719 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=01:46 IST=> training   16.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.244 Loss=2.783 Prec@1=38.627 Prec@5=64.783 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=01:46 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.244 Loss=2.783 Prec@1=38.627 Prec@5=64.783 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=01:46 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.244 Loss=2.783 Prec@1=38.627 Prec@5=64.783 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=01:46 IST=> training   20.02% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.343 DataTime=0.244 Loss=2.783 Prec@1=38.641 Prec@5=64.797 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=01:46 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.343 DataTime=0.244 Loss=2.783 Prec@1=38.641 Prec@5=64.797 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:46 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.343 DataTime=0.244 Loss=2.783 Prec@1=38.641 Prec@5=64.797 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:47 IST=> training   24.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.242 Loss=2.784 Prec@1=38.685 Prec@5=64.778 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:47 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.242 Loss=2.784 Prec@1=38.685 Prec@5=64.778 rate=3.00 Hz, eta=0:10:01, total=0:03:54, wall=01:47 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.242 Loss=2.784 Prec@1=38.685 Prec@5=64.778 rate=3.00 Hz, eta=0:10:01, total=0:03:54, wall=01:48 IST=> training   28.01% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.240 Loss=2.783 Prec@1=38.709 Prec@5=64.824 rate=3.00 Hz, eta=0:10:01, total=0:03:54, wall=01:48 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.240 Loss=2.783 Prec@1=38.709 Prec@5=64.824 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=01:48 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.240 Loss=2.783 Prec@1=38.709 Prec@5=64.824 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=01:48 IST=> training   32.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.241 Loss=2.782 Prec@1=38.741 Prec@5=64.844 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=01:48 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.241 Loss=2.782 Prec@1=38.741 Prec@5=64.844 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:48 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.342 DataTime=0.241 Loss=2.782 Prec@1=38.741 Prec@5=64.844 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:49 IST=> training   36.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.239 Loss=2.780 Prec@1=38.766 Prec@5=64.885 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:49 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.239 Loss=2.780 Prec@1=38.766 Prec@5=64.885 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:49 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.341 DataTime=0.239 Loss=2.780 Prec@1=38.766 Prec@5=64.885 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:49 IST=> training   39.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.780 Prec@1=38.779 Prec@5=64.895 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:49 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.780 Prec@1=38.779 Prec@5=64.895 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=01:49 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.780 Prec@1=38.779 Prec@5=64.895 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=01:50 IST=> training   43.99% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.779 Prec@1=38.780 Prec@5=64.906 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=01:50 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.779 Prec@1=38.780 Prec@5=64.906 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=01:50 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.340 DataTime=0.237 Loss=2.779 Prec@1=38.780 Prec@5=64.906 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=01:50 IST=> training   47.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.828 Prec@5=64.937 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=01:50 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.828 Prec@5=64.937 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=01:50 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.828 Prec@5=64.937 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=01:51 IST=> training   51.98% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.835 Prec@5=64.962 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=01:51 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.835 Prec@5=64.962 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:51 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.835 Prec@5=64.962 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:52 IST=> training   55.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.779 Prec@1=38.805 Prec@5=64.942 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:52 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.779 Prec@1=38.805 Prec@5=64.942 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:52 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.779 Prec@1=38.805 Prec@5=64.942 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:52 IST=> training   59.97% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.832 Prec@5=64.945 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:52 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.832 Prec@5=64.945 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=01:52 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.778 Prec@1=38.832 Prec@5=64.945 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=01:53 IST=> training   63.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.837 Prec@5=64.957 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=01:53 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.837 Prec@5=64.957 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=01:53 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.837 Prec@5=64.957 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=01:53 IST=> training   67.96% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.840 Prec@5=64.958 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=01:53 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.840 Prec@5=64.958 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=01:53 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.339 DataTime=0.236 Loss=2.777 Prec@1=38.840 Prec@5=64.958 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=01:54 IST=> training   71.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.832 Prec@5=64.957 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=01:54 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.832 Prec@5=64.957 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=01:54 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.832 Prec@5=64.957 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=01:54 IST=> training   75.95% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.846 Prec@5=64.958 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=01:54 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.846 Prec@5=64.958 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=01:54 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.777 Prec@1=38.846 Prec@5=64.958 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=01:55 IST=> training   79.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.236 Loss=2.776 Prec@1=38.863 Prec@5=64.971 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=01:55 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.236 Loss=2.776 Prec@1=38.863 Prec@5=64.971 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=01:55 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.236 Loss=2.776 Prec@1=38.863 Prec@5=64.971 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=01:55 IST=> training   83.94% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.776 Prec@1=38.867 Prec@5=64.969 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=01:55 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.776 Prec@1=38.867 Prec@5=64.969 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=01:55 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.776 Prec@1=38.867 Prec@5=64.969 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=01:56 IST=> training   87.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.776 Prec@1=38.860 Prec@5=64.959 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=01:56 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.776 Prec@1=38.860 Prec@5=64.959 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=01:56 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.776 Prec@1=38.860 Prec@5=64.959 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=01:57 IST=> training   91.93% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.775 Prec@1=38.875 Prec@5=64.970 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=01:57 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.775 Prec@1=38.875 Prec@5=64.970 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=01:57 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.338 DataTime=0.235 Loss=2.775 Prec@1=38.875 Prec@5=64.970 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=01:57 IST=> training   95.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.774 Prec@1=38.883 Prec@5=64.987 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=01:57 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.774 Prec@1=38.883 Prec@5=64.987 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=01:57 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.235 Loss=2.774 Prec@1=38.883 Prec@5=64.987 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=01:57 IST=> training   99.92% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.234 Loss=2.775 Prec@1=38.883 Prec@5=64.986 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=01:57 IST=> training   100.00% of 1x2503...Epoch=11/150 LR=0.09891 Time=0.337 DataTime=0.234 Loss=2.775 Prec@1=38.883 Prec@5=64.986 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=01:57 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:57 IST=> validation 0.00% of 1x98...Epoch=11/150 LR=0.09891 Time=6.655 Loss=2.913 Prec@1=36.914 Prec@5=62.500 rate=0 Hz, eta=?, total=0:00:00, wall=01:57 IST=> validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.655 Loss=2.913 Prec@1=36.914 Prec@5=62.500 rate=3984.63 Hz, eta=0:00:00, total=0:00:00, wall=01:57 IST** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=6.655 Loss=2.913 Prec@1=36.914 Prec@5=62.500 rate=3984.63 Hz, eta=0:00:00, total=0:00:00, wall=01:58 IST** validation 1.02% of 1x98...Epoch=11/150 LR=0.09891 Time=0.394 Loss=2.831 Prec@1=37.398 Prec@5=63.876 rate=3984.63 Hz, eta=0:00:00, total=0:00:00, wall=01:58 IST** validation 100.00% of 1x98...Epoch=11/150 LR=0.09891 Time=0.394 Loss=2.831 Prec@1=37.398 Prec@5=63.876 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=01:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:58 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:58 IST=> training   0.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=5.722 DataTime=5.584 Loss=2.649 Prec@1=37.305 Prec@5=66.797 rate=0 Hz, eta=?, total=0:00:00, wall=01:58 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=5.722 DataTime=5.584 Loss=2.649 Prec@1=37.305 Prec@5=66.797 rate=6429.01 Hz, eta=0:00:00, total=0:00:00, wall=01:58 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=5.722 DataTime=5.584 Loss=2.649 Prec@1=37.305 Prec@5=66.797 rate=6429.01 Hz, eta=0:00:00, total=0:00:00, wall=01:58 IST=> training   0.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.383 DataTime=0.289 Loss=2.702 Prec@1=39.906 Prec@5=66.344 rate=6429.01 Hz, eta=0:00:00, total=0:00:00, wall=01:58 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.383 DataTime=0.289 Loss=2.702 Prec@1=39.906 Prec@5=66.344 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:58 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.383 DataTime=0.289 Loss=2.702 Prec@1=39.906 Prec@5=66.344 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:59 IST=> training   4.04% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.355 DataTime=0.257 Loss=2.708 Prec@1=40.019 Prec@5=66.176 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=01:59 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.355 DataTime=0.257 Loss=2.708 Prec@1=40.019 Prec@5=66.176 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:59 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.355 DataTime=0.257 Loss=2.708 Prec@1=40.019 Prec@5=66.176 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:59 IST=> training   8.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.244 Loss=2.709 Prec@1=39.987 Prec@5=66.116 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=01:59 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.244 Loss=2.709 Prec@1=39.987 Prec@5=66.116 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=01:59 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.244 Loss=2.709 Prec@1=39.987 Prec@5=66.116 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:00 IST=> training   12.03% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.242 Loss=2.713 Prec@1=39.986 Prec@5=66.030 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:00 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.242 Loss=2.713 Prec@1=39.986 Prec@5=66.030 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=02:00 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.346 DataTime=0.242 Loss=2.713 Prec@1=39.986 Prec@5=66.030 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=02:01 IST=> training   16.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.342 DataTime=0.238 Loss=2.715 Prec@1=39.994 Prec@5=65.994 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=02:01 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.342 DataTime=0.238 Loss=2.715 Prec@1=39.994 Prec@5=65.994 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=02:01 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.342 DataTime=0.238 Loss=2.715 Prec@1=39.994 Prec@5=65.994 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=02:01 IST=> training   20.02% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.715 Prec@1=39.985 Prec@5=65.997 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=02:01 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.715 Prec@1=39.985 Prec@5=65.997 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:01 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.715 Prec@1=39.985 Prec@5=65.997 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:02 IST=> training   24.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.716 Prec@1=39.964 Prec@5=65.985 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:02 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.716 Prec@1=39.964 Prec@5=65.985 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:02 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.236 Loss=2.716 Prec@1=39.964 Prec@5=65.985 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:02 IST=> training   28.01% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.235 Loss=2.717 Prec@1=39.942 Prec@5=65.971 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:02 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.235 Loss=2.717 Prec@1=39.942 Prec@5=65.971 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=02:02 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.235 Loss=2.717 Prec@1=39.942 Prec@5=65.971 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=02:03 IST=> training   32.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.973 Prec@5=65.972 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=02:03 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.973 Prec@5=65.972 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=02:03 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.973 Prec@5=65.972 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=02:03 IST=> training   36.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.718 Prec@1=39.959 Prec@5=65.966 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=02:03 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.718 Prec@1=39.959 Prec@5=65.966 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=02:03 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.718 Prec@1=39.959 Prec@5=65.966 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=02:04 IST=> training   39.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.716 Prec@1=39.987 Prec@5=65.998 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=02:04 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.716 Prec@1=39.987 Prec@5=65.998 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:04 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.340 DataTime=0.235 Loss=2.716 Prec@1=39.987 Prec@5=65.998 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:05 IST=> training   43.99% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.234 Loss=2.717 Prec@1=39.965 Prec@5=65.978 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:05 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.234 Loss=2.717 Prec@1=39.965 Prec@5=65.978 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:05 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.234 Loss=2.717 Prec@1=39.965 Prec@5=65.978 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:05 IST=> training   47.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.984 Prec@5=65.990 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:05 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.984 Prec@5=65.990 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=02:05 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.234 Loss=2.717 Prec@1=39.984 Prec@5=65.990 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=02:06 IST=> training   51.98% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.717 Prec@1=39.959 Prec@5=66.002 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=02:06 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.717 Prec@1=39.959 Prec@5=66.002 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:06 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.717 Prec@1=39.959 Prec@5=66.002 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:06 IST=> training   55.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.233 Loss=2.718 Prec@1=39.923 Prec@5=65.998 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:06 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.233 Loss=2.718 Prec@1=39.923 Prec@5=65.998 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:06 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.233 Loss=2.718 Prec@1=39.923 Prec@5=65.998 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:07 IST=> training   59.97% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.719 Prec@1=39.927 Prec@5=65.973 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:07 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.719 Prec@1=39.927 Prec@5=65.973 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=02:07 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.233 Loss=2.719 Prec@1=39.927 Prec@5=65.973 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=02:07 IST=> training   63.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.719 Prec@1=39.924 Prec@5=65.985 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=02:07 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.719 Prec@1=39.924 Prec@5=65.985 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:07 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.719 Prec@1=39.924 Prec@5=65.985 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:08 IST=> training   67.96% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.935 Prec@5=65.996 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:08 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.935 Prec@5=65.996 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=02:08 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.935 Prec@5=65.996 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=02:08 IST=> training   71.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.718 Prec@1=39.952 Prec@5=66.007 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=02:08 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.718 Prec@1=39.952 Prec@5=66.007 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:08 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.232 Loss=2.718 Prec@1=39.952 Prec@5=66.007 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:09 IST=> training   75.95% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.959 Prec@5=66.017 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:09 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.959 Prec@5=66.017 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:09 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.959 Prec@5=66.017 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:10 IST=> training   79.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.972 Prec@5=66.006 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:10 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.972 Prec@5=66.006 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:10 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.718 Prec@1=39.972 Prec@5=66.006 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:10 IST=> training   83.94% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.232 Loss=2.717 Prec@1=39.998 Prec@5=66.015 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:10 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.232 Loss=2.717 Prec@1=39.998 Prec@5=66.015 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=02:10 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.232 Loss=2.717 Prec@1=39.998 Prec@5=66.015 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=02:11 IST=> training   87.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.231 Loss=2.716 Prec@1=40.026 Prec@5=66.027 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=02:11 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.231 Loss=2.716 Prec@1=40.026 Prec@5=66.027 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=02:11 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.339 DataTime=0.231 Loss=2.716 Prec@1=40.026 Prec@5=66.027 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=02:11 IST=> training   91.93% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.716 Prec@1=40.034 Prec@5=66.035 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=02:11 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.716 Prec@1=40.034 Prec@5=66.035 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:11 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.716 Prec@1=40.034 Prec@5=66.035 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:12 IST=> training   95.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.715 Prec@1=40.059 Prec@5=66.043 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:12 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.715 Prec@1=40.059 Prec@5=66.043 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=02:12 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.715 Prec@1=40.059 Prec@5=66.043 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=02:12 IST=> training   99.92% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.715 Prec@1=40.058 Prec@5=66.042 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=02:12 IST=> training   100.00% of 1x2503...Epoch=12/150 LR=0.09868 Time=0.338 DataTime=0.231 Loss=2.715 Prec@1=40.058 Prec@5=66.042 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=02:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:12 IST=> validation 0.00% of 1x98...Epoch=12/150 LR=0.09868 Time=6.554 Loss=3.386 Prec@1=30.664 Prec@5=53.320 rate=0 Hz, eta=?, total=0:00:00, wall=02:12 IST=> validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=6.554 Loss=3.386 Prec@1=30.664 Prec@5=53.320 rate=5987.41 Hz, eta=0:00:00, total=0:00:00, wall=02:12 IST** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=6.554 Loss=3.386 Prec@1=30.664 Prec@5=53.320 rate=5987.41 Hz, eta=0:00:00, total=0:00:00, wall=02:12 IST** validation 1.02% of 1x98...Epoch=12/150 LR=0.09868 Time=0.387 Loss=3.529 Prec@1=27.512 Prec@5=51.920 rate=5987.41 Hz, eta=0:00:00, total=0:00:00, wall=02:12 IST** validation 100.00% of 1x98...Epoch=12/150 LR=0.09868 Time=0.387 Loss=3.529 Prec@1=27.512 Prec@5=51.920 rate=3.13 Hz, eta=0:00:00, total=0:00:31, wall=02:12 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST=> training   0.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=4.936 DataTime=4.791 Loss=2.565 Prec@1=41.406 Prec@5=68.164 rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=4.936 DataTime=4.791 Loss=2.565 Prec@1=41.406 Prec@5=68.164 rate=2644.48 Hz, eta=0:00:00, total=0:00:00, wall=02:13 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=4.936 DataTime=4.791 Loss=2.565 Prec@1=41.406 Prec@5=68.164 rate=2644.48 Hz, eta=0:00:00, total=0:00:00, wall=02:13 IST=> training   0.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.381 DataTime=0.284 Loss=2.659 Prec@1=41.010 Prec@5=66.942 rate=2644.48 Hz, eta=0:00:00, total=0:00:00, wall=02:13 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.381 DataTime=0.284 Loss=2.659 Prec@1=41.010 Prec@5=66.942 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=02:13 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.381 DataTime=0.284 Loss=2.659 Prec@1=41.010 Prec@5=66.942 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=02:14 IST=> training   4.04% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.361 DataTime=0.261 Loss=2.654 Prec@1=40.980 Prec@5=66.917 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=02:14 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.361 DataTime=0.261 Loss=2.654 Prec@1=40.980 Prec@5=66.917 rate=2.98 Hz, eta=0:12:53, total=0:01:07, wall=02:14 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.361 DataTime=0.261 Loss=2.654 Prec@1=40.980 Prec@5=66.917 rate=2.98 Hz, eta=0:12:53, total=0:01:07, wall=02:14 IST=> training   8.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.352 DataTime=0.249 Loss=2.652 Prec@1=41.083 Prec@5=67.032 rate=2.98 Hz, eta=0:12:53, total=0:01:07, wall=02:14 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.352 DataTime=0.249 Loss=2.652 Prec@1=41.083 Prec@5=67.032 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=02:14 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.352 DataTime=0.249 Loss=2.652 Prec@1=41.083 Prec@5=67.032 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=02:15 IST=> training   12.03% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.351 DataTime=0.247 Loss=2.653 Prec@1=41.126 Prec@5=67.015 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=02:15 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.351 DataTime=0.247 Loss=2.653 Prec@1=41.126 Prec@5=67.015 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=02:15 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.351 DataTime=0.247 Loss=2.653 Prec@1=41.126 Prec@5=67.015 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=02:15 IST=> training   16.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.348 DataTime=0.244 Loss=2.658 Prec@1=41.002 Prec@5=66.917 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=02:15 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.348 DataTime=0.244 Loss=2.658 Prec@1=41.002 Prec@5=66.917 rate=2.95 Hz, eta=0:11:17, total=0:02:49, wall=02:15 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.348 DataTime=0.244 Loss=2.658 Prec@1=41.002 Prec@5=66.917 rate=2.95 Hz, eta=0:11:17, total=0:02:49, wall=02:16 IST=> training   20.02% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.658 Prec@1=40.933 Prec@5=66.961 rate=2.95 Hz, eta=0:11:17, total=0:02:49, wall=02:16 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.658 Prec@1=40.933 Prec@5=66.961 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=02:16 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.658 Prec@1=40.933 Prec@5=66.961 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=02:17 IST=> training   24.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.659 Prec@1=40.939 Prec@5=66.986 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=02:17 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.659 Prec@1=40.939 Prec@5=66.986 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=02:17 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.345 DataTime=0.241 Loss=2.659 Prec@1=40.939 Prec@5=66.986 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=02:17 IST=> training   28.01% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.344 DataTime=0.239 Loss=2.659 Prec@1=40.956 Prec@5=66.978 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=02:17 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.344 DataTime=0.239 Loss=2.659 Prec@1=40.956 Prec@5=66.978 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=02:17 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.344 DataTime=0.239 Loss=2.659 Prec@1=40.956 Prec@5=66.978 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=02:18 IST=> training   32.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.660 Prec@1=40.948 Prec@5=66.947 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=02:18 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.660 Prec@1=40.948 Prec@5=66.947 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=02:18 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.660 Prec@1=40.948 Prec@5=66.947 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=02:18 IST=> training   36.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.663 Prec@1=40.876 Prec@5=66.914 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=02:18 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.663 Prec@1=40.876 Prec@5=66.914 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=02:18 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.238 Loss=2.663 Prec@1=40.876 Prec@5=66.914 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=02:19 IST=> training   39.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.664 Prec@1=40.884 Prec@5=66.894 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=02:19 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.664 Prec@1=40.884 Prec@5=66.894 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=02:19 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.664 Prec@1=40.884 Prec@5=66.894 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=02:19 IST=> training   43.99% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.664 Prec@1=40.913 Prec@5=66.908 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=02:19 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.664 Prec@1=40.913 Prec@5=66.908 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=02:19 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.664 Prec@1=40.913 Prec@5=66.908 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=02:20 IST=> training   47.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.665 Prec@1=40.890 Prec@5=66.895 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=02:20 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.665 Prec@1=40.890 Prec@5=66.895 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=02:20 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.343 DataTime=0.237 Loss=2.665 Prec@1=40.890 Prec@5=66.895 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=02:20 IST=> training   51.98% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.665 Prec@1=40.899 Prec@5=66.898 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=02:20 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.665 Prec@1=40.899 Prec@5=66.898 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=02:20 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.236 Loss=2.665 Prec@1=40.899 Prec@5=66.898 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=02:21 IST=> training   55.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.663 Prec@1=40.910 Prec@5=66.934 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=02:21 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.663 Prec@1=40.910 Prec@5=66.934 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=02:21 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.663 Prec@1=40.910 Prec@5=66.934 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=02:22 IST=> training   59.97% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.235 Loss=2.664 Prec@1=40.881 Prec@5=66.919 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=02:22 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.235 Loss=2.664 Prec@1=40.881 Prec@5=66.919 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=02:22 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.342 DataTime=0.235 Loss=2.664 Prec@1=40.881 Prec@5=66.919 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=02:22 IST=> training   63.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.664 Prec@1=40.880 Prec@5=66.922 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=02:22 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.664 Prec@1=40.880 Prec@5=66.922 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=02:22 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.235 Loss=2.664 Prec@1=40.880 Prec@5=66.922 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=02:23 IST=> training   67.96% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.884 Prec@5=66.931 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=02:23 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.884 Prec@5=66.931 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=02:23 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.884 Prec@5=66.931 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=02:23 IST=> training   71.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.889 Prec@5=66.926 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=02:23 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.889 Prec@5=66.926 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:23 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.889 Prec@5=66.926 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:24 IST=> training   75.95% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.909 Prec@5=66.937 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:24 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.909 Prec@5=66.937 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=02:24 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.664 Prec@1=40.909 Prec@5=66.937 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=02:24 IST=> training   79.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.923 Prec@5=66.948 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=02:24 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.923 Prec@5=66.948 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=02:24 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.923 Prec@5=66.948 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=02:25 IST=> training   83.94% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.929 Prec@5=66.951 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=02:25 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.929 Prec@5=66.951 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:25 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.341 DataTime=0.234 Loss=2.663 Prec@1=40.929 Prec@5=66.951 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:26 IST=> training   87.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.234 Loss=2.662 Prec@1=40.952 Prec@5=66.969 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:26 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.234 Loss=2.662 Prec@1=40.952 Prec@5=66.969 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=02:26 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.234 Loss=2.662 Prec@1=40.952 Prec@5=66.969 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=02:26 IST=> training   91.93% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.952 Prec@5=66.965 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=02:26 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.952 Prec@5=66.965 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:26 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.952 Prec@5=66.965 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:27 IST=> training   95.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.950 Prec@5=66.961 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:27 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.950 Prec@5=66.961 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:27 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.340 DataTime=0.233 Loss=2.662 Prec@1=40.950 Prec@5=66.961 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:27 IST=> training   99.92% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.339 DataTime=0.233 Loss=2.663 Prec@1=40.948 Prec@5=66.959 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:27 IST=> training   100.00% of 1x2503...Epoch=13/150 LR=0.09843 Time=0.339 DataTime=0.233 Loss=2.663 Prec@1=40.948 Prec@5=66.959 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> validation 0.00% of 1x98...Epoch=13/150 LR=0.09843 Time=6.053 Loss=3.080 Prec@1=34.961 Prec@5=59.180 rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.053 Loss=3.080 Prec@1=34.961 Prec@5=59.180 rate=4435.53 Hz, eta=0:00:00, total=0:00:00, wall=02:27 IST** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=6.053 Loss=3.080 Prec@1=34.961 Prec@5=59.180 rate=4435.53 Hz, eta=0:00:00, total=0:00:00, wall=02:27 IST** validation 1.02% of 1x98...Epoch=13/150 LR=0.09843 Time=0.397 Loss=2.968 Prec@1=35.412 Prec@5=61.562 rate=4435.53 Hz, eta=0:00:00, total=0:00:00, wall=02:27 IST** validation 100.00% of 1x98...Epoch=13/150 LR=0.09843 Time=0.397 Loss=2.968 Prec@1=35.412 Prec@5=61.562 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=02:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> training   0.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.522 DataTime=5.352 Loss=2.494 Prec@1=44.531 Prec@5=70.898 rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.522 DataTime=5.352 Loss=2.494 Prec@1=44.531 Prec@5=70.898 rate=2981.40 Hz, eta=0:00:00, total=0:00:00, wall=02:27 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=5.522 DataTime=5.352 Loss=2.494 Prec@1=44.531 Prec@5=70.898 rate=2981.40 Hz, eta=0:00:00, total=0:00:00, wall=02:28 IST=> training   0.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.375 DataTime=0.281 Loss=2.613 Prec@1=41.778 Prec@5=67.737 rate=2981.40 Hz, eta=0:00:00, total=0:00:00, wall=02:28 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.375 DataTime=0.281 Loss=2.613 Prec@1=41.778 Prec@5=67.737 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=02:28 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.375 DataTime=0.281 Loss=2.613 Prec@1=41.778 Prec@5=67.737 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=02:29 IST=> training   4.04% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.353 DataTime=0.258 Loss=2.610 Prec@1=41.685 Prec@5=67.842 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=02:29 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.353 DataTime=0.258 Loss=2.610 Prec@1=41.685 Prec@5=67.842 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=02:29 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.353 DataTime=0.258 Loss=2.610 Prec@1=41.685 Prec@5=67.842 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=02:29 IST=> training   8.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.345 DataTime=0.248 Loss=2.608 Prec@1=41.788 Prec@5=67.957 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=02:29 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.345 DataTime=0.248 Loss=2.608 Prec@1=41.788 Prec@5=67.957 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:29 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.345 DataTime=0.248 Loss=2.608 Prec@1=41.788 Prec@5=67.957 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:30 IST=> training   12.03% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.342 DataTime=0.243 Loss=2.613 Prec@1=41.755 Prec@5=67.823 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:30 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.342 DataTime=0.243 Loss=2.613 Prec@1=41.755 Prec@5=67.823 rate=3.04 Hz, eta=0:11:30, total=0:02:11, wall=02:30 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.342 DataTime=0.243 Loss=2.613 Prec@1=41.755 Prec@5=67.823 rate=3.04 Hz, eta=0:11:30, total=0:02:11, wall=02:30 IST=> training   16.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.612 Prec@1=41.743 Prec@5=67.819 rate=3.04 Hz, eta=0:11:30, total=0:02:11, wall=02:30 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.612 Prec@1=41.743 Prec@5=67.819 rate=3.04 Hz, eta=0:10:57, total=0:02:44, wall=02:30 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.612 Prec@1=41.743 Prec@5=67.819 rate=3.04 Hz, eta=0:10:57, total=0:02:44, wall=02:31 IST=> training   20.02% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.613 Prec@1=41.764 Prec@5=67.817 rate=3.04 Hz, eta=0:10:57, total=0:02:44, wall=02:31 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.613 Prec@1=41.764 Prec@5=67.817 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=02:31 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.339 DataTime=0.240 Loss=2.613 Prec@1=41.764 Prec@5=67.817 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=02:31 IST=> training   24.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.237 Loss=2.614 Prec@1=41.729 Prec@5=67.776 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=02:31 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.237 Loss=2.614 Prec@1=41.729 Prec@5=67.776 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=02:31 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.237 Loss=2.614 Prec@1=41.729 Prec@5=67.776 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=02:32 IST=> training   28.01% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.236 Loss=2.613 Prec@1=41.764 Prec@5=67.772 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=02:32 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.236 Loss=2.613 Prec@1=41.764 Prec@5=67.772 rate=3.03 Hz, eta=0:09:21, total=0:04:24, wall=02:32 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.236 Loss=2.613 Prec@1=41.764 Prec@5=67.772 rate=3.03 Hz, eta=0:09:21, total=0:04:24, wall=02:32 IST=> training   32.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.235 Loss=2.614 Prec@1=41.785 Prec@5=67.747 rate=3.03 Hz, eta=0:09:21, total=0:04:24, wall=02:32 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.235 Loss=2.614 Prec@1=41.785 Prec@5=67.747 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=02:32 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.235 Loss=2.614 Prec@1=41.785 Prec@5=67.747 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=02:33 IST=> training   36.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.235 Loss=2.616 Prec@1=41.756 Prec@5=67.714 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=02:33 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.235 Loss=2.616 Prec@1=41.756 Prec@5=67.714 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=02:33 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.338 DataTime=0.235 Loss=2.616 Prec@1=41.756 Prec@5=67.714 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=02:34 IST=> training   39.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.234 Loss=2.617 Prec@1=41.756 Prec@5=67.714 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=02:34 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.234 Loss=2.617 Prec@1=41.756 Prec@5=67.714 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:34 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.234 Loss=2.617 Prec@1=41.756 Prec@5=67.714 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:34 IST=> training   43.99% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.233 Loss=2.618 Prec@1=41.735 Prec@5=67.694 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:34 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.233 Loss=2.618 Prec@1=41.735 Prec@5=67.694 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=02:34 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.233 Loss=2.618 Prec@1=41.735 Prec@5=67.694 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=02:35 IST=> training   47.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.233 Loss=2.619 Prec@1=41.723 Prec@5=67.668 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=02:35 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.233 Loss=2.619 Prec@1=41.723 Prec@5=67.668 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:35 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.337 DataTime=0.233 Loss=2.619 Prec@1=41.723 Prec@5=67.668 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:35 IST=> training   51.98% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.232 Loss=2.618 Prec@1=41.743 Prec@5=67.677 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:35 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.232 Loss=2.618 Prec@1=41.743 Prec@5=67.677 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:35 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.232 Loss=2.618 Prec@1=41.743 Prec@5=67.677 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:36 IST=> training   55.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.617 Prec@1=41.744 Prec@5=67.676 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:36 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.617 Prec@1=41.744 Prec@5=67.676 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=02:36 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.617 Prec@1=41.744 Prec@5=67.676 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=02:36 IST=> training   59.97% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.742 Prec@5=67.679 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=02:36 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.742 Prec@5=67.679 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=02:36 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.742 Prec@5=67.679 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=02:37 IST=> training   63.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.734 Prec@5=67.674 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=02:37 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.734 Prec@5=67.674 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=02:37 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.618 Prec@1=41.734 Prec@5=67.674 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=02:37 IST=> training   67.96% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.619 Prec@1=41.739 Prec@5=67.672 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=02:37 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.619 Prec@1=41.739 Prec@5=67.672 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=02:37 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.619 Prec@1=41.739 Prec@5=67.672 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=02:38 IST=> training   71.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.729 Prec@5=67.653 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=02:38 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.729 Prec@5=67.653 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=02:38 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.729 Prec@5=67.653 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=02:39 IST=> training   75.95% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.718 Prec@5=67.656 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=02:39 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.718 Prec@5=67.656 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:39 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.231 Loss=2.620 Prec@1=41.718 Prec@5=67.656 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:39 IST=> training   79.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.620 Prec@1=41.730 Prec@5=67.651 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:39 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.620 Prec@1=41.730 Prec@5=67.651 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=02:39 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.620 Prec@1=41.730 Prec@5=67.651 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=02:40 IST=> training   83.94% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.750 Prec@5=67.680 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=02:40 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.750 Prec@5=67.680 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:40 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.750 Prec@5=67.680 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:40 IST=> training   87.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.741 Prec@5=67.688 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:40 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.741 Prec@5=67.688 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:40 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.741 Prec@5=67.688 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:41 IST=> training   91.93% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.754 Prec@5=67.696 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:41 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.754 Prec@5=67.696 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=02:41 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.754 Prec@5=67.696 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=02:41 IST=> training   95.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.753 Prec@5=67.692 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=02:41 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.753 Prec@5=67.692 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=02:41 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.336 DataTime=0.230 Loss=2.619 Prec@1=41.753 Prec@5=67.692 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=02:41 IST=> training   99.92% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.335 DataTime=0.229 Loss=2.619 Prec@1=41.752 Prec@5=67.690 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=02:41 IST=> training   100.00% of 1x2503...Epoch=14/150 LR=0.09816 Time=0.335 DataTime=0.229 Loss=2.619 Prec@1=41.752 Prec@5=67.690 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=02:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:41 IST=> validation 0.00% of 1x98...Epoch=14/150 LR=0.09816 Time=6.443 Loss=2.950 Prec@1=35.547 Prec@5=62.695 rate=0 Hz, eta=?, total=0:00:00, wall=02:41 IST=> validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=6.443 Loss=2.950 Prec@1=35.547 Prec@5=62.695 rate=3293.45 Hz, eta=0:00:00, total=0:00:00, wall=02:41 IST** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=6.443 Loss=2.950 Prec@1=35.547 Prec@5=62.695 rate=3293.45 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST** validation 1.02% of 1x98...Epoch=14/150 LR=0.09816 Time=0.403 Loss=2.879 Prec@1=37.008 Prec@5=63.054 rate=3293.45 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST** validation 100.00% of 1x98...Epoch=14/150 LR=0.09816 Time=0.403 Loss=2.879 Prec@1=37.008 Prec@5=63.054 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=02:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST=> training   0.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.474 DataTime=5.300 Loss=2.549 Prec@1=44.727 Prec@5=69.727 rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.474 DataTime=5.300 Loss=2.549 Prec@1=44.727 Prec@5=69.727 rate=4242.43 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=5.474 DataTime=5.300 Loss=2.549 Prec@1=44.727 Prec@5=69.727 rate=4242.43 Hz, eta=0:00:00, total=0:00:00, wall=02:43 IST=> training   0.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.371 DataTime=0.273 Loss=2.573 Prec@1=42.360 Prec@5=68.326 rate=4242.43 Hz, eta=0:00:00, total=0:00:00, wall=02:43 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.371 DataTime=0.273 Loss=2.573 Prec@1=42.360 Prec@5=68.326 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:43 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.371 DataTime=0.273 Loss=2.573 Prec@1=42.360 Prec@5=68.326 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:43 IST=> training   4.04% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.355 DataTime=0.259 Loss=2.575 Prec@1=42.369 Prec@5=68.376 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:43 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.355 DataTime=0.259 Loss=2.575 Prec@1=42.369 Prec@5=68.376 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=02:43 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.355 DataTime=0.259 Loss=2.575 Prec@1=42.369 Prec@5=68.376 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=02:44 IST=> training   8.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.347 DataTime=0.247 Loss=2.572 Prec@1=42.345 Prec@5=68.536 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=02:44 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.347 DataTime=0.247 Loss=2.572 Prec@1=42.345 Prec@5=68.536 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=02:44 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.347 DataTime=0.247 Loss=2.572 Prec@1=42.345 Prec@5=68.536 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=02:44 IST=> training   12.03% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.349 DataTime=0.246 Loss=2.569 Prec@1=42.346 Prec@5=68.578 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=02:44 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.349 DataTime=0.246 Loss=2.569 Prec@1=42.346 Prec@5=68.578 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=02:44 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.349 DataTime=0.246 Loss=2.569 Prec@1=42.346 Prec@5=68.578 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=02:45 IST=> training   16.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.345 DataTime=0.242 Loss=2.571 Prec@1=42.369 Prec@5=68.522 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=02:45 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.345 DataTime=0.242 Loss=2.571 Prec@1=42.369 Prec@5=68.522 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=02:45 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.345 DataTime=0.242 Loss=2.571 Prec@1=42.369 Prec@5=68.522 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=02:45 IST=> training   20.02% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.570 Prec@1=42.448 Prec@5=68.523 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=02:45 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.570 Prec@1=42.448 Prec@5=68.523 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=02:45 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.570 Prec@1=42.448 Prec@5=68.523 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=02:46 IST=> training   24.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.572 Prec@1=42.430 Prec@5=68.487 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=02:46 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.572 Prec@1=42.430 Prec@5=68.487 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=02:46 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.343 DataTime=0.239 Loss=2.572 Prec@1=42.430 Prec@5=68.487 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=02:47 IST=> training   28.01% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.455 Prec@5=68.455 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=02:47 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.455 Prec@5=68.455 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=02:47 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.455 Prec@5=68.455 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=02:47 IST=> training   32.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.485 Prec@5=68.481 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=02:47 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.485 Prec@5=68.481 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:47 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.485 Prec@5=68.481 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:48 IST=> training   36.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.480 Prec@5=68.467 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:48 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.480 Prec@5=68.467 rate=2.97 Hz, eta=0:08:24, total=0:05:36, wall=02:48 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.574 Prec@1=42.480 Prec@5=68.467 rate=2.97 Hz, eta=0:08:24, total=0:05:36, wall=02:48 IST=> training   39.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.507 Prec@5=68.487 rate=2.97 Hz, eta=0:08:24, total=0:05:36, wall=02:48 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.507 Prec@5=68.487 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=02:48 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.342 DataTime=0.238 Loss=2.573 Prec@1=42.507 Prec@5=68.487 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=02:49 IST=> training   43.99% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.575 Prec@1=42.501 Prec@5=68.453 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=02:49 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.575 Prec@1=42.501 Prec@5=68.453 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=02:49 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.575 Prec@1=42.501 Prec@5=68.453 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=02:49 IST=> training   47.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.238 Loss=2.575 Prec@1=42.516 Prec@5=68.454 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=02:49 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.238 Loss=2.575 Prec@1=42.516 Prec@5=68.454 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=02:49 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.238 Loss=2.575 Prec@1=42.516 Prec@5=68.454 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=02:50 IST=> training   51.98% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.576 Prec@1=42.512 Prec@5=68.445 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=02:50 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.576 Prec@1=42.512 Prec@5=68.445 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:50 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.576 Prec@1=42.512 Prec@5=68.445 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:51 IST=> training   55.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.483 Prec@5=68.427 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:51 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.483 Prec@5=68.427 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=02:51 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.483 Prec@5=68.427 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=02:51 IST=> training   59.97% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.487 Prec@5=68.422 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=02:51 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.487 Prec@5=68.422 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=02:51 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.487 Prec@5=68.422 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=02:52 IST=> training   63.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.577 Prec@1=42.484 Prec@5=68.425 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=02:52 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.577 Prec@1=42.484 Prec@5=68.425 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:52 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.577 Prec@1=42.484 Prec@5=68.425 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:52 IST=> training   67.96% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.478 Prec@5=68.422 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:52 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.478 Prec@5=68.422 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=02:52 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.577 Prec@1=42.478 Prec@5=68.422 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=02:53 IST=> training   71.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.578 Prec@1=42.473 Prec@5=68.401 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=02:53 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.578 Prec@1=42.473 Prec@5=68.401 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:53 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.578 Prec@1=42.473 Prec@5=68.401 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:53 IST=> training   75.95% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.578 Prec@1=42.479 Prec@5=68.403 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=02:53 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.578 Prec@1=42.479 Prec@5=68.403 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=02:53 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.578 Prec@1=42.479 Prec@5=68.403 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=02:54 IST=> training   79.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.579 Prec@1=42.462 Prec@5=68.391 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=02:54 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.579 Prec@1=42.462 Prec@5=68.391 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:54 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.237 Loss=2.579 Prec@1=42.462 Prec@5=68.391 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:55 IST=> training   83.94% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.579 Prec@1=42.473 Prec@5=68.394 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:55 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.579 Prec@1=42.473 Prec@5=68.394 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:55 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.237 Loss=2.579 Prec@1=42.473 Prec@5=68.394 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:55 IST=> training   87.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.236 Loss=2.579 Prec@1=42.481 Prec@5=68.403 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=02:55 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.236 Loss=2.579 Prec@1=42.481 Prec@5=68.403 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=02:55 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.341 DataTime=0.236 Loss=2.579 Prec@1=42.481 Prec@5=68.403 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=02:56 IST=> training   91.93% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.486 Prec@5=68.399 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=02:56 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.486 Prec@5=68.399 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=02:56 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.486 Prec@5=68.399 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=02:56 IST=> training   95.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.487 Prec@5=68.415 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=02:56 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.487 Prec@5=68.415 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:56 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.236 Loss=2.579 Prec@1=42.487 Prec@5=68.415 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:56 IST=> training   99.92% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.235 Loss=2.579 Prec@1=42.487 Prec@5=68.415 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:56 IST=> training   100.00% of 1x2503...Epoch=15/150 LR=0.09787 Time=0.340 DataTime=0.235 Loss=2.579 Prec@1=42.487 Prec@5=68.415 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=02:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:56 IST=> validation 0.00% of 1x98...Epoch=15/150 LR=0.09787 Time=6.180 Loss=3.168 Prec@1=33.984 Prec@5=59.961 rate=0 Hz, eta=?, total=0:00:00, wall=02:56 IST=> validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.180 Loss=3.168 Prec@1=33.984 Prec@5=59.961 rate=6483.07 Hz, eta=0:00:00, total=0:00:00, wall=02:56 IST** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=6.180 Loss=3.168 Prec@1=33.984 Prec@5=59.961 rate=6483.07 Hz, eta=0:00:00, total=0:00:00, wall=02:57 IST** validation 1.02% of 1x98...Epoch=15/150 LR=0.09787 Time=0.394 Loss=3.200 Prec@1=32.482 Prec@5=58.150 rate=6483.07 Hz, eta=0:00:00, total=0:00:00, wall=02:57 IST** validation 100.00% of 1x98...Epoch=15/150 LR=0.09787 Time=0.394 Loss=3.200 Prec@1=32.482 Prec@5=58.150 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=02:57 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:57 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:57 IST=> training   0.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.831 DataTime=4.661 Loss=2.716 Prec@1=39.648 Prec@5=65.430 rate=0 Hz, eta=?, total=0:00:00, wall=02:57 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.831 DataTime=4.661 Loss=2.716 Prec@1=39.648 Prec@5=65.430 rate=4672.11 Hz, eta=0:00:00, total=0:00:00, wall=02:57 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=4.831 DataTime=4.661 Loss=2.716 Prec@1=39.648 Prec@5=65.430 rate=4672.11 Hz, eta=0:00:00, total=0:00:00, wall=02:58 IST=> training   0.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.375 DataTime=0.278 Loss=2.530 Prec@1=43.342 Prec@5=69.094 rate=4672.11 Hz, eta=0:00:00, total=0:00:00, wall=02:58 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.375 DataTime=0.278 Loss=2.530 Prec@1=43.342 Prec@5=69.094 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=02:58 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.375 DataTime=0.278 Loss=2.530 Prec@1=43.342 Prec@5=69.094 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=02:58 IST=> training   4.04% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.355 DataTime=0.257 Loss=2.527 Prec@1=43.462 Prec@5=69.126 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=02:58 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.355 DataTime=0.257 Loss=2.527 Prec@1=43.462 Prec@5=69.126 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=02:58 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.355 DataTime=0.257 Loss=2.527 Prec@1=43.462 Prec@5=69.126 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=02:59 IST=> training   8.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.346 DataTime=0.245 Loss=2.530 Prec@1=43.419 Prec@5=69.123 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=02:59 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.346 DataTime=0.245 Loss=2.530 Prec@1=43.419 Prec@5=69.123 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=02:59 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.346 DataTime=0.245 Loss=2.530 Prec@1=43.419 Prec@5=69.123 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=02:59 IST=> training   12.03% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.347 DataTime=0.245 Loss=2.534 Prec@1=43.300 Prec@5=69.153 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=02:59 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.347 DataTime=0.245 Loss=2.534 Prec@1=43.300 Prec@5=69.153 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=02:59 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.347 DataTime=0.245 Loss=2.534 Prec@1=43.300 Prec@5=69.153 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=03:00 IST=> training   16.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.344 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.095 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=03:00 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.344 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.095 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:00 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.344 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.095 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:00 IST=> training   20.02% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.536 Prec@1=43.250 Prec@5=69.144 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:00 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.536 Prec@1=43.250 Prec@5=69.144 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:00 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.536 Prec@1=43.250 Prec@5=69.144 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:01 IST=> training   24.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.343 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.135 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:01 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.343 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.135 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=03:01 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.343 DataTime=0.241 Loss=2.538 Prec@1=43.189 Prec@5=69.135 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=03:01 IST=> training   28.01% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.175 Prec@5=69.097 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=03:01 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.175 Prec@5=69.097 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=03:01 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.175 Prec@5=69.097 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=03:02 IST=> training   32.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.239 Loss=2.540 Prec@1=43.163 Prec@5=69.078 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=03:02 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.239 Loss=2.540 Prec@1=43.163 Prec@5=69.078 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:02 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.239 Loss=2.540 Prec@1=43.163 Prec@5=69.078 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:03 IST=> training   36.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.167 Prec@5=69.091 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:03 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.167 Prec@5=69.091 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=03:03 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.240 Loss=2.539 Prec@1=43.167 Prec@5=69.091 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=03:03 IST=> training   39.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.161 Prec@5=69.080 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=03:03 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.161 Prec@5=69.080 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:03 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.161 Prec@5=69.080 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:04 IST=> training   43.99% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.149 Prec@5=69.102 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:04 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.149 Prec@5=69.102 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:04 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.238 Loss=2.539 Prec@1=43.149 Prec@5=69.102 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:04 IST=> training   47.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.540 Prec@1=43.139 Prec@5=69.086 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:04 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.540 Prec@1=43.139 Prec@5=69.086 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=03:04 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.239 Loss=2.540 Prec@1=43.139 Prec@5=69.086 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=03:05 IST=> training   51.98% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.237 Loss=2.541 Prec@1=43.127 Prec@5=69.087 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=03:05 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.237 Loss=2.541 Prec@1=43.127 Prec@5=69.087 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:05 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.237 Loss=2.541 Prec@1=43.127 Prec@5=69.087 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:05 IST=> training   55.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.148 Prec@5=69.077 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:05 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.148 Prec@5=69.077 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=03:05 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.148 Prec@5=69.077 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=03:06 IST=> training   59.97% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.144 Prec@5=69.079 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=03:06 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.144 Prec@5=69.079 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=03:06 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.342 DataTime=0.236 Loss=2.541 Prec@1=43.144 Prec@5=69.079 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=03:07 IST=> training   63.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.136 Prec@5=69.080 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=03:07 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.136 Prec@5=69.080 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=03:07 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.136 Prec@5=69.080 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=03:07 IST=> training   67.96% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.141 Prec@5=69.072 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=03:07 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.141 Prec@5=69.072 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:07 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.141 Prec@5=69.072 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:08 IST=> training   71.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.148 Prec@5=69.083 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:08 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.148 Prec@5=69.083 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:08 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.541 Prec@1=43.148 Prec@5=69.083 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:08 IST=> training   75.95% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.124 Prec@5=69.066 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:08 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.124 Prec@5=69.066 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=03:08 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.124 Prec@5=69.066 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=03:09 IST=> training   79.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.234 Loss=2.542 Prec@1=43.144 Prec@5=69.072 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=03:09 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.234 Loss=2.542 Prec@1=43.144 Prec@5=69.072 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=03:09 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.234 Loss=2.542 Prec@1=43.144 Prec@5=69.072 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=03:09 IST=> training   83.94% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.542 Prec@1=43.134 Prec@5=69.071 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=03:09 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.542 Prec@1=43.134 Prec@5=69.071 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=03:09 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.542 Prec@1=43.134 Prec@5=69.071 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=03:10 IST=> training   87.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.133 Prec@5=69.052 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.133 Prec@5=69.052 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.341 DataTime=0.235 Loss=2.543 Prec@1=43.133 Prec@5=69.052 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:10 IST=> training   91.93% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.139 Prec@5=69.055 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:10 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.139 Prec@5=69.055 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:10 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.139 Prec@5=69.055 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:11 IST=> training   95.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.137 Prec@5=69.053 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.137 Prec@5=69.053 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.235 Loss=2.542 Prec@1=43.137 Prec@5=69.053 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=03:11 IST=> training   99.92% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.234 Loss=2.542 Prec@1=43.136 Prec@5=69.053 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=03:11 IST=> training   100.00% of 1x2503...Epoch=16/150 LR=0.09755 Time=0.340 DataTime=0.234 Loss=2.542 Prec@1=43.136 Prec@5=69.053 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=03:11 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 0.00% of 1x98...Epoch=16/150 LR=0.09755 Time=6.017 Loss=2.865 Prec@1=37.500 Prec@5=63.867 rate=0 Hz, eta=?, total=0:00:00, wall=03:11 IST=> validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.017 Loss=2.865 Prec@1=37.500 Prec@5=63.867 rate=5561.21 Hz, eta=0:00:00, total=0:00:00, wall=03:11 IST** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=6.017 Loss=2.865 Prec@1=37.500 Prec@5=63.867 rate=5561.21 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST** validation 1.02% of 1x98...Epoch=16/150 LR=0.09755 Time=0.406 Loss=3.004 Prec@1=34.698 Prec@5=61.092 rate=5561.21 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST** validation 100.00% of 1x98...Epoch=16/150 LR=0.09755 Time=0.406 Loss=3.004 Prec@1=34.698 Prec@5=61.092 rate=2.91 Hz, eta=0:00:00, total=0:00:33, wall=03:12 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> training   0.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=4.613 DataTime=4.463 Loss=2.456 Prec@1=43.359 Prec@5=70.703 rate=0 Hz, eta=?, total=0:00:00, wall=03:12 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=4.613 DataTime=4.463 Loss=2.456 Prec@1=43.359 Prec@5=70.703 rate=4327.86 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=4.613 DataTime=4.463 Loss=2.456 Prec@1=43.359 Prec@5=70.703 rate=4327.86 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST=> training   0.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.380 DataTime=0.281 Loss=2.504 Prec@1=43.640 Prec@5=69.566 rate=4327.86 Hz, eta=0:00:00, total=0:00:00, wall=03:12 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.380 DataTime=0.281 Loss=2.504 Prec@1=43.640 Prec@5=69.566 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=03:12 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.380 DataTime=0.281 Loss=2.504 Prec@1=43.640 Prec@5=69.566 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=03:13 IST=> training   4.04% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.354 DataTime=0.255 Loss=2.512 Prec@1=43.593 Prec@5=69.428 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=03:13 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.354 DataTime=0.255 Loss=2.512 Prec@1=43.593 Prec@5=69.428 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=03:13 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.354 DataTime=0.255 Loss=2.512 Prec@1=43.593 Prec@5=69.428 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=03:13 IST=> training   8.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.243 Loss=2.503 Prec@1=43.776 Prec@5=69.605 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=03:13 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.243 Loss=2.503 Prec@1=43.776 Prec@5=69.605 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=03:13 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.243 Loss=2.503 Prec@1=43.776 Prec@5=69.605 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=03:14 IST=> training   12.03% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.242 Loss=2.508 Prec@1=43.739 Prec@5=69.512 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=03:14 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.242 Loss=2.508 Prec@1=43.739 Prec@5=69.512 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=03:14 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.242 Loss=2.508 Prec@1=43.739 Prec@5=69.512 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=03:15 IST=> training   16.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.240 Loss=2.508 Prec@1=43.688 Prec@5=69.487 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=03:15 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.240 Loss=2.508 Prec@1=43.688 Prec@5=69.487 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=03:15 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.345 DataTime=0.240 Loss=2.508 Prec@1=43.688 Prec@5=69.487 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=03:15 IST=> training   20.02% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.507 Prec@1=43.748 Prec@5=69.517 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=03:15 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.507 Prec@1=43.748 Prec@5=69.517 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:15 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.507 Prec@1=43.748 Prec@5=69.517 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:16 IST=> training   24.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.506 Prec@1=43.781 Prec@5=69.552 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:16 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.506 Prec@1=43.781 Prec@5=69.552 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=03:16 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.342 DataTime=0.236 Loss=2.506 Prec@1=43.781 Prec@5=69.552 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=03:16 IST=> training   28.01% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.236 Loss=2.507 Prec@1=43.777 Prec@5=69.544 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=03:16 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.236 Loss=2.507 Prec@1=43.777 Prec@5=69.544 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=03:16 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.236 Loss=2.507 Prec@1=43.777 Prec@5=69.544 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=03:17 IST=> training   32.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.235 Loss=2.507 Prec@1=43.772 Prec@5=69.547 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=03:17 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.235 Loss=2.507 Prec@1=43.772 Prec@5=69.547 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:17 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.235 Loss=2.507 Prec@1=43.772 Prec@5=69.547 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:17 IST=> training   36.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.508 Prec@1=43.753 Prec@5=69.549 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:17 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.508 Prec@1=43.753 Prec@5=69.549 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:17 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.508 Prec@1=43.753 Prec@5=69.549 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:18 IST=> training   39.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.509 Prec@1=43.736 Prec@5=69.528 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:18 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.509 Prec@1=43.736 Prec@5=69.528 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=03:18 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.341 DataTime=0.235 Loss=2.509 Prec@1=43.736 Prec@5=69.528 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=03:19 IST=> training   43.99% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.724 Prec@5=69.526 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=03:19 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.724 Prec@5=69.526 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=03:19 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.724 Prec@5=69.526 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=03:19 IST=> training   47.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.712 Prec@5=69.537 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=03:19 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.712 Prec@5=69.537 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:19 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.234 Loss=2.510 Prec@1=43.712 Prec@5=69.537 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:20 IST=> training   51.98% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.233 Loss=2.509 Prec@1=43.739 Prec@5=69.545 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:20 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.233 Loss=2.509 Prec@1=43.739 Prec@5=69.545 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=03:20 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.340 DataTime=0.233 Loss=2.509 Prec@1=43.739 Prec@5=69.545 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=03:20 IST=> training   55.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.730 Prec@5=69.539 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=03:20 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.730 Prec@5=69.539 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:20 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.730 Prec@5=69.539 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:21 IST=> training   59.97% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.727 Prec@5=69.542 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:21 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.727 Prec@5=69.542 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:21 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.232 Loss=2.510 Prec@1=43.727 Prec@5=69.542 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:21 IST=> training   63.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.511 Prec@1=43.730 Prec@5=69.531 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:21 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.511 Prec@1=43.730 Prec@5=69.531 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=03:21 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.511 Prec@1=43.730 Prec@5=69.531 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=03:22 IST=> training   67.96% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.510 Prec@1=43.721 Prec@5=69.530 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=03:22 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.510 Prec@1=43.721 Prec@5=69.530 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=03:22 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.510 Prec@1=43.721 Prec@5=69.530 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=03:22 IST=> training   71.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.708 Prec@5=69.520 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=03:22 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.708 Prec@5=69.520 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=03:22 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.708 Prec@5=69.520 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=03:23 IST=> training   75.95% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.698 Prec@5=69.516 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=03:23 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.698 Prec@5=69.516 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=03:23 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.511 Prec@1=43.698 Prec@5=69.516 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=03:24 IST=> training   79.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.708 Prec@5=69.514 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=03:24 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.708 Prec@5=69.514 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:24 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.708 Prec@5=69.514 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:24 IST=> training   83.94% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.710 Prec@5=69.514 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:24 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.710 Prec@5=69.514 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=03:24 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.339 DataTime=0.231 Loss=2.512 Prec@1=43.710 Prec@5=69.514 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=03:25 IST=> training   87.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.709 Prec@5=69.520 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=03:25 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.709 Prec@5=69.520 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=03:25 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.709 Prec@5=69.520 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=03:25 IST=> training   91.93% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.701 Prec@5=69.514 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=03:25 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.701 Prec@5=69.514 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:25 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.701 Prec@5=69.514 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:26 IST=> training   95.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.703 Prec@5=69.523 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:26 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.703 Prec@5=69.523 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=03:26 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.231 Loss=2.512 Prec@1=43.703 Prec@5=69.523 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=03:26 IST=> training   99.92% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.230 Loss=2.512 Prec@1=43.702 Prec@5=69.523 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=03:26 IST=> training   100.00% of 1x2503...Epoch=17/150 LR=0.09722 Time=0.338 DataTime=0.230 Loss=2.512 Prec@1=43.702 Prec@5=69.523 rate=2.98 Hz, eta=0:00:00, total=0:14:01, wall=03:26 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST=> validation 0.00% of 1x98...Epoch=17/150 LR=0.09722 Time=7.095 Loss=3.018 Prec@1=34.570 Prec@5=60.938 rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST=> validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=7.095 Loss=3.018 Prec@1=34.570 Prec@5=60.938 rate=6779.61 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=7.095 Loss=3.018 Prec@1=34.570 Prec@5=60.938 rate=6779.61 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST** validation 1.02% of 1x98...Epoch=17/150 LR=0.09722 Time=0.402 Loss=3.035 Prec@1=34.558 Prec@5=60.524 rate=6779.61 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST** validation 100.00% of 1x98...Epoch=17/150 LR=0.09722 Time=0.402 Loss=3.035 Prec@1=34.558 Prec@5=60.524 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=03:26 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST=> training   0.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.695 DataTime=5.562 Loss=2.451 Prec@1=44.531 Prec@5=70.312 rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.695 DataTime=5.562 Loss=2.451 Prec@1=44.531 Prec@5=70.312 rate=5809.63 Hz, eta=0:00:00, total=0:00:00, wall=03:27 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=5.695 DataTime=5.562 Loss=2.451 Prec@1=44.531 Prec@5=70.312 rate=5809.63 Hz, eta=0:00:00, total=0:00:00, wall=03:27 IST=> training   0.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.380 DataTime=0.281 Loss=2.476 Prec@1=44.340 Prec@5=69.988 rate=5809.63 Hz, eta=0:00:00, total=0:00:00, wall=03:27 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.380 DataTime=0.281 Loss=2.476 Prec@1=44.340 Prec@5=69.988 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:27 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.380 DataTime=0.281 Loss=2.476 Prec@1=44.340 Prec@5=69.988 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:28 IST=> training   4.04% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.351 DataTime=0.250 Loss=2.477 Prec@1=44.388 Prec@5=69.939 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:28 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.351 DataTime=0.250 Loss=2.477 Prec@1=44.388 Prec@5=69.939 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=03:28 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.351 DataTime=0.250 Loss=2.477 Prec@1=44.388 Prec@5=69.939 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=03:28 IST=> training   8.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.347 DataTime=0.245 Loss=2.475 Prec@1=44.346 Prec@5=70.037 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=03:28 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.347 DataTime=0.245 Loss=2.475 Prec@1=44.346 Prec@5=70.037 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=03:28 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.347 DataTime=0.245 Loss=2.475 Prec@1=44.346 Prec@5=70.037 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=03:29 IST=> training   12.03% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.344 DataTime=0.242 Loss=2.477 Prec@1=44.340 Prec@5=70.037 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=03:29 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.344 DataTime=0.242 Loss=2.477 Prec@1=44.340 Prec@5=70.037 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=03:29 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.344 DataTime=0.242 Loss=2.477 Prec@1=44.340 Prec@5=70.037 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=03:29 IST=> training   16.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.340 DataTime=0.239 Loss=2.478 Prec@1=44.329 Prec@5=70.014 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=03:29 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.340 DataTime=0.239 Loss=2.478 Prec@1=44.329 Prec@5=70.014 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=03:29 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.340 DataTime=0.239 Loss=2.478 Prec@1=44.329 Prec@5=70.014 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=03:30 IST=> training   20.02% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.238 Loss=2.480 Prec@1=44.309 Prec@5=70.012 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=03:30 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.238 Loss=2.480 Prec@1=44.309 Prec@5=70.012 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=03:30 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.238 Loss=2.480 Prec@1=44.309 Prec@5=70.012 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=03:30 IST=> training   24.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.237 Loss=2.481 Prec@1=44.270 Prec@5=69.986 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=03:30 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.237 Loss=2.481 Prec@1=44.270 Prec@5=69.986 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=03:30 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.339 DataTime=0.237 Loss=2.481 Prec@1=44.270 Prec@5=69.986 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=03:31 IST=> training   28.01% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.236 Loss=2.477 Prec@1=44.321 Prec@5=70.060 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=03:31 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.236 Loss=2.477 Prec@1=44.321 Prec@5=70.060 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=03:31 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.236 Loss=2.477 Prec@1=44.321 Prec@5=70.060 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=03:32 IST=> training   32.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.237 Loss=2.478 Prec@1=44.331 Prec@5=70.084 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=03:32 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.237 Loss=2.478 Prec@1=44.331 Prec@5=70.084 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=03:32 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.237 Loss=2.478 Prec@1=44.331 Prec@5=70.084 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=03:32 IST=> training   36.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.479 Prec@1=44.338 Prec@5=70.091 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=03:32 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.479 Prec@1=44.338 Prec@5=70.091 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=03:32 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.479 Prec@1=44.338 Prec@5=70.091 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=03:33 IST=> training   39.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.234 Loss=2.480 Prec@1=44.311 Prec@5=70.059 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=03:33 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.234 Loss=2.480 Prec@1=44.311 Prec@5=70.059 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:33 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.234 Loss=2.480 Prec@1=44.311 Prec@5=70.059 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:33 IST=> training   43.99% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.483 Prec@1=44.252 Prec@5=70.009 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:33 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.483 Prec@1=44.252 Prec@5=70.009 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=03:33 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.235 Loss=2.483 Prec@1=44.252 Prec@5=70.009 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=03:34 IST=> training   47.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.020 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=03:34 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.020 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:34 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.020 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:34 IST=> training   51.98% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.251 Prec@5=70.005 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:34 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.251 Prec@5=70.005 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=03:34 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.251 Prec@5=70.005 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=03:35 IST=> training   55.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.007 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=03:35 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.007 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=03:35 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.234 Loss=2.482 Prec@1=44.256 Prec@5=70.007 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=03:36 IST=> training   59.97% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.263 Prec@5=70.010 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=03:36 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.263 Prec@5=70.010 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=03:36 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.233 Loss=2.483 Prec@1=44.263 Prec@5=70.010 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=03:36 IST=> training   63.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.483 Prec@1=44.238 Prec@5=69.989 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=03:36 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.483 Prec@1=44.238 Prec@5=69.989 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=03:36 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.483 Prec@1=44.238 Prec@5=69.989 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=03:37 IST=> training   67.96% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.233 Loss=2.483 Prec@1=44.253 Prec@5=70.000 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=03:37 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.233 Loss=2.483 Prec@1=44.253 Prec@5=70.000 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=03:37 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.338 DataTime=0.233 Loss=2.483 Prec@1=44.253 Prec@5=70.000 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=03:37 IST=> training   71.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.239 Prec@5=70.002 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=03:37 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.239 Prec@5=70.002 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=03:37 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.239 Prec@5=70.002 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=03:38 IST=> training   75.95% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.226 Prec@5=69.995 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=03:38 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.226 Prec@5=69.995 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=03:38 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.484 Prec@1=44.226 Prec@5=69.995 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=03:38 IST=> training   79.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.485 Prec@1=44.218 Prec@5=69.980 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=03:38 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.485 Prec@1=44.218 Prec@5=69.980 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=03:38 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.232 Loss=2.485 Prec@1=44.218 Prec@5=69.980 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=03:39 IST=> training   83.94% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.203 Prec@5=69.962 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=03:39 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.203 Prec@5=69.962 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=03:39 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.203 Prec@5=69.962 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=03:39 IST=> training   87.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.216 Prec@5=69.972 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=03:39 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.216 Prec@5=69.972 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=03:39 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.486 Prec@1=44.216 Prec@5=69.972 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=03:40 IST=> training   91.93% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.485 Prec@1=44.229 Prec@5=69.978 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=03:40 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.485 Prec@1=44.229 Prec@5=69.978 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=03:40 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.337 DataTime=0.231 Loss=2.485 Prec@1=44.229 Prec@5=69.978 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=03:41 IST=> training   95.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.336 DataTime=0.230 Loss=2.485 Prec@1=44.228 Prec@5=69.977 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=03:41 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.336 DataTime=0.230 Loss=2.485 Prec@1=44.228 Prec@5=69.977 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=03:41 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.336 DataTime=0.230 Loss=2.485 Prec@1=44.228 Prec@5=69.977 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=03:41 IST=> training   99.92% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.336 DataTime=0.230 Loss=2.485 Prec@1=44.228 Prec@5=69.976 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=03:41 IST=> training   100.00% of 1x2503...Epoch=18/150 LR=0.09686 Time=0.336 DataTime=0.230 Loss=2.485 Prec@1=44.228 Prec@5=69.976 rate=3.00 Hz, eta=0:00:00, total=0:13:55, wall=03:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> validation 0.00% of 1x98...Epoch=18/150 LR=0.09686 Time=6.043 Loss=2.626 Prec@1=43.555 Prec@5=69.727 rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=6.043 Loss=2.626 Prec@1=43.555 Prec@5=69.727 rate=4371.16 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=6.043 Loss=2.626 Prec@1=43.555 Prec@5=69.727 rate=4371.16 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST** validation 1.02% of 1x98...Epoch=18/150 LR=0.09686 Time=0.398 Loss=2.624 Prec@1=41.310 Prec@5=67.440 rate=4371.16 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST** validation 100.00% of 1x98...Epoch=18/150 LR=0.09686 Time=0.398 Loss=2.624 Prec@1=41.310 Prec@5=67.440 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=03:41 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> training   0.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.294 DataTime=5.119 Loss=2.510 Prec@1=45.117 Prec@5=67.578 rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.294 DataTime=5.119 Loss=2.510 Prec@1=45.117 Prec@5=67.578 rate=2418.33 Hz, eta=0:00:01, total=0:00:00, wall=03:41 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=5.294 DataTime=5.119 Loss=2.510 Prec@1=45.117 Prec@5=67.578 rate=2418.33 Hz, eta=0:00:01, total=0:00:00, wall=03:42 IST=> training   0.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.392 DataTime=0.294 Loss=2.442 Prec@1=45.111 Prec@5=70.777 rate=2418.33 Hz, eta=0:00:01, total=0:00:00, wall=03:42 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.392 DataTime=0.294 Loss=2.442 Prec@1=45.111 Prec@5=70.777 rate=2.94 Hz, eta=0:13:36, total=0:00:34, wall=03:42 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.392 DataTime=0.294 Loss=2.442 Prec@1=45.111 Prec@5=70.777 rate=2.94 Hz, eta=0:13:36, total=0:00:34, wall=03:42 IST=> training   4.04% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.368 DataTime=0.265 Loss=2.428 Prec@1=45.182 Prec@5=70.955 rate=2.94 Hz, eta=0:13:36, total=0:00:34, wall=03:42 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.368 DataTime=0.265 Loss=2.428 Prec@1=45.182 Prec@5=70.955 rate=2.93 Hz, eta=0:13:05, total=0:01:08, wall=03:42 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.368 DataTime=0.265 Loss=2.428 Prec@1=45.182 Prec@5=70.955 rate=2.93 Hz, eta=0:13:05, total=0:01:08, wall=03:43 IST=> training   8.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.356 DataTime=0.253 Loss=2.437 Prec@1=45.090 Prec@5=70.789 rate=2.93 Hz, eta=0:13:05, total=0:01:08, wall=03:43 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.356 DataTime=0.253 Loss=2.437 Prec@1=45.090 Prec@5=70.789 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=03:43 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.356 DataTime=0.253 Loss=2.437 Prec@1=45.090 Prec@5=70.789 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=03:44 IST=> training   12.03% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.351 DataTime=0.248 Loss=2.444 Prec@1=45.034 Prec@5=70.694 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=03:44 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.351 DataTime=0.248 Loss=2.444 Prec@1=45.034 Prec@5=70.694 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=03:44 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.351 DataTime=0.248 Loss=2.444 Prec@1=45.034 Prec@5=70.694 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=03:44 IST=> training   16.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.345 DataTime=0.243 Loss=2.446 Prec@1=44.950 Prec@5=70.668 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=03:44 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.345 DataTime=0.243 Loss=2.446 Prec@1=44.950 Prec@5=70.668 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:44 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.345 DataTime=0.243 Loss=2.446 Prec@1=44.950 Prec@5=70.668 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:45 IST=> training   20.02% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.344 DataTime=0.241 Loss=2.449 Prec@1=44.938 Prec@5=70.620 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.344 DataTime=0.241 Loss=2.449 Prec@1=44.938 Prec@5=70.620 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.344 DataTime=0.241 Loss=2.449 Prec@1=44.938 Prec@5=70.620 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=03:45 IST=> training   24.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.343 DataTime=0.240 Loss=2.451 Prec@1=44.859 Prec@5=70.593 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=03:45 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.343 DataTime=0.240 Loss=2.451 Prec@1=44.859 Prec@5=70.593 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=03:45 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.343 DataTime=0.240 Loss=2.451 Prec@1=44.859 Prec@5=70.593 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=03:46 IST=> training   28.01% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.239 Loss=2.452 Prec@1=44.825 Prec@5=70.585 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=03:46 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.239 Loss=2.452 Prec@1=44.825 Prec@5=70.585 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=03:46 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.239 Loss=2.452 Prec@1=44.825 Prec@5=70.585 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=03:46 IST=> training   32.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.784 Prec@5=70.521 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=03:46 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.784 Prec@5=70.521 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=03:46 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.784 Prec@5=70.521 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=03:47 IST=> training   36.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.781 Prec@5=70.513 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=03:47 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.781 Prec@5=70.513 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:47 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.455 Prec@1=44.781 Prec@5=70.513 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:47 IST=> training   39.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.456 Prec@1=44.790 Prec@5=70.494 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:47 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.456 Prec@1=44.790 Prec@5=70.494 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:47 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.456 Prec@1=44.790 Prec@5=70.494 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:48 IST=> training   43.99% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.763 Prec@5=70.480 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:48 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.763 Prec@5=70.480 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:48 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.763 Prec@5=70.480 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:49 IST=> training   47.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.457 Prec@1=44.780 Prec@5=70.487 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:49 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.457 Prec@1=44.780 Prec@5=70.487 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=03:49 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.342 DataTime=0.239 Loss=2.457 Prec@1=44.780 Prec@5=70.487 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=03:49 IST=> training   51.98% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.792 Prec@5=70.484 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=03:49 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.792 Prec@5=70.484 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=03:49 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.457 Prec@1=44.792 Prec@5=70.484 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=03:50 IST=> training   55.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.238 Loss=2.458 Prec@1=44.769 Prec@5=70.453 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=03:50 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.238 Loss=2.458 Prec@1=44.769 Prec@5=70.453 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:50 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.238 Loss=2.458 Prec@1=44.769 Prec@5=70.453 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:50 IST=> training   59.97% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.758 Prec@5=70.442 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:50 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.758 Prec@5=70.442 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:50 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.758 Prec@5=70.442 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:51 IST=> training   63.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.753 Prec@5=70.464 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:51 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.753 Prec@5=70.464 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=03:51 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.238 Loss=2.459 Prec@1=44.753 Prec@5=70.464 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=03:51 IST=> training   67.96% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.459 Prec@1=44.735 Prec@5=70.450 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=03:51 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.459 Prec@1=44.735 Prec@5=70.450 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=03:51 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.459 Prec@1=44.735 Prec@5=70.450 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=03:52 IST=> training   71.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.237 Loss=2.459 Prec@1=44.720 Prec@5=70.448 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=03:52 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.237 Loss=2.459 Prec@1=44.720 Prec@5=70.448 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:52 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.341 DataTime=0.237 Loss=2.459 Prec@1=44.720 Prec@5=70.448 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:53 IST=> training   75.95% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.236 Loss=2.459 Prec@1=44.722 Prec@5=70.449 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:53 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.236 Loss=2.459 Prec@1=44.722 Prec@5=70.449 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=03:53 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.236 Loss=2.459 Prec@1=44.722 Prec@5=70.449 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=03:53 IST=> training   79.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.460 Prec@1=44.718 Prec@5=70.425 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=03:53 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.460 Prec@1=44.718 Prec@5=70.425 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:53 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.460 Prec@1=44.718 Prec@5=70.425 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:54 IST=> training   83.94% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.460 Prec@1=44.702 Prec@5=70.415 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:54 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.460 Prec@1=44.702 Prec@5=70.415 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=03:54 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.340 DataTime=0.237 Loss=2.460 Prec@1=44.702 Prec@5=70.415 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=03:54 IST=> training   87.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.407 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=03:54 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.407 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=03:54 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.407 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=03:55 IST=> training   91.93% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.405 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=03:55 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.405 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:55 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.236 Loss=2.461 Prec@1=44.698 Prec@5=70.405 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:55 IST=> training   95.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.235 Loss=2.461 Prec@1=44.701 Prec@5=70.403 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=03:55 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.235 Loss=2.461 Prec@1=44.701 Prec@5=70.403 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=03:55 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.235 Loss=2.461 Prec@1=44.701 Prec@5=70.403 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=03:55 IST=> training   99.92% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.235 Loss=2.461 Prec@1=44.699 Prec@5=70.402 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=03:55 IST=> training   100.00% of 1x2503...Epoch=19/150 LR=0.09649 Time=0.339 DataTime=0.235 Loss=2.461 Prec@1=44.699 Prec@5=70.402 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=03:55 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST=> validation 0.00% of 1x98...Epoch=19/150 LR=0.09649 Time=6.959 Loss=3.050 Prec@1=32.812 Prec@5=59.375 rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST=> validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.959 Loss=3.050 Prec@1=32.812 Prec@5=59.375 rate=6471.86 Hz, eta=0:00:00, total=0:00:00, wall=03:55 IST** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=6.959 Loss=3.050 Prec@1=32.812 Prec@5=59.375 rate=6471.86 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST** validation 1.02% of 1x98...Epoch=19/150 LR=0.09649 Time=0.397 Loss=3.177 Prec@1=32.620 Prec@5=58.116 rate=6471.86 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST** validation 100.00% of 1x98...Epoch=19/150 LR=0.09649 Time=0.397 Loss=3.177 Prec@1=32.620 Prec@5=58.116 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=03:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST=> training   0.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.211 DataTime=5.002 Loss=2.428 Prec@1=42.578 Prec@5=71.484 rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.211 DataTime=5.002 Loss=2.428 Prec@1=42.578 Prec@5=71.484 rate=5776.04 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=5.211 DataTime=5.002 Loss=2.428 Prec@1=42.578 Prec@5=71.484 rate=5776.04 Hz, eta=0:00:00, total=0:00:00, wall=03:57 IST=> training   0.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.375 DataTime=0.278 Loss=2.420 Prec@1=45.326 Prec@5=71.094 rate=5776.04 Hz, eta=0:00:00, total=0:00:00, wall=03:57 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.375 DataTime=0.278 Loss=2.420 Prec@1=45.326 Prec@5=71.094 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:57 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.375 DataTime=0.278 Loss=2.420 Prec@1=45.326 Prec@5=71.094 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:57 IST=> training   4.04% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.347 DataTime=0.252 Loss=2.416 Prec@1=45.328 Prec@5=71.056 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=03:57 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.347 DataTime=0.252 Loss=2.416 Prec@1=45.328 Prec@5=71.056 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=03:57 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.347 DataTime=0.252 Loss=2.416 Prec@1=45.328 Prec@5=71.056 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=03:58 IST=> training   8.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.246 Loss=2.422 Prec@1=45.250 Prec@5=70.947 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=03:58 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.246 Loss=2.422 Prec@1=45.250 Prec@5=70.947 rate=3.08 Hz, eta=0:11:56, total=0:01:37, wall=03:58 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.246 Loss=2.422 Prec@1=45.250 Prec@5=70.947 rate=3.08 Hz, eta=0:11:56, total=0:01:37, wall=03:58 IST=> training   12.03% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.244 Loss=2.429 Prec@1=45.133 Prec@5=70.843 rate=3.08 Hz, eta=0:11:56, total=0:01:37, wall=03:58 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.244 Loss=2.429 Prec@1=45.133 Prec@5=70.843 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=03:58 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.342 DataTime=0.244 Loss=2.429 Prec@1=45.133 Prec@5=70.843 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=03:59 IST=> training   16.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.338 DataTime=0.240 Loss=2.433 Prec@1=45.168 Prec@5=70.817 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=03:59 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.338 DataTime=0.240 Loss=2.433 Prec@1=45.168 Prec@5=70.817 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=03:59 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.338 DataTime=0.240 Loss=2.433 Prec@1=45.168 Prec@5=70.817 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=03:59 IST=> training   20.02% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.431 Prec@1=45.211 Prec@5=70.842 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=03:59 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.431 Prec@1=45.211 Prec@5=70.842 rate=3.05 Hz, eta=0:10:23, total=0:03:16, wall=03:59 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.431 Prec@1=45.211 Prec@5=70.842 rate=3.05 Hz, eta=0:10:23, total=0:03:16, wall=04:00 IST=> training   24.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.337 DataTime=0.238 Loss=2.430 Prec@1=45.231 Prec@5=70.872 rate=3.05 Hz, eta=0:10:23, total=0:03:16, wall=04:00 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.337 DataTime=0.238 Loss=2.430 Prec@1=45.231 Prec@5=70.872 rate=3.03 Hz, eta=0:09:53, total=0:03:50, wall=04:00 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.337 DataTime=0.238 Loss=2.430 Prec@1=45.231 Prec@5=70.872 rate=3.03 Hz, eta=0:09:53, total=0:03:50, wall=04:01 IST=> training   28.01% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.430 Prec@1=45.226 Prec@5=70.879 rate=3.03 Hz, eta=0:09:53, total=0:03:50, wall=04:01 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.430 Prec@1=45.226 Prec@5=70.879 rate=3.04 Hz, eta=0:09:20, total=0:04:23, wall=04:01 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.237 Loss=2.430 Prec@1=45.226 Prec@5=70.879 rate=3.04 Hz, eta=0:09:20, total=0:04:23, wall=04:01 IST=> training   32.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.236 Loss=2.431 Prec@1=45.204 Prec@5=70.855 rate=3.04 Hz, eta=0:09:20, total=0:04:23, wall=04:01 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.236 Loss=2.431 Prec@1=45.204 Prec@5=70.855 rate=3.04 Hz, eta=0:08:47, total=0:04:56, wall=04:01 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.236 Loss=2.431 Prec@1=45.204 Prec@5=70.855 rate=3.04 Hz, eta=0:08:47, total=0:04:56, wall=04:02 IST=> training   36.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.236 Loss=2.431 Prec@1=45.217 Prec@5=70.875 rate=3.04 Hz, eta=0:08:47, total=0:04:56, wall=04:02 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.236 Loss=2.431 Prec@1=45.217 Prec@5=70.875 rate=3.03 Hz, eta=0:08:16, total=0:05:30, wall=04:02 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.336 DataTime=0.236 Loss=2.431 Prec@1=45.217 Prec@5=70.875 rate=3.03 Hz, eta=0:08:16, total=0:05:30, wall=04:02 IST=> training   39.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.432 Prec@1=45.203 Prec@5=70.861 rate=3.03 Hz, eta=0:08:16, total=0:05:30, wall=04:02 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.432 Prec@1=45.203 Prec@5=70.861 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=04:02 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.432 Prec@1=45.203 Prec@5=70.861 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=04:03 IST=> training   43.99% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.433 Prec@1=45.188 Prec@5=70.841 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=04:03 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.433 Prec@1=45.188 Prec@5=70.841 rate=3.03 Hz, eta=0:07:09, total=0:06:36, wall=04:03 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.433 Prec@1=45.188 Prec@5=70.841 rate=3.03 Hz, eta=0:07:09, total=0:06:36, wall=04:03 IST=> training   47.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.433 Prec@1=45.190 Prec@5=70.859 rate=3.03 Hz, eta=0:07:09, total=0:06:36, wall=04:03 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.433 Prec@1=45.190 Prec@5=70.859 rate=3.02 Hz, eta=0:06:38, total=0:07:10, wall=04:03 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.433 Prec@1=45.190 Prec@5=70.859 rate=3.02 Hz, eta=0:06:38, total=0:07:10, wall=04:04 IST=> training   51.98% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.434 Prec@1=45.177 Prec@5=70.828 rate=3.02 Hz, eta=0:06:38, total=0:07:10, wall=04:04 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.434 Prec@1=45.177 Prec@5=70.828 rate=3.02 Hz, eta=0:06:04, total=0:07:43, wall=04:04 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.434 Prec@1=45.177 Prec@5=70.828 rate=3.02 Hz, eta=0:06:04, total=0:07:43, wall=04:04 IST=> training   55.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.434 Prec@1=45.173 Prec@5=70.838 rate=3.02 Hz, eta=0:06:04, total=0:07:43, wall=04:04 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.434 Prec@1=45.173 Prec@5=70.838 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=04:04 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.434 Prec@1=45.173 Prec@5=70.838 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=04:05 IST=> training   59.97% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.435 Prec@1=45.151 Prec@5=70.825 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=04:05 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.435 Prec@1=45.151 Prec@5=70.825 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=04:05 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.235 Loss=2.435 Prec@1=45.151 Prec@5=70.825 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=04:06 IST=> training   63.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.139 Prec@5=70.810 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=04:06 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.139 Prec@5=70.810 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=04:06 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.139 Prec@5=70.810 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=04:06 IST=> training   67.96% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.145 Prec@5=70.826 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=04:06 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.145 Prec@5=70.826 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=04:06 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.436 Prec@1=45.145 Prec@5=70.826 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=04:07 IST=> training   71.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.117 Prec@5=70.794 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=04:07 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.117 Prec@5=70.794 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=04:07 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.117 Prec@5=70.794 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=04:07 IST=> training   75.95% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.127 Prec@5=70.786 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=04:07 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.127 Prec@5=70.786 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=04:07 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.234 Loss=2.437 Prec@1=45.127 Prec@5=70.786 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=04:08 IST=> training   79.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.121 Prec@5=70.779 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=04:08 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.121 Prec@5=70.779 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:08 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.121 Prec@5=70.779 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:08 IST=> training   83.94% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.136 Prec@5=70.787 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:08 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.136 Prec@5=70.787 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=04:08 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.335 DataTime=0.233 Loss=2.437 Prec@1=45.136 Prec@5=70.787 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=04:09 IST=> training   87.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.233 Loss=2.437 Prec@1=45.141 Prec@5=70.786 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=04:09 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.233 Loss=2.437 Prec@1=45.141 Prec@5=70.786 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=04:09 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.233 Loss=2.437 Prec@1=45.141 Prec@5=70.786 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=04:09 IST=> training   91.93% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.232 Loss=2.438 Prec@1=45.115 Prec@5=70.773 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=04:09 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.232 Loss=2.438 Prec@1=45.115 Prec@5=70.773 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=04:09 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.232 Loss=2.438 Prec@1=45.115 Prec@5=70.773 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=04:10 IST=> training   95.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.231 Loss=2.438 Prec@1=45.114 Prec@5=70.778 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=04:10 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.231 Loss=2.438 Prec@1=45.114 Prec@5=70.778 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=04:10 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.334 DataTime=0.231 Loss=2.438 Prec@1=45.114 Prec@5=70.778 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=04:10 IST=> training   99.92% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.333 DataTime=0.231 Loss=2.438 Prec@1=45.113 Prec@5=70.777 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=04:10 IST=> training   100.00% of 1x2503...Epoch=20/150 LR=0.09609 Time=0.333 DataTime=0.231 Loss=2.438 Prec@1=45.113 Prec@5=70.777 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=04:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:10 IST=> validation 0.00% of 1x98...Epoch=20/150 LR=0.09609 Time=6.986 Loss=2.421 Prec@1=44.141 Prec@5=69.922 rate=0 Hz, eta=?, total=0:00:00, wall=04:10 IST=> validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=6.986 Loss=2.421 Prec@1=44.141 Prec@5=69.922 rate=6805.96 Hz, eta=0:00:00, total=0:00:00, wall=04:10 IST** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=6.986 Loss=2.421 Prec@1=44.141 Prec@5=69.922 rate=6805.96 Hz, eta=0:00:00, total=0:00:00, wall=04:11 IST** validation 1.02% of 1x98...Epoch=20/150 LR=0.09609 Time=0.401 Loss=2.583 Prec@1=42.012 Prec@5=68.512 rate=6805.96 Hz, eta=0:00:00, total=0:00:00, wall=04:11 IST** validation 100.00% of 1x98...Epoch=20/150 LR=0.09609 Time=0.401 Loss=2.583 Prec@1=42.012 Prec@5=68.512 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=04:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:11 IST=> training   0.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=4.705 DataTime=4.517 Loss=2.422 Prec@1=44.922 Prec@5=69.531 rate=0 Hz, eta=?, total=0:00:00, wall=04:11 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=4.705 DataTime=4.517 Loss=2.422 Prec@1=44.922 Prec@5=69.531 rate=4421.00 Hz, eta=0:00:00, total=0:00:00, wall=04:11 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=4.705 DataTime=4.517 Loss=2.422 Prec@1=44.922 Prec@5=69.531 rate=4421.00 Hz, eta=0:00:00, total=0:00:00, wall=04:11 IST=> training   0.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.367 DataTime=0.257 Loss=2.380 Prec@1=46.198 Prec@5=71.854 rate=4421.00 Hz, eta=0:00:00, total=0:00:00, wall=04:11 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.367 DataTime=0.257 Loss=2.380 Prec@1=46.198 Prec@5=71.854 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=04:11 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.367 DataTime=0.257 Loss=2.380 Prec@1=46.198 Prec@5=71.854 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=04:12 IST=> training   4.04% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.350 DataTime=0.243 Loss=2.385 Prec@1=46.052 Prec@5=71.748 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=04:12 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.350 DataTime=0.243 Loss=2.385 Prec@1=46.052 Prec@5=71.748 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=04:12 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.350 DataTime=0.243 Loss=2.385 Prec@1=46.052 Prec@5=71.748 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=04:12 IST=> training   8.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.345 DataTime=0.238 Loss=2.389 Prec@1=46.060 Prec@5=71.570 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=04:12 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.345 DataTime=0.238 Loss=2.389 Prec@1=46.060 Prec@5=71.570 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:12 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.345 DataTime=0.238 Loss=2.389 Prec@1=46.060 Prec@5=71.570 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:13 IST=> training   12.03% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.390 Prec@1=46.070 Prec@5=71.551 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:13 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.390 Prec@1=46.070 Prec@5=71.551 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=04:13 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.390 Prec@1=46.070 Prec@5=71.551 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=04:14 IST=> training   16.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.344 DataTime=0.239 Loss=2.391 Prec@1=46.052 Prec@5=71.552 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=04:14 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.344 DataTime=0.239 Loss=2.391 Prec@1=46.052 Prec@5=71.552 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:14 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.344 DataTime=0.239 Loss=2.391 Prec@1=46.052 Prec@5=71.552 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:14 IST=> training   20.02% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.395 Prec@1=45.957 Prec@5=71.462 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:14 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.395 Prec@1=45.957 Prec@5=71.462 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=04:14 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.343 DataTime=0.237 Loss=2.395 Prec@1=45.957 Prec@5=71.462 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=04:15 IST=> training   24.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.235 Loss=2.400 Prec@1=45.885 Prec@5=71.402 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=04:15 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.235 Loss=2.400 Prec@1=45.885 Prec@5=71.402 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:15 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.235 Loss=2.400 Prec@1=45.885 Prec@5=71.402 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:15 IST=> training   28.01% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.342 DataTime=0.235 Loss=2.399 Prec@1=45.851 Prec@5=71.408 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:15 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.342 DataTime=0.235 Loss=2.399 Prec@1=45.851 Prec@5=71.408 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=04:15 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.342 DataTime=0.235 Loss=2.399 Prec@1=45.851 Prec@5=71.408 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=04:16 IST=> training   32.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.401 Prec@1=45.814 Prec@5=71.393 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=04:16 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.401 Prec@1=45.814 Prec@5=71.393 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:16 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.401 Prec@1=45.814 Prec@5=71.393 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:16 IST=> training   36.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.403 Prec@1=45.773 Prec@5=71.342 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:16 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.403 Prec@1=45.773 Prec@5=71.342 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=04:16 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.403 Prec@1=45.773 Prec@5=71.342 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=04:17 IST=> training   39.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.403 Prec@1=45.786 Prec@5=71.347 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=04:17 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.403 Prec@1=45.786 Prec@5=71.347 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:17 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.403 Prec@1=45.786 Prec@5=71.347 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:17 IST=> training   43.99% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.404 Prec@1=45.786 Prec@5=71.336 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:17 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.404 Prec@1=45.786 Prec@5=71.336 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:17 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.404 Prec@1=45.786 Prec@5=71.336 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:18 IST=> training   47.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.406 Prec@1=45.738 Prec@5=71.307 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:18 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.406 Prec@1=45.738 Prec@5=71.307 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=04:18 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.406 Prec@1=45.738 Prec@5=71.307 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=04:19 IST=> training   51.98% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.407 Prec@1=45.714 Prec@5=71.294 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=04:19 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.407 Prec@1=45.714 Prec@5=71.294 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=04:19 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.407 Prec@1=45.714 Prec@5=71.294 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=04:19 IST=> training   55.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.407 Prec@1=45.705 Prec@5=71.286 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=04:19 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.407 Prec@1=45.705 Prec@5=71.286 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:19 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.407 Prec@1=45.705 Prec@5=71.286 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:20 IST=> training   59.97% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.408 Prec@1=45.676 Prec@5=71.264 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:20 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.408 Prec@1=45.676 Prec@5=71.264 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:20 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.408 Prec@1=45.676 Prec@5=71.264 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:20 IST=> training   63.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.409 Prec@1=45.653 Prec@5=71.250 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:20 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.409 Prec@1=45.653 Prec@5=71.250 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:20 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.409 Prec@1=45.653 Prec@5=71.250 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:21 IST=> training   67.96% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.411 Prec@1=45.630 Prec@5=71.235 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:21 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.411 Prec@1=45.630 Prec@5=71.235 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:21 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.233 Loss=2.411 Prec@1=45.630 Prec@5=71.235 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:21 IST=> training   71.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.411 Prec@1=45.612 Prec@5=71.223 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:21 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.411 Prec@1=45.612 Prec@5=71.223 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:21 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.411 Prec@1=45.612 Prec@5=71.223 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:22 IST=> training   75.95% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.413 Prec@1=45.588 Prec@5=71.205 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:22 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.413 Prec@1=45.588 Prec@5=71.205 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=04:22 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.341 DataTime=0.234 Loss=2.413 Prec@1=45.588 Prec@5=71.205 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=04:23 IST=> training   79.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.568 Prec@5=71.197 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=04:23 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.568 Prec@5=71.197 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:23 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.568 Prec@5=71.197 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:23 IST=> training   83.94% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.559 Prec@5=71.188 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:23 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.559 Prec@5=71.188 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=04:23 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.559 Prec@5=71.188 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=04:24 IST=> training   87.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.560 Prec@5=71.188 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=04:24 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.560 Prec@5=71.188 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=04:24 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.414 Prec@1=45.560 Prec@5=71.188 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=04:24 IST=> training   91.93% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.415 Prec@1=45.547 Prec@5=71.172 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=04:24 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.415 Prec@1=45.547 Prec@5=71.172 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=04:24 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.340 DataTime=0.233 Loss=2.415 Prec@1=45.547 Prec@5=71.172 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=04:25 IST=> training   95.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.339 DataTime=0.232 Loss=2.416 Prec@1=45.542 Prec@5=71.153 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=04:25 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.339 DataTime=0.232 Loss=2.416 Prec@1=45.542 Prec@5=71.153 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:25 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.339 DataTime=0.232 Loss=2.416 Prec@1=45.542 Prec@5=71.153 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:25 IST=> training   99.92% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.339 DataTime=0.232 Loss=2.416 Prec@1=45.543 Prec@5=71.153 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:25 IST=> training   100.00% of 1x2503...Epoch=21/150 LR=0.09568 Time=0.339 DataTime=0.232 Loss=2.416 Prec@1=45.543 Prec@5=71.153 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=04:25 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:25 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:25 IST=> validation 0.00% of 1x98...Epoch=21/150 LR=0.09568 Time=5.741 Loss=2.873 Prec@1=33.984 Prec@5=64.453 rate=0 Hz, eta=?, total=0:00:00, wall=04:25 IST=> validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=5.741 Loss=2.873 Prec@1=33.984 Prec@5=64.453 rate=4658.07 Hz, eta=0:00:00, total=0:00:00, wall=04:25 IST** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=5.741 Loss=2.873 Prec@1=33.984 Prec@5=64.453 rate=4658.07 Hz, eta=0:00:00, total=0:00:00, wall=04:25 IST** validation 1.02% of 1x98...Epoch=21/150 LR=0.09568 Time=0.398 Loss=2.909 Prec@1=36.562 Prec@5=62.702 rate=4658.07 Hz, eta=0:00:00, total=0:00:00, wall=04:25 IST** validation 100.00% of 1x98...Epoch=21/150 LR=0.09568 Time=0.398 Loss=2.909 Prec@1=36.562 Prec@5=62.702 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=04:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:26 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:26 IST=> training   0.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.973 DataTime=5.887 Loss=2.490 Prec@1=45.508 Prec@5=70.312 rate=0 Hz, eta=?, total=0:00:00, wall=04:26 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.973 DataTime=5.887 Loss=2.490 Prec@1=45.508 Prec@5=70.312 rate=4851.12 Hz, eta=0:00:00, total=0:00:00, wall=04:26 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=5.973 DataTime=5.887 Loss=2.490 Prec@1=45.508 Prec@5=70.312 rate=4851.12 Hz, eta=0:00:00, total=0:00:00, wall=04:26 IST=> training   0.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.376 DataTime=0.288 Loss=2.386 Prec@1=45.893 Prec@5=71.641 rate=4851.12 Hz, eta=0:00:00, total=0:00:00, wall=04:26 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.376 DataTime=0.288 Loss=2.386 Prec@1=45.893 Prec@5=71.641 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=04:26 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.376 DataTime=0.288 Loss=2.386 Prec@1=45.893 Prec@5=71.641 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=04:27 IST=> training   4.04% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.351 DataTime=0.258 Loss=2.383 Prec@1=46.041 Prec@5=71.702 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=04:27 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.351 DataTime=0.258 Loss=2.383 Prec@1=46.041 Prec@5=71.702 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=04:27 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.351 DataTime=0.258 Loss=2.383 Prec@1=46.041 Prec@5=71.702 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=04:27 IST=> training   8.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.348 DataTime=0.254 Loss=2.382 Prec@1=46.055 Prec@5=71.652 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=04:27 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.348 DataTime=0.254 Loss=2.382 Prec@1=46.055 Prec@5=71.652 rate=3.05 Hz, eta=0:12:03, total=0:01:38, wall=04:27 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.348 DataTime=0.254 Loss=2.382 Prec@1=46.055 Prec@5=71.652 rate=3.05 Hz, eta=0:12:03, total=0:01:38, wall=04:28 IST=> training   12.03% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.345 DataTime=0.249 Loss=2.384 Prec@1=45.926 Prec@5=71.641 rate=3.05 Hz, eta=0:12:03, total=0:01:38, wall=04:28 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.345 DataTime=0.249 Loss=2.384 Prec@1=45.926 Prec@5=71.641 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=04:28 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.345 DataTime=0.249 Loss=2.384 Prec@1=45.926 Prec@5=71.641 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=04:28 IST=> training   16.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.246 Loss=2.384 Prec@1=45.983 Prec@5=71.681 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=04:28 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.246 Loss=2.384 Prec@1=45.983 Prec@5=71.681 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=04:28 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.246 Loss=2.384 Prec@1=45.983 Prec@5=71.681 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=04:29 IST=> training   20.02% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.919 Prec@5=71.606 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=04:29 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.919 Prec@5=71.606 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:29 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.919 Prec@5=71.606 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:29 IST=> training   24.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.943 Prec@5=71.617 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:29 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.943 Prec@5=71.617 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:29 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.343 DataTime=0.244 Loss=2.386 Prec@1=45.943 Prec@5=71.617 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:30 IST=> training   28.01% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.243 Loss=2.385 Prec@1=45.931 Prec@5=71.645 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=04:30 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.243 Loss=2.385 Prec@1=45.931 Prec@5=71.645 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=04:30 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.243 Loss=2.385 Prec@1=45.931 Prec@5=71.645 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=04:31 IST=> training   32.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.387 Prec@1=45.924 Prec@5=71.617 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=04:31 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.387 Prec@1=45.924 Prec@5=71.617 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=04:31 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.387 Prec@1=45.924 Prec@5=71.617 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=04:31 IST=> training   36.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.388 Prec@1=45.946 Prec@5=71.612 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=04:31 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.388 Prec@1=45.946 Prec@5=71.612 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:31 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.388 Prec@1=45.946 Prec@5=71.612 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:32 IST=> training   39.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.242 Loss=2.390 Prec@1=45.945 Prec@5=71.576 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:32 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.242 Loss=2.390 Prec@1=45.945 Prec@5=71.576 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=04:32 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.342 DataTime=0.242 Loss=2.390 Prec@1=45.945 Prec@5=71.576 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=04:32 IST=> training   43.99% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.390 Prec@1=45.952 Prec@5=71.558 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=04:32 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.390 Prec@1=45.952 Prec@5=71.558 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=04:32 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.390 Prec@1=45.952 Prec@5=71.558 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=04:33 IST=> training   47.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.391 Prec@1=45.951 Prec@5=71.562 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=04:33 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.391 Prec@1=45.951 Prec@5=71.562 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=04:33 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.391 Prec@1=45.951 Prec@5=71.562 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=04:33 IST=> training   51.98% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.391 Prec@1=45.953 Prec@5=71.559 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=04:33 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.391 Prec@1=45.953 Prec@5=71.559 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=04:33 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.341 DataTime=0.241 Loss=2.391 Prec@1=45.953 Prec@5=71.559 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=04:34 IST=> training   55.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.392 Prec@1=45.921 Prec@5=71.553 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=04:34 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.392 Prec@1=45.921 Prec@5=71.553 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=04:34 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.240 Loss=2.392 Prec@1=45.921 Prec@5=71.553 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=04:35 IST=> training   59.97% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.923 Prec@5=71.563 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=04:35 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.923 Prec@5=71.563 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=04:35 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.923 Prec@5=71.563 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=04:35 IST=> training   63.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.924 Prec@5=71.543 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=04:35 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.924 Prec@5=71.543 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=04:35 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.340 DataTime=0.239 Loss=2.392 Prec@1=45.924 Prec@5=71.543 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=04:36 IST=> training   67.96% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.239 Loss=2.393 Prec@1=45.917 Prec@5=71.531 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=04:36 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.239 Loss=2.393 Prec@1=45.917 Prec@5=71.531 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=04:36 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.239 Loss=2.393 Prec@1=45.917 Prec@5=71.531 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=04:36 IST=> training   71.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.393 Prec@1=45.921 Prec@5=71.521 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=04:36 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.393 Prec@1=45.921 Prec@5=71.521 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=04:36 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.393 Prec@1=45.921 Prec@5=71.521 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=04:37 IST=> training   75.95% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.394 Prec@1=45.911 Prec@5=71.510 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=04:37 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.394 Prec@1=45.911 Prec@5=71.510 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=04:37 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.394 Prec@1=45.911 Prec@5=71.510 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=04:37 IST=> training   79.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.394 Prec@1=45.887 Prec@5=71.497 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=04:37 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.394 Prec@1=45.887 Prec@5=71.497 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=04:37 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.394 Prec@1=45.887 Prec@5=71.497 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=04:38 IST=> training   83.94% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.881 Prec@5=71.487 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=04:38 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.881 Prec@5=71.487 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=04:38 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.881 Prec@5=71.487 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=04:38 IST=> training   87.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.874 Prec@5=71.476 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=04:38 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.874 Prec@5=71.476 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:38 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.339 DataTime=0.238 Loss=2.395 Prec@1=45.874 Prec@5=71.476 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:39 IST=> training   91.93% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.396 Prec@1=45.866 Prec@5=71.461 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:39 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.396 Prec@1=45.866 Prec@5=71.461 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:39 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.238 Loss=2.396 Prec@1=45.866 Prec@5=71.461 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:40 IST=> training   95.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.396 Prec@1=45.881 Prec@5=71.464 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:40 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.396 Prec@1=45.881 Prec@5=71.464 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=04:40 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.396 Prec@1=45.881 Prec@5=71.464 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=04:40 IST=> training   99.92% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.396 Prec@1=45.882 Prec@5=71.464 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=04:40 IST=> training   100.00% of 1x2503...Epoch=22/150 LR=0.09524 Time=0.338 DataTime=0.237 Loss=2.396 Prec@1=45.882 Prec@5=71.464 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=04:40 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> validation 0.00% of 1x98...Epoch=22/150 LR=0.09524 Time=6.352 Loss=2.318 Prec@1=46.094 Prec@5=73.438 rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=6.352 Loss=2.318 Prec@1=46.094 Prec@5=73.438 rate=6258.88 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=6.352 Loss=2.318 Prec@1=46.094 Prec@5=73.438 rate=6258.88 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST** validation 1.02% of 1x98...Epoch=22/150 LR=0.09524 Time=0.403 Loss=2.434 Prec@1=44.852 Prec@5=70.972 rate=6258.88 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST** validation 100.00% of 1x98...Epoch=22/150 LR=0.09524 Time=0.403 Loss=2.434 Prec@1=44.852 Prec@5=70.972 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=04:40 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> training   0.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.588 DataTime=5.456 Loss=2.357 Prec@1=47.070 Prec@5=71.875 rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.588 DataTime=5.456 Loss=2.357 Prec@1=47.070 Prec@5=71.875 rate=2984.64 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=5.588 DataTime=5.456 Loss=2.357 Prec@1=47.070 Prec@5=71.875 rate=2984.64 Hz, eta=0:00:00, total=0:00:00, wall=04:41 IST=> training   0.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.373 DataTime=0.274 Loss=2.354 Prec@1=46.349 Prec@5=72.167 rate=2984.64 Hz, eta=0:00:00, total=0:00:00, wall=04:41 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.373 DataTime=0.274 Loss=2.354 Prec@1=46.349 Prec@5=72.167 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=04:41 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.373 DataTime=0.274 Loss=2.354 Prec@1=46.349 Prec@5=72.167 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=04:41 IST=> training   4.04% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.350 DataTime=0.250 Loss=2.356 Prec@1=46.599 Prec@5=72.099 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=04:41 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.350 DataTime=0.250 Loss=2.356 Prec@1=46.599 Prec@5=72.099 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=04:41 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.350 DataTime=0.250 Loss=2.356 Prec@1=46.599 Prec@5=72.099 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=04:42 IST=> training   8.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.348 DataTime=0.249 Loss=2.359 Prec@1=46.497 Prec@5=72.044 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=04:42 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.348 DataTime=0.249 Loss=2.359 Prec@1=46.497 Prec@5=72.044 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=04:42 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.348 DataTime=0.249 Loss=2.359 Prec@1=46.497 Prec@5=72.044 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=04:43 IST=> training   12.03% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.364 Prec@1=46.466 Prec@5=71.986 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=04:43 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.364 Prec@1=46.466 Prec@5=71.986 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=04:43 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.364 Prec@1=46.466 Prec@5=71.986 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=04:43 IST=> training   16.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.241 Loss=2.368 Prec@1=46.367 Prec@5=71.889 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=04:43 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.241 Loss=2.368 Prec@1=46.367 Prec@5=71.889 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=04:43 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.241 Loss=2.368 Prec@1=46.367 Prec@5=71.889 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=04:44 IST=> training   20.02% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.370 Prec@1=46.365 Prec@5=71.901 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=04:44 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.370 Prec@1=46.365 Prec@5=71.901 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:44 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.342 DataTime=0.243 Loss=2.370 Prec@1=46.365 Prec@5=71.901 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:44 IST=> training   24.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.240 Loss=2.370 Prec@1=46.367 Prec@5=71.872 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=04:44 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.240 Loss=2.370 Prec@1=46.367 Prec@5=71.872 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=04:44 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.340 DataTime=0.240 Loss=2.370 Prec@1=46.367 Prec@5=71.872 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=04:45 IST=> training   28.01% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.369 Prec@1=46.383 Prec@5=71.896 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=04:45 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.369 Prec@1=46.383 Prec@5=71.896 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=04:45 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.369 Prec@1=46.383 Prec@5=71.896 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=04:45 IST=> training   32.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.370 Prec@1=46.379 Prec@5=71.886 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=04:45 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.370 Prec@1=46.379 Prec@5=71.886 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=04:45 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.370 Prec@1=46.379 Prec@5=71.886 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=04:46 IST=> training   36.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.371 Prec@1=46.378 Prec@5=71.893 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=04:46 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.371 Prec@1=46.378 Prec@5=71.893 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=04:46 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.371 Prec@1=46.378 Prec@5=71.893 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=04:46 IST=> training   39.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.371 Prec@1=46.381 Prec@5=71.891 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=04:46 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.371 Prec@1=46.381 Prec@5=71.891 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=04:46 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.338 DataTime=0.239 Loss=2.371 Prec@1=46.381 Prec@5=71.891 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=04:47 IST=> training   43.99% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.372 Prec@1=46.350 Prec@5=71.875 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=04:47 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.372 Prec@1=46.350 Prec@5=71.875 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=04:47 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.238 Loss=2.372 Prec@1=46.350 Prec@5=71.875 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=04:48 IST=> training   47.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.373 Prec@1=46.334 Prec@5=71.874 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=04:48 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.373 Prec@1=46.334 Prec@5=71.874 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=04:48 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.373 Prec@1=46.334 Prec@5=71.874 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=04:48 IST=> training   51.98% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.374 Prec@1=46.325 Prec@5=71.843 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=04:48 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.374 Prec@1=46.325 Prec@5=71.843 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=04:48 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.337 DataTime=0.237 Loss=2.374 Prec@1=46.325 Prec@5=71.843 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=04:49 IST=> training   55.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.375 Prec@1=46.298 Prec@5=71.833 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=04:49 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.375 Prec@1=46.298 Prec@5=71.833 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=04:49 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.375 Prec@1=46.298 Prec@5=71.833 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=04:49 IST=> training   59.97% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.291 Prec@5=71.812 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=04:49 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.291 Prec@5=71.812 rate=3.01 Hz, eta=0:04:59, total=0:08:52, wall=04:49 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.291 Prec@5=71.812 rate=3.01 Hz, eta=0:04:59, total=0:08:52, wall=04:50 IST=> training   63.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.300 Prec@5=71.799 rate=3.01 Hz, eta=0:04:59, total=0:08:52, wall=04:50 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.300 Prec@5=71.799 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=04:50 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.236 Loss=2.376 Prec@1=46.300 Prec@5=71.799 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=04:50 IST=> training   67.96% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.376 Prec@1=46.312 Prec@5=71.806 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=04:50 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.376 Prec@1=46.312 Prec@5=71.806 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=04:50 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.336 DataTime=0.235 Loss=2.376 Prec@1=46.312 Prec@5=71.806 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=04:51 IST=> training   71.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.294 Prec@5=71.786 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=04:51 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.294 Prec@5=71.786 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=04:51 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.294 Prec@5=71.786 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=04:51 IST=> training   75.95% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.266 Prec@5=71.779 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=04:51 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.266 Prec@5=71.779 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=04:51 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.266 Prec@5=71.779 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=04:52 IST=> training   79.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.250 Prec@5=71.771 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=04:52 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.250 Prec@5=71.771 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:52 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.250 Prec@5=71.771 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:53 IST=> training   83.94% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.251 Prec@5=71.762 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=04:53 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.251 Prec@5=71.762 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=04:53 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.377 Prec@1=46.251 Prec@5=71.762 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=04:53 IST=> training   87.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.257 Prec@5=71.760 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=04:53 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.257 Prec@5=71.760 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=04:53 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.378 Prec@1=46.257 Prec@5=71.760 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=04:54 IST=> training   91.93% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.379 Prec@1=46.226 Prec@5=71.743 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=04:54 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.379 Prec@1=46.226 Prec@5=71.743 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=04:54 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.234 Loss=2.379 Prec@1=46.226 Prec@5=71.743 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=04:54 IST=> training   95.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.233 Loss=2.379 Prec@1=46.230 Prec@5=71.739 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=04:54 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.233 Loss=2.379 Prec@1=46.230 Prec@5=71.739 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=04:54 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.233 Loss=2.379 Prec@1=46.230 Prec@5=71.739 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=04:54 IST=> training   99.92% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.233 Loss=2.379 Prec@1=46.231 Prec@5=71.739 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=04:54 IST=> training   100.00% of 1x2503...Epoch=23/150 LR=0.09479 Time=0.335 DataTime=0.233 Loss=2.379 Prec@1=46.231 Prec@5=71.739 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=04:54 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST=> validation 0.00% of 1x98...Epoch=23/150 LR=0.09479 Time=6.851 Loss=2.645 Prec@1=41.797 Prec@5=68.164 rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST=> validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=6.851 Loss=2.645 Prec@1=41.797 Prec@5=68.164 rate=6753.34 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=6.851 Loss=2.645 Prec@1=41.797 Prec@5=68.164 rate=6753.34 Hz, eta=0:00:00, total=0:00:00, wall=04:55 IST** validation 1.02% of 1x98...Epoch=23/150 LR=0.09479 Time=0.406 Loss=2.569 Prec@1=42.284 Prec@5=68.702 rate=6753.34 Hz, eta=0:00:00, total=0:00:00, wall=04:55 IST** validation 100.00% of 1x98...Epoch=23/150 LR=0.09479 Time=0.406 Loss=2.569 Prec@1=42.284 Prec@5=68.702 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=04:55 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:55 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:55 IST=> training   0.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=4.826 DataTime=4.631 Loss=2.209 Prec@1=48.438 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=04:55 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=4.826 DataTime=4.631 Loss=2.209 Prec@1=48.438 Prec@5=76.367 rate=6078.55 Hz, eta=0:00:00, total=0:00:00, wall=04:55 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=4.826 DataTime=4.631 Loss=2.209 Prec@1=48.438 Prec@5=76.367 rate=6078.55 Hz, eta=0:00:00, total=0:00:00, wall=04:56 IST=> training   0.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.378 DataTime=0.282 Loss=2.324 Prec@1=47.082 Prec@5=72.736 rate=6078.55 Hz, eta=0:00:00, total=0:00:00, wall=04:56 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.378 DataTime=0.282 Loss=2.324 Prec@1=47.082 Prec@5=72.736 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=04:56 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.378 DataTime=0.282 Loss=2.324 Prec@1=47.082 Prec@5=72.736 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=04:56 IST=> training   4.04% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.351 DataTime=0.258 Loss=2.336 Prec@1=46.957 Prec@5=72.467 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=04:56 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.351 DataTime=0.258 Loss=2.336 Prec@1=46.957 Prec@5=72.467 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=04:56 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.351 DataTime=0.258 Loss=2.336 Prec@1=46.957 Prec@5=72.467 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=04:57 IST=> training   8.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.352 DataTime=0.253 Loss=2.340 Prec@1=46.861 Prec@5=72.367 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=04:57 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.352 DataTime=0.253 Loss=2.340 Prec@1=46.861 Prec@5=72.367 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=04:57 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.352 DataTime=0.253 Loss=2.340 Prec@1=46.861 Prec@5=72.367 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=04:57 IST=> training   12.03% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.347 DataTime=0.244 Loss=2.348 Prec@1=46.736 Prec@5=72.259 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=04:57 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.347 DataTime=0.244 Loss=2.348 Prec@1=46.736 Prec@5=72.259 rate=2.99 Hz, eta=0:11:44, total=0:02:14, wall=04:57 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.347 DataTime=0.244 Loss=2.348 Prec@1=46.736 Prec@5=72.259 rate=2.99 Hz, eta=0:11:44, total=0:02:14, wall=04:58 IST=> training   16.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.345 DataTime=0.241 Loss=2.354 Prec@1=46.689 Prec@5=72.177 rate=2.99 Hz, eta=0:11:44, total=0:02:14, wall=04:58 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.345 DataTime=0.241 Loss=2.354 Prec@1=46.689 Prec@5=72.177 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:58 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.345 DataTime=0.241 Loss=2.354 Prec@1=46.689 Prec@5=72.177 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:58 IST=> training   20.02% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.344 DataTime=0.240 Loss=2.354 Prec@1=46.688 Prec@5=72.189 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=04:58 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.344 DataTime=0.240 Loss=2.354 Prec@1=46.688 Prec@5=72.189 rate=2.98 Hz, eta=0:10:39, total=0:03:21, wall=04:58 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.344 DataTime=0.240 Loss=2.354 Prec@1=46.688 Prec@5=72.189 rate=2.98 Hz, eta=0:10:39, total=0:03:21, wall=04:59 IST=> training   24.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.237 Loss=2.355 Prec@1=46.638 Prec@5=72.158 rate=2.98 Hz, eta=0:10:39, total=0:03:21, wall=04:59 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.237 Loss=2.355 Prec@1=46.638 Prec@5=72.158 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=04:59 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.237 Loss=2.355 Prec@1=46.638 Prec@5=72.158 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=04:59 IST=> training   28.01% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.341 DataTime=0.236 Loss=2.357 Prec@1=46.637 Prec@5=72.152 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=04:59 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.341 DataTime=0.236 Loss=2.357 Prec@1=46.637 Prec@5=72.152 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=04:59 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.341 DataTime=0.236 Loss=2.357 Prec@1=46.637 Prec@5=72.152 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=05:00 IST=> training   32.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.236 Loss=2.358 Prec@1=46.625 Prec@5=72.118 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=05:00 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.236 Loss=2.358 Prec@1=46.625 Prec@5=72.118 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:00 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.342 DataTime=0.236 Loss=2.358 Prec@1=46.625 Prec@5=72.118 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:01 IST=> training   36.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.235 Loss=2.358 Prec@1=46.612 Prec@5=72.111 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:01 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.235 Loss=2.358 Prec@1=46.612 Prec@5=72.111 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=05:01 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.235 Loss=2.358 Prec@1=46.612 Prec@5=72.111 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=05:01 IST=> training   39.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.234 Loss=2.360 Prec@1=46.590 Prec@5=72.085 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=05:01 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.234 Loss=2.360 Prec@1=46.590 Prec@5=72.085 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=05:01 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.234 Loss=2.360 Prec@1=46.590 Prec@5=72.085 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=05:02 IST=> training   43.99% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.580 Prec@5=72.061 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=05:02 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.580 Prec@5=72.061 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=05:02 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.580 Prec@5=72.061 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=05:02 IST=> training   47.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.564 Prec@5=72.072 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=05:02 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.564 Prec@5=72.072 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:02 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.340 DataTime=0.234 Loss=2.361 Prec@1=46.564 Prec@5=72.072 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:03 IST=> training   51.98% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.575 Prec@5=72.062 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:03 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.575 Prec@5=72.062 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=05:03 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.575 Prec@5=72.062 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=05:03 IST=> training   55.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.574 Prec@5=72.077 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=05:03 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.574 Prec@5=72.077 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:03 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.574 Prec@5=72.077 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:04 IST=> training   59.97% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.570 Prec@5=72.081 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:04 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.570 Prec@5=72.081 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:04 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.360 Prec@1=46.570 Prec@5=72.081 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:05 IST=> training   63.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.562 Prec@5=72.067 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:05 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.562 Prec@5=72.067 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:05 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.562 Prec@5=72.067 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:05 IST=> training   67.96% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.537 Prec@5=72.063 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:05 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.537 Prec@5=72.063 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=05:05 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.537 Prec@5=72.063 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=05:06 IST=> training   71.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.512 Prec@5=72.052 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=05:06 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.512 Prec@5=72.052 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=05:06 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.512 Prec@5=72.052 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=05:06 IST=> training   75.95% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.514 Prec@5=72.046 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=05:06 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.514 Prec@5=72.046 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:06 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.233 Loss=2.361 Prec@1=46.514 Prec@5=72.046 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:07 IST=> training   79.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.511 Prec@5=72.042 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:07 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.511 Prec@5=72.042 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:07 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.511 Prec@5=72.042 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:07 IST=> training   83.94% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.508 Prec@5=72.033 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:07 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.508 Prec@5=72.033 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:07 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.339 DataTime=0.232 Loss=2.362 Prec@1=46.508 Prec@5=72.033 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:08 IST=> training   87.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.508 Prec@5=72.036 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:08 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.508 Prec@5=72.036 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=05:08 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.508 Prec@5=72.036 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=05:08 IST=> training   91.93% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.500 Prec@5=72.035 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=05:08 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.500 Prec@5=72.035 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:08 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.363 Prec@1=46.500 Prec@5=72.035 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:09 IST=> training   95.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.364 Prec@1=46.478 Prec@5=72.009 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:09 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.364 Prec@1=46.478 Prec@5=72.009 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=05:09 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.338 DataTime=0.232 Loss=2.364 Prec@1=46.478 Prec@5=72.009 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=05:09 IST=> training   99.92% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.337 DataTime=0.231 Loss=2.364 Prec@1=46.477 Prec@5=72.009 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=05:09 IST=> training   100.00% of 1x2503...Epoch=24/150 LR=0.09431 Time=0.337 DataTime=0.231 Loss=2.364 Prec@1=46.477 Prec@5=72.009 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=05:09 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST=> validation 0.00% of 1x98...Epoch=24/150 LR=0.09431 Time=6.113 Loss=2.397 Prec@1=44.922 Prec@5=71.289 rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST=> validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.113 Loss=2.397 Prec@1=44.922 Prec@5=71.289 rate=4435.63 Hz, eta=0:00:00, total=0:00:00, wall=05:09 IST** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=6.113 Loss=2.397 Prec@1=44.922 Prec@5=71.289 rate=4435.63 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST** validation 1.02% of 1x98...Epoch=24/150 LR=0.09431 Time=0.397 Loss=2.505 Prec@1=43.676 Prec@5=69.840 rate=4435.63 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST** validation 100.00% of 1x98...Epoch=24/150 LR=0.09431 Time=0.397 Loss=2.505 Prec@1=43.676 Prec@5=69.840 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=05:10 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> training   0.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.773 DataTime=4.528 Loss=2.365 Prec@1=45.898 Prec@5=71.484 rate=0 Hz, eta=?, total=0:00:00, wall=05:10 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.773 DataTime=4.528 Loss=2.365 Prec@1=45.898 Prec@5=71.484 rate=2843.98 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=4.773 DataTime=4.528 Loss=2.365 Prec@1=45.898 Prec@5=71.484 rate=2843.98 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST=> training   0.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.391 DataTime=0.297 Loss=2.337 Prec@1=46.871 Prec@5=72.337 rate=2843.98 Hz, eta=0:00:00, total=0:00:00, wall=05:10 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.391 DataTime=0.297 Loss=2.337 Prec@1=46.871 Prec@5=72.337 rate=2.91 Hz, eta=0:13:46, total=0:00:34, wall=05:10 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.391 DataTime=0.297 Loss=2.337 Prec@1=46.871 Prec@5=72.337 rate=2.91 Hz, eta=0:13:46, total=0:00:34, wall=05:11 IST=> training   4.04% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.360 DataTime=0.258 Loss=2.330 Prec@1=47.205 Prec@5=72.465 rate=2.91 Hz, eta=0:13:46, total=0:00:34, wall=05:11 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.360 DataTime=0.258 Loss=2.330 Prec@1=47.205 Prec@5=72.465 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=05:11 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.360 DataTime=0.258 Loss=2.330 Prec@1=47.205 Prec@5=72.465 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=05:11 IST=> training   8.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.333 Prec@1=47.163 Prec@5=72.524 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=05:11 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.333 Prec@1=47.163 Prec@5=72.524 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=05:11 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.333 Prec@1=47.163 Prec@5=72.524 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=05:12 IST=> training   12.03% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.336 Prec@1=47.102 Prec@5=72.505 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=05:12 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.336 Prec@1=47.102 Prec@5=72.505 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=05:12 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.348 DataTime=0.244 Loss=2.336 Prec@1=47.102 Prec@5=72.505 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=05:13 IST=> training   16.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.347 DataTime=0.242 Loss=2.339 Prec@1=47.041 Prec@5=72.437 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=05:13 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.347 DataTime=0.242 Loss=2.339 Prec@1=47.041 Prec@5=72.437 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=05:13 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.347 DataTime=0.242 Loss=2.339 Prec@1=47.041 Prec@5=72.437 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=05:13 IST=> training   20.02% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.083 Prec@5=72.510 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=05:13 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.083 Prec@5=72.510 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:13 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.083 Prec@5=72.510 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:14 IST=> training   24.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.087 Prec@5=72.529 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:14 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.087 Prec@5=72.529 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=05:14 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.345 DataTime=0.240 Loss=2.336 Prec@1=47.087 Prec@5=72.529 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=05:14 IST=> training   28.01% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.343 DataTime=0.238 Loss=2.335 Prec@1=47.068 Prec@5=72.543 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=05:14 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.343 DataTime=0.238 Loss=2.335 Prec@1=47.068 Prec@5=72.543 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=05:14 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.343 DataTime=0.238 Loss=2.335 Prec@1=47.068 Prec@5=72.543 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=05:15 IST=> training   32.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.335 Prec@1=47.077 Prec@5=72.549 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=05:15 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.335 Prec@1=47.077 Prec@5=72.549 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:15 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.335 Prec@1=47.077 Prec@5=72.549 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:15 IST=> training   36.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.337 Prec@1=47.038 Prec@5=72.524 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:15 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.337 Prec@1=47.038 Prec@5=72.524 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=05:15 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.342 DataTime=0.237 Loss=2.337 Prec@1=47.038 Prec@5=72.524 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=05:16 IST=> training   39.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.236 Loss=2.338 Prec@1=46.998 Prec@5=72.520 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=05:16 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.236 Loss=2.338 Prec@1=46.998 Prec@5=72.520 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:16 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.236 Loss=2.338 Prec@1=46.998 Prec@5=72.520 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:16 IST=> training   43.99% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.339 Prec@1=47.004 Prec@5=72.498 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:16 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.339 Prec@1=47.004 Prec@5=72.498 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:16 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.339 Prec@1=47.004 Prec@5=72.498 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:17 IST=> training   47.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.340 Prec@1=46.996 Prec@5=72.476 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:17 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.340 Prec@1=46.996 Prec@5=72.476 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=05:17 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.341 DataTime=0.236 Loss=2.340 Prec@1=46.996 Prec@5=72.476 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=05:18 IST=> training   51.98% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.340 Prec@1=47.001 Prec@5=72.464 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=05:18 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.340 Prec@1=47.001 Prec@5=72.464 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=05:18 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.340 Prec@1=47.001 Prec@5=72.464 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=05:18 IST=> training   55.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.341 Prec@1=46.989 Prec@5=72.452 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=05:18 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.341 Prec@1=46.989 Prec@5=72.452 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:18 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.341 Prec@1=46.989 Prec@5=72.452 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:19 IST=> training   59.97% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.342 Prec@1=46.971 Prec@5=72.430 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=05:19 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.342 Prec@1=46.971 Prec@5=72.430 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=05:19 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.340 DataTime=0.235 Loss=2.342 Prec@1=46.971 Prec@5=72.430 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=05:19 IST=> training   63.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.342 Prec@1=46.966 Prec@5=72.429 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=05:19 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.342 Prec@1=46.966 Prec@5=72.429 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:19 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.342 Prec@1=46.966 Prec@5=72.429 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:20 IST=> training   67.96% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.342 Prec@1=46.944 Prec@5=72.423 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:20 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.342 Prec@1=46.944 Prec@5=72.423 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=05:20 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.342 Prec@1=46.944 Prec@5=72.423 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=05:20 IST=> training   71.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.343 Prec@1=46.923 Prec@5=72.399 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=05:20 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.343 Prec@1=46.923 Prec@5=72.399 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:20 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.343 Prec@1=46.923 Prec@5=72.399 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:21 IST=> training   75.95% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.344 Prec@1=46.905 Prec@5=72.380 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:21 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.344 Prec@1=46.905 Prec@5=72.380 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=05:21 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.344 Prec@1=46.905 Prec@5=72.380 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=05:22 IST=> training   79.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.344 Prec@1=46.904 Prec@5=72.370 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=05:22 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.344 Prec@1=46.904 Prec@5=72.370 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:22 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.344 Prec@1=46.904 Prec@5=72.370 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:22 IST=> training   83.94% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.345 Prec@1=46.890 Prec@5=72.357 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:22 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.345 Prec@1=46.890 Prec@5=72.357 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:22 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.235 Loss=2.345 Prec@1=46.890 Prec@5=72.357 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:23 IST=> training   87.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.346 Prec@1=46.872 Prec@5=72.341 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:23 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.346 Prec@1=46.872 Prec@5=72.341 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:23 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.339 DataTime=0.234 Loss=2.346 Prec@1=46.872 Prec@5=72.341 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:23 IST=> training   91.93% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.888 Prec@5=72.344 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:23 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.888 Prec@5=72.344 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:23 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.888 Prec@5=72.344 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:24 IST=> training   95.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.887 Prec@5=72.353 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.887 Prec@5=72.353 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.234 Loss=2.346 Prec@1=46.887 Prec@5=72.353 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:24 IST=> training   99.92% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.233 Loss=2.346 Prec@1=46.887 Prec@5=72.353 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:24 IST=> training   100.00% of 1x2503...Epoch=25/150 LR=0.09382 Time=0.338 DataTime=0.233 Loss=2.346 Prec@1=46.887 Prec@5=72.353 rate=2.98 Hz, eta=0:00:00, total=0:14:01, wall=05:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 0.00% of 1x98...Epoch=25/150 LR=0.09382 Time=7.156 Loss=2.470 Prec@1=45.703 Prec@5=70.117 rate=0 Hz, eta=?, total=0:00:00, wall=05:24 IST=> validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=7.156 Loss=2.470 Prec@1=45.703 Prec@5=70.117 rate=5935.60 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=7.156 Loss=2.470 Prec@1=45.703 Prec@5=70.117 rate=5935.60 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST** validation 1.02% of 1x98...Epoch=25/150 LR=0.09382 Time=0.401 Loss=2.422 Prec@1=44.930 Prec@5=71.342 rate=5935.60 Hz, eta=0:00:00, total=0:00:00, wall=05:24 IST** validation 100.00% of 1x98...Epoch=25/150 LR=0.09382 Time=0.401 Loss=2.422 Prec@1=44.930 Prec@5=71.342 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=05:24 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.200 DataTime=4.087 Loss=2.267 Prec@1=50.195 Prec@5=72.852 rate=0 Hz, eta=?, total=0:00:00, wall=05:25 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.200 DataTime=4.087 Loss=2.267 Prec@1=50.195 Prec@5=72.852 rate=5552.38 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=4.200 DataTime=4.087 Loss=2.267 Prec@1=50.195 Prec@5=72.852 rate=5552.38 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST=> training   0.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.375 DataTime=0.279 Loss=2.319 Prec@1=47.407 Prec@5=72.712 rate=5552.38 Hz, eta=0:00:00, total=0:00:00, wall=05:25 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.375 DataTime=0.279 Loss=2.319 Prec@1=47.407 Prec@5=72.712 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=05:25 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.375 DataTime=0.279 Loss=2.319 Prec@1=47.407 Prec@5=72.712 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=05:26 IST=> training   4.04% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.352 DataTime=0.258 Loss=2.317 Prec@1=47.398 Prec@5=72.640 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=05:26 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.352 DataTime=0.258 Loss=2.317 Prec@1=47.398 Prec@5=72.640 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=05:26 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.352 DataTime=0.258 Loss=2.317 Prec@1=47.398 Prec@5=72.640 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=05:26 IST=> training   8.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.350 DataTime=0.255 Loss=2.317 Prec@1=47.363 Prec@5=72.640 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=05:26 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.350 DataTime=0.255 Loss=2.317 Prec@1=47.363 Prec@5=72.640 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:26 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.350 DataTime=0.255 Loss=2.317 Prec@1=47.363 Prec@5=72.640 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:27 IST=> training   12.03% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.345 DataTime=0.251 Loss=2.320 Prec@1=47.305 Prec@5=72.673 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:27 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.345 DataTime=0.251 Loss=2.320 Prec@1=47.305 Prec@5=72.673 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=05:27 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.345 DataTime=0.251 Loss=2.320 Prec@1=47.305 Prec@5=72.673 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=05:27 IST=> training   16.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.341 DataTime=0.247 Loss=2.321 Prec@1=47.304 Prec@5=72.648 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=05:27 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.341 DataTime=0.247 Loss=2.321 Prec@1=47.304 Prec@5=72.648 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=05:27 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.341 DataTime=0.247 Loss=2.321 Prec@1=47.304 Prec@5=72.648 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=05:28 IST=> training   20.02% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.342 DataTime=0.248 Loss=2.321 Prec@1=47.242 Prec@5=72.658 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=05:28 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.342 DataTime=0.248 Loss=2.321 Prec@1=47.242 Prec@5=72.658 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=05:28 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.342 DataTime=0.248 Loss=2.321 Prec@1=47.242 Prec@5=72.658 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=05:28 IST=> training   24.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.340 DataTime=0.246 Loss=2.324 Prec@1=47.161 Prec@5=72.630 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=05:28 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.340 DataTime=0.246 Loss=2.324 Prec@1=47.161 Prec@5=72.630 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=05:28 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.340 DataTime=0.246 Loss=2.324 Prec@1=47.161 Prec@5=72.630 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=05:29 IST=> training   28.01% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.244 Loss=2.325 Prec@1=47.113 Prec@5=72.601 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=05:29 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.244 Loss=2.325 Prec@1=47.113 Prec@5=72.601 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=05:29 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.244 Loss=2.325 Prec@1=47.113 Prec@5=72.601 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=05:30 IST=> training   32.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.242 Loss=2.325 Prec@1=47.146 Prec@5=72.581 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=05:30 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.242 Loss=2.325 Prec@1=47.146 Prec@5=72.581 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:30 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.339 DataTime=0.242 Loss=2.325 Prec@1=47.146 Prec@5=72.581 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:30 IST=> training   36.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.240 Loss=2.325 Prec@1=47.189 Prec@5=72.596 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:30 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.240 Loss=2.325 Prec@1=47.189 Prec@5=72.596 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=05:30 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.240 Loss=2.325 Prec@1=47.189 Prec@5=72.596 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=05:31 IST=> training   39.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.239 Loss=2.326 Prec@1=47.158 Prec@5=72.581 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=05:31 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.239 Loss=2.326 Prec@1=47.158 Prec@5=72.581 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=05:31 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.239 Loss=2.326 Prec@1=47.158 Prec@5=72.581 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=05:31 IST=> training   43.99% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.239 Loss=2.327 Prec@1=47.144 Prec@5=72.577 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=05:31 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.239 Loss=2.327 Prec@1=47.144 Prec@5=72.577 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=05:31 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.338 DataTime=0.239 Loss=2.327 Prec@1=47.144 Prec@5=72.577 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=05:32 IST=> training   47.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.238 Loss=2.328 Prec@1=47.135 Prec@5=72.554 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=05:32 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.238 Loss=2.328 Prec@1=47.135 Prec@5=72.554 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=05:32 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.238 Loss=2.328 Prec@1=47.135 Prec@5=72.554 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=05:32 IST=> training   51.98% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.327 Prec@1=47.147 Prec@5=72.580 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=05:32 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.327 Prec@1=47.147 Prec@5=72.580 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=05:32 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.327 Prec@1=47.147 Prec@5=72.580 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=05:33 IST=> training   55.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.328 Prec@1=47.134 Prec@5=72.569 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=05:33 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.328 Prec@1=47.134 Prec@5=72.569 rate=2.99 Hz, eta=0:05:35, total=0:08:21, wall=05:33 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.337 DataTime=0.237 Loss=2.328 Prec@1=47.134 Prec@5=72.569 rate=2.99 Hz, eta=0:05:35, total=0:08:21, wall=05:33 IST=> training   59.97% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.328 Prec@1=47.115 Prec@5=72.562 rate=2.99 Hz, eta=0:05:35, total=0:08:21, wall=05:33 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.328 Prec@1=47.115 Prec@5=72.562 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=05:33 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.328 Prec@1=47.115 Prec@5=72.562 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=05:34 IST=> training   63.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.329 Prec@1=47.092 Prec@5=72.546 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=05:34 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.329 Prec@1=47.092 Prec@5=72.546 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:34 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.329 Prec@1=47.092 Prec@5=72.546 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:35 IST=> training   67.96% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.329 Prec@1=47.098 Prec@5=72.548 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:35 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.329 Prec@1=47.098 Prec@5=72.548 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=05:35 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.329 Prec@1=47.098 Prec@5=72.548 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=05:35 IST=> training   71.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.330 Prec@1=47.083 Prec@5=72.528 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=05:35 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.330 Prec@1=47.083 Prec@5=72.528 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:35 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.236 Loss=2.330 Prec@1=47.083 Prec@5=72.528 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:36 IST=> training   75.95% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.331 Prec@1=47.056 Prec@5=72.506 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:36 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.331 Prec@1=47.056 Prec@5=72.506 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=05:36 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.331 Prec@1=47.056 Prec@5=72.506 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=05:36 IST=> training   79.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.332 Prec@1=47.039 Prec@5=72.486 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=05:36 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.332 Prec@1=47.039 Prec@5=72.486 rate=3.00 Hz, eta=0:02:14, total=0:11:41, wall=05:36 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.336 DataTime=0.235 Loss=2.332 Prec@1=47.039 Prec@5=72.486 rate=3.00 Hz, eta=0:02:14, total=0:11:41, wall=05:37 IST=> training   83.94% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.332 Prec@1=47.042 Prec@5=72.489 rate=3.00 Hz, eta=0:02:14, total=0:11:41, wall=05:37 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.332 Prec@1=47.042 Prec@5=72.489 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=05:37 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.235 Loss=2.332 Prec@1=47.042 Prec@5=72.489 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=05:37 IST=> training   87.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.041 Prec@5=72.499 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=05:37 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.041 Prec@5=72.499 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=05:37 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.041 Prec@5=72.499 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=05:38 IST=> training   91.93% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.045 Prec@5=72.499 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=05:38 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.045 Prec@5=72.499 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=05:38 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.332 Prec@1=47.045 Prec@5=72.499 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=05:38 IST=> training   95.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.334 Prec@1=47.018 Prec@5=72.481 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=05:38 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.334 Prec@1=47.018 Prec@5=72.481 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=05:38 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.335 DataTime=0.234 Loss=2.334 Prec@1=47.018 Prec@5=72.481 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=05:38 IST=> training   99.92% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.334 DataTime=0.234 Loss=2.334 Prec@1=47.017 Prec@5=72.481 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=05:38 IST=> training   100.00% of 1x2503...Epoch=26/150 LR=0.09330 Time=0.334 DataTime=0.234 Loss=2.334 Prec@1=47.017 Prec@5=72.481 rate=3.01 Hz, eta=0:00:00, total=0:13:52, wall=05:38 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> validation 0.00% of 1x98...Epoch=26/150 LR=0.09330 Time=6.386 Loss=2.496 Prec@1=41.602 Prec@5=69.727 rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=6.386 Loss=2.496 Prec@1=41.602 Prec@5=69.727 rate=6629.45 Hz, eta=0:00:00, total=0:00:00, wall=05:39 IST** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=6.386 Loss=2.496 Prec@1=41.602 Prec@5=69.727 rate=6629.45 Hz, eta=0:00:00, total=0:00:00, wall=05:39 IST** validation 1.02% of 1x98...Epoch=26/150 LR=0.09330 Time=0.403 Loss=2.544 Prec@1=42.878 Prec@5=69.038 rate=6629.45 Hz, eta=0:00:00, total=0:00:00, wall=05:39 IST** validation 100.00% of 1x98...Epoch=26/150 LR=0.09330 Time=0.403 Loss=2.544 Prec@1=42.878 Prec@5=69.038 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=05:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> training   0.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.184 DataTime=4.945 Loss=2.392 Prec@1=49.219 Prec@5=69.531 rate=0 Hz, eta=?, total=0:00:00, wall=05:39 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.184 DataTime=4.945 Loss=2.392 Prec@1=49.219 Prec@5=69.531 rate=6354.53 Hz, eta=0:00:00, total=0:00:00, wall=05:39 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=5.184 DataTime=4.945 Loss=2.392 Prec@1=49.219 Prec@5=69.531 rate=6354.53 Hz, eta=0:00:00, total=0:00:00, wall=05:40 IST=> training   0.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.365 DataTime=0.259 Loss=2.288 Prec@1=47.676 Prec@5=73.242 rate=6354.53 Hz, eta=0:00:00, total=0:00:00, wall=05:40 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.365 DataTime=0.259 Loss=2.288 Prec@1=47.676 Prec@5=73.242 rate=3.19 Hz, eta=0:12:33, total=0:00:31, wall=05:40 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.365 DataTime=0.259 Loss=2.288 Prec@1=47.676 Prec@5=73.242 rate=3.19 Hz, eta=0:12:33, total=0:00:31, wall=05:40 IST=> training   4.04% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.352 DataTime=0.249 Loss=2.294 Prec@1=47.670 Prec@5=73.194 rate=3.19 Hz, eta=0:12:33, total=0:00:31, wall=05:40 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.352 DataTime=0.249 Loss=2.294 Prec@1=47.670 Prec@5=73.194 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=05:40 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.352 DataTime=0.249 Loss=2.294 Prec@1=47.670 Prec@5=73.194 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=05:41 IST=> training   8.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.346 DataTime=0.243 Loss=2.295 Prec@1=47.685 Prec@5=73.097 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=05:41 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.346 DataTime=0.243 Loss=2.295 Prec@1=47.685 Prec@5=73.097 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=05:41 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.346 DataTime=0.243 Loss=2.295 Prec@1=47.685 Prec@5=73.097 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=05:41 IST=> training   12.03% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.345 DataTime=0.242 Loss=2.297 Prec@1=47.659 Prec@5=73.053 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=05:41 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.345 DataTime=0.242 Loss=2.297 Prec@1=47.659 Prec@5=73.053 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=05:41 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.345 DataTime=0.242 Loss=2.297 Prec@1=47.659 Prec@5=73.053 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=05:42 IST=> training   16.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.343 DataTime=0.241 Loss=2.298 Prec@1=47.644 Prec@5=73.068 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=05:42 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.343 DataTime=0.241 Loss=2.298 Prec@1=47.644 Prec@5=73.068 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=05:42 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.343 DataTime=0.241 Loss=2.298 Prec@1=47.644 Prec@5=73.068 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=05:42 IST=> training   20.02% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.236 Loss=2.298 Prec@1=47.698 Prec@5=73.063 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=05:42 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.236 Loss=2.298 Prec@1=47.698 Prec@5=73.063 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=05:42 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.236 Loss=2.298 Prec@1=47.698 Prec@5=73.063 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=05:43 IST=> training   24.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.300 Prec@1=47.680 Prec@5=73.053 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=05:43 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.300 Prec@1=47.680 Prec@5=73.053 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=05:43 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.300 Prec@1=47.680 Prec@5=73.053 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=05:44 IST=> training   28.01% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.303 Prec@1=47.619 Prec@5=73.020 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=05:44 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.303 Prec@1=47.619 Prec@5=73.020 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=05:44 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.339 DataTime=0.237 Loss=2.303 Prec@1=47.619 Prec@5=73.020 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=05:44 IST=> training   32.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.237 Loss=2.304 Prec@1=47.629 Prec@5=72.986 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=05:44 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.237 Loss=2.304 Prec@1=47.629 Prec@5=72.986 rate=3.01 Hz, eta=0:08:53, total=0:04:59, wall=05:44 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.237 Loss=2.304 Prec@1=47.629 Prec@5=72.986 rate=3.01 Hz, eta=0:08:53, total=0:04:59, wall=05:45 IST=> training   36.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.305 Prec@1=47.611 Prec@5=72.963 rate=3.01 Hz, eta=0:08:53, total=0:04:59, wall=05:45 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.305 Prec@1=47.611 Prec@5=72.963 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=05:45 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.305 Prec@1=47.611 Prec@5=72.963 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=05:45 IST=> training   39.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.308 Prec@1=47.585 Prec@5=72.927 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=05:45 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.308 Prec@1=47.585 Prec@5=72.927 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=05:45 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.308 Prec@1=47.585 Prec@5=72.927 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=05:46 IST=> training   43.99% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.309 Prec@1=47.558 Prec@5=72.900 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=05:46 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.309 Prec@1=47.558 Prec@5=72.900 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=05:46 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.338 DataTime=0.236 Loss=2.309 Prec@1=47.558 Prec@5=72.900 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=05:46 IST=> training   47.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.310 Prec@1=47.528 Prec@5=72.878 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=05:46 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.310 Prec@1=47.528 Prec@5=72.878 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=05:46 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.310 Prec@1=47.528 Prec@5=72.878 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=05:47 IST=> training   51.98% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.313 Prec@1=47.491 Prec@5=72.812 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=05:47 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.313 Prec@1=47.491 Prec@5=72.812 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=05:47 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.313 Prec@1=47.491 Prec@5=72.812 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=05:48 IST=> training   55.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.315 Prec@1=47.451 Prec@5=72.801 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=05:48 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.315 Prec@1=47.451 Prec@5=72.801 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=05:48 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.315 Prec@1=47.451 Prec@5=72.801 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=05:48 IST=> training   59.97% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.316 Prec@1=47.431 Prec@5=72.803 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=05:48 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.316 Prec@1=47.431 Prec@5=72.803 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=05:48 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.235 Loss=2.316 Prec@1=47.431 Prec@5=72.803 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=05:49 IST=> training   63.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.316 Prec@1=47.421 Prec@5=72.789 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=05:49 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.316 Prec@1=47.421 Prec@5=72.789 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:49 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.234 Loss=2.316 Prec@1=47.421 Prec@5=72.789 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:49 IST=> training   67.96% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.316 Prec@1=47.409 Prec@5=72.784 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=05:49 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.316 Prec@1=47.409 Prec@5=72.784 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=05:49 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.316 Prec@1=47.409 Prec@5=72.784 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=05:50 IST=> training   71.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.317 Prec@1=47.416 Prec@5=72.776 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=05:50 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.317 Prec@1=47.416 Prec@5=72.776 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:50 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.317 Prec@1=47.416 Prec@5=72.776 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:50 IST=> training   75.95% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.317 Prec@1=47.412 Prec@5=72.779 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=05:50 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.317 Prec@1=47.412 Prec@5=72.779 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=05:50 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.317 Prec@1=47.412 Prec@5=72.779 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=05:51 IST=> training   79.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.318 Prec@1=47.409 Prec@5=72.773 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=05:51 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.318 Prec@1=47.409 Prec@5=72.773 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=05:51 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.234 Loss=2.318 Prec@1=47.409 Prec@5=72.773 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=05:51 IST=> training   83.94% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.318 Prec@1=47.403 Prec@5=72.775 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=05:51 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.318 Prec@1=47.403 Prec@5=72.775 rate=2.99 Hz, eta=0:01:40, total=0:12:16, wall=05:51 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.318 Prec@1=47.403 Prec@5=72.775 rate=2.99 Hz, eta=0:01:40, total=0:12:16, wall=05:52 IST=> training   87.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.318 Prec@1=47.414 Prec@5=72.761 rate=2.99 Hz, eta=0:01:40, total=0:12:16, wall=05:52 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.318 Prec@1=47.414 Prec@5=72.761 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=05:52 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.233 Loss=2.318 Prec@1=47.414 Prec@5=72.761 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=05:53 IST=> training   91.93% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.320 Prec@1=47.379 Prec@5=72.744 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=05:53 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.320 Prec@1=47.379 Prec@5=72.744 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=05:53 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.337 DataTime=0.233 Loss=2.320 Prec@1=47.379 Prec@5=72.744 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=05:53 IST=> training   95.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.232 Loss=2.320 Prec@1=47.371 Prec@5=72.739 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=05:53 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.232 Loss=2.320 Prec@1=47.371 Prec@5=72.739 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=05:53 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.232 Loss=2.320 Prec@1=47.371 Prec@5=72.739 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=05:53 IST=> training   99.92% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.232 Loss=2.320 Prec@1=47.371 Prec@5=72.739 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=05:53 IST=> training   100.00% of 1x2503...Epoch=27/150 LR=0.09277 Time=0.336 DataTime=0.232 Loss=2.320 Prec@1=47.371 Prec@5=72.739 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=05:53 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST=> validation 0.00% of 1x98...Epoch=27/150 LR=0.09277 Time=6.204 Loss=2.623 Prec@1=41.016 Prec@5=67.383 rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST=> validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=6.204 Loss=2.623 Prec@1=41.016 Prec@5=67.383 rate=4021.28 Hz, eta=0:00:00, total=0:00:00, wall=05:53 IST** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=6.204 Loss=2.623 Prec@1=41.016 Prec@5=67.383 rate=4021.28 Hz, eta=0:00:00, total=0:00:00, wall=05:54 IST** validation 1.02% of 1x98...Epoch=27/150 LR=0.09277 Time=0.398 Loss=2.606 Prec@1=42.112 Prec@5=68.216 rate=4021.28 Hz, eta=0:00:00, total=0:00:00, wall=05:54 IST** validation 100.00% of 1x98...Epoch=27/150 LR=0.09277 Time=0.398 Loss=2.606 Prec@1=42.112 Prec@5=68.216 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=05:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST=> training   0.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.272 DataTime=5.117 Loss=2.320 Prec@1=45.312 Prec@5=72.266 rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.272 DataTime=5.117 Loss=2.320 Prec@1=45.312 Prec@5=72.266 rate=1657.29 Hz, eta=0:00:01, total=0:00:00, wall=05:54 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=5.272 DataTime=5.117 Loss=2.320 Prec@1=45.312 Prec@5=72.266 rate=1657.29 Hz, eta=0:00:01, total=0:00:00, wall=05:54 IST=> training   0.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.378 DataTime=0.287 Loss=2.278 Prec@1=47.948 Prec@5=73.515 rate=1657.29 Hz, eta=0:00:01, total=0:00:00, wall=05:54 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.378 DataTime=0.287 Loss=2.278 Prec@1=47.948 Prec@5=73.515 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=05:54 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.378 DataTime=0.287 Loss=2.278 Prec@1=47.948 Prec@5=73.515 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=05:55 IST=> training   4.04% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.356 DataTime=0.260 Loss=2.288 Prec@1=47.864 Prec@5=73.319 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=05:55 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.356 DataTime=0.260 Loss=2.288 Prec@1=47.864 Prec@5=73.319 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=05:55 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.356 DataTime=0.260 Loss=2.288 Prec@1=47.864 Prec@5=73.319 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=05:56 IST=> training   8.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.352 DataTime=0.253 Loss=2.286 Prec@1=47.958 Prec@5=73.369 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=05:56 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.352 DataTime=0.253 Loss=2.286 Prec@1=47.958 Prec@5=73.369 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=05:56 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.352 DataTime=0.253 Loss=2.286 Prec@1=47.958 Prec@5=73.369 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=05:56 IST=> training   12.03% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.351 DataTime=0.250 Loss=2.290 Prec@1=47.903 Prec@5=73.268 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=05:56 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.351 DataTime=0.250 Loss=2.290 Prec@1=47.903 Prec@5=73.268 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=05:56 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.351 DataTime=0.250 Loss=2.290 Prec@1=47.903 Prec@5=73.268 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=05:57 IST=> training   16.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.347 DataTime=0.243 Loss=2.288 Prec@1=47.947 Prec@5=73.287 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=05:57 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.347 DataTime=0.243 Loss=2.288 Prec@1=47.947 Prec@5=73.287 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:57 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.347 DataTime=0.243 Loss=2.288 Prec@1=47.947 Prec@5=73.287 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:57 IST=> training   20.02% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.345 DataTime=0.241 Loss=2.292 Prec@1=47.884 Prec@5=73.241 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:57 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.345 DataTime=0.241 Loss=2.292 Prec@1=47.884 Prec@5=73.241 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:57 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.345 DataTime=0.241 Loss=2.292 Prec@1=47.884 Prec@5=73.241 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:58 IST=> training   24.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.291 Prec@1=47.911 Prec@5=73.242 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=05:58 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.291 Prec@1=47.911 Prec@5=73.242 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=05:58 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.291 Prec@1=47.911 Prec@5=73.242 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=05:58 IST=> training   28.01% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.293 Prec@1=47.867 Prec@5=73.206 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=05:58 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.293 Prec@1=47.867 Prec@5=73.206 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=05:58 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.240 Loss=2.293 Prec@1=47.867 Prec@5=73.206 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=05:59 IST=> training   32.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.293 Prec@1=47.833 Prec@5=73.213 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=05:59 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.293 Prec@1=47.833 Prec@5=73.213 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:59 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.293 Prec@1=47.833 Prec@5=73.213 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:59 IST=> training   36.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.294 Prec@1=47.819 Prec@5=73.215 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=05:59 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.294 Prec@1=47.819 Prec@5=73.215 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:59 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.294 Prec@1=47.819 Prec@5=73.215 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=06:00 IST=> training   39.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.295 Prec@1=47.798 Prec@5=73.189 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=06:00 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.295 Prec@1=47.798 Prec@5=73.189 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:00 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.295 Prec@1=47.798 Prec@5=73.189 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:01 IST=> training   43.99% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.296 Prec@1=47.777 Prec@5=73.160 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:01 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.296 Prec@1=47.777 Prec@5=73.160 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:01 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.237 Loss=2.296 Prec@1=47.777 Prec@5=73.160 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:01 IST=> training   47.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.297 Prec@1=47.790 Prec@5=73.146 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:01 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.297 Prec@1=47.790 Prec@5=73.146 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=06:01 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.237 Loss=2.297 Prec@1=47.790 Prec@5=73.146 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=06:02 IST=> training   51.98% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.781 Prec@5=73.137 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=06:02 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.781 Prec@5=73.137 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:02 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.781 Prec@5=73.137 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:02 IST=> training   55.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.787 Prec@5=73.127 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:02 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.787 Prec@5=73.127 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=06:02 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.236 Loss=2.297 Prec@1=47.787 Prec@5=73.127 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=06:03 IST=> training   59.97% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.236 Loss=2.298 Prec@1=47.778 Prec@5=73.098 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=06:03 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.236 Loss=2.298 Prec@1=47.778 Prec@5=73.098 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=06:03 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.236 Loss=2.298 Prec@1=47.778 Prec@5=73.098 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=06:03 IST=> training   63.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.299 Prec@1=47.778 Prec@5=73.081 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=06:03 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.299 Prec@1=47.778 Prec@5=73.081 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=06:03 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.299 Prec@1=47.778 Prec@5=73.081 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=06:04 IST=> training   67.96% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.301 Prec@1=47.727 Prec@5=73.046 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=06:04 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.301 Prec@1=47.727 Prec@5=73.046 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=06:04 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.236 Loss=2.301 Prec@1=47.727 Prec@5=73.046 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=06:05 IST=> training   71.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.236 Loss=2.302 Prec@1=47.722 Prec@5=73.040 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=06:05 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.236 Loss=2.302 Prec@1=47.722 Prec@5=73.040 rate=2.93 Hz, eta=0:03:25, total=0:10:47, wall=06:05 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.344 DataTime=0.236 Loss=2.302 Prec@1=47.722 Prec@5=73.040 rate=2.93 Hz, eta=0:03:25, total=0:10:47, wall=06:05 IST=> training   75.95% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.302 Prec@1=47.707 Prec@5=73.041 rate=2.93 Hz, eta=0:03:25, total=0:10:47, wall=06:05 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.302 Prec@1=47.707 Prec@5=73.041 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=06:05 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.302 Prec@1=47.707 Prec@5=73.041 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=06:06 IST=> training   79.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.303 Prec@1=47.694 Prec@5=73.019 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=06:06 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.303 Prec@1=47.694 Prec@5=73.019 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=06:06 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.303 Prec@1=47.694 Prec@5=73.019 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=06:06 IST=> training   83.94% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.304 Prec@1=47.666 Prec@5=73.012 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=06:06 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.304 Prec@1=47.666 Prec@5=73.012 rate=2.94 Hz, eta=0:01:42, total=0:12:29, wall=06:06 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.304 Prec@1=47.666 Prec@5=73.012 rate=2.94 Hz, eta=0:01:42, total=0:12:29, wall=06:07 IST=> training   87.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.305 Prec@1=47.645 Prec@5=72.995 rate=2.94 Hz, eta=0:01:42, total=0:12:29, wall=06:07 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.305 Prec@1=47.645 Prec@5=72.995 rate=2.94 Hz, eta=0:01:08, total=0:13:02, wall=06:07 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.343 DataTime=0.235 Loss=2.305 Prec@1=47.645 Prec@5=72.995 rate=2.94 Hz, eta=0:01:08, total=0:13:02, wall=06:07 IST=> training   91.93% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.235 Loss=2.306 Prec@1=47.626 Prec@5=72.966 rate=2.94 Hz, eta=0:01:08, total=0:13:02, wall=06:07 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.235 Loss=2.306 Prec@1=47.626 Prec@5=72.966 rate=2.94 Hz, eta=0:00:34, total=0:13:36, wall=06:07 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.342 DataTime=0.235 Loss=2.306 Prec@1=47.626 Prec@5=72.966 rate=2.94 Hz, eta=0:00:34, total=0:13:36, wall=06:08 IST=> training   95.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.234 Loss=2.307 Prec@1=47.617 Prec@5=72.952 rate=2.94 Hz, eta=0:00:34, total=0:13:36, wall=06:08 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.234 Loss=2.307 Prec@1=47.617 Prec@5=72.952 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=06:08 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.234 Loss=2.307 Prec@1=47.617 Prec@5=72.952 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=06:08 IST=> training   99.92% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.234 Loss=2.307 Prec@1=47.617 Prec@5=72.952 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=06:08 IST=> training   100.00% of 1x2503...Epoch=28/150 LR=0.09222 Time=0.341 DataTime=0.234 Loss=2.307 Prec@1=47.617 Prec@5=72.952 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=06:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:08 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:08 IST=> validation 0.00% of 1x98...Epoch=28/150 LR=0.09222 Time=5.843 Loss=2.759 Prec@1=39.453 Prec@5=65.430 rate=0 Hz, eta=?, total=0:00:00, wall=06:08 IST=> validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=5.843 Loss=2.759 Prec@1=39.453 Prec@5=65.430 rate=3961.79 Hz, eta=0:00:00, total=0:00:00, wall=06:08 IST** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=5.843 Loss=2.759 Prec@1=39.453 Prec@5=65.430 rate=3961.79 Hz, eta=0:00:00, total=0:00:00, wall=06:09 IST** validation 1.02% of 1x98...Epoch=28/150 LR=0.09222 Time=0.395 Loss=2.603 Prec@1=42.410 Prec@5=68.038 rate=3961.79 Hz, eta=0:00:00, total=0:00:00, wall=06:09 IST** validation 100.00% of 1x98...Epoch=28/150 LR=0.09222 Time=0.395 Loss=2.603 Prec@1=42.410 Prec@5=68.038 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=06:09 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:09 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:09 IST=> training   0.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.032 DataTime=4.860 Loss=2.177 Prec@1=48.633 Prec@5=74.805 rate=0 Hz, eta=?, total=0:00:00, wall=06:09 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.032 DataTime=4.860 Loss=2.177 Prec@1=48.633 Prec@5=74.805 rate=2640.28 Hz, eta=0:00:00, total=0:00:00, wall=06:09 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=5.032 DataTime=4.860 Loss=2.177 Prec@1=48.633 Prec@5=74.805 rate=2640.28 Hz, eta=0:00:00, total=0:00:00, wall=06:09 IST=> training   0.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.389 DataTime=0.277 Loss=2.253 Prec@1=48.343 Prec@5=73.820 rate=2640.28 Hz, eta=0:00:00, total=0:00:00, wall=06:09 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.389 DataTime=0.277 Loss=2.253 Prec@1=48.343 Prec@5=73.820 rate=2.95 Hz, eta=0:13:33, total=0:00:34, wall=06:09 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.389 DataTime=0.277 Loss=2.253 Prec@1=48.343 Prec@5=73.820 rate=2.95 Hz, eta=0:13:33, total=0:00:34, wall=06:10 IST=> training   4.04% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.364 DataTime=0.253 Loss=2.266 Prec@1=48.126 Prec@5=73.685 rate=2.95 Hz, eta=0:13:33, total=0:00:34, wall=06:10 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.364 DataTime=0.253 Loss=2.266 Prec@1=48.126 Prec@5=73.685 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=06:10 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.364 DataTime=0.253 Loss=2.266 Prec@1=48.126 Prec@5=73.685 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=06:10 IST=> training   8.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.350 DataTime=0.241 Loss=2.268 Prec@1=48.125 Prec@5=73.595 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=06:10 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.350 DataTime=0.241 Loss=2.268 Prec@1=48.125 Prec@5=73.595 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=06:10 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.350 DataTime=0.241 Loss=2.268 Prec@1=48.125 Prec@5=73.595 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=06:11 IST=> training   12.03% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.347 DataTime=0.240 Loss=2.271 Prec@1=48.129 Prec@5=73.532 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=06:11 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.347 DataTime=0.240 Loss=2.271 Prec@1=48.129 Prec@5=73.532 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:11 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.347 DataTime=0.240 Loss=2.271 Prec@1=48.129 Prec@5=73.532 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:12 IST=> training   16.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.345 DataTime=0.237 Loss=2.273 Prec@1=48.115 Prec@5=73.491 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:12 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.345 DataTime=0.237 Loss=2.273 Prec@1=48.115 Prec@5=73.491 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:12 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.345 DataTime=0.237 Loss=2.273 Prec@1=48.115 Prec@5=73.491 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:12 IST=> training   20.02% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.236 Loss=2.276 Prec@1=48.034 Prec@5=73.430 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:12 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.236 Loss=2.276 Prec@1=48.034 Prec@5=73.430 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:12 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.236 Loss=2.276 Prec@1=48.034 Prec@5=73.430 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:13 IST=> training   24.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.344 DataTime=0.237 Loss=2.278 Prec@1=47.982 Prec@5=73.438 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:13 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.344 DataTime=0.237 Loss=2.278 Prec@1=47.982 Prec@5=73.438 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=06:13 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.344 DataTime=0.237 Loss=2.278 Prec@1=47.982 Prec@5=73.438 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=06:13 IST=> training   28.01% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.281 Prec@1=47.958 Prec@5=73.385 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=06:13 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.281 Prec@1=47.958 Prec@5=73.385 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=06:13 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.281 Prec@1=47.958 Prec@5=73.385 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=06:14 IST=> training   32.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.282 Prec@1=47.973 Prec@5=73.356 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=06:14 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.282 Prec@1=47.973 Prec@5=73.356 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=06:14 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.282 Prec@1=47.973 Prec@5=73.356 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=06:14 IST=> training   36.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.284 Prec@1=47.949 Prec@5=73.322 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=06:14 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.284 Prec@1=47.949 Prec@5=73.322 rate=2.96 Hz, eta=0:08:27, total=0:05:37, wall=06:14 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.343 DataTime=0.235 Loss=2.284 Prec@1=47.949 Prec@5=73.322 rate=2.96 Hz, eta=0:08:27, total=0:05:37, wall=06:15 IST=> training   39.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.342 DataTime=0.234 Loss=2.284 Prec@1=47.930 Prec@5=73.322 rate=2.96 Hz, eta=0:08:27, total=0:05:37, wall=06:15 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.342 DataTime=0.234 Loss=2.284 Prec@1=47.930 Prec@5=73.322 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=06:15 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.342 DataTime=0.234 Loss=2.284 Prec@1=47.930 Prec@5=73.322 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=06:15 IST=> training   43.99% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.285 Prec@1=47.915 Prec@5=73.303 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=06:15 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.285 Prec@1=47.915 Prec@5=73.303 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:15 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.285 Prec@1=47.915 Prec@5=73.303 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:16 IST=> training   47.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.287 Prec@1=47.886 Prec@5=73.285 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:16 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.287 Prec@1=47.886 Prec@5=73.285 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=06:16 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.341 DataTime=0.234 Loss=2.287 Prec@1=47.886 Prec@5=73.285 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=06:17 IST=> training   51.98% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.233 Loss=2.287 Prec@1=47.869 Prec@5=73.271 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=06:17 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.233 Loss=2.287 Prec@1=47.869 Prec@5=73.271 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:17 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.233 Loss=2.287 Prec@1=47.869 Prec@5=73.271 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:17 IST=> training   55.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.288 Prec@1=47.852 Prec@5=73.259 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:17 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.288 Prec@1=47.852 Prec@5=73.259 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:17 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.288 Prec@1=47.852 Prec@5=73.259 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:18 IST=> training   59.97% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.289 Prec@1=47.858 Prec@5=73.247 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:18 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.289 Prec@1=47.858 Prec@5=73.247 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=06:18 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.289 Prec@1=47.858 Prec@5=73.247 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=06:18 IST=> training   63.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.290 Prec@1=47.838 Prec@5=73.222 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=06:18 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.290 Prec@1=47.838 Prec@5=73.222 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:18 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.290 Prec@1=47.838 Prec@5=73.222 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:19 IST=> training   67.96% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.290 Prec@1=47.848 Prec@5=73.208 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:19 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.290 Prec@1=47.848 Prec@5=73.208 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:19 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.290 Prec@1=47.848 Prec@5=73.208 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:19 IST=> training   71.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.291 Prec@1=47.842 Prec@5=73.209 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:19 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.291 Prec@1=47.842 Prec@5=73.209 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:19 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.340 DataTime=0.232 Loss=2.291 Prec@1=47.842 Prec@5=73.209 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:20 IST=> training   75.95% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.292 Prec@1=47.821 Prec@5=73.188 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:20 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.292 Prec@1=47.821 Prec@5=73.188 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=06:20 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.292 Prec@1=47.821 Prec@5=73.188 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=06:21 IST=> training   79.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.292 Prec@1=47.827 Prec@5=73.176 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=06:21 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.292 Prec@1=47.827 Prec@5=73.176 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=06:21 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.292 Prec@1=47.827 Prec@5=73.176 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=06:21 IST=> training   83.94% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.293 Prec@1=47.829 Prec@5=73.171 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=06:21 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.293 Prec@1=47.829 Prec@5=73.171 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:21 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.339 DataTime=0.232 Loss=2.293 Prec@1=47.829 Prec@5=73.171 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:22 IST=> training   87.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.232 Loss=2.293 Prec@1=47.821 Prec@5=73.156 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:22 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.232 Loss=2.293 Prec@1=47.821 Prec@5=73.156 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=06:22 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.232 Loss=2.293 Prec@1=47.821 Prec@5=73.156 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=06:22 IST=> training   91.93% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.294 Prec@1=47.805 Prec@5=73.138 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=06:22 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.294 Prec@1=47.805 Prec@5=73.138 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=06:22 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.294 Prec@1=47.805 Prec@5=73.138 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=06:23 IST=> training   95.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.295 Prec@1=47.797 Prec@5=73.125 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=06:23 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.295 Prec@1=47.797 Prec@5=73.125 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:23 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.295 Prec@1=47.797 Prec@5=73.125 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:23 IST=> training   99.92% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.295 Prec@1=47.796 Prec@5=73.125 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:23 IST=> training   100.00% of 1x2503...Epoch=29/150 LR=0.09165 Time=0.338 DataTime=0.231 Loss=2.295 Prec@1=47.796 Prec@5=73.125 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:23 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST=> validation 0.00% of 1x98...Epoch=29/150 LR=0.09165 Time=7.538 Loss=3.021 Prec@1=34.375 Prec@5=61.523 rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST=> validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=7.538 Loss=3.021 Prec@1=34.375 Prec@5=61.523 rate=6055.21 Hz, eta=0:00:00, total=0:00:00, wall=06:23 IST** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=7.538 Loss=3.021 Prec@1=34.375 Prec@5=61.523 rate=6055.21 Hz, eta=0:00:00, total=0:00:00, wall=06:23 IST** validation 1.02% of 1x98...Epoch=29/150 LR=0.09165 Time=0.399 Loss=3.008 Prec@1=35.704 Prec@5=61.060 rate=6055.21 Hz, eta=0:00:00, total=0:00:00, wall=06:23 IST** validation 100.00% of 1x98...Epoch=29/150 LR=0.09165 Time=0.399 Loss=3.008 Prec@1=35.704 Prec@5=61.060 rate=3.10 Hz, eta=0:00:00, total=0:00:31, wall=06:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST=> training   0.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.798 DataTime=5.651 Loss=2.036 Prec@1=50.391 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=06:24 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.798 DataTime=5.651 Loss=2.036 Prec@1=50.391 Prec@5=77.344 rate=5087.30 Hz, eta=0:00:00, total=0:00:00, wall=06:24 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=5.798 DataTime=5.651 Loss=2.036 Prec@1=50.391 Prec@5=77.344 rate=5087.30 Hz, eta=0:00:00, total=0:00:00, wall=06:24 IST=> training   0.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.382 DataTime=0.289 Loss=2.248 Prec@1=48.443 Prec@5=73.847 rate=5087.30 Hz, eta=0:00:00, total=0:00:00, wall=06:24 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.382 DataTime=0.289 Loss=2.248 Prec@1=48.443 Prec@5=73.847 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=06:24 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.382 DataTime=0.289 Loss=2.248 Prec@1=48.443 Prec@5=73.847 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=06:25 IST=> training   4.04% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.352 DataTime=0.253 Loss=2.250 Prec@1=48.443 Prec@5=73.923 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=06:25 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.352 DataTime=0.253 Loss=2.250 Prec@1=48.443 Prec@5=73.923 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:25 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.352 DataTime=0.253 Loss=2.250 Prec@1=48.443 Prec@5=73.923 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:25 IST=> training   8.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.348 DataTime=0.248 Loss=2.256 Prec@1=48.388 Prec@5=73.872 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:25 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.348 DataTime=0.248 Loss=2.256 Prec@1=48.388 Prec@5=73.872 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=06:25 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.348 DataTime=0.248 Loss=2.256 Prec@1=48.388 Prec@5=73.872 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=06:26 IST=> training   12.03% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.344 DataTime=0.242 Loss=2.263 Prec@1=48.213 Prec@5=73.729 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=06:26 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.344 DataTime=0.242 Loss=2.263 Prec@1=48.213 Prec@5=73.729 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=06:26 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.344 DataTime=0.242 Loss=2.263 Prec@1=48.213 Prec@5=73.729 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=06:26 IST=> training   16.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.345 DataTime=0.244 Loss=2.263 Prec@1=48.227 Prec@5=73.715 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=06:26 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.345 DataTime=0.244 Loss=2.263 Prec@1=48.227 Prec@5=73.715 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=06:26 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.345 DataTime=0.244 Loss=2.263 Prec@1=48.227 Prec@5=73.715 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=06:27 IST=> training   20.02% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.266 Prec@1=48.223 Prec@5=73.626 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=06:27 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.266 Prec@1=48.223 Prec@5=73.626 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=06:27 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.266 Prec@1=48.223 Prec@5=73.626 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=06:27 IST=> training   24.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.268 Prec@1=48.227 Prec@5=73.599 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=06:27 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.268 Prec@1=48.227 Prec@5=73.599 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:27 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.268 Prec@1=48.227 Prec@5=73.599 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:28 IST=> training   28.01% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.273 Prec@1=48.153 Prec@5=73.553 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:28 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.273 Prec@1=48.153 Prec@5=73.553 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=06:28 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.343 DataTime=0.241 Loss=2.273 Prec@1=48.153 Prec@5=73.553 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=06:29 IST=> training   32.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.274 Prec@1=48.139 Prec@5=73.506 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=06:29 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.274 Prec@1=48.139 Prec@5=73.506 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:29 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.240 Loss=2.274 Prec@1=48.139 Prec@5=73.506 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:29 IST=> training   36.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.239 Loss=2.275 Prec@1=48.119 Prec@5=73.511 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:29 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.239 Loss=2.275 Prec@1=48.119 Prec@5=73.511 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:29 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.239 Loss=2.275 Prec@1=48.119 Prec@5=73.511 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:30 IST=> training   39.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.239 Loss=2.278 Prec@1=48.090 Prec@5=73.446 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:30 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.239 Loss=2.278 Prec@1=48.090 Prec@5=73.446 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:30 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.342 DataTime=0.239 Loss=2.278 Prec@1=48.090 Prec@5=73.446 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:30 IST=> training   43.99% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.278 Prec@1=48.083 Prec@5=73.452 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:30 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.278 Prec@1=48.083 Prec@5=73.452 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=06:30 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.278 Prec@1=48.083 Prec@5=73.452 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=06:31 IST=> training   47.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.280 Prec@1=48.052 Prec@5=73.421 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.280 Prec@1=48.052 Prec@5=73.421 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.280 Prec@1=48.052 Prec@5=73.421 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:31 IST=> training   51.98% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.281 Prec@1=48.033 Prec@5=73.392 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:31 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.281 Prec@1=48.033 Prec@5=73.392 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:31 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.341 DataTime=0.238 Loss=2.281 Prec@1=48.033 Prec@5=73.392 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:32 IST=> training   55.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.281 Prec@1=48.039 Prec@5=73.380 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=06:32 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.281 Prec@1=48.039 Prec@5=73.380 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=06:32 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.237 Loss=2.281 Prec@1=48.039 Prec@5=73.380 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=06:33 IST=> training   59.97% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.282 Prec@1=48.022 Prec@5=73.379 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=06:33 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.282 Prec@1=48.022 Prec@5=73.379 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:33 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.282 Prec@1=48.022 Prec@5=73.379 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:33 IST=> training   63.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.283 Prec@1=48.004 Prec@5=73.353 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:33 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.283 Prec@1=48.004 Prec@5=73.353 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:33 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.283 Prec@1=48.004 Prec@5=73.353 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:34 IST=> training   67.96% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.284 Prec@1=47.983 Prec@5=73.342 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=06:34 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.284 Prec@1=47.983 Prec@5=73.342 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:34 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.236 Loss=2.284 Prec@1=47.983 Prec@5=73.342 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:34 IST=> training   71.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.284 Prec@1=47.986 Prec@5=73.325 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:34 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.284 Prec@1=47.986 Prec@5=73.325 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=06:34 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.284 Prec@1=47.986 Prec@5=73.325 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=06:35 IST=> training   75.95% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.285 Prec@1=47.989 Prec@5=73.323 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=06:35 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.285 Prec@1=47.989 Prec@5=73.323 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:35 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.340 DataTime=0.235 Loss=2.285 Prec@1=47.989 Prec@5=73.323 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:35 IST=> training   79.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.284 Prec@1=47.997 Prec@5=73.326 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:35 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.284 Prec@1=47.997 Prec@5=73.326 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:35 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.284 Prec@1=47.997 Prec@5=73.326 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:36 IST=> training   83.94% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.285 Prec@1=47.994 Prec@5=73.315 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:36 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.285 Prec@1=47.994 Prec@5=73.315 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:36 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.285 Prec@1=47.994 Prec@5=73.315 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:36 IST=> training   87.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.286 Prec@1=47.985 Prec@5=73.303 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:36 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.286 Prec@1=47.985 Prec@5=73.303 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=06:36 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.235 Loss=2.286 Prec@1=47.985 Prec@5=73.303 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=06:37 IST=> training   91.93% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.286 Prec@1=47.985 Prec@5=73.292 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=06:37 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.286 Prec@1=47.985 Prec@5=73.292 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:37 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.339 DataTime=0.234 Loss=2.286 Prec@1=47.985 Prec@5=73.292 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:38 IST=> training   95.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.338 DataTime=0.234 Loss=2.287 Prec@1=47.968 Prec@5=73.274 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:38 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.338 DataTime=0.234 Loss=2.287 Prec@1=47.968 Prec@5=73.274 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:38 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.338 DataTime=0.234 Loss=2.287 Prec@1=47.968 Prec@5=73.274 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:38 IST=> training   99.92% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.338 DataTime=0.234 Loss=2.287 Prec@1=47.966 Prec@5=73.272 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=06:38 IST=> training   100.00% of 1x2503...Epoch=30/150 LR=0.09106 Time=0.338 DataTime=0.234 Loss=2.287 Prec@1=47.966 Prec@5=73.272 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=06:38 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> validation 0.00% of 1x98...Epoch=30/150 LR=0.09106 Time=6.655 Loss=2.555 Prec@1=42.188 Prec@5=70.117 rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.655 Loss=2.555 Prec@1=42.188 Prec@5=70.117 rate=6515.93 Hz, eta=0:00:00, total=0:00:00, wall=06:38 IST** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=6.655 Loss=2.555 Prec@1=42.188 Prec@5=70.117 rate=6515.93 Hz, eta=0:00:00, total=0:00:00, wall=06:38 IST** validation 1.02% of 1x98...Epoch=30/150 LR=0.09106 Time=0.403 Loss=2.499 Prec@1=43.850 Prec@5=69.940 rate=6515.93 Hz, eta=0:00:00, total=0:00:00, wall=06:38 IST** validation 100.00% of 1x98...Epoch=30/150 LR=0.09106 Time=0.403 Loss=2.499 Prec@1=43.850 Prec@5=69.940 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=06:38 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> training   0.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.742 DataTime=5.461 Loss=2.121 Prec@1=51.172 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=06:38 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.742 DataTime=5.461 Loss=2.121 Prec@1=51.172 Prec@5=76.367 rate=3747.34 Hz, eta=0:00:00, total=0:00:00, wall=06:38 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=5.742 DataTime=5.461 Loss=2.121 Prec@1=51.172 Prec@5=76.367 rate=3747.34 Hz, eta=0:00:00, total=0:00:00, wall=06:39 IST=> training   0.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.373 DataTime=0.271 Loss=2.243 Prec@1=48.822 Prec@5=73.950 rate=3747.34 Hz, eta=0:00:00, total=0:00:00, wall=06:39 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.373 DataTime=0.271 Loss=2.243 Prec@1=48.822 Prec@5=73.950 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=06:39 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.373 DataTime=0.271 Loss=2.243 Prec@1=48.822 Prec@5=73.950 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=06:39 IST=> training   4.04% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.352 DataTime=0.252 Loss=2.247 Prec@1=48.683 Prec@5=73.844 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=06:39 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.352 DataTime=0.252 Loss=2.247 Prec@1=48.683 Prec@5=73.844 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:39 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.352 DataTime=0.252 Loss=2.247 Prec@1=48.683 Prec@5=73.844 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:40 IST=> training   8.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.347 DataTime=0.246 Loss=2.253 Prec@1=48.557 Prec@5=73.798 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=06:40 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.347 DataTime=0.246 Loss=2.253 Prec@1=48.557 Prec@5=73.798 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=06:40 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.347 DataTime=0.246 Loss=2.253 Prec@1=48.557 Prec@5=73.798 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=06:41 IST=> training   12.03% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.244 Loss=2.253 Prec@1=48.588 Prec@5=73.797 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=06:41 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.244 Loss=2.253 Prec@1=48.588 Prec@5=73.797 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=06:41 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.244 Loss=2.253 Prec@1=48.588 Prec@5=73.797 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=06:41 IST=> training   16.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.243 Loss=2.258 Prec@1=48.502 Prec@5=73.692 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=06:41 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.243 Loss=2.258 Prec@1=48.502 Prec@5=73.692 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=06:41 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.345 DataTime=0.243 Loss=2.258 Prec@1=48.502 Prec@5=73.692 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=06:42 IST=> training   20.02% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.240 Loss=2.258 Prec@1=48.490 Prec@5=73.690 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=06:42 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.240 Loss=2.258 Prec@1=48.490 Prec@5=73.690 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:42 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.240 Loss=2.258 Prec@1=48.490 Prec@5=73.690 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:42 IST=> training   24.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.238 Loss=2.260 Prec@1=48.461 Prec@5=73.700 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:42 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.238 Loss=2.260 Prec@1=48.461 Prec@5=73.700 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=06:42 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.342 DataTime=0.238 Loss=2.260 Prec@1=48.461 Prec@5=73.700 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=06:43 IST=> training   28.01% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.261 Prec@1=48.417 Prec@5=73.669 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=06:43 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.261 Prec@1=48.417 Prec@5=73.669 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=06:43 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.261 Prec@1=48.417 Prec@5=73.669 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=06:43 IST=> training   32.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.407 Prec@5=73.659 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=06:43 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.407 Prec@5=73.659 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=06:43 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.407 Prec@5=73.659 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=06:44 IST=> training   36.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.419 Prec@5=73.658 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=06:44 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.419 Prec@5=73.658 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:44 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.341 DataTime=0.238 Loss=2.263 Prec@1=48.419 Prec@5=73.658 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:44 IST=> training   39.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.237 Loss=2.265 Prec@1=48.372 Prec@5=73.614 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:44 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.237 Loss=2.265 Prec@1=48.372 Prec@5=73.614 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=06:44 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.237 Loss=2.265 Prec@1=48.372 Prec@5=73.614 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=06:45 IST=> training   43.99% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.342 Prec@5=73.600 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=06:45 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.342 Prec@5=73.600 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=06:45 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.342 Prec@5=73.600 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=06:46 IST=> training   47.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.337 Prec@5=73.590 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=06:46 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.337 Prec@5=73.590 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=06:46 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.267 Prec@1=48.337 Prec@5=73.590 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=06:46 IST=> training   51.98% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.268 Prec@1=48.322 Prec@5=73.563 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=06:46 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.268 Prec@1=48.322 Prec@5=73.563 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=06:46 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.236 Loss=2.268 Prec@1=48.322 Prec@5=73.563 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=06:47 IST=> training   55.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.269 Prec@1=48.313 Prec@5=73.558 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=06:47 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.269 Prec@1=48.313 Prec@5=73.558 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:47 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.269 Prec@1=48.313 Prec@5=73.558 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:47 IST=> training   59.97% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.270 Prec@1=48.270 Prec@5=73.539 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:47 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.270 Prec@1=48.270 Prec@5=73.539 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:47 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.235 Loss=2.270 Prec@1=48.270 Prec@5=73.539 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:48 IST=> training   63.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.270 Prec@1=48.252 Prec@5=73.527 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:48 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.270 Prec@1=48.252 Prec@5=73.527 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=06:48 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.270 Prec@1=48.252 Prec@5=73.527 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=06:48 IST=> training   67.96% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.271 Prec@1=48.247 Prec@5=73.514 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=06:48 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.271 Prec@1=48.247 Prec@5=73.514 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:48 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.271 Prec@1=48.247 Prec@5=73.514 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:49 IST=> training   71.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.272 Prec@1=48.226 Prec@5=73.501 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:49 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.272 Prec@1=48.226 Prec@5=73.501 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:49 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.272 Prec@1=48.226 Prec@5=73.501 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:50 IST=> training   75.95% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.273 Prec@1=48.217 Prec@5=73.488 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:50 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.273 Prec@1=48.217 Prec@5=73.488 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:50 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.234 Loss=2.273 Prec@1=48.217 Prec@5=73.488 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:50 IST=> training   79.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.273 Prec@1=48.209 Prec@5=73.489 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:50 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.273 Prec@1=48.209 Prec@5=73.489 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:50 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.273 Prec@1=48.209 Prec@5=73.489 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:51 IST=> training   83.94% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.233 Loss=2.274 Prec@1=48.194 Prec@5=73.478 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:51 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.233 Loss=2.274 Prec@1=48.194 Prec@5=73.478 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=06:51 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.340 DataTime=0.233 Loss=2.274 Prec@1=48.194 Prec@5=73.478 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=06:51 IST=> training   87.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.274 Prec@1=48.190 Prec@5=73.471 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=06:51 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.274 Prec@1=48.190 Prec@5=73.471 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:51 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.233 Loss=2.274 Prec@1=48.190 Prec@5=73.471 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:52 IST=> training   91.93% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.177 Prec@5=73.462 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:52 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.177 Prec@5=73.462 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:52 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.177 Prec@5=73.462 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:52 IST=> training   95.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.173 Prec@5=73.448 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:52 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.173 Prec@5=73.448 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=06:52 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.173 Prec@5=73.448 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=06:52 IST=> training   99.92% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.174 Prec@5=73.447 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=06:52 IST=> training   100.00% of 1x2503...Epoch=31/150 LR=0.09045 Time=0.339 DataTime=0.232 Loss=2.275 Prec@1=48.174 Prec@5=73.447 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:52 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:52 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:52 IST=> validation 0.00% of 1x98...Epoch=31/150 LR=0.09045 Time=6.395 Loss=2.641 Prec@1=39.844 Prec@5=70.312 rate=0 Hz, eta=?, total=0:00:00, wall=06:52 IST=> validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=6.395 Loss=2.641 Prec@1=39.844 Prec@5=70.312 rate=1113.61 Hz, eta=0:00:00, total=0:00:00, wall=06:52 IST** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=6.395 Loss=2.641 Prec@1=39.844 Prec@5=70.312 rate=1113.61 Hz, eta=0:00:00, total=0:00:00, wall=06:53 IST** validation 1.02% of 1x98...Epoch=31/150 LR=0.09045 Time=0.404 Loss=2.610 Prec@1=42.040 Prec@5=67.960 rate=1113.61 Hz, eta=0:00:00, total=0:00:00, wall=06:53 IST** validation 100.00% of 1x98...Epoch=31/150 LR=0.09045 Time=0.404 Loss=2.610 Prec@1=42.040 Prec@5=67.960 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=06:53 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:53 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:53 IST=> training   0.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.703 DataTime=5.548 Loss=2.151 Prec@1=49.414 Prec@5=75.000 rate=0 Hz, eta=?, total=0:00:00, wall=06:53 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.703 DataTime=5.548 Loss=2.151 Prec@1=49.414 Prec@5=75.000 rate=7223.03 Hz, eta=0:00:00, total=0:00:00, wall=06:53 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=5.703 DataTime=5.548 Loss=2.151 Prec@1=49.414 Prec@5=75.000 rate=7223.03 Hz, eta=0:00:00, total=0:00:00, wall=06:54 IST=> training   0.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.377 DataTime=0.281 Loss=2.240 Prec@1=48.726 Prec@5=74.155 rate=7223.03 Hz, eta=0:00:00, total=0:00:00, wall=06:54 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.377 DataTime=0.281 Loss=2.240 Prec@1=48.726 Prec@5=74.155 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=06:54 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.377 DataTime=0.281 Loss=2.240 Prec@1=48.726 Prec@5=74.155 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=06:54 IST=> training   4.04% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.353 DataTime=0.257 Loss=2.238 Prec@1=48.724 Prec@5=74.238 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=06:54 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.353 DataTime=0.257 Loss=2.238 Prec@1=48.724 Prec@5=74.238 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=06:54 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.353 DataTime=0.257 Loss=2.238 Prec@1=48.724 Prec@5=74.238 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=06:55 IST=> training   8.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.346 DataTime=0.246 Loss=2.244 Prec@1=48.680 Prec@5=74.146 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=06:55 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.346 DataTime=0.246 Loss=2.244 Prec@1=48.680 Prec@5=74.146 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=06:55 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.346 DataTime=0.246 Loss=2.244 Prec@1=48.680 Prec@5=74.146 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=06:55 IST=> training   12.03% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.242 Loss=2.247 Prec@1=48.653 Prec@5=74.044 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=06:55 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.242 Loss=2.247 Prec@1=48.653 Prec@5=74.044 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=06:55 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.242 Loss=2.247 Prec@1=48.653 Prec@5=74.044 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=06:56 IST=> training   16.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.242 Loss=2.244 Prec@1=48.697 Prec@5=74.036 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=06:56 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.242 Loss=2.244 Prec@1=48.697 Prec@5=74.036 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=06:56 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.242 Loss=2.244 Prec@1=48.697 Prec@5=74.036 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=06:56 IST=> training   20.02% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.241 Loss=2.246 Prec@1=48.650 Prec@5=73.989 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=06:56 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.241 Loss=2.246 Prec@1=48.650 Prec@5=73.989 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:56 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.241 Loss=2.246 Prec@1=48.650 Prec@5=73.989 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:57 IST=> training   24.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.239 Loss=2.247 Prec@1=48.649 Prec@5=73.951 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=06:57 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.239 Loss=2.247 Prec@1=48.649 Prec@5=73.951 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=06:57 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.239 Loss=2.247 Prec@1=48.649 Prec@5=73.951 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=06:58 IST=> training   28.01% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.240 Loss=2.249 Prec@1=48.616 Prec@5=73.913 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=06:58 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.240 Loss=2.249 Prec@1=48.616 Prec@5=73.913 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=06:58 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.343 DataTime=0.240 Loss=2.249 Prec@1=48.616 Prec@5=73.913 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=06:58 IST=> training   32.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.238 Loss=2.250 Prec@1=48.609 Prec@5=73.892 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=06:58 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.238 Loss=2.250 Prec@1=48.609 Prec@5=73.892 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:58 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.238 Loss=2.250 Prec@1=48.609 Prec@5=73.892 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:59 IST=> training   36.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.238 Loss=2.253 Prec@1=48.560 Prec@5=73.845 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=06:59 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.238 Loss=2.253 Prec@1=48.560 Prec@5=73.845 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:59 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.238 Loss=2.253 Prec@1=48.560 Prec@5=73.845 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:59 IST=> training   39.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.255 Prec@1=48.530 Prec@5=73.809 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=06:59 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.255 Prec@1=48.530 Prec@5=73.809 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=06:59 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.255 Prec@1=48.530 Prec@5=73.809 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=07:00 IST=> training   43.99% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.256 Prec@1=48.491 Prec@5=73.787 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=07:00 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.256 Prec@1=48.491 Prec@5=73.787 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=07:00 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.256 Prec@1=48.491 Prec@5=73.787 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=07:00 IST=> training   47.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.257 Prec@1=48.487 Prec@5=73.775 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=07:00 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.257 Prec@1=48.487 Prec@5=73.775 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:00 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.236 Loss=2.257 Prec@1=48.487 Prec@5=73.775 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:01 IST=> training   51.98% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.258 Prec@1=48.486 Prec@5=73.768 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:01 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.258 Prec@1=48.486 Prec@5=73.768 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=07:01 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.342 DataTime=0.237 Loss=2.258 Prec@1=48.486 Prec@5=73.768 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=07:02 IST=> training   55.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.257 Prec@1=48.488 Prec@5=73.785 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=07:02 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.257 Prec@1=48.488 Prec@5=73.785 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:02 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.257 Prec@1=48.488 Prec@5=73.785 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:02 IST=> training   59.97% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.235 Loss=2.258 Prec@1=48.477 Prec@5=73.758 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:02 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.235 Loss=2.258 Prec@1=48.477 Prec@5=73.758 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:02 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.235 Loss=2.258 Prec@1=48.477 Prec@5=73.758 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:03 IST=> training   63.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.259 Prec@1=48.464 Prec@5=73.755 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:03 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.259 Prec@1=48.464 Prec@5=73.755 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=07:03 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.341 DataTime=0.235 Loss=2.259 Prec@1=48.464 Prec@5=73.755 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=07:03 IST=> training   67.96% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.234 Loss=2.260 Prec@1=48.445 Prec@5=73.745 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=07:03 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.234 Loss=2.260 Prec@1=48.445 Prec@5=73.745 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:03 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.340 DataTime=0.234 Loss=2.260 Prec@1=48.445 Prec@5=73.745 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:04 IST=> training   71.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.260 Prec@1=48.442 Prec@5=73.731 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:04 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.260 Prec@1=48.442 Prec@5=73.731 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:04 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.260 Prec@1=48.442 Prec@5=73.731 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:04 IST=> training   75.95% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.424 Prec@5=73.702 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:04 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.424 Prec@5=73.702 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=07:04 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.424 Prec@5=73.702 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=07:05 IST=> training   79.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.432 Prec@5=73.697 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=07:05 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.432 Prec@5=73.697 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:05 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.262 Prec@1=48.432 Prec@5=73.697 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:05 IST=> training   83.94% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.421 Prec@5=73.689 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:05 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.421 Prec@5=73.689 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:05 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.421 Prec@5=73.689 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:06 IST=> training   87.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.423 Prec@5=73.682 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:06 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.423 Prec@5=73.682 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:06 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.263 Prec@1=48.423 Prec@5=73.682 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:07 IST=> training   91.93% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.264 Prec@1=48.416 Prec@5=73.679 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:07 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.264 Prec@1=48.416 Prec@5=73.679 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:07 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.339 DataTime=0.233 Loss=2.264 Prec@1=48.416 Prec@5=73.679 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:07 IST=> training   95.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.338 DataTime=0.232 Loss=2.265 Prec@1=48.385 Prec@5=73.667 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:07 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.338 DataTime=0.232 Loss=2.265 Prec@1=48.385 Prec@5=73.667 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:07 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.338 DataTime=0.232 Loss=2.265 Prec@1=48.385 Prec@5=73.667 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:07 IST=> training   99.92% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.338 DataTime=0.232 Loss=2.265 Prec@1=48.384 Prec@5=73.666 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:07 IST=> training   100.00% of 1x2503...Epoch=32/150 LR=0.08983 Time=0.338 DataTime=0.232 Loss=2.265 Prec@1=48.384 Prec@5=73.666 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:07 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST=> validation 0.00% of 1x98...Epoch=32/150 LR=0.08983 Time=6.109 Loss=2.534 Prec@1=44.141 Prec@5=69.531 rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST=> validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=6.109 Loss=2.534 Prec@1=44.141 Prec@5=69.531 rate=6501.96 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=6.109 Loss=2.534 Prec@1=44.141 Prec@5=69.531 rate=6501.96 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST** validation 1.02% of 1x98...Epoch=32/150 LR=0.08983 Time=0.400 Loss=2.486 Prec@1=44.010 Prec@5=70.194 rate=6501.96 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST** validation 100.00% of 1x98...Epoch=32/150 LR=0.08983 Time=0.400 Loss=2.486 Prec@1=44.010 Prec@5=70.194 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=07:08 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> training   0.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.863 DataTime=5.724 Loss=2.181 Prec@1=48.047 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=07:08 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.863 DataTime=5.724 Loss=2.181 Prec@1=48.047 Prec@5=76.367 rate=9604.85 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=5.863 DataTime=5.724 Loss=2.181 Prec@1=48.047 Prec@5=76.367 rate=9604.85 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST=> training   0.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.382 DataTime=0.287 Loss=2.235 Prec@1=48.824 Prec@5=74.141 rate=9604.85 Hz, eta=0:00:00, total=0:00:00, wall=07:08 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.382 DataTime=0.287 Loss=2.235 Prec@1=48.824 Prec@5=74.141 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=07:08 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.382 DataTime=0.287 Loss=2.235 Prec@1=48.824 Prec@5=74.141 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=07:09 IST=> training   4.04% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.356 DataTime=0.257 Loss=2.226 Prec@1=48.989 Prec@5=74.242 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=07:09 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.356 DataTime=0.257 Loss=2.226 Prec@1=48.989 Prec@5=74.242 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=07:09 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.356 DataTime=0.257 Loss=2.226 Prec@1=48.989 Prec@5=74.242 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=07:10 IST=> training   8.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.353 DataTime=0.251 Loss=2.226 Prec@1=49.106 Prec@5=74.224 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=07:10 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.353 DataTime=0.251 Loss=2.226 Prec@1=49.106 Prec@5=74.224 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=07:10 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.353 DataTime=0.251 Loss=2.226 Prec@1=49.106 Prec@5=74.224 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=07:10 IST=> training   12.03% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.349 DataTime=0.244 Loss=2.231 Prec@1=48.996 Prec@5=74.102 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=07:10 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.349 DataTime=0.244 Loss=2.231 Prec@1=48.996 Prec@5=74.102 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=07:10 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.349 DataTime=0.244 Loss=2.231 Prec@1=48.996 Prec@5=74.102 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=07:11 IST=> training   16.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.233 Prec@1=48.948 Prec@5=74.072 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=07:11 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.233 Prec@1=48.948 Prec@5=74.072 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=07:11 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.233 Prec@1=48.948 Prec@5=74.072 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=07:11 IST=> training   20.02% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.234 Prec@1=48.933 Prec@5=74.077 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=07:11 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.234 Prec@1=48.933 Prec@5=74.077 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=07:11 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.347 DataTime=0.241 Loss=2.234 Prec@1=48.933 Prec@5=74.077 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=07:12 IST=> training   24.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.239 Loss=2.235 Prec@1=48.888 Prec@5=74.078 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=07:12 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.239 Loss=2.235 Prec@1=48.888 Prec@5=74.078 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=07:12 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.239 Loss=2.235 Prec@1=48.888 Prec@5=74.078 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=07:12 IST=> training   28.01% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.238 Loss=2.238 Prec@1=48.805 Prec@5=74.022 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=07:12 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.238 Loss=2.238 Prec@1=48.805 Prec@5=74.022 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=07:12 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.344 DataTime=0.238 Loss=2.238 Prec@1=48.805 Prec@5=74.022 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=07:13 IST=> training   32.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.237 Loss=2.240 Prec@1=48.747 Prec@5=73.991 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=07:13 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.237 Loss=2.240 Prec@1=48.747 Prec@5=73.991 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=07:13 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.237 Loss=2.240 Prec@1=48.747 Prec@5=73.991 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=07:14 IST=> training   36.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.236 Loss=2.241 Prec@1=48.750 Prec@5=73.971 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=07:14 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.236 Loss=2.241 Prec@1=48.750 Prec@5=73.971 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=07:14 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.343 DataTime=0.236 Loss=2.241 Prec@1=48.750 Prec@5=73.971 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=07:14 IST=> training   39.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.240 Prec@1=48.748 Prec@5=73.988 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=07:14 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.240 Prec@1=48.748 Prec@5=73.988 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=07:14 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.240 Prec@1=48.748 Prec@5=73.988 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=07:15 IST=> training   43.99% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.342 DataTime=0.235 Loss=2.241 Prec@1=48.747 Prec@5=73.972 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=07:15 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.342 DataTime=0.235 Loss=2.241 Prec@1=48.747 Prec@5=73.972 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=07:15 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.342 DataTime=0.235 Loss=2.241 Prec@1=48.747 Prec@5=73.972 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=07:15 IST=> training   47.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.242 Prec@1=48.746 Prec@5=73.966 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=07:15 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.242 Prec@1=48.746 Prec@5=73.966 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:15 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.242 Prec@1=48.746 Prec@5=73.966 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:16 IST=> training   51.98% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.243 Prec@1=48.733 Prec@5=73.956 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=07:16 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.243 Prec@1=48.733 Prec@5=73.956 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:16 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.243 Prec@1=48.733 Prec@5=73.956 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:16 IST=> training   55.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.245 Prec@1=48.718 Prec@5=73.913 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:16 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.245 Prec@1=48.718 Prec@5=73.913 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:16 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.245 Prec@1=48.718 Prec@5=73.913 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:17 IST=> training   59.97% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.247 Prec@1=48.696 Prec@5=73.889 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=07:17 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.247 Prec@1=48.696 Prec@5=73.889 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:17 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.341 DataTime=0.234 Loss=2.247 Prec@1=48.696 Prec@5=73.889 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:17 IST=> training   63.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.248 Prec@1=48.676 Prec@5=73.881 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=07:17 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.248 Prec@1=48.676 Prec@5=73.881 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:17 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.248 Prec@1=48.676 Prec@5=73.881 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:18 IST=> training   67.96% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.249 Prec@1=48.660 Prec@5=73.864 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:18 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.249 Prec@1=48.660 Prec@5=73.864 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:18 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.340 DataTime=0.233 Loss=2.249 Prec@1=48.660 Prec@5=73.864 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:19 IST=> training   71.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.250 Prec@1=48.657 Prec@5=73.859 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=07:19 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.250 Prec@1=48.657 Prec@5=73.859 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:19 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.250 Prec@1=48.657 Prec@5=73.859 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:19 IST=> training   75.95% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.638 Prec@5=73.850 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:19 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.638 Prec@5=73.850 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:19 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.638 Prec@5=73.850 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:20 IST=> training   79.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.634 Prec@5=73.854 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:20 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.634 Prec@5=73.854 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:20 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.251 Prec@1=48.634 Prec@5=73.854 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:20 IST=> training   83.94% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.252 Prec@1=48.625 Prec@5=73.836 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:20 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.252 Prec@1=48.625 Prec@5=73.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:20 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.252 Prec@1=48.625 Prec@5=73.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:21 IST=> training   87.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.253 Prec@1=48.617 Prec@5=73.832 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:21 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.253 Prec@1=48.617 Prec@5=73.832 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:21 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.253 Prec@1=48.617 Prec@5=73.832 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:21 IST=> training   91.93% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.254 Prec@1=48.606 Prec@5=73.814 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=07:21 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.254 Prec@1=48.606 Prec@5=73.814 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:21 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.339 DataTime=0.232 Loss=2.254 Prec@1=48.606 Prec@5=73.814 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:22 IST=> training   95.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.338 DataTime=0.231 Loss=2.254 Prec@1=48.598 Prec@5=73.796 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.338 DataTime=0.231 Loss=2.254 Prec@1=48.598 Prec@5=73.796 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.338 DataTime=0.231 Loss=2.254 Prec@1=48.598 Prec@5=73.796 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:22 IST=> training   99.92% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.338 DataTime=0.231 Loss=2.254 Prec@1=48.600 Prec@5=73.796 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:22 IST=> training   100.00% of 1x2503...Epoch=33/150 LR=0.08918 Time=0.338 DataTime=0.231 Loss=2.254 Prec@1=48.600 Prec@5=73.796 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 0.00% of 1x98...Epoch=33/150 LR=0.08918 Time=6.605 Loss=2.542 Prec@1=44.727 Prec@5=70.117 rate=0 Hz, eta=?, total=0:00:00, wall=07:22 IST=> validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=6.605 Loss=2.542 Prec@1=44.727 Prec@5=70.117 rate=6013.23 Hz, eta=0:00:00, total=0:00:00, wall=07:22 IST** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=6.605 Loss=2.542 Prec@1=44.727 Prec@5=70.117 rate=6013.23 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST** validation 1.02% of 1x98...Epoch=33/150 LR=0.08918 Time=0.398 Loss=2.480 Prec@1=44.294 Prec@5=70.176 rate=6013.23 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST** validation 100.00% of 1x98...Epoch=33/150 LR=0.08918 Time=0.398 Loss=2.480 Prec@1=44.294 Prec@5=70.176 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=07:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.013 DataTime=5.920 Loss=2.117 Prec@1=49.414 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=07:23 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.013 DataTime=5.920 Loss=2.117 Prec@1=49.414 Prec@5=76.953 rate=4850.65 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=6.013 DataTime=5.920 Loss=2.117 Prec@1=49.414 Prec@5=76.953 rate=4850.65 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST=> training   0.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.372 DataTime=0.274 Loss=2.215 Prec@1=49.298 Prec@5=74.381 rate=4850.65 Hz, eta=0:00:00, total=0:00:00, wall=07:23 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.372 DataTime=0.274 Loss=2.215 Prec@1=49.298 Prec@5=74.381 rate=3.20 Hz, eta=0:12:30, total=0:00:31, wall=07:23 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.372 DataTime=0.274 Loss=2.215 Prec@1=49.298 Prec@5=74.381 rate=3.20 Hz, eta=0:12:30, total=0:00:31, wall=07:24 IST=> training   4.04% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.348 DataTime=0.252 Loss=2.220 Prec@1=49.246 Prec@5=74.292 rate=3.20 Hz, eta=0:12:30, total=0:00:31, wall=07:24 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.348 DataTime=0.252 Loss=2.220 Prec@1=49.246 Prec@5=74.292 rate=3.14 Hz, eta=0:12:12, total=0:01:03, wall=07:24 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.348 DataTime=0.252 Loss=2.220 Prec@1=49.246 Prec@5=74.292 rate=3.14 Hz, eta=0:12:12, total=0:01:03, wall=07:24 IST=> training   8.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.346 DataTime=0.250 Loss=2.225 Prec@1=49.172 Prec@5=74.202 rate=3.14 Hz, eta=0:12:12, total=0:01:03, wall=07:24 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.346 DataTime=0.250 Loss=2.225 Prec@1=49.172 Prec@5=74.202 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=07:24 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.346 DataTime=0.250 Loss=2.225 Prec@1=49.172 Prec@5=74.202 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=07:25 IST=> training   12.03% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.347 DataTime=0.248 Loss=2.226 Prec@1=49.163 Prec@5=74.190 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=07:25 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.347 DataTime=0.248 Loss=2.226 Prec@1=49.163 Prec@5=74.190 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=07:25 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.347 DataTime=0.248 Loss=2.226 Prec@1=49.163 Prec@5=74.190 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=07:25 IST=> training   16.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.343 DataTime=0.241 Loss=2.227 Prec@1=49.142 Prec@5=74.168 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=07:25 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.343 DataTime=0.241 Loss=2.227 Prec@1=49.142 Prec@5=74.168 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=07:25 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.343 DataTime=0.241 Loss=2.227 Prec@1=49.142 Prec@5=74.168 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=07:26 IST=> training   20.02% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.238 Loss=2.230 Prec@1=49.068 Prec@5=74.156 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=07:26 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.238 Loss=2.230 Prec@1=49.068 Prec@5=74.156 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=07:26 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.238 Loss=2.230 Prec@1=49.068 Prec@5=74.156 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=07:27 IST=> training   24.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.342 DataTime=0.239 Loss=2.232 Prec@1=48.993 Prec@5=74.147 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=07:27 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.342 DataTime=0.239 Loss=2.232 Prec@1=48.993 Prec@5=74.147 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:27 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.342 DataTime=0.239 Loss=2.232 Prec@1=48.993 Prec@5=74.147 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:27 IST=> training   28.01% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.235 Loss=2.233 Prec@1=48.962 Prec@5=74.120 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:27 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.235 Loss=2.233 Prec@1=48.962 Prec@5=74.120 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=07:27 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.235 Loss=2.233 Prec@1=48.962 Prec@5=74.120 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=07:28 IST=> training   32.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.233 Prec@1=48.938 Prec@5=74.108 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=07:28 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.233 Prec@1=48.938 Prec@5=74.108 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=07:28 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.233 Prec@1=48.938 Prec@5=74.108 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=07:28 IST=> training   36.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.236 Loss=2.235 Prec@1=48.903 Prec@5=74.082 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=07:28 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.236 Loss=2.235 Prec@1=48.903 Prec@5=74.082 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=07:28 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.341 DataTime=0.236 Loss=2.235 Prec@1=48.903 Prec@5=74.082 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=07:29 IST=> training   39.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.237 Prec@1=48.876 Prec@5=74.042 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=07:29 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.237 Prec@1=48.876 Prec@5=74.042 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=07:29 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.237 Prec@1=48.876 Prec@5=74.042 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=07:29 IST=> training   43.99% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.234 Loss=2.237 Prec@1=48.862 Prec@5=74.049 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=07:29 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.234 Loss=2.237 Prec@1=48.862 Prec@5=74.049 rate=3.00 Hz, eta=0:07:13, total=0:06:40, wall=07:29 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.234 Loss=2.237 Prec@1=48.862 Prec@5=74.049 rate=3.00 Hz, eta=0:07:13, total=0:06:40, wall=07:30 IST=> training   47.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.235 Loss=2.238 Prec@1=48.863 Prec@5=74.044 rate=3.00 Hz, eta=0:07:13, total=0:06:40, wall=07:30 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.235 Loss=2.238 Prec@1=48.863 Prec@5=74.044 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:30 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.235 Loss=2.238 Prec@1=48.863 Prec@5=74.044 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:31 IST=> training   51.98% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.240 Prec@1=48.848 Prec@5=74.020 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:31 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.240 Prec@1=48.848 Prec@5=74.020 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=07:31 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.240 Prec@1=48.848 Prec@5=74.020 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=07:31 IST=> training   55.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.826 Prec@5=73.989 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=07:31 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.826 Prec@5=73.989 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=07:31 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.826 Prec@5=73.989 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=07:32 IST=> training   59.97% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.242 Prec@1=48.810 Prec@5=73.979 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=07:32 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.242 Prec@1=48.810 Prec@5=73.979 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=07:32 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.242 Prec@1=48.810 Prec@5=73.979 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=07:32 IST=> training   63.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.825 Prec@5=74.004 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=07:32 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.825 Prec@5=74.004 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=07:32 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.234 Loss=2.241 Prec@1=48.825 Prec@5=74.004 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=07:33 IST=> training   67.96% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.811 Prec@5=73.994 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=07:33 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.811 Prec@5=73.994 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=07:33 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.811 Prec@5=73.994 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=07:33 IST=> training   71.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.234 Loss=2.241 Prec@1=48.824 Prec@5=73.991 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=07:33 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.234 Loss=2.241 Prec@1=48.824 Prec@5=73.991 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:33 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.340 DataTime=0.234 Loss=2.241 Prec@1=48.824 Prec@5=73.991 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:34 IST=> training   75.95% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.819 Prec@5=73.982 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:34 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.819 Prec@5=73.982 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=07:34 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.242 Prec@1=48.819 Prec@5=73.982 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=07:34 IST=> training   79.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.243 Prec@1=48.796 Prec@5=73.971 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=07:34 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.243 Prec@1=48.796 Prec@5=73.971 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:34 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.243 Prec@1=48.796 Prec@5=73.971 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:35 IST=> training   83.94% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.244 Prec@1=48.776 Prec@5=73.954 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:35 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.244 Prec@1=48.776 Prec@5=73.954 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=07:35 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.244 Prec@1=48.776 Prec@5=73.954 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=07:36 IST=> training   87.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.245 Prec@1=48.748 Prec@5=73.935 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=07:36 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.245 Prec@1=48.748 Prec@5=73.935 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=07:36 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.339 DataTime=0.233 Loss=2.245 Prec@1=48.748 Prec@5=73.935 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=07:36 IST=> training   91.93% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.747 Prec@5=73.931 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=07:36 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.747 Prec@5=73.931 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:36 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.747 Prec@5=73.931 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:37 IST=> training   95.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.739 Prec@5=73.926 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:37 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.739 Prec@5=73.926 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=07:37 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.739 Prec@5=73.926 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=07:37 IST=> training   99.92% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.739 Prec@5=73.926 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=07:37 IST=> training   100.00% of 1x2503...Epoch=34/150 LR=0.08853 Time=0.338 DataTime=0.232 Loss=2.245 Prec@1=48.739 Prec@5=73.926 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=07:37 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> validation 0.00% of 1x98...Epoch=34/150 LR=0.08853 Time=5.988 Loss=2.469 Prec@1=46.094 Prec@5=69.141 rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=5.988 Loss=2.469 Prec@1=46.094 Prec@5=69.141 rate=6456.11 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=5.988 Loss=2.469 Prec@1=46.094 Prec@5=69.141 rate=6456.11 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST** validation 1.02% of 1x98...Epoch=34/150 LR=0.08853 Time=0.391 Loss=2.449 Prec@1=44.994 Prec@5=70.768 rate=6456.11 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST** validation 100.00% of 1x98...Epoch=34/150 LR=0.08853 Time=0.391 Loss=2.449 Prec@1=44.994 Prec@5=70.768 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=07:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.699 DataTime=4.584 Loss=2.372 Prec@1=46.289 Prec@5=73.633 rate=0 Hz, eta=?, total=0:00:00, wall=07:37 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.699 DataTime=4.584 Loss=2.372 Prec@1=46.289 Prec@5=73.633 rate=4378.96 Hz, eta=0:00:00, total=0:00:00, wall=07:37 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=4.699 DataTime=4.584 Loss=2.372 Prec@1=46.289 Prec@5=73.633 rate=4378.96 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST=> training   0.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.378 DataTime=0.282 Loss=2.221 Prec@1=49.022 Prec@5=74.515 rate=4378.96 Hz, eta=0:00:00, total=0:00:00, wall=07:38 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.378 DataTime=0.282 Loss=2.221 Prec@1=49.022 Prec@5=74.515 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=07:38 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.378 DataTime=0.282 Loss=2.221 Prec@1=49.022 Prec@5=74.515 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=07:39 IST=> training   4.04% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.355 DataTime=0.256 Loss=2.217 Prec@1=49.152 Prec@5=74.500 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.355 DataTime=0.256 Loss=2.217 Prec@1=49.152 Prec@5=74.500 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.355 DataTime=0.256 Loss=2.217 Prec@1=49.152 Prec@5=74.500 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:39 IST=> training   8.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.345 DataTime=0.246 Loss=2.212 Prec@1=49.245 Prec@5=74.523 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:39 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.345 DataTime=0.246 Loss=2.212 Prec@1=49.245 Prec@5=74.523 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:39 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.345 DataTime=0.246 Loss=2.212 Prec@1=49.245 Prec@5=74.523 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:40 IST=> training   12.03% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.346 DataTime=0.244 Loss=2.216 Prec@1=49.224 Prec@5=74.440 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:40 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.346 DataTime=0.244 Loss=2.216 Prec@1=49.224 Prec@5=74.440 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=07:40 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.346 DataTime=0.244 Loss=2.216 Prec@1=49.224 Prec@5=74.440 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=07:40 IST=> training   16.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.343 DataTime=0.241 Loss=2.219 Prec@1=49.180 Prec@5=74.407 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=07:40 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.343 DataTime=0.241 Loss=2.219 Prec@1=49.180 Prec@5=74.407 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=07:40 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.343 DataTime=0.241 Loss=2.219 Prec@1=49.180 Prec@5=74.407 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=07:41 IST=> training   20.02% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.221 Prec@1=49.169 Prec@5=74.381 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=07:41 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.221 Prec@1=49.169 Prec@5=74.381 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:41 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.221 Prec@1=49.169 Prec@5=74.381 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:41 IST=> training   24.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.223 Prec@1=49.110 Prec@5=74.322 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:41 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.223 Prec@1=49.110 Prec@5=74.322 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:41 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.237 Loss=2.223 Prec@1=49.110 Prec@5=74.322 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:42 IST=> training   28.01% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.238 Loss=2.227 Prec@1=49.022 Prec@5=74.242 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=07:42 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.238 Loss=2.227 Prec@1=49.022 Prec@5=74.242 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=07:42 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.238 Loss=2.227 Prec@1=49.022 Prec@5=74.242 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=07:42 IST=> training   32.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.235 Loss=2.226 Prec@1=49.003 Prec@5=74.248 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=07:42 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.235 Loss=2.226 Prec@1=49.003 Prec@5=74.248 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=07:42 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.235 Loss=2.226 Prec@1=49.003 Prec@5=74.248 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=07:43 IST=> training   36.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.236 Loss=2.227 Prec@1=49.018 Prec@5=74.229 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=07:43 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.236 Loss=2.227 Prec@1=49.018 Prec@5=74.229 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=07:43 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.340 DataTime=0.236 Loss=2.227 Prec@1=49.018 Prec@5=74.229 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=07:44 IST=> training   39.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.339 DataTime=0.234 Loss=2.229 Prec@1=49.003 Prec@5=74.186 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=07:44 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.339 DataTime=0.234 Loss=2.229 Prec@1=49.003 Prec@5=74.186 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=07:44 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.339 DataTime=0.234 Loss=2.229 Prec@1=49.003 Prec@5=74.186 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=07:44 IST=> training   43.99% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.234 Loss=2.230 Prec@1=48.997 Prec@5=74.163 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=07:44 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.234 Loss=2.230 Prec@1=48.997 Prec@5=74.163 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:44 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.234 Loss=2.230 Prec@1=48.997 Prec@5=74.163 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:45 IST=> training   47.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.233 Loss=2.232 Prec@1=49.010 Prec@5=74.149 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:45 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.233 Loss=2.232 Prec@1=49.010 Prec@5=74.149 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=07:45 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.338 DataTime=0.233 Loss=2.232 Prec@1=49.010 Prec@5=74.149 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=07:45 IST=> training   51.98% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.233 Loss=2.232 Prec@1=49.012 Prec@5=74.142 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=07:45 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.233 Loss=2.232 Prec@1=49.012 Prec@5=74.142 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=07:45 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.233 Loss=2.232 Prec@1=49.012 Prec@5=74.142 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=07:46 IST=> training   55.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.232 Loss=2.232 Prec@1=49.002 Prec@5=74.141 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=07:46 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.232 Loss=2.232 Prec@1=49.002 Prec@5=74.141 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=07:46 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.337 DataTime=0.232 Loss=2.232 Prec@1=49.002 Prec@5=74.141 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=07:46 IST=> training   59.97% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.985 Prec@5=74.126 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=07:46 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.985 Prec@5=74.126 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=07:46 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.985 Prec@5=74.126 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=07:47 IST=> training   63.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.981 Prec@5=74.129 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=07:47 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.981 Prec@5=74.129 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=07:47 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.233 Prec@1=48.981 Prec@5=74.129 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=07:47 IST=> training   67.96% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.234 Prec@1=48.970 Prec@5=74.120 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=07:47 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.234 Prec@1=48.970 Prec@5=74.120 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=07:47 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.234 Prec@1=48.970 Prec@5=74.120 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=07:48 IST=> training   71.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.234 Prec@1=48.947 Prec@5=74.111 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=07:48 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.234 Prec@1=48.947 Prec@5=74.111 rate=3.00 Hz, eta=0:03:20, total=0:10:32, wall=07:48 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.234 Prec@1=48.947 Prec@5=74.111 rate=3.00 Hz, eta=0:03:20, total=0:10:32, wall=07:49 IST=> training   75.95% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.235 Prec@1=48.929 Prec@5=74.107 rate=3.00 Hz, eta=0:03:20, total=0:10:32, wall=07:49 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.235 Prec@1=48.929 Prec@5=74.107 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=07:49 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.235 Prec@1=48.929 Prec@5=74.107 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=07:49 IST=> training   79.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.236 Prec@1=48.905 Prec@5=74.093 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=07:49 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.236 Prec@1=48.905 Prec@5=74.093 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=07:49 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.231 Loss=2.236 Prec@1=48.905 Prec@5=74.093 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=07:50 IST=> training   83.94% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.236 Prec@1=48.903 Prec@5=74.081 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=07:50 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.236 Prec@1=48.903 Prec@5=74.081 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=07:50 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.336 DataTime=0.232 Loss=2.236 Prec@1=48.903 Prec@5=74.081 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=07:50 IST=> training   87.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.880 Prec@5=74.062 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=07:50 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.880 Prec@5=74.062 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=07:50 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.880 Prec@5=74.062 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=07:51 IST=> training   91.93% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.238 Prec@1=48.887 Prec@5=74.059 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=07:51 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.238 Prec@1=48.887 Prec@5=74.059 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=07:51 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.238 Prec@1=48.887 Prec@5=74.059 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=07:51 IST=> training   95.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.891 Prec@5=74.066 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=07:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.891 Prec@5=74.066 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=07:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.891 Prec@5=74.066 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=07:51 IST=> training   99.92% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.893 Prec@5=74.067 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=07:51 IST=> training   100.00% of 1x2503...Epoch=35/150 LR=0.08785 Time=0.335 DataTime=0.231 Loss=2.237 Prec@1=48.893 Prec@5=74.067 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=07:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:51 IST=> validation 0.00% of 1x98...Epoch=35/150 LR=0.08785 Time=6.613 Loss=2.354 Prec@1=47.852 Prec@5=73.047 rate=0 Hz, eta=?, total=0:00:00, wall=07:51 IST=> validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.613 Loss=2.354 Prec@1=47.852 Prec@5=73.047 rate=3436.38 Hz, eta=0:00:00, total=0:00:00, wall=07:51 IST** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=6.613 Loss=2.354 Prec@1=47.852 Prec@5=73.047 rate=3436.38 Hz, eta=0:00:00, total=0:00:00, wall=07:52 IST** validation 1.02% of 1x98...Epoch=35/150 LR=0.08785 Time=0.403 Loss=2.349 Prec@1=46.690 Prec@5=72.438 rate=3436.38 Hz, eta=0:00:00, total=0:00:00, wall=07:52 IST** validation 100.00% of 1x98...Epoch=35/150 LR=0.08785 Time=0.403 Loss=2.349 Prec@1=46.690 Prec@5=72.438 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=07:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:52 IST=> training   0.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.751 DataTime=5.642 Loss=2.244 Prec@1=50.000 Prec@5=73.438 rate=0 Hz, eta=?, total=0:00:00, wall=07:52 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.751 DataTime=5.642 Loss=2.244 Prec@1=50.000 Prec@5=73.438 rate=6616.65 Hz, eta=0:00:00, total=0:00:00, wall=07:52 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=5.751 DataTime=5.642 Loss=2.244 Prec@1=50.000 Prec@5=73.438 rate=6616.65 Hz, eta=0:00:00, total=0:00:00, wall=07:53 IST=> training   0.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.380 DataTime=0.284 Loss=2.188 Prec@1=49.509 Prec@5=74.801 rate=6616.65 Hz, eta=0:00:00, total=0:00:00, wall=07:53 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.380 DataTime=0.284 Loss=2.188 Prec@1=49.509 Prec@5=74.801 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=07:53 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.380 DataTime=0.284 Loss=2.188 Prec@1=49.509 Prec@5=74.801 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=07:53 IST=> training   4.04% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.351 DataTime=0.253 Loss=2.193 Prec@1=49.622 Prec@5=74.799 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=07:53 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.351 DataTime=0.253 Loss=2.193 Prec@1=49.622 Prec@5=74.799 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=07:53 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.351 DataTime=0.253 Loss=2.193 Prec@1=49.622 Prec@5=74.799 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=07:54 IST=> training   8.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.346 DataTime=0.246 Loss=2.199 Prec@1=49.502 Prec@5=74.697 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=07:54 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.346 DataTime=0.246 Loss=2.199 Prec@1=49.502 Prec@5=74.697 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=07:54 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.346 DataTime=0.246 Loss=2.199 Prec@1=49.502 Prec@5=74.697 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=07:54 IST=> training   12.03% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.343 DataTime=0.243 Loss=2.203 Prec@1=49.427 Prec@5=74.640 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=07:54 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.343 DataTime=0.243 Loss=2.203 Prec@1=49.427 Prec@5=74.640 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=07:54 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.343 DataTime=0.243 Loss=2.203 Prec@1=49.427 Prec@5=74.640 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=07:55 IST=> training   16.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.205 Prec@1=49.400 Prec@5=74.628 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=07:55 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.205 Prec@1=49.400 Prec@5=74.628 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=07:55 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.205 Prec@1=49.400 Prec@5=74.628 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=07:55 IST=> training   20.02% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.209 Prec@1=49.367 Prec@5=74.599 rate=3.05 Hz, eta=0:10:56, total=0:02:44, wall=07:55 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.209 Prec@1=49.367 Prec@5=74.599 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=07:55 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.340 DataTime=0.238 Loss=2.209 Prec@1=49.367 Prec@5=74.599 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=07:56 IST=> training   24.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.238 Loss=2.214 Prec@1=49.307 Prec@5=74.497 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=07:56 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.238 Loss=2.214 Prec@1=49.307 Prec@5=74.497 rate=3.02 Hz, eta=0:09:55, total=0:03:51, wall=07:56 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.238 Loss=2.214 Prec@1=49.307 Prec@5=74.497 rate=3.02 Hz, eta=0:09:55, total=0:03:51, wall=07:57 IST=> training   28.01% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.237 Loss=2.215 Prec@1=49.301 Prec@5=74.489 rate=3.02 Hz, eta=0:09:55, total=0:03:51, wall=07:57 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.237 Loss=2.215 Prec@1=49.301 Prec@5=74.489 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=07:57 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.339 DataTime=0.237 Loss=2.215 Prec@1=49.301 Prec@5=74.489 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=07:57 IST=> training   32.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.217 Prec@1=49.281 Prec@5=74.447 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=07:57 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.217 Prec@1=49.281 Prec@5=74.447 rate=3.03 Hz, eta=0:08:49, total=0:04:57, wall=07:57 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.217 Prec@1=49.281 Prec@5=74.447 rate=3.03 Hz, eta=0:08:49, total=0:04:57, wall=07:58 IST=> training   36.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.216 Prec@1=49.282 Prec@5=74.443 rate=3.03 Hz, eta=0:08:49, total=0:04:57, wall=07:58 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.216 Prec@1=49.282 Prec@5=74.443 rate=3.02 Hz, eta=0:08:18, total=0:05:32, wall=07:58 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.235 Loss=2.216 Prec@1=49.282 Prec@5=74.443 rate=3.02 Hz, eta=0:08:18, total=0:05:32, wall=07:58 IST=> training   39.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.217 Prec@1=49.253 Prec@5=74.417 rate=3.02 Hz, eta=0:08:18, total=0:05:32, wall=07:58 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.217 Prec@1=49.253 Prec@5=74.417 rate=3.02 Hz, eta=0:07:43, total=0:06:04, wall=07:58 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.217 Prec@1=49.253 Prec@5=74.417 rate=3.02 Hz, eta=0:07:43, total=0:06:04, wall=07:59 IST=> training   43.99% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.219 Prec@1=49.182 Prec@5=74.384 rate=3.02 Hz, eta=0:07:43, total=0:06:04, wall=07:59 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.219 Prec@1=49.182 Prec@5=74.384 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=07:59 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.219 Prec@1=49.182 Prec@5=74.384 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=07:59 IST=> training   47.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.220 Prec@1=49.188 Prec@5=74.363 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=07:59 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.220 Prec@1=49.188 Prec@5=74.363 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=07:59 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.233 Loss=2.220 Prec@1=49.188 Prec@5=74.363 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=08:00 IST=> training   51.98% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.221 Prec@1=49.184 Prec@5=74.340 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=08:00 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.221 Prec@1=49.184 Prec@5=74.340 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=08:00 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.221 Prec@1=49.184 Prec@5=74.340 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=08:00 IST=> training   55.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.166 Prec@5=74.314 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=08:00 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.166 Prec@5=74.314 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=08:00 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.166 Prec@5=74.314 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=08:01 IST=> training   59.97% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.152 Prec@5=74.305 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=08:01 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.152 Prec@5=74.305 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=08:01 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.231 Loss=2.222 Prec@1=49.152 Prec@5=74.305 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=08:02 IST=> training   63.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.233 Loss=2.223 Prec@1=49.143 Prec@5=74.291 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=08:02 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.233 Loss=2.223 Prec@1=49.143 Prec@5=74.291 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=08:02 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.337 DataTime=0.233 Loss=2.223 Prec@1=49.143 Prec@5=74.291 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=08:02 IST=> training   67.96% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.223 Prec@1=49.145 Prec@5=74.282 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=08:02 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.223 Prec@1=49.145 Prec@5=74.282 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=08:02 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.336 DataTime=0.232 Loss=2.223 Prec@1=49.145 Prec@5=74.282 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=08:03 IST=> training   71.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.223 Prec@1=49.167 Prec@5=74.294 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=08:03 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.223 Prec@1=49.167 Prec@5=74.294 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=08:03 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.223 Prec@1=49.167 Prec@5=74.294 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=08:03 IST=> training   75.95% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.224 Prec@1=49.153 Prec@5=74.283 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=08:03 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.224 Prec@1=49.153 Prec@5=74.283 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=08:03 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.224 Prec@1=49.153 Prec@5=74.283 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=08:04 IST=> training   79.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.225 Prec@1=49.141 Prec@5=74.273 rate=3.01 Hz, eta=0:02:46, total=0:11:05, wall=08:04 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.225 Prec@1=49.141 Prec@5=74.273 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=08:04 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.225 Prec@1=49.141 Prec@5=74.273 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=08:04 IST=> training   83.94% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.226 Prec@1=49.127 Prec@5=74.249 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=08:04 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.226 Prec@1=49.127 Prec@5=74.249 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=08:04 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.226 Prec@1=49.127 Prec@5=74.249 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=08:05 IST=> training   87.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.227 Prec@1=49.122 Prec@5=74.248 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=08:05 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.227 Prec@1=49.122 Prec@5=74.248 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=08:05 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.335 DataTime=0.231 Loss=2.227 Prec@1=49.122 Prec@5=74.248 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=08:05 IST=> training   91.93% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.227 Prec@1=49.127 Prec@5=74.250 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=08:05 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.227 Prec@1=49.127 Prec@5=74.250 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=08:05 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.227 Prec@1=49.127 Prec@5=74.250 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=08:06 IST=> training   95.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.228 Prec@1=49.114 Prec@5=74.236 rate=3.01 Hz, eta=0:00:33, total=0:13:16, wall=08:06 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.228 Prec@1=49.114 Prec@5=74.236 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=08:06 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.228 Prec@1=49.114 Prec@5=74.236 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=08:06 IST=> training   99.92% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.228 Prec@1=49.113 Prec@5=74.235 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=08:06 IST=> training   100.00% of 1x2503...Epoch=36/150 LR=0.08716 Time=0.334 DataTime=0.230 Loss=2.228 Prec@1=49.113 Prec@5=74.235 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=08:06 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:06 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:06 IST=> validation 0.00% of 1x98...Epoch=36/150 LR=0.08716 Time=6.312 Loss=2.193 Prec@1=49.219 Prec@5=75.586 rate=0 Hz, eta=?, total=0:00:00, wall=08:06 IST=> validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.312 Loss=2.193 Prec@1=49.219 Prec@5=75.586 rate=6282.83 Hz, eta=0:00:00, total=0:00:00, wall=08:06 IST** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=6.312 Loss=2.193 Prec@1=49.219 Prec@5=75.586 rate=6282.83 Hz, eta=0:00:00, total=0:00:00, wall=08:07 IST** validation 1.02% of 1x98...Epoch=36/150 LR=0.08716 Time=0.391 Loss=2.467 Prec@1=44.392 Prec@5=70.444 rate=6282.83 Hz, eta=0:00:00, total=0:00:00, wall=08:07 IST** validation 100.00% of 1x98...Epoch=36/150 LR=0.08716 Time=0.391 Loss=2.467 Prec@1=44.392 Prec@5=70.444 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=08:07 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:07 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:07 IST=> training   0.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.519 DataTime=5.379 Loss=2.151 Prec@1=48.633 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=08:07 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.519 DataTime=5.379 Loss=2.151 Prec@1=48.633 Prec@5=76.367 rate=3340.34 Hz, eta=0:00:00, total=0:00:00, wall=08:07 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=5.519 DataTime=5.379 Loss=2.151 Prec@1=48.633 Prec@5=76.367 rate=3340.34 Hz, eta=0:00:00, total=0:00:00, wall=08:07 IST=> training   0.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.384 DataTime=0.285 Loss=2.200 Prec@1=49.584 Prec@5=74.517 rate=3340.34 Hz, eta=0:00:00, total=0:00:00, wall=08:07 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.384 DataTime=0.285 Loss=2.200 Prec@1=49.584 Prec@5=74.517 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=08:07 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.384 DataTime=0.285 Loss=2.200 Prec@1=49.584 Prec@5=74.517 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=08:08 IST=> training   4.04% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.360 DataTime=0.260 Loss=2.197 Prec@1=49.674 Prec@5=74.698 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=08:08 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.360 DataTime=0.260 Loss=2.197 Prec@1=49.674 Prec@5=74.698 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=08:08 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.360 DataTime=0.260 Loss=2.197 Prec@1=49.674 Prec@5=74.698 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=08:08 IST=> training   8.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.203 Prec@1=49.542 Prec@5=74.606 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=08:08 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.203 Prec@1=49.542 Prec@5=74.606 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=08:08 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.203 Prec@1=49.542 Prec@5=74.606 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=08:09 IST=> training   12.03% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.198 Prec@1=49.648 Prec@5=74.690 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=08:09 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.198 Prec@1=49.648 Prec@5=74.690 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=08:09 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.352 DataTime=0.250 Loss=2.198 Prec@1=49.648 Prec@5=74.690 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=08:10 IST=> training   16.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.349 DataTime=0.247 Loss=2.200 Prec@1=49.561 Prec@5=74.637 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=08:10 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.349 DataTime=0.247 Loss=2.200 Prec@1=49.561 Prec@5=74.637 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:10 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.349 DataTime=0.247 Loss=2.200 Prec@1=49.561 Prec@5=74.637 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:10 IST=> training   20.02% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.346 DataTime=0.244 Loss=2.201 Prec@1=49.566 Prec@5=74.589 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:10 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.346 DataTime=0.244 Loss=2.201 Prec@1=49.566 Prec@5=74.589 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:10 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.346 DataTime=0.244 Loss=2.201 Prec@1=49.566 Prec@5=74.589 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:11 IST=> training   24.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.347 DataTime=0.245 Loss=2.204 Prec@1=49.531 Prec@5=74.563 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:11 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.347 DataTime=0.245 Loss=2.204 Prec@1=49.531 Prec@5=74.563 rate=2.95 Hz, eta=0:10:11, total=0:03:57, wall=08:11 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.347 DataTime=0.245 Loss=2.204 Prec@1=49.531 Prec@5=74.563 rate=2.95 Hz, eta=0:10:11, total=0:03:57, wall=08:11 IST=> training   28.01% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.206 Prec@1=49.469 Prec@5=74.532 rate=2.95 Hz, eta=0:10:11, total=0:03:57, wall=08:11 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.206 Prec@1=49.469 Prec@5=74.532 rate=2.96 Hz, eta=0:09:35, total=0:04:31, wall=08:11 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.206 Prec@1=49.469 Prec@5=74.532 rate=2.96 Hz, eta=0:09:35, total=0:04:31, wall=08:12 IST=> training   32.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.344 DataTime=0.243 Loss=2.207 Prec@1=49.464 Prec@5=74.538 rate=2.96 Hz, eta=0:09:35, total=0:04:31, wall=08:12 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.344 DataTime=0.243 Loss=2.207 Prec@1=49.464 Prec@5=74.538 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:12 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.344 DataTime=0.243 Loss=2.207 Prec@1=49.464 Prec@5=74.538 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:12 IST=> training   36.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.208 Prec@1=49.441 Prec@5=74.532 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:12 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.208 Prec@1=49.441 Prec@5=74.532 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:12 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.345 DataTime=0.243 Loss=2.208 Prec@1=49.441 Prec@5=74.532 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:13 IST=> training   39.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.209 Prec@1=49.421 Prec@5=74.532 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:13 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.209 Prec@1=49.421 Prec@5=74.532 rate=2.96 Hz, eta=0:07:53, total=0:06:12, wall=08:13 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.209 Prec@1=49.421 Prec@5=74.532 rate=2.96 Hz, eta=0:07:53, total=0:06:12, wall=08:14 IST=> training   43.99% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.240 Loss=2.210 Prec@1=49.402 Prec@5=74.524 rate=2.96 Hz, eta=0:07:53, total=0:06:12, wall=08:14 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.240 Loss=2.210 Prec@1=49.402 Prec@5=74.524 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:14 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.240 Loss=2.210 Prec@1=49.402 Prec@5=74.524 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:14 IST=> training   47.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.211 Prec@1=49.403 Prec@5=74.501 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:14 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.211 Prec@1=49.403 Prec@5=74.501 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:14 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.241 Loss=2.211 Prec@1=49.403 Prec@5=74.501 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:15 IST=> training   51.98% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.378 Prec@5=74.486 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:15 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.378 Prec@5=74.486 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:15 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.378 Prec@5=74.486 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:15 IST=> training   55.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.373 Prec@5=74.478 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:15 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.373 Prec@5=74.478 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:15 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.240 Loss=2.212 Prec@1=49.373 Prec@5=74.478 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:16 IST=> training   59.97% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.239 Loss=2.213 Prec@1=49.360 Prec@5=74.457 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:16 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.239 Loss=2.213 Prec@1=49.360 Prec@5=74.457 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=08:16 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.343 DataTime=0.239 Loss=2.213 Prec@1=49.360 Prec@5=74.457 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=08:16 IST=> training   63.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.215 Prec@1=49.332 Prec@5=74.420 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=08:16 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.215 Prec@1=49.332 Prec@5=74.420 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=08:16 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.215 Prec@1=49.332 Prec@5=74.420 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=08:17 IST=> training   67.96% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.216 Prec@1=49.317 Prec@5=74.405 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=08:17 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.216 Prec@1=49.317 Prec@5=74.405 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=08:17 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.216 Prec@1=49.317 Prec@5=74.405 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=08:18 IST=> training   71.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.217 Prec@1=49.312 Prec@5=74.389 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=08:18 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.217 Prec@1=49.312 Prec@5=74.389 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:18 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.217 Prec@1=49.312 Prec@5=74.389 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:18 IST=> training   75.95% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.216 Prec@1=49.322 Prec@5=74.401 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:18 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.216 Prec@1=49.322 Prec@5=74.401 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:18 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.216 Prec@1=49.322 Prec@5=74.401 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:19 IST=> training   79.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.217 Prec@1=49.298 Prec@5=74.385 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:19 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.217 Prec@1=49.298 Prec@5=74.385 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:19 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.217 Prec@1=49.298 Prec@5=74.385 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:19 IST=> training   83.94% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.290 Prec@5=74.385 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:19 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.290 Prec@5=74.385 rate=2.94 Hz, eta=0:01:42, total=0:12:27, wall=08:19 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.290 Prec@5=74.385 rate=2.94 Hz, eta=0:01:42, total=0:12:27, wall=08:20 IST=> training   87.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.303 Prec@5=74.379 rate=2.94 Hz, eta=0:01:42, total=0:12:27, wall=08:20 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.303 Prec@5=74.379 rate=2.94 Hz, eta=0:01:08, total=0:13:01, wall=08:20 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.238 Loss=2.218 Prec@1=49.303 Prec@5=74.379 rate=2.94 Hz, eta=0:01:08, total=0:13:01, wall=08:20 IST=> training   91.93% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.218 Prec@1=49.286 Prec@5=74.373 rate=2.94 Hz, eta=0:01:08, total=0:13:01, wall=08:20 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.218 Prec@1=49.286 Prec@5=74.373 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:20 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.342 DataTime=0.237 Loss=2.218 Prec@1=49.286 Prec@5=74.373 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:21 IST=> training   95.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.218 Prec@1=49.296 Prec@5=74.374 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:21 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.218 Prec@1=49.296 Prec@5=74.374 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=08:21 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.218 Prec@1=49.296 Prec@5=74.374 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=08:21 IST=> training   99.92% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.218 Prec@1=49.297 Prec@5=74.373 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=08:21 IST=> training   100.00% of 1x2503...Epoch=37/150 LR=0.08645 Time=0.341 DataTime=0.237 Loss=2.218 Prec@1=49.297 Prec@5=74.373 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=08:21 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST=> validation 0.00% of 1x98...Epoch=37/150 LR=0.08645 Time=5.772 Loss=2.484 Prec@1=43.945 Prec@5=70.508 rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST=> validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=5.772 Loss=2.484 Prec@1=43.945 Prec@5=70.508 rate=5397.99 Hz, eta=0:00:00, total=0:00:00, wall=08:21 IST** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=5.772 Loss=2.484 Prec@1=43.945 Prec@5=70.508 rate=5397.99 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST** validation 1.02% of 1x98...Epoch=37/150 LR=0.08645 Time=0.391 Loss=2.407 Prec@1=45.600 Prec@5=71.442 rate=5397.99 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST** validation 100.00% of 1x98...Epoch=37/150 LR=0.08645 Time=0.391 Loss=2.407 Prec@1=45.600 Prec@5=71.442 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=08:22 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:22 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:22 IST=> training   0.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.291 DataTime=5.018 Loss=2.231 Prec@1=49.219 Prec@5=72.070 rate=0 Hz, eta=?, total=0:00:00, wall=08:22 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.291 DataTime=5.018 Loss=2.231 Prec@1=49.219 Prec@5=72.070 rate=8049.07 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=5.291 DataTime=5.018 Loss=2.231 Prec@1=49.219 Prec@5=72.070 rate=8049.07 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST=> training   0.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.387 DataTime=0.289 Loss=2.190 Prec@1=49.580 Prec@5=74.814 rate=8049.07 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.387 DataTime=0.289 Loss=2.190 Prec@1=49.580 Prec@5=74.814 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=08:22 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.387 DataTime=0.289 Loss=2.190 Prec@1=49.580 Prec@5=74.814 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=08:23 IST=> training   4.04% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.360 DataTime=0.258 Loss=2.190 Prec@1=49.736 Prec@5=74.818 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=08:23 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.360 DataTime=0.258 Loss=2.190 Prec@1=49.736 Prec@5=74.818 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=08:23 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.360 DataTime=0.258 Loss=2.190 Prec@1=49.736 Prec@5=74.818 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=08:23 IST=> training   8.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.349 DataTime=0.247 Loss=2.187 Prec@1=49.826 Prec@5=74.822 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=08:23 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.349 DataTime=0.247 Loss=2.187 Prec@1=49.826 Prec@5=74.822 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=08:23 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.349 DataTime=0.247 Loss=2.187 Prec@1=49.826 Prec@5=74.822 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=08:24 IST=> training   12.03% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.350 DataTime=0.248 Loss=2.186 Prec@1=49.905 Prec@5=74.871 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=08:24 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.350 DataTime=0.248 Loss=2.186 Prec@1=49.905 Prec@5=74.871 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=08:24 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.350 DataTime=0.248 Loss=2.186 Prec@1=49.905 Prec@5=74.871 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=08:24 IST=> training   16.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.242 Loss=2.190 Prec@1=49.851 Prec@5=74.794 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=08:24 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.242 Loss=2.190 Prec@1=49.851 Prec@5=74.794 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:24 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.242 Loss=2.190 Prec@1=49.851 Prec@5=74.794 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:25 IST=> training   20.02% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.243 Loss=2.190 Prec@1=49.818 Prec@5=74.777 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:25 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.243 Loss=2.190 Prec@1=49.818 Prec@5=74.777 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=08:25 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.344 DataTime=0.243 Loss=2.190 Prec@1=49.818 Prec@5=74.777 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=08:26 IST=> training   24.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.718 Prec@5=74.731 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=08:26 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.718 Prec@5=74.731 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=08:26 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.718 Prec@5=74.731 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=08:26 IST=> training   28.01% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.345 DataTime=0.242 Loss=2.192 Prec@1=49.747 Prec@5=74.756 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=08:26 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.345 DataTime=0.242 Loss=2.192 Prec@1=49.747 Prec@5=74.756 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:26 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.345 DataTime=0.242 Loss=2.192 Prec@1=49.747 Prec@5=74.756 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:27 IST=> training   32.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.703 Prec@5=74.703 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:27 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.703 Prec@5=74.703 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=08:27 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.240 Loss=2.194 Prec@1=49.703 Prec@5=74.703 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=08:27 IST=> training   36.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.238 Loss=2.194 Prec@1=49.677 Prec@5=74.705 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=08:27 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.238 Loss=2.194 Prec@1=49.677 Prec@5=74.705 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=08:27 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.238 Loss=2.194 Prec@1=49.677 Prec@5=74.705 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=08:28 IST=> training   39.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.239 Loss=2.196 Prec@1=49.650 Prec@5=74.681 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=08:28 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.239 Loss=2.196 Prec@1=49.650 Prec@5=74.681 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=08:28 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.343 DataTime=0.239 Loss=2.196 Prec@1=49.650 Prec@5=74.681 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=08:28 IST=> training   43.99% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.659 Prec@5=74.678 rate=2.96 Hz, eta=0:07:54, total=0:06:12, wall=08:28 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.659 Prec@5=74.678 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:28 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.659 Prec@5=74.678 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:29 IST=> training   47.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.664 Prec@5=74.687 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:29 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.664 Prec@5=74.687 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=08:29 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.196 Prec@1=49.664 Prec@5=74.687 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=08:30 IST=> training   51.98% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.199 Prec@1=49.618 Prec@5=74.649 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=08:30 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.199 Prec@1=49.618 Prec@5=74.649 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=08:30 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.199 Prec@1=49.618 Prec@5=74.649 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=08:30 IST=> training   55.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.200 Prec@1=49.593 Prec@5=74.638 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=08:30 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.200 Prec@1=49.593 Prec@5=74.638 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=08:30 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.200 Prec@1=49.593 Prec@5=74.638 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=08:31 IST=> training   59.97% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.202 Prec@1=49.546 Prec@5=74.592 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=08:31 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.202 Prec@1=49.546 Prec@5=74.592 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=08:31 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.237 Loss=2.202 Prec@1=49.546 Prec@5=74.592 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=08:31 IST=> training   63.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.202 Prec@1=49.545 Prec@5=74.604 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=08:31 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.202 Prec@1=49.545 Prec@5=74.604 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:31 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.342 DataTime=0.236 Loss=2.202 Prec@1=49.545 Prec@5=74.604 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:32 IST=> training   67.96% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.204 Prec@1=49.508 Prec@5=74.568 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:32 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.204 Prec@1=49.508 Prec@5=74.568 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=08:32 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.204 Prec@1=49.508 Prec@5=74.568 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=08:32 IST=> training   71.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.205 Prec@1=49.491 Prec@5=74.548 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=08:32 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.205 Prec@1=49.491 Prec@5=74.548 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=08:32 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.205 Prec@1=49.491 Prec@5=74.548 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=08:33 IST=> training   75.95% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.206 Prec@1=49.477 Prec@5=74.535 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=08:33 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.206 Prec@1=49.477 Prec@5=74.535 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:33 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.206 Prec@1=49.477 Prec@5=74.535 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:34 IST=> training   79.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.208 Prec@1=49.449 Prec@5=74.513 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=08:34 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.208 Prec@1=49.449 Prec@5=74.513 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:34 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.208 Prec@1=49.449 Prec@5=74.513 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:34 IST=> training   83.94% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.235 Loss=2.208 Prec@1=49.436 Prec@5=74.505 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=08:34 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.235 Loss=2.208 Prec@1=49.436 Prec@5=74.505 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=08:34 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.235 Loss=2.208 Prec@1=49.436 Prec@5=74.505 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=08:35 IST=> training   87.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.209 Prec@1=49.419 Prec@5=74.487 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=08:35 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.209 Prec@1=49.419 Prec@5=74.487 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=08:35 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.341 DataTime=0.236 Loss=2.209 Prec@1=49.419 Prec@5=74.487 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=08:35 IST=> training   91.93% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.209 Prec@1=49.412 Prec@5=74.484 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=08:35 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.209 Prec@1=49.412 Prec@5=74.484 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=08:35 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.209 Prec@1=49.412 Prec@5=74.484 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=08:36 IST=> training   95.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.210 Prec@1=49.405 Prec@5=74.473 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=08:36 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.210 Prec@1=49.405 Prec@5=74.473 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:36 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.210 Prec@1=49.405 Prec@5=74.473 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:36 IST=> training   99.92% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.210 Prec@1=49.402 Prec@5=74.471 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:36 IST=> training   100.00% of 1x2503...Epoch=38/150 LR=0.08572 Time=0.340 DataTime=0.235 Loss=2.210 Prec@1=49.402 Prec@5=74.471 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=08:36 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:36 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:36 IST=> validation 0.00% of 1x98...Epoch=38/150 LR=0.08572 Time=7.212 Loss=2.596 Prec@1=40.820 Prec@5=69.141 rate=0 Hz, eta=?, total=0:00:00, wall=08:36 IST=> validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=7.212 Loss=2.596 Prec@1=40.820 Prec@5=69.141 rate=6233.76 Hz, eta=0:00:00, total=0:00:00, wall=08:36 IST** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=7.212 Loss=2.596 Prec@1=40.820 Prec@5=69.141 rate=6233.76 Hz, eta=0:00:00, total=0:00:00, wall=08:36 IST** validation 1.02% of 1x98...Epoch=38/150 LR=0.08572 Time=0.416 Loss=2.498 Prec@1=43.600 Prec@5=70.026 rate=6233.76 Hz, eta=0:00:00, total=0:00:00, wall=08:36 IST** validation 100.00% of 1x98...Epoch=38/150 LR=0.08572 Time=0.416 Loss=2.498 Prec@1=43.600 Prec@5=70.026 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=08:36 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:37 IST=> training   0.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.823 DataTime=5.658 Loss=2.304 Prec@1=47.070 Prec@5=74.023 rate=0 Hz, eta=?, total=0:00:00, wall=08:37 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.823 DataTime=5.658 Loss=2.304 Prec@1=47.070 Prec@5=74.023 rate=7623.87 Hz, eta=0:00:00, total=0:00:00, wall=08:37 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=5.823 DataTime=5.658 Loss=2.304 Prec@1=47.070 Prec@5=74.023 rate=7623.87 Hz, eta=0:00:00, total=0:00:00, wall=08:37 IST=> training   0.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.384 DataTime=0.292 Loss=2.187 Prec@1=49.832 Prec@5=74.981 rate=7623.87 Hz, eta=0:00:00, total=0:00:00, wall=08:37 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.384 DataTime=0.292 Loss=2.187 Prec@1=49.832 Prec@5=74.981 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=08:37 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.384 DataTime=0.292 Loss=2.187 Prec@1=49.832 Prec@5=74.981 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=08:38 IST=> training   4.04% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.356 DataTime=0.256 Loss=2.188 Prec@1=49.809 Prec@5=74.966 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=08:38 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.356 DataTime=0.256 Loss=2.188 Prec@1=49.809 Prec@5=74.966 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=08:38 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.356 DataTime=0.256 Loss=2.188 Prec@1=49.809 Prec@5=74.966 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=08:38 IST=> training   8.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.352 DataTime=0.250 Loss=2.188 Prec@1=49.826 Prec@5=74.912 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=08:38 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.352 DataTime=0.250 Loss=2.188 Prec@1=49.826 Prec@5=74.912 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=08:38 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.352 DataTime=0.250 Loss=2.188 Prec@1=49.826 Prec@5=74.912 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=08:39 IST=> training   12.03% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.245 Loss=2.186 Prec@1=49.841 Prec@5=74.925 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=08:39 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.245 Loss=2.186 Prec@1=49.841 Prec@5=74.925 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:39 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.245 Loss=2.186 Prec@1=49.841 Prec@5=74.925 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:39 IST=> training   16.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.247 Loss=2.185 Prec@1=49.866 Prec@5=74.923 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:39 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.247 Loss=2.185 Prec@1=49.866 Prec@5=74.923 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:39 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.247 Loss=2.185 Prec@1=49.866 Prec@5=74.923 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:40 IST=> training   20.02% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.245 Loss=2.186 Prec@1=49.869 Prec@5=74.891 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:40 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.245 Loss=2.186 Prec@1=49.869 Prec@5=74.891 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=08:40 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.348 DataTime=0.245 Loss=2.186 Prec@1=49.869 Prec@5=74.891 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=08:40 IST=> training   24.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.345 DataTime=0.242 Loss=2.185 Prec@1=49.894 Prec@5=74.925 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=08:40 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.345 DataTime=0.242 Loss=2.185 Prec@1=49.894 Prec@5=74.925 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=08:40 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.345 DataTime=0.242 Loss=2.185 Prec@1=49.894 Prec@5=74.925 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=08:41 IST=> training   28.01% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.242 Loss=2.187 Prec@1=49.858 Prec@5=74.893 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=08:41 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.242 Loss=2.187 Prec@1=49.858 Prec@5=74.893 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:41 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.346 DataTime=0.242 Loss=2.187 Prec@1=49.858 Prec@5=74.893 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:42 IST=> training   32.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.344 DataTime=0.239 Loss=2.190 Prec@1=49.786 Prec@5=74.838 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=08:42 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.344 DataTime=0.239 Loss=2.190 Prec@1=49.786 Prec@5=74.838 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:42 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.344 DataTime=0.239 Loss=2.190 Prec@1=49.786 Prec@5=74.838 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:42 IST=> training   36.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.343 DataTime=0.238 Loss=2.192 Prec@1=49.743 Prec@5=74.788 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:42 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.343 DataTime=0.238 Loss=2.192 Prec@1=49.743 Prec@5=74.788 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:42 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.343 DataTime=0.238 Loss=2.192 Prec@1=49.743 Prec@5=74.788 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:43 IST=> training   39.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.342 DataTime=0.237 Loss=2.194 Prec@1=49.722 Prec@5=74.753 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:43 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.342 DataTime=0.237 Loss=2.194 Prec@1=49.722 Prec@5=74.753 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=08:43 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.342 DataTime=0.237 Loss=2.194 Prec@1=49.722 Prec@5=74.753 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=08:43 IST=> training   43.99% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.727 Prec@5=74.754 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=08:43 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.727 Prec@5=74.754 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:43 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.727 Prec@5=74.754 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:44 IST=> training   47.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.717 Prec@5=74.773 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:44 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.717 Prec@5=74.773 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:44 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.194 Prec@1=49.717 Prec@5=74.773 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:44 IST=> training   51.98% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.195 Prec@1=49.702 Prec@5=74.762 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:44 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.195 Prec@1=49.702 Prec@5=74.762 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:44 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.236 Loss=2.195 Prec@1=49.702 Prec@5=74.762 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:45 IST=> training   55.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.669 Prec@5=74.730 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:45 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.669 Prec@5=74.730 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:45 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.669 Prec@5=74.730 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:46 IST=> training   59.97% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.235 Loss=2.196 Prec@1=49.676 Prec@5=74.742 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:46 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.235 Loss=2.196 Prec@1=49.676 Prec@5=74.742 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=08:46 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.235 Loss=2.196 Prec@1=49.676 Prec@5=74.742 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=08:46 IST=> training   63.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.196 Prec@1=49.686 Prec@5=74.737 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=08:46 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.196 Prec@1=49.686 Prec@5=74.737 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=08:46 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.196 Prec@1=49.686 Prec@5=74.737 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=08:47 IST=> training   67.96% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.689 Prec@5=74.730 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=08:47 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.689 Prec@5=74.730 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:47 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.341 DataTime=0.235 Loss=2.197 Prec@1=49.689 Prec@5=74.730 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:47 IST=> training   71.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.197 Prec@1=49.683 Prec@5=74.718 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:47 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.197 Prec@1=49.683 Prec@5=74.718 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=08:47 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.197 Prec@1=49.683 Prec@5=74.718 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=08:48 IST=> training   75.95% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.664 Prec@5=74.710 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=08:48 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.664 Prec@5=74.710 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:48 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.664 Prec@5=74.710 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:48 IST=> training   79.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.665 Prec@5=74.701 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:48 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.665 Prec@5=74.701 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=08:48 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.198 Prec@1=49.665 Prec@5=74.701 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=08:49 IST=> training   83.94% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.199 Prec@1=49.654 Prec@5=74.687 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=08:49 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.199 Prec@1=49.654 Prec@5=74.687 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=08:49 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.199 Prec@1=49.654 Prec@5=74.687 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=08:49 IST=> training   87.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.200 Prec@1=49.632 Prec@5=74.669 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=08:49 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.200 Prec@1=49.632 Prec@5=74.669 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=08:49 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.234 Loss=2.200 Prec@1=49.632 Prec@5=74.669 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=08:50 IST=> training   91.93% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.233 Loss=2.200 Prec@1=49.626 Prec@5=74.669 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=08:50 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.233 Loss=2.200 Prec@1=49.626 Prec@5=74.669 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:50 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.340 DataTime=0.233 Loss=2.200 Prec@1=49.626 Prec@5=74.669 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:51 IST=> training   95.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.339 DataTime=0.233 Loss=2.201 Prec@1=49.601 Prec@5=74.650 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:51 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.339 DataTime=0.233 Loss=2.201 Prec@1=49.601 Prec@5=74.650 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:51 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.339 DataTime=0.233 Loss=2.201 Prec@1=49.601 Prec@5=74.650 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:51 IST=> training   99.92% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.339 DataTime=0.233 Loss=2.201 Prec@1=49.601 Prec@5=74.650 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:51 IST=> training   100.00% of 1x2503...Epoch=39/150 LR=0.08498 Time=0.339 DataTime=0.233 Loss=2.201 Prec@1=49.601 Prec@5=74.650 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> validation 0.00% of 1x98...Epoch=39/150 LR=0.08498 Time=7.266 Loss=2.610 Prec@1=43.750 Prec@5=67.969 rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.266 Loss=2.610 Prec@1=43.750 Prec@5=67.969 rate=2975.39 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=7.266 Loss=2.610 Prec@1=43.750 Prec@5=67.969 rate=2975.39 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST** validation 1.02% of 1x98...Epoch=39/150 LR=0.08498 Time=0.399 Loss=2.486 Prec@1=44.364 Prec@5=70.104 rate=2975.39 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST** validation 100.00% of 1x98...Epoch=39/150 LR=0.08498 Time=0.399 Loss=2.486 Prec@1=44.364 Prec@5=70.104 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=08:51 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training   0.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.594 DataTime=4.479 Loss=2.092 Prec@1=52.930 Prec@5=75.977 rate=0 Hz, eta=?, total=0:00:00, wall=08:51 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.594 DataTime=4.479 Loss=2.092 Prec@1=52.930 Prec@5=75.977 rate=1989.51 Hz, eta=0:00:01, total=0:00:00, wall=08:51 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=4.594 DataTime=4.479 Loss=2.092 Prec@1=52.930 Prec@5=75.977 rate=1989.51 Hz, eta=0:00:01, total=0:00:00, wall=08:52 IST=> training   0.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.381 DataTime=0.286 Loss=2.158 Prec@1=50.195 Prec@5=75.251 rate=1989.51 Hz, eta=0:00:01, total=0:00:00, wall=08:52 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.381 DataTime=0.286 Loss=2.158 Prec@1=50.195 Prec@5=75.251 rate=2.99 Hz, eta=0:13:24, total=0:00:33, wall=08:52 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.381 DataTime=0.286 Loss=2.158 Prec@1=50.195 Prec@5=75.251 rate=2.99 Hz, eta=0:13:24, total=0:00:33, wall=08:53 IST=> training   4.04% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.362 DataTime=0.261 Loss=2.162 Prec@1=50.200 Prec@5=75.246 rate=2.99 Hz, eta=0:13:24, total=0:00:33, wall=08:53 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.362 DataTime=0.261 Loss=2.162 Prec@1=50.200 Prec@5=75.246 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=08:53 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.362 DataTime=0.261 Loss=2.162 Prec@1=50.200 Prec@5=75.246 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=08:53 IST=> training   8.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.248 Loss=2.168 Prec@1=50.164 Prec@5=75.127 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=08:53 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.248 Loss=2.168 Prec@1=50.164 Prec@5=75.127 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=08:53 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.248 Loss=2.168 Prec@1=50.164 Prec@5=75.127 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=08:54 IST=> training   12.03% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.247 Loss=2.178 Prec@1=49.969 Prec@5=74.977 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=08:54 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.247 Loss=2.178 Prec@1=49.969 Prec@5=74.977 rate=2.93 Hz, eta=0:11:56, total=0:02:16, wall=08:54 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.352 DataTime=0.247 Loss=2.178 Prec@1=49.969 Prec@5=74.977 rate=2.93 Hz, eta=0:11:56, total=0:02:16, wall=08:54 IST=> training   16.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.347 DataTime=0.241 Loss=2.179 Prec@1=49.954 Prec@5=74.933 rate=2.93 Hz, eta=0:11:56, total=0:02:16, wall=08:54 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.347 DataTime=0.241 Loss=2.179 Prec@1=49.954 Prec@5=74.933 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:54 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.347 DataTime=0.241 Loss=2.179 Prec@1=49.954 Prec@5=74.933 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:55 IST=> training   20.02% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.237 Loss=2.180 Prec@1=49.903 Prec@5=74.932 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=08:55 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.237 Loss=2.180 Prec@1=49.903 Prec@5=74.932 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:55 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.237 Loss=2.180 Prec@1=49.903 Prec@5=74.932 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:55 IST=> training   24.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.238 Loss=2.182 Prec@1=49.879 Prec@5=74.904 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=08:55 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.238 Loss=2.182 Prec@1=49.879 Prec@5=74.904 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=08:55 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.345 DataTime=0.238 Loss=2.182 Prec@1=49.879 Prec@5=74.904 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=08:56 IST=> training   28.01% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.236 Loss=2.183 Prec@1=49.890 Prec@5=74.896 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=08:56 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.236 Loss=2.183 Prec@1=49.890 Prec@5=74.896 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:56 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.236 Loss=2.183 Prec@1=49.890 Prec@5=74.896 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:56 IST=> training   32.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.183 Prec@1=49.871 Prec@5=74.892 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:56 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.183 Prec@1=49.871 Prec@5=74.892 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:56 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.183 Prec@1=49.871 Prec@5=74.892 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:57 IST=> training   36.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.237 Loss=2.185 Prec@1=49.869 Prec@5=74.862 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:57 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.237 Loss=2.185 Prec@1=49.869 Prec@5=74.862 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:57 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.344 DataTime=0.237 Loss=2.185 Prec@1=49.869 Prec@5=74.862 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:58 IST=> training   39.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.185 Prec@1=49.860 Prec@5=74.864 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=08:58 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.185 Prec@1=49.860 Prec@5=74.864 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=08:58 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.343 DataTime=0.236 Loss=2.185 Prec@1=49.860 Prec@5=74.864 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=08:58 IST=> training   43.99% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.236 Loss=2.187 Prec@1=49.830 Prec@5=74.845 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=08:58 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.236 Loss=2.187 Prec@1=49.830 Prec@5=74.845 rate=2.95 Hz, eta=0:07:20, total=0:06:46, wall=08:58 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.236 Loss=2.187 Prec@1=49.830 Prec@5=74.845 rate=2.95 Hz, eta=0:07:20, total=0:06:46, wall=08:59 IST=> training   47.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.235 Loss=2.186 Prec@1=49.822 Prec@5=74.848 rate=2.95 Hz, eta=0:07:20, total=0:06:46, wall=08:59 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.235 Loss=2.186 Prec@1=49.822 Prec@5=74.848 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:59 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.342 DataTime=0.235 Loss=2.186 Prec@1=49.822 Prec@5=74.848 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:59 IST=> training   51.98% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.795 Prec@5=74.821 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=08:59 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.795 Prec@5=74.821 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=08:59 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.795 Prec@5=74.821 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=09:00 IST=> training   55.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.811 Prec@5=74.833 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=09:00 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.811 Prec@5=74.833 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=09:00 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.187 Prec@1=49.811 Prec@5=74.833 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=09:00 IST=> training   59.97% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.792 Prec@5=74.823 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=09:00 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.792 Prec@5=74.823 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=09:00 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.792 Prec@5=74.823 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=09:01 IST=> training   63.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.804 Prec@5=74.830 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=09:01 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.804 Prec@5=74.830 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:01 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.188 Prec@1=49.804 Prec@5=74.830 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:02 IST=> training   67.96% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.189 Prec@1=49.791 Prec@5=74.817 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:02 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.189 Prec@1=49.791 Prec@5=74.817 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:02 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.189 Prec@1=49.791 Prec@5=74.817 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:02 IST=> training   71.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.772 Prec@5=74.790 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:02 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.772 Prec@5=74.790 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=09:02 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.772 Prec@5=74.790 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=09:03 IST=> training   75.95% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.773 Prec@5=74.790 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=09:03 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.773 Prec@5=74.790 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=09:03 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.190 Prec@1=49.773 Prec@5=74.790 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=09:03 IST=> training   79.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.736 Prec@5=74.765 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=09:03 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.736 Prec@5=74.765 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=09:03 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.736 Prec@5=74.765 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=09:04 IST=> training   83.94% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.727 Prec@5=74.762 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=09:04 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.727 Prec@5=74.762 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:04 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.341 DataTime=0.234 Loss=2.192 Prec@1=49.727 Prec@5=74.762 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:04 IST=> training   87.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.193 Prec@1=49.710 Prec@5=74.749 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:04 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.193 Prec@1=49.710 Prec@5=74.749 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=09:04 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.193 Prec@1=49.710 Prec@5=74.749 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=09:05 IST=> training   91.93% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.194 Prec@1=49.690 Prec@5=74.734 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=09:05 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.194 Prec@1=49.690 Prec@5=74.734 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:05 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.340 DataTime=0.233 Loss=2.194 Prec@1=49.690 Prec@5=74.734 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:05 IST=> training   95.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.339 DataTime=0.232 Loss=2.195 Prec@1=49.683 Prec@5=74.730 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:05 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.339 DataTime=0.232 Loss=2.195 Prec@1=49.683 Prec@5=74.730 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=09:05 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.339 DataTime=0.232 Loss=2.195 Prec@1=49.683 Prec@5=74.730 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=09:05 IST=> training   99.92% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.339 DataTime=0.232 Loss=2.195 Prec@1=49.683 Prec@5=74.730 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=09:05 IST=> training   100.00% of 1x2503...Epoch=40/150 LR=0.08423 Time=0.339 DataTime=0.232 Loss=2.195 Prec@1=49.683 Prec@5=74.730 rate=2.97 Hz, eta=0:00:00, total=0:14:04, wall=09:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> validation 0.00% of 1x98...Epoch=40/150 LR=0.08423 Time=7.179 Loss=3.099 Prec@1=35.547 Prec@5=60.156 rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=7.179 Loss=3.099 Prec@1=35.547 Prec@5=60.156 rate=2165.88 Hz, eta=0:00:00, total=0:00:00, wall=09:06 IST** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=7.179 Loss=3.099 Prec@1=35.547 Prec@5=60.156 rate=2165.88 Hz, eta=0:00:00, total=0:00:00, wall=09:06 IST** validation 1.02% of 1x98...Epoch=40/150 LR=0.08423 Time=0.410 Loss=2.994 Prec@1=36.206 Prec@5=61.586 rate=2165.88 Hz, eta=0:00:00, total=0:00:00, wall=09:06 IST** validation 100.00% of 1x98...Epoch=40/150 LR=0.08423 Time=0.410 Loss=2.994 Prec@1=36.206 Prec@5=61.586 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=09:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> training   0.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.741 DataTime=4.583 Loss=2.140 Prec@1=49.609 Prec@5=74.023 rate=0 Hz, eta=?, total=0:00:00, wall=09:06 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.741 DataTime=4.583 Loss=2.140 Prec@1=49.609 Prec@5=74.023 rate=6363.07 Hz, eta=0:00:00, total=0:00:00, wall=09:06 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=4.741 DataTime=4.583 Loss=2.140 Prec@1=49.609 Prec@5=74.023 rate=6363.07 Hz, eta=0:00:00, total=0:00:00, wall=09:07 IST=> training   0.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.378 DataTime=0.281 Loss=2.170 Prec@1=50.255 Prec@5=75.197 rate=6363.07 Hz, eta=0:00:00, total=0:00:00, wall=09:07 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.378 DataTime=0.281 Loss=2.170 Prec@1=50.255 Prec@5=75.197 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=09:07 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.378 DataTime=0.281 Loss=2.170 Prec@1=50.255 Prec@5=75.197 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=09:07 IST=> training   4.04% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.351 DataTime=0.253 Loss=2.162 Prec@1=50.279 Prec@5=75.245 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=09:07 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.351 DataTime=0.253 Loss=2.162 Prec@1=50.279 Prec@5=75.245 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=09:07 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.351 DataTime=0.253 Loss=2.162 Prec@1=50.279 Prec@5=75.245 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=09:08 IST=> training   8.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.348 DataTime=0.247 Loss=2.165 Prec@1=50.167 Prec@5=75.232 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=09:08 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.348 DataTime=0.247 Loss=2.165 Prec@1=50.167 Prec@5=75.232 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=09:08 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.348 DataTime=0.247 Loss=2.165 Prec@1=50.167 Prec@5=75.232 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=09:08 IST=> training   12.03% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.349 DataTime=0.248 Loss=2.161 Prec@1=50.184 Prec@5=75.286 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=09:08 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.349 DataTime=0.248 Loss=2.161 Prec@1=50.184 Prec@5=75.286 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=09:08 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.349 DataTime=0.248 Loss=2.161 Prec@1=50.184 Prec@5=75.286 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=09:09 IST=> training   16.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.242 Loss=2.163 Prec@1=50.136 Prec@5=75.267 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=09:09 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.242 Loss=2.163 Prec@1=50.136 Prec@5=75.267 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:09 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.242 Loss=2.163 Prec@1=50.136 Prec@5=75.267 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:10 IST=> training   20.02% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.239 Loss=2.165 Prec@1=50.165 Prec@5=75.244 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:10 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.239 Loss=2.165 Prec@1=50.165 Prec@5=75.244 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:10 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.239 Loss=2.165 Prec@1=50.165 Prec@5=75.244 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:10 IST=> training   24.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.240 Loss=2.167 Prec@1=50.165 Prec@5=75.199 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:10 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.240 Loss=2.167 Prec@1=50.165 Prec@5=75.199 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=09:10 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.240 Loss=2.167 Prec@1=50.165 Prec@5=75.199 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=09:11 IST=> training   28.01% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.168 Prec@1=50.177 Prec@5=75.183 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=09:11 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.168 Prec@1=50.177 Prec@5=75.183 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:11 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.168 Prec@1=50.177 Prec@5=75.183 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:11 IST=> training   32.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.170 Prec@1=50.147 Prec@5=75.141 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:11 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.170 Prec@1=50.147 Prec@5=75.141 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:11 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.170 Prec@1=50.147 Prec@5=75.141 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:12 IST=> training   36.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.239 Loss=2.172 Prec@1=50.112 Prec@5=75.109 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:12 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.239 Loss=2.172 Prec@1=50.112 Prec@5=75.109 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=09:12 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.343 DataTime=0.239 Loss=2.172 Prec@1=50.112 Prec@5=75.109 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=09:12 IST=> training   39.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.173 Prec@1=50.108 Prec@5=75.078 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=09:12 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.173 Prec@1=50.108 Prec@5=75.078 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=09:12 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.342 DataTime=0.238 Loss=2.173 Prec@1=50.108 Prec@5=75.078 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=09:13 IST=> training   43.99% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.175 Prec@1=50.069 Prec@5=75.047 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=09:13 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.175 Prec@1=50.069 Prec@5=75.047 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=09:13 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.175 Prec@1=50.069 Prec@5=75.047 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=09:14 IST=> training   47.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.175 Prec@1=50.094 Prec@5=75.041 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=09:14 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.175 Prec@1=50.094 Prec@5=75.041 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:14 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.237 Loss=2.175 Prec@1=50.094 Prec@5=75.041 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:14 IST=> training   51.98% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.176 Prec@1=50.086 Prec@5=75.023 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:14 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.176 Prec@1=50.086 Prec@5=75.023 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:14 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.176 Prec@1=50.086 Prec@5=75.023 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:15 IST=> training   55.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.177 Prec@1=50.073 Prec@5=75.023 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:15 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.177 Prec@1=50.073 Prec@5=75.023 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=09:15 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.236 Loss=2.177 Prec@1=50.073 Prec@5=75.023 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=09:15 IST=> training   59.97% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.236 Loss=2.179 Prec@1=50.042 Prec@5=74.994 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=09:15 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.236 Loss=2.179 Prec@1=50.042 Prec@5=74.994 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=09:15 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.341 DataTime=0.236 Loss=2.179 Prec@1=50.042 Prec@5=74.994 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=09:16 IST=> training   63.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.180 Prec@1=50.000 Prec@5=74.981 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=09:16 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.180 Prec@1=50.000 Prec@5=74.981 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=09:16 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.180 Prec@1=50.000 Prec@5=74.981 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=09:16 IST=> training   67.96% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.181 Prec@1=49.972 Prec@5=74.963 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=09:16 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.181 Prec@1=49.972 Prec@5=74.963 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:16 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.181 Prec@1=49.972 Prec@5=74.963 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:17 IST=> training   71.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.181 Prec@1=49.967 Prec@5=74.952 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:17 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.181 Prec@1=49.967 Prec@5=74.952 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:17 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.340 DataTime=0.235 Loss=2.181 Prec@1=49.967 Prec@5=74.952 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:17 IST=> training   75.95% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.182 Prec@1=49.964 Prec@5=74.936 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:17 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.182 Prec@1=49.964 Prec@5=74.936 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=09:17 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.182 Prec@1=49.964 Prec@5=74.936 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=09:18 IST=> training   79.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.183 Prec@1=49.949 Prec@5=74.920 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=09:18 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.183 Prec@1=49.949 Prec@5=74.920 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=09:18 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.183 Prec@1=49.949 Prec@5=74.920 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=09:19 IST=> training   83.94% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.183 Prec@1=49.936 Prec@5=74.908 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=09:19 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.183 Prec@1=49.936 Prec@5=74.908 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=09:19 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.183 Prec@1=49.936 Prec@5=74.908 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=09:19 IST=> training   87.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.184 Prec@1=49.925 Prec@5=74.900 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=09:19 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.184 Prec@1=49.925 Prec@5=74.900 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=09:19 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.234 Loss=2.184 Prec@1=49.925 Prec@5=74.900 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=09:20 IST=> training   91.93% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.185 Prec@1=49.905 Prec@5=74.887 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=09:20 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.185 Prec@1=49.905 Prec@5=74.887 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:20 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.339 DataTime=0.233 Loss=2.185 Prec@1=49.905 Prec@5=74.887 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:20 IST=> training   95.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.186 Prec@1=49.882 Prec@5=74.873 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:20 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.186 Prec@1=49.882 Prec@5=74.873 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:20 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.186 Prec@1=49.882 Prec@5=74.873 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:20 IST=> training   99.92% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.186 Prec@1=49.881 Prec@5=74.872 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:20 IST=> training   100.00% of 1x2503...Epoch=41/150 LR=0.08346 Time=0.338 DataTime=0.233 Loss=2.186 Prec@1=49.881 Prec@5=74.872 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST=> validation 0.00% of 1x98...Epoch=41/150 LR=0.08346 Time=6.884 Loss=2.352 Prec@1=44.336 Prec@5=74.219 rate=0 Hz, eta=?, total=0:00:00, wall=09:20 IST=> validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=6.884 Loss=2.352 Prec@1=44.336 Prec@5=74.219 rate=4798.26 Hz, eta=0:00:00, total=0:00:00, wall=09:20 IST** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=6.884 Loss=2.352 Prec@1=44.336 Prec@5=74.219 rate=4798.26 Hz, eta=0:00:00, total=0:00:00, wall=09:21 IST** validation 1.02% of 1x98...Epoch=41/150 LR=0.08346 Time=0.399 Loss=2.354 Prec@1=46.532 Prec@5=72.248 rate=4798.26 Hz, eta=0:00:00, total=0:00:00, wall=09:21 IST** validation 100.00% of 1x98...Epoch=41/150 LR=0.08346 Time=0.399 Loss=2.354 Prec@1=46.532 Prec@5=72.248 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=09:21 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:21 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:21 IST=> training   0.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.738 DataTime=4.570 Loss=2.102 Prec@1=52.344 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=09:21 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.738 DataTime=4.570 Loss=2.102 Prec@1=52.344 Prec@5=74.609 rate=6935.15 Hz, eta=0:00:00, total=0:00:00, wall=09:21 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=4.738 DataTime=4.570 Loss=2.102 Prec@1=52.344 Prec@5=74.609 rate=6935.15 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST=> training   0.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.379 DataTime=0.279 Loss=2.153 Prec@1=50.485 Prec@5=75.391 rate=6935.15 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.379 DataTime=0.279 Loss=2.153 Prec@1=50.485 Prec@5=75.391 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=09:22 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.379 DataTime=0.279 Loss=2.153 Prec@1=50.485 Prec@5=75.391 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=09:22 IST=> training   4.04% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.352 DataTime=0.252 Loss=2.157 Prec@1=50.466 Prec@5=75.297 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=09:22 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.352 DataTime=0.252 Loss=2.157 Prec@1=50.466 Prec@5=75.297 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=09:22 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.352 DataTime=0.252 Loss=2.157 Prec@1=50.466 Prec@5=75.297 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=09:23 IST=> training   8.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.346 DataTime=0.245 Loss=2.159 Prec@1=50.440 Prec@5=75.317 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=09:23 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.346 DataTime=0.245 Loss=2.159 Prec@1=50.440 Prec@5=75.317 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:23 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.346 DataTime=0.245 Loss=2.159 Prec@1=50.440 Prec@5=75.317 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:23 IST=> training   12.03% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.345 DataTime=0.243 Loss=2.161 Prec@1=50.402 Prec@5=75.304 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:23 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.345 DataTime=0.243 Loss=2.161 Prec@1=50.402 Prec@5=75.304 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:23 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.345 DataTime=0.243 Loss=2.161 Prec@1=50.402 Prec@5=75.304 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:24 IST=> training   16.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.344 DataTime=0.241 Loss=2.162 Prec@1=50.392 Prec@5=75.282 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:24 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.344 DataTime=0.241 Loss=2.162 Prec@1=50.392 Prec@5=75.282 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=09:24 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.344 DataTime=0.241 Loss=2.162 Prec@1=50.392 Prec@5=75.282 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=09:24 IST=> training   20.02% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.342 DataTime=0.239 Loss=2.163 Prec@1=50.370 Prec@5=75.257 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=09:24 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.342 DataTime=0.239 Loss=2.163 Prec@1=50.370 Prec@5=75.257 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:24 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.342 DataTime=0.239 Loss=2.163 Prec@1=50.370 Prec@5=75.257 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:25 IST=> training   24.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.320 Prec@5=75.238 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=09:25 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.320 Prec@5=75.238 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:25 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.320 Prec@5=75.238 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:25 IST=> training   28.01% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.165 Prec@1=50.353 Prec@5=75.229 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:25 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.165 Prec@1=50.353 Prec@5=75.229 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=09:25 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.165 Prec@1=50.353 Prec@5=75.229 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=09:26 IST=> training   32.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.165 Prec@1=50.335 Prec@5=75.227 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=09:26 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.165 Prec@1=50.335 Prec@5=75.227 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:26 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.165 Prec@1=50.335 Prec@5=75.227 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:27 IST=> training   36.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.167 Prec@1=50.300 Prec@5=75.202 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=09:27 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.167 Prec@1=50.300 Prec@5=75.202 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:27 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.236 Loss=2.167 Prec@1=50.300 Prec@5=75.202 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:27 IST=> training   39.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.167 Prec@1=50.301 Prec@5=75.192 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:27 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.167 Prec@1=50.301 Prec@5=75.192 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:27 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.237 Loss=2.167 Prec@1=50.301 Prec@5=75.192 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:28 IST=> training   43.99% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.236 Loss=2.168 Prec@1=50.257 Prec@5=75.183 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:28 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.236 Loss=2.168 Prec@1=50.257 Prec@5=75.183 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:28 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.341 DataTime=0.236 Loss=2.168 Prec@1=50.257 Prec@5=75.183 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:28 IST=> training   47.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.170 Prec@1=50.202 Prec@5=75.144 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:28 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.170 Prec@1=50.202 Prec@5=75.144 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=09:28 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.170 Prec@1=50.202 Prec@5=75.144 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=09:29 IST=> training   51.98% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.171 Prec@1=50.194 Prec@5=75.140 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=09:29 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.171 Prec@1=50.194 Prec@5=75.140 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:29 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.235 Loss=2.171 Prec@1=50.194 Prec@5=75.140 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:29 IST=> training   55.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.171 Prec@1=50.181 Prec@5=75.141 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=09:29 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.171 Prec@1=50.181 Prec@5=75.141 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=09:29 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.171 Prec@1=50.181 Prec@5=75.141 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=09:30 IST=> training   59.97% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.171 Prec@1=50.188 Prec@5=75.118 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=09:30 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.171 Prec@1=50.188 Prec@5=75.118 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=09:30 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.171 Prec@1=50.188 Prec@5=75.118 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=09:31 IST=> training   63.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.172 Prec@1=50.193 Prec@5=75.112 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=09:31 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.172 Prec@1=50.193 Prec@5=75.112 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=09:31 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.172 Prec@1=50.193 Prec@5=75.112 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=09:31 IST=> training   67.96% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.173 Prec@1=50.177 Prec@5=75.092 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=09:31 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.173 Prec@1=50.177 Prec@5=75.092 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:31 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.173 Prec@1=50.177 Prec@5=75.092 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:32 IST=> training   71.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.174 Prec@1=50.162 Prec@5=75.076 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=09:32 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.174 Prec@1=50.162 Prec@5=75.076 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=09:32 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.234 Loss=2.174 Prec@1=50.162 Prec@5=75.076 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=09:32 IST=> training   75.95% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.175 Prec@1=50.147 Prec@5=75.072 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=09:32 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.175 Prec@1=50.147 Prec@5=75.072 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=09:32 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.340 DataTime=0.234 Loss=2.175 Prec@1=50.147 Prec@5=75.072 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=09:33 IST=> training   79.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.176 Prec@1=50.127 Prec@5=75.041 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=09:33 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.176 Prec@1=50.127 Prec@5=75.041 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=09:33 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.176 Prec@1=50.127 Prec@5=75.041 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=09:33 IST=> training   83.94% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.177 Prec@1=50.117 Prec@5=75.022 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=09:33 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.177 Prec@1=50.117 Prec@5=75.022 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=09:33 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.177 Prec@1=50.117 Prec@5=75.022 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=09:34 IST=> training   87.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.178 Prec@1=50.109 Prec@5=75.006 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=09:34 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.178 Prec@1=50.109 Prec@5=75.006 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=09:34 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.178 Prec@1=50.109 Prec@5=75.006 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=09:35 IST=> training   91.93% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.179 Prec@1=50.101 Prec@5=74.997 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=09:35 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.179 Prec@1=50.101 Prec@5=74.997 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=09:35 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.339 DataTime=0.233 Loss=2.179 Prec@1=50.101 Prec@5=74.997 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=09:35 IST=> training   95.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.338 DataTime=0.233 Loss=2.179 Prec@1=50.102 Prec@5=75.001 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=09:35 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.338 DataTime=0.233 Loss=2.179 Prec@1=50.102 Prec@5=75.001 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:35 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.338 DataTime=0.233 Loss=2.179 Prec@1=50.102 Prec@5=75.001 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:35 IST=> training   99.92% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.338 DataTime=0.233 Loss=2.179 Prec@1=50.101 Prec@5=75.001 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=09:35 IST=> training   100.00% of 1x2503...Epoch=42/150 LR=0.08267 Time=0.338 DataTime=0.233 Loss=2.179 Prec@1=50.101 Prec@5=75.001 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=09:35 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> validation 0.00% of 1x98...Epoch=42/150 LR=0.08267 Time=6.462 Loss=2.191 Prec@1=47.070 Prec@5=74.219 rate=0 Hz, eta=?, total=0:00:00, wall=09:35 IST=> validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=6.462 Loss=2.191 Prec@1=47.070 Prec@5=74.219 rate=3102.29 Hz, eta=0:00:00, total=0:00:00, wall=09:35 IST** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=6.462 Loss=2.191 Prec@1=47.070 Prec@5=74.219 rate=3102.29 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST** validation 1.02% of 1x98...Epoch=42/150 LR=0.08267 Time=0.395 Loss=2.208 Prec@1=49.000 Prec@5=74.838 rate=3102.29 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST** validation 100.00% of 1x98...Epoch=42/150 LR=0.08267 Time=0.395 Loss=2.208 Prec@1=49.000 Prec@5=74.838 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=09:36 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:36 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:36 IST=> training   0.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=6.041 DataTime=5.938 Loss=2.016 Prec@1=54.492 Prec@5=76.172 rate=0 Hz, eta=?, total=0:00:00, wall=09:36 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=6.041 DataTime=5.938 Loss=2.016 Prec@1=54.492 Prec@5=76.172 rate=7318.40 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=6.041 DataTime=5.938 Loss=2.016 Prec@1=54.492 Prec@5=76.172 rate=7318.40 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST=> training   0.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.382 DataTime=0.290 Loss=2.156 Prec@1=50.698 Prec@5=75.228 rate=7318.40 Hz, eta=0:00:00, total=0:00:00, wall=09:36 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.382 DataTime=0.290 Loss=2.156 Prec@1=50.698 Prec@5=75.228 rate=3.11 Hz, eta=0:12:52, total=0:00:32, wall=09:36 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.382 DataTime=0.290 Loss=2.156 Prec@1=50.698 Prec@5=75.228 rate=3.11 Hz, eta=0:12:52, total=0:00:32, wall=09:37 IST=> training   4.04% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.353 DataTime=0.260 Loss=2.150 Prec@1=50.666 Prec@5=75.464 rate=3.11 Hz, eta=0:12:52, total=0:00:32, wall=09:37 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.353 DataTime=0.260 Loss=2.150 Prec@1=50.666 Prec@5=75.464 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=09:37 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.353 DataTime=0.260 Loss=2.150 Prec@1=50.666 Prec@5=75.464 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=09:37 IST=> training   8.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.346 DataTime=0.250 Loss=2.152 Prec@1=50.592 Prec@5=75.425 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=09:37 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.346 DataTime=0.250 Loss=2.152 Prec@1=50.592 Prec@5=75.425 rate=3.07 Hz, eta=0:11:58, total=0:01:38, wall=09:37 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.346 DataTime=0.250 Loss=2.152 Prec@1=50.592 Prec@5=75.425 rate=3.07 Hz, eta=0:11:58, total=0:01:38, wall=09:38 IST=> training   12.03% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.342 DataTime=0.244 Loss=2.148 Prec@1=50.640 Prec@5=75.471 rate=3.07 Hz, eta=0:11:58, total=0:01:38, wall=09:38 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.342 DataTime=0.244 Loss=2.148 Prec@1=50.640 Prec@5=75.471 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=09:38 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.342 DataTime=0.244 Loss=2.148 Prec@1=50.640 Prec@5=75.471 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=09:39 IST=> training   16.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.241 Loss=2.150 Prec@1=50.579 Prec@5=75.432 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=09:39 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.241 Loss=2.150 Prec@1=50.579 Prec@5=75.432 rate=3.06 Hz, eta=0:10:54, total=0:02:43, wall=09:39 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.241 Loss=2.150 Prec@1=50.579 Prec@5=75.432 rate=3.06 Hz, eta=0:10:54, total=0:02:43, wall=09:39 IST=> training   20.02% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.239 Loss=2.152 Prec@1=50.531 Prec@5=75.447 rate=3.06 Hz, eta=0:10:54, total=0:02:43, wall=09:39 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.239 Loss=2.152 Prec@1=50.531 Prec@5=75.447 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=09:39 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.339 DataTime=0.239 Loss=2.152 Prec@1=50.531 Prec@5=75.447 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=09:40 IST=> training   24.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.238 Loss=2.153 Prec@1=50.486 Prec@5=75.447 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=09:40 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.238 Loss=2.153 Prec@1=50.486 Prec@5=75.447 rate=3.04 Hz, eta=0:09:53, total=0:03:50, wall=09:40 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.238 Loss=2.153 Prec@1=50.486 Prec@5=75.447 rate=3.04 Hz, eta=0:09:53, total=0:03:50, wall=09:40 IST=> training   28.01% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.236 Loss=2.156 Prec@1=50.457 Prec@5=75.425 rate=3.04 Hz, eta=0:09:53, total=0:03:50, wall=09:40 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.236 Loss=2.156 Prec@1=50.457 Prec@5=75.425 rate=3.04 Hz, eta=0:09:19, total=0:04:23, wall=09:40 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.236 Loss=2.156 Prec@1=50.457 Prec@5=75.425 rate=3.04 Hz, eta=0:09:19, total=0:04:23, wall=09:41 IST=> training   32.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.236 Loss=2.159 Prec@1=50.405 Prec@5=75.364 rate=3.04 Hz, eta=0:09:19, total=0:04:23, wall=09:41 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.236 Loss=2.159 Prec@1=50.405 Prec@5=75.364 rate=3.02 Hz, eta=0:08:49, total=0:04:58, wall=09:41 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.338 DataTime=0.236 Loss=2.159 Prec@1=50.405 Prec@5=75.364 rate=3.02 Hz, eta=0:08:49, total=0:04:58, wall=09:41 IST=> training   36.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.236 Loss=2.161 Prec@1=50.338 Prec@5=75.352 rate=3.02 Hz, eta=0:08:49, total=0:04:58, wall=09:41 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.236 Loss=2.161 Prec@1=50.338 Prec@5=75.352 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=09:41 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.236 Loss=2.161 Prec@1=50.338 Prec@5=75.352 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=09:42 IST=> training   39.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.161 Prec@1=50.359 Prec@5=75.329 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=09:42 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.161 Prec@1=50.359 Prec@5=75.329 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=09:42 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.161 Prec@1=50.359 Prec@5=75.329 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=09:42 IST=> training   43.99% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.235 Loss=2.164 Prec@1=50.332 Prec@5=75.273 rate=3.03 Hz, eta=0:07:42, total=0:06:03, wall=09:42 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.235 Loss=2.164 Prec@1=50.332 Prec@5=75.273 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=09:42 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.337 DataTime=0.235 Loss=2.164 Prec@1=50.332 Prec@5=75.273 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=09:43 IST=> training   47.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.165 Prec@1=50.318 Prec@5=75.239 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=09:43 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.165 Prec@1=50.318 Prec@5=75.239 rate=3.02 Hz, eta=0:06:37, total=0:07:10, wall=09:43 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.165 Prec@1=50.318 Prec@5=75.239 rate=3.02 Hz, eta=0:06:37, total=0:07:10, wall=09:44 IST=> training   51.98% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.166 Prec@1=50.302 Prec@5=75.220 rate=3.02 Hz, eta=0:06:37, total=0:07:10, wall=09:44 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.166 Prec@1=50.302 Prec@5=75.220 rate=3.03 Hz, eta=0:06:04, total=0:07:43, wall=09:44 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.166 Prec@1=50.302 Prec@5=75.220 rate=3.03 Hz, eta=0:06:04, total=0:07:43, wall=09:44 IST=> training   55.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.235 Loss=2.167 Prec@1=50.289 Prec@5=75.200 rate=3.03 Hz, eta=0:06:04, total=0:07:43, wall=09:44 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.235 Loss=2.167 Prec@1=50.289 Prec@5=75.200 rate=3.01 Hz, eta=0:05:32, total=0:08:17, wall=09:44 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.235 Loss=2.167 Prec@1=50.289 Prec@5=75.200 rate=3.01 Hz, eta=0:05:32, total=0:08:17, wall=09:45 IST=> training   59.97% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.235 Loss=2.166 Prec@1=50.304 Prec@5=75.227 rate=3.01 Hz, eta=0:05:32, total=0:08:17, wall=09:45 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.235 Loss=2.166 Prec@1=50.304 Prec@5=75.227 rate=3.02 Hz, eta=0:04:58, total=0:08:50, wall=09:45 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.235 Loss=2.166 Prec@1=50.304 Prec@5=75.227 rate=3.02 Hz, eta=0:04:58, total=0:08:50, wall=09:45 IST=> training   63.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.167 Prec@1=50.264 Prec@5=75.214 rate=3.02 Hz, eta=0:04:58, total=0:08:50, wall=09:45 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.167 Prec@1=50.264 Prec@5=75.214 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=09:45 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.167 Prec@1=50.264 Prec@5=75.214 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=09:46 IST=> training   67.96% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.168 Prec@1=50.246 Prec@5=75.203 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=09:46 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.168 Prec@1=50.246 Prec@5=75.203 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=09:46 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.168 Prec@1=50.246 Prec@5=75.203 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=09:46 IST=> training   71.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.169 Prec@1=50.232 Prec@5=75.181 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=09:46 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.169 Prec@1=50.232 Prec@5=75.181 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=09:46 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.234 Loss=2.169 Prec@1=50.232 Prec@5=75.181 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=09:47 IST=> training   75.95% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.170 Prec@1=50.217 Prec@5=75.161 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=09:47 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.170 Prec@1=50.217 Prec@5=75.161 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=09:47 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.336 DataTime=0.234 Loss=2.170 Prec@1=50.217 Prec@5=75.161 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=09:47 IST=> training   79.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.171 Prec@1=50.203 Prec@5=75.143 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=09:47 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.171 Prec@1=50.203 Prec@5=75.143 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=09:47 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.171 Prec@1=50.203 Prec@5=75.143 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=09:48 IST=> training   83.94% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.172 Prec@1=50.193 Prec@5=75.135 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=09:48 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.172 Prec@1=50.193 Prec@5=75.135 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=09:48 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.172 Prec@1=50.193 Prec@5=75.135 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=09:49 IST=> training   87.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.172 Prec@5=75.123 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=09:49 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.172 Prec@5=75.123 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=09:49 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.172 Prec@5=75.123 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=09:49 IST=> training   91.93% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.160 Prec@5=75.121 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=09:49 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.160 Prec@5=75.121 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=09:49 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.335 DataTime=0.233 Loss=2.173 Prec@1=50.160 Prec@5=75.121 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=09:50 IST=> training   95.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.334 DataTime=0.232 Loss=2.174 Prec@1=50.140 Prec@5=75.100 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=09:50 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.334 DataTime=0.232 Loss=2.174 Prec@1=50.140 Prec@5=75.100 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=09:50 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.334 DataTime=0.232 Loss=2.174 Prec@1=50.140 Prec@5=75.100 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=09:50 IST=> training   99.92% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.334 DataTime=0.232 Loss=2.174 Prec@1=50.140 Prec@5=75.100 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=09:50 IST=> training   100.00% of 1x2503...Epoch=43/150 LR=0.08187 Time=0.334 DataTime=0.232 Loss=2.174 Prec@1=50.140 Prec@5=75.100 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=09:50 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> validation 0.00% of 1x98...Epoch=43/150 LR=0.08187 Time=6.253 Loss=2.711 Prec@1=38.477 Prec@5=65.820 rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.253 Loss=2.711 Prec@1=38.477 Prec@5=65.820 rate=3316.53 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=6.253 Loss=2.711 Prec@1=38.477 Prec@5=65.820 rate=3316.53 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST** validation 1.02% of 1x98...Epoch=43/150 LR=0.08187 Time=0.404 Loss=2.725 Prec@1=40.170 Prec@5=66.030 rate=3316.53 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST** validation 100.00% of 1x98...Epoch=43/150 LR=0.08187 Time=0.404 Loss=2.725 Prec@1=40.170 Prec@5=66.030 rate=2.94 Hz, eta=0:00:00, total=0:00:33, wall=09:50 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> training   0.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.930 DataTime=5.840 Loss=2.203 Prec@1=48.828 Prec@5=74.023 rate=0 Hz, eta=?, total=0:00:00, wall=09:50 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.930 DataTime=5.840 Loss=2.203 Prec@1=48.828 Prec@5=74.023 rate=2595.89 Hz, eta=0:00:00, total=0:00:00, wall=09:50 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=5.930 DataTime=5.840 Loss=2.203 Prec@1=48.828 Prec@5=74.023 rate=2595.89 Hz, eta=0:00:00, total=0:00:00, wall=09:51 IST=> training   0.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.383 DataTime=0.293 Loss=2.149 Prec@1=50.526 Prec@5=75.485 rate=2595.89 Hz, eta=0:00:00, total=0:00:00, wall=09:51 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.383 DataTime=0.293 Loss=2.149 Prec@1=50.526 Prec@5=75.485 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=09:51 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.383 DataTime=0.293 Loss=2.149 Prec@1=50.526 Prec@5=75.485 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=09:52 IST=> training   4.04% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.357 DataTime=0.264 Loss=2.143 Prec@1=50.600 Prec@5=75.549 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=09:52 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.357 DataTime=0.264 Loss=2.143 Prec@1=50.600 Prec@5=75.549 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=09:52 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.357 DataTime=0.264 Loss=2.143 Prec@1=50.600 Prec@5=75.549 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=09:52 IST=> training   8.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.253 Loss=2.147 Prec@1=50.612 Prec@5=75.552 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=09:52 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.253 Loss=2.147 Prec@1=50.612 Prec@5=75.552 rate=3.04 Hz, eta=0:12:05, total=0:01:39, wall=09:52 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.253 Loss=2.147 Prec@1=50.612 Prec@5=75.552 rate=3.04 Hz, eta=0:12:05, total=0:01:39, wall=09:53 IST=> training   12.03% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.251 Loss=2.149 Prec@1=50.549 Prec@5=75.453 rate=3.04 Hz, eta=0:12:05, total=0:01:39, wall=09:53 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.251 Loss=2.149 Prec@1=50.549 Prec@5=75.453 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=09:53 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.349 DataTime=0.251 Loss=2.149 Prec@1=50.549 Prec@5=75.453 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=09:53 IST=> training   16.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.347 DataTime=0.246 Loss=2.150 Prec@1=50.470 Prec@5=75.449 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=09:53 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.347 DataTime=0.246 Loss=2.150 Prec@1=50.470 Prec@5=75.449 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=09:53 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.347 DataTime=0.246 Loss=2.150 Prec@1=50.470 Prec@5=75.449 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=09:54 IST=> training   20.02% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.243 Loss=2.150 Prec@1=50.489 Prec@5=75.455 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=09:54 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.243 Loss=2.150 Prec@1=50.489 Prec@5=75.455 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:54 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.243 Loss=2.150 Prec@1=50.489 Prec@5=75.455 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:54 IST=> training   24.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.242 Loss=2.150 Prec@1=50.463 Prec@5=75.472 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:54 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.242 Loss=2.150 Prec@1=50.463 Prec@5=75.472 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:54 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.344 DataTime=0.242 Loss=2.150 Prec@1=50.463 Prec@5=75.472 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:55 IST=> training   28.01% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.342 DataTime=0.240 Loss=2.153 Prec@1=50.421 Prec@5=75.459 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:55 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.342 DataTime=0.240 Loss=2.153 Prec@1=50.421 Prec@5=75.459 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=09:55 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.342 DataTime=0.240 Loss=2.153 Prec@1=50.421 Prec@5=75.459 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=09:55 IST=> training   32.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.418 Prec@5=75.459 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=09:55 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.418 Prec@5=75.459 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=09:55 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.418 Prec@5=75.459 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=09:56 IST=> training   36.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.424 Prec@5=75.441 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=09:56 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.424 Prec@5=75.441 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:56 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.341 DataTime=0.239 Loss=2.154 Prec@1=50.424 Prec@5=75.441 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:57 IST=> training   39.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.154 Prec@1=50.434 Prec@5=75.433 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=09:57 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.154 Prec@1=50.434 Prec@5=75.433 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=09:57 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.154 Prec@1=50.434 Prec@5=75.433 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=09:57 IST=> training   43.99% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.155 Prec@1=50.444 Prec@5=75.409 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=09:57 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.155 Prec@1=50.444 Prec@5=75.409 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:57 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.155 Prec@1=50.444 Prec@5=75.409 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:58 IST=> training   47.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.157 Prec@1=50.423 Prec@5=75.372 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:58 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.157 Prec@1=50.423 Prec@5=75.372 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=09:58 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.157 Prec@1=50.423 Prec@5=75.372 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=09:58 IST=> training   51.98% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.339 DataTime=0.238 Loss=2.157 Prec@1=50.410 Prec@5=75.364 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=09:58 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.339 DataTime=0.238 Loss=2.157 Prec@1=50.410 Prec@5=75.364 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:58 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.339 DataTime=0.238 Loss=2.157 Prec@1=50.410 Prec@5=75.364 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:59 IST=> training   55.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.158 Prec@1=50.396 Prec@5=75.347 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:59 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.158 Prec@1=50.396 Prec@5=75.347 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=09:59 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.158 Prec@1=50.396 Prec@5=75.347 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=09:59 IST=> training   59.97% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.398 Prec@5=75.326 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=09:59 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.398 Prec@5=75.326 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=09:59 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.398 Prec@5=75.326 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=10:00 IST=> training   63.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.381 Prec@5=75.307 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=10:00 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.381 Prec@5=75.307 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:00 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.160 Prec@1=50.381 Prec@5=75.307 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:01 IST=> training   67.96% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.376 Prec@5=75.295 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:01 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.376 Prec@5=75.295 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=10:01 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.376 Prec@5=75.295 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=10:01 IST=> training   71.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.363 Prec@5=75.301 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=10:01 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.363 Prec@5=75.301 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:01 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.161 Prec@1=50.363 Prec@5=75.301 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:02 IST=> training   75.95% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.163 Prec@1=50.322 Prec@5=75.268 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:02 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.163 Prec@1=50.322 Prec@5=75.268 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=10:02 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.238 Loss=2.163 Prec@1=50.322 Prec@5=75.268 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=10:02 IST=> training   79.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.164 Prec@1=50.299 Prec@5=75.249 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=10:02 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.164 Prec@1=50.299 Prec@5=75.249 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:02 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.164 Prec@1=50.299 Prec@5=75.249 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:03 IST=> training   83.94% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.290 Prec@5=75.237 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:03 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.290 Prec@5=75.237 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=10:03 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.165 Prec@1=50.290 Prec@5=75.237 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=10:03 IST=> training   87.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.269 Prec@5=75.219 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=10:03 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.269 Prec@5=75.219 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:03 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.269 Prec@5=75.219 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:04 IST=> training   91.93% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.261 Prec@5=75.219 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:04 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.261 Prec@5=75.219 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:04 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.166 Prec@1=50.261 Prec@5=75.219 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:05 IST=> training   95.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.167 Prec@1=50.259 Prec@5=75.212 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:05 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.167 Prec@1=50.259 Prec@5=75.212 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:05 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.167 Prec@1=50.259 Prec@5=75.212 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:05 IST=> training   99.92% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.167 Prec@1=50.257 Prec@5=75.211 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:05 IST=> training   100.00% of 1x2503...Epoch=44/150 LR=0.08106 Time=0.340 DataTime=0.237 Loss=2.167 Prec@1=50.257 Prec@5=75.211 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=10:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> validation 0.00% of 1x98...Epoch=44/150 LR=0.08106 Time=6.316 Loss=2.618 Prec@1=39.258 Prec@5=67.383 rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=6.316 Loss=2.618 Prec@1=39.258 Prec@5=67.383 rate=2854.94 Hz, eta=0:00:00, total=0:00:00, wall=10:05 IST** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=6.316 Loss=2.618 Prec@1=39.258 Prec@5=67.383 rate=2854.94 Hz, eta=0:00:00, total=0:00:00, wall=10:05 IST** validation 1.02% of 1x98...Epoch=44/150 LR=0.08106 Time=0.399 Loss=2.775 Prec@1=39.108 Prec@5=65.004 rate=2854.94 Hz, eta=0:00:00, total=0:00:00, wall=10:05 IST** validation 100.00% of 1x98...Epoch=44/150 LR=0.08106 Time=0.399 Loss=2.775 Prec@1=39.108 Prec@5=65.004 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=10:05 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> training   0.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=5.363 DataTime=5.214 Loss=2.389 Prec@1=46.680 Prec@5=72.656 rate=0 Hz, eta=?, total=0:00:00, wall=10:05 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=5.363 DataTime=5.214 Loss=2.389 Prec@1=46.680 Prec@5=72.656 rate=3587.04 Hz, eta=0:00:00, total=0:00:00, wall=10:05 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=5.363 DataTime=5.214 Loss=2.389 Prec@1=46.680 Prec@5=72.656 rate=3587.04 Hz, eta=0:00:00, total=0:00:00, wall=10:06 IST=> training   0.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.374 DataTime=0.275 Loss=2.123 Prec@1=51.038 Prec@5=75.870 rate=3587.04 Hz, eta=0:00:00, total=0:00:00, wall=10:06 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.374 DataTime=0.275 Loss=2.123 Prec@1=51.038 Prec@5=75.870 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=10:06 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.374 DataTime=0.275 Loss=2.123 Prec@1=51.038 Prec@5=75.870 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=10:06 IST=> training   4.04% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.353 DataTime=0.254 Loss=2.121 Prec@1=50.989 Prec@5=75.848 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=10:06 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.353 DataTime=0.254 Loss=2.121 Prec@1=50.989 Prec@5=75.848 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=10:06 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.353 DataTime=0.254 Loss=2.121 Prec@1=50.989 Prec@5=75.848 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=10:07 IST=> training   8.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.347 DataTime=0.246 Loss=2.130 Prec@1=50.800 Prec@5=75.694 rate=3.07 Hz, eta=0:12:30, total=0:01:05, wall=10:07 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.347 DataTime=0.246 Loss=2.130 Prec@1=50.800 Prec@5=75.694 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:07 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.347 DataTime=0.246 Loss=2.130 Prec@1=50.800 Prec@5=75.694 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:08 IST=> training   12.03% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.345 DataTime=0.243 Loss=2.130 Prec@1=50.739 Prec@5=75.728 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:08 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.345 DataTime=0.243 Loss=2.130 Prec@1=50.739 Prec@5=75.728 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:08 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.345 DataTime=0.243 Loss=2.130 Prec@1=50.739 Prec@5=75.728 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:08 IST=> training   16.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.806 Prec@5=75.675 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:08 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.806 Prec@5=75.675 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:08 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.806 Prec@5=75.675 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:09 IST=> training   20.02% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.340 DataTime=0.237 Loss=2.135 Prec@1=50.744 Prec@5=75.671 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:09 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.340 DataTime=0.237 Loss=2.135 Prec@1=50.744 Prec@5=75.671 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=10:09 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.340 DataTime=0.237 Loss=2.135 Prec@1=50.744 Prec@5=75.671 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=10:09 IST=> training   24.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.339 DataTime=0.237 Loss=2.138 Prec@1=50.705 Prec@5=75.643 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=10:09 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.339 DataTime=0.237 Loss=2.138 Prec@1=50.705 Prec@5=75.643 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=10:09 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.339 DataTime=0.237 Loss=2.138 Prec@1=50.705 Prec@5=75.643 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=10:10 IST=> training   28.01% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.235 Loss=2.141 Prec@1=50.686 Prec@5=75.603 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=10:10 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.235 Loss=2.141 Prec@1=50.686 Prec@5=75.603 rate=3.02 Hz, eta=0:09:22, total=0:04:24, wall=10:10 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.235 Loss=2.141 Prec@1=50.686 Prec@5=75.603 rate=3.02 Hz, eta=0:09:22, total=0:04:24, wall=10:10 IST=> training   32.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.234 Loss=2.144 Prec@1=50.628 Prec@5=75.530 rate=3.02 Hz, eta=0:09:22, total=0:04:24, wall=10:10 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.234 Loss=2.144 Prec@1=50.628 Prec@5=75.530 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=10:10 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.234 Loss=2.144 Prec@1=50.628 Prec@5=75.530 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=10:11 IST=> training   36.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.235 Loss=2.146 Prec@1=50.590 Prec@5=75.517 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=10:11 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.235 Loss=2.146 Prec@1=50.590 Prec@5=75.517 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=10:11 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.235 Loss=2.146 Prec@1=50.590 Prec@5=75.517 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=10:11 IST=> training   39.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.147 Prec@1=50.585 Prec@5=75.471 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=10:11 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.147 Prec@1=50.585 Prec@5=75.471 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:11 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.147 Prec@1=50.585 Prec@5=75.471 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:12 IST=> training   43.99% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.148 Prec@1=50.578 Prec@5=75.466 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:12 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.148 Prec@1=50.578 Prec@5=75.466 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=10:12 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.233 Loss=2.148 Prec@1=50.578 Prec@5=75.466 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=10:13 IST=> training   47.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.233 Loss=2.149 Prec@1=50.549 Prec@5=75.456 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=10:13 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.233 Loss=2.149 Prec@1=50.549 Prec@5=75.456 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=10:13 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.338 DataTime=0.233 Loss=2.149 Prec@1=50.549 Prec@5=75.456 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=10:13 IST=> training   51.98% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.531 Prec@5=75.442 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=10:13 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.531 Prec@5=75.442 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=10:13 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.531 Prec@5=75.442 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=10:14 IST=> training   55.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.550 Prec@5=75.439 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=10:14 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.550 Prec@5=75.439 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=10:14 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.150 Prec@1=50.550 Prec@5=75.439 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=10:14 IST=> training   59.97% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.152 Prec@1=50.510 Prec@5=75.403 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=10:14 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.152 Prec@1=50.510 Prec@5=75.403 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=10:14 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.232 Loss=2.152 Prec@1=50.510 Prec@5=75.403 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=10:15 IST=> training   63.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.153 Prec@1=50.499 Prec@5=75.379 rate=3.00 Hz, eta=0:05:00, total=0:08:54, wall=10:15 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.153 Prec@1=50.499 Prec@5=75.379 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=10:15 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.153 Prec@1=50.499 Prec@5=75.379 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=10:15 IST=> training   67.96% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.154 Prec@1=50.491 Prec@5=75.370 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=10:15 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.154 Prec@1=50.491 Prec@5=75.370 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=10:15 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.231 Loss=2.154 Prec@1=50.491 Prec@5=75.370 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=10:16 IST=> training   71.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.231 Loss=2.155 Prec@1=50.464 Prec@5=75.341 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=10:16 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.231 Loss=2.155 Prec@1=50.464 Prec@5=75.341 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=10:16 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.337 DataTime=0.231 Loss=2.155 Prec@1=50.464 Prec@5=75.341 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=10:16 IST=> training   75.95% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.156 Prec@1=50.444 Prec@5=75.337 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=10:16 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.156 Prec@1=50.444 Prec@5=75.337 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=10:16 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.156 Prec@1=50.444 Prec@5=75.337 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=10:17 IST=> training   79.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.157 Prec@1=50.431 Prec@5=75.325 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=10:17 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.157 Prec@1=50.431 Prec@5=75.325 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=10:17 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.157 Prec@1=50.431 Prec@5=75.325 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=10:18 IST=> training   83.94% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.158 Prec@1=50.418 Prec@5=75.320 rate=3.00 Hz, eta=0:02:13, total=0:11:40, wall=10:18 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.158 Prec@1=50.418 Prec@5=75.320 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=10:18 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.230 Loss=2.158 Prec@1=50.418 Prec@5=75.320 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=10:18 IST=> training   87.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.229 Loss=2.159 Prec@1=50.398 Prec@5=75.293 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=10:18 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.229 Loss=2.159 Prec@1=50.398 Prec@5=75.293 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=10:18 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.336 DataTime=0.229 Loss=2.159 Prec@1=50.398 Prec@5=75.293 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=10:19 IST=> training   91.93% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.229 Loss=2.160 Prec@1=50.384 Prec@5=75.283 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=10:19 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.229 Loss=2.160 Prec@1=50.384 Prec@5=75.283 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=10:19 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.229 Loss=2.160 Prec@1=50.384 Prec@5=75.283 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=10:19 IST=> training   95.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.228 Loss=2.161 Prec@1=50.388 Prec@5=75.277 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=10:19 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.228 Loss=2.161 Prec@1=50.388 Prec@5=75.277 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=10:19 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.228 Loss=2.161 Prec@1=50.388 Prec@5=75.277 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=10:19 IST=> training   99.92% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.228 Loss=2.161 Prec@1=50.388 Prec@5=75.275 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=10:19 IST=> training   100.00% of 1x2503...Epoch=45/150 LR=0.08023 Time=0.335 DataTime=0.228 Loss=2.161 Prec@1=50.388 Prec@5=75.275 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=10:19 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:19 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:19 IST=> validation 0.00% of 1x98...Epoch=45/150 LR=0.08023 Time=6.304 Loss=2.473 Prec@1=44.141 Prec@5=71.094 rate=0 Hz, eta=?, total=0:00:00, wall=10:19 IST=> validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.304 Loss=2.473 Prec@1=44.141 Prec@5=71.094 rate=5479.99 Hz, eta=0:00:00, total=0:00:00, wall=10:19 IST** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=6.304 Loss=2.473 Prec@1=44.141 Prec@5=71.094 rate=5479.99 Hz, eta=0:00:00, total=0:00:00, wall=10:20 IST** validation 1.02% of 1x98...Epoch=45/150 LR=0.08023 Time=0.397 Loss=2.558 Prec@1=43.172 Prec@5=68.524 rate=5479.99 Hz, eta=0:00:00, total=0:00:00, wall=10:20 IST** validation 100.00% of 1x98...Epoch=45/150 LR=0.08023 Time=0.397 Loss=2.558 Prec@1=43.172 Prec@5=68.524 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=10:20 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:20 IST=> training   0.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=4.737 DataTime=4.602 Loss=2.141 Prec@1=50.977 Prec@5=75.195 rate=0 Hz, eta=?, total=0:00:00, wall=10:20 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=4.737 DataTime=4.602 Loss=2.141 Prec@1=50.977 Prec@5=75.195 rate=4414.23 Hz, eta=0:00:00, total=0:00:00, wall=10:20 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=4.737 DataTime=4.602 Loss=2.141 Prec@1=50.977 Prec@5=75.195 rate=4414.23 Hz, eta=0:00:00, total=0:00:00, wall=10:20 IST=> training   0.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.379 DataTime=0.286 Loss=2.120 Prec@1=51.524 Prec@5=75.808 rate=4414.23 Hz, eta=0:00:00, total=0:00:00, wall=10:20 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.379 DataTime=0.286 Loss=2.120 Prec@1=51.524 Prec@5=75.808 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=10:20 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.379 DataTime=0.286 Loss=2.120 Prec@1=51.524 Prec@5=75.808 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=10:21 IST=> training   4.04% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.349 DataTime=0.253 Loss=2.120 Prec@1=51.349 Prec@5=75.872 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=10:21 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.349 DataTime=0.253 Loss=2.120 Prec@1=51.349 Prec@5=75.872 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=10:21 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.349 DataTime=0.253 Loss=2.120 Prec@1=51.349 Prec@5=75.872 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=10:22 IST=> training   8.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.346 DataTime=0.249 Loss=2.124 Prec@1=51.220 Prec@5=75.864 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=10:22 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.346 DataTime=0.249 Loss=2.124 Prec@1=51.220 Prec@5=75.864 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=10:22 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.346 DataTime=0.249 Loss=2.124 Prec@1=51.220 Prec@5=75.864 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=10:22 IST=> training   12.03% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.245 Loss=2.128 Prec@1=51.083 Prec@5=75.804 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=10:22 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.245 Loss=2.128 Prec@1=51.083 Prec@5=75.804 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=10:22 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.245 Loss=2.128 Prec@1=51.083 Prec@5=75.804 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=10:23 IST=> training   16.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.242 Loss=2.127 Prec@1=51.025 Prec@5=75.800 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=10:23 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.242 Loss=2.127 Prec@1=51.025 Prec@5=75.800 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=10:23 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.242 Loss=2.127 Prec@1=51.025 Prec@5=75.800 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=10:23 IST=> training   20.02% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.244 Loss=2.130 Prec@1=50.956 Prec@5=75.761 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=10:23 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.244 Loss=2.130 Prec@1=50.956 Prec@5=75.761 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=10:23 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.343 DataTime=0.244 Loss=2.130 Prec@1=50.956 Prec@5=75.761 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=10:24 IST=> training   24.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.241 Loss=2.132 Prec@1=50.912 Prec@5=75.735 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=10:24 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.241 Loss=2.132 Prec@1=50.912 Prec@5=75.735 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=10:24 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.241 Loss=2.132 Prec@1=50.912 Prec@5=75.735 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=10:24 IST=> training   28.01% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.239 Loss=2.135 Prec@1=50.822 Prec@5=75.667 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=10:24 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.239 Loss=2.135 Prec@1=50.822 Prec@5=75.667 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=10:24 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.239 Loss=2.135 Prec@1=50.822 Prec@5=75.667 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=10:25 IST=> training   32.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.342 DataTime=0.240 Loss=2.137 Prec@1=50.791 Prec@5=75.651 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=10:25 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.342 DataTime=0.240 Loss=2.137 Prec@1=50.791 Prec@5=75.651 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:25 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.342 DataTime=0.240 Loss=2.137 Prec@1=50.791 Prec@5=75.651 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:26 IST=> training   36.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.780 Prec@5=75.643 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:26 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.780 Prec@5=75.643 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=10:26 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.780 Prec@5=75.643 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=10:26 IST=> training   39.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.757 Prec@5=75.643 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=10:26 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.757 Prec@5=75.643 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=10:26 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.757 Prec@5=75.643 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=10:27 IST=> training   43.99% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.752 Prec@5=75.639 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=10:27 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.752 Prec@5=75.639 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=10:27 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.752 Prec@5=75.639 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=10:27 IST=> training   47.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.237 Loss=2.140 Prec@1=50.733 Prec@5=75.627 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=10:27 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.237 Loss=2.140 Prec@1=50.733 Prec@5=75.627 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=10:27 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.341 DataTime=0.237 Loss=2.140 Prec@1=50.733 Prec@5=75.627 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=10:28 IST=> training   51.98% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.142 Prec@1=50.679 Prec@5=75.601 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=10:28 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.142 Prec@1=50.679 Prec@5=75.601 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=10:28 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.142 Prec@1=50.679 Prec@5=75.601 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=10:28 IST=> training   55.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.688 Prec@5=75.599 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=10:28 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.688 Prec@5=75.599 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:28 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.688 Prec@5=75.599 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:29 IST=> training   59.97% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.687 Prec@5=75.595 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:29 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.687 Prec@5=75.595 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:29 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.687 Prec@5=75.595 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:29 IST=> training   63.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.144 Prec@1=50.675 Prec@5=75.583 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:29 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.144 Prec@1=50.675 Prec@5=75.583 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:29 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.144 Prec@1=50.675 Prec@5=75.583 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:30 IST=> training   67.96% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.145 Prec@1=50.649 Prec@5=75.570 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:30 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.145 Prec@1=50.649 Prec@5=75.570 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:30 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.235 Loss=2.145 Prec@1=50.649 Prec@5=75.570 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:31 IST=> training   71.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.147 Prec@1=50.604 Prec@5=75.537 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:31 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.147 Prec@1=50.604 Prec@5=75.537 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:31 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.147 Prec@1=50.604 Prec@5=75.537 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:31 IST=> training   75.95% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.148 Prec@1=50.593 Prec@5=75.521 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:31 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.148 Prec@1=50.593 Prec@5=75.521 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=10:31 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.148 Prec@1=50.593 Prec@5=75.521 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=10:32 IST=> training   79.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.149 Prec@1=50.574 Prec@5=75.503 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=10:32 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.149 Prec@1=50.574 Prec@5=75.503 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:32 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.339 DataTime=0.234 Loss=2.149 Prec@1=50.574 Prec@5=75.503 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:32 IST=> training   83.94% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.150 Prec@1=50.564 Prec@5=75.492 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:32 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.150 Prec@1=50.564 Prec@5=75.492 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:32 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.234 Loss=2.150 Prec@1=50.564 Prec@5=75.492 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:33 IST=> training   87.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.151 Prec@1=50.544 Prec@5=75.479 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:33 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.151 Prec@1=50.544 Prec@5=75.479 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=10:33 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.151 Prec@1=50.544 Prec@5=75.479 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=10:33 IST=> training   91.93% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.152 Prec@1=50.525 Prec@5=75.469 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=10:33 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.152 Prec@1=50.525 Prec@5=75.469 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=10:33 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.338 DataTime=0.233 Loss=2.152 Prec@1=50.525 Prec@5=75.469 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=10:34 IST=> training   95.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.337 DataTime=0.232 Loss=2.153 Prec@1=50.519 Prec@5=75.450 rate=2.98 Hz, eta=0:00:34, total=0:13:27, wall=10:34 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.337 DataTime=0.232 Loss=2.153 Prec@1=50.519 Prec@5=75.450 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:34 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.337 DataTime=0.232 Loss=2.153 Prec@1=50.519 Prec@5=75.450 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:34 IST=> training   99.92% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.337 DataTime=0.232 Loss=2.152 Prec@1=50.520 Prec@5=75.451 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:34 IST=> training   100.00% of 1x2503...Epoch=46/150 LR=0.07939 Time=0.337 DataTime=0.232 Loss=2.152 Prec@1=50.520 Prec@5=75.451 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> validation 0.00% of 1x98...Epoch=46/150 LR=0.07939 Time=6.305 Loss=2.718 Prec@1=42.969 Prec@5=66.797 rate=0 Hz, eta=?, total=0:00:00, wall=10:34 IST=> validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=6.305 Loss=2.718 Prec@1=42.969 Prec@5=66.797 rate=1917.19 Hz, eta=0:00:00, total=0:00:00, wall=10:34 IST** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=6.305 Loss=2.718 Prec@1=42.969 Prec@5=66.797 rate=1917.19 Hz, eta=0:00:00, total=0:00:00, wall=10:35 IST** validation 1.02% of 1x98...Epoch=46/150 LR=0.07939 Time=0.392 Loss=2.724 Prec@1=40.312 Prec@5=65.912 rate=1917.19 Hz, eta=0:00:00, total=0:00:00, wall=10:35 IST** validation 100.00% of 1x98...Epoch=46/150 LR=0.07939 Time=0.392 Loss=2.724 Prec@1=40.312 Prec@5=65.912 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=10:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:35 IST=> training   0.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.563 DataTime=4.378 Loss=2.225 Prec@1=51.367 Prec@5=72.656 rate=0 Hz, eta=?, total=0:00:00, wall=10:35 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.563 DataTime=4.378 Loss=2.225 Prec@1=51.367 Prec@5=72.656 rate=2262.15 Hz, eta=0:00:01, total=0:00:00, wall=10:35 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=4.563 DataTime=4.378 Loss=2.225 Prec@1=51.367 Prec@5=72.656 rate=2262.15 Hz, eta=0:00:01, total=0:00:00, wall=10:35 IST=> training   0.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.381 DataTime=0.290 Loss=2.127 Prec@1=51.100 Prec@5=75.791 rate=2262.15 Hz, eta=0:00:01, total=0:00:00, wall=10:35 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.381 DataTime=0.290 Loss=2.127 Prec@1=51.100 Prec@5=75.791 rate=2.98 Hz, eta=0:13:25, total=0:00:33, wall=10:35 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.381 DataTime=0.290 Loss=2.127 Prec@1=51.100 Prec@5=75.791 rate=2.98 Hz, eta=0:13:25, total=0:00:33, wall=10:36 IST=> training   4.04% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.360 DataTime=0.260 Loss=2.123 Prec@1=51.013 Prec@5=75.938 rate=2.98 Hz, eta=0:13:25, total=0:00:33, wall=10:36 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.360 DataTime=0.260 Loss=2.123 Prec@1=51.013 Prec@5=75.938 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=10:36 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.360 DataTime=0.260 Loss=2.123 Prec@1=51.013 Prec@5=75.938 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=10:36 IST=> training   8.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.350 DataTime=0.248 Loss=2.126 Prec@1=50.968 Prec@5=75.908 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=10:36 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.350 DataTime=0.248 Loss=2.126 Prec@1=50.968 Prec@5=75.908 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=10:36 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.350 DataTime=0.248 Loss=2.126 Prec@1=50.968 Prec@5=75.908 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=10:37 IST=> training   12.03% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.351 DataTime=0.249 Loss=2.124 Prec@1=51.013 Prec@5=75.923 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=10:37 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.351 DataTime=0.249 Loss=2.124 Prec@1=51.013 Prec@5=75.923 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=10:37 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.351 DataTime=0.249 Loss=2.124 Prec@1=51.013 Prec@5=75.923 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=10:37 IST=> training   16.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.347 DataTime=0.244 Loss=2.124 Prec@1=51.016 Prec@5=75.892 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=10:37 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.347 DataTime=0.244 Loss=2.124 Prec@1=51.016 Prec@5=75.892 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=10:37 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.347 DataTime=0.244 Loss=2.124 Prec@1=51.016 Prec@5=75.892 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=10:38 IST=> training   20.02% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.344 DataTime=0.241 Loss=2.129 Prec@1=50.927 Prec@5=75.849 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=10:38 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.344 DataTime=0.241 Loss=2.129 Prec@1=50.927 Prec@5=75.849 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=10:38 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.344 DataTime=0.241 Loss=2.129 Prec@1=50.927 Prec@5=75.849 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=10:39 IST=> training   24.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.346 DataTime=0.243 Loss=2.129 Prec@1=50.909 Prec@5=75.824 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=10:39 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.346 DataTime=0.243 Loss=2.129 Prec@1=50.909 Prec@5=75.824 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=10:39 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.346 DataTime=0.243 Loss=2.129 Prec@1=50.909 Prec@5=75.824 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=10:39 IST=> training   28.01% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.343 DataTime=0.240 Loss=2.129 Prec@1=50.912 Prec@5=75.820 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=10:39 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.343 DataTime=0.240 Loss=2.129 Prec@1=50.912 Prec@5=75.820 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=10:39 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.343 DataTime=0.240 Loss=2.129 Prec@1=50.912 Prec@5=75.820 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=10:40 IST=> training   32.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.342 DataTime=0.238 Loss=2.130 Prec@1=50.912 Prec@5=75.798 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=10:40 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.342 DataTime=0.238 Loss=2.130 Prec@1=50.912 Prec@5=75.798 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:40 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.342 DataTime=0.238 Loss=2.130 Prec@1=50.912 Prec@5=75.798 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:40 IST=> training   36.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.131 Prec@1=50.893 Prec@5=75.777 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=10:40 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.131 Prec@1=50.893 Prec@5=75.777 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=10:40 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.131 Prec@1=50.893 Prec@5=75.777 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=10:41 IST=> training   39.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.863 Prec@5=75.748 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=10:41 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.863 Prec@5=75.748 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=10:41 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.132 Prec@1=50.863 Prec@5=75.748 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=10:41 IST=> training   43.99% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.134 Prec@1=50.853 Prec@5=75.731 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=10:41 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.134 Prec@1=50.853 Prec@5=75.731 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:41 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.134 Prec@1=50.853 Prec@5=75.731 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:42 IST=> training   47.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.819 Prec@5=75.692 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:42 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.819 Prec@5=75.692 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=10:42 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.819 Prec@5=75.692 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=10:43 IST=> training   51.98% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.810 Prec@5=75.693 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=10:43 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.810 Prec@5=75.693 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=10:43 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.136 Prec@1=50.810 Prec@5=75.693 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=10:43 IST=> training   55.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.678 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=10:43 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.678 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:43 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.678 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:44 IST=> training   59.97% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.669 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:44 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.669 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:44 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.341 DataTime=0.238 Loss=2.138 Prec@1=50.791 Prec@5=75.669 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:44 IST=> training   63.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.765 Prec@5=75.658 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:44 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.765 Prec@5=75.658 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:44 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.139 Prec@1=50.765 Prec@5=75.658 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:45 IST=> training   67.96% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.748 Prec@5=75.643 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:45 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.748 Prec@5=75.643 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=10:45 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.748 Prec@5=75.643 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=10:45 IST=> training   71.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.736 Prec@5=75.627 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=10:45 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.736 Prec@5=75.627 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=10:45 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.237 Loss=2.140 Prec@1=50.736 Prec@5=75.627 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=10:46 IST=> training   75.95% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.698 Prec@5=75.611 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=10:46 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.698 Prec@5=75.611 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=10:46 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.142 Prec@1=50.698 Prec@5=75.611 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=10:46 IST=> training   79.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.143 Prec@1=50.684 Prec@5=75.593 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=10:46 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.143 Prec@1=50.684 Prec@5=75.593 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:46 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.143 Prec@1=50.684 Prec@5=75.593 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:47 IST=> training   83.94% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.144 Prec@1=50.665 Prec@5=75.578 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:47 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.144 Prec@1=50.665 Prec@5=75.578 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:47 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.144 Prec@1=50.665 Prec@5=75.578 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:48 IST=> training   87.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.145 Prec@1=50.660 Prec@5=75.573 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:48 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.145 Prec@1=50.660 Prec@5=75.573 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:48 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.340 DataTime=0.236 Loss=2.145 Prec@1=50.660 Prec@5=75.573 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:48 IST=> training   91.93% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.236 Loss=2.146 Prec@1=50.646 Prec@5=75.566 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=10:48 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.236 Loss=2.146 Prec@1=50.646 Prec@5=75.566 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:48 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.236 Loss=2.146 Prec@1=50.646 Prec@5=75.566 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:49 IST=> training   95.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.235 Loss=2.147 Prec@1=50.633 Prec@5=75.544 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=10:49 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.235 Loss=2.147 Prec@1=50.633 Prec@5=75.544 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:49 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.235 Loss=2.147 Prec@1=50.633 Prec@5=75.544 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:49 IST=> training   99.92% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.235 Loss=2.147 Prec@1=50.633 Prec@5=75.544 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:49 IST=> training   100.00% of 1x2503...Epoch=47/150 LR=0.07854 Time=0.339 DataTime=0.235 Loss=2.147 Prec@1=50.633 Prec@5=75.544 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=10:49 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:49 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:49 IST=> validation 0.00% of 1x98...Epoch=47/150 LR=0.07854 Time=6.113 Loss=2.652 Prec@1=43.359 Prec@5=67.383 rate=0 Hz, eta=?, total=0:00:00, wall=10:49 IST=> validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=6.113 Loss=2.652 Prec@1=43.359 Prec@5=67.383 rate=5927.75 Hz, eta=0:00:00, total=0:00:00, wall=10:49 IST** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=6.113 Loss=2.652 Prec@1=43.359 Prec@5=67.383 rate=5927.75 Hz, eta=0:00:00, total=0:00:00, wall=10:49 IST** validation 1.02% of 1x98...Epoch=47/150 LR=0.07854 Time=0.404 Loss=2.609 Prec@1=42.304 Prec@5=67.928 rate=5927.75 Hz, eta=0:00:00, total=0:00:00, wall=10:49 IST** validation 100.00% of 1x98...Epoch=47/150 LR=0.07854 Time=0.404 Loss=2.609 Prec@1=42.304 Prec@5=67.928 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=10:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:50 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:50 IST=> training   0.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.189 DataTime=4.996 Loss=2.266 Prec@1=47.266 Prec@5=72.852 rate=0 Hz, eta=?, total=0:00:00, wall=10:50 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.189 DataTime=4.996 Loss=2.266 Prec@1=47.266 Prec@5=72.852 rate=3776.41 Hz, eta=0:00:00, total=0:00:00, wall=10:50 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=5.189 DataTime=4.996 Loss=2.266 Prec@1=47.266 Prec@5=72.852 rate=3776.41 Hz, eta=0:00:00, total=0:00:00, wall=10:50 IST=> training   0.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.382 DataTime=0.283 Loss=2.118 Prec@1=51.156 Prec@5=75.990 rate=3776.41 Hz, eta=0:00:00, total=0:00:00, wall=10:50 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.382 DataTime=0.283 Loss=2.118 Prec@1=51.156 Prec@5=75.990 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=10:50 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.382 DataTime=0.283 Loss=2.118 Prec@1=51.156 Prec@5=75.990 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=10:51 IST=> training   4.04% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.355 DataTime=0.257 Loss=2.113 Prec@1=51.135 Prec@5=76.033 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=10:51 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.355 DataTime=0.257 Loss=2.113 Prec@1=51.135 Prec@5=76.033 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=10:51 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.355 DataTime=0.257 Loss=2.113 Prec@1=51.135 Prec@5=76.033 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=10:51 IST=> training   8.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.350 DataTime=0.249 Loss=2.116 Prec@1=51.084 Prec@5=75.956 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=10:51 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.350 DataTime=0.249 Loss=2.116 Prec@1=51.084 Prec@5=75.956 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=10:51 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.350 DataTime=0.249 Loss=2.116 Prec@1=51.084 Prec@5=75.956 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=10:52 IST=> training   12.03% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.246 Loss=2.116 Prec@1=51.110 Prec@5=75.986 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=10:52 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.246 Loss=2.116 Prec@1=51.110 Prec@5=75.986 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=10:52 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.246 Loss=2.116 Prec@1=51.110 Prec@5=75.986 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=10:52 IST=> training   16.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.245 Loss=2.119 Prec@1=51.117 Prec@5=75.919 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=10:52 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.245 Loss=2.119 Prec@1=51.117 Prec@5=75.919 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:52 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.346 DataTime=0.245 Loss=2.119 Prec@1=51.117 Prec@5=75.919 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:53 IST=> training   20.02% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.122 Prec@1=51.083 Prec@5=75.886 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:53 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.122 Prec@1=51.083 Prec@5=75.886 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:53 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.122 Prec@1=51.083 Prec@5=75.886 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:53 IST=> training   24.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.126 Prec@1=51.026 Prec@5=75.813 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:53 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.126 Prec@1=51.026 Prec@5=75.813 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=10:53 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.344 DataTime=0.242 Loss=2.126 Prec@1=51.026 Prec@5=75.813 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=10:54 IST=> training   28.01% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.241 Loss=2.128 Prec@1=50.951 Prec@5=75.790 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=10:54 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.241 Loss=2.128 Prec@1=50.951 Prec@5=75.790 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=10:54 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.241 Loss=2.128 Prec@1=50.951 Prec@5=75.790 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=10:55 IST=> training   32.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.240 Loss=2.129 Prec@1=50.950 Prec@5=75.771 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=10:55 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.240 Loss=2.129 Prec@1=50.950 Prec@5=75.771 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=10:55 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.240 Loss=2.129 Prec@1=50.950 Prec@5=75.771 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=10:55 IST=> training   36.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.240 Loss=2.130 Prec@1=50.908 Prec@5=75.767 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=10:55 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.240 Loss=2.130 Prec@1=50.908 Prec@5=75.767 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=10:55 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.343 DataTime=0.240 Loss=2.130 Prec@1=50.908 Prec@5=75.767 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=10:56 IST=> training   39.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.132 Prec@1=50.856 Prec@5=75.745 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=10:56 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.132 Prec@1=50.856 Prec@5=75.745 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=10:56 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.132 Prec@1=50.856 Prec@5=75.745 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=10:56 IST=> training   43.99% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.238 Loss=2.133 Prec@1=50.821 Prec@5=75.729 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=10:56 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.238 Loss=2.133 Prec@1=50.821 Prec@5=75.729 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=10:56 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.238 Loss=2.133 Prec@1=50.821 Prec@5=75.729 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=10:57 IST=> training   47.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.134 Prec@1=50.797 Prec@5=75.721 rate=2.97 Hz, eta=0:07:17, total=0:06:44, wall=10:57 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.134 Prec@1=50.797 Prec@5=75.721 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=10:57 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.134 Prec@1=50.797 Prec@5=75.721 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=10:57 IST=> training   51.98% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.135 Prec@1=50.788 Prec@5=75.695 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=10:57 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.135 Prec@1=50.788 Prec@5=75.695 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=10:57 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.342 DataTime=0.239 Loss=2.135 Prec@1=50.788 Prec@5=75.695 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=10:58 IST=> training   55.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.135 Prec@1=50.790 Prec@5=75.694 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=10:58 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.135 Prec@1=50.790 Prec@5=75.694 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:58 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.135 Prec@1=50.790 Prec@5=75.694 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:59 IST=> training   59.97% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.136 Prec@1=50.780 Prec@5=75.670 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=10:59 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.136 Prec@1=50.780 Prec@5=75.670 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:59 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.237 Loss=2.136 Prec@1=50.780 Prec@5=75.670 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:59 IST=> training   63.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.138 Prec@1=50.750 Prec@5=75.645 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=10:59 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.138 Prec@1=50.750 Prec@5=75.645 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:59 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.138 Prec@1=50.750 Prec@5=75.645 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=11:00 IST=> training   67.96% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.138 Prec@1=50.746 Prec@5=75.652 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=11:00 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.138 Prec@1=50.746 Prec@5=75.652 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=11:00 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.138 Prec@1=50.746 Prec@5=75.652 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=11:00 IST=> training   71.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.139 Prec@1=50.743 Prec@5=75.637 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=11:00 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.139 Prec@1=50.743 Prec@5=75.637 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:00 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.139 Prec@1=50.743 Prec@5=75.637 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:01 IST=> training   75.95% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.140 Prec@1=50.738 Prec@5=75.628 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:01 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.140 Prec@1=50.738 Prec@5=75.628 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:01 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.341 DataTime=0.236 Loss=2.140 Prec@1=50.738 Prec@5=75.628 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:01 IST=> training   79.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.140 Prec@1=50.734 Prec@5=75.616 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:01 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.140 Prec@1=50.734 Prec@5=75.616 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:01 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.140 Prec@1=50.734 Prec@5=75.616 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:02 IST=> training   83.94% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.141 Prec@1=50.711 Prec@5=75.596 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:02 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.141 Prec@1=50.711 Prec@5=75.596 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=11:02 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.141 Prec@1=50.711 Prec@5=75.596 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=11:02 IST=> training   87.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.708 Prec@5=75.585 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=11:02 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.708 Prec@5=75.585 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=11:02 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.708 Prec@5=75.585 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=11:03 IST=> training   91.93% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.701 Prec@5=75.575 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=11:03 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.701 Prec@5=75.575 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:03 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.340 DataTime=0.235 Loss=2.142 Prec@1=50.701 Prec@5=75.575 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:04 IST=> training   95.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.339 DataTime=0.234 Loss=2.143 Prec@1=50.693 Prec@5=75.563 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:04 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.339 DataTime=0.234 Loss=2.143 Prec@1=50.693 Prec@5=75.563 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=11:04 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.339 DataTime=0.234 Loss=2.143 Prec@1=50.693 Prec@5=75.563 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=11:04 IST=> training   99.92% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.339 DataTime=0.234 Loss=2.143 Prec@1=50.693 Prec@5=75.564 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=11:04 IST=> training   100.00% of 1x2503...Epoch=48/150 LR=0.07767 Time=0.339 DataTime=0.234 Loss=2.143 Prec@1=50.693 Prec@5=75.564 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=11:04 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> validation 0.00% of 1x98...Epoch=48/150 LR=0.07767 Time=6.110 Loss=2.454 Prec@1=44.922 Prec@5=69.141 rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=6.110 Loss=2.454 Prec@1=44.922 Prec@5=69.141 rate=2364.27 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=6.110 Loss=2.454 Prec@1=44.922 Prec@5=69.141 rate=2364.27 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST** validation 1.02% of 1x98...Epoch=48/150 LR=0.07767 Time=0.398 Loss=2.324 Prec@1=46.846 Prec@5=72.784 rate=2364.27 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST** validation 100.00% of 1x98...Epoch=48/150 LR=0.07767 Time=0.398 Loss=2.324 Prec@1=46.846 Prec@5=72.784 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=11:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> training   0.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.866 DataTime=5.527 Loss=2.093 Prec@1=53.516 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=11:04 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.866 DataTime=5.527 Loss=2.093 Prec@1=53.516 Prec@5=74.609 rate=9980.05 Hz, eta=0:00:00, total=0:00:00, wall=11:04 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=5.866 DataTime=5.527 Loss=2.093 Prec@1=53.516 Prec@5=74.609 rate=9980.05 Hz, eta=0:00:00, total=0:00:00, wall=11:05 IST=> training   0.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.375 DataTime=0.279 Loss=2.108 Prec@1=51.400 Prec@5=76.125 rate=9980.05 Hz, eta=0:00:00, total=0:00:00, wall=11:05 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.375 DataTime=0.279 Loss=2.108 Prec@1=51.400 Prec@5=76.125 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=11:05 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.375 DataTime=0.279 Loss=2.108 Prec@1=51.400 Prec@5=76.125 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=11:05 IST=> training   4.04% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.354 DataTime=0.258 Loss=2.111 Prec@1=51.208 Prec@5=76.125 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=11:05 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.354 DataTime=0.258 Loss=2.111 Prec@1=51.208 Prec@5=76.125 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=11:05 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.354 DataTime=0.258 Loss=2.111 Prec@1=51.208 Prec@5=76.125 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=11:06 IST=> training   8.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.351 DataTime=0.252 Loss=2.108 Prec@1=51.214 Prec@5=76.139 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=11:06 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.351 DataTime=0.252 Loss=2.108 Prec@1=51.214 Prec@5=76.139 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=11:06 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.351 DataTime=0.252 Loss=2.108 Prec@1=51.214 Prec@5=76.139 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=11:07 IST=> training   12.03% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.345 DataTime=0.244 Loss=2.111 Prec@1=51.174 Prec@5=76.135 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=11:07 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.345 DataTime=0.244 Loss=2.111 Prec@1=51.174 Prec@5=76.135 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=11:07 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.345 DataTime=0.244 Loss=2.111 Prec@1=51.174 Prec@5=76.135 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=11:07 IST=> training   16.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.239 Loss=2.112 Prec@1=51.211 Prec@5=76.092 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=11:07 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.239 Loss=2.112 Prec@1=51.211 Prec@5=76.092 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=11:07 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.239 Loss=2.112 Prec@1=51.211 Prec@5=76.092 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=11:08 IST=> training   20.02% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.342 DataTime=0.240 Loss=2.115 Prec@1=51.206 Prec@5=76.022 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=11:08 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.342 DataTime=0.240 Loss=2.115 Prec@1=51.206 Prec@5=76.022 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=11:08 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.342 DataTime=0.240 Loss=2.115 Prec@1=51.206 Prec@5=76.022 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=11:08 IST=> training   24.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.237 Loss=2.118 Prec@1=51.176 Prec@5=75.986 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=11:08 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.237 Loss=2.118 Prec@1=51.176 Prec@5=75.986 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=11:08 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.237 Loss=2.118 Prec@1=51.176 Prec@5=75.986 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=11:09 IST=> training   28.01% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.236 Loss=2.121 Prec@1=51.091 Prec@5=75.930 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=11:09 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.236 Loss=2.121 Prec@1=51.091 Prec@5=75.930 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=11:09 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.236 Loss=2.121 Prec@1=51.091 Prec@5=75.930 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=11:09 IST=> training   32.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.050 Prec@5=75.871 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=11:09 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.050 Prec@5=75.871 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=11:09 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.050 Prec@5=75.871 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=11:10 IST=> training   36.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.047 Prec@5=75.843 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=11:10 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.047 Prec@5=75.843 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=11:10 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.047 Prec@5=75.843 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=11:10 IST=> training   39.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.123 Prec@1=51.077 Prec@5=75.867 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=11:10 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.123 Prec@1=51.077 Prec@5=75.867 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=11:10 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.123 Prec@1=51.077 Prec@5=75.867 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=11:11 IST=> training   43.99% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.040 Prec@5=75.855 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=11:11 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.040 Prec@5=75.855 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=11:11 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.040 Prec@5=75.855 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=11:12 IST=> training   47.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.126 Prec@1=51.051 Prec@5=75.830 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=11:12 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.126 Prec@1=51.051 Prec@5=75.830 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=11:12 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.339 DataTime=0.234 Loss=2.126 Prec@1=51.051 Prec@5=75.830 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=11:12 IST=> training   51.98% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.126 Prec@1=51.057 Prec@5=75.827 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=11:12 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.126 Prec@1=51.057 Prec@5=75.827 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=11:12 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.126 Prec@1=51.057 Prec@5=75.827 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=11:13 IST=> training   55.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.126 Prec@1=51.055 Prec@5=75.822 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=11:13 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.126 Prec@1=51.055 Prec@5=75.822 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=11:13 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.126 Prec@1=51.055 Prec@5=75.822 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=11:13 IST=> training   59.97% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.128 Prec@1=51.048 Prec@5=75.799 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=11:13 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.128 Prec@1=51.048 Prec@5=75.799 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:13 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.233 Loss=2.128 Prec@1=51.048 Prec@5=75.799 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:14 IST=> training   63.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.128 Prec@1=51.032 Prec@5=75.793 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:14 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.128 Prec@1=51.032 Prec@5=75.793 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=11:14 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.128 Prec@1=51.032 Prec@5=75.793 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=11:14 IST=> training   67.96% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.130 Prec@1=51.007 Prec@5=75.764 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=11:14 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.130 Prec@1=51.007 Prec@5=75.764 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=11:14 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.338 DataTime=0.232 Loss=2.130 Prec@1=51.007 Prec@5=75.764 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=11:15 IST=> training   71.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.232 Loss=2.132 Prec@1=50.976 Prec@5=75.747 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=11:15 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.232 Loss=2.132 Prec@1=50.976 Prec@5=75.747 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=11:15 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.232 Loss=2.132 Prec@1=50.976 Prec@5=75.747 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=11:15 IST=> training   75.95% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.131 Prec@1=50.986 Prec@5=75.749 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=11:15 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.131 Prec@1=50.986 Prec@5=75.749 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=11:15 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.131 Prec@1=50.986 Prec@5=75.749 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=11:16 IST=> training   79.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.133 Prec@1=50.960 Prec@5=75.733 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=11:16 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.133 Prec@1=50.960 Prec@5=75.733 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=11:16 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.133 Prec@1=50.960 Prec@5=75.733 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=11:17 IST=> training   83.94% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.133 Prec@1=50.945 Prec@5=75.715 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=11:17 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.133 Prec@1=50.945 Prec@5=75.715 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=11:17 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.133 Prec@1=50.945 Prec@5=75.715 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=11:17 IST=> training   87.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.134 Prec@1=50.937 Prec@5=75.701 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=11:17 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.134 Prec@1=50.937 Prec@5=75.701 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=11:17 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.134 Prec@1=50.937 Prec@5=75.701 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=11:18 IST=> training   91.93% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.135 Prec@1=50.925 Prec@5=75.679 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=11:18 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.135 Prec@1=50.925 Prec@5=75.679 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=11:18 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.337 DataTime=0.231 Loss=2.135 Prec@1=50.925 Prec@5=75.679 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=11:18 IST=> training   95.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.136 Prec@1=50.914 Prec@5=75.668 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=11:18 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.136 Prec@1=50.914 Prec@5=75.668 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=11:18 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.136 Prec@1=50.914 Prec@5=75.668 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=11:18 IST=> training   99.92% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.136 Prec@1=50.915 Prec@5=75.668 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=11:18 IST=> training   100.00% of 1x2503...Epoch=49/150 LR=0.07679 Time=0.336 DataTime=0.230 Loss=2.136 Prec@1=50.915 Prec@5=75.668 rate=3.00 Hz, eta=0:00:00, total=0:13:55, wall=11:18 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:18 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:18 IST=> validation 0.00% of 1x98...Epoch=49/150 LR=0.07679 Time=6.886 Loss=2.312 Prec@1=47.852 Prec@5=73.438 rate=0 Hz, eta=?, total=0:00:00, wall=11:18 IST=> validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.886 Loss=2.312 Prec@1=47.852 Prec@5=73.438 rate=6232.05 Hz, eta=0:00:00, total=0:00:00, wall=11:18 IST** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=6.886 Loss=2.312 Prec@1=47.852 Prec@5=73.438 rate=6232.05 Hz, eta=0:00:00, total=0:00:00, wall=11:19 IST** validation 1.02% of 1x98...Epoch=49/150 LR=0.07679 Time=0.404 Loss=2.194 Prec@1=49.626 Prec@5=74.978 rate=6232.05 Hz, eta=0:00:00, total=0:00:00, wall=11:19 IST** validation 100.00% of 1x98...Epoch=49/150 LR=0.07679 Time=0.404 Loss=2.194 Prec@1=49.626 Prec@5=74.978 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=11:19 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST=> training   0.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.083 DataTime=5.993 Loss=1.934 Prec@1=56.445 Prec@5=78.711 rate=0 Hz, eta=?, total=0:00:00, wall=11:19 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.083 DataTime=5.993 Loss=1.934 Prec@1=56.445 Prec@5=78.711 rate=7270.83 Hz, eta=0:00:00, total=0:00:00, wall=11:19 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=6.083 DataTime=5.993 Loss=1.934 Prec@1=56.445 Prec@5=78.711 rate=7270.83 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST=> training   0.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.381 DataTime=0.289 Loss=2.097 Prec@1=51.557 Prec@5=76.271 rate=7270.83 Hz, eta=0:00:00, total=0:00:00, wall=11:20 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.381 DataTime=0.289 Loss=2.097 Prec@1=51.557 Prec@5=76.271 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=11:20 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.381 DataTime=0.289 Loss=2.097 Prec@1=51.557 Prec@5=76.271 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=11:20 IST=> training   4.04% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.353 DataTime=0.259 Loss=2.100 Prec@1=51.533 Prec@5=76.240 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=11:20 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.353 DataTime=0.259 Loss=2.100 Prec@1=51.533 Prec@5=76.240 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=11:20 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.353 DataTime=0.259 Loss=2.100 Prec@1=51.533 Prec@5=76.240 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=11:21 IST=> training   8.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.348 DataTime=0.251 Loss=2.098 Prec@1=51.537 Prec@5=76.286 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=11:21 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.348 DataTime=0.251 Loss=2.098 Prec@1=51.537 Prec@5=76.286 rate=3.05 Hz, eta=0:12:00, total=0:01:38, wall=11:21 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.348 DataTime=0.251 Loss=2.098 Prec@1=51.537 Prec@5=76.286 rate=3.05 Hz, eta=0:12:00, total=0:01:38, wall=11:21 IST=> training   12.03% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.347 DataTime=0.249 Loss=2.101 Prec@1=51.466 Prec@5=76.225 rate=3.05 Hz, eta=0:12:00, total=0:01:38, wall=11:21 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.347 DataTime=0.249 Loss=2.101 Prec@1=51.466 Prec@5=76.225 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:21 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.347 DataTime=0.249 Loss=2.101 Prec@1=51.466 Prec@5=76.225 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:22 IST=> training   16.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.344 DataTime=0.245 Loss=2.105 Prec@1=51.390 Prec@5=76.116 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:22 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.344 DataTime=0.245 Loss=2.105 Prec@1=51.390 Prec@5=76.116 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:22 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.344 DataTime=0.245 Loss=2.105 Prec@1=51.390 Prec@5=76.116 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:22 IST=> training   20.02% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.240 Loss=2.107 Prec@1=51.419 Prec@5=76.105 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:22 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.240 Loss=2.107 Prec@1=51.419 Prec@5=76.105 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=11:22 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.240 Loss=2.107 Prec@1=51.419 Prec@5=76.105 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=11:23 IST=> training   24.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.241 Loss=2.109 Prec@1=51.329 Prec@5=76.086 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=11:23 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.241 Loss=2.109 Prec@1=51.329 Prec@5=76.086 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=11:23 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.241 Loss=2.109 Prec@1=51.329 Prec@5=76.086 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=11:24 IST=> training   28.01% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.240 Loss=2.111 Prec@1=51.292 Prec@5=76.041 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=11:24 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.240 Loss=2.111 Prec@1=51.292 Prec@5=76.041 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=11:24 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.240 Loss=2.111 Prec@1=51.292 Prec@5=76.041 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=11:24 IST=> training   32.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.112 Prec@1=51.284 Prec@5=76.026 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=11:24 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.112 Prec@1=51.284 Prec@5=76.026 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=11:24 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.112 Prec@1=51.284 Prec@5=76.026 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=11:25 IST=> training   36.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.113 Prec@1=51.268 Prec@5=76.000 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=11:25 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.113 Prec@1=51.268 Prec@5=76.000 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=11:25 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.342 DataTime=0.239 Loss=2.113 Prec@1=51.268 Prec@5=76.000 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=11:25 IST=> training   39.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.238 Loss=2.113 Prec@1=51.269 Prec@5=76.013 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=11:25 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.238 Loss=2.113 Prec@1=51.269 Prec@5=76.013 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=11:25 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.238 Loss=2.113 Prec@1=51.269 Prec@5=76.013 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=11:26 IST=> training   43.99% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.114 Prec@1=51.254 Prec@5=75.986 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=11:26 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.114 Prec@1=51.254 Prec@5=75.986 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=11:26 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.114 Prec@1=51.254 Prec@5=75.986 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=11:26 IST=> training   47.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.116 Prec@1=51.212 Prec@5=75.978 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=11:26 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.116 Prec@1=51.212 Prec@5=75.978 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=11:26 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.237 Loss=2.116 Prec@1=51.212 Prec@5=75.978 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=11:27 IST=> training   51.98% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.117 Prec@1=51.191 Prec@5=75.961 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=11:27 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.117 Prec@1=51.191 Prec@5=75.961 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=11:27 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.117 Prec@1=51.191 Prec@5=75.961 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=11:27 IST=> training   55.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.119 Prec@1=51.165 Prec@5=75.926 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=11:27 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.119 Prec@1=51.165 Prec@5=75.926 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=11:27 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.236 Loss=2.119 Prec@1=51.165 Prec@5=75.926 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=11:28 IST=> training   59.97% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.236 Loss=2.120 Prec@1=51.157 Prec@5=75.914 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=11:28 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.236 Loss=2.120 Prec@1=51.157 Prec@5=75.914 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=11:28 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.341 DataTime=0.236 Loss=2.120 Prec@1=51.157 Prec@5=75.914 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=11:29 IST=> training   63.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.121 Prec@1=51.128 Prec@5=75.908 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=11:29 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.121 Prec@1=51.128 Prec@5=75.908 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=11:29 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.121 Prec@1=51.128 Prec@5=75.908 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=11:29 IST=> training   67.96% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.235 Loss=2.122 Prec@1=51.131 Prec@5=75.889 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=11:29 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.235 Loss=2.122 Prec@1=51.131 Prec@5=75.889 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=11:29 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.235 Loss=2.122 Prec@1=51.131 Prec@5=75.889 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=11:30 IST=> training   71.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.096 Prec@5=75.867 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=11:30 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.096 Prec@5=75.867 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=11:30 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.123 Prec@1=51.096 Prec@5=75.867 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=11:30 IST=> training   75.95% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.090 Prec@5=75.867 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=11:30 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.090 Prec@5=75.867 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=11:30 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.235 Loss=2.124 Prec@1=51.090 Prec@5=75.867 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=11:31 IST=> training   79.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.059 Prec@5=75.845 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=11:31 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.059 Prec@5=75.845 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=11:31 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.125 Prec@1=51.059 Prec@5=75.845 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=11:31 IST=> training   83.94% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.234 Loss=2.126 Prec@1=51.046 Prec@5=75.832 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=11:31 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.234 Loss=2.126 Prec@1=51.046 Prec@5=75.832 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=11:31 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.340 DataTime=0.234 Loss=2.126 Prec@1=51.046 Prec@5=75.832 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=11:32 IST=> training   87.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.127 Prec@1=51.023 Prec@5=75.820 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=11:32 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.127 Prec@1=51.023 Prec@5=75.820 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=11:32 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.234 Loss=2.127 Prec@1=51.023 Prec@5=75.820 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=11:33 IST=> training   91.93% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.233 Loss=2.128 Prec@1=51.009 Prec@5=75.802 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=11:33 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.233 Loss=2.128 Prec@1=51.009 Prec@5=75.802 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=11:33 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.339 DataTime=0.233 Loss=2.128 Prec@1=51.009 Prec@5=75.802 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=11:33 IST=> training   95.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.338 DataTime=0.233 Loss=2.129 Prec@1=51.000 Prec@5=75.787 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=11:33 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.338 DataTime=0.233 Loss=2.129 Prec@1=51.000 Prec@5=75.787 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=11:33 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.338 DataTime=0.233 Loss=2.129 Prec@1=51.000 Prec@5=75.787 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=11:33 IST=> training   99.92% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.338 DataTime=0.233 Loss=2.129 Prec@1=50.999 Prec@5=75.786 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=11:33 IST=> training   100.00% of 1x2503...Epoch=50/150 LR=0.07590 Time=0.338 DataTime=0.233 Loss=2.129 Prec@1=50.999 Prec@5=75.786 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=11:33 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:33 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:33 IST=> validation 0.00% of 1x98...Epoch=50/150 LR=0.07590 Time=6.611 Loss=2.366 Prec@1=46.875 Prec@5=71.289 rate=0 Hz, eta=?, total=0:00:00, wall=11:33 IST=> validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=6.611 Loss=2.366 Prec@1=46.875 Prec@5=71.289 rate=2023.37 Hz, eta=0:00:00, total=0:00:00, wall=11:33 IST** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=6.611 Loss=2.366 Prec@1=46.875 Prec@5=71.289 rate=2023.37 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST** validation 1.02% of 1x98...Epoch=50/150 LR=0.07590 Time=0.398 Loss=2.246 Prec@1=48.538 Prec@5=74.146 rate=2023.37 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST** validation 100.00% of 1x98...Epoch=50/150 LR=0.07590 Time=0.398 Loss=2.246 Prec@1=48.538 Prec@5=74.146 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=11:34 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> training   0.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.477 DataTime=5.329 Loss=2.026 Prec@1=49.805 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=11:34 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.477 DataTime=5.329 Loss=2.026 Prec@1=49.805 Prec@5=79.883 rate=3817.03 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=5.477 DataTime=5.329 Loss=2.026 Prec@1=49.805 Prec@5=79.883 rate=3817.03 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST=> training   0.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.382 DataTime=0.290 Loss=2.100 Prec@1=51.319 Prec@5=76.228 rate=3817.03 Hz, eta=0:00:00, total=0:00:00, wall=11:34 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.382 DataTime=0.290 Loss=2.100 Prec@1=51.319 Prec@5=76.228 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=11:34 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.382 DataTime=0.290 Loss=2.100 Prec@1=51.319 Prec@5=76.228 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=11:35 IST=> training   4.04% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.351 DataTime=0.256 Loss=2.094 Prec@1=51.492 Prec@5=76.282 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=11:35 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.351 DataTime=0.256 Loss=2.094 Prec@1=51.492 Prec@5=76.282 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=11:35 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.351 DataTime=0.256 Loss=2.094 Prec@1=51.492 Prec@5=76.282 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=11:35 IST=> training   8.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.247 Loss=2.096 Prec@1=51.529 Prec@5=76.254 rate=3.09 Hz, eta=0:12:25, total=0:01:05, wall=11:35 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.247 Loss=2.096 Prec@1=51.529 Prec@5=76.254 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:35 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.247 Loss=2.096 Prec@1=51.529 Prec@5=76.254 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:36 IST=> training   12.03% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.246 Loss=2.097 Prec@1=51.591 Prec@5=76.256 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:36 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.246 Loss=2.097 Prec@1=51.591 Prec@5=76.256 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:36 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.345 DataTime=0.246 Loss=2.097 Prec@1=51.591 Prec@5=76.256 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:37 IST=> training   16.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.342 DataTime=0.241 Loss=2.101 Prec@1=51.531 Prec@5=76.159 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:37 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.342 DataTime=0.241 Loss=2.101 Prec@1=51.531 Prec@5=76.159 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=11:37 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.342 DataTime=0.241 Loss=2.101 Prec@1=51.531 Prec@5=76.159 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=11:37 IST=> training   20.02% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.106 Prec@1=51.430 Prec@5=76.092 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=11:37 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.106 Prec@1=51.430 Prec@5=76.092 rate=3.03 Hz, eta=0:10:26, total=0:03:18, wall=11:37 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.106 Prec@1=51.430 Prec@5=76.092 rate=3.03 Hz, eta=0:10:26, total=0:03:18, wall=11:38 IST=> training   24.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.107 Prec@1=51.411 Prec@5=76.062 rate=3.03 Hz, eta=0:10:26, total=0:03:18, wall=11:38 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.107 Prec@1=51.411 Prec@5=76.062 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:38 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.107 Prec@1=51.411 Prec@5=76.062 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:38 IST=> training   28.01% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.109 Prec@1=51.397 Prec@5=76.056 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:38 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.109 Prec@1=51.397 Prec@5=76.056 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=11:38 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.109 Prec@1=51.397 Prec@5=76.056 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=11:39 IST=> training   32.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.378 Prec@5=76.051 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=11:39 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.378 Prec@5=76.051 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=11:39 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.378 Prec@5=76.051 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=11:39 IST=> training   36.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.112 Prec@1=51.336 Prec@5=76.018 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=11:39 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.112 Prec@1=51.336 Prec@5=76.018 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=11:39 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.340 DataTime=0.237 Loss=2.112 Prec@1=51.336 Prec@5=76.018 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=11:40 IST=> training   39.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.112 Prec@1=51.338 Prec@5=76.030 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=11:40 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.112 Prec@1=51.338 Prec@5=76.030 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:40 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.237 Loss=2.112 Prec@1=51.338 Prec@5=76.030 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:41 IST=> training   43.99% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.328 Prec@5=76.033 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:41 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.328 Prec@5=76.033 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:41 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.328 Prec@5=76.033 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:41 IST=> training   47.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.325 Prec@5=76.031 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:41 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.325 Prec@5=76.031 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:41 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.236 Loss=2.112 Prec@1=51.325 Prec@5=76.031 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:42 IST=> training   51.98% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.289 Prec@5=76.000 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:42 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.289 Prec@5=76.000 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:42 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.289 Prec@5=76.000 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:42 IST=> training   55.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.234 Loss=2.114 Prec@1=51.275 Prec@5=76.007 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:42 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.234 Loss=2.114 Prec@1=51.275 Prec@5=76.007 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=11:42 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.234 Loss=2.114 Prec@1=51.275 Prec@5=76.007 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=11:43 IST=> training   59.97% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.115 Prec@1=51.286 Prec@5=76.006 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=11:43 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.115 Prec@1=51.286 Prec@5=76.006 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:43 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.115 Prec@1=51.286 Prec@5=76.006 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:43 IST=> training   63.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.234 Loss=2.115 Prec@1=51.280 Prec@5=75.998 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=11:43 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.234 Loss=2.115 Prec@1=51.280 Prec@5=75.998 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:43 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.339 DataTime=0.234 Loss=2.115 Prec@1=51.280 Prec@5=75.998 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:44 IST=> training   67.96% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.116 Prec@1=51.272 Prec@5=75.990 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:44 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.116 Prec@1=51.272 Prec@5=75.990 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=11:44 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.116 Prec@1=51.272 Prec@5=75.990 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=11:44 IST=> training   71.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.118 Prec@1=51.240 Prec@5=75.948 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=11:44 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.118 Prec@1=51.240 Prec@5=75.948 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=11:44 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.118 Prec@1=51.240 Prec@5=75.948 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=11:45 IST=> training   75.95% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.119 Prec@1=51.210 Prec@5=75.928 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=11:45 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.119 Prec@1=51.210 Prec@5=75.928 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=11:45 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.119 Prec@1=51.210 Prec@5=75.928 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=11:46 IST=> training   79.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.120 Prec@1=51.196 Prec@5=75.918 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=11:46 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.120 Prec@1=51.196 Prec@5=75.918 rate=2.98 Hz, eta=0:02:14, total=0:11:43, wall=11:46 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.120 Prec@1=51.196 Prec@5=75.918 rate=2.98 Hz, eta=0:02:14, total=0:11:43, wall=11:46 IST=> training   83.94% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.337 DataTime=0.232 Loss=2.121 Prec@1=51.184 Prec@5=75.905 rate=2.98 Hz, eta=0:02:14, total=0:11:43, wall=11:46 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.337 DataTime=0.232 Loss=2.121 Prec@1=51.184 Prec@5=75.905 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=11:46 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.337 DataTime=0.232 Loss=2.121 Prec@1=51.184 Prec@5=75.905 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=11:47 IST=> training   87.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.122 Prec@1=51.160 Prec@5=75.878 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=11:47 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.122 Prec@1=51.160 Prec@5=75.878 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:47 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.233 Loss=2.122 Prec@1=51.160 Prec@5=75.878 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:47 IST=> training   91.93% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.122 Prec@1=51.155 Prec@5=75.870 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:47 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.122 Prec@1=51.155 Prec@5=75.870 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=11:47 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.122 Prec@1=51.155 Prec@5=75.870 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=11:48 IST=> training   95.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.123 Prec@1=51.153 Prec@5=75.866 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=11:48 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.123 Prec@1=51.153 Prec@5=75.866 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=11:48 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.338 DataTime=0.232 Loss=2.123 Prec@1=51.153 Prec@5=75.866 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=11:48 IST=> training   99.92% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.337 DataTime=0.232 Loss=2.123 Prec@1=51.152 Prec@5=75.866 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=11:48 IST=> training   100.00% of 1x2503...Epoch=51/150 LR=0.07500 Time=0.337 DataTime=0.232 Loss=2.123 Prec@1=51.152 Prec@5=75.866 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=11:48 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:48 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:48 IST=> validation 0.00% of 1x98...Epoch=51/150 LR=0.07500 Time=6.209 Loss=2.242 Prec@1=49.023 Prec@5=71.680 rate=0 Hz, eta=?, total=0:00:00, wall=11:48 IST=> validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.209 Loss=2.242 Prec@1=49.023 Prec@5=71.680 rate=3221.14 Hz, eta=0:00:00, total=0:00:00, wall=11:48 IST** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=6.209 Loss=2.242 Prec@1=49.023 Prec@5=71.680 rate=3221.14 Hz, eta=0:00:00, total=0:00:00, wall=11:48 IST** validation 1.02% of 1x98...Epoch=51/150 LR=0.07500 Time=0.404 Loss=2.340 Prec@1=47.232 Prec@5=72.616 rate=3221.14 Hz, eta=0:00:00, total=0:00:00, wall=11:48 IST** validation 100.00% of 1x98...Epoch=51/150 LR=0.07500 Time=0.404 Loss=2.340 Prec@1=47.232 Prec@5=72.616 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=11:48 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:49 IST=> training   0.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.862 DataTime=4.685 Loss=1.987 Prec@1=51.367 Prec@5=77.930 rate=0 Hz, eta=?, total=0:00:00, wall=11:49 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.862 DataTime=4.685 Loss=1.987 Prec@1=51.367 Prec@5=77.930 rate=4194.37 Hz, eta=0:00:00, total=0:00:00, wall=11:49 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=4.862 DataTime=4.685 Loss=1.987 Prec@1=51.367 Prec@5=77.930 rate=4194.37 Hz, eta=0:00:00, total=0:00:00, wall=11:49 IST=> training   0.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.376 DataTime=0.282 Loss=2.070 Prec@1=52.181 Prec@5=76.860 rate=4194.37 Hz, eta=0:00:00, total=0:00:00, wall=11:49 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.376 DataTime=0.282 Loss=2.070 Prec@1=52.181 Prec@5=76.860 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=11:49 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.376 DataTime=0.282 Loss=2.070 Prec@1=52.181 Prec@5=76.860 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=11:50 IST=> training   4.04% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.350 DataTime=0.255 Loss=2.079 Prec@1=51.957 Prec@5=76.751 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=11:50 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.350 DataTime=0.255 Loss=2.079 Prec@1=51.957 Prec@5=76.751 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=11:50 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.350 DataTime=0.255 Loss=2.079 Prec@1=51.957 Prec@5=76.751 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=11:50 IST=> training   8.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.343 DataTime=0.248 Loss=2.091 Prec@1=51.721 Prec@5=76.547 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=11:50 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.343 DataTime=0.248 Loss=2.091 Prec@1=51.721 Prec@5=76.547 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:50 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.343 DataTime=0.248 Loss=2.091 Prec@1=51.721 Prec@5=76.547 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:51 IST=> training   12.03% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.345 DataTime=0.250 Loss=2.091 Prec@1=51.712 Prec@5=76.488 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=11:51 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.345 DataTime=0.250 Loss=2.091 Prec@1=51.712 Prec@5=76.488 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=11:51 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.345 DataTime=0.250 Loss=2.091 Prec@1=51.712 Prec@5=76.488 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=11:51 IST=> training   16.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.245 Loss=2.094 Prec@1=51.642 Prec@5=76.432 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=11:51 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.245 Loss=2.094 Prec@1=51.642 Prec@5=76.432 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:51 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.245 Loss=2.094 Prec@1=51.642 Prec@5=76.432 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:52 IST=> training   20.02% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.243 Loss=2.096 Prec@1=51.573 Prec@5=76.383 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=11:52 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.243 Loss=2.096 Prec@1=51.573 Prec@5=76.383 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=11:52 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.243 Loss=2.096 Prec@1=51.573 Prec@5=76.383 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=11:52 IST=> training   24.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.241 Loss=2.098 Prec@1=51.554 Prec@5=76.367 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=11:52 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.241 Loss=2.098 Prec@1=51.554 Prec@5=76.367 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=11:52 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.241 Loss=2.098 Prec@1=51.554 Prec@5=76.367 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=11:53 IST=> training   28.01% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.242 Loss=2.097 Prec@1=51.531 Prec@5=76.370 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=11:53 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.242 Loss=2.097 Prec@1=51.531 Prec@5=76.370 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=11:53 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.342 DataTime=0.242 Loss=2.097 Prec@1=51.531 Prec@5=76.370 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=11:54 IST=> training   32.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.240 Loss=2.099 Prec@1=51.494 Prec@5=76.330 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=11:54 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.240 Loss=2.099 Prec@1=51.494 Prec@5=76.330 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:54 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.341 DataTime=0.240 Loss=2.099 Prec@1=51.494 Prec@5=76.330 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:54 IST=> training   36.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.101 Prec@1=51.470 Prec@5=76.291 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:54 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.101 Prec@1=51.470 Prec@5=76.291 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=11:54 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.101 Prec@1=51.470 Prec@5=76.291 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=11:55 IST=> training   39.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.103 Prec@1=51.445 Prec@5=76.262 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=11:55 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.103 Prec@1=51.445 Prec@5=76.262 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:55 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.103 Prec@1=51.445 Prec@5=76.262 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:55 IST=> training   43.99% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.106 Prec@1=51.375 Prec@5=76.221 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=11:55 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.106 Prec@1=51.375 Prec@5=76.221 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=11:55 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.238 Loss=2.106 Prec@1=51.375 Prec@5=76.221 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=11:56 IST=> training   47.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.237 Loss=2.107 Prec@1=51.372 Prec@5=76.205 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=11:56 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.237 Loss=2.107 Prec@1=51.372 Prec@5=76.205 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:56 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.237 Loss=2.107 Prec@1=51.372 Prec@5=76.205 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:56 IST=> training   51.98% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.109 Prec@1=51.342 Prec@5=76.182 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=11:56 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.109 Prec@1=51.342 Prec@5=76.182 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:56 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.109 Prec@1=51.342 Prec@5=76.182 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:57 IST=> training   55.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.316 Prec@5=76.130 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:57 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.316 Prec@5=76.130 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=11:57 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.237 Loss=2.110 Prec@1=51.316 Prec@5=76.130 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=11:58 IST=> training   59.97% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.124 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=11:58 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.124 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=11:58 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.124 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=11:58 IST=> training   63.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.113 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=11:58 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.113 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:58 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.113 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:59 IST=> training   67.96% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.120 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:59 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.120 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=11:59 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.236 Loss=2.111 Prec@1=51.302 Prec@5=76.120 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=11:59 IST=> training   71.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.301 Prec@5=76.104 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=11:59 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.301 Prec@5=76.104 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=11:59 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.301 Prec@5=76.104 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=12:00 IST=> training   75.95% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.096 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=12:00 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.096 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=12:00 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.293 Prec@5=76.096 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=12:00 IST=> training   79.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.291 Prec@5=76.101 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=12:00 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.291 Prec@5=76.101 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=12:00 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.112 Prec@1=51.291 Prec@5=76.101 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=12:01 IST=> training   83.94% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.113 Prec@1=51.290 Prec@5=76.099 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=12:01 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.113 Prec@1=51.290 Prec@5=76.099 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=12:01 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.113 Prec@1=51.290 Prec@5=76.099 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=12:02 IST=> training   87.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.271 Prec@5=76.084 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=12:02 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.271 Prec@5=76.084 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=12:02 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.339 DataTime=0.235 Loss=2.114 Prec@1=51.271 Prec@5=76.084 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=12:02 IST=> training   91.93% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.114 Prec@1=51.269 Prec@5=76.073 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=12:02 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.114 Prec@1=51.269 Prec@5=76.073 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=12:02 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.235 Loss=2.114 Prec@1=51.269 Prec@5=76.073 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=12:03 IST=> training   95.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.234 Loss=2.115 Prec@1=51.254 Prec@5=76.061 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=12:03 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.234 Loss=2.115 Prec@1=51.254 Prec@5=76.061 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=12:03 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.234 Loss=2.115 Prec@1=51.254 Prec@5=76.061 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=12:03 IST=> training   99.92% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.234 Loss=2.115 Prec@1=51.254 Prec@5=76.061 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=12:03 IST=> training   100.00% of 1x2503...Epoch=52/150 LR=0.07409 Time=0.338 DataTime=0.234 Loss=2.115 Prec@1=51.254 Prec@5=76.061 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=12:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> validation 0.00% of 1x98...Epoch=52/150 LR=0.07409 Time=6.996 Loss=2.424 Prec@1=47.656 Prec@5=68.555 rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.996 Loss=2.424 Prec@1=47.656 Prec@5=68.555 rate=5095.67 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=6.996 Loss=2.424 Prec@1=47.656 Prec@5=68.555 rate=5095.67 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST** validation 1.02% of 1x98...Epoch=52/150 LR=0.07409 Time=0.401 Loss=2.321 Prec@1=47.096 Prec@5=72.920 rate=5095.67 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST** validation 100.00% of 1x98...Epoch=52/150 LR=0.07409 Time=0.401 Loss=2.321 Prec@1=47.096 Prec@5=72.920 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=12:03 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> training   0.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=5.583 DataTime=5.417 Loss=2.162 Prec@1=50.586 Prec@5=76.758 rate=0 Hz, eta=?, total=0:00:00, wall=12:03 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=5.583 DataTime=5.417 Loss=2.162 Prec@1=50.586 Prec@5=76.758 rate=3141.46 Hz, eta=0:00:00, total=0:00:00, wall=12:03 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=5.583 DataTime=5.417 Loss=2.162 Prec@1=50.586 Prec@5=76.758 rate=3141.46 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST=> training   0.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.371 DataTime=0.274 Loss=2.078 Prec@1=52.040 Prec@5=76.650 rate=3141.46 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.371 DataTime=0.274 Loss=2.078 Prec@1=52.040 Prec@5=76.650 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=12:04 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.371 DataTime=0.274 Loss=2.078 Prec@1=52.040 Prec@5=76.650 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=12:04 IST=> training   4.04% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.349 DataTime=0.253 Loss=2.091 Prec@1=51.840 Prec@5=76.420 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=12:04 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.349 DataTime=0.253 Loss=2.091 Prec@1=51.840 Prec@5=76.420 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=12:04 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.349 DataTime=0.253 Loss=2.091 Prec@1=51.840 Prec@5=76.420 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=12:05 IST=> training   8.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.346 DataTime=0.246 Loss=2.093 Prec@1=51.684 Prec@5=76.382 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=12:05 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.346 DataTime=0.246 Loss=2.093 Prec@1=51.684 Prec@5=76.382 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=12:05 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.346 DataTime=0.246 Loss=2.093 Prec@1=51.684 Prec@5=76.382 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=12:06 IST=> training   12.03% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.341 DataTime=0.239 Loss=2.096 Prec@1=51.586 Prec@5=76.298 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=12:06 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.341 DataTime=0.239 Loss=2.096 Prec@1=51.586 Prec@5=76.298 rate=3.06 Hz, eta=0:11:26, total=0:02:11, wall=12:06 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.341 DataTime=0.239 Loss=2.096 Prec@1=51.586 Prec@5=76.298 rate=3.06 Hz, eta=0:11:26, total=0:02:11, wall=12:06 IST=> training   16.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.654 Prec@5=76.349 rate=3.06 Hz, eta=0:11:26, total=0:02:11, wall=12:06 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.654 Prec@5=76.349 rate=3.06 Hz, eta=0:10:55, total=0:02:43, wall=12:06 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.654 Prec@5=76.349 rate=3.06 Hz, eta=0:10:55, total=0:02:43, wall=12:07 IST=> training   20.02% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.239 Loss=2.092 Prec@1=51.637 Prec@5=76.350 rate=3.06 Hz, eta=0:10:55, total=0:02:43, wall=12:07 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.239 Loss=2.092 Prec@1=51.637 Prec@5=76.350 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=12:07 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.239 Loss=2.092 Prec@1=51.637 Prec@5=76.350 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=12:07 IST=> training   24.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.237 Loss=2.091 Prec@1=51.676 Prec@5=76.396 rate=3.03 Hz, eta=0:10:28, total=0:03:18, wall=12:07 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.237 Loss=2.091 Prec@1=51.676 Prec@5=76.396 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=12:07 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.237 Loss=2.091 Prec@1=51.676 Prec@5=76.396 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=12:08 IST=> training   28.01% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.093 Prec@1=51.645 Prec@5=76.352 rate=3.03 Hz, eta=0:09:54, total=0:03:51, wall=12:08 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.093 Prec@1=51.645 Prec@5=76.352 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=12:08 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.093 Prec@1=51.645 Prec@5=76.352 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=12:08 IST=> training   32.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.704 Prec@5=76.369 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=12:08 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.704 Prec@5=76.369 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=12:08 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.237 Loss=2.092 Prec@1=51.704 Prec@5=76.369 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=12:09 IST=> training   36.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.695 Prec@5=76.363 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=12:09 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.695 Prec@5=76.363 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=12:09 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.695 Prec@5=76.363 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=12:10 IST=> training   39.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.654 Prec@5=76.375 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=12:10 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.654 Prec@5=76.375 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=12:10 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.340 DataTime=0.237 Loss=2.093 Prec@1=51.654 Prec@5=76.375 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=12:10 IST=> training   43.99% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.236 Loss=2.095 Prec@1=51.611 Prec@5=76.357 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=12:10 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.236 Loss=2.095 Prec@1=51.611 Prec@5=76.357 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:10 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.339 DataTime=0.236 Loss=2.095 Prec@1=51.611 Prec@5=76.357 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:11 IST=> training   47.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.097 Prec@1=51.576 Prec@5=76.328 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:11 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.097 Prec@1=51.576 Prec@5=76.328 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=12:11 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.097 Prec@1=51.576 Prec@5=76.328 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=12:11 IST=> training   51.98% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.098 Prec@1=51.554 Prec@5=76.311 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=12:11 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.098 Prec@1=51.554 Prec@5=76.311 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=12:11 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.098 Prec@1=51.554 Prec@5=76.311 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=12:12 IST=> training   55.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.099 Prec@1=51.552 Prec@5=76.284 rate=2.99 Hz, eta=0:06:08, total=0:07:47, wall=12:12 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.099 Prec@1=51.552 Prec@5=76.284 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=12:12 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.235 Loss=2.099 Prec@1=51.552 Prec@5=76.284 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=12:12 IST=> training   59.97% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.100 Prec@1=51.520 Prec@5=76.272 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=12:12 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.100 Prec@1=51.520 Prec@5=76.272 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=12:12 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.100 Prec@1=51.520 Prec@5=76.272 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=12:13 IST=> training   63.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.101 Prec@1=51.505 Prec@5=76.256 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=12:13 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.101 Prec@1=51.505 Prec@5=76.256 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=12:13 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.101 Prec@1=51.505 Prec@5=76.256 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=12:13 IST=> training   67.96% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.103 Prec@1=51.467 Prec@5=76.238 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=12:13 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.103 Prec@1=51.467 Prec@5=76.238 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=12:13 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.103 Prec@1=51.467 Prec@5=76.238 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=12:14 IST=> training   71.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.103 Prec@1=51.457 Prec@5=76.229 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=12:14 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.103 Prec@1=51.457 Prec@5=76.229 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=12:14 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.234 Loss=2.103 Prec@1=51.457 Prec@5=76.229 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=12:15 IST=> training   75.95% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.105 Prec@1=51.428 Prec@5=76.203 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=12:15 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.105 Prec@1=51.428 Prec@5=76.203 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=12:15 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.234 Loss=2.105 Prec@1=51.428 Prec@5=76.203 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=12:15 IST=> training   79.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.105 Prec@1=51.430 Prec@5=76.198 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=12:15 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.105 Prec@1=51.430 Prec@5=76.198 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=12:15 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.105 Prec@1=51.430 Prec@5=76.198 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=12:16 IST=> training   83.94% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.106 Prec@1=51.407 Prec@5=76.176 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=12:16 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.106 Prec@1=51.407 Prec@5=76.176 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=12:16 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.106 Prec@1=51.407 Prec@5=76.176 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=12:16 IST=> training   87.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.106 Prec@1=51.398 Prec@5=76.170 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=12:16 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.106 Prec@1=51.398 Prec@5=76.170 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=12:16 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.338 DataTime=0.233 Loss=2.106 Prec@1=51.398 Prec@5=76.170 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=12:17 IST=> training   91.93% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.107 Prec@1=51.383 Prec@5=76.158 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=12:17 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.107 Prec@1=51.383 Prec@5=76.158 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=12:17 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.233 Loss=2.107 Prec@1=51.383 Prec@5=76.158 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=12:17 IST=> training   95.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.232 Loss=2.108 Prec@1=51.368 Prec@5=76.144 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=12:17 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.232 Loss=2.108 Prec@1=51.368 Prec@5=76.144 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=12:17 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.232 Loss=2.108 Prec@1=51.368 Prec@5=76.144 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=12:17 IST=> training   99.92% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.232 Loss=2.108 Prec@1=51.368 Prec@5=76.142 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=12:17 IST=> training   100.00% of 1x2503...Epoch=53/150 LR=0.07316 Time=0.337 DataTime=0.232 Loss=2.108 Prec@1=51.368 Prec@5=76.142 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=12:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:17 IST=> validation 0.00% of 1x98...Epoch=53/150 LR=0.07316 Time=7.152 Loss=2.328 Prec@1=48.438 Prec@5=75.000 rate=0 Hz, eta=?, total=0:00:00, wall=12:17 IST=> validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=7.152 Loss=2.328 Prec@1=48.438 Prec@5=75.000 rate=6249.22 Hz, eta=0:00:00, total=0:00:00, wall=12:17 IST** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=7.152 Loss=2.328 Prec@1=48.438 Prec@5=75.000 rate=6249.22 Hz, eta=0:00:00, total=0:00:00, wall=12:18 IST** validation 1.02% of 1x98...Epoch=53/150 LR=0.07316 Time=0.398 Loss=2.350 Prec@1=46.374 Prec@5=72.584 rate=6249.22 Hz, eta=0:00:00, total=0:00:00, wall=12:18 IST** validation 100.00% of 1x98...Epoch=53/150 LR=0.07316 Time=0.398 Loss=2.350 Prec@1=46.374 Prec@5=72.584 rate=3.07 Hz, eta=0:00:00, total=0:00:31, wall=12:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:18 IST=> training   0.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.107 DataTime=4.824 Loss=2.107 Prec@1=50.000 Prec@5=76.758 rate=0 Hz, eta=?, total=0:00:00, wall=12:18 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.107 DataTime=4.824 Loss=2.107 Prec@1=50.000 Prec@5=76.758 rate=7518.63 Hz, eta=0:00:00, total=0:00:00, wall=12:18 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=5.107 DataTime=4.824 Loss=2.107 Prec@1=50.000 Prec@5=76.758 rate=7518.63 Hz, eta=0:00:00, total=0:00:00, wall=12:19 IST=> training   0.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.380 DataTime=0.282 Loss=2.065 Prec@1=52.338 Prec@5=76.640 rate=7518.63 Hz, eta=0:00:00, total=0:00:00, wall=12:19 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.380 DataTime=0.282 Loss=2.065 Prec@1=52.338 Prec@5=76.640 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=12:19 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.380 DataTime=0.282 Loss=2.065 Prec@1=52.338 Prec@5=76.640 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=12:19 IST=> training   4.04% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.356 DataTime=0.255 Loss=2.073 Prec@1=52.043 Prec@5=76.587 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=12:19 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.356 DataTime=0.255 Loss=2.073 Prec@1=52.043 Prec@5=76.587 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=12:19 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.356 DataTime=0.255 Loss=2.073 Prec@1=52.043 Prec@5=76.587 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=12:20 IST=> training   8.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.352 DataTime=0.249 Loss=2.078 Prec@1=52.028 Prec@5=76.529 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=12:20 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.352 DataTime=0.249 Loss=2.078 Prec@1=52.028 Prec@5=76.529 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=12:20 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.352 DataTime=0.249 Loss=2.078 Prec@1=52.028 Prec@5=76.529 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=12:20 IST=> training   12.03% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.347 DataTime=0.243 Loss=2.083 Prec@1=51.942 Prec@5=76.441 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=12:20 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.347 DataTime=0.243 Loss=2.083 Prec@1=51.942 Prec@5=76.441 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=12:20 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.347 DataTime=0.243 Loss=2.083 Prec@1=51.942 Prec@5=76.441 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=12:21 IST=> training   16.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.238 Loss=2.084 Prec@1=51.875 Prec@5=76.456 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=12:21 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.238 Loss=2.084 Prec@1=51.875 Prec@5=76.456 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=12:21 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.238 Loss=2.084 Prec@1=51.875 Prec@5=76.456 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=12:21 IST=> training   20.02% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.344 DataTime=0.239 Loss=2.086 Prec@1=51.835 Prec@5=76.382 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=12:21 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.344 DataTime=0.239 Loss=2.086 Prec@1=51.835 Prec@5=76.382 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=12:21 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.344 DataTime=0.239 Loss=2.086 Prec@1=51.835 Prec@5=76.382 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=12:22 IST=> training   24.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.237 Loss=2.089 Prec@1=51.776 Prec@5=76.348 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=12:22 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.237 Loss=2.089 Prec@1=51.776 Prec@5=76.348 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=12:22 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.343 DataTime=0.237 Loss=2.089 Prec@1=51.776 Prec@5=76.348 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=12:23 IST=> training   28.01% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.089 Prec@1=51.770 Prec@5=76.354 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=12:23 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.089 Prec@1=51.770 Prec@5=76.354 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=12:23 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.089 Prec@1=51.770 Prec@5=76.354 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=12:23 IST=> training   32.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.684 Prec@5=76.287 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=12:23 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.684 Prec@5=76.287 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=12:23 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.684 Prec@5=76.287 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=12:24 IST=> training   36.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.340 DataTime=0.233 Loss=2.095 Prec@1=51.656 Prec@5=76.280 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=12:24 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.340 DataTime=0.233 Loss=2.095 Prec@1=51.656 Prec@5=76.280 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=12:24 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.340 DataTime=0.233 Loss=2.095 Prec@1=51.656 Prec@5=76.280 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=12:24 IST=> training   39.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.338 DataTime=0.232 Loss=2.096 Prec@1=51.614 Prec@5=76.259 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=12:24 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.338 DataTime=0.232 Loss=2.096 Prec@1=51.614 Prec@5=76.259 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=12:24 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.338 DataTime=0.232 Loss=2.096 Prec@1=51.614 Prec@5=76.259 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=12:25 IST=> training   43.99% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.339 DataTime=0.233 Loss=2.098 Prec@1=51.570 Prec@5=76.229 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=12:25 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.339 DataTime=0.233 Loss=2.098 Prec@1=51.570 Prec@5=76.229 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:25 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.339 DataTime=0.233 Loss=2.098 Prec@1=51.570 Prec@5=76.229 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:25 IST=> training   47.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.545 Prec@5=76.218 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=12:25 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.545 Prec@5=76.218 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=12:25 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.545 Prec@5=76.218 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=12:26 IST=> training   51.98% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.232 Loss=2.100 Prec@1=51.538 Prec@5=76.209 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=12:26 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.232 Loss=2.100 Prec@1=51.538 Prec@5=76.209 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=12:26 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.232 Loss=2.100 Prec@1=51.538 Prec@5=76.209 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=12:26 IST=> training   55.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.549 Prec@5=76.224 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=12:26 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.549 Prec@5=76.224 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=12:26 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.099 Prec@1=51.549 Prec@5=76.224 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=12:27 IST=> training   59.97% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.099 Prec@1=51.539 Prec@5=76.230 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=12:27 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.099 Prec@1=51.539 Prec@5=76.230 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=12:27 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.099 Prec@1=51.539 Prec@5=76.230 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=12:28 IST=> training   63.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.100 Prec@1=51.527 Prec@5=76.220 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=12:28 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.100 Prec@1=51.527 Prec@5=76.220 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=12:28 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.100 Prec@1=51.527 Prec@5=76.220 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=12:28 IST=> training   67.96% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.100 Prec@1=51.512 Prec@5=76.205 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=12:28 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.100 Prec@1=51.512 Prec@5=76.205 rate=3.00 Hz, eta=0:03:53, total=0:10:00, wall=12:28 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.100 Prec@1=51.512 Prec@5=76.205 rate=3.00 Hz, eta=0:03:53, total=0:10:00, wall=12:29 IST=> training   71.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.101 Prec@1=51.497 Prec@5=76.188 rate=3.00 Hz, eta=0:03:53, total=0:10:00, wall=12:29 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.101 Prec@1=51.497 Prec@5=76.188 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=12:29 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.101 Prec@1=51.497 Prec@5=76.188 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=12:29 IST=> training   75.95% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.102 Prec@1=51.472 Prec@5=76.172 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=12:29 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.102 Prec@1=51.472 Prec@5=76.172 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=12:29 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.337 DataTime=0.231 Loss=2.102 Prec@1=51.472 Prec@5=76.172 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=12:30 IST=> training   79.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.231 Loss=2.102 Prec@1=51.483 Prec@5=76.164 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=12:30 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.231 Loss=2.102 Prec@1=51.483 Prec@5=76.164 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=12:30 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.231 Loss=2.102 Prec@1=51.483 Prec@5=76.164 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=12:30 IST=> training   83.94% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.497 Prec@5=76.167 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=12:30 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.497 Prec@5=76.167 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=12:30 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.497 Prec@5=76.167 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=12:31 IST=> training   87.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.495 Prec@5=76.171 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=12:31 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.495 Prec@5=76.171 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=12:31 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.102 Prec@1=51.495 Prec@5=76.171 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=12:31 IST=> training   91.93% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.103 Prec@1=51.494 Prec@5=76.167 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=12:31 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.103 Prec@1=51.494 Prec@5=76.167 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=12:31 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.230 Loss=2.103 Prec@1=51.494 Prec@5=76.167 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=12:32 IST=> training   95.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.229 Loss=2.103 Prec@1=51.503 Prec@5=76.175 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=12:32 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.229 Loss=2.103 Prec@1=51.503 Prec@5=76.175 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=12:32 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.229 Loss=2.103 Prec@1=51.503 Prec@5=76.175 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=12:32 IST=> training   99.92% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.229 Loss=2.103 Prec@1=51.504 Prec@5=76.175 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=12:32 IST=> training   100.00% of 1x2503...Epoch=54/150 LR=0.07223 Time=0.336 DataTime=0.229 Loss=2.103 Prec@1=51.504 Prec@5=76.175 rate=3.00 Hz, eta=0:00:00, total=0:13:55, wall=12:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:32 IST=> validation 0.00% of 1x98...Epoch=54/150 LR=0.07223 Time=7.588 Loss=2.052 Prec@1=51.562 Prec@5=77.930 rate=0 Hz, eta=?, total=0:00:00, wall=12:32 IST=> validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=7.588 Loss=2.052 Prec@1=51.562 Prec@5=77.930 rate=6366.14 Hz, eta=0:00:00, total=0:00:00, wall=12:32 IST** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=7.588 Loss=2.052 Prec@1=51.562 Prec@5=77.930 rate=6366.14 Hz, eta=0:00:00, total=0:00:00, wall=12:33 IST** validation 1.02% of 1x98...Epoch=54/150 LR=0.07223 Time=0.411 Loss=2.156 Prec@1=50.362 Prec@5=75.568 rate=6366.14 Hz, eta=0:00:00, total=0:00:00, wall=12:33 IST** validation 100.00% of 1x98...Epoch=54/150 LR=0.07223 Time=0.411 Loss=2.156 Prec@1=50.362 Prec@5=75.568 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=12:33 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:33 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:33 IST=> training   0.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=6.009 DataTime=5.920 Loss=1.973 Prec@1=53.516 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=12:33 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=6.009 DataTime=5.920 Loss=1.973 Prec@1=53.516 Prec@5=76.953 rate=6607.46 Hz, eta=0:00:00, total=0:00:00, wall=12:33 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=6.009 DataTime=5.920 Loss=1.973 Prec@1=53.516 Prec@5=76.953 rate=6607.46 Hz, eta=0:00:00, total=0:00:00, wall=12:33 IST=> training   0.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.396 DataTime=0.299 Loss=2.065 Prec@1=51.938 Prec@5=76.721 rate=6607.46 Hz, eta=0:00:00, total=0:00:00, wall=12:33 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.396 DataTime=0.299 Loss=2.065 Prec@1=51.938 Prec@5=76.721 rate=2.97 Hz, eta=0:13:27, total=0:00:33, wall=12:33 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.396 DataTime=0.299 Loss=2.065 Prec@1=51.938 Prec@5=76.721 rate=2.97 Hz, eta=0:13:27, total=0:00:33, wall=12:34 IST=> training   4.04% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.363 DataTime=0.260 Loss=2.064 Prec@1=52.024 Prec@5=76.872 rate=2.97 Hz, eta=0:13:27, total=0:00:33, wall=12:34 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.363 DataTime=0.260 Loss=2.064 Prec@1=52.024 Prec@5=76.872 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=12:34 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.363 DataTime=0.260 Loss=2.064 Prec@1=52.024 Prec@5=76.872 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=12:35 IST=> training   8.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.357 DataTime=0.254 Loss=2.070 Prec@1=51.995 Prec@5=76.771 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=12:35 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.357 DataTime=0.254 Loss=2.070 Prec@1=51.995 Prec@5=76.771 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=12:35 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.357 DataTime=0.254 Loss=2.070 Prec@1=51.995 Prec@5=76.771 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=12:35 IST=> training   12.03% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.350 DataTime=0.245 Loss=2.072 Prec@1=51.954 Prec@5=76.744 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=12:35 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.350 DataTime=0.245 Loss=2.072 Prec@1=51.954 Prec@5=76.744 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=12:35 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.350 DataTime=0.245 Loss=2.072 Prec@1=51.954 Prec@5=76.744 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=12:36 IST=> training   16.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.346 DataTime=0.240 Loss=2.074 Prec@1=51.965 Prec@5=76.694 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=12:36 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.346 DataTime=0.240 Loss=2.074 Prec@1=51.965 Prec@5=76.694 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=12:36 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.346 DataTime=0.240 Loss=2.074 Prec@1=51.965 Prec@5=76.694 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=12:36 IST=> training   20.02% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.347 DataTime=0.241 Loss=2.074 Prec@1=51.996 Prec@5=76.677 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=12:36 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.347 DataTime=0.241 Loss=2.074 Prec@1=51.996 Prec@5=76.677 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=12:36 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.347 DataTime=0.241 Loss=2.074 Prec@1=51.996 Prec@5=76.677 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=12:37 IST=> training   24.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.345 DataTime=0.239 Loss=2.078 Prec@1=51.952 Prec@5=76.624 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=12:37 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.345 DataTime=0.239 Loss=2.078 Prec@1=51.952 Prec@5=76.624 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=12:37 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.345 DataTime=0.239 Loss=2.078 Prec@1=51.952 Prec@5=76.624 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=12:37 IST=> training   28.01% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.343 DataTime=0.238 Loss=2.079 Prec@1=51.910 Prec@5=76.613 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=12:37 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.343 DataTime=0.238 Loss=2.079 Prec@1=51.910 Prec@5=76.613 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=12:37 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.343 DataTime=0.238 Loss=2.079 Prec@1=51.910 Prec@5=76.613 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=12:38 IST=> training   32.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.344 DataTime=0.238 Loss=2.080 Prec@1=51.880 Prec@5=76.589 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=12:38 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.344 DataTime=0.238 Loss=2.080 Prec@1=51.880 Prec@5=76.589 rate=2.96 Hz, eta=0:09:00, total=0:05:03, wall=12:38 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.344 DataTime=0.238 Loss=2.080 Prec@1=51.880 Prec@5=76.589 rate=2.96 Hz, eta=0:09:00, total=0:05:03, wall=12:38 IST=> training   36.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.082 Prec@1=51.863 Prec@5=76.570 rate=2.96 Hz, eta=0:09:00, total=0:05:03, wall=12:38 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.082 Prec@1=51.863 Prec@5=76.570 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=12:38 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.082 Prec@1=51.863 Prec@5=76.570 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=12:39 IST=> training   39.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.083 Prec@1=51.827 Prec@5=76.539 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=12:39 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.083 Prec@1=51.827 Prec@5=76.539 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=12:39 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.083 Prec@1=51.827 Prec@5=76.539 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=12:40 IST=> training   43.99% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.085 Prec@1=51.795 Prec@5=76.522 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=12:40 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.085 Prec@1=51.795 Prec@5=76.522 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=12:40 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.085 Prec@1=51.795 Prec@5=76.522 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=12:40 IST=> training   47.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.087 Prec@1=51.748 Prec@5=76.467 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=12:40 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.087 Prec@1=51.748 Prec@5=76.467 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=12:40 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.087 Prec@1=51.748 Prec@5=76.467 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=12:41 IST=> training   51.98% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.089 Prec@1=51.728 Prec@5=76.424 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=12:41 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.089 Prec@1=51.728 Prec@5=76.424 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=12:41 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.089 Prec@1=51.728 Prec@5=76.424 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=12:41 IST=> training   55.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.342 DataTime=0.236 Loss=2.090 Prec@1=51.704 Prec@5=76.406 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=12:41 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.342 DataTime=0.236 Loss=2.090 Prec@1=51.704 Prec@5=76.406 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=12:41 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.342 DataTime=0.236 Loss=2.090 Prec@1=51.704 Prec@5=76.406 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=12:42 IST=> training   59.97% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.694 Prec@5=76.398 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=12:42 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.694 Prec@5=76.398 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=12:42 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.694 Prec@5=76.398 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=12:42 IST=> training   63.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.681 Prec@5=76.392 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=12:42 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.681 Prec@5=76.392 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=12:42 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.091 Prec@1=51.681 Prec@5=76.392 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=12:43 IST=> training   67.96% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.092 Prec@1=51.676 Prec@5=76.383 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=12:43 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.092 Prec@1=51.676 Prec@5=76.383 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:43 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.092 Prec@1=51.676 Prec@5=76.383 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:44 IST=> training   71.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.092 Prec@1=51.658 Prec@5=76.376 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:44 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.092 Prec@1=51.658 Prec@5=76.376 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=12:44 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.092 Prec@1=51.658 Prec@5=76.376 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=12:44 IST=> training   75.95% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.093 Prec@1=51.640 Prec@5=76.370 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=12:44 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.093 Prec@1=51.640 Prec@5=76.370 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=12:44 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.093 Prec@1=51.640 Prec@5=76.370 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=12:45 IST=> training   79.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.622 Prec@5=76.358 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=12:45 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.622 Prec@5=76.358 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=12:45 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.094 Prec@1=51.622 Prec@5=76.358 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=12:45 IST=> training   83.94% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.095 Prec@1=51.626 Prec@5=76.349 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=12:45 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.095 Prec@1=51.626 Prec@5=76.349 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=12:45 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.095 Prec@1=51.626 Prec@5=76.349 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=12:46 IST=> training   87.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.096 Prec@1=51.613 Prec@5=76.333 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=12:46 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.096 Prec@1=51.613 Prec@5=76.333 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=12:46 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.234 Loss=2.096 Prec@1=51.613 Prec@5=76.333 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=12:46 IST=> training   91.93% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.096 Prec@1=51.608 Prec@5=76.323 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=12:46 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.096 Prec@1=51.608 Prec@5=76.323 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=12:46 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.341 DataTime=0.235 Loss=2.096 Prec@1=51.608 Prec@5=76.323 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=12:47 IST=> training   95.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.097 Prec@1=51.609 Prec@5=76.317 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=12:47 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.097 Prec@1=51.609 Prec@5=76.317 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=12:47 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.097 Prec@1=51.609 Prec@5=76.317 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=12:47 IST=> training   99.92% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.097 Prec@1=51.609 Prec@5=76.316 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=12:47 IST=> training   100.00% of 1x2503...Epoch=55/150 LR=0.07129 Time=0.340 DataTime=0.234 Loss=2.097 Prec@1=51.609 Prec@5=76.316 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=12:47 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:47 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:47 IST=> validation 0.00% of 1x98...Epoch=55/150 LR=0.07129 Time=6.485 Loss=2.876 Prec@1=40.234 Prec@5=64.844 rate=0 Hz, eta=?, total=0:00:00, wall=12:47 IST=> validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.485 Loss=2.876 Prec@1=40.234 Prec@5=64.844 rate=6911.33 Hz, eta=0:00:00, total=0:00:00, wall=12:47 IST** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=6.485 Loss=2.876 Prec@1=40.234 Prec@5=64.844 rate=6911.33 Hz, eta=0:00:00, total=0:00:00, wall=12:48 IST** validation 1.02% of 1x98...Epoch=55/150 LR=0.07129 Time=0.399 Loss=2.825 Prec@1=38.346 Prec@5=64.402 rate=6911.33 Hz, eta=0:00:00, total=0:00:00, wall=12:48 IST** validation 100.00% of 1x98...Epoch=55/150 LR=0.07129 Time=0.399 Loss=2.825 Prec@1=38.346 Prec@5=64.402 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=12:48 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:48 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:48 IST=> training   0.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.449 DataTime=5.298 Loss=2.101 Prec@1=51.172 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=12:48 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.449 DataTime=5.298 Loss=2.101 Prec@1=51.172 Prec@5=76.367 rate=4722.95 Hz, eta=0:00:00, total=0:00:00, wall=12:48 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=5.449 DataTime=5.298 Loss=2.101 Prec@1=51.172 Prec@5=76.367 rate=4722.95 Hz, eta=0:00:00, total=0:00:00, wall=12:48 IST=> training   0.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.374 DataTime=0.273 Loss=2.068 Prec@1=52.104 Prec@5=76.657 rate=4722.95 Hz, eta=0:00:00, total=0:00:00, wall=12:48 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.374 DataTime=0.273 Loss=2.068 Prec@1=52.104 Prec@5=76.657 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=12:48 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.374 DataTime=0.273 Loss=2.068 Prec@1=52.104 Prec@5=76.657 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=12:49 IST=> training   4.04% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.357 DataTime=0.256 Loss=2.070 Prec@1=52.148 Prec@5=76.607 rate=3.12 Hz, eta=0:12:49, total=0:00:32, wall=12:49 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.357 DataTime=0.256 Loss=2.070 Prec@1=52.148 Prec@5=76.607 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=12:49 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.357 DataTime=0.256 Loss=2.070 Prec@1=52.148 Prec@5=76.607 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=12:49 IST=> training   8.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.351 DataTime=0.251 Loss=2.070 Prec@1=52.189 Prec@5=76.615 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=12:49 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.351 DataTime=0.251 Loss=2.070 Prec@1=52.189 Prec@5=76.615 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=12:49 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.351 DataTime=0.251 Loss=2.070 Prec@1=52.189 Prec@5=76.615 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=12:50 IST=> training   12.03% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.248 Loss=2.070 Prec@1=52.157 Prec@5=76.620 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=12:50 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.248 Loss=2.070 Prec@1=52.157 Prec@5=76.620 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=12:50 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.248 Loss=2.070 Prec@1=52.157 Prec@5=76.620 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=12:51 IST=> training   16.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.247 Loss=2.073 Prec@1=52.093 Prec@5=76.575 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=12:51 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.247 Loss=2.073 Prec@1=52.093 Prec@5=76.575 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=12:51 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.348 DataTime=0.247 Loss=2.073 Prec@1=52.093 Prec@5=76.575 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=12:51 IST=> training   20.02% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.243 Loss=2.072 Prec@1=52.107 Prec@5=76.628 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=12:51 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.243 Loss=2.072 Prec@1=52.107 Prec@5=76.628 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=12:51 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.243 Loss=2.072 Prec@1=52.107 Prec@5=76.628 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=12:52 IST=> training   24.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.242 Loss=2.074 Prec@1=52.096 Prec@5=76.582 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=12:52 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.242 Loss=2.074 Prec@1=52.096 Prec@5=76.582 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=12:52 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.345 DataTime=0.242 Loss=2.074 Prec@1=52.096 Prec@5=76.582 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=12:52 IST=> training   28.01% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.240 Loss=2.074 Prec@1=52.077 Prec@5=76.587 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=12:52 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.240 Loss=2.074 Prec@1=52.077 Prec@5=76.587 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=12:52 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.240 Loss=2.074 Prec@1=52.077 Prec@5=76.587 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=12:53 IST=> training   32.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.075 Prec@1=52.069 Prec@5=76.585 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=12:53 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.075 Prec@1=52.069 Prec@5=76.585 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=12:53 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.075 Prec@1=52.069 Prec@5=76.585 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=12:53 IST=> training   36.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.076 Prec@1=52.087 Prec@5=76.580 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=12:53 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.076 Prec@1=52.087 Prec@5=76.580 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=12:53 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.344 DataTime=0.241 Loss=2.076 Prec@1=52.087 Prec@5=76.580 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=12:54 IST=> training   39.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.239 Loss=2.076 Prec@1=52.062 Prec@5=76.572 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=12:54 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.239 Loss=2.076 Prec@1=52.062 Prec@5=76.572 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=12:54 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.239 Loss=2.076 Prec@1=52.062 Prec@5=76.572 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=12:54 IST=> training   43.99% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.552 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=12:54 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.552 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=12:54 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.552 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=12:55 IST=> training   47.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.539 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=12:55 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.539 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=12:55 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.238 Loss=2.078 Prec@1=52.034 Prec@5=76.539 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=12:56 IST=> training   51.98% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.080 Prec@1=51.978 Prec@5=76.521 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=12:56 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.080 Prec@1=51.978 Prec@5=76.521 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=12:56 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.080 Prec@1=51.978 Prec@5=76.521 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=12:56 IST=> training   55.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.081 Prec@1=51.962 Prec@5=76.517 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=12:56 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.081 Prec@1=51.962 Prec@5=76.517 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=12:56 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.081 Prec@1=51.962 Prec@5=76.517 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=12:57 IST=> training   59.97% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.082 Prec@1=51.937 Prec@5=76.494 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=12:57 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.082 Prec@1=51.937 Prec@5=76.494 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=12:57 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.342 DataTime=0.237 Loss=2.082 Prec@1=51.937 Prec@5=76.494 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=12:57 IST=> training   63.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.082 Prec@1=51.922 Prec@5=76.496 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=12:57 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.082 Prec@1=51.922 Prec@5=76.496 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=12:57 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.237 Loss=2.082 Prec@1=51.922 Prec@5=76.496 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=12:58 IST=> training   67.96% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.895 Prec@5=76.484 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=12:58 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.895 Prec@5=76.484 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:58 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.895 Prec@5=76.484 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:58 IST=> training   71.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.887 Prec@5=76.490 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=12:58 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.887 Prec@5=76.490 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=12:58 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.083 Prec@1=51.887 Prec@5=76.490 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=12:59 IST=> training   75.95% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.084 Prec@1=51.867 Prec@5=76.481 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=12:59 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.084 Prec@1=51.867 Prec@5=76.481 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=12:59 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.084 Prec@1=51.867 Prec@5=76.481 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=13:00 IST=> training   79.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.086 Prec@1=51.837 Prec@5=76.454 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=13:00 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.086 Prec@1=51.837 Prec@5=76.454 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=13:00 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.086 Prec@1=51.837 Prec@5=76.454 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=13:00 IST=> training   83.94% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.087 Prec@1=51.824 Prec@5=76.441 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=13:00 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.087 Prec@1=51.824 Prec@5=76.441 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=13:00 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.341 DataTime=0.236 Loss=2.087 Prec@1=51.824 Prec@5=76.441 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=13:01 IST=> training   87.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.088 Prec@1=51.806 Prec@5=76.423 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=13:01 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.088 Prec@1=51.806 Prec@5=76.423 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=13:01 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.088 Prec@1=51.806 Prec@5=76.423 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=13:01 IST=> training   91.93% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.089 Prec@1=51.789 Prec@5=76.410 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=13:01 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.089 Prec@1=51.789 Prec@5=76.410 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=13:01 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.089 Prec@1=51.789 Prec@5=76.410 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=13:02 IST=> training   95.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.090 Prec@1=51.781 Prec@5=76.396 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=13:02 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.090 Prec@1=51.781 Prec@5=76.396 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=13:02 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.090 Prec@1=51.781 Prec@5=76.396 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=13:02 IST=> training   99.92% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.090 Prec@1=51.781 Prec@5=76.396 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=13:02 IST=> training   100.00% of 1x2503...Epoch=56/150 LR=0.07034 Time=0.340 DataTime=0.235 Loss=2.090 Prec@1=51.781 Prec@5=76.396 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=13:02 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:02 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:02 IST=> validation 0.00% of 1x98...Epoch=56/150 LR=0.07034 Time=6.371 Loss=2.421 Prec@1=45.898 Prec@5=71.484 rate=0 Hz, eta=?, total=0:00:00, wall=13:02 IST=> validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.371 Loss=2.421 Prec@1=45.898 Prec@5=71.484 rate=2087.65 Hz, eta=0:00:00, total=0:00:00, wall=13:02 IST** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=6.371 Loss=2.421 Prec@1=45.898 Prec@5=71.484 rate=2087.65 Hz, eta=0:00:00, total=0:00:00, wall=13:02 IST** validation 1.02% of 1x98...Epoch=56/150 LR=0.07034 Time=0.406 Loss=2.364 Prec@1=46.554 Prec@5=72.098 rate=2087.65 Hz, eta=0:00:00, total=0:00:00, wall=13:02 IST** validation 100.00% of 1x98...Epoch=56/150 LR=0.07034 Time=0.406 Loss=2.364 Prec@1=46.554 Prec@5=72.098 rate=2.93 Hz, eta=0:00:00, total=0:00:33, wall=13:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST=> training   0.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=5.298 DataTime=5.044 Loss=2.151 Prec@1=51.562 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=13:03 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=5.298 DataTime=5.044 Loss=2.151 Prec@1=51.562 Prec@5=76.953 rate=3698.91 Hz, eta=0:00:00, total=0:00:00, wall=13:03 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=5.298 DataTime=5.044 Loss=2.151 Prec@1=51.562 Prec@5=76.953 rate=3698.91 Hz, eta=0:00:00, total=0:00:00, wall=13:03 IST=> training   0.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.371 DataTime=0.268 Loss=2.045 Prec@1=52.460 Prec@5=77.235 rate=3698.91 Hz, eta=0:00:00, total=0:00:00, wall=13:03 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.371 DataTime=0.268 Loss=2.045 Prec@1=52.460 Prec@5=77.235 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=13:03 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.371 DataTime=0.268 Loss=2.045 Prec@1=52.460 Prec@5=77.235 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=13:04 IST=> training   4.04% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.354 DataTime=0.252 Loss=2.056 Prec@1=52.428 Prec@5=77.038 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=13:04 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.354 DataTime=0.252 Loss=2.056 Prec@1=52.428 Prec@5=77.038 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=13:04 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.354 DataTime=0.252 Loss=2.056 Prec@1=52.428 Prec@5=77.038 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=13:04 IST=> training   8.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.355 DataTime=0.254 Loss=2.050 Prec@1=52.536 Prec@5=77.100 rate=3.05 Hz, eta=0:12:33, total=0:01:05, wall=13:04 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.355 DataTime=0.254 Loss=2.050 Prec@1=52.536 Prec@5=77.100 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=13:04 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.355 DataTime=0.254 Loss=2.050 Prec@1=52.536 Prec@5=77.100 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=13:05 IST=> training   12.03% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.350 DataTime=0.246 Loss=2.052 Prec@1=52.528 Prec@5=77.093 rate=2.96 Hz, eta=0:12:24, total=0:01:41, wall=13:05 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.350 DataTime=0.246 Loss=2.052 Prec@1=52.528 Prec@5=77.093 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=13:05 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.350 DataTime=0.246 Loss=2.052 Prec@1=52.528 Prec@5=77.093 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=13:05 IST=> training   16.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.345 DataTime=0.241 Loss=2.056 Prec@1=52.419 Prec@5=77.016 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=13:05 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.345 DataTime=0.241 Loss=2.056 Prec@1=52.419 Prec@5=77.016 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=13:05 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.345 DataTime=0.241 Loss=2.056 Prec@1=52.419 Prec@5=77.016 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=13:06 IST=> training   20.02% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.347 DataTime=0.242 Loss=2.058 Prec@1=52.379 Prec@5=76.956 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=13:06 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.347 DataTime=0.242 Loss=2.058 Prec@1=52.379 Prec@5=76.956 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=13:06 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.347 DataTime=0.242 Loss=2.058 Prec@1=52.379 Prec@5=76.956 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=13:06 IST=> training   24.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.344 DataTime=0.239 Loss=2.062 Prec@1=52.301 Prec@5=76.910 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=13:06 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.344 DataTime=0.239 Loss=2.062 Prec@1=52.301 Prec@5=76.910 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=13:06 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.344 DataTime=0.239 Loss=2.062 Prec@1=52.301 Prec@5=76.910 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=13:07 IST=> training   28.01% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.062 Prec@1=52.290 Prec@5=76.909 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=13:07 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.062 Prec@1=52.290 Prec@5=76.909 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=13:07 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.062 Prec@1=52.290 Prec@5=76.909 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=13:08 IST=> training   32.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.064 Prec@1=52.225 Prec@5=76.867 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=13:08 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.064 Prec@1=52.225 Prec@5=76.867 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=13:08 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.343 DataTime=0.238 Loss=2.064 Prec@1=52.225 Prec@5=76.867 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=13:08 IST=> training   36.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.236 Loss=2.067 Prec@1=52.173 Prec@5=76.825 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=13:08 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.236 Loss=2.067 Prec@1=52.173 Prec@5=76.825 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=13:08 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.236 Loss=2.067 Prec@1=52.173 Prec@5=76.825 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=13:09 IST=> training   39.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.068 Prec@1=52.169 Prec@5=76.806 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=13:09 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.068 Prec@1=52.169 Prec@5=76.806 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:09 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.068 Prec@1=52.169 Prec@5=76.806 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:09 IST=> training   43.99% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.070 Prec@1=52.141 Prec@5=76.775 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:09 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.070 Prec@1=52.141 Prec@5=76.775 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=13:09 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.070 Prec@1=52.141 Prec@5=76.775 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=13:10 IST=> training   47.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.235 Loss=2.071 Prec@1=52.125 Prec@5=76.774 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=13:10 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.235 Loss=2.071 Prec@1=52.125 Prec@5=76.774 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:10 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.341 DataTime=0.235 Loss=2.071 Prec@1=52.125 Prec@5=76.774 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:10 IST=> training   51.98% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.740 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:10 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.740 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=13:10 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.740 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=13:11 IST=> training   55.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.234 Loss=2.074 Prec@1=52.061 Prec@5=76.708 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=13:11 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.234 Loss=2.074 Prec@1=52.061 Prec@5=76.708 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=13:11 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.234 Loss=2.074 Prec@1=52.061 Prec@5=76.708 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=13:12 IST=> training   59.97% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.075 Prec@1=52.049 Prec@5=76.681 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=13:12 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.075 Prec@1=52.049 Prec@5=76.681 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=13:12 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.340 DataTime=0.235 Loss=2.075 Prec@1=52.049 Prec@5=76.681 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=13:12 IST=> training   63.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.076 Prec@1=52.018 Prec@5=76.663 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=13:12 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.076 Prec@1=52.018 Prec@5=76.663 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=13:12 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.076 Prec@1=52.018 Prec@5=76.663 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=13:13 IST=> training   67.96% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.077 Prec@1=52.010 Prec@5=76.641 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=13:13 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.077 Prec@1=52.010 Prec@5=76.641 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:13 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.077 Prec@1=52.010 Prec@5=76.641 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:13 IST=> training   71.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.079 Prec@1=51.974 Prec@5=76.615 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:13 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.079 Prec@1=51.974 Prec@5=76.615 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:13 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.079 Prec@1=51.974 Prec@5=76.615 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:14 IST=> training   75.95% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.961 Prec@5=76.613 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:14 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.961 Prec@5=76.613 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=13:14 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.961 Prec@5=76.613 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=13:14 IST=> training   79.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.081 Prec@1=51.942 Prec@5=76.596 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=13:14 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.081 Prec@1=51.942 Prec@5=76.596 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:14 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.235 Loss=2.081 Prec@1=51.942 Prec@5=76.596 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:15 IST=> training   83.94% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.082 Prec@1=51.914 Prec@5=76.578 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:15 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.082 Prec@1=51.914 Prec@5=76.578 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=13:15 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.082 Prec@1=51.914 Prec@5=76.578 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=13:15 IST=> training   87.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.083 Prec@1=51.894 Prec@5=76.558 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=13:15 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.083 Prec@1=51.894 Prec@5=76.558 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:15 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.083 Prec@1=51.894 Prec@5=76.558 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:16 IST=> training   91.93% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.084 Prec@1=51.878 Prec@5=76.542 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:16 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.084 Prec@1=51.878 Prec@5=76.542 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=13:16 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.339 DataTime=0.234 Loss=2.084 Prec@1=51.878 Prec@5=76.542 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=13:17 IST=> training   95.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.084 Prec@1=51.872 Prec@5=76.524 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=13:17 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.084 Prec@1=51.872 Prec@5=76.524 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:17 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.084 Prec@1=51.872 Prec@5=76.524 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:17 IST=> training   99.92% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.084 Prec@1=51.870 Prec@5=76.523 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:17 IST=> training   100.00% of 1x2503...Epoch=57/150 LR=0.06938 Time=0.338 DataTime=0.234 Loss=2.084 Prec@1=51.870 Prec@5=76.523 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=13:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> validation 0.00% of 1x98...Epoch=57/150 LR=0.06938 Time=5.889 Loss=2.184 Prec@1=48.633 Prec@5=77.539 rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=5.889 Loss=2.184 Prec@1=48.633 Prec@5=77.539 rate=3932.86 Hz, eta=0:00:00, total=0:00:00, wall=13:17 IST** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=5.889 Loss=2.184 Prec@1=48.633 Prec@5=77.539 rate=3932.86 Hz, eta=0:00:00, total=0:00:00, wall=13:17 IST** validation 1.02% of 1x98...Epoch=57/150 LR=0.06938 Time=0.399 Loss=2.134 Prec@1=50.860 Prec@5=75.908 rate=3932.86 Hz, eta=0:00:00, total=0:00:00, wall=13:17 IST** validation 100.00% of 1x98...Epoch=57/150 LR=0.06938 Time=0.399 Loss=2.134 Prec@1=50.860 Prec@5=75.908 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=13:17 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> training   0.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=4.866 DataTime=4.693 Loss=1.993 Prec@1=52.734 Prec@5=79.102 rate=0 Hz, eta=?, total=0:00:00, wall=13:17 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=4.866 DataTime=4.693 Loss=1.993 Prec@1=52.734 Prec@5=79.102 rate=5636.53 Hz, eta=0:00:00, total=0:00:00, wall=13:17 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=4.866 DataTime=4.693 Loss=1.993 Prec@1=52.734 Prec@5=79.102 rate=5636.53 Hz, eta=0:00:00, total=0:00:00, wall=13:18 IST=> training   0.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.381 DataTime=0.280 Loss=2.059 Prec@1=52.299 Prec@5=76.845 rate=5636.53 Hz, eta=0:00:00, total=0:00:00, wall=13:18 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.381 DataTime=0.280 Loss=2.059 Prec@1=52.299 Prec@5=76.845 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=13:18 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.381 DataTime=0.280 Loss=2.059 Prec@1=52.299 Prec@5=76.845 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=13:18 IST=> training   4.04% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.351 DataTime=0.250 Loss=2.056 Prec@1=52.354 Prec@5=76.945 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=13:18 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.351 DataTime=0.250 Loss=2.056 Prec@1=52.354 Prec@5=76.945 rate=3.07 Hz, eta=0:12:31, total=0:01:05, wall=13:18 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.351 DataTime=0.250 Loss=2.056 Prec@1=52.354 Prec@5=76.945 rate=3.07 Hz, eta=0:12:31, total=0:01:05, wall=13:19 IST=> training   8.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.245 Loss=2.052 Prec@1=52.544 Prec@5=77.023 rate=3.07 Hz, eta=0:12:31, total=0:01:05, wall=13:19 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.245 Loss=2.052 Prec@1=52.544 Prec@5=77.023 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=13:19 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.245 Loss=2.052 Prec@1=52.544 Prec@5=77.023 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=13:20 IST=> training   12.03% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.241 Loss=2.055 Prec@1=52.490 Prec@5=76.918 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=13:20 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.241 Loss=2.055 Prec@1=52.490 Prec@5=76.918 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=13:20 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.241 Loss=2.055 Prec@1=52.490 Prec@5=76.918 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=13:20 IST=> training   16.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.938 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=13:20 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.938 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=13:20 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.345 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.938 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=13:21 IST=> training   20.02% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.344 DataTime=0.242 Loss=2.061 Prec@1=52.315 Prec@5=76.901 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=13:21 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.344 DataTime=0.242 Loss=2.061 Prec@1=52.315 Prec@5=76.901 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=13:21 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.344 DataTime=0.242 Loss=2.061 Prec@1=52.315 Prec@5=76.901 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=13:21 IST=> training   24.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.240 Loss=2.063 Prec@1=52.259 Prec@5=76.876 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=13:21 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.240 Loss=2.063 Prec@1=52.259 Prec@5=76.876 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=13:21 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.342 DataTime=0.240 Loss=2.063 Prec@1=52.259 Prec@5=76.876 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=13:22 IST=> training   28.01% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.343 DataTime=0.241 Loss=2.063 Prec@1=52.235 Prec@5=76.880 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=13:22 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.343 DataTime=0.241 Loss=2.063 Prec@1=52.235 Prec@5=76.880 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=13:22 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.343 DataTime=0.241 Loss=2.063 Prec@1=52.235 Prec@5=76.880 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=13:22 IST=> training   32.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.066 Prec@1=52.173 Prec@5=76.813 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=13:22 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.066 Prec@1=52.173 Prec@5=76.813 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=13:22 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.066 Prec@1=52.173 Prec@5=76.813 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=13:23 IST=> training   36.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.067 Prec@1=52.134 Prec@5=76.805 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=13:23 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.067 Prec@1=52.134 Prec@5=76.805 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:23 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.067 Prec@1=52.134 Prec@5=76.805 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:23 IST=> training   39.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.068 Prec@1=52.119 Prec@5=76.779 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:23 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.068 Prec@1=52.119 Prec@5=76.779 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=13:23 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.068 Prec@1=52.119 Prec@5=76.779 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=13:24 IST=> training   43.99% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.069 Prec@1=52.100 Prec@5=76.787 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=13:24 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.069 Prec@1=52.100 Prec@5=76.787 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=13:24 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.238 Loss=2.069 Prec@1=52.100 Prec@5=76.787 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=13:25 IST=> training   47.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.070 Prec@1=52.072 Prec@5=76.749 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=13:25 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.070 Prec@1=52.072 Prec@5=76.749 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:25 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.070 Prec@1=52.072 Prec@5=76.749 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:25 IST=> training   51.98% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.237 Loss=2.071 Prec@1=52.071 Prec@5=76.745 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=13:25 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.237 Loss=2.071 Prec@1=52.071 Prec@5=76.745 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=13:25 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.341 DataTime=0.237 Loss=2.071 Prec@1=52.071 Prec@5=76.745 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=13:26 IST=> training   55.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.071 Prec@1=52.072 Prec@5=76.732 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=13:26 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.071 Prec@1=52.072 Prec@5=76.732 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=13:26 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.071 Prec@1=52.072 Prec@5=76.732 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=13:26 IST=> training   59.97% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.236 Loss=2.073 Prec@1=52.037 Prec@5=76.701 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=13:26 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.236 Loss=2.073 Prec@1=52.037 Prec@5=76.701 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=13:26 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.236 Loss=2.073 Prec@1=52.037 Prec@5=76.701 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=13:27 IST=> training   63.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.073 Prec@1=52.023 Prec@5=76.677 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=13:27 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.073 Prec@1=52.023 Prec@5=76.677 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=13:27 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.340 DataTime=0.237 Loss=2.073 Prec@1=52.023 Prec@5=76.677 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=13:27 IST=> training   67.96% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.028 Prec@5=76.670 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=13:27 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.028 Prec@5=76.670 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:27 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.028 Prec@5=76.670 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:28 IST=> training   71.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.027 Prec@5=76.658 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=13:28 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.027 Prec@5=76.658 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=13:28 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.027 Prec@5=76.658 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=13:29 IST=> training   75.95% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.037 Prec@5=76.664 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=13:29 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.037 Prec@5=76.664 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:29 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.074 Prec@1=52.037 Prec@5=76.664 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:29 IST=> training   79.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.075 Prec@1=52.019 Prec@5=76.659 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:29 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.075 Prec@1=52.019 Prec@5=76.659 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:29 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.075 Prec@1=52.019 Prec@5=76.659 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:30 IST=> training   83.94% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.004 Prec@5=76.643 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=13:30 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.004 Prec@5=76.643 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=13:30 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.075 Prec@1=52.004 Prec@5=76.643 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=13:30 IST=> training   87.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.076 Prec@1=52.001 Prec@5=76.625 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=13:30 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.076 Prec@1=52.001 Prec@5=76.625 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=13:30 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.235 Loss=2.076 Prec@1=52.001 Prec@5=76.625 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=13:31 IST=> training   91.93% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.078 Prec@1=51.984 Prec@5=76.602 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=13:31 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.078 Prec@1=51.984 Prec@5=76.602 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:31 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.339 DataTime=0.234 Loss=2.078 Prec@1=51.984 Prec@5=76.602 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:31 IST=> training   95.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.974 Prec@5=76.586 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:31 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.974 Prec@5=76.586 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=13:31 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.974 Prec@5=76.586 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=13:31 IST=> training   99.92% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.973 Prec@5=76.585 rate=2.97 Hz, eta=0:00:00, total=0:14:00, wall=13:31 IST=> training   100.00% of 1x2503...Epoch=58/150 LR=0.06841 Time=0.338 DataTime=0.234 Loss=2.079 Prec@1=51.973 Prec@5=76.585 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=13:31 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:31 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:31 IST=> validation 0.00% of 1x98...Epoch=58/150 LR=0.06841 Time=6.645 Loss=2.104 Prec@1=50.000 Prec@5=76.172 rate=0 Hz, eta=?, total=0:00:00, wall=13:31 IST=> validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=6.645 Loss=2.104 Prec@1=50.000 Prec@5=76.172 rate=3573.98 Hz, eta=0:00:00, total=0:00:00, wall=13:31 IST** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=6.645 Loss=2.104 Prec@1=50.000 Prec@5=76.172 rate=3573.98 Hz, eta=0:00:00, total=0:00:00, wall=13:32 IST** validation 1.02% of 1x98...Epoch=58/150 LR=0.06841 Time=0.398 Loss=2.219 Prec@1=49.032 Prec@5=74.576 rate=3573.98 Hz, eta=0:00:00, total=0:00:00, wall=13:32 IST** validation 100.00% of 1x98...Epoch=58/150 LR=0.06841 Time=0.398 Loss=2.219 Prec@1=49.032 Prec@5=74.576 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=13:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> training   0.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=6.073 DataTime=5.808 Loss=2.142 Prec@1=51.367 Prec@5=74.805 rate=0 Hz, eta=?, total=0:00:00, wall=13:32 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=6.073 DataTime=5.808 Loss=2.142 Prec@1=51.367 Prec@5=74.805 rate=6517.16 Hz, eta=0:00:00, total=0:00:00, wall=13:32 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=6.073 DataTime=5.808 Loss=2.142 Prec@1=51.367 Prec@5=74.805 rate=6517.16 Hz, eta=0:00:00, total=0:00:00, wall=13:33 IST=> training   0.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.382 DataTime=0.289 Loss=2.048 Prec@1=52.669 Prec@5=77.135 rate=6517.16 Hz, eta=0:00:00, total=0:00:00, wall=13:33 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.382 DataTime=0.289 Loss=2.048 Prec@1=52.669 Prec@5=77.135 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=13:33 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.382 DataTime=0.289 Loss=2.048 Prec@1=52.669 Prec@5=77.135 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=13:33 IST=> training   4.04% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.353 DataTime=0.259 Loss=2.043 Prec@1=52.698 Prec@5=77.251 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=13:33 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.353 DataTime=0.259 Loss=2.043 Prec@1=52.698 Prec@5=77.251 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=13:33 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.353 DataTime=0.259 Loss=2.043 Prec@1=52.698 Prec@5=77.251 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=13:34 IST=> training   8.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.347 DataTime=0.251 Loss=2.048 Prec@1=52.601 Prec@5=77.138 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=13:34 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.347 DataTime=0.251 Loss=2.048 Prec@1=52.601 Prec@5=77.138 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=13:34 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.347 DataTime=0.251 Loss=2.048 Prec@1=52.601 Prec@5=77.138 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=13:34 IST=> training   12.03% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.245 Loss=2.055 Prec@1=52.462 Prec@5=76.997 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=13:34 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.245 Loss=2.055 Prec@1=52.462 Prec@5=76.997 rate=3.06 Hz, eta=0:11:27, total=0:02:11, wall=13:34 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.245 Loss=2.055 Prec@1=52.462 Prec@5=76.997 rate=3.06 Hz, eta=0:11:27, total=0:02:11, wall=13:35 IST=> training   16.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.243 Loss=2.057 Prec@1=52.426 Prec@5=76.957 rate=3.06 Hz, eta=0:11:27, total=0:02:11, wall=13:35 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.243 Loss=2.057 Prec@1=52.426 Prec@5=76.957 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=13:35 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.243 Loss=2.057 Prec@1=52.426 Prec@5=76.957 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=13:35 IST=> training   20.02% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.985 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=13:35 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.985 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=13:35 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.342 DataTime=0.243 Loss=2.057 Prec@1=52.401 Prec@5=76.985 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=13:36 IST=> training   24.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.240 Loss=2.058 Prec@1=52.340 Prec@5=76.984 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=13:36 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.240 Loss=2.058 Prec@1=52.340 Prec@5=76.984 rate=3.02 Hz, eta=0:09:56, total=0:03:51, wall=13:36 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.240 Loss=2.058 Prec@1=52.340 Prec@5=76.984 rate=3.02 Hz, eta=0:09:56, total=0:03:51, wall=13:37 IST=> training   28.01% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.239 Loss=2.059 Prec@1=52.333 Prec@5=76.960 rate=3.02 Hz, eta=0:09:56, total=0:03:51, wall=13:37 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.239 Loss=2.059 Prec@1=52.333 Prec@5=76.960 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=13:37 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.239 Loss=2.059 Prec@1=52.333 Prec@5=76.960 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=13:37 IST=> training   32.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.240 Loss=2.060 Prec@1=52.318 Prec@5=76.952 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=13:37 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.240 Loss=2.060 Prec@1=52.318 Prec@5=76.952 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=13:37 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.341 DataTime=0.240 Loss=2.060 Prec@1=52.318 Prec@5=76.952 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=13:38 IST=> training   36.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.238 Loss=2.061 Prec@1=52.273 Prec@5=76.936 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=13:38 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.238 Loss=2.061 Prec@1=52.273 Prec@5=76.936 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=13:38 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.238 Loss=2.061 Prec@1=52.273 Prec@5=76.936 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=13:38 IST=> training   39.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.237 Loss=2.063 Prec@1=52.234 Prec@5=76.904 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=13:38 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.237 Loss=2.063 Prec@1=52.234 Prec@5=76.904 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=13:38 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.237 Loss=2.063 Prec@1=52.234 Prec@5=76.904 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=13:39 IST=> training   43.99% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.238 Loss=2.064 Prec@1=52.210 Prec@5=76.869 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=13:39 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.238 Loss=2.064 Prec@1=52.210 Prec@5=76.869 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=13:39 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.238 Loss=2.064 Prec@1=52.210 Prec@5=76.869 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=13:39 IST=> training   47.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.237 Loss=2.064 Prec@1=52.200 Prec@5=76.849 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=13:39 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.237 Loss=2.064 Prec@1=52.200 Prec@5=76.849 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=13:39 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.237 Loss=2.064 Prec@1=52.200 Prec@5=76.849 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=13:40 IST=> training   51.98% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.065 Prec@1=52.201 Prec@5=76.836 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=13:40 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.065 Prec@1=52.201 Prec@5=76.836 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=13:40 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.065 Prec@1=52.201 Prec@5=76.836 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=13:41 IST=> training   55.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.236 Loss=2.067 Prec@1=52.194 Prec@5=76.812 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=13:41 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.236 Loss=2.067 Prec@1=52.194 Prec@5=76.812 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=13:41 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.340 DataTime=0.236 Loss=2.067 Prec@1=52.194 Prec@5=76.812 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=13:41 IST=> training   59.97% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.067 Prec@1=52.196 Prec@5=76.801 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=13:41 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.067 Prec@1=52.196 Prec@5=76.801 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:41 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.236 Loss=2.067 Prec@1=52.196 Prec@5=76.801 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:42 IST=> training   63.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.234 Loss=2.068 Prec@1=52.193 Prec@5=76.779 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:42 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.234 Loss=2.068 Prec@1=52.193 Prec@5=76.779 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=13:42 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.234 Loss=2.068 Prec@1=52.193 Prec@5=76.779 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=13:42 IST=> training   67.96% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.235 Loss=2.068 Prec@1=52.181 Prec@5=76.780 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=13:42 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.235 Loss=2.068 Prec@1=52.181 Prec@5=76.780 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=13:42 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.235 Loss=2.068 Prec@1=52.181 Prec@5=76.780 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=13:43 IST=> training   71.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.069 Prec@1=52.171 Prec@5=76.763 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=13:43 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.069 Prec@1=52.171 Prec@5=76.763 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=13:43 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.069 Prec@1=52.171 Prec@5=76.763 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=13:43 IST=> training   75.95% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.140 Prec@5=76.742 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=13:43 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.140 Prec@5=76.742 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=13:43 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.140 Prec@5=76.742 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=13:44 IST=> training   79.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.142 Prec@5=76.737 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=13:44 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.142 Prec@5=76.737 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:44 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.142 Prec@5=76.737 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:44 IST=> training   83.94% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.124 Prec@5=76.728 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:44 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.124 Prec@5=76.728 rate=2.97 Hz, eta=0:01:41, total=0:12:19, wall=13:44 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.070 Prec@1=52.124 Prec@5=76.728 rate=2.97 Hz, eta=0:01:41, total=0:12:19, wall=13:45 IST=> training   87.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.233 Loss=2.071 Prec@1=52.115 Prec@5=76.720 rate=2.97 Hz, eta=0:01:41, total=0:12:19, wall=13:45 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.233 Loss=2.071 Prec@1=52.115 Prec@5=76.720 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:45 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.233 Loss=2.071 Prec@1=52.115 Prec@5=76.720 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:46 IST=> training   91.93% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.715 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=13:46 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.715 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:46 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.339 DataTime=0.234 Loss=2.072 Prec@1=52.108 Prec@5=76.715 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:46 IST=> training   95.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.233 Loss=2.073 Prec@1=52.087 Prec@5=76.697 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.233 Loss=2.073 Prec@1=52.087 Prec@5=76.697 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.233 Loss=2.073 Prec@1=52.087 Prec@5=76.697 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:46 IST=> training   99.92% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.233 Loss=2.073 Prec@1=52.086 Prec@5=76.697 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=13:46 IST=> training   100.00% of 1x2503...Epoch=59/150 LR=0.06743 Time=0.338 DataTime=0.233 Loss=2.073 Prec@1=52.086 Prec@5=76.697 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=13:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 0.00% of 1x98...Epoch=59/150 LR=0.06743 Time=6.922 Loss=2.113 Prec@1=50.781 Prec@5=76.172 rate=0 Hz, eta=?, total=0:00:00, wall=13:46 IST=> validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.922 Loss=2.113 Prec@1=50.781 Prec@5=76.172 rate=3844.47 Hz, eta=0:00:00, total=0:00:00, wall=13:46 IST** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=6.922 Loss=2.113 Prec@1=50.781 Prec@5=76.172 rate=3844.47 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST** validation 1.02% of 1x98...Epoch=59/150 LR=0.06743 Time=0.395 Loss=2.229 Prec@1=48.736 Prec@5=74.384 rate=3844.47 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST** validation 100.00% of 1x98...Epoch=59/150 LR=0.06743 Time=0.395 Loss=2.229 Prec@1=48.736 Prec@5=74.384 rate=3.09 Hz, eta=0:00:00, total=0:00:31, wall=13:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.292 DataTime=5.151 Loss=1.949 Prec@1=53.711 Prec@5=78.906 rate=0 Hz, eta=?, total=0:00:00, wall=13:47 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.292 DataTime=5.151 Loss=1.949 Prec@1=53.711 Prec@5=78.906 rate=3739.84 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=5.292 DataTime=5.151 Loss=1.949 Prec@1=53.711 Prec@5=78.906 rate=3739.84 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST=> training   0.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.374 DataTime=0.270 Loss=2.050 Prec@1=52.701 Prec@5=77.208 rate=3739.84 Hz, eta=0:00:00, total=0:00:00, wall=13:47 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.374 DataTime=0.270 Loss=2.050 Prec@1=52.701 Prec@5=77.208 rate=3.11 Hz, eta=0:12:51, total=0:00:32, wall=13:47 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.374 DataTime=0.270 Loss=2.050 Prec@1=52.701 Prec@5=77.208 rate=3.11 Hz, eta=0:12:51, total=0:00:32, wall=13:48 IST=> training   4.04% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.356 DataTime=0.254 Loss=2.044 Prec@1=52.586 Prec@5=77.273 rate=3.11 Hz, eta=0:12:51, total=0:00:32, wall=13:48 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.356 DataTime=0.254 Loss=2.044 Prec@1=52.586 Prec@5=77.273 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=13:48 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.356 DataTime=0.254 Loss=2.044 Prec@1=52.586 Prec@5=77.273 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=13:49 IST=> training   8.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.351 DataTime=0.249 Loss=2.044 Prec@1=52.689 Prec@5=77.248 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=13:49 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.351 DataTime=0.249 Loss=2.044 Prec@1=52.689 Prec@5=77.248 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=13:49 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.351 DataTime=0.249 Loss=2.044 Prec@1=52.689 Prec@5=77.248 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=13:49 IST=> training   12.03% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.346 DataTime=0.244 Loss=2.046 Prec@1=52.646 Prec@5=77.171 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=13:49 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.346 DataTime=0.244 Loss=2.046 Prec@1=52.646 Prec@5=77.171 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=13:49 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.346 DataTime=0.244 Loss=2.046 Prec@1=52.646 Prec@5=77.171 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=13:50 IST=> training   16.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.344 DataTime=0.241 Loss=2.047 Prec@1=52.613 Prec@5=77.124 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=13:50 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.344 DataTime=0.241 Loss=2.047 Prec@1=52.613 Prec@5=77.124 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=13:50 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.344 DataTime=0.241 Loss=2.047 Prec@1=52.613 Prec@5=77.124 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=13:50 IST=> training   20.02% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.047 Prec@1=52.604 Prec@5=77.123 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=13:50 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.047 Prec@1=52.604 Prec@5=77.123 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=13:50 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.047 Prec@1=52.604 Prec@5=77.123 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=13:51 IST=> training   24.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.048 Prec@1=52.618 Prec@5=77.066 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=13:51 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.048 Prec@1=52.618 Prec@5=77.066 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=13:51 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.342 DataTime=0.238 Loss=2.048 Prec@1=52.618 Prec@5=77.066 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=13:51 IST=> training   28.01% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.050 Prec@1=52.581 Prec@5=77.019 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=13:51 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.050 Prec@1=52.581 Prec@5=77.019 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=13:51 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.050 Prec@1=52.581 Prec@5=77.019 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=13:52 IST=> training   32.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.236 Loss=2.052 Prec@1=52.550 Prec@5=77.004 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=13:52 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.236 Loss=2.052 Prec@1=52.550 Prec@5=77.004 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=13:52 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.236 Loss=2.052 Prec@1=52.550 Prec@5=77.004 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=13:52 IST=> training   36.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.054 Prec@1=52.472 Prec@5=76.979 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=13:52 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.054 Prec@1=52.472 Prec@5=76.979 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:52 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.341 DataTime=0.237 Loss=2.054 Prec@1=52.472 Prec@5=76.979 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:53 IST=> training   39.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.054 Prec@1=52.468 Prec@5=76.985 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=13:53 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.054 Prec@1=52.468 Prec@5=76.985 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:53 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.054 Prec@1=52.468 Prec@5=76.985 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:54 IST=> training   43.99% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.055 Prec@1=52.414 Prec@5=76.973 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=13:54 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.055 Prec@1=52.414 Prec@5=76.973 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=13:54 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.055 Prec@1=52.414 Prec@5=76.973 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=13:54 IST=> training   47.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.056 Prec@1=52.385 Prec@5=76.942 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=13:54 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.056 Prec@1=52.385 Prec@5=76.942 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=13:54 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.340 DataTime=0.235 Loss=2.056 Prec@1=52.385 Prec@5=76.942 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=13:55 IST=> training   51.98% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.058 Prec@1=52.370 Prec@5=76.928 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=13:55 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.058 Prec@1=52.370 Prec@5=76.928 rate=2.99 Hz, eta=0:06:08, total=0:07:49, wall=13:55 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.058 Prec@1=52.370 Prec@5=76.928 rate=2.99 Hz, eta=0:06:08, total=0:07:49, wall=13:55 IST=> training   55.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.058 Prec@1=52.368 Prec@5=76.909 rate=2.99 Hz, eta=0:06:08, total=0:07:49, wall=13:55 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.058 Prec@1=52.368 Prec@5=76.909 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=13:55 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.058 Prec@1=52.368 Prec@5=76.909 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=13:56 IST=> training   59.97% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.059 Prec@1=52.367 Prec@5=76.893 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=13:56 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.059 Prec@1=52.367 Prec@5=76.893 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:56 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.059 Prec@1=52.367 Prec@5=76.893 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:56 IST=> training   63.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.061 Prec@1=52.326 Prec@5=76.865 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=13:56 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.061 Prec@1=52.326 Prec@5=76.865 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=13:56 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.234 Loss=2.061 Prec@1=52.326 Prec@5=76.865 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=13:57 IST=> training   67.96% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.062 Prec@1=52.308 Prec@5=76.835 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=13:57 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.062 Prec@1=52.308 Prec@5=76.835 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=13:57 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.062 Prec@1=52.308 Prec@5=76.835 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=13:58 IST=> training   71.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.063 Prec@1=52.288 Prec@5=76.815 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=13:58 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.063 Prec@1=52.288 Prec@5=76.815 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:58 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.063 Prec@1=52.288 Prec@5=76.815 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:58 IST=> training   75.95% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.064 Prec@1=52.287 Prec@5=76.805 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=13:58 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.064 Prec@1=52.287 Prec@5=76.805 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:58 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.064 Prec@1=52.287 Prec@5=76.805 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:59 IST=> training   79.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.065 Prec@1=52.257 Prec@5=76.794 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=13:59 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.065 Prec@1=52.257 Prec@5=76.794 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:59 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.065 Prec@1=52.257 Prec@5=76.794 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:59 IST=> training   83.94% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.244 Prec@5=76.788 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=13:59 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.244 Prec@5=76.788 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=13:59 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.244 Prec@5=76.788 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=14:00 IST=> training   87.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.236 Prec@5=76.771 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=14:00 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.236 Prec@5=76.771 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=14:00 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.339 DataTime=0.234 Loss=2.066 Prec@1=52.236 Prec@5=76.771 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=14:00 IST=> training   91.93% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.228 Prec@5=76.763 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=14:00 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.228 Prec@5=76.763 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=14:00 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.228 Prec@5=76.763 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=14:01 IST=> training   95.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.221 Prec@5=76.762 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=14:01 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.221 Prec@5=76.762 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=14:01 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.221 Prec@5=76.762 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=14:01 IST=> training   99.92% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.221 Prec@5=76.763 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=14:01 IST=> training   100.00% of 1x2503...Epoch=60/150 LR=0.06644 Time=0.338 DataTime=0.233 Loss=2.067 Prec@1=52.221 Prec@5=76.763 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=14:01 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:01 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:01 IST=> validation 0.00% of 1x98...Epoch=60/150 LR=0.06644 Time=6.261 Loss=2.299 Prec@1=49.805 Prec@5=73.438 rate=0 Hz, eta=?, total=0:00:00, wall=14:01 IST=> validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=6.261 Loss=2.299 Prec@1=49.805 Prec@5=73.438 rate=5985.41 Hz, eta=0:00:00, total=0:00:00, wall=14:01 IST** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=6.261 Loss=2.299 Prec@1=49.805 Prec@5=73.438 rate=5985.41 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST** validation 1.02% of 1x98...Epoch=60/150 LR=0.06644 Time=0.394 Loss=2.331 Prec@1=46.948 Prec@5=72.736 rate=5985.41 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST** validation 100.00% of 1x98...Epoch=60/150 LR=0.06644 Time=0.394 Loss=2.331 Prec@1=46.948 Prec@5=72.736 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=14:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST=> training   0.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=5.482 DataTime=5.339 Loss=1.995 Prec@1=53.906 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=5.482 DataTime=5.339 Loss=1.995 Prec@1=53.906 Prec@5=77.344 rate=5957.42 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=5.482 DataTime=5.339 Loss=1.995 Prec@1=53.906 Prec@5=77.344 rate=5957.42 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST=> training   0.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.383 DataTime=0.281 Loss=2.025 Prec@1=52.968 Prec@5=77.384 rate=5957.42 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.383 DataTime=0.281 Loss=2.025 Prec@1=52.968 Prec@5=77.384 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:02 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.383 DataTime=0.281 Loss=2.025 Prec@1=52.968 Prec@5=77.384 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:03 IST=> training   4.04% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.359 DataTime=0.255 Loss=2.031 Prec@1=53.011 Prec@5=77.289 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:03 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.359 DataTime=0.255 Loss=2.031 Prec@1=53.011 Prec@5=77.289 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=14:03 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.359 DataTime=0.255 Loss=2.031 Prec@1=53.011 Prec@5=77.289 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=14:03 IST=> training   8.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.352 DataTime=0.247 Loss=2.033 Prec@1=52.868 Prec@5=77.246 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=14:03 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.352 DataTime=0.247 Loss=2.033 Prec@1=52.868 Prec@5=77.246 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=14:03 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.352 DataTime=0.247 Loss=2.033 Prec@1=52.868 Prec@5=77.246 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=14:04 IST=> training   12.03% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.348 DataTime=0.242 Loss=2.033 Prec@1=52.842 Prec@5=77.237 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=14:04 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.348 DataTime=0.242 Loss=2.033 Prec@1=52.842 Prec@5=77.237 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=14:04 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.348 DataTime=0.242 Loss=2.033 Prec@1=52.842 Prec@5=77.237 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=14:04 IST=> training   16.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.240 Loss=2.038 Prec@1=52.755 Prec@5=77.201 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=14:04 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.240 Loss=2.038 Prec@1=52.755 Prec@5=77.201 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=14:04 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.240 Loss=2.038 Prec@1=52.755 Prec@5=77.201 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=14:05 IST=> training   20.02% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.241 Loss=2.041 Prec@1=52.664 Prec@5=77.161 rate=2.98 Hz, eta=0:11:10, total=0:02:47, wall=14:05 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.241 Loss=2.041 Prec@1=52.664 Prec@5=77.161 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=14:05 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.346 DataTime=0.241 Loss=2.041 Prec@1=52.664 Prec@5=77.161 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=14:06 IST=> training   24.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.344 DataTime=0.238 Loss=2.042 Prec@1=52.627 Prec@5=77.134 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=14:06 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.344 DataTime=0.238 Loss=2.042 Prec@1=52.627 Prec@5=77.134 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=14:06 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.344 DataTime=0.238 Loss=2.042 Prec@1=52.627 Prec@5=77.134 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=14:06 IST=> training   28.01% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.569 Prec@5=77.088 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=14:06 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.569 Prec@5=77.088 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=14:06 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.569 Prec@5=77.088 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=14:07 IST=> training   32.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.557 Prec@5=77.076 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=14:07 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.557 Prec@5=77.076 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=14:07 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.237 Loss=2.046 Prec@1=52.557 Prec@5=77.076 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=14:07 IST=> training   36.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.048 Prec@1=52.541 Prec@5=77.063 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=14:07 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.048 Prec@1=52.541 Prec@5=77.063 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=14:07 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.048 Prec@1=52.541 Prec@5=77.063 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=14:08 IST=> training   39.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.049 Prec@1=52.530 Prec@5=77.051 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=14:08 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.049 Prec@1=52.530 Prec@5=77.051 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=14:08 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.236 Loss=2.049 Prec@1=52.530 Prec@5=77.051 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=14:08 IST=> training   43.99% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.235 Loss=2.051 Prec@1=52.509 Prec@5=77.019 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=14:08 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.235 Loss=2.051 Prec@1=52.509 Prec@5=77.019 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=14:08 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.342 DataTime=0.235 Loss=2.051 Prec@1=52.509 Prec@5=77.019 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=14:09 IST=> training   47.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.234 Loss=2.051 Prec@1=52.500 Prec@5=77.018 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=14:09 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.234 Loss=2.051 Prec@1=52.500 Prec@5=77.018 rate=2.98 Hz, eta=0:06:44, total=0:07:17, wall=14:09 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.234 Loss=2.051 Prec@1=52.500 Prec@5=77.018 rate=2.98 Hz, eta=0:06:44, total=0:07:17, wall=14:10 IST=> training   51.98% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.483 Prec@5=77.001 rate=2.98 Hz, eta=0:06:44, total=0:07:17, wall=14:10 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.483 Prec@5=77.001 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:10 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.483 Prec@5=77.001 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:10 IST=> training   55.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.467 Prec@5=76.998 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:10 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.467 Prec@5=76.998 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:10 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.052 Prec@1=52.467 Prec@5=76.998 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:11 IST=> training   59.97% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.053 Prec@1=52.443 Prec@5=76.986 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:11 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.053 Prec@1=52.443 Prec@5=76.986 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:11 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.053 Prec@1=52.443 Prec@5=76.986 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:11 IST=> training   63.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.054 Prec@1=52.426 Prec@5=76.967 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:11 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.054 Prec@1=52.426 Prec@5=76.967 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=14:11 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.341 DataTime=0.234 Loss=2.054 Prec@1=52.426 Prec@5=76.967 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=14:12 IST=> training   67.96% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.056 Prec@1=52.392 Prec@5=76.949 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=14:12 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.056 Prec@1=52.392 Prec@5=76.949 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=14:12 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.056 Prec@1=52.392 Prec@5=76.949 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=14:12 IST=> training   71.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.057 Prec@1=52.382 Prec@5=76.925 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=14:12 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.057 Prec@1=52.382 Prec@5=76.925 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:12 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.057 Prec@1=52.382 Prec@5=76.925 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:13 IST=> training   75.95% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.058 Prec@1=52.359 Prec@5=76.909 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:13 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.058 Prec@1=52.359 Prec@5=76.909 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=14:13 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.058 Prec@1=52.359 Prec@5=76.909 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=14:13 IST=> training   79.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.059 Prec@1=52.335 Prec@5=76.903 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=14:13 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.059 Prec@1=52.335 Prec@5=76.903 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=14:13 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.233 Loss=2.059 Prec@1=52.335 Prec@5=76.903 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=14:14 IST=> training   83.94% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.314 Prec@5=76.888 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=14:14 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.314 Prec@5=76.888 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=14:14 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.314 Prec@5=76.888 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=14:15 IST=> training   87.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.319 Prec@5=76.879 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=14:15 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.319 Prec@5=76.879 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=14:15 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.060 Prec@1=52.319 Prec@5=76.879 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=14:15 IST=> training   91.93% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.061 Prec@1=52.295 Prec@5=76.863 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=14:15 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.061 Prec@1=52.295 Prec@5=76.863 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=14:15 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.340 DataTime=0.232 Loss=2.061 Prec@1=52.295 Prec@5=76.863 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=14:16 IST=> training   95.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.339 DataTime=0.231 Loss=2.061 Prec@1=52.292 Prec@5=76.869 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=14:16 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.339 DataTime=0.231 Loss=2.061 Prec@1=52.292 Prec@5=76.869 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=14:16 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.339 DataTime=0.231 Loss=2.061 Prec@1=52.292 Prec@5=76.869 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=14:16 IST=> training   99.92% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.339 DataTime=0.231 Loss=2.061 Prec@1=52.293 Prec@5=76.869 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=14:16 IST=> training   100.00% of 1x2503...Epoch=61/150 LR=0.06545 Time=0.339 DataTime=0.231 Loss=2.061 Prec@1=52.293 Prec@5=76.869 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=14:16 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> validation 0.00% of 1x98...Epoch=61/150 LR=0.06545 Time=6.484 Loss=2.201 Prec@1=48.242 Prec@5=74.414 rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.484 Loss=2.201 Prec@1=48.242 Prec@5=74.414 rate=6589.70 Hz, eta=0:00:00, total=0:00:00, wall=14:16 IST** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=6.484 Loss=2.201 Prec@1=48.242 Prec@5=74.414 rate=6589.70 Hz, eta=0:00:00, total=0:00:00, wall=14:16 IST** validation 1.02% of 1x98...Epoch=61/150 LR=0.06545 Time=0.388 Loss=2.207 Prec@1=49.434 Prec@5=74.842 rate=6589.70 Hz, eta=0:00:00, total=0:00:00, wall=14:16 IST** validation 100.00% of 1x98...Epoch=61/150 LR=0.06545 Time=0.388 Loss=2.207 Prec@1=49.434 Prec@5=74.842 rate=3.10 Hz, eta=0:00:00, total=0:00:31, wall=14:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> training   0.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.685 DataTime=5.438 Loss=1.993 Prec@1=55.859 Prec@5=76.367 rate=0 Hz, eta=?, total=0:00:00, wall=14:16 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.685 DataTime=5.438 Loss=1.993 Prec@1=55.859 Prec@5=76.367 rate=5168.28 Hz, eta=0:00:00, total=0:00:00, wall=14:16 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=5.685 DataTime=5.438 Loss=1.993 Prec@1=55.859 Prec@5=76.367 rate=5168.28 Hz, eta=0:00:00, total=0:00:00, wall=14:17 IST=> training   0.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.383 DataTime=0.287 Loss=2.027 Prec@1=53.094 Prec@5=77.508 rate=5168.28 Hz, eta=0:00:00, total=0:00:00, wall=14:17 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.383 DataTime=0.287 Loss=2.027 Prec@1=53.094 Prec@5=77.508 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=14:17 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.383 DataTime=0.287 Loss=2.027 Prec@1=53.094 Prec@5=77.508 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=14:18 IST=> training   4.04% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.357 DataTime=0.260 Loss=2.032 Prec@1=52.861 Prec@5=77.450 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=14:18 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.357 DataTime=0.260 Loss=2.032 Prec@1=52.861 Prec@5=77.450 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=14:18 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.357 DataTime=0.260 Loss=2.032 Prec@1=52.861 Prec@5=77.450 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=14:18 IST=> training   8.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.349 DataTime=0.253 Loss=2.031 Prec@1=52.888 Prec@5=77.474 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=14:18 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.349 DataTime=0.253 Loss=2.031 Prec@1=52.888 Prec@5=77.474 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=14:18 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.349 DataTime=0.253 Loss=2.031 Prec@1=52.888 Prec@5=77.474 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=14:19 IST=> training   12.03% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.344 DataTime=0.248 Loss=2.030 Prec@1=52.859 Prec@5=77.493 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=14:19 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.344 DataTime=0.248 Loss=2.030 Prec@1=52.859 Prec@5=77.493 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=14:19 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.344 DataTime=0.248 Loss=2.030 Prec@1=52.859 Prec@5=77.493 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=14:19 IST=> training   16.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.341 DataTime=0.244 Loss=2.032 Prec@1=52.873 Prec@5=77.421 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=14:19 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.341 DataTime=0.244 Loss=2.032 Prec@1=52.873 Prec@5=77.421 rate=3.04 Hz, eta=0:10:59, total=0:02:45, wall=14:19 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.341 DataTime=0.244 Loss=2.032 Prec@1=52.873 Prec@5=77.421 rate=3.04 Hz, eta=0:10:59, total=0:02:45, wall=14:20 IST=> training   20.02% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.034 Prec@1=52.826 Prec@5=77.359 rate=3.04 Hz, eta=0:10:59, total=0:02:45, wall=14:20 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.034 Prec@1=52.826 Prec@5=77.359 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=14:20 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.034 Prec@1=52.826 Prec@5=77.359 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=14:20 IST=> training   24.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.038 Prec@1=52.743 Prec@5=77.284 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=14:20 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.038 Prec@1=52.743 Prec@5=77.284 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=14:20 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.340 DataTime=0.242 Loss=2.038 Prec@1=52.743 Prec@5=77.284 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=14:21 IST=> training   28.01% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.038 Prec@1=52.767 Prec@5=77.268 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=14:21 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.038 Prec@1=52.767 Prec@5=77.268 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=14:21 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.038 Prec@1=52.767 Prec@5=77.268 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=14:21 IST=> training   32.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.039 Prec@1=52.753 Prec@5=77.233 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=14:21 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.039 Prec@1=52.753 Prec@5=77.233 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=14:21 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.338 DataTime=0.238 Loss=2.039 Prec@1=52.753 Prec@5=77.233 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=14:22 IST=> training   36.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.237 Loss=2.040 Prec@1=52.721 Prec@5=77.221 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=14:22 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.237 Loss=2.040 Prec@1=52.721 Prec@5=77.221 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=14:22 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.237 Loss=2.040 Prec@1=52.721 Prec@5=77.221 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=14:23 IST=> training   39.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.041 Prec@1=52.702 Prec@5=77.181 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=14:23 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.041 Prec@1=52.702 Prec@5=77.181 rate=3.02 Hz, eta=0:07:44, total=0:06:05, wall=14:23 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.041 Prec@1=52.702 Prec@5=77.181 rate=3.02 Hz, eta=0:07:44, total=0:06:05, wall=14:23 IST=> training   43.99% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.044 Prec@1=52.671 Prec@5=77.150 rate=3.02 Hz, eta=0:07:44, total=0:06:05, wall=14:23 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.044 Prec@1=52.671 Prec@5=77.150 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=14:23 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.236 Loss=2.044 Prec@1=52.671 Prec@5=77.150 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=14:24 IST=> training   47.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.235 Loss=2.045 Prec@1=52.643 Prec@5=77.138 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=14:24 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.235 Loss=2.045 Prec@1=52.643 Prec@5=77.138 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=14:24 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.337 DataTime=0.235 Loss=2.045 Prec@1=52.643 Prec@5=77.138 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=14:24 IST=> training   51.98% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.048 Prec@1=52.621 Prec@5=77.093 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=14:24 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.048 Prec@1=52.621 Prec@5=77.093 rate=3.01 Hz, eta=0:06:05, total=0:07:44, wall=14:24 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.048 Prec@1=52.621 Prec@5=77.093 rate=3.01 Hz, eta=0:06:05, total=0:07:44, wall=14:25 IST=> training   55.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.047 Prec@1=52.629 Prec@5=77.110 rate=3.01 Hz, eta=0:06:05, total=0:07:44, wall=14:25 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.047 Prec@1=52.629 Prec@5=77.110 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=14:25 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.336 DataTime=0.234 Loss=2.047 Prec@1=52.629 Prec@5=77.110 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=14:25 IST=> training   59.97% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.049 Prec@1=52.587 Prec@5=77.076 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=14:25 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.049 Prec@1=52.587 Prec@5=77.076 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=14:25 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.049 Prec@1=52.587 Prec@5=77.076 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=14:26 IST=> training   63.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.576 Prec@5=77.060 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=14:26 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.576 Prec@5=77.060 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=14:26 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.576 Prec@5=77.060 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=14:26 IST=> training   67.96% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.572 Prec@5=77.057 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=14:26 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.572 Prec@5=77.057 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=14:26 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.050 Prec@1=52.572 Prec@5=77.057 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=14:27 IST=> training   71.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.052 Prec@1=52.538 Prec@5=77.037 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=14:27 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.052 Prec@1=52.538 Prec@5=77.037 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=14:27 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.233 Loss=2.052 Prec@1=52.538 Prec@5=77.037 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=14:28 IST=> training   75.95% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.526 Prec@5=77.026 rate=3.01 Hz, eta=0:03:19, total=0:10:30, wall=14:28 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.526 Prec@5=77.026 rate=3.02 Hz, eta=0:02:46, total=0:11:02, wall=14:28 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.526 Prec@5=77.026 rate=3.02 Hz, eta=0:02:46, total=0:11:02, wall=14:28 IST=> training   79.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.520 Prec@5=77.032 rate=3.02 Hz, eta=0:02:46, total=0:11:02, wall=14:28 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.520 Prec@5=77.032 rate=3.01 Hz, eta=0:02:13, total=0:11:36, wall=14:28 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.052 Prec@1=52.520 Prec@5=77.032 rate=3.01 Hz, eta=0:02:13, total=0:11:36, wall=14:29 IST=> training   83.94% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.506 Prec@5=77.022 rate=3.01 Hz, eta=0:02:13, total=0:11:36, wall=14:29 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.506 Prec@5=77.022 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=14:29 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.506 Prec@5=77.022 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=14:29 IST=> training   87.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.486 Prec@5=77.014 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=14:29 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.486 Prec@5=77.014 rate=3.02 Hz, eta=0:01:06, total=0:12:42, wall=14:29 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.053 Prec@1=52.486 Prec@5=77.014 rate=3.02 Hz, eta=0:01:06, total=0:12:42, wall=14:30 IST=> training   91.93% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.232 Loss=2.054 Prec@1=52.460 Prec@5=76.983 rate=3.02 Hz, eta=0:01:06, total=0:12:42, wall=14:30 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.232 Loss=2.054 Prec@1=52.460 Prec@5=76.983 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=14:30 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.335 DataTime=0.232 Loss=2.054 Prec@1=52.460 Prec@5=76.983 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=14:30 IST=> training   95.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.056 Prec@1=52.442 Prec@5=76.962 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=14:30 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.056 Prec@1=52.442 Prec@5=76.962 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=14:30 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.056 Prec@1=52.442 Prec@5=76.962 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=14:30 IST=> training   99.92% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.056 Prec@1=52.443 Prec@5=76.962 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=14:30 IST=> training   100.00% of 1x2503...Epoch=62/150 LR=0.06445 Time=0.334 DataTime=0.232 Loss=2.056 Prec@1=52.443 Prec@5=76.962 rate=3.02 Hz, eta=0:00:00, total=0:13:49, wall=14:30 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 0.00% of 1x98...Epoch=62/150 LR=0.06445 Time=7.075 Loss=2.306 Prec@1=47.266 Prec@5=73.828 rate=0 Hz, eta=?, total=0:00:00, wall=14:30 IST=> validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=7.075 Loss=2.306 Prec@1=47.266 Prec@5=73.828 rate=8613.41 Hz, eta=0:00:00, total=0:00:00, wall=14:30 IST** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=7.075 Loss=2.306 Prec@1=47.266 Prec@5=73.828 rate=8613.41 Hz, eta=0:00:00, total=0:00:00, wall=14:31 IST** validation 1.02% of 1x98...Epoch=62/150 LR=0.06445 Time=0.405 Loss=2.216 Prec@1=49.140 Prec@5=74.572 rate=8613.41 Hz, eta=0:00:00, total=0:00:00, wall=14:31 IST** validation 100.00% of 1x98...Epoch=62/150 LR=0.06445 Time=0.405 Loss=2.216 Prec@1=49.140 Prec@5=74.572 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=14:31 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training   0.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.241 DataTime=4.982 Loss=2.137 Prec@1=48.828 Prec@5=77.734 rate=0 Hz, eta=?, total=0:00:00, wall=14:31 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.241 DataTime=4.982 Loss=2.137 Prec@1=48.828 Prec@5=77.734 rate=4088.47 Hz, eta=0:00:00, total=0:00:00, wall=14:31 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=5.241 DataTime=4.982 Loss=2.137 Prec@1=48.828 Prec@5=77.734 rate=4088.47 Hz, eta=0:00:00, total=0:00:00, wall=14:32 IST=> training   0.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.381 DataTime=0.280 Loss=2.014 Prec@1=53.334 Prec@5=77.562 rate=4088.47 Hz, eta=0:00:00, total=0:00:00, wall=14:32 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.381 DataTime=0.280 Loss=2.014 Prec@1=53.334 Prec@5=77.562 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:32 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.381 DataTime=0.280 Loss=2.014 Prec@1=53.334 Prec@5=77.562 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:32 IST=> training   4.04% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.355 DataTime=0.254 Loss=2.024 Prec@1=52.992 Prec@5=77.454 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=14:32 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.355 DataTime=0.254 Loss=2.024 Prec@1=52.992 Prec@5=77.454 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=14:32 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.355 DataTime=0.254 Loss=2.024 Prec@1=52.992 Prec@5=77.454 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=14:33 IST=> training   8.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.349 DataTime=0.246 Loss=2.019 Prec@1=53.080 Prec@5=77.551 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=14:33 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.349 DataTime=0.246 Loss=2.019 Prec@1=53.080 Prec@5=77.551 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=14:33 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.349 DataTime=0.246 Loss=2.019 Prec@1=53.080 Prec@5=77.551 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=14:33 IST=> training   12.03% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.025 Prec@1=52.934 Prec@5=77.435 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=14:33 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.025 Prec@1=52.934 Prec@5=77.435 rate=3.00 Hz, eta=0:11:39, total=0:02:13, wall=14:33 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.025 Prec@1=52.934 Prec@5=77.435 rate=3.00 Hz, eta=0:11:39, total=0:02:13, wall=14:34 IST=> training   16.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.026 Prec@1=52.888 Prec@5=77.430 rate=3.00 Hz, eta=0:11:39, total=0:02:13, wall=14:34 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.026 Prec@1=52.888 Prec@5=77.430 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=14:34 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.346 DataTime=0.241 Loss=2.026 Prec@1=52.888 Prec@5=77.430 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=14:34 IST=> training   20.02% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.344 DataTime=0.240 Loss=2.031 Prec@1=52.824 Prec@5=77.383 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=14:34 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.344 DataTime=0.240 Loss=2.031 Prec@1=52.824 Prec@5=77.383 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=14:34 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.344 DataTime=0.240 Loss=2.031 Prec@1=52.824 Prec@5=77.383 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=14:35 IST=> training   24.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.032 Prec@1=52.813 Prec@5=77.337 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=14:35 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.032 Prec@1=52.813 Prec@5=77.337 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=14:35 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.032 Prec@1=52.813 Prec@5=77.337 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=14:36 IST=> training   28.01% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.031 Prec@1=52.823 Prec@5=77.329 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=14:36 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.031 Prec@1=52.823 Prec@5=77.329 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=14:36 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.236 Loss=2.031 Prec@1=52.823 Prec@5=77.329 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=14:36 IST=> training   32.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.342 DataTime=0.235 Loss=2.033 Prec@1=52.818 Prec@5=77.317 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=14:36 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.342 DataTime=0.235 Loss=2.033 Prec@1=52.818 Prec@5=77.317 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=14:36 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.342 DataTime=0.235 Loss=2.033 Prec@1=52.818 Prec@5=77.317 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=14:37 IST=> training   36.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.033 Prec@1=52.784 Prec@5=77.310 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.033 Prec@1=52.784 Prec@5=77.310 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.033 Prec@1=52.784 Prec@5=77.310 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=14:37 IST=> training   39.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.035 Prec@1=52.766 Prec@5=77.287 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=14:37 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.035 Prec@1=52.766 Prec@5=77.287 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=14:37 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.035 Prec@1=52.766 Prec@5=77.287 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=14:38 IST=> training   43.99% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.036 Prec@1=52.748 Prec@5=77.261 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=14:38 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.036 Prec@1=52.748 Prec@5=77.261 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=14:38 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.036 Prec@1=52.748 Prec@5=77.261 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=14:38 IST=> training   47.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.751 Prec@5=77.244 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=14:38 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.751 Prec@5=77.244 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=14:38 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.751 Prec@5=77.244 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=14:39 IST=> training   51.98% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.768 Prec@5=77.255 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=14:39 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.768 Prec@5=77.255 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:39 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.341 DataTime=0.234 Loss=2.038 Prec@1=52.768 Prec@5=77.255 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:40 IST=> training   55.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.038 Prec@1=52.768 Prec@5=77.258 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=14:40 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.038 Prec@1=52.768 Prec@5=77.258 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:40 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.038 Prec@1=52.768 Prec@5=77.258 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:40 IST=> training   59.97% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.040 Prec@1=52.732 Prec@5=77.229 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=14:40 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.040 Prec@1=52.732 Prec@5=77.229 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:40 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.040 Prec@1=52.732 Prec@5=77.229 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:41 IST=> training   63.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.041 Prec@1=52.698 Prec@5=77.199 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=14:41 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.041 Prec@1=52.698 Prec@5=77.199 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=14:41 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.041 Prec@1=52.698 Prec@5=77.199 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=14:41 IST=> training   67.96% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.042 Prec@1=52.682 Prec@5=77.190 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=14:41 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.042 Prec@1=52.682 Prec@5=77.190 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=14:41 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.042 Prec@1=52.682 Prec@5=77.190 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=14:42 IST=> training   71.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.043 Prec@1=52.666 Prec@5=77.171 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=14:42 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.043 Prec@1=52.666 Prec@5=77.171 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:42 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.043 Prec@1=52.666 Prec@5=77.171 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:42 IST=> training   75.95% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.044 Prec@1=52.640 Prec@5=77.148 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=14:42 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.044 Prec@1=52.640 Prec@5=77.148 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=14:42 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.044 Prec@1=52.640 Prec@5=77.148 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=14:43 IST=> training   79.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.045 Prec@1=52.609 Prec@5=77.132 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=14:43 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.045 Prec@1=52.609 Prec@5=77.132 rate=2.96 Hz, eta=0:02:15, total=0:11:48, wall=14:43 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.045 Prec@1=52.609 Prec@5=77.132 rate=2.96 Hz, eta=0:02:15, total=0:11:48, wall=14:43 IST=> training   83.94% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.232 Loss=2.046 Prec@1=52.603 Prec@5=77.113 rate=2.96 Hz, eta=0:02:15, total=0:11:48, wall=14:43 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.232 Loss=2.046 Prec@1=52.603 Prec@5=77.113 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=14:43 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.232 Loss=2.046 Prec@1=52.603 Prec@5=77.113 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=14:44 IST=> training   87.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.047 Prec@1=52.588 Prec@5=77.102 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=14:44 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.047 Prec@1=52.588 Prec@5=77.102 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=14:44 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.047 Prec@1=52.588 Prec@5=77.102 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=14:45 IST=> training   91.93% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.048 Prec@1=52.580 Prec@5=77.092 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=14:45 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.048 Prec@1=52.580 Prec@5=77.092 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=14:45 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.340 DataTime=0.233 Loss=2.048 Prec@1=52.580 Prec@5=77.092 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=14:45 IST=> training   95.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.049 Prec@1=52.554 Prec@5=77.073 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=14:45 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.049 Prec@1=52.554 Prec@5=77.073 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=14:45 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.049 Prec@1=52.554 Prec@5=77.073 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=14:45 IST=> training   99.92% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.049 Prec@1=52.554 Prec@5=77.074 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=14:45 IST=> training   100.00% of 1x2503...Epoch=63/150 LR=0.06345 Time=0.339 DataTime=0.232 Loss=2.049 Prec@1=52.554 Prec@5=77.074 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=14:45 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:45 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:45 IST=> validation 0.00% of 1x98...Epoch=63/150 LR=0.06345 Time=7.258 Loss=2.130 Prec@1=53.516 Prec@5=75.391 rate=0 Hz, eta=?, total=0:00:00, wall=14:45 IST=> validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=7.258 Loss=2.130 Prec@1=53.516 Prec@5=75.391 rate=6908.75 Hz, eta=0:00:00, total=0:00:00, wall=14:45 IST** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=7.258 Loss=2.130 Prec@1=53.516 Prec@5=75.391 rate=6908.75 Hz, eta=0:00:00, total=0:00:00, wall=14:46 IST** validation 1.02% of 1x98...Epoch=63/150 LR=0.06345 Time=0.404 Loss=2.129 Prec@1=50.910 Prec@5=76.158 rate=6908.75 Hz, eta=0:00:00, total=0:00:00, wall=14:46 IST** validation 100.00% of 1x98...Epoch=63/150 LR=0.06345 Time=0.404 Loss=2.129 Prec@1=50.910 Prec@5=76.158 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=14:46 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:46 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:46 IST=> training   0.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.666 DataTime=5.453 Loss=1.891 Prec@1=57.422 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=14:46 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.666 DataTime=5.453 Loss=1.891 Prec@1=57.422 Prec@5=80.664 rate=3542.90 Hz, eta=0:00:00, total=0:00:00, wall=14:46 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=5.666 DataTime=5.453 Loss=1.891 Prec@1=57.422 Prec@5=80.664 rate=3542.90 Hz, eta=0:00:00, total=0:00:00, wall=14:46 IST=> training   0.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.381 DataTime=0.290 Loss=1.992 Prec@1=53.902 Prec@5=77.806 rate=3542.90 Hz, eta=0:00:00, total=0:00:00, wall=14:46 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.381 DataTime=0.290 Loss=1.992 Prec@1=53.902 Prec@5=77.806 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=14:46 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.381 DataTime=0.290 Loss=1.992 Prec@1=53.902 Prec@5=77.806 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=14:47 IST=> training   4.04% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.352 DataTime=0.262 Loss=2.004 Prec@1=53.462 Prec@5=77.642 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=14:47 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.352 DataTime=0.262 Loss=2.004 Prec@1=53.462 Prec@5=77.642 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=14:47 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.352 DataTime=0.262 Loss=2.004 Prec@1=53.462 Prec@5=77.642 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=14:48 IST=> training   8.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.347 DataTime=0.254 Loss=2.007 Prec@1=53.388 Prec@5=77.609 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=14:48 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.347 DataTime=0.254 Loss=2.007 Prec@1=53.388 Prec@5=77.609 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=14:48 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.347 DataTime=0.254 Loss=2.007 Prec@1=53.388 Prec@5=77.609 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=14:48 IST=> training   12.03% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.346 DataTime=0.249 Loss=2.012 Prec@1=53.295 Prec@5=77.494 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=14:48 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.346 DataTime=0.249 Loss=2.012 Prec@1=53.295 Prec@5=77.494 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=14:48 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.346 DataTime=0.249 Loss=2.012 Prec@1=53.295 Prec@5=77.494 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=14:49 IST=> training   16.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.342 DataTime=0.244 Loss=2.016 Prec@1=53.189 Prec@5=77.430 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=14:49 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.342 DataTime=0.244 Loss=2.016 Prec@1=53.189 Prec@5=77.430 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=14:49 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.342 DataTime=0.244 Loss=2.016 Prec@1=53.189 Prec@5=77.430 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=14:49 IST=> training   20.02% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.017 Prec@1=53.195 Prec@5=77.449 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=14:49 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.017 Prec@1=53.195 Prec@5=77.449 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=14:49 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.017 Prec@1=53.195 Prec@5=77.449 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=14:50 IST=> training   24.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.021 Prec@1=53.111 Prec@5=77.408 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=14:50 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.021 Prec@1=53.111 Prec@5=77.408 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=14:50 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.341 DataTime=0.242 Loss=2.021 Prec@1=53.111 Prec@5=77.408 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=14:50 IST=> training   28.01% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.339 DataTime=0.240 Loss=2.024 Prec@1=53.015 Prec@5=77.355 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=14:50 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.339 DataTime=0.240 Loss=2.024 Prec@1=53.015 Prec@5=77.355 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=14:50 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.339 DataTime=0.240 Loss=2.024 Prec@1=53.015 Prec@5=77.355 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=14:51 IST=> training   32.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.238 Loss=2.026 Prec@1=52.933 Prec@5=77.332 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=14:51 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.238 Loss=2.026 Prec@1=52.933 Prec@5=77.332 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=14:51 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.238 Loss=2.026 Prec@1=52.933 Prec@5=77.332 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=14:51 IST=> training   36.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.238 Loss=2.027 Prec@1=52.933 Prec@5=77.320 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=14:51 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.238 Loss=2.027 Prec@1=52.933 Prec@5=77.320 rate=3.01 Hz, eta=0:08:19, total=0:05:33, wall=14:51 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.238 Loss=2.027 Prec@1=52.933 Prec@5=77.320 rate=3.01 Hz, eta=0:08:19, total=0:05:33, wall=14:52 IST=> training   39.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.029 Prec@1=52.910 Prec@5=77.297 rate=3.01 Hz, eta=0:08:19, total=0:05:33, wall=14:52 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.029 Prec@1=52.910 Prec@5=77.297 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=14:52 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.029 Prec@1=52.910 Prec@5=77.297 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=14:53 IST=> training   43.99% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.030 Prec@1=52.887 Prec@5=77.281 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=14:53 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.030 Prec@1=52.887 Prec@5=77.281 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=14:53 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.237 Loss=2.030 Prec@1=52.887 Prec@5=77.281 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=14:53 IST=> training   47.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.237 Loss=2.033 Prec@1=52.830 Prec@5=77.245 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=14:53 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.237 Loss=2.033 Prec@1=52.830 Prec@5=77.245 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=14:53 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.237 Loss=2.033 Prec@1=52.830 Prec@5=77.245 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=14:54 IST=> training   51.98% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.799 Prec@5=77.243 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=14:54 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.799 Prec@5=77.243 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=14:54 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.799 Prec@5=77.243 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=14:54 IST=> training   55.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.791 Prec@5=77.222 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=14:54 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.791 Prec@5=77.222 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=14:54 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.034 Prec@1=52.791 Prec@5=77.222 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=14:55 IST=> training   59.97% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.036 Prec@1=52.758 Prec@5=77.202 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=14:55 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.036 Prec@1=52.758 Prec@5=77.202 rate=3.00 Hz, eta=0:05:01, total=0:08:54, wall=14:55 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.236 Loss=2.036 Prec@1=52.758 Prec@5=77.202 rate=3.00 Hz, eta=0:05:01, total=0:08:54, wall=14:55 IST=> training   63.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.236 Loss=2.037 Prec@1=52.734 Prec@5=77.196 rate=3.00 Hz, eta=0:05:01, total=0:08:54, wall=14:55 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.236 Loss=2.037 Prec@1=52.734 Prec@5=77.196 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=14:55 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.236 Loss=2.037 Prec@1=52.734 Prec@5=77.196 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=14:56 IST=> training   67.96% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.038 Prec@1=52.712 Prec@5=77.166 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=14:56 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.038 Prec@1=52.712 Prec@5=77.166 rate=3.00 Hz, eta=0:03:54, total=0:10:01, wall=14:56 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.038 Prec@1=52.712 Prec@5=77.166 rate=3.00 Hz, eta=0:03:54, total=0:10:01, wall=14:57 IST=> training   71.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.235 Loss=2.038 Prec@1=52.713 Prec@5=77.176 rate=3.00 Hz, eta=0:03:54, total=0:10:01, wall=14:57 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.235 Loss=2.038 Prec@1=52.713 Prec@5=77.176 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=14:57 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.338 DataTime=0.235 Loss=2.038 Prec@1=52.713 Prec@5=77.176 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=14:57 IST=> training   75.95% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.039 Prec@1=52.709 Prec@5=77.163 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=14:57 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.039 Prec@1=52.709 Prec@5=77.163 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=14:57 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.039 Prec@1=52.709 Prec@5=77.163 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=14:58 IST=> training   79.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.040 Prec@1=52.695 Prec@5=77.155 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=14:58 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.040 Prec@1=52.695 Prec@5=77.155 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=14:58 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.235 Loss=2.040 Prec@1=52.695 Prec@5=77.155 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=14:58 IST=> training   83.94% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.041 Prec@1=52.679 Prec@5=77.134 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=14:58 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.041 Prec@1=52.679 Prec@5=77.134 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=14:58 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.041 Prec@1=52.679 Prec@5=77.134 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=14:59 IST=> training   87.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.042 Prec@1=52.669 Prec@5=77.115 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=14:59 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.042 Prec@1=52.669 Prec@5=77.115 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=14:59 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.042 Prec@1=52.669 Prec@5=77.115 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=14:59 IST=> training   91.93% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.043 Prec@1=52.655 Prec@5=77.104 rate=2.99 Hz, eta=0:01:07, total=0:12:50, wall=14:59 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.043 Prec@1=52.655 Prec@5=77.104 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=14:59 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.337 DataTime=0.234 Loss=2.043 Prec@1=52.655 Prec@5=77.104 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=15:00 IST=> training   95.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.336 DataTime=0.233 Loss=2.044 Prec@1=52.634 Prec@5=77.092 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=15:00 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.336 DataTime=0.233 Loss=2.044 Prec@1=52.634 Prec@5=77.092 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=15:00 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.336 DataTime=0.233 Loss=2.044 Prec@1=52.634 Prec@5=77.092 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=15:00 IST=> training   99.92% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.336 DataTime=0.233 Loss=2.044 Prec@1=52.633 Prec@5=77.092 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=15:00 IST=> training   100.00% of 1x2503...Epoch=64/150 LR=0.06243 Time=0.336 DataTime=0.233 Loss=2.044 Prec@1=52.633 Prec@5=77.092 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=15:00 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:00 IST=> validation 0.00% of 1x98...Epoch=64/150 LR=0.06243 Time=6.404 Loss=2.210 Prec@1=48.828 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=15:00 IST=> validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.404 Loss=2.210 Prec@1=48.828 Prec@5=74.609 rate=5381.55 Hz, eta=0:00:00, total=0:00:00, wall=15:00 IST** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=6.404 Loss=2.210 Prec@1=48.828 Prec@5=74.609 rate=5381.55 Hz, eta=0:00:00, total=0:00:00, wall=15:00 IST** validation 1.02% of 1x98...Epoch=64/150 LR=0.06243 Time=0.394 Loss=2.276 Prec@1=48.244 Prec@5=73.450 rate=5381.55 Hz, eta=0:00:00, total=0:00:00, wall=15:00 IST** validation 100.00% of 1x98...Epoch=64/150 LR=0.06243 Time=0.394 Loss=2.276 Prec@1=48.244 Prec@5=73.450 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=15:00 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:01 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:01 IST=> training   0.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=5.242 DataTime=5.071 Loss=1.954 Prec@1=53.125 Prec@5=79.492 rate=0 Hz, eta=?, total=0:00:00, wall=15:01 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=5.242 DataTime=5.071 Loss=1.954 Prec@1=53.125 Prec@5=79.492 rate=4404.59 Hz, eta=0:00:00, total=0:00:00, wall=15:01 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=5.242 DataTime=5.071 Loss=1.954 Prec@1=53.125 Prec@5=79.492 rate=4404.59 Hz, eta=0:00:00, total=0:00:00, wall=15:01 IST=> training   0.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.381 DataTime=0.288 Loss=1.991 Prec@1=53.666 Prec@5=78.144 rate=4404.59 Hz, eta=0:00:00, total=0:00:00, wall=15:01 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.381 DataTime=0.288 Loss=1.991 Prec@1=53.666 Prec@5=78.144 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=15:01 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.381 DataTime=0.288 Loss=1.991 Prec@1=53.666 Prec@5=78.144 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=15:02 IST=> training   4.04% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.360 DataTime=0.265 Loss=2.005 Prec@1=53.362 Prec@5=77.779 rate=3.03 Hz, eta=0:13:11, total=0:00:33, wall=15:02 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.360 DataTime=0.265 Loss=2.005 Prec@1=53.362 Prec@5=77.779 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=15:02 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.360 DataTime=0.265 Loss=2.005 Prec@1=53.362 Prec@5=77.779 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=15:02 IST=> training   8.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.352 DataTime=0.254 Loss=2.013 Prec@1=53.189 Prec@5=77.592 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=15:02 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.352 DataTime=0.254 Loss=2.013 Prec@1=53.189 Prec@5=77.592 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=15:02 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.352 DataTime=0.254 Loss=2.013 Prec@1=53.189 Prec@5=77.592 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=15:03 IST=> training   12.03% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.351 DataTime=0.250 Loss=2.019 Prec@1=53.056 Prec@5=77.535 rate=2.99 Hz, eta=0:12:17, total=0:01:40, wall=15:03 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.351 DataTime=0.250 Loss=2.019 Prec@1=53.056 Prec@5=77.535 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=15:03 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.351 DataTime=0.250 Loss=2.019 Prec@1=53.056 Prec@5=77.535 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=15:03 IST=> training   16.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.349 DataTime=0.247 Loss=2.019 Prec@1=53.124 Prec@5=77.492 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=15:03 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.349 DataTime=0.247 Loss=2.019 Prec@1=53.124 Prec@5=77.492 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:03 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.349 DataTime=0.247 Loss=2.019 Prec@1=53.124 Prec@5=77.492 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:04 IST=> training   20.02% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.347 DataTime=0.244 Loss=2.020 Prec@1=53.159 Prec@5=77.486 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:04 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.347 DataTime=0.244 Loss=2.020 Prec@1=53.159 Prec@5=77.486 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=15:04 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.347 DataTime=0.244 Loss=2.020 Prec@1=53.159 Prec@5=77.486 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=15:05 IST=> training   24.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.346 DataTime=0.244 Loss=2.021 Prec@1=53.161 Prec@5=77.498 rate=2.96 Hz, eta=0:10:42, total=0:03:23, wall=15:05 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.346 DataTime=0.244 Loss=2.021 Prec@1=53.161 Prec@5=77.498 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=15:05 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.346 DataTime=0.244 Loss=2.021 Prec@1=53.161 Prec@5=77.498 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=15:05 IST=> training   28.01% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.345 DataTime=0.243 Loss=2.023 Prec@1=53.087 Prec@5=77.448 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=15:05 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.345 DataTime=0.243 Loss=2.023 Prec@1=53.087 Prec@5=77.448 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:05 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.345 DataTime=0.243 Loss=2.023 Prec@1=53.087 Prec@5=77.448 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:06 IST=> training   32.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.344 DataTime=0.241 Loss=2.024 Prec@1=53.074 Prec@5=77.417 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:06 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.344 DataTime=0.241 Loss=2.024 Prec@1=53.074 Prec@5=77.417 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=15:06 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.344 DataTime=0.241 Loss=2.024 Prec@1=53.074 Prec@5=77.417 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=15:06 IST=> training   36.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.343 DataTime=0.241 Loss=2.026 Prec@1=53.038 Prec@5=77.402 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=15:06 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.343 DataTime=0.241 Loss=2.026 Prec@1=53.038 Prec@5=77.402 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=15:06 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.343 DataTime=0.241 Loss=2.026 Prec@1=53.038 Prec@5=77.402 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=15:07 IST=> training   39.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.342 DataTime=0.240 Loss=2.028 Prec@1=53.001 Prec@5=77.369 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=15:07 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.342 DataTime=0.240 Loss=2.028 Prec@1=53.001 Prec@5=77.369 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=15:07 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.342 DataTime=0.240 Loss=2.028 Prec@1=53.001 Prec@5=77.369 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=15:07 IST=> training   43.99% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.029 Prec@1=52.975 Prec@5=77.352 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=15:07 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.029 Prec@1=52.975 Prec@5=77.352 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:07 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.029 Prec@1=52.975 Prec@5=77.352 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:08 IST=> training   47.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.031 Prec@1=52.966 Prec@5=77.332 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:08 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.031 Prec@1=52.966 Prec@5=77.332 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=15:08 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.031 Prec@1=52.966 Prec@5=77.332 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=15:08 IST=> training   51.98% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.946 Prec@5=77.325 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=15:08 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.946 Prec@5=77.325 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=15:08 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.946 Prec@5=77.325 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=15:09 IST=> training   55.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.959 Prec@5=77.306 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=15:09 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.959 Prec@5=77.306 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=15:09 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.032 Prec@1=52.959 Prec@5=77.306 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=15:10 IST=> training   59.97% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.033 Prec@1=52.953 Prec@5=77.292 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=15:10 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.033 Prec@1=52.953 Prec@5=77.292 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=15:10 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.033 Prec@1=52.953 Prec@5=77.292 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=15:10 IST=> training   63.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.034 Prec@1=52.922 Prec@5=77.278 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=15:10 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.034 Prec@1=52.922 Prec@5=77.278 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:10 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.238 Loss=2.034 Prec@1=52.922 Prec@5=77.278 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:11 IST=> training   67.96% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.237 Loss=2.035 Prec@1=52.899 Prec@5=77.251 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:11 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.237 Loss=2.035 Prec@1=52.899 Prec@5=77.251 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=15:11 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.237 Loss=2.035 Prec@1=52.899 Prec@5=77.251 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=15:11 IST=> training   71.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.036 Prec@1=52.873 Prec@5=77.233 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=15:11 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.036 Prec@1=52.873 Prec@5=77.233 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=15:11 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.036 Prec@1=52.873 Prec@5=77.233 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=15:12 IST=> training   75.95% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.038 Prec@1=52.840 Prec@5=77.207 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=15:12 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.038 Prec@1=52.840 Prec@5=77.207 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=15:12 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.237 Loss=2.038 Prec@1=52.840 Prec@5=77.207 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=15:12 IST=> training   79.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.823 Prec@5=77.212 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=15:12 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.823 Prec@5=77.212 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=15:12 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.823 Prec@5=77.212 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=15:13 IST=> training   83.94% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.814 Prec@5=77.219 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=15:13 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.814 Prec@5=77.219 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=15:13 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.038 Prec@1=52.814 Prec@5=77.219 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=15:14 IST=> training   87.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.236 Loss=2.038 Prec@1=52.805 Prec@5=77.221 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=15:14 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.236 Loss=2.038 Prec@1=52.805 Prec@5=77.221 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:14 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.341 DataTime=0.236 Loss=2.038 Prec@1=52.805 Prec@5=77.221 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:14 IST=> training   91.93% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.039 Prec@1=52.796 Prec@5=77.204 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:14 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.039 Prec@1=52.796 Prec@5=77.204 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:14 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.236 Loss=2.039 Prec@1=52.796 Prec@5=77.204 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:15 IST=> training   95.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.235 Loss=2.039 Prec@1=52.790 Prec@5=77.202 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:15 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.235 Loss=2.039 Prec@1=52.790 Prec@5=77.202 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:15 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.235 Loss=2.039 Prec@1=52.790 Prec@5=77.202 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:15 IST=> training   99.92% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.235 Loss=2.039 Prec@1=52.790 Prec@5=77.203 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:15 IST=> training   100.00% of 1x2503...Epoch=65/150 LR=0.06142 Time=0.340 DataTime=0.235 Loss=2.039 Prec@1=52.790 Prec@5=77.203 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=15:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> validation 0.00% of 1x98...Epoch=65/150 LR=0.06142 Time=7.078 Loss=2.342 Prec@1=47.852 Prec@5=72.070 rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.078 Loss=2.342 Prec@1=47.852 Prec@5=72.070 rate=6258.92 Hz, eta=0:00:00, total=0:00:00, wall=15:15 IST** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=7.078 Loss=2.342 Prec@1=47.852 Prec@5=72.070 rate=6258.92 Hz, eta=0:00:00, total=0:00:00, wall=15:15 IST** validation 1.02% of 1x98...Epoch=65/150 LR=0.06142 Time=0.409 Loss=2.271 Prec@1=48.140 Prec@5=73.772 rate=6258.92 Hz, eta=0:00:00, total=0:00:00, wall=15:15 IST** validation 100.00% of 1x98...Epoch=65/150 LR=0.06142 Time=0.409 Loss=2.271 Prec@1=48.140 Prec@5=73.772 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=15:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> training   0.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=5.287 DataTime=5.106 Loss=1.979 Prec@1=52.734 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=15:15 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=5.287 DataTime=5.106 Loss=1.979 Prec@1=52.734 Prec@5=77.344 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=15:15 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=5.287 DataTime=5.106 Loss=1.979 Prec@1=52.734 Prec@5=77.344 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST=> training   0.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.376 DataTime=0.279 Loss=2.005 Prec@1=53.313 Prec@5=77.740 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.376 DataTime=0.279 Loss=2.005 Prec@1=53.313 Prec@5=77.740 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=15:16 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.376 DataTime=0.279 Loss=2.005 Prec@1=53.313 Prec@5=77.740 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=15:17 IST=> training   4.04% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.351 DataTime=0.257 Loss=2.005 Prec@1=53.226 Prec@5=77.746 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=15:17 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.351 DataTime=0.257 Loss=2.005 Prec@1=53.226 Prec@5=77.746 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=15:17 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.351 DataTime=0.257 Loss=2.005 Prec@1=53.226 Prec@5=77.746 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=15:17 IST=> training   8.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.349 DataTime=0.255 Loss=2.004 Prec@1=53.289 Prec@5=77.759 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=15:17 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.349 DataTime=0.255 Loss=2.004 Prec@1=53.289 Prec@5=77.759 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=15:17 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.349 DataTime=0.255 Loss=2.004 Prec@1=53.289 Prec@5=77.759 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=15:18 IST=> training   12.03% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.248 Loss=2.009 Prec@1=53.250 Prec@5=77.635 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=15:18 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.248 Loss=2.009 Prec@1=53.250 Prec@5=77.635 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=15:18 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.248 Loss=2.009 Prec@1=53.250 Prec@5=77.635 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=15:18 IST=> training   16.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.246 Loss=2.009 Prec@1=53.295 Prec@5=77.632 rate=3.03 Hz, eta=0:11:32, total=0:02:12, wall=15:18 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.246 Loss=2.009 Prec@1=53.295 Prec@5=77.632 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=15:18 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.343 DataTime=0.246 Loss=2.009 Prec@1=53.295 Prec@5=77.632 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=15:19 IST=> training   20.02% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.341 DataTime=0.243 Loss=2.010 Prec@1=53.242 Prec@5=77.636 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=15:19 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.341 DataTime=0.243 Loss=2.010 Prec@1=53.242 Prec@5=77.636 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=15:19 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.341 DataTime=0.243 Loss=2.010 Prec@1=53.242 Prec@5=77.636 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=15:19 IST=> training   24.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.240 Loss=2.013 Prec@1=53.231 Prec@5=77.612 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=15:19 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.240 Loss=2.013 Prec@1=53.231 Prec@5=77.612 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=15:19 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.240 Loss=2.013 Prec@1=53.231 Prec@5=77.612 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=15:20 IST=> training   28.01% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.014 Prec@1=53.225 Prec@5=77.595 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=15:20 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.014 Prec@1=53.225 Prec@5=77.595 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=15:20 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.014 Prec@1=53.225 Prec@5=77.595 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=15:20 IST=> training   32.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.015 Prec@1=53.219 Prec@5=77.566 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=15:20 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.015 Prec@1=53.219 Prec@5=77.566 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=15:20 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.339 DataTime=0.239 Loss=2.015 Prec@1=53.219 Prec@5=77.566 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=15:21 IST=> training   36.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.238 Loss=2.016 Prec@1=53.200 Prec@5=77.551 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=15:21 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.238 Loss=2.016 Prec@1=53.200 Prec@5=77.551 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=15:21 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.238 Loss=2.016 Prec@1=53.200 Prec@5=77.551 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=15:22 IST=> training   39.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.237 Loss=2.018 Prec@1=53.160 Prec@5=77.524 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=15:22 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.237 Loss=2.018 Prec@1=53.160 Prec@5=77.524 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=15:22 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.338 DataTime=0.237 Loss=2.018 Prec@1=53.160 Prec@5=77.524 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=15:22 IST=> training   43.99% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.018 Prec@1=53.138 Prec@5=77.526 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=15:22 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.018 Prec@1=53.138 Prec@5=77.526 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=15:22 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.018 Prec@1=53.138 Prec@5=77.526 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=15:23 IST=> training   47.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.020 Prec@1=53.083 Prec@5=77.505 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=15:23 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.020 Prec@1=53.083 Prec@5=77.505 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=15:23 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.020 Prec@1=53.083 Prec@5=77.505 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=15:23 IST=> training   51.98% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.022 Prec@1=53.061 Prec@5=77.475 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=15:23 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.022 Prec@1=53.061 Prec@5=77.475 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=15:23 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.022 Prec@1=53.061 Prec@5=77.475 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=15:24 IST=> training   55.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.023 Prec@1=53.024 Prec@5=77.451 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=15:24 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.023 Prec@1=53.024 Prec@5=77.451 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=15:24 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.023 Prec@1=53.024 Prec@5=77.451 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=15:24 IST=> training   59.97% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.024 Prec@1=53.014 Prec@5=77.430 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=15:24 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.024 Prec@1=53.014 Prec@5=77.430 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=15:24 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.337 DataTime=0.236 Loss=2.024 Prec@1=53.014 Prec@5=77.430 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=15:25 IST=> training   63.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.025 Prec@1=52.986 Prec@5=77.413 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=15:25 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.025 Prec@1=52.986 Prec@5=77.413 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=15:25 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.025 Prec@1=52.986 Prec@5=77.413 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=15:25 IST=> training   67.96% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.026 Prec@1=52.948 Prec@5=77.397 rate=3.00 Hz, eta=0:04:27, total=0:09:26, wall=15:25 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.026 Prec@1=52.948 Prec@5=77.397 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=15:25 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.236 Loss=2.026 Prec@1=52.948 Prec@5=77.397 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=15:26 IST=> training   71.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.027 Prec@1=52.932 Prec@5=77.385 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=15:26 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.027 Prec@1=52.932 Prec@5=77.385 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=15:26 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.027 Prec@1=52.932 Prec@5=77.385 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=15:27 IST=> training   75.95% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.029 Prec@1=52.909 Prec@5=77.377 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=15:27 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.029 Prec@1=52.909 Prec@5=77.377 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=15:27 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.029 Prec@1=52.909 Prec@5=77.377 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=15:27 IST=> training   79.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.030 Prec@1=52.886 Prec@5=77.357 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=15:27 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.030 Prec@1=52.886 Prec@5=77.357 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=15:27 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.030 Prec@1=52.886 Prec@5=77.357 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=15:28 IST=> training   83.94% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.335 DataTime=0.234 Loss=2.030 Prec@1=52.883 Prec@5=77.354 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=15:28 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.335 DataTime=0.234 Loss=2.030 Prec@1=52.883 Prec@5=77.354 rate=3.00 Hz, eta=0:01:40, total=0:12:12, wall=15:28 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.335 DataTime=0.234 Loss=2.030 Prec@1=52.883 Prec@5=77.354 rate=3.00 Hz, eta=0:01:40, total=0:12:12, wall=15:28 IST=> training   87.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.031 Prec@1=52.881 Prec@5=77.343 rate=3.00 Hz, eta=0:01:40, total=0:12:12, wall=15:28 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.031 Prec@1=52.881 Prec@5=77.343 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=15:28 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.235 Loss=2.031 Prec@1=52.881 Prec@5=77.343 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=15:29 IST=> training   91.93% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.872 Prec@5=77.338 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=15:29 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.872 Prec@5=77.338 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=15:29 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.872 Prec@5=77.338 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=15:29 IST=> training   95.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.873 Prec@5=77.332 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=15:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.873 Prec@5=77.332 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=15:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.873 Prec@5=77.332 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=15:29 IST=> training   99.92% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.872 Prec@5=77.331 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=15:29 IST=> training   100.00% of 1x2503...Epoch=66/150 LR=0.06040 Time=0.336 DataTime=0.234 Loss=2.031 Prec@1=52.872 Prec@5=77.331 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=15:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> validation 0.00% of 1x98...Epoch=66/150 LR=0.06040 Time=6.114 Loss=2.047 Prec@1=50.977 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.114 Loss=2.047 Prec@1=50.977 Prec@5=76.953 rate=6854.53 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=6.114 Loss=2.047 Prec@1=50.977 Prec@5=76.953 rate=6854.53 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST** validation 1.02% of 1x98...Epoch=66/150 LR=0.06040 Time=0.400 Loss=2.125 Prec@1=50.702 Prec@5=76.146 rate=6854.53 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST** validation 100.00% of 1x98...Epoch=66/150 LR=0.06040 Time=0.400 Loss=2.125 Prec@1=50.702 Prec@5=76.146 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=15:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> training   0.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.846 DataTime=5.694 Loss=1.925 Prec@1=55.469 Prec@5=78.711 rate=0 Hz, eta=?, total=0:00:00, wall=15:30 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.846 DataTime=5.694 Loss=1.925 Prec@1=55.469 Prec@5=78.711 rate=7181.54 Hz, eta=0:00:00, total=0:00:00, wall=15:30 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=5.846 DataTime=5.694 Loss=1.925 Prec@1=55.469 Prec@5=78.711 rate=7181.54 Hz, eta=0:00:00, total=0:00:00, wall=15:31 IST=> training   0.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.385 DataTime=0.289 Loss=2.004 Prec@1=53.344 Prec@5=77.659 rate=7181.54 Hz, eta=0:00:00, total=0:00:00, wall=15:31 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.385 DataTime=0.289 Loss=2.004 Prec@1=53.344 Prec@5=77.659 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=15:31 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.385 DataTime=0.289 Loss=2.004 Prec@1=53.344 Prec@5=77.659 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=15:31 IST=> training   4.04% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.357 DataTime=0.259 Loss=1.995 Prec@1=53.454 Prec@5=77.896 rate=3.06 Hz, eta=0:13:05, total=0:00:33, wall=15:31 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.357 DataTime=0.259 Loss=1.995 Prec@1=53.454 Prec@5=77.896 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=15:31 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.357 DataTime=0.259 Loss=1.995 Prec@1=53.454 Prec@5=77.896 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=15:32 IST=> training   8.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=2.001 Prec@1=53.446 Prec@5=77.736 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=15:32 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=2.001 Prec@1=53.446 Prec@5=77.736 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=15:32 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=2.001 Prec@1=53.446 Prec@5=77.736 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=15:32 IST=> training   12.03% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=1.999 Prec@1=53.490 Prec@5=77.800 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=15:32 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=1.999 Prec@1=53.490 Prec@5=77.800 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=15:32 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.350 DataTime=0.250 Loss=1.999 Prec@1=53.490 Prec@5=77.800 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=15:33 IST=> training   16.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.346 DataTime=0.245 Loss=2.006 Prec@1=53.350 Prec@5=77.735 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=15:33 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.346 DataTime=0.245 Loss=2.006 Prec@1=53.350 Prec@5=77.735 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=15:33 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.346 DataTime=0.245 Loss=2.006 Prec@1=53.350 Prec@5=77.735 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=15:34 IST=> training   20.02% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.241 Loss=2.009 Prec@1=53.277 Prec@5=77.699 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=15:34 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.241 Loss=2.009 Prec@1=53.277 Prec@5=77.699 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=15:34 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.241 Loss=2.009 Prec@1=53.277 Prec@5=77.699 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=15:34 IST=> training   24.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.241 Loss=2.010 Prec@1=53.274 Prec@5=77.699 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=15:34 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.241 Loss=2.010 Prec@1=53.274 Prec@5=77.699 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=15:34 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.241 Loss=2.010 Prec@1=53.274 Prec@5=77.699 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=15:35 IST=> training   28.01% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.240 Loss=2.012 Prec@1=53.231 Prec@5=77.659 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=15:35 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.240 Loss=2.012 Prec@1=53.231 Prec@5=77.659 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=15:35 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.344 DataTime=0.240 Loss=2.012 Prec@1=53.231 Prec@5=77.659 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=15:35 IST=> training   32.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.014 Prec@1=53.195 Prec@5=77.610 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=15:35 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.014 Prec@1=53.195 Prec@5=77.610 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=15:35 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.014 Prec@1=53.195 Prec@5=77.610 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=15:36 IST=> training   36.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.240 Loss=2.015 Prec@1=53.183 Prec@5=77.594 rate=2.97 Hz, eta=0:08:58, total=0:05:02, wall=15:36 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.240 Loss=2.015 Prec@1=53.183 Prec@5=77.594 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=15:36 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.240 Loss=2.015 Prec@1=53.183 Prec@5=77.594 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=15:36 IST=> training   39.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.014 Prec@1=53.184 Prec@5=77.617 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=15:36 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.014 Prec@1=53.184 Prec@5=77.617 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=15:36 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.014 Prec@1=53.184 Prec@5=77.617 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=15:37 IST=> training   43.99% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.015 Prec@1=53.154 Prec@5=77.597 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=15:37 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.015 Prec@1=53.154 Prec@5=77.597 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:37 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.239 Loss=2.015 Prec@1=53.154 Prec@5=77.597 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:37 IST=> training   47.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.015 Prec@1=53.172 Prec@5=77.589 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=15:37 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.015 Prec@1=53.172 Prec@5=77.589 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=15:37 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.343 DataTime=0.239 Loss=2.015 Prec@1=53.172 Prec@5=77.589 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=15:38 IST=> training   51.98% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.017 Prec@1=53.130 Prec@5=77.559 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=15:38 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.017 Prec@1=53.130 Prec@5=77.559 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=15:38 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.017 Prec@1=53.130 Prec@5=77.559 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=15:39 IST=> training   55.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.090 Prec@5=77.517 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=15:39 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.090 Prec@5=77.517 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=15:39 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.090 Prec@5=77.517 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=15:39 IST=> training   59.97% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.019 Prec@1=53.112 Prec@5=77.521 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=15:39 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.019 Prec@1=53.112 Prec@5=77.521 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:39 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.238 Loss=2.019 Prec@1=53.112 Prec@5=77.521 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:40 IST=> training   63.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.119 Prec@5=77.528 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:40 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.119 Prec@5=77.528 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=15:40 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.019 Prec@1=53.119 Prec@5=77.528 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=15:40 IST=> training   67.96% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.020 Prec@1=53.096 Prec@5=77.500 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=15:40 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.020 Prec@1=53.096 Prec@5=77.500 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=15:40 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.020 Prec@1=53.096 Prec@5=77.500 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=15:41 IST=> training   71.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.237 Loss=2.021 Prec@1=53.093 Prec@5=77.478 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=15:41 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.237 Loss=2.021 Prec@1=53.093 Prec@5=77.478 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=15:41 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.342 DataTime=0.237 Loss=2.021 Prec@1=53.093 Prec@5=77.478 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=15:41 IST=> training   75.95% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.022 Prec@1=53.085 Prec@5=77.460 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=15:41 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.022 Prec@1=53.085 Prec@5=77.460 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=15:41 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.022 Prec@1=53.085 Prec@5=77.460 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=15:42 IST=> training   79.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.023 Prec@1=53.074 Prec@5=77.439 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=15:42 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.023 Prec@1=53.074 Prec@5=77.439 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=15:42 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.023 Prec@1=53.074 Prec@5=77.439 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=15:43 IST=> training   83.94% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.024 Prec@1=53.065 Prec@5=77.430 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=15:43 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.024 Prec@1=53.065 Prec@5=77.430 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=15:43 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.237 Loss=2.024 Prec@1=53.065 Prec@5=77.430 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=15:43 IST=> training   87.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.024 Prec@1=53.065 Prec@5=77.424 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=15:43 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.024 Prec@1=53.065 Prec@5=77.424 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:43 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.024 Prec@1=53.065 Prec@5=77.424 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:44 IST=> training   91.93% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.025 Prec@1=53.052 Prec@5=77.410 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.025 Prec@1=53.052 Prec@5=77.410 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.341 DataTime=0.236 Loss=2.025 Prec@1=53.052 Prec@5=77.410 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=15:44 IST=> training   95.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.340 DataTime=0.235 Loss=2.026 Prec@1=53.050 Prec@5=77.409 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.340 DataTime=0.235 Loss=2.026 Prec@1=53.050 Prec@5=77.409 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.340 DataTime=0.235 Loss=2.026 Prec@1=53.050 Prec@5=77.409 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=15:44 IST=> training   99.92% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.340 DataTime=0.235 Loss=2.026 Prec@1=53.049 Prec@5=77.409 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=15:44 IST=> training   100.00% of 1x2503...Epoch=67/150 LR=0.05937 Time=0.340 DataTime=0.235 Loss=2.026 Prec@1=53.049 Prec@5=77.409 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=15:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 0.00% of 1x98...Epoch=67/150 LR=0.05937 Time=6.845 Loss=2.328 Prec@1=46.289 Prec@5=71.094 rate=0 Hz, eta=?, total=0:00:00, wall=15:44 IST=> validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=6.845 Loss=2.328 Prec@1=46.289 Prec@5=71.094 rate=6464.96 Hz, eta=0:00:00, total=0:00:00, wall=15:44 IST** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=6.845 Loss=2.328 Prec@1=46.289 Prec@5=71.094 rate=6464.96 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST** validation 1.02% of 1x98...Epoch=67/150 LR=0.05937 Time=0.393 Loss=2.202 Prec@1=49.218 Prec@5=74.550 rate=6464.96 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST** validation 100.00% of 1x98...Epoch=67/150 LR=0.05937 Time=0.393 Loss=2.202 Prec@1=49.218 Prec@5=74.550 rate=3.09 Hz, eta=0:00:00, total=0:00:31, wall=15:45 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.329 DataTime=5.085 Loss=2.111 Prec@1=51.758 Prec@5=76.758 rate=0 Hz, eta=?, total=0:00:00, wall=15:45 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.329 DataTime=5.085 Loss=2.111 Prec@1=51.758 Prec@5=76.758 rate=5156.23 Hz, eta=0:00:00, total=0:00:00, wall=15:45 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=5.329 DataTime=5.085 Loss=2.111 Prec@1=51.758 Prec@5=76.758 rate=5156.23 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST=> training   0.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.378 DataTime=0.279 Loss=2.000 Prec@1=53.543 Prec@5=77.934 rate=5156.23 Hz, eta=0:00:00, total=0:00:00, wall=15:46 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.378 DataTime=0.279 Loss=2.000 Prec@1=53.543 Prec@5=77.934 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=15:46 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.378 DataTime=0.279 Loss=2.000 Prec@1=53.543 Prec@5=77.934 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=15:46 IST=> training   4.04% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.256 Loss=1.996 Prec@1=53.529 Prec@5=77.800 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=15:46 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.256 Loss=1.996 Prec@1=53.529 Prec@5=77.800 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=15:46 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.256 Loss=1.996 Prec@1=53.529 Prec@5=77.800 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=15:47 IST=> training   8.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.254 Loss=1.998 Prec@1=53.529 Prec@5=77.761 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=15:47 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.254 Loss=1.998 Prec@1=53.529 Prec@5=77.761 rate=2.94 Hz, eta=0:12:28, total=0:01:42, wall=15:47 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.358 DataTime=0.254 Loss=1.998 Prec@1=53.529 Prec@5=77.761 rate=2.94 Hz, eta=0:12:28, total=0:01:42, wall=15:47 IST=> training   12.03% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.352 DataTime=0.247 Loss=2.001 Prec@1=53.417 Prec@5=77.786 rate=2.94 Hz, eta=0:12:28, total=0:01:42, wall=15:47 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.352 DataTime=0.247 Loss=2.001 Prec@1=53.417 Prec@5=77.786 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=15:47 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.352 DataTime=0.247 Loss=2.001 Prec@1=53.417 Prec@5=77.786 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=15:48 IST=> training   16.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.349 DataTime=0.244 Loss=2.001 Prec@1=53.439 Prec@5=77.806 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=15:48 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.349 DataTime=0.244 Loss=2.001 Prec@1=53.439 Prec@5=77.806 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:48 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.349 DataTime=0.244 Loss=2.001 Prec@1=53.439 Prec@5=77.806 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:48 IST=> training   20.02% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.350 DataTime=0.244 Loss=2.004 Prec@1=53.408 Prec@5=77.730 rate=2.95 Hz, eta=0:11:18, total=0:02:49, wall=15:48 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.350 DataTime=0.244 Loss=2.004 Prec@1=53.408 Prec@5=77.730 rate=2.93 Hz, eta=0:10:48, total=0:03:25, wall=15:48 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.350 DataTime=0.244 Loss=2.004 Prec@1=53.408 Prec@5=77.730 rate=2.93 Hz, eta=0:10:48, total=0:03:25, wall=15:49 IST=> training   24.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.347 DataTime=0.240 Loss=2.006 Prec@1=53.375 Prec@5=77.714 rate=2.93 Hz, eta=0:10:48, total=0:03:25, wall=15:49 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.347 DataTime=0.240 Loss=2.006 Prec@1=53.375 Prec@5=77.714 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=15:49 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.347 DataTime=0.240 Loss=2.006 Prec@1=53.375 Prec@5=77.714 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=15:50 IST=> training   28.01% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.339 Prec@5=77.684 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=15:50 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.339 Prec@5=77.684 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:50 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.339 Prec@5=77.684 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:50 IST=> training   32.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.330 Prec@5=77.665 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=15:50 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.330 Prec@5=77.665 rate=2.94 Hz, eta=0:09:04, total=0:05:06, wall=15:50 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.346 DataTime=0.238 Loss=2.007 Prec@1=53.330 Prec@5=77.665 rate=2.94 Hz, eta=0:09:04, total=0:05:06, wall=15:51 IST=> training   36.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.237 Loss=2.009 Prec@1=53.314 Prec@5=77.636 rate=2.94 Hz, eta=0:09:04, total=0:05:06, wall=15:51 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.237 Loss=2.009 Prec@1=53.314 Prec@5=77.636 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=15:51 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.237 Loss=2.009 Prec@1=53.314 Prec@5=77.636 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=15:51 IST=> training   39.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.236 Loss=2.009 Prec@1=53.307 Prec@5=77.629 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=15:51 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.236 Loss=2.009 Prec@1=53.307 Prec@5=77.629 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=15:51 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.344 DataTime=0.236 Loss=2.009 Prec@1=53.307 Prec@5=77.629 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=15:52 IST=> training   43.99% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.343 DataTime=0.235 Loss=2.009 Prec@1=53.307 Prec@5=77.616 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=15:52 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.343 DataTime=0.235 Loss=2.009 Prec@1=53.307 Prec@5=77.616 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=15:52 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.343 DataTime=0.235 Loss=2.009 Prec@1=53.307 Prec@5=77.616 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=15:52 IST=> training   47.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.234 Loss=2.009 Prec@1=53.300 Prec@5=77.634 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=15:52 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.234 Loss=2.009 Prec@1=53.300 Prec@5=77.634 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=15:52 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.234 Loss=2.009 Prec@1=53.300 Prec@5=77.634 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=15:53 IST=> training   51.98% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.009 Prec@1=53.294 Prec@5=77.639 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=15:53 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.009 Prec@1=53.294 Prec@5=77.639 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=15:53 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.009 Prec@1=53.294 Prec@5=77.639 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=15:53 IST=> training   55.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.235 Loss=2.010 Prec@1=53.273 Prec@5=77.624 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=15:53 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.235 Loss=2.010 Prec@1=53.273 Prec@5=77.624 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=15:53 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.235 Loss=2.010 Prec@1=53.273 Prec@5=77.624 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=15:54 IST=> training   59.97% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.012 Prec@1=53.254 Prec@5=77.610 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=15:54 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.012 Prec@1=53.254 Prec@5=77.610 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:54 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.234 Loss=2.012 Prec@1=53.254 Prec@5=77.610 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:55 IST=> training   63.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.012 Prec@1=53.222 Prec@5=77.607 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=15:55 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.012 Prec@1=53.222 Prec@5=77.607 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:55 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.012 Prec@1=53.222 Prec@5=77.607 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:55 IST=> training   67.96% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.233 Loss=2.013 Prec@1=53.227 Prec@5=77.600 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=15:55 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.233 Loss=2.013 Prec@1=53.227 Prec@5=77.600 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=15:55 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.342 DataTime=0.233 Loss=2.013 Prec@1=53.227 Prec@5=77.600 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=15:56 IST=> training   71.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.014 Prec@1=53.222 Prec@5=77.597 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=15:56 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.014 Prec@1=53.222 Prec@5=77.597 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=15:56 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.014 Prec@1=53.222 Prec@5=77.597 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=15:56 IST=> training   75.95% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.016 Prec@1=53.192 Prec@5=77.564 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=15:56 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.016 Prec@1=53.192 Prec@5=77.564 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=15:56 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.016 Prec@1=53.192 Prec@5=77.564 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=15:57 IST=> training   79.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.017 Prec@1=53.179 Prec@5=77.547 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=15:57 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.017 Prec@1=53.179 Prec@5=77.547 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=15:57 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.017 Prec@1=53.179 Prec@5=77.547 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=15:57 IST=> training   83.94% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.018 Prec@1=53.170 Prec@5=77.533 rate=2.96 Hz, eta=0:02:16, total=0:11:50, wall=15:57 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.018 Prec@1=53.170 Prec@5=77.533 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=15:57 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.341 DataTime=0.232 Loss=2.018 Prec@1=53.170 Prec@5=77.533 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=15:58 IST=> training   87.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.018 Prec@1=53.159 Prec@5=77.523 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=15:58 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.018 Prec@1=53.159 Prec@5=77.523 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=15:58 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.018 Prec@1=53.159 Prec@5=77.523 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=15:59 IST=> training   91.93% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.019 Prec@1=53.141 Prec@5=77.504 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=15:59 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.019 Prec@1=53.141 Prec@5=77.504 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:59 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.340 DataTime=0.231 Loss=2.019 Prec@1=53.141 Prec@5=77.504 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:59 IST=> training   95.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.339 DataTime=0.230 Loss=2.020 Prec@1=53.129 Prec@5=77.498 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=15:59 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.339 DataTime=0.230 Loss=2.020 Prec@1=53.129 Prec@5=77.498 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:59 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.339 DataTime=0.230 Loss=2.020 Prec@1=53.129 Prec@5=77.498 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:59 IST=> training   99.92% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.339 DataTime=0.230 Loss=2.020 Prec@1=53.128 Prec@5=77.496 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=15:59 IST=> training   100.00% of 1x2503...Epoch=68/150 LR=0.05834 Time=0.339 DataTime=0.230 Loss=2.020 Prec@1=53.128 Prec@5=77.496 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=15:59 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> validation 0.00% of 1x98...Epoch=68/150 LR=0.05834 Time=6.662 Loss=2.168 Prec@1=50.195 Prec@5=75.586 rate=0 Hz, eta=?, total=0:00:00, wall=15:59 IST=> validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.662 Loss=2.168 Prec@1=50.195 Prec@5=75.586 rate=6389.04 Hz, eta=0:00:00, total=0:00:00, wall=15:59 IST** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=6.662 Loss=2.168 Prec@1=50.195 Prec@5=75.586 rate=6389.04 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST** validation 1.02% of 1x98...Epoch=68/150 LR=0.05834 Time=0.403 Loss=2.115 Prec@1=51.130 Prec@5=76.300 rate=6389.04 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST** validation 100.00% of 1x98...Epoch=68/150 LR=0.05834 Time=0.403 Loss=2.115 Prec@1=51.130 Prec@5=76.300 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=16:00 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:00 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:00 IST=> training   0.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.396 DataTime=5.249 Loss=1.986 Prec@1=56.641 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=16:00 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.396 DataTime=5.249 Loss=1.986 Prec@1=56.641 Prec@5=77.148 rate=3357.76 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=5.396 DataTime=5.249 Loss=1.986 Prec@1=56.641 Prec@5=77.148 rate=3357.76 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST=> training   0.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.382 DataTime=0.284 Loss=2.007 Prec@1=53.287 Prec@5=77.640 rate=3357.76 Hz, eta=0:00:00, total=0:00:00, wall=16:00 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.382 DataTime=0.284 Loss=2.007 Prec@1=53.287 Prec@5=77.640 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=16:00 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.382 DataTime=0.284 Loss=2.007 Prec@1=53.287 Prec@5=77.640 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=16:01 IST=> training   4.04% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.360 DataTime=0.261 Loss=1.996 Prec@1=53.589 Prec@5=77.730 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=16:01 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.360 DataTime=0.261 Loss=1.996 Prec@1=53.589 Prec@5=77.730 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=16:01 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.360 DataTime=0.261 Loss=1.996 Prec@1=53.589 Prec@5=77.730 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=16:02 IST=> training   8.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.350 DataTime=0.248 Loss=1.998 Prec@1=53.501 Prec@5=77.799 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=16:02 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.350 DataTime=0.248 Loss=1.998 Prec@1=53.501 Prec@5=77.799 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=16:02 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.350 DataTime=0.248 Loss=1.998 Prec@1=53.501 Prec@5=77.799 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=16:02 IST=> training   12.03% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.348 DataTime=0.244 Loss=2.000 Prec@1=53.500 Prec@5=77.776 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=16:02 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.348 DataTime=0.244 Loss=2.000 Prec@1=53.500 Prec@5=77.776 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=16:02 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.348 DataTime=0.244 Loss=2.000 Prec@1=53.500 Prec@5=77.776 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=16:03 IST=> training   16.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.345 DataTime=0.240 Loss=2.000 Prec@1=53.493 Prec@5=77.779 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=16:03 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.345 DataTime=0.240 Loss=2.000 Prec@1=53.493 Prec@5=77.779 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=16:03 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.345 DataTime=0.240 Loss=2.000 Prec@1=53.493 Prec@5=77.779 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=16:03 IST=> training   20.02% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.237 Loss=2.001 Prec@1=53.504 Prec@5=77.765 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=16:03 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.237 Loss=2.001 Prec@1=53.504 Prec@5=77.765 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=16:03 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.237 Loss=2.001 Prec@1=53.504 Prec@5=77.765 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=16:04 IST=> training   24.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.236 Loss=2.001 Prec@1=53.490 Prec@5=77.777 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=16:04 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.236 Loss=2.001 Prec@1=53.490 Prec@5=77.777 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=16:04 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.342 DataTime=0.236 Loss=2.001 Prec@1=53.490 Prec@5=77.777 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=16:04 IST=> training   28.01% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.340 DataTime=0.234 Loss=2.003 Prec@1=53.484 Prec@5=77.754 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=16:04 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.340 DataTime=0.234 Loss=2.003 Prec@1=53.484 Prec@5=77.754 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=16:04 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.340 DataTime=0.234 Loss=2.003 Prec@1=53.484 Prec@5=77.754 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=16:05 IST=> training   32.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.003 Prec@1=53.451 Prec@5=77.752 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=16:05 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.003 Prec@1=53.451 Prec@5=77.752 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:05 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.003 Prec@1=53.451 Prec@5=77.752 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:05 IST=> training   36.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.339 DataTime=0.234 Loss=2.003 Prec@1=53.437 Prec@5=77.767 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:05 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.339 DataTime=0.234 Loss=2.003 Prec@1=53.437 Prec@5=77.767 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=16:05 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.339 DataTime=0.234 Loss=2.003 Prec@1=53.437 Prec@5=77.767 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=16:06 IST=> training   39.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.234 Loss=2.004 Prec@1=53.399 Prec@5=77.763 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=16:06 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.234 Loss=2.004 Prec@1=53.399 Prec@5=77.763 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=16:06 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.234 Loss=2.004 Prec@1=53.399 Prec@5=77.763 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=16:07 IST=> training   43.99% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.006 Prec@1=53.350 Prec@5=77.725 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=16:07 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.006 Prec@1=53.350 Prec@5=77.725 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=16:07 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.006 Prec@1=53.350 Prec@5=77.725 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=16:07 IST=> training   47.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.007 Prec@1=53.344 Prec@5=77.719 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=16:07 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.007 Prec@1=53.344 Prec@5=77.719 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:07 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.007 Prec@1=53.344 Prec@5=77.719 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:08 IST=> training   51.98% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.007 Prec@1=53.327 Prec@5=77.718 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:08 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.007 Prec@1=53.327 Prec@5=77.718 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:08 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.233 Loss=2.007 Prec@1=53.327 Prec@5=77.718 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:08 IST=> training   55.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.008 Prec@1=53.314 Prec@5=77.711 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:08 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.008 Prec@1=53.314 Prec@5=77.711 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=16:08 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.008 Prec@1=53.314 Prec@5=77.711 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=16:09 IST=> training   59.97% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.008 Prec@1=53.308 Prec@5=77.708 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=16:09 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.008 Prec@1=53.308 Prec@5=77.708 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:09 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.338 DataTime=0.233 Loss=2.008 Prec@1=53.308 Prec@5=77.708 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:09 IST=> training   63.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.009 Prec@1=53.304 Prec@5=77.688 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:09 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.009 Prec@1=53.304 Prec@5=77.688 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:09 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.009 Prec@1=53.304 Prec@5=77.688 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:10 IST=> training   67.96% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.010 Prec@1=53.272 Prec@5=77.663 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:10 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.010 Prec@1=53.272 Prec@5=77.663 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=16:10 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.010 Prec@1=53.272 Prec@5=77.663 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=16:10 IST=> training   71.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.011 Prec@1=53.273 Prec@5=77.646 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=16:10 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.011 Prec@1=53.273 Prec@5=77.646 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:10 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.011 Prec@1=53.273 Prec@5=77.646 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:11 IST=> training   75.95% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.012 Prec@1=53.256 Prec@5=77.633 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:11 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.012 Prec@1=53.256 Prec@5=77.633 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:11 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.012 Prec@1=53.256 Prec@5=77.633 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:12 IST=> training   79.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.013 Prec@1=53.231 Prec@5=77.610 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:12 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.013 Prec@1=53.231 Prec@5=77.610 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=16:12 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.013 Prec@1=53.231 Prec@5=77.610 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=16:12 IST=> training   83.94% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.014 Prec@1=53.204 Prec@5=77.585 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=16:12 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.014 Prec@1=53.204 Prec@5=77.585 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=16:12 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.232 Loss=2.014 Prec@1=53.204 Prec@5=77.585 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=16:13 IST=> training   87.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.176 Prec@5=77.568 rate=2.99 Hz, eta=0:01:41, total=0:12:17, wall=16:13 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.176 Prec@5=77.568 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:13 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.176 Prec@5=77.568 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:13 IST=> training   91.93% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.185 Prec@5=77.577 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:13 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.185 Prec@5=77.577 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=16:13 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.015 Prec@1=53.185 Prec@5=77.577 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=16:14 IST=> training   95.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.016 Prec@1=53.180 Prec@5=77.571 rate=2.99 Hz, eta=0:00:34, total=0:13:22, wall=16:14 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.016 Prec@1=53.180 Prec@5=77.571 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=16:14 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.016 Prec@1=53.180 Prec@5=77.571 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=16:14 IST=> training   99.92% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.016 Prec@1=53.178 Prec@5=77.569 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=16:14 IST=> training   100.00% of 1x2503...Epoch=69/150 LR=0.05730 Time=0.337 DataTime=0.231 Loss=2.016 Prec@1=53.178 Prec@5=77.569 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=16:14 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST=> validation 0.00% of 1x98...Epoch=69/150 LR=0.05730 Time=6.915 Loss=1.980 Prec@1=52.930 Prec@5=78.906 rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST=> validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=6.915 Loss=1.980 Prec@1=52.930 Prec@5=78.906 rate=5539.09 Hz, eta=0:00:00, total=0:00:00, wall=16:14 IST** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=6.915 Loss=1.980 Prec@1=52.930 Prec@5=78.906 rate=5539.09 Hz, eta=0:00:00, total=0:00:00, wall=16:14 IST** validation 1.02% of 1x98...Epoch=69/150 LR=0.05730 Time=0.399 Loss=2.081 Prec@1=51.758 Prec@5=76.932 rate=5539.09 Hz, eta=0:00:00, total=0:00:00, wall=16:14 IST** validation 100.00% of 1x98...Epoch=69/150 LR=0.05730 Time=0.399 Loss=2.081 Prec@1=51.758 Prec@5=76.932 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=16:14 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST=> training   0.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.478 DataTime=5.336 Loss=2.138 Prec@1=49.219 Prec@5=75.586 rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.478 DataTime=5.336 Loss=2.138 Prec@1=49.219 Prec@5=75.586 rate=1683.65 Hz, eta=0:00:01, total=0:00:00, wall=16:15 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=5.478 DataTime=5.336 Loss=2.138 Prec@1=49.219 Prec@5=75.586 rate=1683.65 Hz, eta=0:00:01, total=0:00:00, wall=16:15 IST=> training   0.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.380 DataTime=0.288 Loss=1.991 Prec@1=53.504 Prec@5=78.023 rate=1683.65 Hz, eta=0:00:01, total=0:00:00, wall=16:15 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.380 DataTime=0.288 Loss=1.991 Prec@1=53.504 Prec@5=78.023 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=16:15 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.380 DataTime=0.288 Loss=1.991 Prec@1=53.504 Prec@5=78.023 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=16:16 IST=> training   4.04% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.355 DataTime=0.260 Loss=1.986 Prec@1=53.556 Prec@5=78.080 rate=3.07 Hz, eta=0:13:02, total=0:00:32, wall=16:16 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.355 DataTime=0.260 Loss=1.986 Prec@1=53.556 Prec@5=78.080 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=16:16 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.355 DataTime=0.260 Loss=1.986 Prec@1=53.556 Prec@5=78.080 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=16:16 IST=> training   8.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.354 DataTime=0.257 Loss=1.983 Prec@1=53.688 Prec@5=78.129 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=16:16 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.354 DataTime=0.257 Loss=1.983 Prec@1=53.688 Prec@5=78.129 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=16:16 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.354 DataTime=0.257 Loss=1.983 Prec@1=53.688 Prec@5=78.129 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=16:17 IST=> training   12.03% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.345 DataTime=0.248 Loss=1.985 Prec@1=53.733 Prec@5=78.070 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=16:17 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.345 DataTime=0.248 Loss=1.985 Prec@1=53.733 Prec@5=78.070 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=16:17 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.345 DataTime=0.248 Loss=1.985 Prec@1=53.733 Prec@5=78.070 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=16:17 IST=> training   16.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.342 DataTime=0.244 Loss=1.989 Prec@1=53.698 Prec@5=78.024 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=16:17 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.342 DataTime=0.244 Loss=1.989 Prec@1=53.698 Prec@5=78.024 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=16:17 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.342 DataTime=0.244 Loss=1.989 Prec@1=53.698 Prec@5=78.024 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=16:18 IST=> training   20.02% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.343 DataTime=0.245 Loss=1.991 Prec@1=53.666 Prec@5=77.966 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=16:18 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.343 DataTime=0.245 Loss=1.991 Prec@1=53.666 Prec@5=77.966 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=16:18 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.343 DataTime=0.245 Loss=1.991 Prec@1=53.666 Prec@5=77.966 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=16:19 IST=> training   24.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.242 Loss=1.990 Prec@1=53.693 Prec@5=78.000 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=16:19 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.242 Loss=1.990 Prec@1=53.693 Prec@5=78.000 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=16:19 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.242 Loss=1.990 Prec@1=53.693 Prec@5=78.000 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=16:19 IST=> training   28.01% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.240 Loss=1.992 Prec@1=53.621 Prec@5=77.945 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=16:19 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.240 Loss=1.992 Prec@1=53.621 Prec@5=77.945 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=16:19 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.340 DataTime=0.240 Loss=1.992 Prec@1=53.621 Prec@5=77.945 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=16:20 IST=> training   32.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.239 Loss=1.994 Prec@1=53.567 Prec@5=77.911 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=16:20 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.239 Loss=1.994 Prec@1=53.567 Prec@5=77.911 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:20 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.239 Loss=1.994 Prec@1=53.567 Prec@5=77.911 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:20 IST=> training   36.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.238 Loss=1.997 Prec@1=53.553 Prec@5=77.870 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=16:20 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.238 Loss=1.997 Prec@1=53.553 Prec@5=77.870 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=16:20 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.238 Loss=1.997 Prec@1=53.553 Prec@5=77.870 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=16:21 IST=> training   39.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.239 Loss=1.998 Prec@1=53.549 Prec@5=77.851 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=16:21 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.239 Loss=1.998 Prec@1=53.549 Prec@5=77.851 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=16:21 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.239 Loss=1.998 Prec@1=53.549 Prec@5=77.851 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=16:21 IST=> training   43.99% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.238 Loss=1.999 Prec@1=53.506 Prec@5=77.835 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=16:21 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.238 Loss=1.999 Prec@1=53.506 Prec@5=77.835 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:21 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.238 Loss=1.999 Prec@1=53.506 Prec@5=77.835 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:22 IST=> training   47.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.465 Prec@5=77.807 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:22 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.465 Prec@5=77.807 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:22 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.465 Prec@5=77.807 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:22 IST=> training   51.98% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.237 Loss=2.001 Prec@1=53.467 Prec@5=77.808 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=16:22 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.237 Loss=2.001 Prec@1=53.467 Prec@5=77.808 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=16:22 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.237 Loss=2.001 Prec@1=53.467 Prec@5=77.808 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=16:23 IST=> training   55.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.463 Prec@5=77.801 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=16:23 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.463 Prec@5=77.801 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:23 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.237 Loss=2.001 Prec@1=53.463 Prec@5=77.801 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:24 IST=> training   59.97% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.236 Loss=2.002 Prec@1=53.445 Prec@5=77.788 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:24 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.236 Loss=2.002 Prec@1=53.445 Prec@5=77.788 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:24 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.236 Loss=2.002 Prec@1=53.445 Prec@5=77.788 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:24 IST=> training   63.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.236 Loss=2.004 Prec@1=53.420 Prec@5=77.770 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:24 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.236 Loss=2.004 Prec@1=53.420 Prec@5=77.770 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=16:24 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.236 Loss=2.004 Prec@1=53.420 Prec@5=77.770 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=16:25 IST=> training   67.96% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.004 Prec@1=53.405 Prec@5=77.763 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=16:25 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.004 Prec@1=53.405 Prec@5=77.763 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=16:25 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.004 Prec@1=53.405 Prec@5=77.763 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=16:25 IST=> training   71.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.005 Prec@1=53.410 Prec@5=77.748 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=16:25 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.005 Prec@1=53.410 Prec@5=77.748 rate=2.98 Hz, eta=0:03:21, total=0:10:36, wall=16:25 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.005 Prec@1=53.410 Prec@5=77.748 rate=2.98 Hz, eta=0:03:21, total=0:10:36, wall=16:26 IST=> training   75.95% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.006 Prec@1=53.386 Prec@5=77.730 rate=2.98 Hz, eta=0:03:21, total=0:10:36, wall=16:26 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.006 Prec@1=53.386 Prec@5=77.730 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=16:26 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.235 Loss=2.006 Prec@1=53.386 Prec@5=77.730 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=16:26 IST=> training   79.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.388 Prec@5=77.719 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=16:26 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.388 Prec@5=77.719 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=16:26 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.388 Prec@5=77.719 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=16:27 IST=> training   83.94% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.382 Prec@5=77.723 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=16:27 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.382 Prec@5=77.723 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=16:27 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.007 Prec@1=53.382 Prec@5=77.723 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=16:28 IST=> training   87.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.234 Loss=2.007 Prec@1=53.363 Prec@5=77.715 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=16:28 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.234 Loss=2.007 Prec@1=53.363 Prec@5=77.715 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=16:28 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.339 DataTime=0.234 Loss=2.007 Prec@1=53.363 Prec@5=77.715 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=16:28 IST=> training   91.93% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.008 Prec@1=53.361 Prec@5=77.714 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=16:28 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.008 Prec@1=53.361 Prec@5=77.714 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=16:28 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.338 DataTime=0.234 Loss=2.008 Prec@1=53.361 Prec@5=77.714 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=16:29 IST=> training   95.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.337 DataTime=0.233 Loss=2.009 Prec@1=53.326 Prec@5=77.696 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=16:29 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.337 DataTime=0.233 Loss=2.009 Prec@1=53.326 Prec@5=77.696 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=16:29 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.337 DataTime=0.233 Loss=2.009 Prec@1=53.326 Prec@5=77.696 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=16:29 IST=> training   99.92% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.337 DataTime=0.233 Loss=2.009 Prec@1=53.326 Prec@5=77.696 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=16:29 IST=> training   100.00% of 1x2503...Epoch=70/150 LR=0.05627 Time=0.337 DataTime=0.233 Loss=2.009 Prec@1=53.326 Prec@5=77.696 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=16:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> validation 0.00% of 1x98...Epoch=70/150 LR=0.05627 Time=6.254 Loss=2.152 Prec@1=47.266 Prec@5=75.977 rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=6.254 Loss=2.152 Prec@1=47.266 Prec@5=75.977 rate=5923.22 Hz, eta=0:00:00, total=0:00:00, wall=16:29 IST** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=6.254 Loss=2.152 Prec@1=47.266 Prec@5=75.977 rate=5923.22 Hz, eta=0:00:00, total=0:00:00, wall=16:29 IST** validation 1.02% of 1x98...Epoch=70/150 LR=0.05627 Time=0.388 Loss=2.149 Prec@1=50.574 Prec@5=75.698 rate=5923.22 Hz, eta=0:00:00, total=0:00:00, wall=16:29 IST** validation 100.00% of 1x98...Epoch=70/150 LR=0.05627 Time=0.388 Loss=2.149 Prec@1=50.574 Prec@5=75.698 rate=3.09 Hz, eta=0:00:00, total=0:00:31, wall=16:29 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> training   0.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.989 DataTime=5.824 Loss=2.005 Prec@1=52.734 Prec@5=77.539 rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.989 DataTime=5.824 Loss=2.005 Prec@1=52.734 Prec@5=77.539 rate=7003.24 Hz, eta=0:00:00, total=0:00:00, wall=16:29 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=5.989 DataTime=5.824 Loss=2.005 Prec@1=52.734 Prec@5=77.539 rate=7003.24 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST=> training   0.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.382 DataTime=0.294 Loss=1.958 Prec@1=54.189 Prec@5=78.403 rate=7003.24 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.382 DataTime=0.294 Loss=1.958 Prec@1=54.189 Prec@5=78.403 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=16:30 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.382 DataTime=0.294 Loss=1.958 Prec@1=54.189 Prec@5=78.403 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=16:30 IST=> training   4.04% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.355 DataTime=0.264 Loss=1.967 Prec@1=54.147 Prec@5=78.296 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=16:30 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.355 DataTime=0.264 Loss=1.967 Prec@1=54.147 Prec@5=78.296 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=16:30 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.355 DataTime=0.264 Loss=1.967 Prec@1=54.147 Prec@5=78.296 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=16:31 IST=> training   8.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.351 DataTime=0.256 Loss=1.966 Prec@1=54.059 Prec@5=78.261 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=16:31 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.351 DataTime=0.256 Loss=1.966 Prec@1=54.059 Prec@5=78.261 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=16:31 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.351 DataTime=0.256 Loss=1.966 Prec@1=54.059 Prec@5=78.261 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=16:32 IST=> training   12.03% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.344 DataTime=0.247 Loss=1.970 Prec@1=53.979 Prec@5=78.222 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=16:32 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.344 DataTime=0.247 Loss=1.970 Prec@1=53.979 Prec@5=78.222 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=16:32 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.344 DataTime=0.247 Loss=1.970 Prec@1=53.979 Prec@5=78.222 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=16:32 IST=> training   16.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.341 DataTime=0.244 Loss=1.973 Prec@1=53.944 Prec@5=78.195 rate=3.04 Hz, eta=0:11:32, total=0:02:12, wall=16:32 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.341 DataTime=0.244 Loss=1.973 Prec@1=53.944 Prec@5=78.195 rate=3.04 Hz, eta=0:10:59, total=0:02:44, wall=16:32 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.341 DataTime=0.244 Loss=1.973 Prec@1=53.944 Prec@5=78.195 rate=3.04 Hz, eta=0:10:59, total=0:02:44, wall=16:33 IST=> training   20.02% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.343 DataTime=0.245 Loss=1.977 Prec@1=53.869 Prec@5=78.123 rate=3.04 Hz, eta=0:10:59, total=0:02:44, wall=16:33 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.343 DataTime=0.245 Loss=1.977 Prec@1=53.869 Prec@5=78.123 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=16:33 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.343 DataTime=0.245 Loss=1.977 Prec@1=53.869 Prec@5=78.123 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=16:33 IST=> training   24.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.242 Loss=1.981 Prec@1=53.753 Prec@5=78.092 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=16:33 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.242 Loss=1.981 Prec@1=53.753 Prec@5=78.092 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=16:33 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.242 Loss=1.981 Prec@1=53.753 Prec@5=78.092 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=16:34 IST=> training   28.01% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.241 Loss=1.984 Prec@1=53.728 Prec@5=78.031 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=16:34 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.241 Loss=1.984 Prec@1=53.728 Prec@5=78.031 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=16:34 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.241 Loss=1.984 Prec@1=53.728 Prec@5=78.031 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=16:34 IST=> training   32.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.241 Loss=1.986 Prec@1=53.716 Prec@5=78.010 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=16:34 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.241 Loss=1.986 Prec@1=53.716 Prec@5=78.010 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=16:34 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.241 Loss=1.986 Prec@1=53.716 Prec@5=78.010 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=16:35 IST=> training   36.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.240 Loss=1.987 Prec@1=53.725 Prec@5=77.991 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=16:35 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.240 Loss=1.987 Prec@1=53.725 Prec@5=77.991 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=16:35 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.340 DataTime=0.240 Loss=1.987 Prec@1=53.725 Prec@5=77.991 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=16:35 IST=> training   39.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.239 Loss=1.990 Prec@1=53.691 Prec@5=77.949 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=16:35 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.239 Loss=1.990 Prec@1=53.691 Prec@5=77.949 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=16:35 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.239 Loss=1.990 Prec@1=53.691 Prec@5=77.949 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=16:36 IST=> training   43.99% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.239 Loss=1.991 Prec@1=53.696 Prec@5=77.948 rate=3.00 Hz, eta=0:07:46, total=0:06:06, wall=16:36 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.239 Loss=1.991 Prec@1=53.696 Prec@5=77.948 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:36 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.339 DataTime=0.239 Loss=1.991 Prec@1=53.696 Prec@5=77.948 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:37 IST=> training   47.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.238 Loss=1.991 Prec@1=53.678 Prec@5=77.954 rate=2.99 Hz, eta=0:07:14, total=0:06:41, wall=16:37 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.238 Loss=1.991 Prec@1=53.678 Prec@5=77.954 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=16:37 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.238 Loss=1.991 Prec@1=53.678 Prec@5=77.954 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=16:37 IST=> training   51.98% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.992 Prec@1=53.638 Prec@5=77.941 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=16:37 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.992 Prec@1=53.638 Prec@5=77.941 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:37 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.992 Prec@1=53.638 Prec@5=77.941 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:38 IST=> training   55.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.993 Prec@1=53.624 Prec@5=77.926 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=16:38 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.993 Prec@1=53.624 Prec@5=77.926 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:38 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.993 Prec@1=53.624 Prec@5=77.926 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:38 IST=> training   59.97% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.994 Prec@1=53.601 Prec@5=77.916 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=16:38 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.994 Prec@1=53.601 Prec@5=77.916 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:38 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.237 Loss=1.994 Prec@1=53.601 Prec@5=77.916 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:39 IST=> training   63.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.236 Loss=1.995 Prec@1=53.600 Prec@5=77.903 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=16:39 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.236 Loss=1.995 Prec@1=53.600 Prec@5=77.903 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:39 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.236 Loss=1.995 Prec@1=53.600 Prec@5=77.903 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:39 IST=> training   67.96% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.236 Loss=1.996 Prec@1=53.568 Prec@5=77.887 rate=2.99 Hz, eta=0:04:27, total=0:09:28, wall=16:39 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.236 Loss=1.996 Prec@1=53.568 Prec@5=77.887 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=16:39 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.236 Loss=1.996 Prec@1=53.568 Prec@5=77.887 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=16:40 IST=> training   71.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.550 Prec@5=77.857 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=16:40 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.550 Prec@5=77.857 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:40 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.550 Prec@5=77.857 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:41 IST=> training   75.95% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.846 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=16:41 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.846 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:41 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.846 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:41 IST=> training   79.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.849 rate=2.99 Hz, eta=0:02:47, total=0:11:09, wall=16:41 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.849 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=16:41 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.338 DataTime=0.235 Loss=1.998 Prec@1=53.535 Prec@5=77.849 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=16:42 IST=> training   83.94% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=1.999 Prec@1=53.533 Prec@5=77.840 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=16:42 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=1.999 Prec@1=53.533 Prec@5=77.840 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=16:42 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=1.999 Prec@1=53.533 Prec@5=77.840 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=16:42 IST=> training   87.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.000 Prec@1=53.514 Prec@5=77.834 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=16:42 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.000 Prec@1=53.514 Prec@5=77.834 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:42 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.000 Prec@1=53.514 Prec@5=77.834 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:43 IST=> training   91.93% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.001 Prec@1=53.501 Prec@5=77.811 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=16:43 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.001 Prec@1=53.501 Prec@5=77.811 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=16:43 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.337 DataTime=0.234 Loss=2.001 Prec@1=53.501 Prec@5=77.811 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=16:43 IST=> training   95.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.336 DataTime=0.234 Loss=2.001 Prec@1=53.493 Prec@5=77.801 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=16:43 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.336 DataTime=0.234 Loss=2.001 Prec@1=53.493 Prec@5=77.801 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=16:43 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.336 DataTime=0.234 Loss=2.001 Prec@1=53.493 Prec@5=77.801 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=16:43 IST=> training   99.92% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.336 DataTime=0.234 Loss=2.001 Prec@1=53.493 Prec@5=77.800 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=16:43 IST=> training   100.00% of 1x2503...Epoch=71/150 LR=0.05523 Time=0.336 DataTime=0.234 Loss=2.001 Prec@1=53.493 Prec@5=77.800 rate=3.00 Hz, eta=0:00:00, total=0:13:55, wall=16:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:43 IST=> validation 0.00% of 1x98...Epoch=71/150 LR=0.05523 Time=6.588 Loss=2.334 Prec@1=49.805 Prec@5=71.680 rate=0 Hz, eta=?, total=0:00:00, wall=16:43 IST=> validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=6.588 Loss=2.334 Prec@1=49.805 Prec@5=71.680 rate=7259.32 Hz, eta=0:00:00, total=0:00:00, wall=16:43 IST** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=6.588 Loss=2.334 Prec@1=49.805 Prec@5=71.680 rate=7259.32 Hz, eta=0:00:00, total=0:00:00, wall=16:44 IST** validation 1.02% of 1x98...Epoch=71/150 LR=0.05523 Time=0.406 Loss=2.164 Prec@1=50.014 Prec@5=75.400 rate=7259.32 Hz, eta=0:00:00, total=0:00:00, wall=16:44 IST** validation 100.00% of 1x98...Epoch=71/150 LR=0.05523 Time=0.406 Loss=2.164 Prec@1=50.014 Prec@5=75.400 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=16:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:44 IST=> training   0.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.733 DataTime=5.605 Loss=1.924 Prec@1=56.836 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=16:44 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.733 DataTime=5.605 Loss=1.924 Prec@1=56.836 Prec@5=80.469 rate=7328.85 Hz, eta=0:00:00, total=0:00:00, wall=16:44 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=5.733 DataTime=5.605 Loss=1.924 Prec@1=56.836 Prec@5=80.469 rate=7328.85 Hz, eta=0:00:00, total=0:00:00, wall=16:45 IST=> training   0.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.385 DataTime=0.289 Loss=1.978 Prec@1=53.966 Prec@5=78.023 rate=7328.85 Hz, eta=0:00:00, total=0:00:00, wall=16:45 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.385 DataTime=0.289 Loss=1.978 Prec@1=53.966 Prec@5=78.023 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=16:45 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.385 DataTime=0.289 Loss=1.978 Prec@1=53.966 Prec@5=78.023 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=16:45 IST=> training   4.04% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.356 DataTime=0.253 Loss=1.984 Prec@1=53.835 Prec@5=77.980 rate=3.04 Hz, eta=0:13:09, total=0:00:33, wall=16:45 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.356 DataTime=0.253 Loss=1.984 Prec@1=53.835 Prec@5=77.980 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=16:45 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.356 DataTime=0.253 Loss=1.984 Prec@1=53.835 Prec@5=77.980 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=16:46 IST=> training   8.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.353 DataTime=0.248 Loss=1.984 Prec@1=53.806 Prec@5=77.978 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=16:46 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.353 DataTime=0.248 Loss=1.984 Prec@1=53.806 Prec@5=77.978 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=16:46 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.353 DataTime=0.248 Loss=1.984 Prec@1=53.806 Prec@5=77.978 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=16:46 IST=> training   12.03% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.348 DataTime=0.241 Loss=1.987 Prec@1=53.744 Prec@5=77.993 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=16:46 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.348 DataTime=0.241 Loss=1.987 Prec@1=53.744 Prec@5=77.993 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=16:46 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.348 DataTime=0.241 Loss=1.987 Prec@1=53.744 Prec@5=77.993 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=16:47 IST=> training   16.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.346 DataTime=0.239 Loss=1.985 Prec@1=53.755 Prec@5=77.984 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=16:47 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.346 DataTime=0.239 Loss=1.985 Prec@1=53.755 Prec@5=77.984 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=16:47 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.346 DataTime=0.239 Loss=1.985 Prec@1=53.755 Prec@5=77.984 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=16:47 IST=> training   20.02% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.347 DataTime=0.240 Loss=1.985 Prec@1=53.764 Prec@5=78.003 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=16:47 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.347 DataTime=0.240 Loss=1.985 Prec@1=53.764 Prec@5=78.003 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=16:47 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.347 DataTime=0.240 Loss=1.985 Prec@1=53.764 Prec@5=78.003 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=16:48 IST=> training   24.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.345 DataTime=0.238 Loss=1.984 Prec@1=53.786 Prec@5=78.004 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=16:48 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.345 DataTime=0.238 Loss=1.984 Prec@1=53.786 Prec@5=78.004 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=16:48 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.345 DataTime=0.238 Loss=1.984 Prec@1=53.786 Prec@5=78.004 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=16:49 IST=> training   28.01% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.235 Loss=1.983 Prec@1=53.783 Prec@5=78.034 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=16:49 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.235 Loss=1.983 Prec@1=53.783 Prec@5=78.034 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=16:49 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.235 Loss=1.983 Prec@1=53.783 Prec@5=78.034 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=16:49 IST=> training   32.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.697 Prec@5=77.995 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=16:49 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.697 Prec@5=77.995 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=16:49 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.697 Prec@5=77.995 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=16:50 IST=> training   36.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.713 Prec@5=78.013 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=16:50 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.713 Prec@5=78.013 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=16:50 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.987 Prec@1=53.713 Prec@5=78.013 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=16:50 IST=> training   39.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.989 Prec@1=53.691 Prec@5=77.993 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=16:50 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.989 Prec@1=53.691 Prec@5=77.993 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=16:50 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.989 Prec@1=53.691 Prec@5=77.993 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=16:51 IST=> training   43.99% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.989 Prec@1=53.684 Prec@5=77.979 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.989 Prec@1=53.684 Prec@5=77.979 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.343 DataTime=0.236 Loss=1.989 Prec@1=53.684 Prec@5=77.979 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=16:51 IST=> training   47.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.686 Prec@5=77.962 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=16:51 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.686 Prec@5=77.962 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=16:51 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.686 Prec@5=77.962 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=16:52 IST=> training   51.98% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.701 Prec@5=77.966 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=16:52 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.701 Prec@5=77.966 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=16:52 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.701 Prec@5=77.966 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=16:53 IST=> training   55.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.680 Prec@5=77.960 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=16:53 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.680 Prec@5=77.960 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=16:53 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.342 DataTime=0.235 Loss=1.990 Prec@1=53.680 Prec@5=77.960 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=16:53 IST=> training   59.97% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.694 Prec@5=77.951 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=16:53 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.694 Prec@5=77.951 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=16:53 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.989 Prec@1=53.694 Prec@5=77.951 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=16:54 IST=> training   63.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.990 Prec@1=53.702 Prec@5=77.949 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=16:54 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.990 Prec@1=53.702 Prec@5=77.949 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=16:54 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.990 Prec@1=53.702 Prec@5=77.949 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=16:54 IST=> training   67.96% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.689 Prec@5=77.937 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=16:54 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.689 Prec@5=77.937 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=16:54 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.689 Prec@5=77.937 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=16:55 IST=> training   71.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.682 Prec@5=77.930 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=16:55 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.682 Prec@5=77.930 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=16:55 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.234 Loss=1.991 Prec@1=53.682 Prec@5=77.930 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=16:55 IST=> training   75.95% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.993 Prec@1=53.664 Prec@5=77.905 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=16:55 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.993 Prec@1=53.664 Prec@5=77.905 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=16:55 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.993 Prec@1=53.664 Prec@5=77.905 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=16:56 IST=> training   79.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.994 Prec@1=53.650 Prec@5=77.874 rate=2.96 Hz, eta=0:02:49, total=0:11:14, wall=16:56 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.994 Prec@1=53.650 Prec@5=77.874 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=16:56 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.994 Prec@1=53.650 Prec@5=77.874 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=16:56 IST=> training   83.94% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.636 Prec@5=77.870 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=16:56 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.636 Prec@5=77.870 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=16:56 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.636 Prec@5=77.870 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=16:57 IST=> training   87.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.621 Prec@5=77.855 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=16:57 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.621 Prec@5=77.855 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=16:57 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.995 Prec@1=53.621 Prec@5=77.855 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=16:58 IST=> training   91.93% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.997 Prec@1=53.594 Prec@5=77.831 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=16:58 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.997 Prec@1=53.594 Prec@5=77.831 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=16:58 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.341 DataTime=0.235 Loss=1.997 Prec@1=53.594 Prec@5=77.831 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=16:58 IST=> training   95.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.997 Prec@1=53.583 Prec@5=77.817 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=16:58 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.997 Prec@1=53.583 Prec@5=77.817 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=16:58 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.997 Prec@1=53.583 Prec@5=77.817 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=16:58 IST=> training   99.92% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.997 Prec@1=53.582 Prec@5=77.816 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=16:58 IST=> training   100.00% of 1x2503...Epoch=72/150 LR=0.05418 Time=0.340 DataTime=0.234 Loss=1.997 Prec@1=53.582 Prec@5=77.816 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=16:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:58 IST=> validation 0.00% of 1x98...Epoch=72/150 LR=0.05418 Time=6.520 Loss=2.010 Prec@1=53.711 Prec@5=75.781 rate=0 Hz, eta=?, total=0:00:00, wall=16:58 IST=> validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.520 Loss=2.010 Prec@1=53.711 Prec@5=75.781 rate=7116.83 Hz, eta=0:00:00, total=0:00:00, wall=16:58 IST** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=6.520 Loss=2.010 Prec@1=53.711 Prec@5=75.781 rate=7116.83 Hz, eta=0:00:00, total=0:00:00, wall=16:59 IST** validation 1.02% of 1x98...Epoch=72/150 LR=0.05418 Time=0.400 Loss=2.128 Prec@1=50.868 Prec@5=75.978 rate=7116.83 Hz, eta=0:00:00, total=0:00:00, wall=16:59 IST** validation 100.00% of 1x98...Epoch=72/150 LR=0.05418 Time=0.400 Loss=2.128 Prec@1=50.868 Prec@5=75.978 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=16:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:59 IST=> training   0.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=6.125 DataTime=5.975 Loss=1.939 Prec@1=56.250 Prec@5=78.906 rate=0 Hz, eta=?, total=0:00:00, wall=16:59 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=6.125 DataTime=5.975 Loss=1.939 Prec@1=56.250 Prec@5=78.906 rate=4751.07 Hz, eta=0:00:00, total=0:00:00, wall=16:59 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=6.125 DataTime=5.975 Loss=1.939 Prec@1=56.250 Prec@5=78.906 rate=4751.07 Hz, eta=0:00:00, total=0:00:00, wall=16:59 IST=> training   0.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.380 DataTime=0.284 Loss=1.948 Prec@1=54.434 Prec@5=78.487 rate=4751.07 Hz, eta=0:00:00, total=0:00:00, wall=16:59 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.380 DataTime=0.284 Loss=1.948 Prec@1=54.434 Prec@5=78.487 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=16:59 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.380 DataTime=0.284 Loss=1.948 Prec@1=54.434 Prec@5=78.487 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=17:00 IST=> training   4.04% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.351 DataTime=0.255 Loss=1.955 Prec@1=54.404 Prec@5=78.476 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=17:00 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.351 DataTime=0.255 Loss=1.955 Prec@1=54.404 Prec@5=78.476 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=17:00 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.351 DataTime=0.255 Loss=1.955 Prec@1=54.404 Prec@5=78.476 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=17:01 IST=> training   8.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.244 Loss=1.960 Prec@1=54.308 Prec@5=78.451 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=17:01 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.244 Loss=1.960 Prec@1=54.308 Prec@5=78.451 rate=3.10 Hz, eta=0:11:49, total=0:01:37, wall=17:01 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.244 Loss=1.960 Prec@1=54.308 Prec@5=78.451 rate=3.10 Hz, eta=0:11:49, total=0:01:37, wall=17:01 IST=> training   12.03% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.344 DataTime=0.244 Loss=1.963 Prec@1=54.251 Prec@5=78.352 rate=3.10 Hz, eta=0:11:49, total=0:01:37, wall=17:01 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.344 DataTime=0.244 Loss=1.963 Prec@1=54.251 Prec@5=78.352 rate=3.05 Hz, eta=0:11:30, total=0:02:11, wall=17:01 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.344 DataTime=0.244 Loss=1.963 Prec@1=54.251 Prec@5=78.352 rate=3.05 Hz, eta=0:11:30, total=0:02:11, wall=17:02 IST=> training   16.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.241 Loss=1.967 Prec@1=54.192 Prec@5=78.286 rate=3.05 Hz, eta=0:11:30, total=0:02:11, wall=17:02 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.241 Loss=1.967 Prec@1=54.192 Prec@5=78.286 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=17:02 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.343 DataTime=0.241 Loss=1.967 Prec@1=54.192 Prec@5=78.286 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=17:02 IST=> training   20.02% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.342 DataTime=0.238 Loss=1.969 Prec@1=54.109 Prec@5=78.245 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=17:02 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.342 DataTime=0.238 Loss=1.969 Prec@1=54.109 Prec@5=78.245 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=17:02 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.342 DataTime=0.238 Loss=1.969 Prec@1=54.109 Prec@5=78.245 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=17:03 IST=> training   24.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.233 Loss=1.970 Prec@1=54.063 Prec@5=78.258 rate=3.01 Hz, eta=0:10:30, total=0:03:19, wall=17:03 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.233 Loss=1.970 Prec@1=54.063 Prec@5=78.258 rate=3.03 Hz, eta=0:09:53, total=0:03:51, wall=17:03 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.233 Loss=1.970 Prec@1=54.063 Prec@5=78.258 rate=3.03 Hz, eta=0:09:53, total=0:03:51, wall=17:03 IST=> training   28.01% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.340 DataTime=0.235 Loss=1.972 Prec@1=54.034 Prec@5=78.228 rate=3.03 Hz, eta=0:09:53, total=0:03:51, wall=17:03 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.340 DataTime=0.235 Loss=1.972 Prec@1=54.034 Prec@5=78.228 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=17:03 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.340 DataTime=0.235 Loss=1.972 Prec@1=54.034 Prec@5=78.228 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=17:04 IST=> training   32.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.233 Loss=1.976 Prec@1=53.965 Prec@5=78.179 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=17:04 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.233 Loss=1.976 Prec@1=53.965 Prec@5=78.179 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=17:04 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.233 Loss=1.976 Prec@1=53.965 Prec@5=78.179 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=17:04 IST=> training   36.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.976 Prec@1=53.980 Prec@5=78.161 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=17:04 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.976 Prec@1=53.980 Prec@5=78.161 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=17:04 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.976 Prec@1=53.980 Prec@5=78.161 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=17:05 IST=> training   39.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.232 Loss=1.977 Prec@1=53.954 Prec@5=78.143 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=17:05 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.232 Loss=1.977 Prec@1=53.954 Prec@5=78.143 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=17:05 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.232 Loss=1.977 Prec@1=53.954 Prec@5=78.143 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=17:06 IST=> training   43.99% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.978 Prec@1=53.909 Prec@5=78.123 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=17:06 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.978 Prec@1=53.909 Prec@5=78.123 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=17:06 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.231 Loss=1.978 Prec@1=53.909 Prec@5=78.123 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=17:06 IST=> training   47.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.337 DataTime=0.230 Loss=1.980 Prec@1=53.872 Prec@5=78.102 rate=3.00 Hz, eta=0:07:13, total=0:06:39, wall=17:06 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.337 DataTime=0.230 Loss=1.980 Prec@1=53.872 Prec@5=78.102 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=17:06 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.337 DataTime=0.230 Loss=1.980 Prec@1=53.872 Prec@5=78.102 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=17:07 IST=> training   51.98% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.980 Prec@1=53.859 Prec@5=78.093 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=17:07 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.980 Prec@1=53.859 Prec@5=78.093 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=17:07 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.980 Prec@1=53.859 Prec@5=78.093 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=17:07 IST=> training   55.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.982 Prec@1=53.828 Prec@5=78.060 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=17:07 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.982 Prec@1=53.828 Prec@5=78.060 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=17:07 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.982 Prec@1=53.828 Prec@5=78.060 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=17:08 IST=> training   59.97% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.983 Prec@1=53.828 Prec@5=78.048 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=17:08 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.983 Prec@1=53.828 Prec@5=78.048 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=17:08 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.983 Prec@1=53.828 Prec@5=78.048 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=17:08 IST=> training   63.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.984 Prec@1=53.802 Prec@5=78.032 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=17:08 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.984 Prec@1=53.802 Prec@5=78.032 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=17:08 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.231 Loss=1.984 Prec@1=53.802 Prec@5=78.032 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=17:09 IST=> training   67.96% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.985 Prec@1=53.782 Prec@5=78.013 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=17:09 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.985 Prec@1=53.782 Prec@5=78.013 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=17:09 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.985 Prec@1=53.782 Prec@5=78.013 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=17:10 IST=> training   71.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.986 Prec@1=53.762 Prec@5=77.996 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=17:10 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.986 Prec@1=53.762 Prec@5=77.996 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=17:10 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.986 Prec@1=53.762 Prec@5=77.996 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=17:10 IST=> training   75.95% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.761 Prec@5=77.982 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=17:10 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.761 Prec@5=77.982 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=17:10 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.761 Prec@5=77.982 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=17:11 IST=> training   79.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.769 Prec@5=77.977 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=17:11 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.769 Prec@5=77.977 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=17:11 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.987 Prec@1=53.769 Prec@5=77.977 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=17:11 IST=> training   83.94% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.989 Prec@1=53.744 Prec@5=77.958 rate=2.98 Hz, eta=0:02:15, total=0:11:45, wall=17:11 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.989 Prec@1=53.744 Prec@5=77.958 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:11 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.230 Loss=1.989 Prec@1=53.744 Prec@5=77.958 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:12 IST=> training   87.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.989 Prec@1=53.732 Prec@5=77.955 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:12 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.989 Prec@1=53.732 Prec@5=77.955 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=17:12 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.230 Loss=1.989 Prec@1=53.732 Prec@5=77.955 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=17:12 IST=> training   91.93% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.229 Loss=1.990 Prec@1=53.719 Prec@5=77.947 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=17:12 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.229 Loss=1.990 Prec@1=53.719 Prec@5=77.947 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=17:12 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.339 DataTime=0.229 Loss=1.990 Prec@1=53.719 Prec@5=77.947 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=17:13 IST=> training   95.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.229 Loss=1.991 Prec@1=53.702 Prec@5=77.939 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=17:13 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.229 Loss=1.991 Prec@1=53.702 Prec@5=77.939 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=17:13 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.229 Loss=1.991 Prec@1=53.702 Prec@5=77.939 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=17:13 IST=> training   99.92% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.229 Loss=1.991 Prec@1=53.702 Prec@5=77.939 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=17:13 IST=> training   100.00% of 1x2503...Epoch=73/150 LR=0.05314 Time=0.338 DataTime=0.229 Loss=1.991 Prec@1=53.702 Prec@5=77.939 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=17:13 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:13 IST=> validation 0.00% of 1x98...Epoch=73/150 LR=0.05314 Time=7.045 Loss=2.191 Prec@1=49.023 Prec@5=75.195 rate=0 Hz, eta=?, total=0:00:00, wall=17:13 IST=> validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=7.045 Loss=2.191 Prec@1=49.023 Prec@5=75.195 rate=6594.87 Hz, eta=0:00:00, total=0:00:00, wall=17:13 IST** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=7.045 Loss=2.191 Prec@1=49.023 Prec@5=75.195 rate=6594.87 Hz, eta=0:00:00, total=0:00:00, wall=17:14 IST** validation 1.02% of 1x98...Epoch=73/150 LR=0.05314 Time=0.397 Loss=2.092 Prec@1=51.406 Prec@5=76.362 rate=6594.87 Hz, eta=0:00:00, total=0:00:00, wall=17:14 IST** validation 100.00% of 1x98...Epoch=73/150 LR=0.05314 Time=0.397 Loss=2.092 Prec@1=51.406 Prec@5=76.362 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=17:14 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:14 IST=> training   0.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=4.924 DataTime=4.786 Loss=1.815 Prec@1=57.031 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=17:14 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=4.924 DataTime=4.786 Loss=1.815 Prec@1=57.031 Prec@5=80.273 rate=5554.91 Hz, eta=0:00:00, total=0:00:00, wall=17:14 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=4.924 DataTime=4.786 Loss=1.815 Prec@1=57.031 Prec@5=80.273 rate=5554.91 Hz, eta=0:00:00, total=0:00:00, wall=17:14 IST=> training   0.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.382 DataTime=0.287 Loss=1.956 Prec@1=54.446 Prec@5=78.448 rate=5554.91 Hz, eta=0:00:00, total=0:00:00, wall=17:14 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.382 DataTime=0.287 Loss=1.956 Prec@1=54.446 Prec@5=78.448 rate=3.00 Hz, eta=0:13:19, total=0:00:33, wall=17:14 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.382 DataTime=0.287 Loss=1.956 Prec@1=54.446 Prec@5=78.448 rate=3.00 Hz, eta=0:13:19, total=0:00:33, wall=17:15 IST=> training   4.04% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.357 DataTime=0.259 Loss=1.965 Prec@1=54.100 Prec@5=78.285 rate=3.00 Hz, eta=0:13:19, total=0:00:33, wall=17:15 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.357 DataTime=0.259 Loss=1.965 Prec@1=54.100 Prec@5=78.285 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:15 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.357 DataTime=0.259 Loss=1.965 Prec@1=54.100 Prec@5=78.285 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:15 IST=> training   8.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.249 Loss=1.965 Prec@1=54.193 Prec@5=78.283 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:15 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.249 Loss=1.965 Prec@1=54.193 Prec@5=78.283 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=17:15 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.249 Loss=1.965 Prec@1=54.193 Prec@5=78.283 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=17:16 IST=> training   12.03% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.350 DataTime=0.248 Loss=1.960 Prec@1=54.326 Prec@5=78.388 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=17:16 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.350 DataTime=0.248 Loss=1.960 Prec@1=54.326 Prec@5=78.388 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=17:16 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.350 DataTime=0.248 Loss=1.960 Prec@1=54.326 Prec@5=78.388 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=17:17 IST=> training   16.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.247 Loss=1.962 Prec@1=54.298 Prec@5=78.360 rate=2.96 Hz, eta=0:11:50, total=0:02:15, wall=17:17 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.247 Loss=1.962 Prec@1=54.298 Prec@5=78.360 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=17:17 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.349 DataTime=0.247 Loss=1.962 Prec@1=54.298 Prec@5=78.360 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=17:17 IST=> training   20.02% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.346 DataTime=0.243 Loss=1.965 Prec@1=54.260 Prec@5=78.313 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=17:17 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.346 DataTime=0.243 Loss=1.965 Prec@1=54.260 Prec@5=78.313 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=17:17 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.346 DataTime=0.243 Loss=1.965 Prec@1=54.260 Prec@5=78.313 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=17:18 IST=> training   24.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.347 DataTime=0.243 Loss=1.964 Prec@1=54.239 Prec@5=78.343 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=17:18 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.347 DataTime=0.243 Loss=1.964 Prec@1=54.239 Prec@5=78.343 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=17:18 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.347 DataTime=0.243 Loss=1.964 Prec@1=54.239 Prec@5=78.343 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=17:18 IST=> training   28.01% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.344 DataTime=0.240 Loss=1.967 Prec@1=54.156 Prec@5=78.292 rate=2.94 Hz, eta=0:10:11, total=0:03:58, wall=17:18 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.344 DataTime=0.240 Loss=1.967 Prec@1=54.156 Prec@5=78.292 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=17:18 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.344 DataTime=0.240 Loss=1.967 Prec@1=54.156 Prec@5=78.292 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=17:19 IST=> training   32.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.343 DataTime=0.238 Loss=1.967 Prec@1=54.148 Prec@5=78.295 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=17:19 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.343 DataTime=0.238 Loss=1.967 Prec@1=54.148 Prec@5=78.295 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=17:19 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.343 DataTime=0.238 Loss=1.967 Prec@1=54.148 Prec@5=78.295 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=17:19 IST=> training   36.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.238 Loss=1.970 Prec@1=54.109 Prec@5=78.244 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=17:19 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.238 Loss=1.970 Prec@1=54.109 Prec@5=78.244 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=17:19 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.238 Loss=1.970 Prec@1=54.109 Prec@5=78.244 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=17:20 IST=> training   39.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.237 Loss=1.972 Prec@1=54.078 Prec@5=78.214 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=17:20 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.237 Loss=1.972 Prec@1=54.078 Prec@5=78.214 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=17:20 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.342 DataTime=0.237 Loss=1.972 Prec@1=54.078 Prec@5=78.214 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=17:20 IST=> training   43.99% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.972 Prec@1=54.097 Prec@5=78.213 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=17:20 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.972 Prec@1=54.097 Prec@5=78.213 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=17:20 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.972 Prec@1=54.097 Prec@5=78.213 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=17:21 IST=> training   47.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.971 Prec@1=54.097 Prec@5=78.205 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=17:21 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.971 Prec@1=54.097 Prec@5=78.205 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=17:21 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.971 Prec@1=54.097 Prec@5=78.205 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=17:22 IST=> training   51.98% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.974 Prec@1=54.062 Prec@5=78.174 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=17:22 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.974 Prec@1=54.062 Prec@5=78.174 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=17:22 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.341 DataTime=0.236 Loss=1.974 Prec@1=54.062 Prec@5=78.174 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=17:22 IST=> training   55.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.975 Prec@1=54.035 Prec@5=78.162 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=17:22 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.975 Prec@1=54.035 Prec@5=78.162 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=17:22 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.975 Prec@1=54.035 Prec@5=78.162 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=17:23 IST=> training   59.97% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.976 Prec@1=54.013 Prec@5=78.133 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=17:23 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.976 Prec@1=54.013 Prec@5=78.133 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=17:23 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.976 Prec@1=54.013 Prec@5=78.133 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=17:23 IST=> training   63.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.977 Prec@1=53.984 Prec@5=78.136 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=17:23 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.977 Prec@1=53.984 Prec@5=78.136 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=17:23 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.977 Prec@1=53.984 Prec@5=78.136 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=17:24 IST=> training   67.96% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.978 Prec@1=53.950 Prec@5=78.123 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.978 Prec@1=53.950 Prec@5=78.123 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.978 Prec@1=53.950 Prec@5=78.123 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=17:24 IST=> training   71.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.979 Prec@1=53.944 Prec@5=78.104 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=17:24 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.979 Prec@1=53.944 Prec@5=78.104 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=17:24 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.340 DataTime=0.235 Loss=1.979 Prec@1=53.944 Prec@5=78.104 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=17:25 IST=> training   75.95% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.925 Prec@5=78.102 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=17:25 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.925 Prec@5=78.102 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=17:25 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.925 Prec@5=78.102 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=17:25 IST=> training   79.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.916 Prec@5=78.095 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=17:25 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.916 Prec@5=78.095 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=17:25 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.980 Prec@1=53.916 Prec@5=78.095 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=17:26 IST=> training   83.94% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.981 Prec@1=53.909 Prec@5=78.087 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=17:26 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.981 Prec@1=53.909 Prec@5=78.087 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=17:26 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.235 Loss=1.981 Prec@1=53.909 Prec@5=78.087 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=17:27 IST=> training   87.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.982 Prec@1=53.881 Prec@5=78.067 rate=2.97 Hz, eta=0:01:41, total=0:12:22, wall=17:27 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.982 Prec@1=53.881 Prec@5=78.067 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=17:27 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.982 Prec@1=53.881 Prec@5=78.067 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=17:27 IST=> training   91.93% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.983 Prec@1=53.864 Prec@5=78.048 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=17:27 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.983 Prec@1=53.864 Prec@5=78.048 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=17:27 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.983 Prec@1=53.864 Prec@5=78.048 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=17:28 IST=> training   95.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.984 Prec@1=53.858 Prec@5=78.043 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=17:28 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.984 Prec@1=53.858 Prec@5=78.043 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=17:28 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.984 Prec@1=53.858 Prec@5=78.043 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=17:28 IST=> training   99.92% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.984 Prec@1=53.856 Prec@5=78.042 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=17:28 IST=> training   100.00% of 1x2503...Epoch=74/150 LR=0.05209 Time=0.339 DataTime=0.234 Loss=1.984 Prec@1=53.856 Prec@5=78.042 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=17:28 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> validation 0.00% of 1x98...Epoch=74/150 LR=0.05209 Time=6.745 Loss=2.264 Prec@1=50.195 Prec@5=74.414 rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=6.745 Loss=2.264 Prec@1=50.195 Prec@5=74.414 rate=4581.52 Hz, eta=0:00:00, total=0:00:00, wall=17:28 IST** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=6.745 Loss=2.264 Prec@1=50.195 Prec@5=74.414 rate=4581.52 Hz, eta=0:00:00, total=0:00:00, wall=17:28 IST** validation 1.02% of 1x98...Epoch=74/150 LR=0.05209 Time=0.402 Loss=2.105 Prec@1=51.070 Prec@5=76.364 rate=4581.52 Hz, eta=0:00:00, total=0:00:00, wall=17:28 IST** validation 100.00% of 1x98...Epoch=74/150 LR=0.05209 Time=0.402 Loss=2.105 Prec@1=51.070 Prec@5=76.364 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=17:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> training   0.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.859 DataTime=4.731 Loss=1.882 Prec@1=55.469 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.859 DataTime=4.731 Loss=1.882 Prec@1=55.469 Prec@5=79.883 rate=5969.79 Hz, eta=0:00:00, total=0:00:00, wall=17:28 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=4.859 DataTime=4.731 Loss=1.882 Prec@1=55.469 Prec@5=79.883 rate=5969.79 Hz, eta=0:00:00, total=0:00:00, wall=17:29 IST=> training   0.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.379 DataTime=0.281 Loss=1.936 Prec@1=54.724 Prec@5=78.668 rate=5969.79 Hz, eta=0:00:00, total=0:00:00, wall=17:29 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.379 DataTime=0.281 Loss=1.936 Prec@1=54.724 Prec@5=78.668 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=17:29 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.379 DataTime=0.281 Loss=1.936 Prec@1=54.724 Prec@5=78.668 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=17:30 IST=> training   4.04% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.355 DataTime=0.255 Loss=1.937 Prec@1=54.786 Prec@5=78.631 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=17:30 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.355 DataTime=0.255 Loss=1.937 Prec@1=54.786 Prec@5=78.631 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=17:30 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.355 DataTime=0.255 Loss=1.937 Prec@1=54.786 Prec@5=78.631 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=17:30 IST=> training   8.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.347 DataTime=0.246 Loss=1.937 Prec@1=54.732 Prec@5=78.701 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=17:30 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.347 DataTime=0.246 Loss=1.937 Prec@1=54.732 Prec@5=78.701 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=17:30 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.347 DataTime=0.246 Loss=1.937 Prec@1=54.732 Prec@5=78.701 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=17:31 IST=> training   12.03% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.242 Loss=1.937 Prec@1=54.638 Prec@5=78.708 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=17:31 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.242 Loss=1.937 Prec@1=54.638 Prec@5=78.708 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=17:31 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.242 Loss=1.937 Prec@1=54.638 Prec@5=78.708 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=17:31 IST=> training   16.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.241 Loss=1.944 Prec@1=54.480 Prec@5=78.640 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=17:31 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.241 Loss=1.944 Prec@1=54.480 Prec@5=78.640 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=17:31 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.345 DataTime=0.241 Loss=1.944 Prec@1=54.480 Prec@5=78.640 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=17:32 IST=> training   20.02% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.342 DataTime=0.239 Loss=1.948 Prec@1=54.432 Prec@5=78.581 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=17:32 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.342 DataTime=0.239 Loss=1.948 Prec@1=54.432 Prec@5=78.581 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=17:32 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.342 DataTime=0.239 Loss=1.948 Prec@1=54.432 Prec@5=78.581 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=17:32 IST=> training   24.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.341 DataTime=0.239 Loss=1.952 Prec@1=54.356 Prec@5=78.493 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=17:32 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.341 DataTime=0.239 Loss=1.952 Prec@1=54.356 Prec@5=78.493 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=17:32 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.341 DataTime=0.239 Loss=1.952 Prec@1=54.356 Prec@5=78.493 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=17:33 IST=> training   28.01% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.237 Loss=1.957 Prec@1=54.259 Prec@5=78.407 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=17:33 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.237 Loss=1.957 Prec@1=54.259 Prec@5=78.407 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=17:33 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.237 Loss=1.957 Prec@1=54.259 Prec@5=78.407 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=17:34 IST=> training   32.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.958 Prec@1=54.260 Prec@5=78.396 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=17:34 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.958 Prec@1=54.260 Prec@5=78.396 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=17:34 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.958 Prec@1=54.260 Prec@5=78.396 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=17:34 IST=> training   36.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.960 Prec@1=54.207 Prec@5=78.377 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=17:34 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.960 Prec@1=54.207 Prec@5=78.377 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=17:34 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.960 Prec@1=54.207 Prec@5=78.377 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=17:35 IST=> training   39.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.963 Prec@1=54.173 Prec@5=78.341 rate=2.99 Hz, eta=0:08:23, total=0:05:35, wall=17:35 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.963 Prec@1=54.173 Prec@5=78.341 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=17:35 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.963 Prec@1=54.173 Prec@5=78.341 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=17:35 IST=> training   43.99% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.964 Prec@1=54.149 Prec@5=78.336 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=17:35 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.964 Prec@1=54.149 Prec@5=78.336 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=17:35 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.236 Loss=1.964 Prec@1=54.149 Prec@5=78.336 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=17:36 IST=> training   47.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.235 Loss=1.965 Prec@1=54.139 Prec@5=78.334 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=17:36 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.235 Loss=1.965 Prec@1=54.139 Prec@5=78.334 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=17:36 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.340 DataTime=0.235 Loss=1.965 Prec@1=54.139 Prec@5=78.334 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=17:36 IST=> training   51.98% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.966 Prec@1=54.106 Prec@5=78.315 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=17:36 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.966 Prec@1=54.106 Prec@5=78.315 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=17:36 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.966 Prec@1=54.106 Prec@5=78.315 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=17:37 IST=> training   55.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.969 Prec@1=54.054 Prec@5=78.264 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=17:37 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.969 Prec@1=54.054 Prec@5=78.264 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=17:37 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.234 Loss=1.969 Prec@1=54.054 Prec@5=78.264 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=17:37 IST=> training   59.97% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.233 Loss=1.970 Prec@1=54.037 Prec@5=78.249 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=17:37 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.233 Loss=1.970 Prec@1=54.037 Prec@5=78.249 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=17:37 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.339 DataTime=0.233 Loss=1.970 Prec@1=54.037 Prec@5=78.249 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=17:38 IST=> training   63.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.020 Prec@5=78.230 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=17:38 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.020 Prec@5=78.230 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=17:38 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.020 Prec@5=78.230 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=17:39 IST=> training   67.96% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.028 Prec@5=78.227 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=17:39 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.028 Prec@5=78.227 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=17:39 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.971 Prec@1=54.028 Prec@5=78.227 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=17:39 IST=> training   71.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.973 Prec@1=53.990 Prec@5=78.205 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=17:39 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.973 Prec@1=53.990 Prec@5=78.205 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=17:39 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.233 Loss=1.973 Prec@1=53.990 Prec@5=78.205 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=17:40 IST=> training   75.95% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.974 Prec@1=53.970 Prec@5=78.188 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=17:40 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.974 Prec@1=53.970 Prec@5=78.188 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=17:40 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.974 Prec@1=53.970 Prec@5=78.188 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=17:40 IST=> training   79.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.975 Prec@1=53.955 Prec@5=78.170 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=17:40 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.975 Prec@1=53.955 Prec@5=78.170 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=17:40 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.975 Prec@1=53.955 Prec@5=78.170 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=17:41 IST=> training   83.94% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.977 Prec@1=53.930 Prec@5=78.148 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=17:41 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.977 Prec@1=53.930 Prec@5=78.148 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:41 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.338 DataTime=0.232 Loss=1.977 Prec@1=53.930 Prec@5=78.148 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:41 IST=> training   87.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.977 Prec@1=53.916 Prec@5=78.131 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=17:41 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.977 Prec@1=53.916 Prec@5=78.131 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=17:41 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.977 Prec@1=53.916 Prec@5=78.131 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=17:42 IST=> training   91.93% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.913 Prec@5=78.125 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=17:42 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.913 Prec@5=78.125 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=17:42 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.913 Prec@5=78.125 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=17:42 IST=> training   95.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.915 Prec@5=78.128 rate=2.98 Hz, eta=0:00:34, total=0:13:24, wall=17:42 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.915 Prec@5=78.128 rate=2.98 Hz, eta=0:00:00, total=0:13:57, wall=17:42 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.915 Prec@5=78.128 rate=2.98 Hz, eta=0:00:00, total=0:13:57, wall=17:42 IST=> training   99.92% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.916 Prec@5=78.128 rate=2.98 Hz, eta=0:00:00, total=0:13:57, wall=17:42 IST=> training   100.00% of 1x2503...Epoch=75/150 LR=0.05105 Time=0.337 DataTime=0.232 Loss=1.978 Prec@1=53.916 Prec@5=78.128 rate=2.99 Hz, eta=0:00:00, total=0:13:58, wall=17:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> validation 0.00% of 1x98...Epoch=75/150 LR=0.05105 Time=6.744 Loss=2.705 Prec@1=41.211 Prec@5=66.992 rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.744 Loss=2.705 Prec@1=41.211 Prec@5=66.992 rate=5973.25 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=6.744 Loss=2.705 Prec@1=41.211 Prec@5=66.992 rate=5973.25 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST** validation 1.02% of 1x98...Epoch=75/150 LR=0.05105 Time=0.393 Loss=2.534 Prec@1=43.698 Prec@5=69.394 rate=5973.25 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST** validation 100.00% of 1x98...Epoch=75/150 LR=0.05105 Time=0.393 Loss=2.534 Prec@1=43.698 Prec@5=69.394 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=17:43 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> training   0.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=4.782 DataTime=4.627 Loss=1.778 Prec@1=57.812 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=4.782 DataTime=4.627 Loss=1.778 Prec@1=57.812 Prec@5=82.227 rate=3076.62 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=4.782 DataTime=4.627 Loss=1.778 Prec@1=57.812 Prec@5=82.227 rate=3076.62 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST=> training   0.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.381 DataTime=0.286 Loss=1.946 Prec@1=54.510 Prec@5=78.651 rate=3076.62 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.381 DataTime=0.286 Loss=1.946 Prec@1=54.510 Prec@5=78.651 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=17:44 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.381 DataTime=0.286 Loss=1.946 Prec@1=54.510 Prec@5=78.651 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=17:44 IST=> training   4.04% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.357 DataTime=0.261 Loss=1.950 Prec@1=54.404 Prec@5=78.609 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=17:44 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.357 DataTime=0.261 Loss=1.950 Prec@1=54.404 Prec@5=78.609 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:44 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.357 DataTime=0.261 Loss=1.950 Prec@1=54.404 Prec@5=78.609 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:45 IST=> training   8.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.356 DataTime=0.257 Loss=1.950 Prec@1=54.366 Prec@5=78.584 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=17:45 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.356 DataTime=0.257 Loss=1.950 Prec@1=54.366 Prec@5=78.584 rate=2.95 Hz, eta=0:12:27, total=0:01:42, wall=17:45 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.356 DataTime=0.257 Loss=1.950 Prec@1=54.366 Prec@5=78.584 rate=2.95 Hz, eta=0:12:27, total=0:01:42, wall=17:45 IST=> training   12.03% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.349 DataTime=0.248 Loss=1.951 Prec@1=54.299 Prec@5=78.593 rate=2.95 Hz, eta=0:12:27, total=0:01:42, wall=17:45 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.349 DataTime=0.248 Loss=1.951 Prec@1=54.299 Prec@5=78.593 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=17:45 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.349 DataTime=0.248 Loss=1.951 Prec@1=54.299 Prec@5=78.593 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=17:46 IST=> training   16.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.346 DataTime=0.243 Loss=1.954 Prec@1=54.307 Prec@5=78.529 rate=2.97 Hz, eta=0:11:48, total=0:02:15, wall=17:46 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.346 DataTime=0.243 Loss=1.954 Prec@1=54.307 Prec@5=78.529 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=17:46 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.346 DataTime=0.243 Loss=1.954 Prec@1=54.307 Prec@5=78.529 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=17:47 IST=> training   20.02% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.347 DataTime=0.244 Loss=1.955 Prec@1=54.291 Prec@5=78.534 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=17:47 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.347 DataTime=0.244 Loss=1.955 Prec@1=54.291 Prec@5=78.534 rate=2.95 Hz, eta=0:10:44, total=0:03:23, wall=17:47 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.347 DataTime=0.244 Loss=1.955 Prec@1=54.291 Prec@5=78.534 rate=2.95 Hz, eta=0:10:44, total=0:03:23, wall=17:47 IST=> training   24.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.345 DataTime=0.241 Loss=1.956 Prec@1=54.270 Prec@5=78.508 rate=2.95 Hz, eta=0:10:44, total=0:03:23, wall=17:47 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.345 DataTime=0.241 Loss=1.956 Prec@1=54.270 Prec@5=78.508 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=17:47 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.345 DataTime=0.241 Loss=1.956 Prec@1=54.270 Prec@5=78.508 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=17:48 IST=> training   28.01% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.956 Prec@1=54.264 Prec@5=78.521 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=17:48 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.956 Prec@1=54.264 Prec@5=78.521 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=17:48 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.956 Prec@1=54.264 Prec@5=78.521 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=17:48 IST=> training   32.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.959 Prec@1=54.249 Prec@5=78.452 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=17:48 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.959 Prec@1=54.249 Prec@5=78.452 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=17:48 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.239 Loss=1.959 Prec@1=54.249 Prec@5=78.452 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=17:49 IST=> training   36.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.238 Loss=1.961 Prec@1=54.236 Prec@5=78.414 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=17:49 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.238 Loss=1.961 Prec@1=54.236 Prec@5=78.414 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=17:49 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.344 DataTime=0.238 Loss=1.961 Prec@1=54.236 Prec@5=78.414 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=17:49 IST=> training   39.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.962 Prec@1=54.208 Prec@5=78.386 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=17:49 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.962 Prec@1=54.208 Prec@5=78.386 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=17:49 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.962 Prec@1=54.208 Prec@5=78.386 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=17:50 IST=> training   43.99% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.964 Prec@1=54.185 Prec@5=78.360 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=17:50 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.964 Prec@1=54.185 Prec@5=78.360 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=17:50 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.343 DataTime=0.237 Loss=1.964 Prec@1=54.185 Prec@5=78.360 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=17:51 IST=> training   47.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.965 Prec@1=54.141 Prec@5=78.323 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=17:51 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.965 Prec@1=54.141 Prec@5=78.323 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=17:51 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.965 Prec@1=54.141 Prec@5=78.323 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=17:51 IST=> training   51.98% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.967 Prec@1=54.108 Prec@5=78.300 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=17:51 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.967 Prec@1=54.108 Prec@5=78.300 rate=2.95 Hz, eta=0:06:12, total=0:07:54, wall=17:51 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.967 Prec@1=54.108 Prec@5=78.300 rate=2.95 Hz, eta=0:06:12, total=0:07:54, wall=17:52 IST=> training   55.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.968 Prec@1=54.076 Prec@5=78.272 rate=2.95 Hz, eta=0:06:12, total=0:07:54, wall=17:52 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.968 Prec@1=54.076 Prec@5=78.272 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=17:52 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.342 DataTime=0.236 Loss=1.968 Prec@1=54.076 Prec@5=78.272 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=17:52 IST=> training   59.97% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.968 Prec@1=54.090 Prec@5=78.280 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=17:52 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.968 Prec@1=54.090 Prec@5=78.280 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=17:52 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.968 Prec@1=54.090 Prec@5=78.280 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=17:53 IST=> training   63.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.969 Prec@1=54.078 Prec@5=78.276 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=17:53 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.969 Prec@1=54.078 Prec@5=78.276 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=17:53 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.969 Prec@1=54.078 Prec@5=78.276 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=17:53 IST=> training   67.96% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.969 Prec@1=54.078 Prec@5=78.254 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=17:53 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.969 Prec@1=54.078 Prec@5=78.254 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=17:53 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.969 Prec@1=54.078 Prec@5=78.254 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=17:54 IST=> training   71.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.970 Prec@1=54.066 Prec@5=78.234 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=17:54 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.970 Prec@1=54.066 Prec@5=78.234 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=17:54 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.970 Prec@1=54.066 Prec@5=78.234 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=17:55 IST=> training   75.95% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.971 Prec@1=54.039 Prec@5=78.217 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=17:55 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.971 Prec@1=54.039 Prec@5=78.217 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=17:55 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.235 Loss=1.971 Prec@1=54.039 Prec@5=78.217 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=17:55 IST=> training   79.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.972 Prec@1=54.025 Prec@5=78.215 rate=2.96 Hz, eta=0:02:49, total=0:11:17, wall=17:55 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.972 Prec@1=54.025 Prec@5=78.215 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=17:55 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.972 Prec@1=54.025 Prec@5=78.215 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=17:56 IST=> training   83.94% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.234 Loss=1.972 Prec@1=54.024 Prec@5=78.211 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=17:56 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.234 Loss=1.972 Prec@1=54.024 Prec@5=78.211 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=17:56 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.341 DataTime=0.234 Loss=1.972 Prec@1=54.024 Prec@5=78.211 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=17:56 IST=> training   87.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.973 Prec@1=54.015 Prec@5=78.205 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=17:56 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.973 Prec@1=54.015 Prec@5=78.205 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=17:56 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.234 Loss=1.973 Prec@1=54.015 Prec@5=78.205 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=17:57 IST=> training   91.93% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=54.006 Prec@5=78.194 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=17:57 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=54.006 Prec@5=78.194 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=17:57 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=54.006 Prec@5=78.194 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=17:57 IST=> training   95.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=53.994 Prec@5=78.181 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=53.994 Prec@5=78.181 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=53.994 Prec@5=78.181 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=17:57 IST=> training   99.92% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=53.993 Prec@5=78.181 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=17:57 IST=> training   100.00% of 1x2503...Epoch=76/150 LR=0.05000 Time=0.340 DataTime=0.233 Loss=1.974 Prec@1=53.993 Prec@5=78.181 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=17:57 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 0.00% of 1x98...Epoch=76/150 LR=0.05000 Time=6.592 Loss=2.177 Prec@1=46.289 Prec@5=74.414 rate=0 Hz, eta=?, total=0:00:00, wall=17:57 IST=> validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.592 Loss=2.177 Prec@1=46.289 Prec@5=74.414 rate=5154.19 Hz, eta=0:00:00, total=0:00:00, wall=17:57 IST** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=6.592 Loss=2.177 Prec@1=46.289 Prec@5=74.414 rate=5154.19 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST** validation 1.02% of 1x98...Epoch=76/150 LR=0.05000 Time=0.401 Loss=2.201 Prec@1=49.722 Prec@5=74.858 rate=5154.19 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST** validation 100.00% of 1x98...Epoch=76/150 LR=0.05000 Time=0.401 Loss=2.201 Prec@1=49.722 Prec@5=74.858 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=17:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=5.750 DataTime=5.615 Loss=1.974 Prec@1=54.297 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=17:58 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=5.750 DataTime=5.615 Loss=1.974 Prec@1=54.297 Prec@5=77.148 rate=7225.96 Hz, eta=0:00:00, total=0:00:00, wall=17:58 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=5.750 DataTime=5.615 Loss=1.974 Prec@1=54.297 Prec@5=77.148 rate=7225.96 Hz, eta=0:00:00, total=0:00:00, wall=17:59 IST=> training   0.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.374 DataTime=0.274 Loss=1.949 Prec@1=54.208 Prec@5=78.802 rate=7225.96 Hz, eta=0:00:00, total=0:00:00, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.374 DataTime=0.274 Loss=1.949 Prec@1=54.208 Prec@5=78.802 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.374 DataTime=0.274 Loss=1.949 Prec@1=54.208 Prec@5=78.802 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=17:59 IST=> training   4.04% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.351 DataTime=0.252 Loss=1.955 Prec@1=54.329 Prec@5=78.579 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=17:59 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.351 DataTime=0.252 Loss=1.955 Prec@1=54.329 Prec@5=78.579 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=17:59 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.351 DataTime=0.252 Loss=1.955 Prec@1=54.329 Prec@5=78.579 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=18:00 IST=> training   8.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.245 Loss=1.950 Prec@1=54.460 Prec@5=78.562 rate=3.10 Hz, eta=0:12:22, total=0:01:04, wall=18:00 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.245 Loss=1.950 Prec@1=54.460 Prec@5=78.562 rate=3.07 Hz, eta=0:11:57, total=0:01:38, wall=18:00 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.245 Loss=1.950 Prec@1=54.460 Prec@5=78.562 rate=3.07 Hz, eta=0:11:57, total=0:01:38, wall=18:00 IST=> training   12.03% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.244 Loss=1.950 Prec@1=54.497 Prec@5=78.525 rate=3.07 Hz, eta=0:11:57, total=0:01:38, wall=18:00 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.244 Loss=1.950 Prec@1=54.497 Prec@5=78.525 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=18:00 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.345 DataTime=0.244 Loss=1.950 Prec@1=54.497 Prec@5=78.525 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=18:01 IST=> training   16.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.342 DataTime=0.241 Loss=1.951 Prec@1=54.468 Prec@5=78.518 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=18:01 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.342 DataTime=0.241 Loss=1.951 Prec@1=54.468 Prec@5=78.518 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=18:01 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.342 DataTime=0.241 Loss=1.951 Prec@1=54.468 Prec@5=78.518 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=18:01 IST=> training   20.02% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.340 DataTime=0.238 Loss=1.950 Prec@1=54.460 Prec@5=78.527 rate=3.02 Hz, eta=0:11:01, total=0:02:45, wall=18:01 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.340 DataTime=0.238 Loss=1.950 Prec@1=54.460 Prec@5=78.527 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=18:01 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.340 DataTime=0.238 Loss=1.950 Prec@1=54.460 Prec@5=78.527 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=18:02 IST=> training   24.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.407 Prec@5=78.490 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=18:02 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.407 Prec@5=78.490 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=18:02 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.407 Prec@5=78.490 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=18:03 IST=> training   28.01% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.401 Prec@5=78.498 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=18:03 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.401 Prec@5=78.498 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=18:03 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.401 Prec@5=78.498 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=18:03 IST=> training   32.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.397 Prec@5=78.504 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=18:03 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.397 Prec@5=78.504 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=18:03 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.339 DataTime=0.238 Loss=1.953 Prec@1=54.397 Prec@5=78.504 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=18:04 IST=> training   36.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.954 Prec@1=54.417 Prec@5=78.511 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=18:04 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.954 Prec@1=54.417 Prec@5=78.511 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=18:04 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.954 Prec@1=54.417 Prec@5=78.511 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=18:04 IST=> training   39.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.955 Prec@1=54.388 Prec@5=78.467 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=18:04 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.955 Prec@1=54.388 Prec@5=78.467 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=18:04 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.338 DataTime=0.237 Loss=1.955 Prec@1=54.388 Prec@5=78.467 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=18:05 IST=> training   43.99% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.956 Prec@1=54.381 Prec@5=78.445 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=18:05 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.956 Prec@1=54.381 Prec@5=78.445 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=18:05 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.956 Prec@1=54.381 Prec@5=78.445 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=18:05 IST=> training   47.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.235 Loss=1.957 Prec@1=54.365 Prec@5=78.421 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=18:05 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.235 Loss=1.957 Prec@1=54.365 Prec@5=78.421 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=18:05 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.235 Loss=1.957 Prec@1=54.365 Prec@5=78.421 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=18:06 IST=> training   51.98% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.958 Prec@1=54.363 Prec@5=78.422 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=18:06 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.958 Prec@1=54.363 Prec@5=78.422 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=18:06 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.958 Prec@1=54.363 Prec@5=78.422 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=18:06 IST=> training   55.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.327 Prec@5=78.401 rate=3.01 Hz, eta=0:06:06, total=0:07:46, wall=18:06 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.327 Prec@5=78.401 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=18:06 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.327 Prec@5=78.401 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=18:07 IST=> training   59.97% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.303 Prec@5=78.402 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=18:07 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.303 Prec@5=78.402 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=18:07 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.960 Prec@1=54.303 Prec@5=78.402 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=18:08 IST=> training   63.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.961 Prec@1=54.277 Prec@5=78.383 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=18:08 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.961 Prec@1=54.277 Prec@5=78.383 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=18:08 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.235 Loss=1.961 Prec@1=54.277 Prec@5=78.383 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=18:08 IST=> training   67.96% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.962 Prec@1=54.268 Prec@5=78.375 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=18:08 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.962 Prec@1=54.268 Prec@5=78.375 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=18:08 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.962 Prec@1=54.268 Prec@5=78.375 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=18:09 IST=> training   71.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.963 Prec@1=54.232 Prec@5=78.363 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=18:09 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.963 Prec@1=54.232 Prec@5=78.363 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=18:09 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.963 Prec@1=54.232 Prec@5=78.363 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=18:09 IST=> training   75.95% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.964 Prec@1=54.216 Prec@5=78.352 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=18:09 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.964 Prec@1=54.216 Prec@5=78.352 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=18:09 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.964 Prec@1=54.216 Prec@5=78.352 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=18:10 IST=> training   79.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.965 Prec@1=54.196 Prec@5=78.342 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=18:10 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.965 Prec@1=54.196 Prec@5=78.342 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=18:10 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.965 Prec@1=54.196 Prec@5=78.342 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=18:10 IST=> training   83.94% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.966 Prec@1=54.172 Prec@5=78.332 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=18:10 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.966 Prec@1=54.172 Prec@5=78.332 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=18:10 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.234 Loss=1.966 Prec@1=54.172 Prec@5=78.332 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=18:11 IST=> training   87.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.966 Prec@1=54.164 Prec@5=78.328 rate=3.00 Hz, eta=0:01:40, total=0:12:13, wall=18:11 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.966 Prec@1=54.164 Prec@5=78.328 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=18:11 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.337 DataTime=0.234 Loss=1.966 Prec@1=54.164 Prec@5=78.328 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=18:11 IST=> training   91.93% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.967 Prec@1=54.157 Prec@5=78.307 rate=2.99 Hz, eta=0:01:07, total=0:12:49, wall=18:11 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.967 Prec@1=54.157 Prec@5=78.307 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=18:11 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.967 Prec@1=54.157 Prec@5=78.307 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=18:12 IST=> training   95.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.968 Prec@1=54.142 Prec@5=78.293 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=18:12 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.968 Prec@1=54.142 Prec@5=78.293 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=18:12 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.968 Prec@1=54.142 Prec@5=78.293 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=18:12 IST=> training   99.92% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.968 Prec@1=54.143 Prec@5=78.294 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=18:12 IST=> training   100.00% of 1x2503...Epoch=77/150 LR=0.04895 Time=0.336 DataTime=0.233 Loss=1.968 Prec@1=54.143 Prec@5=78.294 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=18:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST=> validation 0.00% of 1x98...Epoch=77/150 LR=0.04895 Time=6.874 Loss=2.297 Prec@1=46.094 Prec@5=73.047 rate=0 Hz, eta=?, total=0:00:00, wall=18:12 IST=> validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.874 Loss=2.297 Prec@1=46.094 Prec@5=73.047 rate=6320.71 Hz, eta=0:00:00, total=0:00:00, wall=18:12 IST** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=6.874 Loss=2.297 Prec@1=46.094 Prec@5=73.047 rate=6320.71 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST** validation 1.02% of 1x98...Epoch=77/150 LR=0.04895 Time=0.398 Loss=2.231 Prec@1=49.022 Prec@5=74.252 rate=6320.71 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST** validation 100.00% of 1x98...Epoch=77/150 LR=0.04895 Time=0.398 Loss=2.231 Prec@1=49.022 Prec@5=74.252 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=18:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST=> training   0.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=5.498 DataTime=5.339 Loss=1.895 Prec@1=54.297 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=18:13 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=5.498 DataTime=5.339 Loss=1.895 Prec@1=54.297 Prec@5=80.469 rate=6291.17 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=5.498 DataTime=5.339 Loss=1.895 Prec@1=54.297 Prec@5=80.469 rate=6291.17 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST=> training   0.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.377 DataTime=0.280 Loss=1.922 Prec@1=55.194 Prec@5=78.999 rate=6291.17 Hz, eta=0:00:00, total=0:00:00, wall=18:13 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.377 DataTime=0.280 Loss=1.922 Prec@1=55.194 Prec@5=78.999 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=18:13 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.377 DataTime=0.280 Loss=1.922 Prec@1=55.194 Prec@5=78.999 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=18:14 IST=> training   4.04% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.353 DataTime=0.256 Loss=1.925 Prec@1=54.997 Prec@5=78.811 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=18:14 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.353 DataTime=0.256 Loss=1.925 Prec@1=54.997 Prec@5=78.811 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=18:14 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.353 DataTime=0.256 Loss=1.925 Prec@1=54.997 Prec@5=78.811 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=18:14 IST=> training   8.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.351 DataTime=0.254 Loss=1.928 Prec@1=54.865 Prec@5=78.810 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=18:14 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.351 DataTime=0.254 Loss=1.928 Prec@1=54.865 Prec@5=78.810 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=18:14 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.351 DataTime=0.254 Loss=1.928 Prec@1=54.865 Prec@5=78.810 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=18:15 IST=> training   12.03% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.347 DataTime=0.248 Loss=1.930 Prec@1=54.809 Prec@5=78.852 rate=3.01 Hz, eta=0:12:12, total=0:01:40, wall=18:15 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.347 DataTime=0.248 Loss=1.930 Prec@1=54.809 Prec@5=78.852 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=18:15 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.347 DataTime=0.248 Loss=1.930 Prec@1=54.809 Prec@5=78.852 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=18:16 IST=> training   16.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.243 Loss=1.933 Prec@1=54.755 Prec@5=78.795 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=18:16 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.243 Loss=1.933 Prec@1=54.755 Prec@5=78.795 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=18:16 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.243 Loss=1.933 Prec@1=54.755 Prec@5=78.795 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=18:16 IST=> training   20.02% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.242 Loss=1.933 Prec@1=54.723 Prec@5=78.806 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=18:16 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.242 Loss=1.933 Prec@1=54.723 Prec@5=78.806 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=18:16 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.344 DataTime=0.242 Loss=1.933 Prec@1=54.723 Prec@5=78.806 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=18:17 IST=> training   24.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.241 Loss=1.935 Prec@1=54.695 Prec@5=78.786 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=18:17 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.241 Loss=1.935 Prec@1=54.695 Prec@5=78.786 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=18:17 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.241 Loss=1.935 Prec@1=54.695 Prec@5=78.786 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=18:17 IST=> training   28.01% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.240 Loss=1.936 Prec@1=54.688 Prec@5=78.749 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=18:17 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.240 Loss=1.936 Prec@1=54.688 Prec@5=78.749 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=18:17 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.240 Loss=1.936 Prec@1=54.688 Prec@5=78.749 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=18:18 IST=> training   32.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.240 Loss=1.938 Prec@1=54.646 Prec@5=78.714 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=18:18 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.240 Loss=1.938 Prec@1=54.646 Prec@5=78.714 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=18:18 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.343 DataTime=0.240 Loss=1.938 Prec@1=54.646 Prec@5=78.714 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=18:18 IST=> training   36.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.239 Loss=1.942 Prec@1=54.591 Prec@5=78.648 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=18:18 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.239 Loss=1.942 Prec@1=54.591 Prec@5=78.648 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=18:18 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.239 Loss=1.942 Prec@1=54.591 Prec@5=78.648 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=18:19 IST=> training   39.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.942 Prec@1=54.579 Prec@5=78.632 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=18:19 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.942 Prec@1=54.579 Prec@5=78.632 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=18:19 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.942 Prec@1=54.579 Prec@5=78.632 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=18:20 IST=> training   43.99% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.944 Prec@1=54.574 Prec@5=78.614 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=18:20 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.944 Prec@1=54.574 Prec@5=78.614 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=18:20 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.238 Loss=1.944 Prec@1=54.574 Prec@5=78.614 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=18:20 IST=> training   47.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.238 Loss=1.945 Prec@1=54.560 Prec@5=78.583 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=18:20 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.238 Loss=1.945 Prec@1=54.560 Prec@5=78.583 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=18:20 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.238 Loss=1.945 Prec@1=54.560 Prec@5=78.583 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=18:21 IST=> training   51.98% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.237 Loss=1.947 Prec@1=54.537 Prec@5=78.548 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=18:21 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.237 Loss=1.947 Prec@1=54.537 Prec@5=78.548 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=18:21 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.237 Loss=1.947 Prec@1=54.537 Prec@5=78.548 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=18:21 IST=> training   55.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.950 Prec@1=54.493 Prec@5=78.518 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=18:21 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.950 Prec@1=54.493 Prec@5=78.518 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:21 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.950 Prec@1=54.493 Prec@5=78.518 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:22 IST=> training   59.97% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.951 Prec@1=54.456 Prec@5=78.496 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:22 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.951 Prec@1=54.456 Prec@5=78.496 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=18:22 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.237 Loss=1.951 Prec@1=54.456 Prec@5=78.496 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=18:22 IST=> training   63.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.424 Prec@5=78.475 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=18:22 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.424 Prec@5=78.475 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=18:22 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.424 Prec@5=78.475 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=18:23 IST=> training   67.96% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.396 Prec@5=78.463 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=18:23 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.396 Prec@5=78.463 rate=2.95 Hz, eta=0:03:58, total=0:10:10, wall=18:23 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.953 Prec@1=54.396 Prec@5=78.463 rate=2.95 Hz, eta=0:03:58, total=0:10:10, wall=18:23 IST=> training   71.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.392 Prec@5=78.464 rate=2.95 Hz, eta=0:03:58, total=0:10:10, wall=18:23 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.392 Prec@5=78.464 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=18:23 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.392 Prec@5=78.464 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=18:24 IST=> training   75.95% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.383 Prec@5=78.464 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=18:24 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.383 Prec@5=78.464 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=18:24 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.954 Prec@1=54.383 Prec@5=78.464 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=18:25 IST=> training   79.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.955 Prec@1=54.362 Prec@5=78.450 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=18:25 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.955 Prec@1=54.362 Prec@5=78.450 rate=2.95 Hz, eta=0:02:16, total=0:11:53, wall=18:25 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.236 Loss=1.955 Prec@1=54.362 Prec@5=78.450 rate=2.95 Hz, eta=0:02:16, total=0:11:53, wall=18:25 IST=> training   83.94% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.956 Prec@1=54.347 Prec@5=78.446 rate=2.95 Hz, eta=0:02:16, total=0:11:53, wall=18:25 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.956 Prec@1=54.347 Prec@5=78.446 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=18:25 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.956 Prec@1=54.347 Prec@5=78.446 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=18:26 IST=> training   87.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.235 Loss=1.957 Prec@1=54.327 Prec@5=78.432 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=18:26 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.235 Loss=1.957 Prec@1=54.327 Prec@5=78.432 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=18:26 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.235 Loss=1.957 Prec@1=54.327 Prec@5=78.432 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=18:26 IST=> training   91.93% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.958 Prec@1=54.310 Prec@5=78.401 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=18:26 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.958 Prec@1=54.310 Prec@5=78.401 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=18:26 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.342 DataTime=0.235 Loss=1.958 Prec@1=54.310 Prec@5=78.401 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=18:27 IST=> training   95.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.234 Loss=1.959 Prec@1=54.295 Prec@5=78.378 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=18:27 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.234 Loss=1.959 Prec@1=54.295 Prec@5=78.378 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=18:27 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.234 Loss=1.959 Prec@1=54.295 Prec@5=78.378 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=18:27 IST=> training   99.92% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.234 Loss=1.959 Prec@1=54.294 Prec@5=78.378 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=18:27 IST=> training   100.00% of 1x2503...Epoch=78/150 LR=0.04791 Time=0.341 DataTime=0.234 Loss=1.959 Prec@1=54.294 Prec@5=78.378 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=18:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:27 IST=> validation 0.00% of 1x98...Epoch=78/150 LR=0.04791 Time=6.622 Loss=2.042 Prec@1=53.516 Prec@5=77.734 rate=0 Hz, eta=?, total=0:00:00, wall=18:27 IST=> validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=6.622 Loss=2.042 Prec@1=53.516 Prec@5=77.734 rate=3588.72 Hz, eta=0:00:00, total=0:00:00, wall=18:27 IST** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=6.622 Loss=2.042 Prec@1=53.516 Prec@5=77.734 rate=3588.72 Hz, eta=0:00:00, total=0:00:00, wall=18:28 IST** validation 1.02% of 1x98...Epoch=78/150 LR=0.04791 Time=0.403 Loss=2.049 Prec@1=52.494 Prec@5=77.212 rate=3588.72 Hz, eta=0:00:00, total=0:00:00, wall=18:28 IST** validation 100.00% of 1x98...Epoch=78/150 LR=0.04791 Time=0.403 Loss=2.049 Prec@1=52.494 Prec@5=77.212 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=18:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:28 IST=> training   0.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=4.949 DataTime=4.654 Loss=1.912 Prec@1=56.445 Prec@5=78.516 rate=0 Hz, eta=?, total=0:00:00, wall=18:28 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=4.949 DataTime=4.654 Loss=1.912 Prec@1=56.445 Prec@5=78.516 rate=3713.22 Hz, eta=0:00:00, total=0:00:00, wall=18:28 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=4.949 DataTime=4.654 Loss=1.912 Prec@1=56.445 Prec@5=78.516 rate=3713.22 Hz, eta=0:00:00, total=0:00:00, wall=18:28 IST=> training   0.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.389 DataTime=0.289 Loss=1.924 Prec@1=55.119 Prec@5=79.063 rate=3713.22 Hz, eta=0:00:00, total=0:00:00, wall=18:28 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.389 DataTime=0.289 Loss=1.924 Prec@1=55.119 Prec@5=79.063 rate=2.94 Hz, eta=0:13:37, total=0:00:34, wall=18:28 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.389 DataTime=0.289 Loss=1.924 Prec@1=55.119 Prec@5=79.063 rate=2.94 Hz, eta=0:13:37, total=0:00:34, wall=18:29 IST=> training   4.04% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.361 DataTime=0.261 Loss=1.922 Prec@1=55.084 Prec@5=78.922 rate=2.94 Hz, eta=0:13:37, total=0:00:34, wall=18:29 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.361 DataTime=0.261 Loss=1.922 Prec@1=55.084 Prec@5=78.922 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=18:29 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.361 DataTime=0.261 Loss=1.922 Prec@1=55.084 Prec@5=78.922 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=18:29 IST=> training   8.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.251 Loss=1.929 Prec@1=55.006 Prec@5=78.811 rate=2.97 Hz, eta=0:12:55, total=0:01:07, wall=18:29 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.251 Loss=1.929 Prec@1=55.006 Prec@5=78.811 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=18:29 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.251 Loss=1.929 Prec@1=55.006 Prec@5=78.811 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=18:30 IST=> training   12.03% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.250 Loss=1.930 Prec@1=54.941 Prec@5=78.821 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=18:30 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.250 Loss=1.930 Prec@1=54.941 Prec@5=78.821 rate=2.94 Hz, eta=0:11:53, total=0:02:16, wall=18:30 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.352 DataTime=0.250 Loss=1.930 Prec@1=54.941 Prec@5=78.821 rate=2.94 Hz, eta=0:11:53, total=0:02:16, wall=18:30 IST=> training   16.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.350 DataTime=0.246 Loss=1.931 Prec@1=54.882 Prec@5=78.820 rate=2.94 Hz, eta=0:11:53, total=0:02:16, wall=18:30 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.350 DataTime=0.246 Loss=1.931 Prec@1=54.882 Prec@5=78.820 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=18:30 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.350 DataTime=0.246 Loss=1.931 Prec@1=54.882 Prec@5=78.820 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=18:31 IST=> training   20.02% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.347 DataTime=0.243 Loss=1.933 Prec@1=54.842 Prec@5=78.765 rate=2.94 Hz, eta=0:11:20, total=0:02:50, wall=18:31 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.347 DataTime=0.243 Loss=1.933 Prec@1=54.842 Prec@5=78.765 rate=2.95 Hz, eta=0:10:45, total=0:03:23, wall=18:31 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.347 DataTime=0.243 Loss=1.933 Prec@1=54.842 Prec@5=78.765 rate=2.95 Hz, eta=0:10:45, total=0:03:23, wall=18:32 IST=> training   24.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.935 Prec@1=54.782 Prec@5=78.747 rate=2.95 Hz, eta=0:10:45, total=0:03:23, wall=18:32 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.935 Prec@1=54.782 Prec@5=78.747 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=18:32 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.935 Prec@1=54.782 Prec@5=78.747 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=18:32 IST=> training   28.01% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.936 Prec@1=54.783 Prec@5=78.758 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=18:32 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.936 Prec@1=54.783 Prec@5=78.758 rate=2.94 Hz, eta=0:09:38, total=0:04:32, wall=18:32 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.346 DataTime=0.242 Loss=1.936 Prec@1=54.783 Prec@5=78.758 rate=2.94 Hz, eta=0:09:38, total=0:04:32, wall=18:33 IST=> training   32.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.240 Loss=1.940 Prec@1=54.699 Prec@5=78.695 rate=2.94 Hz, eta=0:09:38, total=0:04:32, wall=18:33 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.240 Loss=1.940 Prec@1=54.699 Prec@5=78.695 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=18:33 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.240 Loss=1.940 Prec@1=54.699 Prec@5=78.695 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=18:33 IST=> training   36.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.940 Prec@1=54.727 Prec@5=78.706 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=18:33 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.940 Prec@1=54.727 Prec@5=78.706 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=18:33 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.940 Prec@1=54.727 Prec@5=78.706 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=18:34 IST=> training   39.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.239 Loss=1.942 Prec@1=54.688 Prec@5=78.661 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=18:34 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.239 Loss=1.942 Prec@1=54.688 Prec@5=78.661 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=18:34 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.239 Loss=1.942 Prec@1=54.688 Prec@5=78.661 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=18:34 IST=> training   43.99% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.944 Prec@1=54.660 Prec@5=78.639 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=18:34 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.944 Prec@1=54.660 Prec@5=78.639 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=18:34 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.344 DataTime=0.238 Loss=1.944 Prec@1=54.660 Prec@5=78.639 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=18:35 IST=> training   47.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.608 Prec@5=78.608 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=18:35 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.608 Prec@5=78.608 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=18:35 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.608 Prec@5=78.608 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=18:36 IST=> training   51.98% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.605 Prec@5=78.602 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=18:36 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.605 Prec@5=78.602 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=18:36 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.343 DataTime=0.237 Loss=1.946 Prec@1=54.605 Prec@5=78.602 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=18:36 IST=> training   55.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.948 Prec@1=54.572 Prec@5=78.568 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=18:36 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.948 Prec@1=54.572 Prec@5=78.568 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:36 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.948 Prec@1=54.572 Prec@5=78.568 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:37 IST=> training   59.97% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.235 Loss=1.949 Prec@1=54.557 Prec@5=78.551 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=18:37 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.235 Loss=1.949 Prec@1=54.557 Prec@5=78.551 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=18:37 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.235 Loss=1.949 Prec@1=54.557 Prec@5=78.551 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=18:37 IST=> training   63.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.950 Prec@1=54.558 Prec@5=78.540 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=18:37 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.950 Prec@1=54.558 Prec@5=78.540 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=18:37 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.342 DataTime=0.235 Loss=1.950 Prec@1=54.558 Prec@5=78.540 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=18:38 IST=> training   67.96% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.951 Prec@1=54.543 Prec@5=78.521 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=18:38 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.951 Prec@1=54.543 Prec@5=78.521 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=18:38 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.951 Prec@1=54.543 Prec@5=78.521 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=18:38 IST=> training   71.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.506 Prec@5=78.501 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=18:38 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.506 Prec@5=78.501 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=18:38 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.506 Prec@5=78.501 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=18:39 IST=> training   75.95% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.495 Prec@5=78.495 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=18:39 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.495 Prec@5=78.495 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=18:39 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.341 DataTime=0.234 Loss=1.952 Prec@1=54.495 Prec@5=78.495 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=18:39 IST=> training   79.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.478 Prec@5=78.488 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=18:39 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.478 Prec@5=78.488 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=18:39 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.478 Prec@5=78.488 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=18:40 IST=> training   83.94% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.464 Prec@5=78.487 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=18:40 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.464 Prec@5=78.487 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=18:40 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.953 Prec@1=54.464 Prec@5=78.487 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=18:41 IST=> training   87.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.954 Prec@1=54.445 Prec@5=78.467 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=18:41 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.954 Prec@1=54.445 Prec@5=78.467 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=18:41 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.954 Prec@1=54.445 Prec@5=78.467 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=18:41 IST=> training   91.93% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.955 Prec@1=54.422 Prec@5=78.454 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=18:41 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.955 Prec@1=54.422 Prec@5=78.454 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=18:41 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.340 DataTime=0.233 Loss=1.955 Prec@1=54.422 Prec@5=78.454 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=18:42 IST=> training   95.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.339 DataTime=0.232 Loss=1.955 Prec@1=54.420 Prec@5=78.454 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=18:42 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.339 DataTime=0.232 Loss=1.955 Prec@1=54.420 Prec@5=78.454 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=18:42 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.339 DataTime=0.232 Loss=1.955 Prec@1=54.420 Prec@5=78.454 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=18:42 IST=> training   99.92% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.339 DataTime=0.233 Loss=1.955 Prec@1=54.421 Prec@5=78.453 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=18:42 IST=> training   100.00% of 1x2503...Epoch=79/150 LR=0.04686 Time=0.339 DataTime=0.233 Loss=1.955 Prec@1=54.421 Prec@5=78.453 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=18:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:42 IST=> validation 0.00% of 1x98...Epoch=79/150 LR=0.04686 Time=7.124 Loss=2.126 Prec@1=50.391 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=18:42 IST=> validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=7.124 Loss=2.126 Prec@1=50.391 Prec@5=74.609 rate=6349.21 Hz, eta=0:00:00, total=0:00:00, wall=18:42 IST** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=7.124 Loss=2.126 Prec@1=50.391 Prec@5=74.609 rate=6349.21 Hz, eta=0:00:00, total=0:00:00, wall=18:42 IST** validation 1.02% of 1x98...Epoch=79/150 LR=0.04686 Time=0.405 Loss=2.102 Prec@1=51.106 Prec@5=76.438 rate=6349.21 Hz, eta=0:00:00, total=0:00:00, wall=18:42 IST** validation 100.00% of 1x98...Epoch=79/150 LR=0.04686 Time=0.405 Loss=2.102 Prec@1=51.106 Prec@5=76.438 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=18:42 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST=> training   0.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.605 DataTime=5.459 Loss=1.974 Prec@1=55.664 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.605 DataTime=5.459 Loss=1.974 Prec@1=55.664 Prec@5=77.344 rate=3075.13 Hz, eta=0:00:00, total=0:00:00, wall=18:43 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=5.605 DataTime=5.459 Loss=1.974 Prec@1=55.664 Prec@5=77.344 rate=3075.13 Hz, eta=0:00:00, total=0:00:00, wall=18:43 IST=> training   0.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.378 DataTime=0.283 Loss=1.920 Prec@1=54.978 Prec@5=78.978 rate=3075.13 Hz, eta=0:00:00, total=0:00:00, wall=18:43 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.378 DataTime=0.283 Loss=1.920 Prec@1=54.978 Prec@5=78.978 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=18:43 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.378 DataTime=0.283 Loss=1.920 Prec@1=54.978 Prec@5=78.978 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=18:44 IST=> training   4.04% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.359 DataTime=0.260 Loss=1.924 Prec@1=54.858 Prec@5=78.886 rate=3.10 Hz, eta=0:12:54, total=0:00:32, wall=18:44 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.359 DataTime=0.260 Loss=1.924 Prec@1=54.858 Prec@5=78.886 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=18:44 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.359 DataTime=0.260 Loss=1.924 Prec@1=54.858 Prec@5=78.886 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=18:44 IST=> training   8.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.347 DataTime=0.248 Loss=1.920 Prec@1=55.026 Prec@5=78.984 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=18:44 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.347 DataTime=0.248 Loss=1.920 Prec@1=55.026 Prec@5=78.984 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=18:44 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.347 DataTime=0.248 Loss=1.920 Prec@1=55.026 Prec@5=78.984 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=18:45 IST=> training   12.03% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.243 Loss=1.921 Prec@1=55.136 Prec@5=78.962 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=18:45 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.243 Loss=1.921 Prec@1=55.136 Prec@5=78.962 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=18:45 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.243 Loss=1.921 Prec@1=55.136 Prec@5=78.962 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=18:45 IST=> training   16.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.344 DataTime=0.244 Loss=1.927 Prec@1=54.965 Prec@5=78.843 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=18:45 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.344 DataTime=0.244 Loss=1.927 Prec@1=54.965 Prec@5=78.843 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=18:45 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.344 DataTime=0.244 Loss=1.927 Prec@1=54.965 Prec@5=78.843 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=18:46 IST=> training   20.02% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.240 Loss=1.929 Prec@1=54.886 Prec@5=78.792 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=18:46 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.240 Loss=1.929 Prec@1=54.886 Prec@5=78.792 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=18:46 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.343 DataTime=0.240 Loss=1.929 Prec@1=54.886 Prec@5=78.792 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=18:46 IST=> training   24.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.930 Prec@1=54.864 Prec@5=78.787 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=18:46 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.930 Prec@1=54.864 Prec@5=78.787 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=18:46 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.930 Prec@1=54.864 Prec@5=78.787 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=18:47 IST=> training   28.01% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.932 Prec@1=54.825 Prec@5=78.795 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=18:47 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.932 Prec@1=54.825 Prec@5=78.795 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=18:47 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.238 Loss=1.932 Prec@1=54.825 Prec@5=78.795 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=18:48 IST=> training   32.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.934 Prec@1=54.801 Prec@5=78.773 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=18:48 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.934 Prec@1=54.801 Prec@5=78.773 rate=2.99 Hz, eta=0:08:54, total=0:05:00, wall=18:48 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.934 Prec@1=54.801 Prec@5=78.773 rate=2.99 Hz, eta=0:08:54, total=0:05:00, wall=18:48 IST=> training   36.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.937 Prec@1=54.723 Prec@5=78.731 rate=2.99 Hz, eta=0:08:54, total=0:05:00, wall=18:48 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.937 Prec@1=54.723 Prec@5=78.731 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=18:48 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.237 Loss=1.937 Prec@1=54.723 Prec@5=78.731 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=18:49 IST=> training   39.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.939 Prec@1=54.687 Prec@5=78.701 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=18:49 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.939 Prec@1=54.687 Prec@5=78.701 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=18:49 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.939 Prec@1=54.687 Prec@5=78.701 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=18:49 IST=> training   43.99% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.940 Prec@1=54.683 Prec@5=78.683 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=18:49 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.940 Prec@1=54.683 Prec@5=78.683 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=18:49 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.341 DataTime=0.237 Loss=1.940 Prec@1=54.683 Prec@5=78.683 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=18:50 IST=> training   47.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.236 Loss=1.941 Prec@1=54.660 Prec@5=78.668 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=18:50 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.236 Loss=1.941 Prec@1=54.660 Prec@5=78.668 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=18:50 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.340 DataTime=0.236 Loss=1.941 Prec@1=54.660 Prec@5=78.668 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=18:50 IST=> training   51.98% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.235 Loss=1.942 Prec@1=54.630 Prec@5=78.666 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=18:50 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.235 Loss=1.942 Prec@1=54.630 Prec@5=78.666 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=18:50 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.235 Loss=1.942 Prec@1=54.630 Prec@5=78.666 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=18:51 IST=> training   55.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.942 Prec@1=54.632 Prec@5=78.654 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=18:51 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.942 Prec@1=54.632 Prec@5=78.654 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=18:51 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.942 Prec@1=54.632 Prec@5=78.654 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=18:51 IST=> training   59.97% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.943 Prec@1=54.616 Prec@5=78.632 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=18:51 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.943 Prec@1=54.616 Prec@5=78.632 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=18:51 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.943 Prec@1=54.616 Prec@5=78.632 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=18:52 IST=> training   63.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.943 Prec@1=54.616 Prec@5=78.631 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=18:52 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.943 Prec@1=54.616 Prec@5=78.631 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=18:52 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.943 Prec@1=54.616 Prec@5=78.631 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=18:53 IST=> training   67.96% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.944 Prec@1=54.594 Prec@5=78.623 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=18:53 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.944 Prec@1=54.594 Prec@5=78.623 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=18:53 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.234 Loss=1.944 Prec@1=54.594 Prec@5=78.623 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=18:53 IST=> training   71.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.945 Prec@1=54.597 Prec@5=78.613 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=18:53 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.945 Prec@1=54.597 Prec@5=78.613 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=18:53 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.945 Prec@1=54.597 Prec@5=78.613 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=18:54 IST=> training   75.95% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.945 Prec@1=54.588 Prec@5=78.607 rate=2.99 Hz, eta=0:03:21, total=0:10:36, wall=18:54 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.945 Prec@1=54.588 Prec@5=78.607 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=18:54 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.339 DataTime=0.234 Loss=1.945 Prec@1=54.588 Prec@5=78.607 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=18:54 IST=> training   79.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.946 Prec@1=54.584 Prec@5=78.607 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=18:54 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.946 Prec@1=54.584 Prec@5=78.607 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=18:54 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.946 Prec@1=54.584 Prec@5=78.607 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=18:55 IST=> training   83.94% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.567 Prec@5=78.586 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=18:55 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.567 Prec@5=78.586 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=18:55 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.567 Prec@5=78.586 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=18:55 IST=> training   87.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.556 Prec@5=78.582 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=18:55 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.556 Prec@5=78.582 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=18:55 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.556 Prec@5=78.582 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=18:56 IST=> training   91.93% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.558 Prec@5=78.578 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=18:56 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.558 Prec@5=78.578 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=18:56 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.338 DataTime=0.233 Loss=1.947 Prec@1=54.558 Prec@5=78.578 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=18:56 IST=> training   95.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.337 DataTime=0.232 Loss=1.947 Prec@1=54.567 Prec@5=78.578 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=18:56 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.337 DataTime=0.232 Loss=1.947 Prec@1=54.567 Prec@5=78.578 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=18:56 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.337 DataTime=0.232 Loss=1.947 Prec@1=54.567 Prec@5=78.578 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=18:56 IST=> training   99.92% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.337 DataTime=0.232 Loss=1.947 Prec@1=54.566 Prec@5=78.579 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=18:56 IST=> training   100.00% of 1x2503...Epoch=80/150 LR=0.04582 Time=0.337 DataTime=0.232 Loss=1.947 Prec@1=54.566 Prec@5=78.579 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=18:56 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> validation 0.00% of 1x98...Epoch=80/150 LR=0.04582 Time=6.910 Loss=2.322 Prec@1=47.852 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.910 Loss=2.322 Prec@1=47.852 Prec@5=74.609 rate=3335.62 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=6.910 Loss=2.322 Prec@1=47.852 Prec@5=74.609 rate=3335.62 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST** validation 1.02% of 1x98...Epoch=80/150 LR=0.04582 Time=0.395 Loss=2.346 Prec@1=46.920 Prec@5=72.502 rate=3335.62 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST** validation 100.00% of 1x98...Epoch=80/150 LR=0.04582 Time=0.395 Loss=2.346 Prec@1=46.920 Prec@5=72.502 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=18:57 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> training   0.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.524 DataTime=4.410 Loss=1.837 Prec@1=53.711 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.524 DataTime=4.410 Loss=1.837 Prec@1=53.711 Prec@5=80.273 rate=6089.54 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=4.524 DataTime=4.410 Loss=1.837 Prec@1=53.711 Prec@5=80.273 rate=6089.54 Hz, eta=0:00:00, total=0:00:00, wall=18:58 IST=> training   0.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.377 DataTime=0.274 Loss=1.911 Prec@1=55.270 Prec@5=79.158 rate=6089.54 Hz, eta=0:00:00, total=0:00:00, wall=18:58 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.377 DataTime=0.274 Loss=1.911 Prec@1=55.270 Prec@5=79.158 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=18:58 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.377 DataTime=0.274 Loss=1.911 Prec@1=55.270 Prec@5=79.158 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=18:58 IST=> training   4.04% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.354 DataTime=0.254 Loss=1.915 Prec@1=55.267 Prec@5=79.066 rate=3.01 Hz, eta=0:13:18, total=0:00:33, wall=18:58 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.354 DataTime=0.254 Loss=1.915 Prec@1=55.267 Prec@5=79.066 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=18:58 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.354 DataTime=0.254 Loss=1.915 Prec@1=55.267 Prec@5=79.066 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=18:59 IST=> training   8.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.352 DataTime=0.250 Loss=1.920 Prec@1=55.090 Prec@5=79.006 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=18:59 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.352 DataTime=0.250 Loss=1.920 Prec@1=55.090 Prec@5=79.006 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=18:59 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.352 DataTime=0.250 Loss=1.920 Prec@1=55.090 Prec@5=79.006 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=18:59 IST=> training   12.03% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.347 DataTime=0.245 Loss=1.922 Prec@1=55.056 Prec@5=78.949 rate=2.97 Hz, eta=0:12:22, total=0:01:41, wall=18:59 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.347 DataTime=0.245 Loss=1.922 Prec@1=55.056 Prec@5=78.949 rate=2.98 Hz, eta=0:11:46, total=0:02:14, wall=18:59 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.347 DataTime=0.245 Loss=1.922 Prec@1=55.056 Prec@5=78.949 rate=2.98 Hz, eta=0:11:46, total=0:02:14, wall=19:00 IST=> training   16.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.241 Loss=1.925 Prec@1=54.995 Prec@5=78.861 rate=2.98 Hz, eta=0:11:46, total=0:02:14, wall=19:00 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.241 Loss=1.925 Prec@1=54.995 Prec@5=78.861 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=19:00 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.241 Loss=1.925 Prec@1=54.995 Prec@5=78.861 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=19:01 IST=> training   20.02% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.240 Loss=1.925 Prec@1=55.000 Prec@5=78.897 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=19:01 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.240 Loss=1.925 Prec@1=55.000 Prec@5=78.897 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=19:01 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.345 DataTime=0.240 Loss=1.925 Prec@1=55.000 Prec@5=78.897 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=19:01 IST=> training   24.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.926 Prec@1=54.955 Prec@5=78.877 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=19:01 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.926 Prec@1=54.955 Prec@5=78.877 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=19:01 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.926 Prec@1=54.955 Prec@5=78.877 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=19:02 IST=> training   28.01% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.928 Prec@1=54.919 Prec@5=78.845 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=19:02 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.928 Prec@1=54.919 Prec@5=78.845 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=19:02 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.342 DataTime=0.237 Loss=1.928 Prec@1=54.919 Prec@5=78.845 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=19:02 IST=> training   32.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.928 Prec@1=54.917 Prec@5=78.817 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=19:02 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.928 Prec@1=54.917 Prec@5=78.817 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=19:02 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.928 Prec@1=54.917 Prec@5=78.817 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=19:03 IST=> training   36.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.235 Loss=1.929 Prec@1=54.878 Prec@5=78.822 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=19:03 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.235 Loss=1.929 Prec@1=54.878 Prec@5=78.822 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=19:03 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.235 Loss=1.929 Prec@1=54.878 Prec@5=78.822 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=19:03 IST=> training   39.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.930 Prec@1=54.868 Prec@5=78.807 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=19:03 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.930 Prec@1=54.868 Prec@5=78.807 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=19:03 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.930 Prec@1=54.868 Prec@5=78.807 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=19:04 IST=> training   43.99% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.931 Prec@1=54.837 Prec@5=78.789 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=19:04 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.931 Prec@1=54.837 Prec@5=78.789 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=19:04 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.235 Loss=1.931 Prec@1=54.837 Prec@5=78.789 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=19:04 IST=> training   47.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.933 Prec@1=54.816 Prec@5=78.775 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=19:04 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.933 Prec@1=54.816 Prec@5=78.775 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=19:04 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.933 Prec@1=54.816 Prec@5=78.775 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=19:05 IST=> training   51.98% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.934 Prec@1=54.794 Prec@5=78.753 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=19:05 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.934 Prec@1=54.794 Prec@5=78.753 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=19:05 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.934 Prec@1=54.794 Prec@5=78.753 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=19:06 IST=> training   55.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.934 Prec@1=54.802 Prec@5=78.767 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.934 Prec@1=54.802 Prec@5=78.767 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.934 Prec@1=54.802 Prec@5=78.767 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=19:06 IST=> training   59.97% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.935 Prec@1=54.790 Prec@5=78.746 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=19:06 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.935 Prec@1=54.790 Prec@5=78.746 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=19:06 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.935 Prec@1=54.790 Prec@5=78.746 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=19:07 IST=> training   63.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.936 Prec@1=54.761 Prec@5=78.721 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=19:07 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.936 Prec@1=54.761 Prec@5=78.721 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=19:07 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.340 DataTime=0.234 Loss=1.936 Prec@1=54.761 Prec@5=78.721 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=19:07 IST=> training   67.96% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.938 Prec@1=54.725 Prec@5=78.685 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=19:07 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.938 Prec@1=54.725 Prec@5=78.685 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=19:07 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.233 Loss=1.938 Prec@1=54.725 Prec@5=78.685 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=19:08 IST=> training   71.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.723 Prec@5=78.690 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=19:08 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.723 Prec@5=78.690 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=19:08 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.723 Prec@5=78.690 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=19:08 IST=> training   75.95% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.720 Prec@5=78.684 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=19:08 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.720 Prec@5=78.684 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=19:08 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.339 DataTime=0.232 Loss=1.939 Prec@1=54.720 Prec@5=78.684 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=19:09 IST=> training   79.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.232 Loss=1.940 Prec@1=54.701 Prec@5=78.670 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=19:09 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.232 Loss=1.940 Prec@1=54.701 Prec@5=78.670 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:09 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.232 Loss=1.940 Prec@1=54.701 Prec@5=78.670 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:10 IST=> training   83.94% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.941 Prec@1=54.677 Prec@5=78.657 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:10 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.941 Prec@1=54.677 Prec@5=78.657 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:10 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.941 Prec@1=54.677 Prec@5=78.657 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:10 IST=> training   87.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.942 Prec@1=54.662 Prec@5=78.643 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:10 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.942 Prec@1=54.662 Prec@5=78.643 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:10 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.942 Prec@1=54.662 Prec@5=78.643 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:11 IST=> training   91.93% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.943 Prec@1=54.647 Prec@5=78.637 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:11 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.943 Prec@1=54.647 Prec@5=78.637 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=19:11 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.338 DataTime=0.231 Loss=1.943 Prec@1=54.647 Prec@5=78.637 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=19:11 IST=> training   95.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.337 DataTime=0.230 Loss=1.944 Prec@1=54.629 Prec@5=78.628 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=19:11 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.337 DataTime=0.230 Loss=1.944 Prec@1=54.629 Prec@5=78.628 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=19:11 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.337 DataTime=0.230 Loss=1.944 Prec@1=54.629 Prec@5=78.628 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=19:11 IST=> training   99.92% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.337 DataTime=0.230 Loss=1.944 Prec@1=54.630 Prec@5=78.627 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=19:11 IST=> training   100.00% of 1x2503...Epoch=81/150 LR=0.04477 Time=0.337 DataTime=0.230 Loss=1.944 Prec@1=54.630 Prec@5=78.627 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=19:11 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:11 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:11 IST=> validation 0.00% of 1x98...Epoch=81/150 LR=0.04477 Time=6.333 Loss=1.999 Prec@1=52.344 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=19:11 IST=> validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=6.333 Loss=1.999 Prec@1=52.344 Prec@5=77.344 rate=6349.53 Hz, eta=0:00:00, total=0:00:00, wall=19:11 IST** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=6.333 Loss=1.999 Prec@1=52.344 Prec@5=77.344 rate=6349.53 Hz, eta=0:00:00, total=0:00:00, wall=19:12 IST** validation 1.02% of 1x98...Epoch=81/150 LR=0.04477 Time=0.391 Loss=2.078 Prec@1=51.768 Prec@5=76.708 rate=6349.53 Hz, eta=0:00:00, total=0:00:00, wall=19:12 IST** validation 100.00% of 1x98...Epoch=81/150 LR=0.04477 Time=0.391 Loss=2.078 Prec@1=51.768 Prec@5=76.708 rate=3.07 Hz, eta=0:00:00, total=0:00:31, wall=19:12 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:12 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:12 IST=> training   0.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.149 DataTime=4.915 Loss=1.848 Prec@1=55.859 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=19:12 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.149 DataTime=4.915 Loss=1.848 Prec@1=55.859 Prec@5=81.445 rate=5915.27 Hz, eta=0:00:00, total=0:00:00, wall=19:12 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=5.149 DataTime=4.915 Loss=1.848 Prec@1=55.859 Prec@5=81.445 rate=5915.27 Hz, eta=0:00:00, total=0:00:00, wall=19:13 IST=> training   0.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.375 DataTime=0.280 Loss=1.904 Prec@1=55.388 Prec@5=79.127 rate=5915.27 Hz, eta=0:00:00, total=0:00:00, wall=19:13 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.375 DataTime=0.280 Loss=1.904 Prec@1=55.388 Prec@5=79.127 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=19:13 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.375 DataTime=0.280 Loss=1.904 Prec@1=55.388 Prec@5=79.127 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=19:13 IST=> training   4.04% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.352 DataTime=0.255 Loss=1.916 Prec@1=55.105 Prec@5=78.989 rate=3.08 Hz, eta=0:12:59, total=0:00:32, wall=19:13 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.352 DataTime=0.255 Loss=1.916 Prec@1=55.105 Prec@5=78.989 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=19:13 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.352 DataTime=0.255 Loss=1.916 Prec@1=55.105 Prec@5=78.989 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=19:14 IST=> training   8.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.343 DataTime=0.246 Loss=1.918 Prec@1=55.071 Prec@5=78.954 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=19:14 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.343 DataTime=0.246 Loss=1.918 Prec@1=55.071 Prec@5=78.954 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=19:14 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.343 DataTime=0.246 Loss=1.918 Prec@1=55.071 Prec@5=78.954 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=19:14 IST=> training   12.03% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.345 DataTime=0.248 Loss=1.918 Prec@1=55.111 Prec@5=78.975 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=19:14 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.345 DataTime=0.248 Loss=1.918 Prec@1=55.111 Prec@5=78.975 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=19:14 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.345 DataTime=0.248 Loss=1.918 Prec@1=55.111 Prec@5=78.975 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=19:15 IST=> training   16.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.342 DataTime=0.244 Loss=1.915 Prec@1=55.116 Prec@5=79.027 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=19:15 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.342 DataTime=0.244 Loss=1.915 Prec@1=55.116 Prec@5=79.027 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:15 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.342 DataTime=0.244 Loss=1.915 Prec@1=55.116 Prec@5=79.027 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:15 IST=> training   20.02% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.239 Loss=1.917 Prec@1=55.098 Prec@5=79.011 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:15 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.239 Loss=1.917 Prec@1=55.098 Prec@5=79.011 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=19:15 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.239 Loss=1.917 Prec@1=55.098 Prec@5=79.011 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=19:16 IST=> training   24.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.240 Loss=1.916 Prec@1=55.072 Prec@5=79.022 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=19:16 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.240 Loss=1.916 Prec@1=55.072 Prec@5=79.022 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:16 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.339 DataTime=0.240 Loss=1.916 Prec@1=55.072 Prec@5=79.022 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:16 IST=> training   28.01% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.917 Prec@1=55.083 Prec@5=78.999 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:16 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.917 Prec@1=55.083 Prec@5=78.999 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=19:16 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.917 Prec@1=55.083 Prec@5=78.999 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=19:17 IST=> training   32.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.918 Prec@1=55.095 Prec@5=78.985 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=19:17 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.918 Prec@1=55.095 Prec@5=78.985 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=19:17 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.918 Prec@1=55.095 Prec@5=78.985 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=19:18 IST=> training   36.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.919 Prec@1=55.098 Prec@5=78.965 rate=3.01 Hz, eta=0:08:51, total=0:04:59, wall=19:18 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.919 Prec@1=55.098 Prec@5=78.965 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:18 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.238 Loss=1.919 Prec@1=55.098 Prec@5=78.965 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:18 IST=> training   39.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.236 Loss=1.922 Prec@1=55.049 Prec@5=78.929 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:18 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.236 Loss=1.922 Prec@1=55.049 Prec@5=78.929 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=19:18 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.236 Loss=1.922 Prec@1=55.049 Prec@5=78.929 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=19:19 IST=> training   43.99% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.924 Prec@1=55.018 Prec@5=78.909 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=19:19 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.924 Prec@1=55.018 Prec@5=78.909 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=19:19 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.924 Prec@1=55.018 Prec@5=78.909 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=19:19 IST=> training   47.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.926 Prec@1=54.974 Prec@5=78.884 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=19:19 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.926 Prec@1=54.974 Prec@5=78.884 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=19:19 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.236 Loss=1.926 Prec@1=54.974 Prec@5=78.884 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=19:20 IST=> training   51.98% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.928 Prec@1=54.942 Prec@5=78.862 rate=3.00 Hz, eta=0:06:41, total=0:07:14, wall=19:20 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.928 Prec@1=54.942 Prec@5=78.862 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=19:20 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.928 Prec@1=54.942 Prec@5=78.862 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=19:20 IST=> training   55.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.929 Prec@1=54.914 Prec@5=78.833 rate=3.00 Hz, eta=0:06:07, total=0:07:47, wall=19:20 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.929 Prec@1=54.914 Prec@5=78.833 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=19:20 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.929 Prec@1=54.914 Prec@5=78.833 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=19:21 IST=> training   59.97% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.888 Prec@5=78.824 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=19:21 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.888 Prec@5=78.824 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=19:21 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.888 Prec@5=78.824 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=19:21 IST=> training   63.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.881 Prec@5=78.801 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=19:21 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.881 Prec@5=78.801 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:21 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.235 Loss=1.930 Prec@1=54.881 Prec@5=78.801 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:22 IST=> training   67.96% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.932 Prec@1=54.841 Prec@5=78.786 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:22 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.932 Prec@1=54.841 Prec@5=78.786 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=19:22 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.235 Loss=1.932 Prec@1=54.841 Prec@5=78.786 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=19:23 IST=> training   71.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.933 Prec@1=54.829 Prec@5=78.771 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=19:23 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.933 Prec@1=54.829 Prec@5=78.771 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=19:23 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.933 Prec@1=54.829 Prec@5=78.771 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=19:23 IST=> training   75.95% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.830 Prec@5=78.773 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=19:23 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.830 Prec@5=78.773 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=19:23 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.830 Prec@5=78.773 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=19:24 IST=> training   79.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.818 Prec@5=78.771 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=19:24 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.818 Prec@5=78.771 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=19:24 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.234 Loss=1.933 Prec@1=54.818 Prec@5=78.771 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=19:24 IST=> training   83.94% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.934 Prec@1=54.799 Prec@5=78.774 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=19:24 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.934 Prec@1=54.799 Prec@5=78.774 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=19:24 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.338 DataTime=0.234 Loss=1.934 Prec@1=54.799 Prec@5=78.774 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=19:25 IST=> training   87.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.781 Prec@5=78.784 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=19:25 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.781 Prec@5=78.784 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=19:25 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.781 Prec@5=78.784 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=19:25 IST=> training   91.93% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.777 Prec@5=78.774 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=19:25 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.777 Prec@5=78.774 rate=2.99 Hz, eta=0:00:34, total=0:13:24, wall=19:25 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.233 Loss=1.935 Prec@1=54.777 Prec@5=78.774 rate=2.99 Hz, eta=0:00:34, total=0:13:24, wall=19:26 IST=> training   95.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.232 Loss=1.936 Prec@1=54.759 Prec@5=78.772 rate=2.99 Hz, eta=0:00:34, total=0:13:24, wall=19:26 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.232 Loss=1.936 Prec@1=54.759 Prec@5=78.772 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=19:26 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.337 DataTime=0.232 Loss=1.936 Prec@1=54.759 Prec@5=78.772 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=19:26 IST=> training   99.92% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.336 DataTime=0.232 Loss=1.936 Prec@1=54.759 Prec@5=78.772 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=19:26 IST=> training   100.00% of 1x2503...Epoch=82/150 LR=0.04373 Time=0.336 DataTime=0.232 Loss=1.936 Prec@1=54.759 Prec@5=78.772 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=19:26 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:26 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:26 IST=> validation 0.00% of 1x98...Epoch=82/150 LR=0.04373 Time=7.446 Loss=2.042 Prec@1=53.906 Prec@5=76.758 rate=0 Hz, eta=?, total=0:00:00, wall=19:26 IST=> validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=7.446 Loss=2.042 Prec@1=53.906 Prec@5=76.758 rate=5914.47 Hz, eta=0:00:00, total=0:00:00, wall=19:26 IST** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=7.446 Loss=2.042 Prec@1=53.906 Prec@5=76.758 rate=5914.47 Hz, eta=0:00:00, total=0:00:00, wall=19:27 IST** validation 1.02% of 1x98...Epoch=82/150 LR=0.04373 Time=0.400 Loss=1.977 Prec@1=53.852 Prec@5=78.276 rate=5914.47 Hz, eta=0:00:00, total=0:00:00, wall=19:27 IST** validation 100.00% of 1x98...Epoch=82/150 LR=0.04373 Time=0.400 Loss=1.977 Prec@1=53.852 Prec@5=78.276 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=19:27 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:27 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:27 IST=> training   0.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.310 DataTime=5.158 Loss=1.810 Prec@1=57.812 Prec@5=79.688 rate=0 Hz, eta=?, total=0:00:00, wall=19:27 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.310 DataTime=5.158 Loss=1.810 Prec@1=57.812 Prec@5=79.688 rate=2687.44 Hz, eta=0:00:00, total=0:00:00, wall=19:27 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=5.310 DataTime=5.158 Loss=1.810 Prec@1=57.812 Prec@5=79.688 rate=2687.44 Hz, eta=0:00:00, total=0:00:00, wall=19:27 IST=> training   0.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.380 DataTime=0.281 Loss=1.898 Prec@1=55.465 Prec@5=79.368 rate=2687.44 Hz, eta=0:00:00, total=0:00:00, wall=19:27 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.380 DataTime=0.281 Loss=1.898 Prec@1=55.465 Prec@5=79.368 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=19:27 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.380 DataTime=0.281 Loss=1.898 Prec@1=55.465 Prec@5=79.368 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=19:28 IST=> training   4.04% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.356 DataTime=0.255 Loss=1.911 Prec@1=55.258 Prec@5=79.116 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=19:28 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.356 DataTime=0.255 Loss=1.911 Prec@1=55.258 Prec@5=79.116 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=19:28 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.356 DataTime=0.255 Loss=1.911 Prec@1=55.258 Prec@5=79.116 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=19:28 IST=> training   8.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.348 DataTime=0.245 Loss=1.910 Prec@1=55.314 Prec@5=79.153 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=19:28 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.348 DataTime=0.245 Loss=1.910 Prec@1=55.314 Prec@5=79.153 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=19:28 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.348 DataTime=0.245 Loss=1.910 Prec@1=55.314 Prec@5=79.153 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=19:29 IST=> training   12.03% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.913 Prec@1=55.259 Prec@5=79.069 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=19:29 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.913 Prec@1=55.259 Prec@5=79.069 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=19:29 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.913 Prec@1=55.259 Prec@5=79.069 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=19:29 IST=> training   16.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.914 Prec@1=55.205 Prec@5=79.096 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=19:29 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.914 Prec@1=55.205 Prec@5=79.096 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:29 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.343 DataTime=0.239 Loss=1.914 Prec@1=55.205 Prec@5=79.096 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:30 IST=> training   20.02% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.342 DataTime=0.238 Loss=1.916 Prec@1=55.206 Prec@5=79.068 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=19:30 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.342 DataTime=0.238 Loss=1.916 Prec@1=55.206 Prec@5=79.068 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=19:30 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.342 DataTime=0.238 Loss=1.916 Prec@1=55.206 Prec@5=79.068 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=19:31 IST=> training   24.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.918 Prec@1=55.200 Prec@5=79.035 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=19:31 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.918 Prec@1=55.200 Prec@5=79.035 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=19:31 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.918 Prec@1=55.200 Prec@5=79.035 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=19:31 IST=> training   28.01% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.919 Prec@1=55.150 Prec@5=78.989 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=19:31 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.919 Prec@1=55.150 Prec@5=78.989 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=19:31 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.341 DataTime=0.236 Loss=1.919 Prec@1=55.150 Prec@5=78.989 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=19:32 IST=> training   32.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.922 Prec@1=55.065 Prec@5=78.917 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=19:32 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.922 Prec@1=55.065 Prec@5=78.917 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=19:32 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.922 Prec@1=55.065 Prec@5=78.917 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=19:32 IST=> training   36.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.924 Prec@1=55.040 Prec@5=78.876 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=19:32 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.924 Prec@1=55.040 Prec@5=78.876 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:32 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.233 Loss=1.924 Prec@1=55.040 Prec@5=78.876 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:33 IST=> training   39.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.340 DataTime=0.233 Loss=1.924 Prec@1=55.024 Prec@5=78.873 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=19:33 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.340 DataTime=0.233 Loss=1.924 Prec@1=55.024 Prec@5=78.873 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=19:33 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.340 DataTime=0.233 Loss=1.924 Prec@1=55.024 Prec@5=78.873 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=19:33 IST=> training   43.99% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.030 Prec@5=78.877 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=19:33 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.030 Prec@5=78.877 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:33 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.030 Prec@5=78.877 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:34 IST=> training   47.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.925 Prec@1=55.024 Prec@5=78.871 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:34 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.925 Prec@1=55.024 Prec@5=78.871 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=19:34 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.925 Prec@1=55.024 Prec@5=78.871 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=19:35 IST=> training   51.98% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.019 Prec@5=78.871 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=19:35 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.019 Prec@5=78.871 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=19:35 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.019 Prec@5=78.871 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=19:35 IST=> training   55.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.007 Prec@5=78.872 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=19:35 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.007 Prec@5=78.872 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=19:35 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=55.007 Prec@5=78.872 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=19:36 IST=> training   59.97% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.926 Prec@1=54.975 Prec@5=78.878 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=19:36 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.926 Prec@1=54.975 Prec@5=78.878 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=19:36 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.926 Prec@1=54.975 Prec@5=78.878 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=19:36 IST=> training   63.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.966 Prec@5=78.870 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=19:36 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.966 Prec@5=78.870 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=19:36 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.966 Prec@5=78.870 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=19:37 IST=> training   67.96% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.960 Prec@5=78.870 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=19:37 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.960 Prec@5=78.870 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=19:37 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.926 Prec@1=54.960 Prec@5=78.870 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=19:37 IST=> training   71.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.927 Prec@1=54.933 Prec@5=78.860 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=19:37 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.927 Prec@1=54.933 Prec@5=78.860 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:37 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.927 Prec@1=54.933 Prec@5=78.860 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:38 IST=> training   75.95% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.927 Prec@1=54.937 Prec@5=78.854 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.927 Prec@1=54.937 Prec@5=78.854 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.927 Prec@1=54.937 Prec@5=78.854 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=19:38 IST=> training   79.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.928 Prec@1=54.926 Prec@5=78.847 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=19:38 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.928 Prec@1=54.926 Prec@5=78.847 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:38 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.928 Prec@1=54.926 Prec@5=78.847 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:39 IST=> training   83.94% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.230 Loss=1.929 Prec@1=54.901 Prec@5=78.836 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=19:39 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.230 Loss=1.929 Prec@1=54.901 Prec@5=78.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:39 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.230 Loss=1.929 Prec@1=54.901 Prec@5=78.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:40 IST=> training   87.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.929 Prec@1=54.883 Prec@5=78.827 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=19:40 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.929 Prec@1=54.883 Prec@5=78.827 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=19:40 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.929 Prec@1=54.883 Prec@5=78.827 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=19:40 IST=> training   91.93% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.930 Prec@1=54.878 Prec@5=78.821 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=19:40 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.930 Prec@1=54.878 Prec@5=78.821 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:40 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.339 DataTime=0.231 Loss=1.930 Prec@1=54.878 Prec@5=78.821 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:41 IST=> training   95.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.930 Prec@1=54.872 Prec@5=78.814 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:41 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.930 Prec@1=54.872 Prec@5=78.814 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:41 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.930 Prec@1=54.872 Prec@5=78.814 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:41 IST=> training   99.92% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.930 Prec@1=54.871 Prec@5=78.814 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:41 IST=> training   100.00% of 1x2503...Epoch=83/150 LR=0.04270 Time=0.338 DataTime=0.230 Loss=1.930 Prec@1=54.871 Prec@5=78.814 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=19:41 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> validation 0.00% of 1x98...Epoch=83/150 LR=0.04270 Time=6.547 Loss=2.011 Prec@1=53.320 Prec@5=75.586 rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=6.547 Loss=2.011 Prec@1=53.320 Prec@5=75.586 rate=5990.93 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=6.547 Loss=2.011 Prec@1=53.320 Prec@5=75.586 rate=5990.93 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST** validation 1.02% of 1x98...Epoch=83/150 LR=0.04270 Time=0.394 Loss=2.046 Prec@1=52.600 Prec@5=77.252 rate=5990.93 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST** validation 100.00% of 1x98...Epoch=83/150 LR=0.04270 Time=0.394 Loss=2.046 Prec@1=52.600 Prec@5=77.252 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=19:41 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> training   0.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.270 DataTime=5.028 Loss=1.882 Prec@1=56.836 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=19:41 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.270 DataTime=5.028 Loss=1.882 Prec@1=56.836 Prec@5=79.883 rate=3793.96 Hz, eta=0:00:00, total=0:00:00, wall=19:41 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=5.270 DataTime=5.028 Loss=1.882 Prec@1=56.836 Prec@5=79.883 rate=3793.96 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST=> training   0.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.373 DataTime=0.273 Loss=1.900 Prec@1=55.552 Prec@5=79.191 rate=3793.96 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.373 DataTime=0.273 Loss=1.900 Prec@1=55.552 Prec@5=79.191 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=19:42 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.373 DataTime=0.273 Loss=1.900 Prec@1=55.552 Prec@5=79.191 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=19:43 IST=> training   4.04% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.348 DataTime=0.248 Loss=1.907 Prec@1=55.468 Prec@5=79.207 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=19:43 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.348 DataTime=0.248 Loss=1.907 Prec@1=55.468 Prec@5=79.207 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=19:43 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.348 DataTime=0.248 Loss=1.907 Prec@1=55.468 Prec@5=79.207 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=19:43 IST=> training   8.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.244 Loss=1.907 Prec@1=55.426 Prec@5=79.146 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=19:43 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.244 Loss=1.907 Prec@1=55.426 Prec@5=79.146 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=19:43 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.244 Loss=1.907 Prec@1=55.426 Prec@5=79.146 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=19:44 IST=> training   12.03% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.243 Loss=1.906 Prec@1=55.386 Prec@5=79.151 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=19:44 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.243 Loss=1.906 Prec@1=55.386 Prec@5=79.151 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=19:44 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.346 DataTime=0.243 Loss=1.906 Prec@1=55.386 Prec@5=79.151 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=19:44 IST=> training   16.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.376 Prec@5=79.120 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=19:44 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.376 Prec@5=79.120 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=19:44 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.376 Prec@5=79.120 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=19:45 IST=> training   20.02% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.235 Loss=1.907 Prec@1=55.378 Prec@5=79.142 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=19:45 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.235 Loss=1.907 Prec@1=55.378 Prec@5=79.142 rate=3.02 Hz, eta=0:10:29, total=0:03:19, wall=19:45 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.235 Loss=1.907 Prec@1=55.378 Prec@5=79.142 rate=3.02 Hz, eta=0:10:29, total=0:03:19, wall=19:45 IST=> training   24.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.234 Loss=1.909 Prec@1=55.315 Prec@5=79.134 rate=3.02 Hz, eta=0:10:29, total=0:03:19, wall=19:45 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.234 Loss=1.909 Prec@1=55.315 Prec@5=79.134 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:45 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.340 DataTime=0.234 Loss=1.909 Prec@1=55.315 Prec@5=79.134 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:46 IST=> training   28.01% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.911 Prec@1=55.284 Prec@5=79.082 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=19:46 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.911 Prec@1=55.284 Prec@5=79.082 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=19:46 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.911 Prec@1=55.284 Prec@5=79.082 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=19:46 IST=> training   32.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.235 Loss=1.911 Prec@1=55.266 Prec@5=79.087 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=19:46 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.235 Loss=1.911 Prec@1=55.266 Prec@5=79.087 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=19:46 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.235 Loss=1.911 Prec@1=55.266 Prec@5=79.087 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=19:47 IST=> training   36.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.913 Prec@1=55.249 Prec@5=79.084 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=19:47 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.913 Prec@1=55.249 Prec@5=79.084 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=19:47 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.913 Prec@1=55.249 Prec@5=79.084 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=19:48 IST=> training   39.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.914 Prec@1=55.227 Prec@5=79.056 rate=3.00 Hz, eta=0:08:21, total=0:05:33, wall=19:48 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.914 Prec@1=55.227 Prec@5=79.056 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=19:48 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.233 Loss=1.914 Prec@1=55.227 Prec@5=79.056 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=19:48 IST=> training   43.99% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.915 Prec@1=55.212 Prec@5=79.065 rate=3.00 Hz, eta=0:07:47, total=0:06:06, wall=19:48 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.915 Prec@1=55.212 Prec@5=79.065 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:48 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.234 Loss=1.915 Prec@1=55.212 Prec@5=79.065 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:49 IST=> training   47.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.915 Prec@1=55.200 Prec@5=79.060 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=19:49 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.915 Prec@1=55.200 Prec@5=79.060 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=19:49 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.915 Prec@1=55.200 Prec@5=79.060 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=19:49 IST=> training   51.98% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.918 Prec@1=55.173 Prec@5=79.029 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=19:49 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.918 Prec@1=55.173 Prec@5=79.029 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=19:49 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.918 Prec@1=55.173 Prec@5=79.029 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=19:50 IST=> training   55.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.919 Prec@1=55.130 Prec@5=79.023 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=19:50 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.919 Prec@1=55.130 Prec@5=79.023 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=19:50 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.919 Prec@1=55.130 Prec@5=79.023 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=19:50 IST=> training   59.97% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.919 Prec@1=55.110 Prec@5=79.031 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=19:50 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.919 Prec@1=55.110 Prec@5=79.031 rate=2.99 Hz, eta=0:05:02, total=0:08:56, wall=19:50 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.919 Prec@1=55.110 Prec@5=79.031 rate=2.99 Hz, eta=0:05:02, total=0:08:56, wall=19:51 IST=> training   63.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.919 Prec@1=55.105 Prec@5=79.023 rate=2.99 Hz, eta=0:05:02, total=0:08:56, wall=19:51 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.919 Prec@1=55.105 Prec@5=79.023 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:51 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.919 Prec@1=55.105 Prec@5=79.023 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:52 IST=> training   67.96% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.920 Prec@1=55.095 Prec@5=79.009 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=19:52 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.920 Prec@1=55.095 Prec@5=79.009 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=19:52 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.920 Prec@1=55.095 Prec@5=79.009 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=19:52 IST=> training   71.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.921 Prec@1=55.067 Prec@5=78.998 rate=2.98 Hz, eta=0:03:55, total=0:10:03, wall=19:52 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.921 Prec@1=55.067 Prec@5=78.998 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:52 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.921 Prec@1=55.067 Prec@5=78.998 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:53 IST=> training   75.95% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.922 Prec@1=55.044 Prec@5=78.977 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=19:53 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.922 Prec@1=55.044 Prec@5=78.977 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=19:53 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.232 Loss=1.922 Prec@1=55.044 Prec@5=78.977 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=19:53 IST=> training   79.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.923 Prec@1=55.014 Prec@5=78.958 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=19:53 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.923 Prec@1=55.014 Prec@5=78.958 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=19:53 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.923 Prec@1=55.014 Prec@5=78.958 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=19:54 IST=> training   83.94% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.008 Prec@5=78.956 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=19:54 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.008 Prec@5=78.956 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=19:54 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=55.008 Prec@5=78.956 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=19:54 IST=> training   87.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=54.993 Prec@5=78.946 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=19:54 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=54.993 Prec@5=78.946 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:54 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.924 Prec@1=54.993 Prec@5=78.946 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:55 IST=> training   91.93% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=54.989 Prec@5=78.939 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=19:55 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=54.989 Prec@5=78.939 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:55 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.339 DataTime=0.232 Loss=1.925 Prec@1=54.989 Prec@5=78.939 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:55 IST=> training   95.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.925 Prec@1=54.978 Prec@5=78.925 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=19:55 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.925 Prec@1=54.978 Prec@5=78.925 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:55 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.925 Prec@1=54.978 Prec@5=78.925 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:55 IST=> training   99.92% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.925 Prec@1=54.977 Prec@5=78.925 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:55 IST=> training   100.00% of 1x2503...Epoch=84/150 LR=0.04166 Time=0.338 DataTime=0.231 Loss=1.925 Prec@1=54.977 Prec@5=78.925 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=19:55 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 0.00% of 1x98...Epoch=84/150 LR=0.04166 Time=6.614 Loss=2.086 Prec@1=51.367 Prec@5=75.781 rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=6.614 Loss=2.086 Prec@1=51.367 Prec@5=75.781 rate=2423.81 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=6.614 Loss=2.086 Prec@1=51.367 Prec@5=75.781 rate=2423.81 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 1.02% of 1x98...Epoch=84/150 LR=0.04166 Time=0.395 Loss=2.099 Prec@1=51.356 Prec@5=76.444 rate=2423.81 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST** validation 100.00% of 1x98...Epoch=84/150 LR=0.04166 Time=0.395 Loss=2.099 Prec@1=51.356 Prec@5=76.444 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=19:56 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.747 DataTime=5.579 Loss=1.834 Prec@1=60.547 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.747 DataTime=5.579 Loss=1.834 Prec@1=60.547 Prec@5=81.055 rate=7004.56 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=5.747 DataTime=5.579 Loss=1.834 Prec@1=60.547 Prec@5=81.055 rate=7004.56 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST=> training   0.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.384 DataTime=0.284 Loss=1.890 Prec@1=55.904 Prec@5=79.380 rate=7004.56 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.384 DataTime=0.284 Loss=1.890 Prec@1=55.904 Prec@5=79.380 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=19:57 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.384 DataTime=0.284 Loss=1.890 Prec@1=55.904 Prec@5=79.380 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=19:57 IST=> training   4.04% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.359 DataTime=0.258 Loss=1.890 Prec@1=55.665 Prec@5=79.519 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=19:57 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.359 DataTime=0.258 Loss=1.890 Prec@1=55.665 Prec@5=79.519 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=19:57 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.359 DataTime=0.258 Loss=1.890 Prec@1=55.665 Prec@5=79.519 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=19:58 IST=> training   8.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.354 DataTime=0.251 Loss=1.895 Prec@1=55.539 Prec@5=79.390 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=19:58 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.354 DataTime=0.251 Loss=1.895 Prec@1=55.539 Prec@5=79.390 rate=2.98 Hz, eta=0:12:17, total=0:01:40, wall=19:58 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.354 DataTime=0.251 Loss=1.895 Prec@1=55.539 Prec@5=79.390 rate=2.98 Hz, eta=0:12:17, total=0:01:40, wall=19:58 IST=> training   12.03% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.348 DataTime=0.244 Loss=1.894 Prec@1=55.597 Prec@5=79.399 rate=2.98 Hz, eta=0:12:17, total=0:01:40, wall=19:58 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.348 DataTime=0.244 Loss=1.894 Prec@1=55.597 Prec@5=79.399 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=19:58 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.348 DataTime=0.244 Loss=1.894 Prec@1=55.597 Prec@5=79.399 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=19:59 IST=> training   16.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.346 DataTime=0.242 Loss=1.896 Prec@1=55.552 Prec@5=79.341 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=19:59 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.346 DataTime=0.242 Loss=1.896 Prec@1=55.552 Prec@5=79.341 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=19:59 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.346 DataTime=0.242 Loss=1.896 Prec@1=55.552 Prec@5=79.341 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=20:00 IST=> training   20.02% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.345 DataTime=0.241 Loss=1.899 Prec@1=55.541 Prec@5=79.316 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=20:00 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.345 DataTime=0.241 Loss=1.899 Prec@1=55.541 Prec@5=79.316 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=20:00 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.345 DataTime=0.241 Loss=1.899 Prec@1=55.541 Prec@5=79.316 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=20:00 IST=> training   24.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.344 DataTime=0.241 Loss=1.900 Prec@1=55.518 Prec@5=79.324 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=20:00 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.344 DataTime=0.241 Loss=1.900 Prec@1=55.518 Prec@5=79.324 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=20:00 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.344 DataTime=0.241 Loss=1.900 Prec@1=55.518 Prec@5=79.324 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=20:01 IST=> training   28.01% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.240 Loss=1.902 Prec@1=55.481 Prec@5=79.293 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=20:01 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.240 Loss=1.902 Prec@1=55.481 Prec@5=79.293 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=20:01 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.240 Loss=1.902 Prec@1=55.481 Prec@5=79.293 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=20:01 IST=> training   32.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.239 Loss=1.903 Prec@1=55.436 Prec@5=79.242 rate=2.98 Hz, eta=0:09:32, total=0:04:29, wall=20:01 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.239 Loss=1.903 Prec@1=55.436 Prec@5=79.242 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=20:01 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.239 Loss=1.903 Prec@1=55.436 Prec@5=79.242 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=20:02 IST=> training   36.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.239 Loss=1.905 Prec@1=55.385 Prec@5=79.205 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=20:02 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.239 Loss=1.905 Prec@1=55.385 Prec@5=79.205 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=20:02 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.343 DataTime=0.239 Loss=1.905 Prec@1=55.385 Prec@5=79.205 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=20:02 IST=> training   39.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.360 Prec@5=79.183 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=20:02 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.360 Prec@5=79.183 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=20:02 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.342 DataTime=0.238 Loss=1.907 Prec@1=55.360 Prec@5=79.183 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=20:03 IST=> training   43.99% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.237 Loss=1.908 Prec@1=55.338 Prec@5=79.156 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=20:03 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.237 Loss=1.908 Prec@1=55.338 Prec@5=79.156 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=20:03 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.237 Loss=1.908 Prec@1=55.338 Prec@5=79.156 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=20:04 IST=> training   47.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.910 Prec@1=55.294 Prec@5=79.124 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=20:04 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.910 Prec@1=55.294 Prec@5=79.124 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=20:04 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.910 Prec@1=55.294 Prec@5=79.124 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=20:04 IST=> training   51.98% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.911 Prec@1=55.286 Prec@5=79.123 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=20:04 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.911 Prec@1=55.286 Prec@5=79.123 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=20:04 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.236 Loss=1.911 Prec@1=55.286 Prec@5=79.123 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=20:05 IST=> training   55.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.911 Prec@1=55.267 Prec@5=79.119 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=20:05 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.911 Prec@1=55.267 Prec@5=79.119 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:05 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.911 Prec@1=55.267 Prec@5=79.119 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:05 IST=> training   59.97% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.255 Prec@5=79.109 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:05 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.255 Prec@5=79.109 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:05 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.255 Prec@5=79.109 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:06 IST=> training   63.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.238 Prec@5=79.100 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:06 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.238 Prec@5=79.100 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=20:06 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.912 Prec@1=55.238 Prec@5=79.100 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=20:06 IST=> training   67.96% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.913 Prec@1=55.239 Prec@5=79.084 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=20:06 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.913 Prec@1=55.239 Prec@5=79.084 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=20:06 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.235 Loss=1.913 Prec@1=55.239 Prec@5=79.084 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=20:07 IST=> training   71.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.913 Prec@1=55.220 Prec@5=79.089 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=20:07 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.913 Prec@1=55.220 Prec@5=79.089 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:07 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.913 Prec@1=55.220 Prec@5=79.089 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:08 IST=> training   75.95% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.915 Prec@1=55.197 Prec@5=79.072 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:08 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.915 Prec@1=55.197 Prec@5=79.072 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:08 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.915 Prec@1=55.197 Prec@5=79.072 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:08 IST=> training   79.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.915 Prec@1=55.187 Prec@5=79.054 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:08 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.915 Prec@1=55.187 Prec@5=79.054 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=20:08 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.341 DataTime=0.235 Loss=1.915 Prec@1=55.187 Prec@5=79.054 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=20:09 IST=> training   83.94% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.916 Prec@1=55.179 Prec@5=79.051 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=20:09 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.916 Prec@1=55.179 Prec@5=79.051 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:09 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.916 Prec@1=55.179 Prec@5=79.051 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:09 IST=> training   87.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.169 Prec@5=79.038 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:09 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.169 Prec@5=79.038 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=20:09 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.169 Prec@5=79.038 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=20:10 IST=> training   91.93% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.150 Prec@5=79.035 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=20:10 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.150 Prec@5=79.035 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=20:10 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.340 DataTime=0.234 Loss=1.917 Prec@1=55.150 Prec@5=79.035 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=20:10 IST=> training   95.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.339 DataTime=0.233 Loss=1.917 Prec@1=55.144 Prec@5=79.031 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=20:10 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.339 DataTime=0.233 Loss=1.917 Prec@1=55.144 Prec@5=79.031 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=20:10 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.339 DataTime=0.233 Loss=1.917 Prec@1=55.144 Prec@5=79.031 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=20:10 IST=> training   99.92% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.339 DataTime=0.233 Loss=1.917 Prec@1=55.143 Prec@5=79.031 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=20:10 IST=> training   100.00% of 1x2503...Epoch=85/150 LR=0.04063 Time=0.339 DataTime=0.233 Loss=1.917 Prec@1=55.143 Prec@5=79.031 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=20:10 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 0.00% of 1x98...Epoch=85/150 LR=0.04063 Time=6.725 Loss=2.225 Prec@1=48.242 Prec@5=75.977 rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST=> validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.725 Loss=2.225 Prec@1=48.242 Prec@5=75.977 rate=4332.60 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=6.725 Loss=2.225 Prec@1=48.242 Prec@5=75.977 rate=4332.60 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST** validation 1.02% of 1x98...Epoch=85/150 LR=0.04063 Time=0.401 Loss=2.224 Prec@1=49.118 Prec@5=74.142 rate=4332.60 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST** validation 100.00% of 1x98...Epoch=85/150 LR=0.04063 Time=0.401 Loss=2.224 Prec@1=49.118 Prec@5=74.142 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=20:11 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST=> training   0.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.879 DataTime=5.742 Loss=1.845 Prec@1=57.031 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.879 DataTime=5.742 Loss=1.845 Prec@1=57.031 Prec@5=79.883 rate=5608.15 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=5.879 DataTime=5.742 Loss=1.845 Prec@1=57.031 Prec@5=79.883 rate=5608.15 Hz, eta=0:00:00, total=0:00:00, wall=20:12 IST=> training   0.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.382 DataTime=0.288 Loss=1.868 Prec@1=56.217 Prec@5=79.747 rate=5608.15 Hz, eta=0:00:00, total=0:00:00, wall=20:12 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.382 DataTime=0.288 Loss=1.868 Prec@1=56.217 Prec@5=79.747 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=20:12 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.382 DataTime=0.288 Loss=1.868 Prec@1=56.217 Prec@5=79.747 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=20:12 IST=> training   4.04% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.357 DataTime=0.258 Loss=1.870 Prec@1=56.221 Prec@5=79.668 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=20:12 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.357 DataTime=0.258 Loss=1.870 Prec@1=56.221 Prec@5=79.668 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=20:12 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.357 DataTime=0.258 Loss=1.870 Prec@1=56.221 Prec@5=79.668 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=20:13 IST=> training   8.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.356 DataTime=0.254 Loss=1.877 Prec@1=56.037 Prec@5=79.541 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.356 DataTime=0.254 Loss=1.877 Prec@1=56.037 Prec@5=79.541 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.356 DataTime=0.254 Loss=1.877 Prec@1=56.037 Prec@5=79.541 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=20:13 IST=> training   12.03% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.347 DataTime=0.244 Loss=1.885 Prec@1=55.864 Prec@5=79.440 rate=2.98 Hz, eta=0:12:20, total=0:01:41, wall=20:13 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.347 DataTime=0.244 Loss=1.885 Prec@1=55.864 Prec@5=79.440 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:13 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.347 DataTime=0.244 Loss=1.885 Prec@1=55.864 Prec@5=79.440 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:14 IST=> training   16.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.240 Loss=1.887 Prec@1=55.804 Prec@5=79.413 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:14 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.240 Loss=1.887 Prec@1=55.804 Prec@5=79.413 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=20:14 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.240 Loss=1.887 Prec@1=55.804 Prec@5=79.413 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=20:14 IST=> training   20.02% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.342 DataTime=0.238 Loss=1.891 Prec@1=55.678 Prec@5=79.405 rate=3.02 Hz, eta=0:11:03, total=0:02:45, wall=20:14 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.342 DataTime=0.238 Loss=1.891 Prec@1=55.678 Prec@5=79.405 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=20:14 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.342 DataTime=0.238 Loss=1.891 Prec@1=55.678 Prec@5=79.405 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=20:15 IST=> training   24.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.239 Loss=1.893 Prec@1=55.641 Prec@5=79.374 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=20:15 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.239 Loss=1.893 Prec@1=55.641 Prec@5=79.374 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=20:15 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.239 Loss=1.893 Prec@1=55.641 Prec@5=79.374 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=20:16 IST=> training   28.01% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.237 Loss=1.896 Prec@1=55.611 Prec@5=79.340 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=20:16 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.237 Loss=1.896 Prec@1=55.611 Prec@5=79.340 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=20:16 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.343 DataTime=0.237 Loss=1.896 Prec@1=55.611 Prec@5=79.340 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=20:16 IST=> training   32.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.897 Prec@1=55.574 Prec@5=79.320 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=20:16 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.897 Prec@1=55.574 Prec@5=79.320 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=20:16 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.897 Prec@1=55.574 Prec@5=79.320 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=20:17 IST=> training   36.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.899 Prec@1=55.528 Prec@5=79.274 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=20:17 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.899 Prec@1=55.528 Prec@5=79.274 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=20:17 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.899 Prec@1=55.528 Prec@5=79.274 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=20:17 IST=> training   39.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.899 Prec@1=55.528 Prec@5=79.293 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=20:17 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.899 Prec@1=55.528 Prec@5=79.293 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=20:17 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.899 Prec@1=55.528 Prec@5=79.293 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=20:18 IST=> training   43.99% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.901 Prec@1=55.468 Prec@5=79.272 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=20:18 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.901 Prec@1=55.468 Prec@5=79.272 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=20:18 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.901 Prec@1=55.468 Prec@5=79.272 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=20:18 IST=> training   47.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.901 Prec@1=55.463 Prec@5=79.259 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=20:18 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.901 Prec@1=55.463 Prec@5=79.259 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=20:18 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.236 Loss=1.901 Prec@1=55.463 Prec@5=79.259 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=20:19 IST=> training   51.98% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.902 Prec@1=55.467 Prec@5=79.252 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=20:19 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.902 Prec@1=55.467 Prec@5=79.252 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=20:19 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.341 DataTime=0.235 Loss=1.902 Prec@1=55.467 Prec@5=79.252 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=20:20 IST=> training   55.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.903 Prec@1=55.442 Prec@5=79.236 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=20:20 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.903 Prec@1=55.442 Prec@5=79.236 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:20 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.903 Prec@1=55.442 Prec@5=79.236 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:20 IST=> training   59.97% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.904 Prec@1=55.419 Prec@5=79.211 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=20:20 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.904 Prec@1=55.419 Prec@5=79.211 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:20 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.904 Prec@1=55.419 Prec@5=79.211 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:21 IST=> training   63.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.905 Prec@1=55.402 Prec@5=79.210 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=20:21 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.905 Prec@1=55.402 Prec@5=79.210 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=20:21 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.905 Prec@1=55.402 Prec@5=79.210 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=20:21 IST=> training   67.96% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.906 Prec@1=55.363 Prec@5=79.195 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=20:21 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.906 Prec@1=55.363 Prec@5=79.195 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=20:21 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.906 Prec@1=55.363 Prec@5=79.195 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=20:22 IST=> training   71.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.908 Prec@1=55.352 Prec@5=79.173 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=20:22 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.908 Prec@1=55.352 Prec@5=79.173 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:22 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.234 Loss=1.908 Prec@1=55.352 Prec@5=79.173 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:22 IST=> training   75.95% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.908 Prec@1=55.359 Prec@5=79.162 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=20:22 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.908 Prec@1=55.359 Prec@5=79.162 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:22 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.908 Prec@1=55.359 Prec@5=79.162 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:23 IST=> training   79.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.909 Prec@1=55.335 Prec@5=79.150 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=20:23 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.909 Prec@1=55.335 Prec@5=79.150 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=20:23 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.909 Prec@1=55.335 Prec@5=79.150 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=20:23 IST=> training   83.94% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.910 Prec@1=55.305 Prec@5=79.131 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=20:23 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.910 Prec@1=55.305 Prec@5=79.131 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:23 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.340 DataTime=0.233 Loss=1.910 Prec@1=55.305 Prec@5=79.131 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:24 IST=> training   87.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.911 Prec@1=55.291 Prec@5=79.107 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=20:24 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.911 Prec@1=55.291 Prec@5=79.107 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=20:24 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.232 Loss=1.911 Prec@1=55.291 Prec@5=79.107 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=20:25 IST=> training   91.93% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.292 Prec@5=79.102 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=20:25 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.292 Prec@5=79.102 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=20:25 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.292 Prec@5=79.102 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=20:25 IST=> training   95.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.271 Prec@5=79.090 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=20:25 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.271 Prec@5=79.090 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=20:25 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.271 Prec@5=79.090 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=20:25 IST=> training   99.92% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.270 Prec@5=79.091 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=20:25 IST=> training   100.00% of 1x2503...Epoch=86/150 LR=0.03960 Time=0.339 DataTime=0.231 Loss=1.912 Prec@1=55.270 Prec@5=79.091 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=20:25 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:25 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:25 IST=> validation 0.00% of 1x98...Epoch=86/150 LR=0.03960 Time=6.811 Loss=2.092 Prec@1=49.805 Prec@5=74.805 rate=0 Hz, eta=?, total=0:00:00, wall=20:25 IST=> validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=6.811 Loss=2.092 Prec@1=49.805 Prec@5=74.805 rate=6316.80 Hz, eta=0:00:00, total=0:00:00, wall=20:25 IST** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=6.811 Loss=2.092 Prec@1=49.805 Prec@5=74.805 rate=6316.80 Hz, eta=0:00:00, total=0:00:00, wall=20:26 IST** validation 1.02% of 1x98...Epoch=86/150 LR=0.03960 Time=0.401 Loss=2.029 Prec@1=52.820 Prec@5=77.562 rate=6316.80 Hz, eta=0:00:00, total=0:00:00, wall=20:26 IST** validation 100.00% of 1x98...Epoch=86/150 LR=0.03960 Time=0.401 Loss=2.029 Prec@1=52.820 Prec@5=77.562 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=20:26 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:26 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:26 IST=> training   0.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=4.570 DataTime=4.448 Loss=2.050 Prec@1=51.562 Prec@5=76.953 rate=0 Hz, eta=?, total=0:00:00, wall=20:26 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=4.570 DataTime=4.448 Loss=2.050 Prec@1=51.562 Prec@5=76.953 rate=3716.74 Hz, eta=0:00:00, total=0:00:00, wall=20:26 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=4.570 DataTime=4.448 Loss=2.050 Prec@1=51.562 Prec@5=76.953 rate=3716.74 Hz, eta=0:00:00, total=0:00:00, wall=20:26 IST=> training   0.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.373 DataTime=0.276 Loss=1.890 Prec@1=55.666 Prec@5=79.496 rate=3716.74 Hz, eta=0:00:00, total=0:00:00, wall=20:26 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.373 DataTime=0.276 Loss=1.890 Prec@1=55.666 Prec@5=79.496 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=20:26 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.373 DataTime=0.276 Loss=1.890 Prec@1=55.666 Prec@5=79.496 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=20:27 IST=> training   4.04% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.355 DataTime=0.257 Loss=1.894 Prec@1=55.512 Prec@5=79.398 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=20:27 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.355 DataTime=0.257 Loss=1.894 Prec@1=55.512 Prec@5=79.398 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=20:27 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.355 DataTime=0.257 Loss=1.894 Prec@1=55.512 Prec@5=79.398 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=20:28 IST=> training   8.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.347 DataTime=0.248 Loss=1.893 Prec@1=55.517 Prec@5=79.373 rate=3.01 Hz, eta=0:12:45, total=0:01:06, wall=20:28 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.347 DataTime=0.248 Loss=1.893 Prec@1=55.517 Prec@5=79.373 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=20:28 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.347 DataTime=0.248 Loss=1.893 Prec@1=55.517 Prec@5=79.373 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=20:28 IST=> training   12.03% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.343 DataTime=0.245 Loss=1.893 Prec@1=55.590 Prec@5=79.391 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=20:28 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.343 DataTime=0.245 Loss=1.893 Prec@1=55.590 Prec@5=79.391 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:28 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.343 DataTime=0.245 Loss=1.893 Prec@1=55.590 Prec@5=79.391 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:29 IST=> training   16.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.345 DataTime=0.246 Loss=1.895 Prec@1=55.586 Prec@5=79.375 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=20:29 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.345 DataTime=0.246 Loss=1.895 Prec@1=55.586 Prec@5=79.375 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=20:29 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.345 DataTime=0.246 Loss=1.895 Prec@1=55.586 Prec@5=79.375 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=20:29 IST=> training   20.02% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.342 DataTime=0.244 Loss=1.892 Prec@1=55.649 Prec@5=79.404 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=20:29 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.342 DataTime=0.244 Loss=1.892 Prec@1=55.649 Prec@5=79.404 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=20:29 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.342 DataTime=0.244 Loss=1.892 Prec@1=55.649 Prec@5=79.404 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=20:30 IST=> training   24.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.242 Loss=1.892 Prec@1=55.657 Prec@5=79.412 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=20:30 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.242 Loss=1.892 Prec@1=55.657 Prec@5=79.412 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=20:30 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.242 Loss=1.892 Prec@1=55.657 Prec@5=79.412 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=20:30 IST=> training   28.01% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.892 Prec@1=55.650 Prec@5=79.412 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=20:30 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.892 Prec@1=55.650 Prec@5=79.412 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=20:30 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.892 Prec@1=55.650 Prec@5=79.412 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=20:31 IST=> training   32.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.895 Prec@1=55.609 Prec@5=79.388 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=20:31 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.895 Prec@1=55.609 Prec@5=79.388 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=20:31 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.243 Loss=1.895 Prec@1=55.609 Prec@5=79.388 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=20:31 IST=> training   36.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.895 Prec@1=55.606 Prec@5=79.379 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=20:31 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.895 Prec@1=55.606 Prec@5=79.379 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=20:31 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.895 Prec@1=55.606 Prec@5=79.379 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=20:32 IST=> training   39.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.240 Loss=1.896 Prec@1=55.597 Prec@5=79.385 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=20:32 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.240 Loss=1.896 Prec@1=55.597 Prec@5=79.385 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=20:32 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.240 Loss=1.896 Prec@1=55.597 Prec@5=79.385 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=20:33 IST=> training   43.99% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.897 Prec@1=55.564 Prec@5=79.365 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=20:33 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.897 Prec@1=55.564 Prec@5=79.365 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=20:33 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.241 Loss=1.897 Prec@1=55.564 Prec@5=79.365 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=20:33 IST=> training   47.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.898 Prec@1=55.553 Prec@5=79.356 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=20:33 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.898 Prec@1=55.553 Prec@5=79.356 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=20:33 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.898 Prec@1=55.553 Prec@5=79.356 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=20:34 IST=> training   51.98% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.899 Prec@1=55.517 Prec@5=79.341 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=20:34 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.899 Prec@1=55.517 Prec@5=79.341 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=20:34 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.899 Prec@1=55.517 Prec@5=79.341 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=20:34 IST=> training   55.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.900 Prec@1=55.483 Prec@5=79.325 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=20:34 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.900 Prec@1=55.483 Prec@5=79.325 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=20:34 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.240 Loss=1.900 Prec@1=55.483 Prec@5=79.325 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=20:35 IST=> training   59.97% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.900 Prec@1=55.485 Prec@5=79.316 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=20:35 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.900 Prec@1=55.485 Prec@5=79.316 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=20:35 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.900 Prec@1=55.485 Prec@5=79.316 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=20:35 IST=> training   63.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.900 Prec@1=55.485 Prec@5=79.297 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=20:35 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.900 Prec@1=55.485 Prec@5=79.297 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=20:35 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.900 Prec@1=55.485 Prec@5=79.297 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=20:36 IST=> training   67.96% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.901 Prec@1=55.481 Prec@5=79.287 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=20:36 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.901 Prec@1=55.481 Prec@5=79.287 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=20:36 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.239 Loss=1.901 Prec@1=55.481 Prec@5=79.287 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=20:37 IST=> training   71.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.901 Prec@1=55.484 Prec@5=79.287 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=20:37 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.901 Prec@1=55.484 Prec@5=79.287 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=20:37 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.901 Prec@1=55.484 Prec@5=79.287 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=20:37 IST=> training   75.95% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.901 Prec@1=55.482 Prec@5=79.286 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=20:37 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.901 Prec@1=55.482 Prec@5=79.286 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=20:37 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.901 Prec@1=55.482 Prec@5=79.286 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=20:38 IST=> training   79.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.903 Prec@1=55.446 Prec@5=79.263 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=20:38 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.903 Prec@1=55.446 Prec@5=79.263 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=20:38 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.341 DataTime=0.238 Loss=1.903 Prec@1=55.446 Prec@5=79.263 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=20:38 IST=> training   83.94% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.904 Prec@1=55.424 Prec@5=79.244 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=20:38 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.904 Prec@1=55.424 Prec@5=79.244 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=20:38 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.238 Loss=1.904 Prec@1=55.424 Prec@5=79.244 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=20:39 IST=> training   87.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.396 Prec@5=79.218 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=20:39 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.396 Prec@5=79.218 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=20:39 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.396 Prec@5=79.218 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=20:39 IST=> training   91.93% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.399 Prec@5=79.215 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=20:39 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.399 Prec@5=79.215 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=20:39 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.905 Prec@1=55.399 Prec@5=79.215 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=20:40 IST=> training   95.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.907 Prec@1=55.372 Prec@5=79.195 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=20:40 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.907 Prec@1=55.372 Prec@5=79.195 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=20:40 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.907 Prec@1=55.372 Prec@5=79.195 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=20:40 IST=> training   99.92% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.907 Prec@1=55.372 Prec@5=79.196 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=20:40 IST=> training   100.00% of 1x2503...Epoch=87/150 LR=0.03858 Time=0.340 DataTime=0.237 Loss=1.907 Prec@1=55.372 Prec@5=79.196 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=20:40 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:40 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:40 IST=> validation 0.00% of 1x98...Epoch=87/150 LR=0.03858 Time=6.139 Loss=2.050 Prec@1=52.539 Prec@5=78.125 rate=0 Hz, eta=?, total=0:00:00, wall=20:40 IST=> validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=6.139 Loss=2.050 Prec@1=52.539 Prec@5=78.125 rate=7849.97 Hz, eta=0:00:00, total=0:00:00, wall=20:40 IST** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=6.139 Loss=2.050 Prec@1=52.539 Prec@5=78.125 rate=7849.97 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST** validation 1.02% of 1x98...Epoch=87/150 LR=0.03858 Time=0.396 Loss=2.053 Prec@1=52.618 Prec@5=76.990 rate=7849.97 Hz, eta=0:00:00, total=0:00:00, wall=20:41 IST** validation 100.00% of 1x98...Epoch=87/150 LR=0.03858 Time=0.396 Loss=2.053 Prec@1=52.618 Prec@5=76.990 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=20:41 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST=> training   0.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=5.290 DataTime=5.045 Loss=1.864 Prec@1=56.055 Prec@5=78.906 rate=0 Hz, eta=?, total=0:00:00, wall=20:41 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=5.290 DataTime=5.045 Loss=1.864 Prec@1=56.055 Prec@5=78.906 rate=1952.80 Hz, eta=0:00:01, total=0:00:00, wall=20:41 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=5.290 DataTime=5.045 Loss=1.864 Prec@1=56.055 Prec@5=78.906 rate=1952.80 Hz, eta=0:00:01, total=0:00:00, wall=20:41 IST=> training   0.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.381 DataTime=0.286 Loss=1.865 Prec@1=56.030 Prec@5=79.778 rate=1952.80 Hz, eta=0:00:01, total=0:00:00, wall=20:41 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.381 DataTime=0.286 Loss=1.865 Prec@1=56.030 Prec@5=79.778 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=20:41 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.381 DataTime=0.286 Loss=1.865 Prec@1=56.030 Prec@5=79.778 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=20:42 IST=> training   4.04% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.354 DataTime=0.258 Loss=1.878 Prec@1=55.849 Prec@5=79.543 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=20:42 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.354 DataTime=0.258 Loss=1.878 Prec@1=55.849 Prec@5=79.543 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:42 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.354 DataTime=0.258 Loss=1.878 Prec@1=55.849 Prec@5=79.543 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:42 IST=> training   8.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.347 DataTime=0.249 Loss=1.876 Prec@1=55.917 Prec@5=79.623 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:42 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.347 DataTime=0.249 Loss=1.876 Prec@1=55.917 Prec@5=79.623 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=20:42 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.347 DataTime=0.249 Loss=1.876 Prec@1=55.917 Prec@5=79.623 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=20:43 IST=> training   12.03% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.343 DataTime=0.245 Loss=1.875 Prec@1=55.988 Prec@5=79.586 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=20:43 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.343 DataTime=0.245 Loss=1.875 Prec@1=55.988 Prec@5=79.586 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=20:43 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.343 DataTime=0.245 Loss=1.875 Prec@1=55.988 Prec@5=79.586 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=20:44 IST=> training   16.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.241 Loss=1.875 Prec@1=55.981 Prec@5=79.613 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=20:44 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.241 Loss=1.875 Prec@1=55.981 Prec@5=79.613 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=20:44 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.241 Loss=1.875 Prec@1=55.981 Prec@5=79.613 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=20:44 IST=> training   20.02% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.240 Loss=1.878 Prec@1=55.899 Prec@5=79.603 rate=3.04 Hz, eta=0:10:58, total=0:02:44, wall=20:44 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.240 Loss=1.878 Prec@1=55.899 Prec@5=79.603 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=20:44 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.240 Loss=1.878 Prec@1=55.899 Prec@5=79.603 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=20:45 IST=> training   24.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.239 Loss=1.878 Prec@1=55.895 Prec@5=79.604 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=20:45 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.239 Loss=1.878 Prec@1=55.895 Prec@5=79.604 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=20:45 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.239 Loss=1.878 Prec@1=55.895 Prec@5=79.604 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=20:45 IST=> training   28.01% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.239 Loss=1.880 Prec@1=55.881 Prec@5=79.575 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=20:45 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.239 Loss=1.880 Prec@1=55.881 Prec@5=79.575 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=20:45 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.339 DataTime=0.239 Loss=1.880 Prec@1=55.881 Prec@5=79.575 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=20:46 IST=> training   32.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.238 Loss=1.883 Prec@1=55.829 Prec@5=79.517 rate=3.01 Hz, eta=0:09:25, total=0:04:25, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.238 Loss=1.883 Prec@1=55.829 Prec@5=79.517 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.338 DataTime=0.238 Loss=1.883 Prec@1=55.829 Prec@5=79.517 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=20:46 IST=> training   36.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.883 Prec@1=55.792 Prec@5=79.505 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=20:46 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.883 Prec@1=55.792 Prec@5=79.505 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=20:46 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.883 Prec@1=55.792 Prec@5=79.505 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=20:47 IST=> training   39.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.886 Prec@1=55.789 Prec@5=79.460 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=20:47 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.886 Prec@1=55.789 Prec@5=79.460 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=20:47 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.337 DataTime=0.237 Loss=1.886 Prec@1=55.789 Prec@5=79.460 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=20:47 IST=> training   43.99% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.887 Prec@1=55.762 Prec@5=79.445 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=20:47 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.887 Prec@1=55.762 Prec@5=79.445 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=20:47 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.887 Prec@1=55.762 Prec@5=79.445 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=20:48 IST=> training   47.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.888 Prec@1=55.746 Prec@5=79.426 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=20:48 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.888 Prec@1=55.746 Prec@5=79.426 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=20:48 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.235 Loss=1.888 Prec@1=55.746 Prec@5=79.426 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=20:49 IST=> training   51.98% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.888 Prec@1=55.744 Prec@5=79.431 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=20:49 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.888 Prec@1=55.744 Prec@5=79.431 rate=3.01 Hz, eta=0:06:05, total=0:07:45, wall=20:49 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.888 Prec@1=55.744 Prec@5=79.431 rate=3.01 Hz, eta=0:06:05, total=0:07:45, wall=20:49 IST=> training   55.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.890 Prec@1=55.715 Prec@5=79.422 rate=3.01 Hz, eta=0:06:05, total=0:07:45, wall=20:49 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.890 Prec@1=55.715 Prec@5=79.422 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=20:49 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.890 Prec@1=55.715 Prec@5=79.422 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=20:50 IST=> training   59.97% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.890 Prec@1=55.698 Prec@5=79.420 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=20:50 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.890 Prec@1=55.698 Prec@5=79.420 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=20:50 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.890 Prec@1=55.698 Prec@5=79.420 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=20:50 IST=> training   63.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.892 Prec@1=55.662 Prec@5=79.398 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=20:50 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.892 Prec@1=55.662 Prec@5=79.398 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=20:50 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.892 Prec@1=55.662 Prec@5=79.398 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=20:51 IST=> training   67.96% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.893 Prec@1=55.633 Prec@5=79.368 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=20:51 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.893 Prec@1=55.633 Prec@5=79.368 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=20:51 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.336 DataTime=0.234 Loss=1.893 Prec@1=55.633 Prec@5=79.368 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=20:51 IST=> training   71.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.894 Prec@1=55.626 Prec@5=79.356 rate=3.00 Hz, eta=0:03:53, total=0:09:59, wall=20:51 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.894 Prec@1=55.626 Prec@5=79.356 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=20:51 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.894 Prec@1=55.626 Prec@5=79.356 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=20:52 IST=> training   75.95% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.334 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=20:52 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.334 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=20:52 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.334 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=20:52 IST=> training   79.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.328 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=20:52 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.328 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=20:52 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.895 Prec@1=55.606 Prec@5=79.328 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=20:53 IST=> training   83.94% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.896 Prec@1=55.590 Prec@5=79.322 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=20:53 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.896 Prec@1=55.590 Prec@5=79.322 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=20:53 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.896 Prec@1=55.590 Prec@5=79.322 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=20:54 IST=> training   87.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.896 Prec@1=55.571 Prec@5=79.313 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=20:54 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.896 Prec@1=55.571 Prec@5=79.313 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=20:54 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.234 Loss=1.896 Prec@1=55.571 Prec@5=79.313 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=20:54 IST=> training   91.93% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.560 Prec@5=79.294 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=20:54 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.560 Prec@5=79.294 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=20:54 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.560 Prec@5=79.294 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=20:55 IST=> training   95.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.546 Prec@5=79.293 rate=3.00 Hz, eta=0:00:33, total=0:13:19, wall=20:55 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.546 Prec@5=79.293 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=20:55 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.546 Prec@5=79.293 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=20:55 IST=> training   99.92% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.545 Prec@5=79.292 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=20:55 IST=> training   100.00% of 1x2503...Epoch=88/150 LR=0.03757 Time=0.335 DataTime=0.233 Loss=1.898 Prec@1=55.545 Prec@5=79.292 rate=3.01 Hz, eta=0:00:00, total=0:13:52, wall=20:55 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> validation 0.00% of 1x98...Epoch=88/150 LR=0.03757 Time=6.704 Loss=1.989 Prec@1=53.516 Prec@5=78.516 rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=6.704 Loss=1.989 Prec@1=53.516 Prec@5=78.516 rate=5766.18 Hz, eta=0:00:00, total=0:00:00, wall=20:55 IST** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=6.704 Loss=1.989 Prec@1=53.516 Prec@5=78.516 rate=5766.18 Hz, eta=0:00:00, total=0:00:00, wall=20:55 IST** validation 1.02% of 1x98...Epoch=88/150 LR=0.03757 Time=0.411 Loss=2.138 Prec@1=50.800 Prec@5=75.554 rate=5766.18 Hz, eta=0:00:00, total=0:00:00, wall=20:55 IST** validation 100.00% of 1x98...Epoch=88/150 LR=0.03757 Time=0.411 Loss=2.138 Prec@1=50.800 Prec@5=75.554 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=20:55 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> training   0.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.377 DataTime=5.185 Loss=1.793 Prec@1=57.031 Prec@5=79.688 rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.377 DataTime=5.185 Loss=1.793 Prec@1=57.031 Prec@5=79.688 rate=6582.63 Hz, eta=0:00:00, total=0:00:00, wall=20:55 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=5.377 DataTime=5.185 Loss=1.793 Prec@1=57.031 Prec@5=79.688 rate=6582.63 Hz, eta=0:00:00, total=0:00:00, wall=20:56 IST=> training   0.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.374 DataTime=0.277 Loss=1.887 Prec@1=55.933 Prec@5=79.442 rate=6582.63 Hz, eta=0:00:00, total=0:00:00, wall=20:56 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.374 DataTime=0.277 Loss=1.887 Prec@1=55.933 Prec@5=79.442 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=20:56 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.374 DataTime=0.277 Loss=1.887 Prec@1=55.933 Prec@5=79.442 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=20:57 IST=> training   4.04% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.354 DataTime=0.257 Loss=1.877 Prec@1=56.081 Prec@5=79.644 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=20:57 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.354 DataTime=0.257 Loss=1.877 Prec@1=56.081 Prec@5=79.644 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:57 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.354 DataTime=0.257 Loss=1.877 Prec@1=56.081 Prec@5=79.644 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:57 IST=> training   8.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.347 DataTime=0.246 Loss=1.875 Prec@1=56.047 Prec@5=79.719 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=20:57 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.347 DataTime=0.246 Loss=1.875 Prec@1=56.047 Prec@5=79.719 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=20:57 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.347 DataTime=0.246 Loss=1.875 Prec@1=56.047 Prec@5=79.719 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=20:58 IST=> training   12.03% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.239 Loss=1.873 Prec@1=56.104 Prec@5=79.756 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=20:58 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.239 Loss=1.873 Prec@1=56.104 Prec@5=79.756 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=20:58 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.239 Loss=1.873 Prec@1=56.104 Prec@5=79.756 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=20:58 IST=> training   16.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.238 Loss=1.874 Prec@1=56.085 Prec@5=79.691 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=20:58 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.238 Loss=1.874 Prec@1=56.085 Prec@5=79.691 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=20:58 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.342 DataTime=0.238 Loss=1.874 Prec@1=56.085 Prec@5=79.691 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=20:59 IST=> training   20.02% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=56.032 Prec@5=79.678 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=20:59 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=56.032 Prec@5=79.678 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=20:59 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=56.032 Prec@5=79.678 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=20:59 IST=> training   24.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.955 Prec@5=79.674 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=20:59 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.955 Prec@5=79.674 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=20:59 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.955 Prec@5=79.674 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=21:00 IST=> training   28.01% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.879 Prec@1=55.902 Prec@5=79.647 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=21:00 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.879 Prec@1=55.902 Prec@5=79.647 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=21:00 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.879 Prec@1=55.902 Prec@5=79.647 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=21:00 IST=> training   32.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.878 Prec@1=55.929 Prec@5=79.648 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=21:00 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.878 Prec@1=55.929 Prec@5=79.648 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=21:00 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.341 DataTime=0.237 Loss=1.878 Prec@1=55.929 Prec@5=79.648 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=21:01 IST=> training   36.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.235 Loss=1.879 Prec@1=55.881 Prec@5=79.619 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=21:01 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.235 Loss=1.879 Prec@1=55.881 Prec@5=79.619 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:01 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.235 Loss=1.879 Prec@1=55.881 Prec@5=79.619 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:02 IST=> training   39.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.885 Prec@5=79.626 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:02 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.885 Prec@5=79.626 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=21:02 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.885 Prec@5=79.626 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=21:02 IST=> training   43.99% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.864 Prec@5=79.619 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=21:02 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.864 Prec@5=79.619 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=21:02 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.880 Prec@1=55.864 Prec@5=79.619 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=21:03 IST=> training   47.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.881 Prec@1=55.846 Prec@5=79.596 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=21:03 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.881 Prec@1=55.846 Prec@5=79.596 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=21:03 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.234 Loss=1.881 Prec@1=55.846 Prec@5=79.596 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=21:03 IST=> training   51.98% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.850 Prec@5=79.577 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=21:03 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.850 Prec@5=79.577 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=21:03 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.850 Prec@5=79.577 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=21:04 IST=> training   55.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.234 Loss=1.883 Prec@1=55.831 Prec@5=79.548 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=21:04 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.234 Loss=1.883 Prec@1=55.831 Prec@5=79.548 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=21:04 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.340 DataTime=0.234 Loss=1.883 Prec@1=55.831 Prec@5=79.548 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=21:04 IST=> training   59.97% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.884 Prec@1=55.808 Prec@5=79.524 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=21:04 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.884 Prec@1=55.808 Prec@5=79.524 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=21:04 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.884 Prec@1=55.808 Prec@5=79.524 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=21:05 IST=> training   63.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.886 Prec@1=55.778 Prec@5=79.507 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=21:05 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.886 Prec@1=55.778 Prec@5=79.507 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=21:05 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.886 Prec@1=55.778 Prec@5=79.507 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=21:06 IST=> training   67.96% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.771 Prec@5=79.493 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=21:06 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.771 Prec@5=79.493 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=21:06 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.771 Prec@5=79.493 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=21:06 IST=> training   71.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.758 Prec@5=79.485 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=21:06 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.758 Prec@5=79.485 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=21:06 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.887 Prec@1=55.758 Prec@5=79.485 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=21:07 IST=> training   75.95% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.888 Prec@1=55.739 Prec@5=79.474 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=21:07 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.888 Prec@1=55.739 Prec@5=79.474 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=21:07 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.888 Prec@1=55.739 Prec@5=79.474 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=21:07 IST=> training   79.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.890 Prec@1=55.719 Prec@5=79.442 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=21:07 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.890 Prec@1=55.719 Prec@5=79.442 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=21:07 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.890 Prec@1=55.719 Prec@5=79.442 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=21:08 IST=> training   83.94% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.891 Prec@1=55.698 Prec@5=79.430 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=21:08 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.891 Prec@1=55.698 Prec@5=79.430 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=21:08 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.233 Loss=1.891 Prec@1=55.698 Prec@5=79.430 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=21:08 IST=> training   87.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.402 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=21:08 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.402 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=21:08 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.402 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=21:09 IST=> training   91.93% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.400 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=21:09 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.400 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=21:09 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.339 DataTime=0.232 Loss=1.892 Prec@1=55.671 Prec@5=79.400 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=21:09 IST=> training   95.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.338 DataTime=0.231 Loss=1.892 Prec@1=55.667 Prec@5=79.399 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=21:09 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.338 DataTime=0.231 Loss=1.892 Prec@1=55.667 Prec@5=79.399 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=21:09 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.338 DataTime=0.231 Loss=1.892 Prec@1=55.667 Prec@5=79.399 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=21:09 IST=> training   99.92% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.338 DataTime=0.231 Loss=1.892 Prec@1=55.666 Prec@5=79.399 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=21:09 IST=> training   100.00% of 1x2503...Epoch=89/150 LR=0.03655 Time=0.338 DataTime=0.231 Loss=1.892 Prec@1=55.666 Prec@5=79.399 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=21:09 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> validation 0.00% of 1x98...Epoch=89/150 LR=0.03655 Time=6.731 Loss=2.071 Prec@1=50.195 Prec@5=75.977 rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=6.731 Loss=2.071 Prec@1=50.195 Prec@5=75.977 rate=5896.02 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=6.731 Loss=2.071 Prec@1=50.195 Prec@5=75.977 rate=5896.02 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST** validation 1.02% of 1x98...Epoch=89/150 LR=0.03655 Time=0.403 Loss=2.086 Prec@1=51.820 Prec@5=76.788 rate=5896.02 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST** validation 100.00% of 1x98...Epoch=89/150 LR=0.03655 Time=0.403 Loss=2.086 Prec@1=51.820 Prec@5=76.788 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=21:10 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> training   0.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=5.848 DataTime=5.764 Loss=1.805 Prec@1=56.445 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=5.848 DataTime=5.764 Loss=1.805 Prec@1=56.445 Prec@5=82.422 rate=9659.69 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=5.848 DataTime=5.764 Loss=1.805 Prec@1=56.445 Prec@5=82.422 rate=9659.69 Hz, eta=0:00:00, total=0:00:00, wall=21:11 IST=> training   0.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.380 DataTime=0.291 Loss=1.876 Prec@1=55.954 Prec@5=79.575 rate=9659.69 Hz, eta=0:00:00, total=0:00:00, wall=21:11 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.380 DataTime=0.291 Loss=1.876 Prec@1=55.954 Prec@5=79.575 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=21:11 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.380 DataTime=0.291 Loss=1.876 Prec@1=55.954 Prec@5=79.575 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=21:11 IST=> training   4.04% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.357 DataTime=0.258 Loss=1.867 Prec@1=56.116 Prec@5=79.790 rate=3.11 Hz, eta=0:12:53, total=0:00:32, wall=21:11 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.357 DataTime=0.258 Loss=1.867 Prec@1=56.116 Prec@5=79.790 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=21:11 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.357 DataTime=0.258 Loss=1.867 Prec@1=56.116 Prec@5=79.790 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=21:12 IST=> training   8.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.346 DataTime=0.245 Loss=1.869 Prec@1=56.131 Prec@5=79.754 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=21:12 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.346 DataTime=0.245 Loss=1.869 Prec@1=56.131 Prec@5=79.754 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=21:12 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.346 DataTime=0.245 Loss=1.869 Prec@1=56.131 Prec@5=79.754 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=21:12 IST=> training   12.03% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.344 DataTime=0.241 Loss=1.868 Prec@1=56.142 Prec@5=79.743 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=21:12 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.344 DataTime=0.241 Loss=1.868 Prec@1=56.142 Prec@5=79.743 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=21:12 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.344 DataTime=0.241 Loss=1.868 Prec@1=56.142 Prec@5=79.743 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=21:13 IST=> training   16.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.343 DataTime=0.240 Loss=1.868 Prec@1=56.118 Prec@5=79.755 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=21:13 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.343 DataTime=0.240 Loss=1.868 Prec@1=56.118 Prec@5=79.755 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=21:13 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.343 DataTime=0.240 Loss=1.868 Prec@1=56.118 Prec@5=79.755 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=21:14 IST=> training   20.02% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.238 Loss=1.867 Prec@1=56.130 Prec@5=79.745 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=21:14 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.238 Loss=1.867 Prec@1=56.130 Prec@5=79.745 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=21:14 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.238 Loss=1.867 Prec@1=56.130 Prec@5=79.745 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=21:14 IST=> training   24.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.871 Prec@1=56.051 Prec@5=79.701 rate=3.02 Hz, eta=0:10:30, total=0:03:19, wall=21:14 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.871 Prec@1=56.051 Prec@5=79.701 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=21:14 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.871 Prec@1=56.051 Prec@5=79.701 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=21:15 IST=> training   28.01% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=55.976 Prec@5=79.630 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=21:15 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=55.976 Prec@5=79.630 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=21:15 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.341 DataTime=0.237 Loss=1.875 Prec@1=55.976 Prec@5=79.630 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=21:15 IST=> training   32.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.950 Prec@5=79.609 rate=3.00 Hz, eta=0:09:27, total=0:04:26, wall=21:15 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.950 Prec@5=79.609 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=21:15 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.237 Loss=1.877 Prec@1=55.950 Prec@5=79.609 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=21:16 IST=> training   36.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.879 Prec@1=55.940 Prec@5=79.589 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=21:16 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.879 Prec@1=55.940 Prec@5=79.589 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=21:16 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.879 Prec@1=55.940 Prec@5=79.589 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=21:16 IST=> training   39.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.878 Prec@1=55.929 Prec@5=79.590 rate=3.00 Hz, eta=0:08:20, total=0:05:33, wall=21:16 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.878 Prec@1=55.929 Prec@5=79.590 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:16 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.235 Loss=1.878 Prec@1=55.929 Prec@5=79.590 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:17 IST=> training   43.99% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.234 Loss=1.879 Prec@1=55.907 Prec@5=79.601 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:17 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.234 Loss=1.879 Prec@1=55.907 Prec@5=79.601 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:17 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.340 DataTime=0.234 Loss=1.879 Prec@1=55.907 Prec@5=79.601 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:17 IST=> training   47.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.879 Prec@1=55.908 Prec@5=79.603 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:17 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.879 Prec@1=55.908 Prec@5=79.603 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:17 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.879 Prec@1=55.908 Prec@5=79.603 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:18 IST=> training   51.98% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.880 Prec@1=55.891 Prec@5=79.593 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:18 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.880 Prec@1=55.891 Prec@5=79.593 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=21:18 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.880 Prec@1=55.891 Prec@5=79.593 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=21:19 IST=> training   55.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.873 Prec@5=79.578 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=21:19 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.873 Prec@5=79.578 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=21:19 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.339 DataTime=0.233 Loss=1.881 Prec@1=55.873 Prec@5=79.578 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=21:19 IST=> training   59.97% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.882 Prec@1=55.850 Prec@5=79.565 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=21:19 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.882 Prec@1=55.850 Prec@5=79.565 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=21:19 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.882 Prec@1=55.850 Prec@5=79.565 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=21:20 IST=> training   63.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.831 Prec@5=79.554 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.831 Prec@5=79.554 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.831 Prec@5=79.554 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=21:20 IST=> training   67.96% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.821 Prec@5=79.553 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=21:20 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.821 Prec@5=79.553 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=21:20 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.883 Prec@1=55.821 Prec@5=79.553 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=21:21 IST=> training   71.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.884 Prec@1=55.799 Prec@5=79.541 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=21:21 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.884 Prec@1=55.799 Prec@5=79.541 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=21:21 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.884 Prec@1=55.799 Prec@5=79.541 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=21:21 IST=> training   75.95% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.233 Loss=1.884 Prec@1=55.812 Prec@5=79.551 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=21:21 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.233 Loss=1.884 Prec@1=55.812 Prec@5=79.551 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=21:21 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.233 Loss=1.884 Prec@1=55.812 Prec@5=79.551 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=21:22 IST=> training   79.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.884 Prec@1=55.805 Prec@5=79.555 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=21:22 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.884 Prec@1=55.805 Prec@5=79.555 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=21:22 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.884 Prec@1=55.805 Prec@5=79.555 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=21:22 IST=> training   83.94% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.885 Prec@1=55.782 Prec@5=79.534 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=21:22 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.885 Prec@1=55.782 Prec@5=79.534 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=21:22 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.885 Prec@1=55.782 Prec@5=79.534 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=21:23 IST=> training   87.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.886 Prec@1=55.764 Prec@5=79.509 rate=2.99 Hz, eta=0:01:41, total=0:12:16, wall=21:23 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.886 Prec@1=55.764 Prec@5=79.509 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=21:23 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.338 DataTime=0.232 Loss=1.886 Prec@1=55.764 Prec@5=79.509 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=21:24 IST=> training   91.93% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.887 Prec@1=55.766 Prec@5=79.503 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=21:24 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.887 Prec@1=55.766 Prec@5=79.503 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=21:24 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.337 DataTime=0.232 Loss=1.887 Prec@1=55.766 Prec@5=79.503 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=21:24 IST=> training   95.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.336 DataTime=0.231 Loss=1.888 Prec@1=55.751 Prec@5=79.488 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=21:24 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.336 DataTime=0.231 Loss=1.888 Prec@1=55.751 Prec@5=79.488 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=21:24 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.336 DataTime=0.231 Loss=1.888 Prec@1=55.751 Prec@5=79.488 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=21:24 IST=> training   99.92% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.336 DataTime=0.231 Loss=1.888 Prec@1=55.751 Prec@5=79.489 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=21:24 IST=> training   100.00% of 1x2503...Epoch=90/150 LR=0.03555 Time=0.336 DataTime=0.231 Loss=1.888 Prec@1=55.751 Prec@5=79.489 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=21:24 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST=> validation 0.00% of 1x98...Epoch=90/150 LR=0.03555 Time=7.797 Loss=1.904 Prec@1=54.883 Prec@5=79.102 rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST=> validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=7.797 Loss=1.904 Prec@1=54.883 Prec@5=79.102 rate=7476.52 Hz, eta=0:00:00, total=0:00:00, wall=21:24 IST** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=7.797 Loss=1.904 Prec@1=54.883 Prec@5=79.102 rate=7476.52 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST** validation 1.02% of 1x98...Epoch=90/150 LR=0.03555 Time=0.408 Loss=2.102 Prec@1=51.454 Prec@5=76.418 rate=7476.52 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST** validation 100.00% of 1x98...Epoch=90/150 LR=0.03555 Time=0.408 Loss=2.102 Prec@1=51.454 Prec@5=76.418 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=21:25 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:25 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:25 IST=> training   0.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.086 DataTime=4.839 Loss=1.703 Prec@1=59.180 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=21:25 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.086 DataTime=4.839 Loss=1.703 Prec@1=59.180 Prec@5=82.617 rate=4258.71 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=5.086 DataTime=4.839 Loss=1.703 Prec@1=59.180 Prec@5=82.617 rate=4258.71 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST=> training   0.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.378 DataTime=0.280 Loss=1.854 Prec@1=56.420 Prec@5=80.051 rate=4258.71 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.378 DataTime=0.280 Loss=1.854 Prec@1=56.420 Prec@5=80.051 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=21:25 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.378 DataTime=0.280 Loss=1.854 Prec@1=56.420 Prec@5=80.051 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=21:26 IST=> training   4.04% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.359 DataTime=0.260 Loss=1.862 Prec@1=56.222 Prec@5=79.862 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=21:26 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.359 DataTime=0.260 Loss=1.862 Prec@1=56.222 Prec@5=79.862 rate=3.00 Hz, eta=0:12:48, total=0:01:07, wall=21:26 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.359 DataTime=0.260 Loss=1.862 Prec@1=56.222 Prec@5=79.862 rate=3.00 Hz, eta=0:12:48, total=0:01:07, wall=21:27 IST=> training   8.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.353 DataTime=0.254 Loss=1.864 Prec@1=56.198 Prec@5=79.821 rate=3.00 Hz, eta=0:12:48, total=0:01:07, wall=21:27 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.353 DataTime=0.254 Loss=1.864 Prec@1=56.198 Prec@5=79.821 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:27 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.353 DataTime=0.254 Loss=1.864 Prec@1=56.198 Prec@5=79.821 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:27 IST=> training   12.03% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.350 DataTime=0.251 Loss=1.861 Prec@1=56.241 Prec@5=79.864 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:27 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.350 DataTime=0.251 Loss=1.861 Prec@1=56.241 Prec@5=79.864 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=21:27 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.350 DataTime=0.251 Loss=1.861 Prec@1=56.241 Prec@5=79.864 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=21:28 IST=> training   16.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.346 DataTime=0.246 Loss=1.859 Prec@1=56.291 Prec@5=79.882 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=21:28 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.346 DataTime=0.246 Loss=1.859 Prec@1=56.291 Prec@5=79.882 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=21:28 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.346 DataTime=0.246 Loss=1.859 Prec@1=56.291 Prec@5=79.882 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=21:28 IST=> training   20.02% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.344 DataTime=0.244 Loss=1.858 Prec@1=56.338 Prec@5=79.890 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=21:28 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.344 DataTime=0.244 Loss=1.858 Prec@1=56.338 Prec@5=79.890 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=21:28 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.344 DataTime=0.244 Loss=1.858 Prec@1=56.338 Prec@5=79.890 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=21:29 IST=> training   24.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.342 DataTime=0.240 Loss=1.861 Prec@1=56.295 Prec@5=79.850 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=21:29 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.342 DataTime=0.240 Loss=1.861 Prec@1=56.295 Prec@5=79.850 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=21:29 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.342 DataTime=0.240 Loss=1.861 Prec@1=56.295 Prec@5=79.850 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=21:29 IST=> training   28.01% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.865 Prec@1=56.203 Prec@5=79.779 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=21:29 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.865 Prec@1=56.203 Prec@5=79.779 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=21:29 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.865 Prec@1=56.203 Prec@5=79.779 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=21:30 IST=> training   32.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.864 Prec@1=56.206 Prec@5=79.798 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=21:30 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.864 Prec@1=56.206 Prec@5=79.798 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=21:30 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.864 Prec@1=56.206 Prec@5=79.798 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=21:31 IST=> training   36.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.868 Prec@1=56.129 Prec@5=79.764 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=21:31 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.868 Prec@1=56.129 Prec@5=79.764 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:31 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.340 DataTime=0.238 Loss=1.868 Prec@1=56.129 Prec@5=79.764 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:31 IST=> training   39.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.869 Prec@1=56.102 Prec@5=79.722 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=21:31 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.869 Prec@1=56.102 Prec@5=79.722 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:31 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.869 Prec@1=56.102 Prec@5=79.722 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:32 IST=> training   43.99% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.871 Prec@1=56.080 Prec@5=79.697 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=21:32 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.871 Prec@1=56.080 Prec@5=79.697 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:32 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.339 DataTime=0.236 Loss=1.871 Prec@1=56.080 Prec@5=79.697 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:32 IST=> training   47.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.338 DataTime=0.236 Loss=1.870 Prec@1=56.084 Prec@5=79.703 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=21:32 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.338 DataTime=0.236 Loss=1.870 Prec@1=56.084 Prec@5=79.703 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:32 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.338 DataTime=0.236 Loss=1.870 Prec@1=56.084 Prec@5=79.703 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:33 IST=> training   51.98% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.710 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=21:33 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.710 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=21:33 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.710 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=21:33 IST=> training   55.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.872 Prec@1=56.048 Prec@5=79.697 rate=3.00 Hz, eta=0:06:07, total=0:07:46, wall=21:33 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.872 Prec@1=56.048 Prec@5=79.697 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=21:33 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.337 DataTime=0.235 Loss=1.872 Prec@1=56.048 Prec@5=79.697 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=21:34 IST=> training   59.97% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.873 Prec@1=56.027 Prec@5=79.680 rate=3.00 Hz, eta=0:05:34, total=0:08:20, wall=21:34 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.873 Prec@1=56.027 Prec@5=79.680 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=21:34 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.873 Prec@1=56.027 Prec@5=79.680 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=21:34 IST=> training   63.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.874 Prec@1=56.030 Prec@5=79.662 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=21:34 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.874 Prec@1=56.030 Prec@5=79.662 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=21:34 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.874 Prec@1=56.030 Prec@5=79.662 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=21:35 IST=> training   67.96% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.016 Prec@5=79.663 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=21:35 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.016 Prec@5=79.663 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=21:35 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.016 Prec@5=79.663 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=21:35 IST=> training   71.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.011 Prec@5=79.666 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=21:35 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.011 Prec@5=79.666 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=21:35 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.336 DataTime=0.235 Loss=1.875 Prec@1=56.011 Prec@5=79.666 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=21:36 IST=> training   75.95% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.876 Prec@1=55.983 Prec@5=79.652 rate=3.00 Hz, eta=0:03:20, total=0:10:33, wall=21:36 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.876 Prec@1=55.983 Prec@5=79.652 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=21:36 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.876 Prec@1=55.983 Prec@5=79.652 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=21:37 IST=> training   79.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.877 Prec@1=55.959 Prec@5=79.622 rate=3.01 Hz, eta=0:02:47, total=0:11:05, wall=21:37 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.877 Prec@1=55.959 Prec@5=79.622 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=21:37 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.235 Loss=1.877 Prec@1=55.959 Prec@5=79.622 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=21:37 IST=> training   83.94% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.878 Prec@1=55.951 Prec@5=79.614 rate=3.01 Hz, eta=0:02:13, total=0:11:38, wall=21:37 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.878 Prec@1=55.951 Prec@5=79.614 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=21:37 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.878 Prec@1=55.951 Prec@5=79.614 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=21:38 IST=> training   87.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.928 Prec@5=79.604 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=21:38 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.928 Prec@5=79.604 rate=3.01 Hz, eta=0:01:07, total=0:12:45, wall=21:38 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.928 Prec@5=79.604 rate=3.01 Hz, eta=0:01:07, total=0:12:45, wall=21:38 IST=> training   91.93% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.920 Prec@5=79.610 rate=3.01 Hz, eta=0:01:07, total=0:12:45, wall=21:38 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.920 Prec@5=79.610 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=21:38 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.335 DataTime=0.234 Loss=1.879 Prec@1=55.920 Prec@5=79.610 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=21:39 IST=> training   95.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.334 DataTime=0.234 Loss=1.880 Prec@1=55.910 Prec@5=79.593 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=21:39 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.334 DataTime=0.234 Loss=1.880 Prec@1=55.910 Prec@5=79.593 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=21:39 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.334 DataTime=0.234 Loss=1.880 Prec@1=55.910 Prec@5=79.593 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=21:39 IST=> training   99.92% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.334 DataTime=0.233 Loss=1.880 Prec@1=55.910 Prec@5=79.593 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=21:39 IST=> training   100.00% of 1x2503...Epoch=91/150 LR=0.03455 Time=0.334 DataTime=0.233 Loss=1.880 Prec@1=55.910 Prec@5=79.593 rate=3.01 Hz, eta=0:00:00, total=0:13:51, wall=21:39 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:39 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:39 IST=> validation 0.00% of 1x98...Epoch=91/150 LR=0.03455 Time=6.320 Loss=2.206 Prec@1=49.023 Prec@5=75.195 rate=0 Hz, eta=?, total=0:00:00, wall=21:39 IST=> validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=6.320 Loss=2.206 Prec@1=49.023 Prec@5=75.195 rate=3798.30 Hz, eta=0:00:00, total=0:00:00, wall=21:39 IST** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=6.320 Loss=2.206 Prec@1=49.023 Prec@5=75.195 rate=3798.30 Hz, eta=0:00:00, total=0:00:00, wall=21:39 IST** validation 1.02% of 1x98...Epoch=91/150 LR=0.03455 Time=0.393 Loss=1.980 Prec@1=53.676 Prec@5=78.312 rate=3798.30 Hz, eta=0:00:00, total=0:00:00, wall=21:39 IST** validation 100.00% of 1x98...Epoch=91/150 LR=0.03455 Time=0.393 Loss=1.980 Prec@1=53.676 Prec@5=78.312 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=21:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:40 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:40 IST=> training   0.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.941 DataTime=4.680 Loss=1.893 Prec@1=55.469 Prec@5=79.297 rate=0 Hz, eta=?, total=0:00:00, wall=21:40 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.941 DataTime=4.680 Loss=1.893 Prec@1=55.469 Prec@5=79.297 rate=6517.93 Hz, eta=0:00:00, total=0:00:00, wall=21:40 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=4.941 DataTime=4.680 Loss=1.893 Prec@1=55.469 Prec@5=79.297 rate=6517.93 Hz, eta=0:00:00, total=0:00:00, wall=21:40 IST=> training   0.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.392 DataTime=0.293 Loss=1.837 Prec@1=56.761 Prec@5=80.268 rate=6517.93 Hz, eta=0:00:00, total=0:00:00, wall=21:40 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.392 DataTime=0.293 Loss=1.837 Prec@1=56.761 Prec@5=80.268 rate=2.91 Hz, eta=0:13:44, total=0:00:34, wall=21:40 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.392 DataTime=0.293 Loss=1.837 Prec@1=56.761 Prec@5=80.268 rate=2.91 Hz, eta=0:13:44, total=0:00:34, wall=21:41 IST=> training   4.04% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.363 DataTime=0.262 Loss=1.855 Prec@1=56.514 Prec@5=79.925 rate=2.91 Hz, eta=0:13:44, total=0:00:34, wall=21:41 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.363 DataTime=0.262 Loss=1.855 Prec@1=56.514 Prec@5=79.925 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=21:41 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.363 DataTime=0.262 Loss=1.855 Prec@1=56.514 Prec@5=79.925 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=21:41 IST=> training   8.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.353 DataTime=0.251 Loss=1.857 Prec@1=56.485 Prec@5=79.919 rate=2.95 Hz, eta=0:13:00, total=0:01:08, wall=21:41 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.353 DataTime=0.251 Loss=1.857 Prec@1=56.485 Prec@5=79.919 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:41 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.353 DataTime=0.251 Loss=1.857 Prec@1=56.485 Prec@5=79.919 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:42 IST=> training   12.03% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.351 DataTime=0.247 Loss=1.856 Prec@1=56.507 Prec@5=79.980 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=21:42 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.351 DataTime=0.247 Loss=1.856 Prec@1=56.507 Prec@5=79.980 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=21:42 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.351 DataTime=0.247 Loss=1.856 Prec@1=56.507 Prec@5=79.980 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=21:42 IST=> training   16.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.858 Prec@1=56.421 Prec@5=79.898 rate=2.96 Hz, eta=0:11:51, total=0:02:15, wall=21:42 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.858 Prec@1=56.421 Prec@5=79.898 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=21:42 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.858 Prec@1=56.421 Prec@5=79.898 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=21:43 IST=> training   20.02% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.860 Prec@1=56.405 Prec@5=79.869 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=21:43 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.860 Prec@1=56.405 Prec@5=79.869 rate=2.96 Hz, eta=0:10:43, total=0:03:23, wall=21:43 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.346 DataTime=0.243 Loss=1.860 Prec@1=56.405 Prec@5=79.869 rate=2.96 Hz, eta=0:10:43, total=0:03:23, wall=21:43 IST=> training   24.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.345 DataTime=0.243 Loss=1.862 Prec@1=56.378 Prec@5=79.856 rate=2.96 Hz, eta=0:10:43, total=0:03:23, wall=21:43 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.345 DataTime=0.243 Loss=1.862 Prec@1=56.378 Prec@5=79.856 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=21:43 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.345 DataTime=0.243 Loss=1.862 Prec@1=56.378 Prec@5=79.856 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=21:44 IST=> training   28.01% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.241 Loss=1.862 Prec@1=56.336 Prec@5=79.852 rate=2.96 Hz, eta=0:10:09, total=0:03:57, wall=21:44 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.241 Loss=1.862 Prec@1=56.336 Prec@5=79.852 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=21:44 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.241 Loss=1.862 Prec@1=56.336 Prec@5=79.852 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=21:45 IST=> training   32.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.863 Prec@1=56.271 Prec@5=79.830 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=21:45 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.863 Prec@1=56.271 Prec@5=79.830 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=21:45 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.863 Prec@1=56.271 Prec@5=79.830 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=21:45 IST=> training   36.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.866 Prec@1=56.215 Prec@5=79.764 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=21:45 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.866 Prec@1=56.215 Prec@5=79.764 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=21:45 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.344 DataTime=0.240 Loss=1.866 Prec@1=56.215 Prec@5=79.764 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=21:46 IST=> training   39.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.239 Loss=1.866 Prec@1=56.188 Prec@5=79.777 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=21:46 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.239 Loss=1.866 Prec@1=56.188 Prec@5=79.777 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=21:46 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.239 Loss=1.866 Prec@1=56.188 Prec@5=79.777 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=21:46 IST=> training   43.99% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.238 Loss=1.867 Prec@1=56.156 Prec@5=79.771 rate=2.95 Hz, eta=0:07:54, total=0:06:13, wall=21:46 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.238 Loss=1.867 Prec@1=56.156 Prec@5=79.771 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=21:46 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.238 Loss=1.867 Prec@1=56.156 Prec@5=79.771 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=21:47 IST=> training   47.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.238 Loss=1.868 Prec@1=56.134 Prec@5=79.757 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=21:47 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.238 Loss=1.868 Prec@1=56.134 Prec@5=79.757 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=21:47 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.343 DataTime=0.238 Loss=1.868 Prec@1=56.134 Prec@5=79.757 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=21:47 IST=> training   51.98% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.237 Loss=1.868 Prec@1=56.110 Prec@5=79.739 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=21:47 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.237 Loss=1.868 Prec@1=56.110 Prec@5=79.739 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=21:47 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.237 Loss=1.868 Prec@1=56.110 Prec@5=79.739 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=21:48 IST=> training   55.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.869 Prec@1=56.091 Prec@5=79.729 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=21:48 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.869 Prec@1=56.091 Prec@5=79.729 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=21:48 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.869 Prec@1=56.091 Prec@5=79.729 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=21:49 IST=> training   59.97% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.236 Loss=1.870 Prec@1=56.097 Prec@5=79.727 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=21:49 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.236 Loss=1.870 Prec@1=56.097 Prec@5=79.727 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=21:49 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.342 DataTime=0.236 Loss=1.870 Prec@1=56.097 Prec@5=79.727 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=21:49 IST=> training   63.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.722 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=21:49 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.722 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=21:49 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.870 Prec@1=56.075 Prec@5=79.722 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=21:50 IST=> training   67.96% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.871 Prec@1=56.052 Prec@5=79.715 rate=2.96 Hz, eta=0:04:31, total=0:09:34, wall=21:50 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.871 Prec@1=56.052 Prec@5=79.715 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=21:50 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.236 Loss=1.871 Prec@1=56.052 Prec@5=79.715 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=21:50 IST=> training   71.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.871 Prec@1=56.032 Prec@5=79.713 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=21:50 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.871 Prec@1=56.032 Prec@5=79.713 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=21:50 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.871 Prec@1=56.032 Prec@5=79.713 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=21:51 IST=> training   75.95% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.035 Prec@5=79.702 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=21:51 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.035 Prec@5=79.702 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=21:51 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.035 Prec@5=79.702 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=21:51 IST=> training   79.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.872 Prec@1=56.038 Prec@5=79.700 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=21:51 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.872 Prec@1=56.038 Prec@5=79.700 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=21:51 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.341 DataTime=0.235 Loss=1.872 Prec@1=56.038 Prec@5=79.700 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=21:52 IST=> training   83.94% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.037 Prec@5=79.692 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.037 Prec@5=79.692 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.872 Prec@1=56.037 Prec@5=79.692 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=21:52 IST=> training   87.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.873 Prec@1=56.013 Prec@5=79.680 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=21:52 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.873 Prec@1=56.013 Prec@5=79.680 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=21:52 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.873 Prec@1=56.013 Prec@5=79.680 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=21:53 IST=> training   91.93% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.874 Prec@1=56.020 Prec@5=79.675 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=21:53 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.874 Prec@1=56.020 Prec@5=79.675 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=21:53 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.235 Loss=1.874 Prec@1=56.020 Prec@5=79.675 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=21:54 IST=> training   95.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.234 Loss=1.874 Prec@1=56.015 Prec@5=79.670 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.234 Loss=1.874 Prec@1=56.015 Prec@5=79.670 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.340 DataTime=0.234 Loss=1.874 Prec@1=56.015 Prec@5=79.670 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=21:54 IST=> training   99.92% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.339 DataTime=0.234 Loss=1.874 Prec@1=56.014 Prec@5=79.669 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=21:54 IST=> training   100.00% of 1x2503...Epoch=92/150 LR=0.03356 Time=0.339 DataTime=0.234 Loss=1.874 Prec@1=56.014 Prec@5=79.669 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=21:54 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 0.00% of 1x98...Epoch=92/150 LR=0.03356 Time=6.125 Loss=1.993 Prec@1=56.445 Prec@5=78.320 rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=6.125 Loss=1.993 Prec@1=56.445 Prec@5=78.320 rate=6658.01 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=6.125 Loss=1.993 Prec@1=56.445 Prec@5=78.320 rate=6658.01 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 1.02% of 1x98...Epoch=92/150 LR=0.03356 Time=0.397 Loss=1.916 Prec@1=55.044 Prec@5=79.186 rate=6658.01 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST** validation 100.00% of 1x98...Epoch=92/150 LR=0.03356 Time=0.397 Loss=1.916 Prec@1=55.044 Prec@5=79.186 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=21:54 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> training   0.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.966 DataTime=5.848 Loss=1.699 Prec@1=59.180 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=21:54 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.966 DataTime=5.848 Loss=1.699 Prec@1=59.180 Prec@5=81.250 rate=7662.24 Hz, eta=0:00:00, total=0:00:00, wall=21:54 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=5.966 DataTime=5.848 Loss=1.699 Prec@1=59.180 Prec@5=81.250 rate=7662.24 Hz, eta=0:00:00, total=0:00:00, wall=21:55 IST=> training   0.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.380 DataTime=0.287 Loss=1.835 Prec@1=56.602 Prec@5=80.287 rate=7662.24 Hz, eta=0:00:00, total=0:00:00, wall=21:55 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.380 DataTime=0.287 Loss=1.835 Prec@1=56.602 Prec@5=80.287 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=21:55 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.380 DataTime=0.287 Loss=1.835 Prec@1=56.602 Prec@5=80.287 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=21:56 IST=> training   4.04% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.361 DataTime=0.257 Loss=1.840 Prec@1=56.540 Prec@5=80.255 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=21:56 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.361 DataTime=0.257 Loss=1.840 Prec@1=56.540 Prec@5=80.255 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=21:56 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.361 DataTime=0.257 Loss=1.840 Prec@1=56.540 Prec@5=80.255 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=21:56 IST=> training   8.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.242 Loss=1.843 Prec@1=56.412 Prec@5=80.229 rate=3.02 Hz, eta=0:12:41, total=0:01:06, wall=21:56 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.242 Loss=1.843 Prec@1=56.412 Prec@5=80.229 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=21:56 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.242 Loss=1.843 Prec@1=56.412 Prec@5=80.229 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=21:57 IST=> training   12.03% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.241 Loss=1.843 Prec@1=56.462 Prec@5=80.175 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=21:57 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.241 Loss=1.843 Prec@1=56.462 Prec@5=80.175 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=21:57 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.350 DataTime=0.241 Loss=1.843 Prec@1=56.462 Prec@5=80.175 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=21:57 IST=> training   16.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.346 DataTime=0.238 Loss=1.848 Prec@1=56.455 Prec@5=80.096 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=21:57 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.346 DataTime=0.238 Loss=1.848 Prec@1=56.455 Prec@5=80.096 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=21:57 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.346 DataTime=0.238 Loss=1.848 Prec@1=56.455 Prec@5=80.096 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=21:58 IST=> training   20.02% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.847 Prec@1=56.452 Prec@5=80.096 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=21:58 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.847 Prec@1=56.452 Prec@5=80.096 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=21:58 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.847 Prec@1=56.452 Prec@5=80.096 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=21:58 IST=> training   24.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.424 Prec@5=80.038 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=21:58 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.424 Prec@5=80.038 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=21:58 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.424 Prec@5=80.038 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=21:59 IST=> training   28.01% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.458 Prec@5=79.997 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=21:59 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.458 Prec@5=79.997 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=21:59 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.344 DataTime=0.235 Loss=1.849 Prec@1=56.458 Prec@5=79.997 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=21:59 IST=> training   32.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.233 Loss=1.853 Prec@1=56.408 Prec@5=79.928 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=21:59 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.233 Loss=1.853 Prec@1=56.408 Prec@5=79.928 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=21:59 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.233 Loss=1.853 Prec@1=56.408 Prec@5=79.928 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=22:00 IST=> training   36.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.343 DataTime=0.234 Loss=1.854 Prec@1=56.409 Prec@5=79.934 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=22:00 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.343 DataTime=0.234 Loss=1.854 Prec@1=56.409 Prec@5=79.934 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=22:00 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.343 DataTime=0.234 Loss=1.854 Prec@1=56.409 Prec@5=79.934 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=22:01 IST=> training   39.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.342 DataTime=0.233 Loss=1.854 Prec@1=56.411 Prec@5=79.937 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=22:01 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.342 DataTime=0.233 Loss=1.854 Prec@1=56.411 Prec@5=79.937 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:01 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.342 DataTime=0.233 Loss=1.854 Prec@1=56.411 Prec@5=79.937 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:01 IST=> training   43.99% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.855 Prec@1=56.385 Prec@5=79.912 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:01 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.855 Prec@1=56.385 Prec@5=79.912 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=22:01 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.855 Prec@1=56.385 Prec@5=79.912 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=22:02 IST=> training   47.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.857 Prec@1=56.333 Prec@5=79.877 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=22:02 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.857 Prec@1=56.333 Prec@5=79.877 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:02 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.857 Prec@1=56.333 Prec@5=79.877 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:02 IST=> training   51.98% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.858 Prec@1=56.313 Prec@5=79.872 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:02 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.858 Prec@1=56.313 Prec@5=79.872 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=22:02 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.858 Prec@1=56.313 Prec@5=79.872 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=22:03 IST=> training   55.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.859 Prec@1=56.283 Prec@5=79.868 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=22:03 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.859 Prec@1=56.283 Prec@5=79.868 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=22:03 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.859 Prec@1=56.283 Prec@5=79.868 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=22:03 IST=> training   59.97% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.247 Prec@5=79.866 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=22:03 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.247 Prec@5=79.866 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=22:03 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.247 Prec@5=79.866 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=22:04 IST=> training   63.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.861 Prec@1=56.220 Prec@5=79.857 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=22:04 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.861 Prec@1=56.220 Prec@5=79.857 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:04 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.861 Prec@1=56.220 Prec@5=79.857 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:04 IST=> training   67.96% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.231 Loss=1.862 Prec@1=56.216 Prec@5=79.849 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:04 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.231 Loss=1.862 Prec@1=56.216 Prec@5=79.849 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=22:04 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.231 Loss=1.862 Prec@1=56.216 Prec@5=79.849 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=22:05 IST=> training   71.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.222 Prec@5=79.856 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=22:05 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.222 Prec@5=79.856 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:05 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.341 DataTime=0.232 Loss=1.861 Prec@1=56.222 Prec@5=79.856 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:06 IST=> training   75.95% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.232 Loss=1.862 Prec@1=56.199 Prec@5=79.843 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:06 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.232 Loss=1.862 Prec@1=56.199 Prec@5=79.843 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:06 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.232 Loss=1.862 Prec@1=56.199 Prec@5=79.843 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:06 IST=> training   79.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.863 Prec@1=56.186 Prec@5=79.835 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:06 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.863 Prec@1=56.186 Prec@5=79.835 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=22:06 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.863 Prec@1=56.186 Prec@5=79.835 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=22:07 IST=> training   83.94% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.864 Prec@1=56.161 Prec@5=79.821 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=22:07 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.864 Prec@1=56.161 Prec@5=79.821 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=22:07 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.864 Prec@1=56.161 Prec@5=79.821 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=22:07 IST=> training   87.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.149 Prec@5=79.812 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=22:07 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.149 Prec@5=79.812 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:07 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.149 Prec@5=79.812 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:08 IST=> training   91.93% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.140 Prec@5=79.810 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:08 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.140 Prec@5=79.810 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=22:08 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.340 DataTime=0.231 Loss=1.865 Prec@1=56.140 Prec@5=79.810 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=22:08 IST=> training   95.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.230 Loss=1.866 Prec@1=56.136 Prec@5=79.794 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.230 Loss=1.866 Prec@1=56.136 Prec@5=79.794 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.230 Loss=1.866 Prec@1=56.136 Prec@5=79.794 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=22:08 IST=> training   99.92% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.230 Loss=1.866 Prec@1=56.135 Prec@5=79.795 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=22:08 IST=> training   100.00% of 1x2503...Epoch=93/150 LR=0.03257 Time=0.339 DataTime=0.230 Loss=1.866 Prec@1=56.135 Prec@5=79.795 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=22:08 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> validation 0.00% of 1x98...Epoch=93/150 LR=0.03257 Time=7.135 Loss=1.942 Prec@1=54.297 Prec@5=80.078 rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=7.135 Loss=1.942 Prec@1=54.297 Prec@5=80.078 rate=3958.55 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=7.135 Loss=1.942 Prec@1=54.297 Prec@5=80.078 rate=3958.55 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST** validation 1.02% of 1x98...Epoch=93/150 LR=0.03257 Time=0.405 Loss=1.976 Prec@1=53.824 Prec@5=78.376 rate=3958.55 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST** validation 100.00% of 1x98...Epoch=93/150 LR=0.03257 Time=0.405 Loss=1.976 Prec@1=53.824 Prec@5=78.376 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=22:09 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> training   0.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.051 DataTime=4.826 Loss=1.601 Prec@1=63.281 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.051 DataTime=4.826 Loss=1.601 Prec@1=63.281 Prec@5=83.594 rate=4124.44 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=5.051 DataTime=4.826 Loss=1.601 Prec@1=63.281 Prec@5=83.594 rate=4124.44 Hz, eta=0:00:00, total=0:00:00, wall=22:10 IST=> training   0.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.373 DataTime=0.275 Loss=1.828 Prec@1=56.991 Prec@5=80.370 rate=4124.44 Hz, eta=0:00:00, total=0:00:00, wall=22:10 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.373 DataTime=0.275 Loss=1.828 Prec@1=56.991 Prec@5=80.370 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=22:10 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.373 DataTime=0.275 Loss=1.828 Prec@1=56.991 Prec@5=80.370 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=22:10 IST=> training   4.04% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.355 DataTime=0.256 Loss=1.835 Prec@1=56.873 Prec@5=80.287 rate=3.09 Hz, eta=0:12:57, total=0:00:32, wall=22:10 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.355 DataTime=0.256 Loss=1.835 Prec@1=56.873 Prec@5=80.287 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=22:10 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.355 DataTime=0.256 Loss=1.835 Prec@1=56.873 Prec@5=80.287 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=22:11 IST=> training   8.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.245 Loss=1.837 Prec@1=56.837 Prec@5=80.203 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=22:11 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.245 Loss=1.837 Prec@1=56.837 Prec@5=80.203 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=22:11 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.245 Loss=1.837 Prec@1=56.837 Prec@5=80.203 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=22:11 IST=> training   12.03% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.244 Loss=1.840 Prec@1=56.772 Prec@5=80.186 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=22:11 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.244 Loss=1.840 Prec@1=56.772 Prec@5=80.186 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=22:11 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.345 DataTime=0.244 Loss=1.840 Prec@1=56.772 Prec@5=80.186 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=22:12 IST=> training   16.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.342 DataTime=0.240 Loss=1.841 Prec@1=56.741 Prec@5=80.183 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=22:12 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.342 DataTime=0.240 Loss=1.841 Prec@1=56.741 Prec@5=80.183 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=22:12 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.342 DataTime=0.240 Loss=1.841 Prec@1=56.741 Prec@5=80.183 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=22:13 IST=> training   20.02% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.339 DataTime=0.237 Loss=1.843 Prec@1=56.689 Prec@5=80.147 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=22:13 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.339 DataTime=0.237 Loss=1.843 Prec@1=56.689 Prec@5=80.147 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=22:13 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.339 DataTime=0.237 Loss=1.843 Prec@1=56.689 Prec@5=80.147 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=22:13 IST=> training   24.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.338 DataTime=0.237 Loss=1.845 Prec@1=56.653 Prec@5=80.100 rate=3.02 Hz, eta=0:10:28, total=0:03:18, wall=22:13 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.338 DataTime=0.237 Loss=1.845 Prec@1=56.653 Prec@5=80.100 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:13 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.338 DataTime=0.237 Loss=1.845 Prec@1=56.653 Prec@5=80.100 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:14 IST=> training   28.01% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.848 Prec@1=56.594 Prec@5=80.085 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:14 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.848 Prec@1=56.594 Prec@5=80.085 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=22:14 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.848 Prec@1=56.594 Prec@5=80.085 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=22:14 IST=> training   32.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.848 Prec@1=56.564 Prec@5=80.077 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=22:14 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.848 Prec@1=56.564 Prec@5=80.077 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=22:14 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.848 Prec@1=56.564 Prec@5=80.077 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=22:15 IST=> training   36.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.849 Prec@1=56.551 Prec@5=80.082 rate=3.02 Hz, eta=0:08:49, total=0:04:57, wall=22:15 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.849 Prec@1=56.551 Prec@5=80.082 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=22:15 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.337 DataTime=0.236 Loss=1.849 Prec@1=56.551 Prec@5=80.082 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=22:15 IST=> training   39.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.847 Prec@1=56.565 Prec@5=80.098 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=22:15 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.847 Prec@1=56.565 Prec@5=80.098 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=22:15 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.847 Prec@1=56.565 Prec@5=80.098 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=22:16 IST=> training   43.99% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.849 Prec@1=56.517 Prec@5=80.062 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=22:16 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.849 Prec@1=56.517 Prec@5=80.062 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=22:16 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.849 Prec@1=56.517 Prec@5=80.062 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=22:16 IST=> training   47.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.851 Prec@1=56.477 Prec@5=80.040 rate=3.02 Hz, eta=0:07:11, total=0:06:38, wall=22:16 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.851 Prec@1=56.477 Prec@5=80.040 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=22:16 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.851 Prec@1=56.477 Prec@5=80.040 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=22:17 IST=> training   51.98% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.852 Prec@1=56.425 Prec@5=80.002 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=22:17 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.852 Prec@1=56.425 Prec@5=80.002 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=22:17 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.852 Prec@1=56.425 Prec@5=80.002 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=22:18 IST=> training   55.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.853 Prec@1=56.413 Prec@5=79.994 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=22:18 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.853 Prec@1=56.413 Prec@5=79.994 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=22:18 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.853 Prec@1=56.413 Prec@5=79.994 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=22:18 IST=> training   59.97% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.855 Prec@1=56.376 Prec@5=79.964 rate=3.01 Hz, eta=0:05:32, total=0:08:18, wall=22:18 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.855 Prec@1=56.376 Prec@5=79.964 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=22:18 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.855 Prec@1=56.376 Prec@5=79.964 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=22:19 IST=> training   63.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.856 Prec@1=56.378 Prec@5=79.947 rate=3.00 Hz, eta=0:05:00, total=0:08:52, wall=22:19 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.856 Prec@1=56.378 Prec@5=79.947 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=22:19 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.856 Prec@1=56.378 Prec@5=79.947 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=22:19 IST=> training   67.96% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.857 Prec@1=56.364 Prec@5=79.922 rate=3.00 Hz, eta=0:04:26, total=0:09:26, wall=22:19 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.857 Prec@1=56.364 Prec@5=79.922 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=22:19 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.235 Loss=1.857 Prec@1=56.364 Prec@5=79.922 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=22:20 IST=> training   71.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.857 Prec@1=56.356 Prec@5=79.916 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=22:20 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.857 Prec@1=56.356 Prec@5=79.916 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=22:20 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.236 Loss=1.857 Prec@1=56.356 Prec@5=79.916 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=22:20 IST=> training   75.95% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.858 Prec@1=56.339 Prec@5=79.904 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=22:20 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.858 Prec@1=56.339 Prec@5=79.904 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=22:20 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.858 Prec@1=56.339 Prec@5=79.904 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=22:21 IST=> training   79.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.344 Prec@5=79.902 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=22:21 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.344 Prec@5=79.902 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=22:21 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.344 Prec@5=79.902 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=22:21 IST=> training   83.94% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.343 Prec@5=79.895 rate=3.00 Hz, eta=0:02:14, total=0:11:40, wall=22:21 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.343 Prec@5=79.895 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=22:21 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.235 Loss=1.859 Prec@1=56.343 Prec@5=79.895 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=22:22 IST=> training   87.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.322 Prec@5=79.890 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=22:22 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.322 Prec@5=79.890 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=22:22 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.322 Prec@5=79.890 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=22:23 IST=> training   91.93% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.332 Prec@5=79.893 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=22:23 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.332 Prec@5=79.893 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=22:23 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.336 DataTime=0.234 Loss=1.860 Prec@1=56.332 Prec@5=79.893 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=22:23 IST=> training   95.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.233 Loss=1.860 Prec@1=56.319 Prec@5=79.892 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=22:23 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.233 Loss=1.860 Prec@1=56.319 Prec@5=79.892 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=22:23 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.233 Loss=1.860 Prec@1=56.319 Prec@5=79.892 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=22:23 IST=> training   99.92% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.233 Loss=1.860 Prec@1=56.318 Prec@5=79.891 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=22:23 IST=> training   100.00% of 1x2503...Epoch=94/150 LR=0.03159 Time=0.335 DataTime=0.233 Loss=1.860 Prec@1=56.318 Prec@5=79.891 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=22:23 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST=> validation 0.00% of 1x98...Epoch=94/150 LR=0.03159 Time=5.942 Loss=2.010 Prec@1=50.977 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST=> validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=5.942 Loss=2.010 Prec@1=50.977 Prec@5=77.148 rate=4404.49 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=5.942 Loss=2.010 Prec@1=50.977 Prec@5=77.148 rate=4404.49 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST** validation 1.02% of 1x98...Epoch=94/150 LR=0.03159 Time=0.403 Loss=2.028 Prec@1=53.108 Prec@5=77.446 rate=4404.49 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST** validation 100.00% of 1x98...Epoch=94/150 LR=0.03159 Time=0.403 Loss=2.028 Prec@1=53.108 Prec@5=77.446 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=22:24 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST=> training   0.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.239 DataTime=4.987 Loss=1.593 Prec@1=61.133 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=22:24 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.239 DataTime=4.987 Loss=1.593 Prec@1=61.133 Prec@5=84.570 rate=3858.75 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=5.239 DataTime=4.987 Loss=1.593 Prec@1=61.133 Prec@5=84.570 rate=3858.75 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST=> training   0.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.382 DataTime=0.284 Loss=1.824 Prec@1=57.080 Prec@5=80.519 rate=3858.75 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.382 DataTime=0.284 Loss=1.824 Prec@1=57.080 Prec@5=80.519 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=22:24 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.382 DataTime=0.284 Loss=1.824 Prec@1=57.080 Prec@5=80.519 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=22:25 IST=> training   4.04% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.354 DataTime=0.255 Loss=1.825 Prec@1=57.076 Prec@5=80.391 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=22:25 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.354 DataTime=0.255 Loss=1.825 Prec@1=57.076 Prec@5=80.391 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=22:25 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.354 DataTime=0.255 Loss=1.825 Prec@1=57.076 Prec@5=80.391 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=22:26 IST=> training   8.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.347 DataTime=0.247 Loss=1.831 Prec@1=56.994 Prec@5=80.330 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=22:26 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.347 DataTime=0.247 Loss=1.831 Prec@1=56.994 Prec@5=80.330 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:26 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.347 DataTime=0.247 Loss=1.831 Prec@1=56.994 Prec@5=80.330 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:26 IST=> training   12.03% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.242 Loss=1.830 Prec@1=56.905 Prec@5=80.359 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:26 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.242 Loss=1.830 Prec@1=56.905 Prec@5=80.359 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=22:26 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.242 Loss=1.830 Prec@1=56.905 Prec@5=80.359 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=22:27 IST=> training   16.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.834 Prec@1=56.857 Prec@5=80.287 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.834 Prec@1=56.857 Prec@5=80.287 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.834 Prec@1=56.857 Prec@5=80.287 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=22:27 IST=> training   20.02% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.239 Loss=1.835 Prec@1=56.793 Prec@5=80.251 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=22:27 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.239 Loss=1.835 Prec@1=56.793 Prec@5=80.251 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=22:27 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.239 Loss=1.835 Prec@1=56.793 Prec@5=80.251 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=22:28 IST=> training   24.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.837 Prec@1=56.780 Prec@5=80.225 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=22:28 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.837 Prec@1=56.780 Prec@5=80.225 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=22:28 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.344 DataTime=0.240 Loss=1.837 Prec@1=56.780 Prec@5=80.225 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=22:28 IST=> training   28.01% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.342 DataTime=0.237 Loss=1.838 Prec@1=56.763 Prec@5=80.219 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=22:28 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.342 DataTime=0.237 Loss=1.838 Prec@1=56.763 Prec@5=80.219 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=22:28 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.342 DataTime=0.237 Loss=1.838 Prec@1=56.763 Prec@5=80.219 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=22:29 IST=> training   32.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.345 DataTime=0.239 Loss=1.840 Prec@1=56.695 Prec@5=80.187 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=22:29 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.345 DataTime=0.239 Loss=1.840 Prec@1=56.695 Prec@5=80.187 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=22:29 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.345 DataTime=0.239 Loss=1.840 Prec@1=56.695 Prec@5=80.187 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=22:30 IST=> training   36.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.237 Loss=1.843 Prec@1=56.665 Prec@5=80.151 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=22:30 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.237 Loss=1.843 Prec@1=56.665 Prec@5=80.151 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=22:30 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.343 DataTime=0.237 Loss=1.843 Prec@1=56.665 Prec@5=80.151 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=22:30 IST=> training   39.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.842 Prec@1=56.686 Prec@5=80.139 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=22:30 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.842 Prec@1=56.686 Prec@5=80.139 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:30 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.842 Prec@1=56.686 Prec@5=80.139 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:31 IST=> training   43.99% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.843 Prec@1=56.671 Prec@5=80.121 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=22:31 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.843 Prec@1=56.671 Prec@5=80.121 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=22:31 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.843 Prec@1=56.671 Prec@5=80.121 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=22:31 IST=> training   47.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.845 Prec@1=56.643 Prec@5=80.082 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=22:31 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.845 Prec@1=56.643 Prec@5=80.082 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:31 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.845 Prec@1=56.643 Prec@5=80.082 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:32 IST=> training   51.98% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.845 Prec@1=56.640 Prec@5=80.088 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=22:32 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.845 Prec@1=56.640 Prec@5=80.088 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:32 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.845 Prec@1=56.640 Prec@5=80.088 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:32 IST=> training   55.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.848 Prec@1=56.581 Prec@5=80.053 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:32 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.848 Prec@1=56.581 Prec@5=80.053 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=22:32 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.341 DataTime=0.236 Loss=1.848 Prec@1=56.581 Prec@5=80.053 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=22:33 IST=> training   59.97% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.849 Prec@1=56.533 Prec@5=80.030 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=22:33 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.849 Prec@1=56.533 Prec@5=80.030 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=22:33 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.849 Prec@1=56.533 Prec@5=80.030 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=22:33 IST=> training   63.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.850 Prec@1=56.527 Prec@5=80.022 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=22:33 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.850 Prec@1=56.527 Prec@5=80.022 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:33 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.850 Prec@1=56.527 Prec@5=80.022 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:34 IST=> training   67.96% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.516 Prec@5=80.007 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=22:34 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.516 Prec@5=80.007 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=22:34 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.516 Prec@5=80.007 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=22:35 IST=> training   71.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.509 Prec@5=80.003 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=22:35 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.509 Prec@5=80.003 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:35 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.851 Prec@1=56.509 Prec@5=80.003 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:35 IST=> training   75.95% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.852 Prec@1=56.510 Prec@5=80.000 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=22:35 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.852 Prec@1=56.510 Prec@5=80.000 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:35 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.852 Prec@1=56.510 Prec@5=80.000 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:36 IST=> training   79.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.853 Prec@1=56.486 Prec@5=79.975 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=22:36 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.853 Prec@1=56.486 Prec@5=79.975 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=22:36 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.853 Prec@1=56.486 Prec@5=79.975 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=22:36 IST=> training   83.94% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.854 Prec@1=56.470 Prec@5=79.957 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=22:36 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.854 Prec@1=56.470 Prec@5=79.957 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=22:36 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.854 Prec@1=56.470 Prec@5=79.957 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=22:37 IST=> training   87.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.855 Prec@1=56.438 Prec@5=79.942 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=22:37 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.855 Prec@1=56.438 Prec@5=79.942 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=22:37 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.855 Prec@1=56.438 Prec@5=79.942 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=22:37 IST=> training   91.93% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.855 Prec@1=56.432 Prec@5=79.934 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=22:37 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.855 Prec@1=56.432 Prec@5=79.934 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=22:37 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.235 Loss=1.855 Prec@1=56.432 Prec@5=79.934 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=22:38 IST=> training   95.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.856 Prec@1=56.429 Prec@5=79.921 rate=2.96 Hz, eta=0:00:34, total=0:13:32, wall=22:38 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.856 Prec@1=56.429 Prec@5=79.921 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=22:38 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.340 DataTime=0.234 Loss=1.856 Prec@1=56.429 Prec@5=79.921 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=22:38 IST=> training   99.92% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.339 DataTime=0.234 Loss=1.856 Prec@1=56.429 Prec@5=79.921 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=22:38 IST=> training   100.00% of 1x2503...Epoch=95/150 LR=0.03062 Time=0.339 DataTime=0.234 Loss=1.856 Prec@1=56.429 Prec@5=79.921 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=22:38 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:38 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:38 IST=> validation 0.00% of 1x98...Epoch=95/150 LR=0.03062 Time=6.740 Loss=2.144 Prec@1=50.000 Prec@5=74.609 rate=0 Hz, eta=?, total=0:00:00, wall=22:38 IST=> validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.740 Loss=2.144 Prec@1=50.000 Prec@5=74.609 rate=3344.64 Hz, eta=0:00:00, total=0:00:00, wall=22:38 IST** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=6.740 Loss=2.144 Prec@1=50.000 Prec@5=74.609 rate=3344.64 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST** validation 1.02% of 1x98...Epoch=95/150 LR=0.03062 Time=0.396 Loss=2.127 Prec@1=50.872 Prec@5=75.942 rate=3344.64 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST** validation 100.00% of 1x98...Epoch=95/150 LR=0.03062 Time=0.396 Loss=2.127 Prec@1=50.872 Prec@5=75.942 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=22:39 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST=> training   0.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.704 DataTime=5.490 Loss=1.703 Prec@1=60.547 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=22:39 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.704 DataTime=5.490 Loss=1.703 Prec@1=60.547 Prec@5=81.055 rate=6591.22 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=5.704 DataTime=5.490 Loss=1.703 Prec@1=60.547 Prec@5=81.055 rate=6591.22 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST=> training   0.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.373 DataTime=0.273 Loss=1.815 Prec@1=56.991 Prec@5=80.480 rate=6591.22 Hz, eta=0:00:00, total=0:00:00, wall=22:39 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.373 DataTime=0.273 Loss=1.815 Prec@1=56.991 Prec@5=80.480 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=22:39 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.373 DataTime=0.273 Loss=1.815 Prec@1=56.991 Prec@5=80.480 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=22:40 IST=> training   4.04% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.358 DataTime=0.257 Loss=1.820 Prec@1=57.077 Prec@5=80.490 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=22:40 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.358 DataTime=0.257 Loss=1.820 Prec@1=57.077 Prec@5=80.490 rate=3.03 Hz, eta=0:12:38, total=0:01:06, wall=22:40 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.358 DataTime=0.257 Loss=1.820 Prec@1=57.077 Prec@5=80.490 rate=3.03 Hz, eta=0:12:38, total=0:01:06, wall=22:40 IST=> training   8.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.349 DataTime=0.246 Loss=1.823 Prec@1=57.047 Prec@5=80.488 rate=3.03 Hz, eta=0:12:38, total=0:01:06, wall=22:40 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.349 DataTime=0.246 Loss=1.823 Prec@1=57.047 Prec@5=80.488 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:40 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.349 DataTime=0.246 Loss=1.823 Prec@1=57.047 Prec@5=80.488 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:41 IST=> training   12.03% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.346 DataTime=0.242 Loss=1.829 Prec@1=56.912 Prec@5=80.440 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=22:41 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.346 DataTime=0.242 Loss=1.829 Prec@1=56.912 Prec@5=80.440 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=22:41 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.346 DataTime=0.242 Loss=1.829 Prec@1=56.912 Prec@5=80.440 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=22:42 IST=> training   16.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.344 DataTime=0.240 Loss=1.830 Prec@1=56.898 Prec@5=80.383 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=22:42 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.344 DataTime=0.240 Loss=1.830 Prec@1=56.898 Prec@5=80.383 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=22:42 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.344 DataTime=0.240 Loss=1.830 Prec@1=56.898 Prec@5=80.383 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=22:42 IST=> training   20.02% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.343 DataTime=0.239 Loss=1.830 Prec@1=56.912 Prec@5=80.406 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=22:42 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.343 DataTime=0.239 Loss=1.830 Prec@1=56.912 Prec@5=80.406 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=22:42 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.343 DataTime=0.239 Loss=1.830 Prec@1=56.912 Prec@5=80.406 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=22:43 IST=> training   24.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.236 Loss=1.831 Prec@1=56.904 Prec@5=80.397 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=22:43 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.236 Loss=1.831 Prec@1=56.904 Prec@5=80.397 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=22:43 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.236 Loss=1.831 Prec@1=56.904 Prec@5=80.397 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=22:43 IST=> training   28.01% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.237 Loss=1.831 Prec@1=56.871 Prec@5=80.400 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=22:43 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.237 Loss=1.831 Prec@1=56.871 Prec@5=80.400 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=22:43 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.237 Loss=1.831 Prec@1=56.871 Prec@5=80.400 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=22:44 IST=> training   32.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.832 Prec@1=56.852 Prec@5=80.370 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=22:44 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.832 Prec@1=56.852 Prec@5=80.370 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=22:44 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.832 Prec@1=56.852 Prec@5=80.370 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=22:44 IST=> training   36.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.833 Prec@1=56.836 Prec@5=80.340 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=22:44 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.833 Prec@1=56.836 Prec@5=80.340 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=22:44 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.235 Loss=1.833 Prec@1=56.836 Prec@5=80.340 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=22:45 IST=> training   39.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.236 Loss=1.835 Prec@1=56.800 Prec@5=80.297 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=22:45 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.236 Loss=1.835 Prec@1=56.800 Prec@5=80.297 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=22:45 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.236 Loss=1.835 Prec@1=56.800 Prec@5=80.297 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=22:45 IST=> training   43.99% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.836 Prec@1=56.786 Prec@5=80.267 rate=2.98 Hz, eta=0:07:51, total=0:06:10, wall=22:45 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.836 Prec@1=56.786 Prec@5=80.267 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=22:45 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.836 Prec@1=56.786 Prec@5=80.267 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=22:46 IST=> training   47.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.233 Loss=1.836 Prec@1=56.781 Prec@5=80.278 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=22:46 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.233 Loss=1.836 Prec@1=56.781 Prec@5=80.278 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=22:46 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.233 Loss=1.836 Prec@1=56.781 Prec@5=80.278 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=22:47 IST=> training   51.98% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.234 Loss=1.838 Prec@1=56.765 Prec@5=80.261 rate=2.98 Hz, eta=0:06:42, total=0:07:16, wall=22:47 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.234 Loss=1.838 Prec@1=56.765 Prec@5=80.261 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:47 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.341 DataTime=0.234 Loss=1.838 Prec@1=56.765 Prec@5=80.261 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:47 IST=> training   55.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.839 Prec@1=56.735 Prec@5=80.233 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=22:47 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.839 Prec@1=56.735 Prec@5=80.233 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=22:47 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.340 DataTime=0.234 Loss=1.839 Prec@1=56.735 Prec@5=80.233 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=22:48 IST=> training   59.97% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.840 Prec@1=56.735 Prec@5=80.207 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=22:48 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.840 Prec@1=56.735 Prec@5=80.207 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=22:48 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.840 Prec@1=56.735 Prec@5=80.207 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=22:48 IST=> training   63.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.841 Prec@1=56.720 Prec@5=80.195 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=22:48 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.841 Prec@1=56.720 Prec@5=80.195 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=22:48 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.233 Loss=1.841 Prec@1=56.720 Prec@5=80.195 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=22:49 IST=> training   67.96% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.842 Prec@1=56.707 Prec@5=80.175 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=22:49 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.842 Prec@1=56.707 Prec@5=80.175 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=22:49 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.842 Prec@1=56.707 Prec@5=80.175 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=22:49 IST=> training   71.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.232 Loss=1.842 Prec@1=56.692 Prec@5=80.155 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=22:49 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.232 Loss=1.842 Prec@1=56.692 Prec@5=80.155 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=22:49 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.232 Loss=1.842 Prec@1=56.692 Prec@5=80.155 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=22:50 IST=> training   75.95% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.683 Prec@5=80.153 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=22:50 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.683 Prec@5=80.153 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=22:50 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.683 Prec@5=80.153 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=22:51 IST=> training   79.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.664 Prec@5=80.148 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=22:51 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.664 Prec@5=80.148 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=22:51 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.843 Prec@1=56.664 Prec@5=80.148 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=22:51 IST=> training   83.94% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.844 Prec@1=56.657 Prec@5=80.149 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=22:51 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.844 Prec@1=56.657 Prec@5=80.149 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=22:51 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.844 Prec@1=56.657 Prec@5=80.149 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=22:52 IST=> training   87.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.845 Prec@1=56.637 Prec@5=80.116 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=22:52 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.845 Prec@1=56.637 Prec@5=80.116 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:52 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.232 Loss=1.845 Prec@1=56.637 Prec@5=80.116 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:52 IST=> training   91.93% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.847 Prec@1=56.613 Prec@5=80.087 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=22:52 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.847 Prec@1=56.613 Prec@5=80.087 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=22:52 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.339 DataTime=0.231 Loss=1.847 Prec@1=56.613 Prec@5=80.087 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=22:53 IST=> training   95.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.231 Loss=1.848 Prec@1=56.587 Prec@5=80.068 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=22:53 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.231 Loss=1.848 Prec@1=56.587 Prec@5=80.068 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=22:53 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.231 Loss=1.848 Prec@1=56.587 Prec@5=80.068 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=22:53 IST=> training   99.92% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.231 Loss=1.848 Prec@1=56.586 Prec@5=80.067 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=22:53 IST=> training   100.00% of 1x2503...Epoch=96/150 LR=0.02966 Time=0.338 DataTime=0.231 Loss=1.848 Prec@1=56.586 Prec@5=80.067 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=22:53 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> validation 0.00% of 1x98...Epoch=96/150 LR=0.02966 Time=6.222 Loss=1.993 Prec@1=51.953 Prec@5=78.320 rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=6.222 Loss=1.993 Prec@1=51.953 Prec@5=78.320 rate=4289.69 Hz, eta=0:00:00, total=0:00:00, wall=22:53 IST** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=6.222 Loss=1.993 Prec@1=51.953 Prec@5=78.320 rate=4289.69 Hz, eta=0:00:00, total=0:00:00, wall=22:53 IST** validation 1.02% of 1x98...Epoch=96/150 LR=0.02966 Time=0.399 Loss=1.996 Prec@1=53.628 Prec@5=78.058 rate=4289.69 Hz, eta=0:00:00, total=0:00:00, wall=22:53 IST** validation 100.00% of 1x98...Epoch=96/150 LR=0.02966 Time=0.399 Loss=1.996 Prec@1=53.628 Prec@5=78.058 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=22:53 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> training   0.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.298 DataTime=5.121 Loss=1.807 Prec@1=57.617 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=22:53 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.298 DataTime=5.121 Loss=1.807 Prec@1=57.617 Prec@5=81.445 rate=5658.57 Hz, eta=0:00:00, total=0:00:00, wall=22:53 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=5.298 DataTime=5.121 Loss=1.807 Prec@1=57.617 Prec@5=81.445 rate=5658.57 Hz, eta=0:00:00, total=0:00:00, wall=22:54 IST=> training   0.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.381 DataTime=0.288 Loss=1.825 Prec@1=56.965 Prec@5=80.285 rate=5658.57 Hz, eta=0:00:00, total=0:00:00, wall=22:54 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.381 DataTime=0.288 Loss=1.825 Prec@1=56.965 Prec@5=80.285 rate=3.04 Hz, eta=0:13:08, total=0:00:33, wall=22:54 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.381 DataTime=0.288 Loss=1.825 Prec@1=56.965 Prec@5=80.285 rate=3.04 Hz, eta=0:13:08, total=0:00:33, wall=22:55 IST=> training   4.04% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.355 DataTime=0.261 Loss=1.821 Prec@1=57.057 Prec@5=80.432 rate=3.04 Hz, eta=0:13:08, total=0:00:33, wall=22:55 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.355 DataTime=0.261 Loss=1.821 Prec@1=57.057 Prec@5=80.432 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=22:55 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.355 DataTime=0.261 Loss=1.821 Prec@1=57.057 Prec@5=80.432 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=22:55 IST=> training   8.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.349 DataTime=0.254 Loss=1.820 Prec@1=57.108 Prec@5=80.516 rate=3.04 Hz, eta=0:12:36, total=0:01:06, wall=22:55 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.349 DataTime=0.254 Loss=1.820 Prec@1=57.108 Prec@5=80.516 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=22:55 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.349 DataTime=0.254 Loss=1.820 Prec@1=57.108 Prec@5=80.516 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=22:56 IST=> training   12.03% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.346 DataTime=0.249 Loss=1.820 Prec@1=57.073 Prec@5=80.497 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=22:56 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.346 DataTime=0.249 Loss=1.820 Prec@1=57.073 Prec@5=80.497 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=22:56 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.346 DataTime=0.249 Loss=1.820 Prec@1=57.073 Prec@5=80.497 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=22:56 IST=> training   16.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.033 Prec@5=80.484 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=22:56 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.033 Prec@5=80.484 rate=3.03 Hz, eta=0:11:01, total=0:02:45, wall=22:56 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.033 Prec@5=80.484 rate=3.03 Hz, eta=0:11:01, total=0:02:45, wall=22:57 IST=> training   20.02% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.017 Prec@5=80.528 rate=3.03 Hz, eta=0:11:01, total=0:02:45, wall=22:57 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.017 Prec@5=80.528 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=22:57 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.341 DataTime=0.243 Loss=1.821 Prec@1=57.017 Prec@5=80.528 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=22:57 IST=> training   24.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.339 DataTime=0.239 Loss=1.821 Prec@1=57.014 Prec@5=80.507 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=22:57 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.339 DataTime=0.239 Loss=1.821 Prec@1=57.014 Prec@5=80.507 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:57 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.339 DataTime=0.239 Loss=1.821 Prec@1=57.014 Prec@5=80.507 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:58 IST=> training   28.01% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.238 Loss=1.823 Prec@1=57.005 Prec@5=80.476 rate=3.02 Hz, eta=0:09:56, total=0:03:52, wall=22:58 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.238 Loss=1.823 Prec@1=57.005 Prec@5=80.476 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=22:58 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.238 Loss=1.823 Prec@1=57.005 Prec@5=80.476 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=22:58 IST=> training   32.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.827 Prec@1=56.925 Prec@5=80.390 rate=3.03 Hz, eta=0:09:22, total=0:04:24, wall=22:58 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.827 Prec@1=56.925 Prec@5=80.390 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=22:58 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.827 Prec@1=56.925 Prec@5=80.390 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=22:59 IST=> training   36.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.829 Prec@1=56.898 Prec@5=80.356 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=22:59 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.829 Prec@1=56.898 Prec@5=80.356 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=22:59 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.237 Loss=1.829 Prec@1=56.898 Prec@5=80.356 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=23:00 IST=> training   39.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.236 Loss=1.829 Prec@1=56.891 Prec@5=80.344 rate=3.02 Hz, eta=0:08:17, total=0:05:31, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.236 Loss=1.829 Prec@1=56.891 Prec@5=80.344 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.337 DataTime=0.236 Loss=1.829 Prec@1=56.891 Prec@5=80.344 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=23:00 IST=> training   43.99% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.829 Prec@1=56.912 Prec@5=80.338 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=23:00 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.829 Prec@1=56.912 Prec@5=80.338 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=23:00 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.829 Prec@1=56.912 Prec@5=80.338 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=23:01 IST=> training   47.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.336 DataTime=0.234 Loss=1.830 Prec@1=56.891 Prec@5=80.329 rate=3.02 Hz, eta=0:07:10, total=0:06:37, wall=23:01 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.336 DataTime=0.234 Loss=1.830 Prec@1=56.891 Prec@5=80.329 rate=3.02 Hz, eta=0:06:38, total=0:07:11, wall=23:01 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.336 DataTime=0.234 Loss=1.830 Prec@1=56.891 Prec@5=80.329 rate=3.02 Hz, eta=0:06:38, total=0:07:11, wall=23:01 IST=> training   51.98% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.831 Prec@1=56.870 Prec@5=80.320 rate=3.02 Hz, eta=0:06:38, total=0:07:11, wall=23:01 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.831 Prec@1=56.870 Prec@5=80.320 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:01 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.831 Prec@1=56.870 Prec@5=80.320 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:02 IST=> training   55.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.832 Prec@1=56.841 Prec@5=80.317 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:02 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.832 Prec@1=56.841 Prec@5=80.317 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=23:02 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.832 Prec@1=56.841 Prec@5=80.317 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=23:02 IST=> training   59.97% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.833 Prec@1=56.810 Prec@5=80.298 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=23:02 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.833 Prec@1=56.810 Prec@5=80.298 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:02 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.234 Loss=1.833 Prec@1=56.810 Prec@5=80.298 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:03 IST=> training   63.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.834 Prec@1=56.799 Prec@5=80.279 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:03 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.834 Prec@1=56.799 Prec@5=80.279 rate=3.01 Hz, eta=0:04:26, total=0:09:24, wall=23:03 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.834 Prec@1=56.799 Prec@5=80.279 rate=3.01 Hz, eta=0:04:26, total=0:09:24, wall=23:03 IST=> training   67.96% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.774 Prec@5=80.244 rate=3.01 Hz, eta=0:04:26, total=0:09:24, wall=23:03 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.774 Prec@5=80.244 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=23:03 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.774 Prec@5=80.244 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=23:04 IST=> training   71.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.773 Prec@5=80.221 rate=3.01 Hz, eta=0:03:52, total=0:09:57, wall=23:04 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.773 Prec@5=80.221 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:04 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.836 Prec@1=56.773 Prec@5=80.221 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:05 IST=> training   75.95% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.232 Loss=1.836 Prec@1=56.772 Prec@5=80.221 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:05 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.232 Loss=1.836 Prec@1=56.772 Prec@5=80.221 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:05 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.232 Loss=1.836 Prec@1=56.772 Prec@5=80.221 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:05 IST=> training   79.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.837 Prec@1=56.769 Prec@5=80.222 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:05 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.837 Prec@1=56.769 Prec@5=80.222 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:05 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.837 Prec@1=56.769 Prec@5=80.222 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:06 IST=> training   83.94% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.838 Prec@1=56.750 Prec@5=80.211 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:06 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.838 Prec@1=56.750 Prec@5=80.211 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:06 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.335 DataTime=0.233 Loss=1.838 Prec@1=56.750 Prec@5=80.211 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:06 IST=> training   87.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.738 Prec@5=80.214 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:06 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.738 Prec@5=80.214 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:06 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.738 Prec@5=80.214 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:07 IST=> training   91.93% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.735 Prec@5=80.216 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:07 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.735 Prec@5=80.216 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=23:07 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.735 Prec@5=80.216 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=23:07 IST=> training   95.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.731 Prec@5=80.209 rate=3.01 Hz, eta=0:00:33, total=0:13:17, wall=23:07 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.731 Prec@5=80.209 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:07 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.731 Prec@5=80.209 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:07 IST=> training   99.92% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.732 Prec@5=80.209 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:07 IST=> training   100.00% of 1x2503...Epoch=97/150 LR=0.02871 Time=0.334 DataTime=0.232 Loss=1.838 Prec@1=56.732 Prec@5=80.209 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:07 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:07 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:07 IST=> validation 0.00% of 1x98...Epoch=97/150 LR=0.02871 Time=7.115 Loss=2.062 Prec@1=52.539 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=23:07 IST=> validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=7.115 Loss=2.062 Prec@1=52.539 Prec@5=77.344 rate=6593.78 Hz, eta=0:00:00, total=0:00:00, wall=23:07 IST** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=7.115 Loss=2.062 Prec@1=52.539 Prec@5=77.344 rate=6593.78 Hz, eta=0:00:00, total=0:00:00, wall=23:08 IST** validation 1.02% of 1x98...Epoch=97/150 LR=0.02871 Time=0.404 Loss=1.933 Prec@1=54.864 Prec@5=79.028 rate=6593.78 Hz, eta=0:00:00, total=0:00:00, wall=23:08 IST** validation 100.00% of 1x98...Epoch=97/150 LR=0.02871 Time=0.404 Loss=1.933 Prec@1=54.864 Prec@5=79.028 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=23:08 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:08 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:08 IST=> training   0.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=5.526 DataTime=5.303 Loss=1.816 Prec@1=57.812 Prec@5=78.711 rate=0 Hz, eta=?, total=0:00:00, wall=23:08 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=5.526 DataTime=5.303 Loss=1.816 Prec@1=57.812 Prec@5=78.711 rate=6460.54 Hz, eta=0:00:00, total=0:00:00, wall=23:08 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=5.526 DataTime=5.303 Loss=1.816 Prec@1=57.812 Prec@5=78.711 rate=6460.54 Hz, eta=0:00:00, total=0:00:00, wall=23:09 IST=> training   0.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.386 DataTime=0.295 Loss=1.820 Prec@1=57.178 Prec@5=80.579 rate=6460.54 Hz, eta=0:00:00, total=0:00:00, wall=23:09 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.386 DataTime=0.295 Loss=1.820 Prec@1=57.178 Prec@5=80.579 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=23:09 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.386 DataTime=0.295 Loss=1.820 Prec@1=57.178 Prec@5=80.579 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=23:09 IST=> training   4.04% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.357 DataTime=0.260 Loss=1.807 Prec@1=57.431 Prec@5=80.705 rate=3.01 Hz, eta=0:13:17, total=0:00:33, wall=23:09 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.357 DataTime=0.260 Loss=1.807 Prec@1=57.431 Prec@5=80.705 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=23:09 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.357 DataTime=0.260 Loss=1.807 Prec@1=57.431 Prec@5=80.705 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=23:10 IST=> training   8.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.349 DataTime=0.247 Loss=1.805 Prec@1=57.417 Prec@5=80.696 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=23:10 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.349 DataTime=0.247 Loss=1.805 Prec@1=57.417 Prec@5=80.696 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=23:10 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.349 DataTime=0.247 Loss=1.805 Prec@1=57.417 Prec@5=80.696 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=23:10 IST=> training   12.03% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.344 DataTime=0.242 Loss=1.806 Prec@1=57.363 Prec@5=80.693 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=23:10 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.344 DataTime=0.242 Loss=1.806 Prec@1=57.363 Prec@5=80.693 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=23:10 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.344 DataTime=0.242 Loss=1.806 Prec@1=57.363 Prec@5=80.693 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=23:11 IST=> training   16.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.341 DataTime=0.240 Loss=1.808 Prec@1=57.329 Prec@5=80.679 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=23:11 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.341 DataTime=0.240 Loss=1.808 Prec@1=57.329 Prec@5=80.679 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=23:11 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.341 DataTime=0.240 Loss=1.808 Prec@1=57.329 Prec@5=80.679 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=23:11 IST=> training   20.02% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.236 Loss=1.812 Prec@1=57.264 Prec@5=80.598 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=23:11 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.236 Loss=1.812 Prec@1=57.264 Prec@5=80.598 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=23:11 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.236 Loss=1.812 Prec@1=57.264 Prec@5=80.598 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=23:12 IST=> training   24.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.339 DataTime=0.237 Loss=1.814 Prec@1=57.222 Prec@5=80.560 rate=3.04 Hz, eta=0:10:25, total=0:03:17, wall=23:12 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.339 DataTime=0.237 Loss=1.814 Prec@1=57.222 Prec@5=80.560 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=23:12 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.339 DataTime=0.237 Loss=1.814 Prec@1=57.222 Prec@5=80.560 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=23:13 IST=> training   28.01% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.235 Loss=1.816 Prec@1=57.179 Prec@5=80.542 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=23:13 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.235 Loss=1.816 Prec@1=57.179 Prec@5=80.542 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=23:13 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.235 Loss=1.816 Prec@1=57.179 Prec@5=80.542 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=23:13 IST=> training   32.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.337 DataTime=0.233 Loss=1.818 Prec@1=57.145 Prec@5=80.519 rate=3.02 Hz, eta=0:09:23, total=0:04:25, wall=23:13 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.337 DataTime=0.233 Loss=1.818 Prec@1=57.145 Prec@5=80.519 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=23:13 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.337 DataTime=0.233 Loss=1.818 Prec@1=57.145 Prec@5=80.519 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=23:14 IST=> training   36.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.233 Loss=1.819 Prec@1=57.130 Prec@5=80.499 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=23:14 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.233 Loss=1.819 Prec@1=57.130 Prec@5=80.499 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=23:14 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.338 DataTime=0.233 Loss=1.819 Prec@1=57.130 Prec@5=80.499 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=23:14 IST=> training   39.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.232 Loss=1.821 Prec@1=57.112 Prec@5=80.475 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=23:14 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.232 Loss=1.821 Prec@1=57.112 Prec@5=80.475 rate=3.02 Hz, eta=0:07:44, total=0:06:04, wall=23:14 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.232 Loss=1.821 Prec@1=57.112 Prec@5=80.475 rate=3.02 Hz, eta=0:07:44, total=0:06:04, wall=23:15 IST=> training   43.99% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.822 Prec@1=57.097 Prec@5=80.465 rate=3.02 Hz, eta=0:07:44, total=0:06:04, wall=23:15 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.822 Prec@1=57.097 Prec@5=80.465 rate=3.03 Hz, eta=0:07:10, total=0:06:36, wall=23:15 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.822 Prec@1=57.097 Prec@5=80.465 rate=3.03 Hz, eta=0:07:10, total=0:06:36, wall=23:15 IST=> training   47.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.231 Loss=1.823 Prec@1=57.086 Prec@5=80.461 rate=3.03 Hz, eta=0:07:10, total=0:06:36, wall=23:15 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.231 Loss=1.823 Prec@1=57.086 Prec@5=80.461 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=23:15 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.231 Loss=1.823 Prec@1=57.086 Prec@5=80.461 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=23:16 IST=> training   51.98% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.825 Prec@1=57.036 Prec@5=80.418 rate=3.01 Hz, eta=0:06:38, total=0:07:11, wall=23:16 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.825 Prec@1=57.036 Prec@5=80.418 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:16 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.825 Prec@1=57.036 Prec@5=80.418 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:16 IST=> training   55.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.827 Prec@1=57.009 Prec@5=80.399 rate=3.02 Hz, eta=0:06:05, total=0:07:44, wall=23:16 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.827 Prec@1=57.009 Prec@5=80.399 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=23:16 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.827 Prec@1=57.009 Prec@5=80.399 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=23:17 IST=> training   59.97% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.230 Loss=1.828 Prec@1=56.980 Prec@5=80.390 rate=3.02 Hz, eta=0:05:31, total=0:08:17, wall=23:17 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.230 Loss=1.828 Prec@1=56.980 Prec@5=80.390 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:17 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.336 DataTime=0.230 Loss=1.828 Prec@1=56.980 Prec@5=80.390 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:17 IST=> training   63.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.229 Loss=1.829 Prec@1=56.954 Prec@5=80.362 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=23:17 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.229 Loss=1.829 Prec@1=56.954 Prec@5=80.362 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=23:17 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.229 Loss=1.829 Prec@1=56.954 Prec@5=80.362 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=23:18 IST=> training   67.96% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.951 Prec@5=80.352 rate=3.02 Hz, eta=0:04:25, total=0:09:23, wall=23:18 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.951 Prec@5=80.352 rate=3.02 Hz, eta=0:03:52, total=0:09:57, wall=23:18 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.951 Prec@5=80.352 rate=3.02 Hz, eta=0:03:52, total=0:09:57, wall=23:19 IST=> training   71.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.929 Prec@5=80.337 rate=3.02 Hz, eta=0:03:52, total=0:09:57, wall=23:19 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.929 Prec@5=80.337 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:19 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.830 Prec@1=56.929 Prec@5=80.337 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:19 IST=> training   75.95% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.832 Prec@1=56.913 Prec@5=80.318 rate=3.01 Hz, eta=0:03:20, total=0:10:31, wall=23:19 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.832 Prec@1=56.913 Prec@5=80.318 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:19 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.832 Prec@1=56.913 Prec@5=80.318 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:20 IST=> training   79.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.832 Prec@1=56.900 Prec@5=80.309 rate=3.01 Hz, eta=0:02:46, total=0:11:04, wall=23:20 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.832 Prec@1=56.900 Prec@5=80.309 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:20 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.832 Prec@1=56.900 Prec@5=80.309 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:20 IST=> training   83.94% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.901 Prec@5=80.288 rate=3.01 Hz, eta=0:02:13, total=0:11:37, wall=23:20 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.901 Prec@5=80.288 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:20 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.901 Prec@5=80.288 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:21 IST=> training   87.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.886 Prec@5=80.287 rate=3.01 Hz, eta=0:01:40, total=0:12:11, wall=23:21 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.886 Prec@5=80.287 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:21 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.230 Loss=1.833 Prec@1=56.886 Prec@5=80.287 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:21 IST=> training   91.93% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.833 Prec@1=56.875 Prec@5=80.279 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=23:21 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.833 Prec@1=56.875 Prec@5=80.279 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=23:21 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.335 DataTime=0.231 Loss=1.833 Prec@1=56.875 Prec@5=80.279 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=23:22 IST=> training   95.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.834 Prec@1=56.867 Prec@5=80.276 rate=3.01 Hz, eta=0:00:33, total=0:13:18, wall=23:22 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.834 Prec@1=56.867 Prec@5=80.276 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:22 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.834 Prec@1=56.867 Prec@5=80.276 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:22 IST=> training   99.92% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.834 Prec@1=56.867 Prec@5=80.275 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:22 IST=> training   100.00% of 1x2503...Epoch=98/150 LR=0.02777 Time=0.334 DataTime=0.230 Loss=1.834 Prec@1=56.867 Prec@5=80.275 rate=3.01 Hz, eta=0:00:00, total=0:13:50, wall=23:22 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST=> validation 0.00% of 1x98...Epoch=98/150 LR=0.02777 Time=7.066 Loss=1.737 Prec@1=60.938 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST=> validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=7.066 Loss=1.737 Prec@1=60.938 Prec@5=81.055 rate=3111.67 Hz, eta=0:00:00, total=0:00:00, wall=23:22 IST** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=7.066 Loss=1.737 Prec@1=60.938 Prec@5=81.055 rate=3111.67 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST** validation 1.02% of 1x98...Epoch=98/150 LR=0.02777 Time=0.399 Loss=1.928 Prec@1=54.868 Prec@5=79.054 rate=3111.67 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST** validation 100.00% of 1x98...Epoch=98/150 LR=0.02777 Time=0.399 Loss=1.928 Prec@1=54.868 Prec@5=79.054 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=23:23 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST=> training   0.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=5.589 DataTime=5.448 Loss=1.741 Prec@1=56.836 Prec@5=80.859 rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=5.589 DataTime=5.448 Loss=1.741 Prec@1=56.836 Prec@5=80.859 rate=4186.13 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=5.589 DataTime=5.448 Loss=1.741 Prec@1=56.836 Prec@5=80.859 rate=4186.13 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST=> training   0.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.384 DataTime=0.286 Loss=1.797 Prec@1=57.478 Prec@5=80.890 rate=4186.13 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.384 DataTime=0.286 Loss=1.797 Prec@1=57.478 Prec@5=80.890 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=23:23 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.384 DataTime=0.286 Loss=1.797 Prec@1=57.478 Prec@5=80.890 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=23:24 IST=> training   4.04% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.363 DataTime=0.261 Loss=1.803 Prec@1=57.332 Prec@5=80.821 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=23:24 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.363 DataTime=0.261 Loss=1.803 Prec@1=57.332 Prec@5=80.821 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=23:24 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.363 DataTime=0.261 Loss=1.803 Prec@1=57.332 Prec@5=80.821 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=23:24 IST=> training   8.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.352 DataTime=0.248 Loss=1.811 Prec@1=57.153 Prec@5=80.695 rate=2.99 Hz, eta=0:12:50, total=0:01:07, wall=23:24 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.352 DataTime=0.248 Loss=1.811 Prec@1=57.153 Prec@5=80.695 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:24 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.352 DataTime=0.248 Loss=1.811 Prec@1=57.153 Prec@5=80.695 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:25 IST=> training   12.03% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.350 DataTime=0.245 Loss=1.810 Prec@1=57.160 Prec@5=80.651 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=23:25 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.350 DataTime=0.245 Loss=1.810 Prec@1=57.160 Prec@5=80.651 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=23:25 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.350 DataTime=0.245 Loss=1.810 Prec@1=57.160 Prec@5=80.651 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=23:26 IST=> training   16.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.347 DataTime=0.242 Loss=1.811 Prec@1=57.178 Prec@5=80.641 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=23:26 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.347 DataTime=0.242 Loss=1.811 Prec@1=57.178 Prec@5=80.641 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=23:26 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.347 DataTime=0.242 Loss=1.811 Prec@1=57.178 Prec@5=80.641 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=23:26 IST=> training   20.02% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.812 Prec@1=57.182 Prec@5=80.636 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=23:26 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.812 Prec@1=57.182 Prec@5=80.636 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=23:26 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.812 Prec@1=57.182 Prec@5=80.636 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=23:27 IST=> training   24.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.814 Prec@1=57.178 Prec@5=80.553 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=23:27 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.814 Prec@1=57.178 Prec@5=80.553 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=23:27 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.346 DataTime=0.241 Loss=1.814 Prec@1=57.178 Prec@5=80.553 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=23:27 IST=> training   28.01% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.345 DataTime=0.240 Loss=1.813 Prec@1=57.229 Prec@5=80.576 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=23:27 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.345 DataTime=0.240 Loss=1.813 Prec@1=57.229 Prec@5=80.576 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=23:27 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.345 DataTime=0.240 Loss=1.813 Prec@1=57.229 Prec@5=80.576 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=23:28 IST=> training   32.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.344 DataTime=0.239 Loss=1.814 Prec@1=57.163 Prec@5=80.558 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=23:28 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.344 DataTime=0.239 Loss=1.814 Prec@1=57.163 Prec@5=80.558 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=23:28 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.344 DataTime=0.239 Loss=1.814 Prec@1=57.163 Prec@5=80.558 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=23:28 IST=> training   36.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.343 DataTime=0.238 Loss=1.815 Prec@1=57.165 Prec@5=80.558 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=23:28 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.343 DataTime=0.238 Loss=1.815 Prec@1=57.165 Prec@5=80.558 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=23:28 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.343 DataTime=0.238 Loss=1.815 Prec@1=57.165 Prec@5=80.558 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=23:29 IST=> training   39.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.141 Prec@5=80.562 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=23:29 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.141 Prec@5=80.562 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=23:29 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.141 Prec@5=80.562 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=23:29 IST=> training   43.99% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.145 Prec@5=80.554 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=23:29 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.145 Prec@5=80.554 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=23:29 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.145 Prec@5=80.554 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=23:30 IST=> training   47.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.161 Prec@5=80.553 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=23:30 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.161 Prec@5=80.553 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=23:30 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.816 Prec@1=57.161 Prec@5=80.553 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=23:31 IST=> training   51.98% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.817 Prec@1=57.134 Prec@5=80.522 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=23:31 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.817 Prec@1=57.134 Prec@5=80.522 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=23:31 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.817 Prec@1=57.134 Prec@5=80.522 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=23:31 IST=> training   55.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.820 Prec@1=57.091 Prec@5=80.479 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=23:31 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.820 Prec@1=57.091 Prec@5=80.479 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=23:31 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.820 Prec@1=57.091 Prec@5=80.479 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=23:32 IST=> training   59.97% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.821 Prec@1=57.071 Prec@5=80.474 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=23:32 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.821 Prec@1=57.071 Prec@5=80.474 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=23:32 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.821 Prec@1=57.071 Prec@5=80.474 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=23:32 IST=> training   63.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.823 Prec@1=57.026 Prec@5=80.443 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=23:32 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.823 Prec@1=57.026 Prec@5=80.443 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=23:32 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.342 DataTime=0.238 Loss=1.823 Prec@1=57.026 Prec@5=80.443 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=23:33 IST=> training   67.96% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.019 Prec@5=80.438 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=23:33 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.019 Prec@5=80.438 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=23:33 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.019 Prec@5=80.438 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=23:33 IST=> training   71.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.027 Prec@5=80.432 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=23:33 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.027 Prec@5=80.432 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=23:33 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.823 Prec@1=57.027 Prec@5=80.432 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=23:34 IST=> training   75.95% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.824 Prec@1=57.029 Prec@5=80.423 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=23:34 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.824 Prec@1=57.029 Prec@5=80.423 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=23:34 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.824 Prec@1=57.029 Prec@5=80.423 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=23:35 IST=> training   79.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.340 DataTime=0.237 Loss=1.825 Prec@1=57.003 Prec@5=80.416 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=23:35 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.340 DataTime=0.237 Loss=1.825 Prec@1=57.003 Prec@5=80.416 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=23:35 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.340 DataTime=0.237 Loss=1.825 Prec@1=57.003 Prec@5=80.416 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=23:35 IST=> training   83.94% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.826 Prec@1=56.998 Prec@5=80.402 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=23:35 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.826 Prec@1=56.998 Prec@5=80.402 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=23:35 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.826 Prec@1=56.998 Prec@5=80.402 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=23:36 IST=> training   87.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.827 Prec@1=56.981 Prec@5=80.377 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=23:36 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.827 Prec@1=56.981 Prec@5=80.377 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=23:36 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.238 Loss=1.827 Prec@1=56.981 Prec@5=80.377 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=23:36 IST=> training   91.93% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.828 Prec@1=56.956 Prec@5=80.368 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=23:36 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.828 Prec@1=56.956 Prec@5=80.368 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=23:36 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.828 Prec@1=56.956 Prec@5=80.368 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=23:37 IST=> training   95.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.827 Prec@1=56.958 Prec@5=80.374 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=23:37 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.827 Prec@1=56.958 Prec@5=80.374 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=23:37 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.827 Prec@1=56.958 Prec@5=80.374 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=23:37 IST=> training   99.92% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.827 Prec@1=56.960 Prec@5=80.376 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=23:37 IST=> training   100.00% of 1x2503...Epoch=99/150 LR=0.02684 Time=0.341 DataTime=0.237 Loss=1.827 Prec@1=56.960 Prec@5=80.376 rate=2.95 Hz, eta=0:00:00, total=0:14:07, wall=23:37 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:37 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:37 IST=> validation 0.00% of 1x98...Epoch=99/150 LR=0.02684 Time=6.695 Loss=2.414 Prec@1=46.289 Prec@5=72.070 rate=0 Hz, eta=?, total=0:00:00, wall=23:37 IST=> validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=6.695 Loss=2.414 Prec@1=46.289 Prec@5=72.070 rate=5789.62 Hz, eta=0:00:00, total=0:00:00, wall=23:37 IST** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=6.695 Loss=2.414 Prec@1=46.289 Prec@5=72.070 rate=5789.62 Hz, eta=0:00:00, total=0:00:00, wall=23:38 IST** validation 1.02% of 1x98...Epoch=99/150 LR=0.02684 Time=0.416 Loss=2.331 Prec@1=47.600 Prec@5=72.620 rate=5789.62 Hz, eta=0:00:00, total=0:00:00, wall=23:38 IST** validation 100.00% of 1x98...Epoch=99/150 LR=0.02684 Time=0.416 Loss=2.331 Prec@1=47.600 Prec@5=72.620 rate=2.87 Hz, eta=0:00:00, total=0:00:34, wall=23:38 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:38 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:38 IST=> training   0.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.629 DataTime=5.457 Loss=1.982 Prec@1=53.320 Prec@5=78.125 rate=0 Hz, eta=?, total=0:00:00, wall=23:38 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.629 DataTime=5.457 Loss=1.982 Prec@1=53.320 Prec@5=78.125 rate=4841.75 Hz, eta=0:00:00, total=0:00:00, wall=23:38 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=5.629 DataTime=5.457 Loss=1.982 Prec@1=53.320 Prec@5=78.125 rate=4841.75 Hz, eta=0:00:00, total=0:00:00, wall=23:38 IST=> training   0.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.378 DataTime=0.280 Loss=1.818 Prec@1=57.056 Prec@5=80.521 rate=4841.75 Hz, eta=0:00:00, total=0:00:00, wall=23:38 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.378 DataTime=0.280 Loss=1.818 Prec@1=57.056 Prec@5=80.521 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=23:38 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.378 DataTime=0.280 Loss=1.818 Prec@1=57.056 Prec@5=80.521 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=23:39 IST=> training   4.04% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.355 DataTime=0.256 Loss=1.810 Prec@1=57.346 Prec@5=80.612 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=23:39 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.355 DataTime=0.256 Loss=1.810 Prec@1=57.346 Prec@5=80.612 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=23:39 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.355 DataTime=0.256 Loss=1.810 Prec@1=57.346 Prec@5=80.612 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=23:39 IST=> training   8.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.245 Loss=1.811 Prec@1=57.343 Prec@5=80.577 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=23:39 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.245 Loss=1.811 Prec@1=57.343 Prec@5=80.577 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=23:39 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.245 Loss=1.811 Prec@1=57.343 Prec@5=80.577 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=23:40 IST=> training   12.03% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.243 Loss=1.806 Prec@1=57.398 Prec@5=80.671 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=23:40 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.243 Loss=1.806 Prec@1=57.398 Prec@5=80.671 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:40 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.347 DataTime=0.243 Loss=1.806 Prec@1=57.398 Prec@5=80.671 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:40 IST=> training   16.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.343 DataTime=0.239 Loss=1.810 Prec@1=57.314 Prec@5=80.648 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:40 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.343 DataTime=0.239 Loss=1.810 Prec@1=57.314 Prec@5=80.648 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=23:40 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.343 DataTime=0.239 Loss=1.810 Prec@1=57.314 Prec@5=80.648 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=23:41 IST=> training   20.02% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.237 Loss=1.811 Prec@1=57.257 Prec@5=80.632 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=23:41 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.237 Loss=1.811 Prec@1=57.257 Prec@5=80.632 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=23:41 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.237 Loss=1.811 Prec@1=57.257 Prec@5=80.632 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=23:42 IST=> training   24.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.342 DataTime=0.237 Loss=1.812 Prec@1=57.264 Prec@5=80.609 rate=3.01 Hz, eta=0:10:31, total=0:03:19, wall=23:42 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.342 DataTime=0.237 Loss=1.812 Prec@1=57.264 Prec@5=80.609 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=23:42 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.342 DataTime=0.237 Loss=1.812 Prec@1=57.264 Prec@5=80.609 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=23:42 IST=> training   28.01% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.236 Loss=1.809 Prec@1=57.313 Prec@5=80.649 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=23:42 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.236 Loss=1.809 Prec@1=57.313 Prec@5=80.649 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=23:42 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.236 Loss=1.809 Prec@1=57.313 Prec@5=80.649 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=23:43 IST=> training   32.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.235 Loss=1.809 Prec@1=57.329 Prec@5=80.650 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=23:43 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.235 Loss=1.809 Prec@1=57.329 Prec@5=80.650 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:43 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.235 Loss=1.809 Prec@1=57.329 Prec@5=80.650 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:43 IST=> training   36.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.235 Loss=1.810 Prec@1=57.316 Prec@5=80.648 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:43 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.235 Loss=1.810 Prec@1=57.316 Prec@5=80.648 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:43 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.341 DataTime=0.235 Loss=1.810 Prec@1=57.316 Prec@5=80.648 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:44 IST=> training   39.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.234 Loss=1.811 Prec@1=57.290 Prec@5=80.635 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=23:44 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.234 Loss=1.811 Prec@1=57.290 Prec@5=80.635 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=23:44 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.340 DataTime=0.234 Loss=1.811 Prec@1=57.290 Prec@5=80.635 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=23:44 IST=> training   43.99% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.812 Prec@1=57.257 Prec@5=80.639 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=23:44 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.812 Prec@1=57.257 Prec@5=80.639 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=23:44 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.812 Prec@1=57.257 Prec@5=80.639 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=23:45 IST=> training   47.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.813 Prec@1=57.228 Prec@5=80.626 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=23:45 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.813 Prec@1=57.228 Prec@5=80.626 rate=2.98 Hz, eta=0:06:42, total=0:07:15, wall=23:45 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.233 Loss=1.813 Prec@1=57.228 Prec@5=80.626 rate=2.98 Hz, eta=0:06:42, total=0:07:15, wall=23:45 IST=> training   51.98% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.196 Prec@5=80.607 rate=2.98 Hz, eta=0:06:42, total=0:07:15, wall=23:45 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.196 Prec@5=80.607 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=23:45 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.196 Prec@5=80.607 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=23:46 IST=> training   55.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.178 Prec@5=80.601 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=23:46 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.178 Prec@5=80.601 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=23:46 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.814 Prec@1=57.178 Prec@5=80.601 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=23:47 IST=> training   59.97% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.815 Prec@1=57.156 Prec@5=80.588 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=23:47 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.815 Prec@1=57.156 Prec@5=80.588 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:47 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.815 Prec@1=57.156 Prec@5=80.588 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:47 IST=> training   63.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.816 Prec@1=57.124 Prec@5=80.564 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=23:47 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.816 Prec@1=57.124 Prec@5=80.564 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:47 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.816 Prec@1=57.124 Prec@5=80.564 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:48 IST=> training   67.96% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.817 Prec@1=57.116 Prec@5=80.560 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=23:48 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.817 Prec@1=57.116 Prec@5=80.560 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=23:48 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.338 DataTime=0.231 Loss=1.817 Prec@1=57.116 Prec@5=80.560 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=23:48 IST=> training   71.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.817 Prec@1=57.110 Prec@5=80.553 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=23:48 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.817 Prec@1=57.110 Prec@5=80.553 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:48 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.817 Prec@1=57.110 Prec@5=80.553 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:49 IST=> training   75.95% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.097 Prec@5=80.545 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=23:49 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.097 Prec@5=80.545 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=23:49 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.097 Prec@5=80.545 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=23:49 IST=> training   79.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.818 Prec@1=57.099 Prec@5=80.556 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=23:49 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.818 Prec@1=57.099 Prec@5=80.556 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=23:49 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.818 Prec@1=57.099 Prec@5=80.556 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=23:50 IST=> training   83.94% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.083 Prec@5=80.545 rate=2.98 Hz, eta=0:02:15, total=0:11:46, wall=23:50 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.083 Prec@5=80.545 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:50 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.818 Prec@1=57.083 Prec@5=80.545 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:51 IST=> training   87.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.819 Prec@1=57.081 Prec@5=80.531 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=23:51 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.819 Prec@1=57.081 Prec@5=80.531 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=23:51 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.819 Prec@1=57.081 Prec@5=80.531 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=23:51 IST=> training   91.93% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.820 Prec@1=57.078 Prec@5=80.515 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=23:51 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.820 Prec@1=57.078 Prec@5=80.515 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=23:51 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.232 Loss=1.820 Prec@1=57.078 Prec@5=80.515 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=23:52 IST=> training   95.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.820 Prec@1=57.065 Prec@5=80.504 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=23:52 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.820 Prec@1=57.065 Prec@5=80.504 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=23:52 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.820 Prec@1=57.065 Prec@5=80.504 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=23:52 IST=> training   99.92% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.821 Prec@1=57.064 Prec@5=80.503 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=23:52 IST=> training   100.00% of 1x2503...Epoch=100/150 LR=0.02591 Time=0.339 DataTime=0.231 Loss=1.821 Prec@1=57.064 Prec@5=80.503 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=23:52 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> validation 0.00% of 1x98...Epoch=100/150 LR=0.02591 Time=5.920 Loss=1.735 Prec@1=57.422 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=5.920 Loss=1.735 Prec@1=57.422 Prec@5=83.008 rate=5986.34 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=5.920 Loss=1.735 Prec@1=57.422 Prec@5=83.008 rate=5986.34 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST** validation 1.02% of 1x98...Epoch=100/150 LR=0.02591 Time=0.405 Loss=1.885 Prec@1=55.862 Prec@5=79.770 rate=5986.34 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST** validation 100.00% of 1x98...Epoch=100/150 LR=0.02591 Time=0.405 Loss=1.885 Prec@1=55.862 Prec@5=79.770 rate=2.90 Hz, eta=0:00:00, total=0:00:33, wall=23:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> training   0.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.375 DataTime=5.153 Loss=1.744 Prec@1=59.570 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=23:52 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.375 DataTime=5.153 Loss=1.744 Prec@1=59.570 Prec@5=79.883 rate=4014.55 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=5.375 DataTime=5.153 Loss=1.744 Prec@1=59.570 Prec@5=79.883 rate=4014.55 Hz, eta=0:00:00, total=0:00:00, wall=23:53 IST=> training   0.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.382 DataTime=0.290 Loss=1.800 Prec@1=57.526 Prec@5=80.672 rate=4014.55 Hz, eta=0:00:00, total=0:00:00, wall=23:53 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.382 DataTime=0.290 Loss=1.800 Prec@1=57.526 Prec@5=80.672 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=23:53 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.382 DataTime=0.290 Loss=1.800 Prec@1=57.526 Prec@5=80.672 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=23:54 IST=> training   4.04% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.356 DataTime=0.260 Loss=1.794 Prec@1=57.594 Prec@5=80.844 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=23:54 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.356 DataTime=0.260 Loss=1.794 Prec@1=57.594 Prec@5=80.844 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=23:54 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.356 DataTime=0.260 Loss=1.794 Prec@1=57.594 Prec@5=80.844 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=23:54 IST=> training   8.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.349 DataTime=0.253 Loss=1.792 Prec@1=57.574 Prec@5=80.876 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=23:54 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.349 DataTime=0.253 Loss=1.792 Prec@1=57.574 Prec@5=80.876 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=23:54 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.349 DataTime=0.253 Loss=1.792 Prec@1=57.574 Prec@5=80.876 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=23:55 IST=> training   12.03% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.347 DataTime=0.249 Loss=1.794 Prec@1=57.585 Prec@5=80.855 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=23:55 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.347 DataTime=0.249 Loss=1.794 Prec@1=57.585 Prec@5=80.855 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:55 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.347 DataTime=0.249 Loss=1.794 Prec@1=57.585 Prec@5=80.855 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:55 IST=> training   16.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.346 DataTime=0.247 Loss=1.793 Prec@1=57.593 Prec@5=80.870 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=23:55 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.346 DataTime=0.247 Loss=1.793 Prec@1=57.593 Prec@5=80.870 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=23:55 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.346 DataTime=0.247 Loss=1.793 Prec@1=57.593 Prec@5=80.870 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=23:56 IST=> training   20.02% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.344 DataTime=0.243 Loss=1.794 Prec@1=57.560 Prec@5=80.853 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=23:56 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.344 DataTime=0.243 Loss=1.794 Prec@1=57.560 Prec@5=80.853 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=23:56 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.344 DataTime=0.243 Loss=1.794 Prec@1=57.560 Prec@5=80.853 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=23:56 IST=> training   24.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.239 Loss=1.795 Prec@1=57.513 Prec@5=80.848 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=23:56 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.239 Loss=1.795 Prec@1=57.513 Prec@5=80.848 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=23:56 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.239 Loss=1.795 Prec@1=57.513 Prec@5=80.848 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=23:57 IST=> training   28.01% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.341 DataTime=0.240 Loss=1.796 Prec@1=57.524 Prec@5=80.856 rate=3.01 Hz, eta=0:09:59, total=0:03:53, wall=23:57 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.341 DataTime=0.240 Loss=1.796 Prec@1=57.524 Prec@5=80.856 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:57 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.341 DataTime=0.240 Loss=1.796 Prec@1=57.524 Prec@5=80.856 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:57 IST=> training   32.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.238 Loss=1.797 Prec@1=57.463 Prec@5=80.821 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=23:57 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.238 Loss=1.797 Prec@1=57.463 Prec@5=80.821 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:57 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.238 Loss=1.797 Prec@1=57.463 Prec@5=80.821 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:58 IST=> training   36.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.237 Loss=1.797 Prec@1=57.463 Prec@5=80.810 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=23:58 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.237 Loss=1.797 Prec@1=57.463 Prec@5=80.810 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:58 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.237 Loss=1.797 Prec@1=57.463 Prec@5=80.810 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:59 IST=> training   39.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.237 Loss=1.798 Prec@1=57.439 Prec@5=80.802 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=23:59 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.237 Loss=1.798 Prec@1=57.439 Prec@5=80.802 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=23:59 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.237 Loss=1.798 Prec@1=57.439 Prec@5=80.802 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=23:59 IST=> training   43.99% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.801 Prec@1=57.394 Prec@5=80.774 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=23:59 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.801 Prec@1=57.394 Prec@5=80.774 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=23:59 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.801 Prec@1=57.394 Prec@5=80.774 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=00:00 IST=> training   47.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.235 Loss=1.802 Prec@1=57.386 Prec@5=80.755 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=00:00 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.235 Loss=1.802 Prec@1=57.386 Prec@5=80.755 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=00:00 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.235 Loss=1.802 Prec@1=57.386 Prec@5=80.755 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=00:00 IST=> training   51.98% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.803 Prec@1=57.379 Prec@5=80.733 rate=2.99 Hz, eta=0:06:41, total=0:07:14, wall=00:00 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.803 Prec@1=57.379 Prec@5=80.733 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=00:00 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.803 Prec@1=57.379 Prec@5=80.733 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=00:01 IST=> training   55.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.805 Prec@1=57.350 Prec@5=80.708 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=00:01 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.805 Prec@1=57.350 Prec@5=80.708 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=00:01 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.805 Prec@1=57.350 Prec@5=80.708 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=00:01 IST=> training   59.97% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.806 Prec@1=57.340 Prec@5=80.697 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=00:01 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.806 Prec@1=57.340 Prec@5=80.697 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=00:01 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.806 Prec@1=57.340 Prec@5=80.697 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=00:02 IST=> training   63.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.807 Prec@1=57.323 Prec@5=80.684 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=00:02 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.807 Prec@1=57.323 Prec@5=80.684 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=00:02 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.807 Prec@1=57.323 Prec@5=80.684 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=00:03 IST=> training   67.96% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.808 Prec@1=57.308 Prec@5=80.663 rate=2.97 Hz, eta=0:04:29, total=0:09:31, wall=00:03 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.808 Prec@1=57.308 Prec@5=80.663 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:03 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.808 Prec@1=57.308 Prec@5=80.663 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:03 IST=> training   71.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.809 Prec@1=57.277 Prec@5=80.634 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=00:03 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.809 Prec@1=57.277 Prec@5=80.634 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=00:03 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.236 Loss=1.809 Prec@1=57.277 Prec@5=80.634 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=00:04 IST=> training   75.95% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.810 Prec@1=57.257 Prec@5=80.628 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=00:04 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.810 Prec@1=57.257 Prec@5=80.628 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:04 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.810 Prec@1=57.257 Prec@5=80.628 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:04 IST=> training   79.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.811 Prec@1=57.242 Prec@5=80.617 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:04 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.811 Prec@1=57.242 Prec@5=80.617 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:04 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.340 DataTime=0.236 Loss=1.811 Prec@1=57.242 Prec@5=80.617 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:05 IST=> training   83.94% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.243 Prec@5=80.604 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:05 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.243 Prec@5=80.604 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:05 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.243 Prec@5=80.604 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:05 IST=> training   87.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.229 Prec@5=80.604 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:05 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.229 Prec@5=80.604 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:05 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.229 Prec@5=80.604 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:06 IST=> training   91.93% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.244 Prec@5=80.606 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.244 Prec@5=80.606 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.235 Loss=1.812 Prec@1=57.244 Prec@5=80.606 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:06 IST=> training   95.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.234 Loss=1.812 Prec@1=57.241 Prec@5=80.593 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=00:06 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.234 Loss=1.812 Prec@1=57.241 Prec@5=80.593 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=00:06 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.338 DataTime=0.234 Loss=1.812 Prec@1=57.241 Prec@5=80.593 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=00:07 IST=> training   99.92% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.234 Loss=1.813 Prec@1=57.241 Prec@5=80.592 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=00:07 IST=> training   100.00% of 1x2503...Epoch=101/150 LR=0.02500 Time=0.339 DataTime=0.234 Loss=1.813 Prec@1=57.241 Prec@5=80.592 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:07 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 0.00% of 1x98...Epoch=101/150 LR=0.02500 Time=7.487 Loss=1.863 Prec@1=54.883 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=7.487 Loss=1.863 Prec@1=54.883 Prec@5=80.664 rate=5099.86 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=7.487 Loss=1.863 Prec@1=54.883 Prec@5=80.664 rate=5099.86 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST** validation 1.02% of 1x98...Epoch=101/150 LR=0.02500 Time=0.403 Loss=1.864 Prec@1=55.986 Prec@5=80.008 rate=5099.86 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST** validation 100.00% of 1x98...Epoch=101/150 LR=0.02500 Time=0.403 Loss=1.864 Prec@1=55.986 Prec@5=80.008 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=00:07 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=5.682 DataTime=5.516 Loss=1.933 Prec@1=56.641 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=00:07 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=5.682 DataTime=5.516 Loss=1.933 Prec@1=56.641 Prec@5=77.148 rate=3807.70 Hz, eta=0:00:00, total=0:00:00, wall=00:07 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=5.682 DataTime=5.516 Loss=1.933 Prec@1=56.641 Prec@5=77.148 rate=3807.70 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST=> training   0.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.377 DataTime=0.281 Loss=1.782 Prec@1=57.961 Prec@5=81.072 rate=3807.70 Hz, eta=0:00:00, total=0:00:00, wall=00:08 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.377 DataTime=0.281 Loss=1.782 Prec@1=57.961 Prec@5=81.072 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=00:08 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.377 DataTime=0.281 Loss=1.782 Prec@1=57.961 Prec@5=81.072 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=00:08 IST=> training   4.04% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.353 DataTime=0.258 Loss=1.791 Prec@1=57.819 Prec@5=80.946 rate=3.12 Hz, eta=0:12:50, total=0:00:32, wall=00:08 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.353 DataTime=0.258 Loss=1.791 Prec@1=57.819 Prec@5=80.946 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=00:08 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.353 DataTime=0.258 Loss=1.791 Prec@1=57.819 Prec@5=80.946 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=00:09 IST=> training   8.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.347 DataTime=0.249 Loss=1.793 Prec@1=57.712 Prec@5=80.916 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=00:09 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.347 DataTime=0.249 Loss=1.793 Prec@1=57.712 Prec@5=80.916 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=00:09 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.347 DataTime=0.249 Loss=1.793 Prec@1=57.712 Prec@5=80.916 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=00:10 IST=> training   12.03% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.348 DataTime=0.249 Loss=1.795 Prec@1=57.662 Prec@5=80.916 rate=3.05 Hz, eta=0:12:01, total=0:01:38, wall=00:10 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.348 DataTime=0.249 Loss=1.795 Prec@1=57.662 Prec@5=80.916 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=00:10 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.348 DataTime=0.249 Loss=1.795 Prec@1=57.662 Prec@5=80.916 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=00:10 IST=> training   16.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.346 DataTime=0.245 Loss=1.793 Prec@1=57.697 Prec@5=80.907 rate=3.00 Hz, eta=0:11:41, total=0:02:13, wall=00:10 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.346 DataTime=0.245 Loss=1.793 Prec@1=57.697 Prec@5=80.907 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=00:10 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.346 DataTime=0.245 Loss=1.793 Prec@1=57.697 Prec@5=80.907 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=00:11 IST=> training   20.02% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.242 Loss=1.794 Prec@1=57.679 Prec@5=80.851 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=00:11 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.242 Loss=1.794 Prec@1=57.679 Prec@5=80.851 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:11 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.242 Loss=1.794 Prec@1=57.679 Prec@5=80.851 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:11 IST=> training   24.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.241 Loss=1.797 Prec@1=57.598 Prec@5=80.820 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:11 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.241 Loss=1.797 Prec@1=57.598 Prec@5=80.820 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:11 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.344 DataTime=0.241 Loss=1.797 Prec@1=57.598 Prec@5=80.820 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:12 IST=> training   28.01% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.343 DataTime=0.240 Loss=1.796 Prec@1=57.617 Prec@5=80.851 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=00:12 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.343 DataTime=0.240 Loss=1.796 Prec@1=57.617 Prec@5=80.851 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=00:12 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.343 DataTime=0.240 Loss=1.796 Prec@1=57.617 Prec@5=80.851 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=00:12 IST=> training   32.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.238 Loss=1.797 Prec@1=57.574 Prec@5=80.827 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=00:12 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.238 Loss=1.797 Prec@1=57.574 Prec@5=80.827 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:12 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.238 Loss=1.797 Prec@1=57.574 Prec@5=80.827 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:13 IST=> training   36.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.237 Loss=1.799 Prec@1=57.542 Prec@5=80.782 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:13 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.237 Loss=1.799 Prec@1=57.542 Prec@5=80.782 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:13 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.342 DataTime=0.237 Loss=1.799 Prec@1=57.542 Prec@5=80.782 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:13 IST=> training   39.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.341 DataTime=0.236 Loss=1.799 Prec@1=57.533 Prec@5=80.784 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=00:13 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.341 DataTime=0.236 Loss=1.799 Prec@1=57.533 Prec@5=80.784 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=00:13 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.341 DataTime=0.236 Loss=1.799 Prec@1=57.533 Prec@5=80.784 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=00:14 IST=> training   43.99% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.489 Prec@5=80.766 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=00:14 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.489 Prec@5=80.766 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=00:14 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.489 Prec@5=80.766 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=00:15 IST=> training   47.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.483 Prec@5=80.752 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=00:15 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.483 Prec@5=80.752 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=00:15 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.340 DataTime=0.235 Loss=1.801 Prec@1=57.483 Prec@5=80.752 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=00:15 IST=> training   51.98% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.802 Prec@1=57.472 Prec@5=80.734 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=00:15 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.802 Prec@1=57.472 Prec@5=80.734 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=00:15 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.802 Prec@1=57.472 Prec@5=80.734 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=00:16 IST=> training   55.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.233 Loss=1.803 Prec@1=57.456 Prec@5=80.724 rate=2.99 Hz, eta=0:06:09, total=0:07:49, wall=00:16 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.233 Loss=1.803 Prec@1=57.456 Prec@5=80.724 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=00:16 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.233 Loss=1.803 Prec@1=57.456 Prec@5=80.724 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=00:16 IST=> training   59.97% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.803 Prec@1=57.436 Prec@5=80.722 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=00:16 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.803 Prec@1=57.436 Prec@5=80.722 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=00:16 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.233 Loss=1.803 Prec@1=57.436 Prec@5=80.722 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=00:17 IST=> training   63.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.232 Loss=1.804 Prec@1=57.421 Prec@5=80.712 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=00:17 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.232 Loss=1.804 Prec@1=57.421 Prec@5=80.712 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=00:17 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.232 Loss=1.804 Prec@1=57.421 Prec@5=80.712 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=00:17 IST=> training   67.96% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.805 Prec@1=57.403 Prec@5=80.694 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=00:17 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.805 Prec@1=57.403 Prec@5=80.694 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=00:17 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.805 Prec@1=57.403 Prec@5=80.694 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=00:18 IST=> training   71.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.232 Loss=1.805 Prec@1=57.410 Prec@5=80.679 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=00:18 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.232 Loss=1.805 Prec@1=57.410 Prec@5=80.679 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=00:18 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.339 DataTime=0.232 Loss=1.805 Prec@1=57.410 Prec@5=80.679 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=00:18 IST=> training   75.95% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.399 Prec@5=80.674 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=00:18 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.399 Prec@5=80.674 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=00:18 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.399 Prec@5=80.674 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=00:19 IST=> training   79.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.406 Prec@5=80.672 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=00:19 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.406 Prec@5=80.672 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=00:19 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.806 Prec@1=57.406 Prec@5=80.672 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=00:20 IST=> training   83.94% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.382 Prec@5=80.653 rate=2.99 Hz, eta=0:02:14, total=0:11:43, wall=00:20 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.382 Prec@5=80.653 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=00:20 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.382 Prec@5=80.653 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=00:20 IST=> training   87.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.385 Prec@5=80.656 rate=2.98 Hz, eta=0:01:41, total=0:12:18, wall=00:20 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.385 Prec@5=80.656 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=00:20 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.338 DataTime=0.231 Loss=1.807 Prec@1=57.385 Prec@5=80.656 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=00:21 IST=> training   91.93% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.808 Prec@1=57.372 Prec@5=80.644 rate=2.98 Hz, eta=0:01:07, total=0:12:51, wall=00:21 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.808 Prec@1=57.372 Prec@5=80.644 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=00:21 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.808 Prec@1=57.372 Prec@5=80.644 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=00:21 IST=> training   95.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.809 Prec@1=57.353 Prec@5=80.631 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=00:21 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.809 Prec@1=57.353 Prec@5=80.631 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=00:21 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.809 Prec@1=57.353 Prec@5=80.631 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=00:21 IST=> training   99.92% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.809 Prec@1=57.352 Prec@5=80.631 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=00:21 IST=> training   100.00% of 1x2503...Epoch=102/150 LR=0.02410 Time=0.337 DataTime=0.230 Loss=1.809 Prec@1=57.352 Prec@5=80.631 rate=2.99 Hz, eta=0:00:00, total=0:13:56, wall=00:21 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:21 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:21 IST=> validation 0.00% of 1x98...Epoch=102/150 LR=0.02410 Time=6.835 Loss=1.884 Prec@1=55.078 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=00:21 IST=> validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=6.835 Loss=1.884 Prec@1=55.078 Prec@5=81.641 rate=7363.94 Hz, eta=0:00:00, total=0:00:00, wall=00:21 IST** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=6.835 Loss=1.884 Prec@1=55.078 Prec@5=81.641 rate=7363.94 Hz, eta=0:00:00, total=0:00:00, wall=00:22 IST** validation 1.02% of 1x98...Epoch=102/150 LR=0.02410 Time=0.392 Loss=1.859 Prec@1=56.398 Prec@5=80.052 rate=7363.94 Hz, eta=0:00:00, total=0:00:00, wall=00:22 IST** validation 100.00% of 1x98...Epoch=102/150 LR=0.02410 Time=0.392 Loss=1.859 Prec@1=56.398 Prec@5=80.052 rate=3.10 Hz, eta=0:00:00, total=0:00:31, wall=00:22 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:22 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:22 IST=> training   0.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.813 DataTime=4.623 Loss=1.698 Prec@1=61.523 Prec@5=80.859 rate=0 Hz, eta=?, total=0:00:00, wall=00:22 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.813 DataTime=4.623 Loss=1.698 Prec@1=61.523 Prec@5=80.859 rate=4110.94 Hz, eta=0:00:00, total=0:00:00, wall=00:22 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=4.813 DataTime=4.623 Loss=1.698 Prec@1=61.523 Prec@5=80.859 rate=4110.94 Hz, eta=0:00:00, total=0:00:00, wall=00:23 IST=> training   0.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.379 DataTime=0.285 Loss=1.785 Prec@1=57.969 Prec@5=81.163 rate=4110.94 Hz, eta=0:00:00, total=0:00:00, wall=00:23 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.379 DataTime=0.285 Loss=1.785 Prec@1=57.969 Prec@5=81.163 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=00:23 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.379 DataTime=0.285 Loss=1.785 Prec@1=57.969 Prec@5=81.163 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=00:23 IST=> training   4.04% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.353 DataTime=0.258 Loss=1.780 Prec@1=57.977 Prec@5=81.166 rate=3.02 Hz, eta=0:13:16, total=0:00:33, wall=00:23 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.353 DataTime=0.258 Loss=1.780 Prec@1=57.977 Prec@5=81.166 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:23 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.353 DataTime=0.258 Loss=1.780 Prec@1=57.977 Prec@5=81.166 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:24 IST=> training   8.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.348 DataTime=0.251 Loss=1.776 Prec@1=57.965 Prec@5=81.203 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:24 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.348 DataTime=0.251 Loss=1.776 Prec@1=57.965 Prec@5=81.203 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:24 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.348 DataTime=0.251 Loss=1.776 Prec@1=57.965 Prec@5=81.203 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:24 IST=> training   12.03% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.346 DataTime=0.248 Loss=1.775 Prec@1=57.959 Prec@5=81.205 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:24 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.346 DataTime=0.248 Loss=1.775 Prec@1=57.959 Prec@5=81.205 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:24 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.346 DataTime=0.248 Loss=1.775 Prec@1=57.959 Prec@5=81.205 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:25 IST=> training   16.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.244 Loss=1.778 Prec@1=57.924 Prec@5=81.161 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=00:25 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.244 Loss=1.778 Prec@1=57.924 Prec@5=81.161 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:25 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.244 Loss=1.778 Prec@1=57.924 Prec@5=81.161 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:25 IST=> training   20.02% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.241 Loss=1.778 Prec@1=57.952 Prec@5=81.159 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:25 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.241 Loss=1.778 Prec@1=57.952 Prec@5=81.159 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:25 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.241 Loss=1.778 Prec@1=57.952 Prec@5=81.159 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:26 IST=> training   24.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.241 Loss=1.780 Prec@1=57.936 Prec@5=81.128 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=00:26 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.241 Loss=1.780 Prec@1=57.936 Prec@5=81.128 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=00:26 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.344 DataTime=0.241 Loss=1.780 Prec@1=57.936 Prec@5=81.128 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=00:27 IST=> training   28.01% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.240 Loss=1.781 Prec@1=57.941 Prec@5=81.121 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=00:27 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.240 Loss=1.781 Prec@1=57.941 Prec@5=81.121 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:27 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.240 Loss=1.781 Prec@1=57.941 Prec@5=81.121 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:27 IST=> training   32.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.239 Loss=1.783 Prec@1=57.894 Prec@5=81.075 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=00:27 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.239 Loss=1.783 Prec@1=57.894 Prec@5=81.075 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=00:27 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.239 Loss=1.783 Prec@1=57.894 Prec@5=81.075 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=00:28 IST=> training   36.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.240 Loss=1.785 Prec@1=57.832 Prec@5=81.025 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=00:28 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.240 Loss=1.785 Prec@1=57.832 Prec@5=81.025 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=00:28 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.343 DataTime=0.240 Loss=1.785 Prec@1=57.832 Prec@5=81.025 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=00:28 IST=> training   39.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.787 Prec@1=57.818 Prec@5=80.988 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=00:28 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.787 Prec@1=57.818 Prec@5=80.988 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:28 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.787 Prec@1=57.818 Prec@5=80.988 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:29 IST=> training   43.99% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.788 Prec@1=57.779 Prec@5=80.955 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:29 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.788 Prec@1=57.779 Prec@5=80.955 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:29 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.788 Prec@1=57.779 Prec@5=80.955 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:29 IST=> training   47.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.789 Prec@1=57.767 Prec@5=80.933 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=00:29 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.789 Prec@1=57.767 Prec@5=80.933 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=00:29 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.342 DataTime=0.238 Loss=1.789 Prec@1=57.767 Prec@5=80.933 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=00:30 IST=> training   51.98% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.791 Prec@1=57.742 Prec@5=80.923 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=00:30 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.791 Prec@1=57.742 Prec@5=80.923 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=00:30 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.237 Loss=1.791 Prec@1=57.742 Prec@5=80.923 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=00:30 IST=> training   55.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.699 Prec@5=80.887 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=00:30 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.699 Prec@5=80.887 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=00:30 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.699 Prec@5=80.887 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=00:31 IST=> training   59.97% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.677 Prec@5=80.881 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=00:31 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.677 Prec@5=80.881 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=00:31 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.677 Prec@5=80.881 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=00:32 IST=> training   63.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.695 Prec@5=80.882 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=00:32 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.695 Prec@5=80.882 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=00:32 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.792 Prec@1=57.695 Prec@5=80.882 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=00:32 IST=> training   67.96% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.692 Prec@5=80.867 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=00:32 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.692 Prec@5=80.867 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=00:32 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.793 Prec@1=57.692 Prec@5=80.867 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=00:33 IST=> training   71.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.794 Prec@1=57.667 Prec@5=80.852 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=00:33 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.794 Prec@1=57.667 Prec@5=80.852 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=00:33 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.236 Loss=1.794 Prec@1=57.667 Prec@5=80.852 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=00:33 IST=> training   75.95% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.235 Loss=1.794 Prec@1=57.662 Prec@5=80.842 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=00:33 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.235 Loss=1.794 Prec@1=57.662 Prec@5=80.842 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:33 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.341 DataTime=0.235 Loss=1.794 Prec@1=57.662 Prec@5=80.842 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:34 IST=> training   79.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.794 Prec@1=57.652 Prec@5=80.837 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=00:34 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.794 Prec@1=57.652 Prec@5=80.837 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:34 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.794 Prec@1=57.652 Prec@5=80.837 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:34 IST=> training   83.94% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.795 Prec@1=57.625 Prec@5=80.827 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=00:34 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.795 Prec@1=57.625 Prec@5=80.827 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=00:34 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.340 DataTime=0.235 Loss=1.795 Prec@1=57.625 Prec@5=80.827 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=00:35 IST=> training   87.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.796 Prec@1=57.605 Prec@5=80.824 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=00:35 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.796 Prec@1=57.605 Prec@5=80.824 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:35 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.796 Prec@1=57.605 Prec@5=80.824 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:36 IST=> training   91.93% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.797 Prec@1=57.597 Prec@5=80.808 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:36 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.797 Prec@1=57.597 Prec@5=80.808 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:36 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.234 Loss=1.797 Prec@1=57.597 Prec@5=80.808 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:36 IST=> training   95.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.233 Loss=1.797 Prec@1=57.591 Prec@5=80.801 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:36 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.233 Loss=1.797 Prec@1=57.591 Prec@5=80.801 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:36 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.233 Loss=1.797 Prec@1=57.591 Prec@5=80.801 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:36 IST=> training   99.92% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.233 Loss=1.797 Prec@1=57.591 Prec@5=80.799 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:36 IST=> training   100.00% of 1x2503...Epoch=103/150 LR=0.02321 Time=0.339 DataTime=0.233 Loss=1.797 Prec@1=57.591 Prec@5=80.799 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:36 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST=> validation 0.00% of 1x98...Epoch=103/150 LR=0.02321 Time=7.409 Loss=1.754 Prec@1=58.398 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST=> validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.409 Loss=1.754 Prec@1=58.398 Prec@5=80.469 rate=5048.52 Hz, eta=0:00:00, total=0:00:00, wall=00:36 IST** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=7.409 Loss=1.754 Prec@1=58.398 Prec@5=80.469 rate=5048.52 Hz, eta=0:00:00, total=0:00:00, wall=00:37 IST** validation 1.02% of 1x98...Epoch=103/150 LR=0.02321 Time=0.407 Loss=1.881 Prec@1=56.144 Prec@5=79.814 rate=5048.52 Hz, eta=0:00:00, total=0:00:00, wall=00:37 IST** validation 100.00% of 1x98...Epoch=103/150 LR=0.02321 Time=0.407 Loss=1.881 Prec@1=56.144 Prec@5=79.814 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=00:37 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:37 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:37 IST=> training   0.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.056 DataTime=4.874 Loss=1.740 Prec@1=60.352 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=00:37 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.056 DataTime=4.874 Loss=1.740 Prec@1=60.352 Prec@5=79.883 rate=2622.68 Hz, eta=0:00:00, total=0:00:00, wall=00:37 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=5.056 DataTime=4.874 Loss=1.740 Prec@1=60.352 Prec@5=79.883 rate=2622.68 Hz, eta=0:00:00, total=0:00:00, wall=00:37 IST=> training   0.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.378 DataTime=0.277 Loss=1.783 Prec@1=57.708 Prec@5=81.031 rate=2622.68 Hz, eta=0:00:00, total=0:00:00, wall=00:37 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.378 DataTime=0.277 Loss=1.783 Prec@1=57.708 Prec@5=81.031 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=00:37 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.378 DataTime=0.277 Loss=1.783 Prec@1=57.708 Prec@5=81.031 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=00:38 IST=> training   4.04% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.354 DataTime=0.252 Loss=1.783 Prec@1=57.803 Prec@5=81.109 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=00:38 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.354 DataTime=0.252 Loss=1.783 Prec@1=57.803 Prec@5=81.109 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:38 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.354 DataTime=0.252 Loss=1.783 Prec@1=57.803 Prec@5=81.109 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:39 IST=> training   8.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.347 DataTime=0.244 Loss=1.782 Prec@1=57.833 Prec@5=81.109 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=00:39 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.347 DataTime=0.244 Loss=1.782 Prec@1=57.833 Prec@5=81.109 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=00:39 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.347 DataTime=0.244 Loss=1.782 Prec@1=57.833 Prec@5=81.109 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=00:39 IST=> training   12.03% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.343 DataTime=0.240 Loss=1.782 Prec@1=57.787 Prec@5=81.126 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=00:39 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.343 DataTime=0.240 Loss=1.782 Prec@1=57.787 Prec@5=81.126 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=00:39 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.343 DataTime=0.240 Loss=1.782 Prec@1=57.787 Prec@5=81.126 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=00:40 IST=> training   16.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.344 DataTime=0.240 Loss=1.781 Prec@1=57.806 Prec@5=81.094 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=00:40 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.344 DataTime=0.240 Loss=1.781 Prec@1=57.806 Prec@5=81.094 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:40 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.344 DataTime=0.240 Loss=1.781 Prec@1=57.806 Prec@5=81.094 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:40 IST=> training   20.02% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.779 Prec@1=57.835 Prec@5=81.103 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=00:40 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.779 Prec@1=57.835 Prec@5=81.103 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=00:40 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.779 Prec@1=57.835 Prec@5=81.103 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=00:41 IST=> training   24.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.778 Prec@1=57.840 Prec@5=81.090 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=00:41 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.778 Prec@1=57.840 Prec@5=81.090 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=00:41 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.778 Prec@1=57.840 Prec@5=81.090 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=00:41 IST=> training   28.01% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.780 Prec@1=57.839 Prec@5=81.075 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=00:41 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.780 Prec@1=57.839 Prec@5=81.075 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=00:41 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.342 DataTime=0.238 Loss=1.780 Prec@1=57.839 Prec@5=81.075 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=00:42 IST=> training   32.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.780 Prec@1=57.832 Prec@5=81.090 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=00:42 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.780 Prec@1=57.832 Prec@5=81.090 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:42 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.237 Loss=1.780 Prec@1=57.832 Prec@5=81.090 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:42 IST=> training   36.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.092 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=00:42 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.092 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=00:42 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.092 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=00:43 IST=> training   39.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.086 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=00:43 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.086 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:43 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.236 Loss=1.780 Prec@1=57.860 Prec@5=81.086 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:44 IST=> training   43.99% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.781 Prec@1=57.853 Prec@5=81.069 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=00:44 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.781 Prec@1=57.853 Prec@5=81.069 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:44 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.236 Loss=1.781 Prec@1=57.853 Prec@5=81.069 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:44 IST=> training   47.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.235 Loss=1.782 Prec@1=57.856 Prec@5=81.048 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=00:44 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.235 Loss=1.782 Prec@1=57.856 Prec@5=81.048 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=00:44 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.235 Loss=1.782 Prec@1=57.856 Prec@5=81.048 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=00:45 IST=> training   51.98% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.235 Loss=1.784 Prec@1=57.836 Prec@5=81.028 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=00:45 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.235 Loss=1.784 Prec@1=57.836 Prec@5=81.028 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:45 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.341 DataTime=0.235 Loss=1.784 Prec@1=57.836 Prec@5=81.028 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:45 IST=> training   55.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.234 Loss=1.783 Prec@1=57.856 Prec@5=81.031 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=00:45 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.234 Loss=1.783 Prec@1=57.856 Prec@5=81.031 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:45 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.234 Loss=1.783 Prec@1=57.856 Prec@5=81.031 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:46 IST=> training   59.97% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.785 Prec@1=57.822 Prec@5=81.002 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=00:46 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.785 Prec@1=57.822 Prec@5=81.002 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:46 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.785 Prec@1=57.822 Prec@5=81.002 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:46 IST=> training   63.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.785 Prec@1=57.816 Prec@5=80.982 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=00:46 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.785 Prec@1=57.816 Prec@5=80.982 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:46 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.785 Prec@1=57.816 Prec@5=80.982 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:47 IST=> training   67.96% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.787 Prec@1=57.796 Prec@5=80.962 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=00:47 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.787 Prec@1=57.796 Prec@5=80.962 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:47 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.787 Prec@1=57.796 Prec@5=80.962 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:48 IST=> training   71.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.787 Prec@1=57.776 Prec@5=80.960 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=00:48 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.787 Prec@1=57.776 Prec@5=80.960 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:48 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.233 Loss=1.787 Prec@1=57.776 Prec@5=80.960 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:48 IST=> training   75.95% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.933 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=00:48 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.933 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:48 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.933 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:49 IST=> training   79.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.939 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=00:49 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.939 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:49 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.789 Prec@1=57.739 Prec@5=80.939 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:49 IST=> training   83.94% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.790 Prec@1=57.728 Prec@5=80.928 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=00:49 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.790 Prec@1=57.728 Prec@5=80.928 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:49 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.790 Prec@1=57.728 Prec@5=80.928 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:50 IST=> training   87.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.705 Prec@5=80.900 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=00:50 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.705 Prec@5=80.900 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:50 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.705 Prec@5=80.900 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:50 IST=> training   91.93% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.708 Prec@5=80.894 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=00:50 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.708 Prec@5=80.894 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:50 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.340 DataTime=0.233 Loss=1.791 Prec@1=57.708 Prec@5=80.894 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:51 IST=> training   95.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.792 Prec@1=57.696 Prec@5=80.874 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=00:51 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.792 Prec@1=57.696 Prec@5=80.874 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:51 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.792 Prec@1=57.696 Prec@5=80.874 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:51 IST=> training   99.92% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.792 Prec@1=57.696 Prec@5=80.873 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=00:51 IST=> training   100.00% of 1x2503...Epoch=104/150 LR=0.02233 Time=0.339 DataTime=0.232 Loss=1.792 Prec@1=57.696 Prec@5=80.873 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=00:51 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:51 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:51 IST=> validation 0.00% of 1x98...Epoch=104/150 LR=0.02233 Time=5.681 Loss=2.019 Prec@1=53.516 Prec@5=78.125 rate=0 Hz, eta=?, total=0:00:00, wall=00:51 IST=> validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=5.681 Loss=2.019 Prec@1=53.516 Prec@5=78.125 rate=6634.25 Hz, eta=0:00:00, total=0:00:00, wall=00:51 IST** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=5.681 Loss=2.019 Prec@1=53.516 Prec@5=78.125 rate=6634.25 Hz, eta=0:00:00, total=0:00:00, wall=00:52 IST** validation 1.02% of 1x98...Epoch=104/150 LR=0.02233 Time=0.392 Loss=2.001 Prec@1=53.436 Prec@5=77.898 rate=6634.25 Hz, eta=0:00:00, total=0:00:00, wall=00:52 IST** validation 100.00% of 1x98...Epoch=104/150 LR=0.02233 Time=0.392 Loss=2.001 Prec@1=53.436 Prec@5=77.898 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=00:52 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:52 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:52 IST=> training   0.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=6.159 DataTime=6.071 Loss=1.756 Prec@1=56.836 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=00:52 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=6.159 DataTime=6.071 Loss=1.756 Prec@1=56.836 Prec@5=81.055 rate=5049.00 Hz, eta=0:00:00, total=0:00:00, wall=00:52 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=6.159 DataTime=6.071 Loss=1.756 Prec@1=56.836 Prec@5=81.055 rate=5049.00 Hz, eta=0:00:00, total=0:00:00, wall=00:52 IST=> training   0.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.380 DataTime=0.285 Loss=1.766 Prec@1=58.338 Prec@5=81.318 rate=5049.00 Hz, eta=0:00:00, total=0:00:00, wall=00:52 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.380 DataTime=0.285 Loss=1.766 Prec@1=58.338 Prec@5=81.318 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=00:52 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.380 DataTime=0.285 Loss=1.766 Prec@1=58.338 Prec@5=81.318 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=00:53 IST=> training   4.04% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.360 DataTime=0.260 Loss=1.760 Prec@1=58.407 Prec@5=81.479 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=00:53 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.360 DataTime=0.260 Loss=1.760 Prec@1=58.407 Prec@5=81.479 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=00:53 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.360 DataTime=0.260 Loss=1.760 Prec@1=58.407 Prec@5=81.479 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=00:53 IST=> training   8.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.352 DataTime=0.250 Loss=1.761 Prec@1=58.337 Prec@5=81.430 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=00:53 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.352 DataTime=0.250 Loss=1.761 Prec@1=58.337 Prec@5=81.430 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:53 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.352 DataTime=0.250 Loss=1.761 Prec@1=58.337 Prec@5=81.430 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:54 IST=> training   12.03% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.247 Loss=1.763 Prec@1=58.273 Prec@5=81.367 rate=3.01 Hz, eta=0:12:10, total=0:01:39, wall=00:54 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.247 Loss=1.763 Prec@1=58.273 Prec@5=81.367 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=00:54 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.247 Loss=1.763 Prec@1=58.273 Prec@5=81.367 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=00:54 IST=> training   16.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.245 Loss=1.765 Prec@1=58.247 Prec@5=81.340 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=00:54 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.245 Loss=1.765 Prec@1=58.247 Prec@5=81.340 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:54 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.349 DataTime=0.245 Loss=1.765 Prec@1=58.247 Prec@5=81.340 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:55 IST=> training   20.02% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.347 DataTime=0.243 Loss=1.766 Prec@1=58.199 Prec@5=81.295 rate=2.97 Hz, eta=0:11:13, total=0:02:48, wall=00:55 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.347 DataTime=0.243 Loss=1.766 Prec@1=58.199 Prec@5=81.295 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:55 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.347 DataTime=0.243 Loss=1.766 Prec@1=58.199 Prec@5=81.295 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:56 IST=> training   24.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.770 Prec@1=58.132 Prec@5=81.244 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=00:56 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.770 Prec@1=58.132 Prec@5=81.244 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=00:56 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.770 Prec@1=58.132 Prec@5=81.244 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=00:56 IST=> training   28.01% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.771 Prec@1=58.151 Prec@5=81.197 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=00:56 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.771 Prec@1=58.151 Prec@5=81.197 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:56 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.241 Loss=1.771 Prec@1=58.151 Prec@5=81.197 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:57 IST=> training   32.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.240 Loss=1.773 Prec@1=58.083 Prec@5=81.171 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=00:57 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.240 Loss=1.773 Prec@1=58.083 Prec@5=81.171 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=00:57 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.345 DataTime=0.240 Loss=1.773 Prec@1=58.083 Prec@5=81.171 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=00:57 IST=> training   36.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.775 Prec@1=58.040 Prec@5=81.136 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=00:57 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.775 Prec@1=58.040 Prec@5=81.136 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:57 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.775 Prec@1=58.040 Prec@5=81.136 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:58 IST=> training   39.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.776 Prec@1=58.030 Prec@5=81.115 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=00:58 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.776 Prec@1=58.030 Prec@5=81.115 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:58 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.343 DataTime=0.237 Loss=1.776 Prec@1=58.030 Prec@5=81.115 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:58 IST=> training   43.99% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.777 Prec@1=58.005 Prec@5=81.107 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=00:58 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.777 Prec@1=58.005 Prec@5=81.107 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:58 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.777 Prec@1=58.005 Prec@5=81.107 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:59 IST=> training   47.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.778 Prec@1=57.979 Prec@5=81.091 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=00:59 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.778 Prec@1=57.979 Prec@5=81.091 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=00:59 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.778 Prec@1=57.979 Prec@5=81.091 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=01:00 IST=> training   51.98% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.779 Prec@1=57.955 Prec@5=81.084 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=01:00 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.779 Prec@1=57.955 Prec@5=81.084 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=01:00 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.779 Prec@1=57.955 Prec@5=81.084 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=01:00 IST=> training   55.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.937 Prec@5=81.065 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=01:00 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.937 Prec@5=81.065 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=01:00 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.937 Prec@5=81.065 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=01:01 IST=> training   59.97% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.933 Prec@5=81.059 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=01:01 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.933 Prec@5=81.059 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=01:01 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.780 Prec@1=57.933 Prec@5=81.059 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=01:01 IST=> training   63.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.781 Prec@1=57.905 Prec@5=81.048 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=01:01 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.781 Prec@1=57.905 Prec@5=81.048 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=01:01 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.236 Loss=1.781 Prec@1=57.905 Prec@5=81.048 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=01:02 IST=> training   67.96% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.235 Loss=1.782 Prec@1=57.891 Prec@5=81.031 rate=2.95 Hz, eta=0:04:31, total=0:09:35, wall=01:02 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.235 Loss=1.782 Prec@1=57.891 Prec@5=81.031 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=01:02 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.342 DataTime=0.235 Loss=1.782 Prec@1=57.891 Prec@5=81.031 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=01:02 IST=> training   71.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.782 Prec@1=57.879 Prec@5=81.024 rate=2.96 Hz, eta=0:03:57, total=0:10:09, wall=01:02 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.782 Prec@1=57.879 Prec@5=81.024 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=01:02 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.782 Prec@1=57.879 Prec@5=81.024 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=01:03 IST=> training   75.95% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.235 Loss=1.783 Prec@1=57.853 Prec@5=81.009 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=01:03 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.235 Loss=1.783 Prec@1=57.853 Prec@5=81.009 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=01:03 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.235 Loss=1.783 Prec@1=57.853 Prec@5=81.009 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=01:04 IST=> training   79.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.825 Prec@5=81.000 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=01:04 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.825 Prec@5=81.000 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=01:04 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.825 Prec@5=81.000 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=01:04 IST=> training   83.94% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.784 Prec@1=57.839 Prec@5=81.001 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=01:04 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.784 Prec@1=57.839 Prec@5=81.001 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:04 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.784 Prec@1=57.839 Prec@5=81.001 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:05 IST=> training   87.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.834 Prec@5=81.003 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=01:05 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.834 Prec@5=81.003 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=01:05 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.341 DataTime=0.234 Loss=1.784 Prec@1=57.834 Prec@5=81.003 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=01:05 IST=> training   91.93% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.785 Prec@1=57.818 Prec@5=80.995 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=01:05 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.785 Prec@1=57.818 Prec@5=80.995 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=01:05 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.234 Loss=1.785 Prec@1=57.818 Prec@5=80.995 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=01:06 IST=> training   95.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.233 Loss=1.786 Prec@1=57.803 Prec@5=80.983 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=01:06 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.233 Loss=1.786 Prec@1=57.803 Prec@5=80.983 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=01:06 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.340 DataTime=0.233 Loss=1.786 Prec@1=57.803 Prec@5=80.983 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=01:06 IST=> training   99.92% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.339 DataTime=0.233 Loss=1.786 Prec@1=57.802 Prec@5=80.982 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=01:06 IST=> training   100.00% of 1x2503...Epoch=105/150 LR=0.02146 Time=0.339 DataTime=0.233 Loss=1.786 Prec@1=57.802 Prec@5=80.982 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=01:06 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST=> validation 0.00% of 1x98...Epoch=105/150 LR=0.02146 Time=7.273 Loss=1.926 Prec@1=55.664 Prec@5=80.859 rate=0 Hz, eta=?, total=0:00:00, wall=01:06 IST=> validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=7.273 Loss=1.926 Prec@1=55.664 Prec@5=80.859 rate=5676.37 Hz, eta=0:00:00, total=0:00:00, wall=01:06 IST** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=7.273 Loss=1.926 Prec@1=55.664 Prec@5=80.859 rate=5676.37 Hz, eta=0:00:00, total=0:00:00, wall=01:06 IST** validation 1.02% of 1x98...Epoch=105/150 LR=0.02146 Time=0.400 Loss=1.832 Prec@1=56.652 Prec@5=80.596 rate=5676.37 Hz, eta=0:00:00, total=0:00:00, wall=01:06 IST** validation 100.00% of 1x98...Epoch=105/150 LR=0.02146 Time=0.400 Loss=1.832 Prec@1=56.652 Prec@5=80.596 rate=3.07 Hz, eta=0:00:00, total=0:00:31, wall=01:06 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST=> training   0.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.360 DataTime=5.217 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=01:07 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.360 DataTime=5.217 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=3881.60 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=5.360 DataTime=5.217 Loss=1.695 Prec@1=61.133 Prec@5=82.227 rate=3881.60 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST=> training   0.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.379 DataTime=0.282 Loss=1.756 Prec@1=58.636 Prec@5=81.455 rate=3881.60 Hz, eta=0:00:00, total=0:00:00, wall=01:07 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.379 DataTime=0.282 Loss=1.756 Prec@1=58.636 Prec@5=81.455 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:07 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.379 DataTime=0.282 Loss=1.756 Prec@1=58.636 Prec@5=81.455 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:08 IST=> training   4.04% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.354 DataTime=0.256 Loss=1.758 Prec@1=58.449 Prec@5=81.398 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=01:08 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.354 DataTime=0.256 Loss=1.758 Prec@1=58.449 Prec@5=81.398 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:08 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.354 DataTime=0.256 Loss=1.758 Prec@1=58.449 Prec@5=81.398 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:08 IST=> training   8.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.348 DataTime=0.251 Loss=1.761 Prec@1=58.412 Prec@5=81.314 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:08 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.348 DataTime=0.251 Loss=1.761 Prec@1=58.412 Prec@5=81.314 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=01:08 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.348 DataTime=0.251 Loss=1.761 Prec@1=58.412 Prec@5=81.314 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=01:09 IST=> training   12.03% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.345 DataTime=0.245 Loss=1.758 Prec@1=58.477 Prec@5=81.358 rate=3.03 Hz, eta=0:12:07, total=0:01:39, wall=01:09 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.345 DataTime=0.245 Loss=1.758 Prec@1=58.477 Prec@5=81.358 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=01:09 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.345 DataTime=0.245 Loss=1.758 Prec@1=58.477 Prec@5=81.358 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=01:09 IST=> training   16.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.240 Loss=1.759 Prec@1=58.432 Prec@5=81.326 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=01:09 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.240 Loss=1.759 Prec@1=58.432 Prec@5=81.326 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=01:09 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.240 Loss=1.759 Prec@1=58.432 Prec@5=81.326 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=01:10 IST=> training   20.02% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.343 DataTime=0.241 Loss=1.763 Prec@1=58.347 Prec@5=81.271 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=01:10 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.343 DataTime=0.241 Loss=1.763 Prec@1=58.347 Prec@5=81.271 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:10 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.343 DataTime=0.241 Loss=1.763 Prec@1=58.347 Prec@5=81.271 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:10 IST=> training   24.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.238 Loss=1.765 Prec@1=58.263 Prec@5=81.263 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=01:10 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.238 Loss=1.765 Prec@1=58.263 Prec@5=81.263 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=01:10 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.238 Loss=1.765 Prec@1=58.263 Prec@5=81.263 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=01:11 IST=> training   28.01% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.765 Prec@1=58.271 Prec@5=81.265 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=01:11 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.765 Prec@1=58.271 Prec@5=81.265 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=01:11 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.765 Prec@1=58.271 Prec@5=81.265 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=01:12 IST=> training   32.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.236 Loss=1.767 Prec@1=58.251 Prec@5=81.257 rate=3.00 Hz, eta=0:09:26, total=0:04:26, wall=01:12 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.236 Loss=1.767 Prec@1=58.251 Prec@5=81.257 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=01:12 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.341 DataTime=0.236 Loss=1.767 Prec@1=58.251 Prec@5=81.257 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=01:12 IST=> training   36.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.766 Prec@1=58.273 Prec@5=81.266 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=01:12 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.766 Prec@1=58.273 Prec@5=81.266 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=01:12 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.340 DataTime=0.235 Loss=1.766 Prec@1=58.273 Prec@5=81.266 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=01:13 IST=> training   39.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.766 Prec@1=58.247 Prec@5=81.262 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=01:13 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.766 Prec@1=58.247 Prec@5=81.262 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=01:13 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.766 Prec@1=58.247 Prec@5=81.262 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=01:13 IST=> training   43.99% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.235 Loss=1.768 Prec@1=58.220 Prec@5=81.249 rate=2.99 Hz, eta=0:07:48, total=0:06:07, wall=01:13 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.235 Loss=1.768 Prec@1=58.220 Prec@5=81.249 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:13 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.235 Loss=1.768 Prec@1=58.220 Prec@5=81.249 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:14 IST=> training   47.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.770 Prec@1=58.172 Prec@5=81.217 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.770 Prec@1=58.172 Prec@5=81.217 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.234 Loss=1.770 Prec@1=58.172 Prec@5=81.217 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=01:14 IST=> training   51.98% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.233 Loss=1.770 Prec@1=58.155 Prec@5=81.196 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=01:14 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.233 Loss=1.770 Prec@1=58.155 Prec@5=81.196 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:14 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.233 Loss=1.770 Prec@1=58.155 Prec@5=81.196 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:15 IST=> training   55.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.233 Loss=1.771 Prec@1=58.156 Prec@5=81.178 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=01:15 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.233 Loss=1.771 Prec@1=58.156 Prec@5=81.178 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=01:15 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.339 DataTime=0.233 Loss=1.771 Prec@1=58.156 Prec@5=81.178 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=01:15 IST=> training   59.97% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.772 Prec@1=58.131 Prec@5=81.178 rate=2.98 Hz, eta=0:05:35, total=0:08:22, wall=01:15 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.772 Prec@1=58.131 Prec@5=81.178 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=01:15 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.772 Prec@1=58.131 Prec@5=81.178 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=01:16 IST=> training   63.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.774 Prec@1=58.099 Prec@5=81.151 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=01:16 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.774 Prec@1=58.099 Prec@5=81.151 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=01:16 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.774 Prec@1=58.099 Prec@5=81.151 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=01:17 IST=> training   67.96% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.775 Prec@1=58.080 Prec@5=81.113 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=01:17 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.775 Prec@1=58.080 Prec@5=81.113 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=01:17 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.338 DataTime=0.232 Loss=1.775 Prec@1=58.080 Prec@5=81.113 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=01:17 IST=> training   71.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.776 Prec@1=58.066 Prec@5=81.102 rate=2.99 Hz, eta=0:03:55, total=0:10:02, wall=01:17 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.776 Prec@1=58.066 Prec@5=81.102 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:17 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.776 Prec@1=58.066 Prec@5=81.102 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:18 IST=> training   75.95% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.035 Prec@5=81.090 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=01:18 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.035 Prec@5=81.090 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=01:18 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.035 Prec@5=81.090 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=01:18 IST=> training   79.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.025 Prec@5=81.078 rate=2.99 Hz, eta=0:02:47, total=0:11:08, wall=01:18 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.025 Prec@5=81.078 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=01:18 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.025 Prec@5=81.078 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=01:19 IST=> training   83.94% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.028 Prec@5=81.079 rate=2.99 Hz, eta=0:02:14, total=0:11:42, wall=01:19 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.028 Prec@5=81.079 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:19 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.231 Loss=1.777 Prec@1=58.028 Prec@5=81.079 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:19 IST=> training   87.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.778 Prec@1=58.003 Prec@5=81.070 rate=2.99 Hz, eta=0:01:40, total=0:12:15, wall=01:19 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.778 Prec@1=58.003 Prec@5=81.070 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:19 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.778 Prec@1=58.003 Prec@5=81.070 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:20 IST=> training   91.93% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.232 Loss=1.779 Prec@1=58.000 Prec@5=81.058 rate=2.99 Hz, eta=0:01:07, total=0:12:48, wall=01:20 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.232 Loss=1.779 Prec@1=58.000 Prec@5=81.058 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=01:20 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.337 DataTime=0.232 Loss=1.779 Prec@1=58.000 Prec@5=81.058 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=01:20 IST=> training   95.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.779 Prec@1=57.998 Prec@5=81.056 rate=2.99 Hz, eta=0:00:34, total=0:13:23, wall=01:20 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.779 Prec@1=57.998 Prec@5=81.056 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=01:20 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.779 Prec@1=57.998 Prec@5=81.056 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=01:20 IST=> training   99.92% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.779 Prec@1=57.998 Prec@5=81.056 rate=2.99 Hz, eta=0:00:00, total=0:13:55, wall=01:20 IST=> training   100.00% of 1x2503...Epoch=106/150 LR=0.02061 Time=0.336 DataTime=0.231 Loss=1.779 Prec@1=57.998 Prec@5=81.056 rate=3.00 Hz, eta=0:00:00, total=0:13:55, wall=01:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> validation 0.00% of 1x98...Epoch=106/150 LR=0.02061 Time=6.428 Loss=2.103 Prec@1=50.586 Prec@5=76.758 rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=6.428 Loss=2.103 Prec@1=50.586 Prec@5=76.758 rate=4033.97 Hz, eta=0:00:00, total=0:00:00, wall=01:21 IST** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=6.428 Loss=2.103 Prec@1=50.586 Prec@5=76.758 rate=4033.97 Hz, eta=0:00:00, total=0:00:00, wall=01:21 IST** validation 1.02% of 1x98...Epoch=106/150 LR=0.02061 Time=0.390 Loss=2.020 Prec@1=53.088 Prec@5=77.608 rate=4033.97 Hz, eta=0:00:00, total=0:00:00, wall=01:21 IST** validation 100.00% of 1x98...Epoch=106/150 LR=0.02061 Time=0.390 Loss=2.020 Prec@1=53.088 Prec@5=77.608 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=01:21 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> training   0.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=4.952 DataTime=4.699 Loss=1.775 Prec@1=58.203 Prec@5=80.859 rate=0 Hz, eta=?, total=0:00:00, wall=01:21 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=4.952 DataTime=4.699 Loss=1.775 Prec@1=58.203 Prec@5=80.859 rate=2609.78 Hz, eta=0:00:00, total=0:00:00, wall=01:21 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=4.952 DataTime=4.699 Loss=1.775 Prec@1=58.203 Prec@5=80.859 rate=2609.78 Hz, eta=0:00:00, total=0:00:00, wall=01:22 IST=> training   0.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.379 DataTime=0.283 Loss=1.735 Prec@1=59.000 Prec@5=81.757 rate=2609.78 Hz, eta=0:00:00, total=0:00:00, wall=01:22 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.379 DataTime=0.283 Loss=1.735 Prec@1=59.000 Prec@5=81.757 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=01:22 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.379 DataTime=0.283 Loss=1.735 Prec@1=59.000 Prec@5=81.757 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=01:22 IST=> training   4.04% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.358 DataTime=0.258 Loss=1.748 Prec@1=58.679 Prec@5=81.611 rate=3.02 Hz, eta=0:13:14, total=0:00:33, wall=01:22 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.358 DataTime=0.258 Loss=1.748 Prec@1=58.679 Prec@5=81.611 rate=2.99 Hz, eta=0:12:48, total=0:01:07, wall=01:22 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.358 DataTime=0.258 Loss=1.748 Prec@1=58.679 Prec@5=81.611 rate=2.99 Hz, eta=0:12:48, total=0:01:07, wall=01:23 IST=> training   8.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.348 DataTime=0.246 Loss=1.758 Prec@1=58.444 Prec@5=81.413 rate=2.99 Hz, eta=0:12:48, total=0:01:07, wall=01:23 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.348 DataTime=0.246 Loss=1.758 Prec@1=58.444 Prec@5=81.413 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=01:23 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.348 DataTime=0.246 Loss=1.758 Prec@1=58.444 Prec@5=81.413 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=01:23 IST=> training   12.03% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.347 DataTime=0.244 Loss=1.757 Prec@1=58.467 Prec@5=81.427 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=01:23 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.347 DataTime=0.244 Loss=1.757 Prec@1=58.467 Prec@5=81.427 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=01:23 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.347 DataTime=0.244 Loss=1.757 Prec@1=58.467 Prec@5=81.427 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=01:24 IST=> training   16.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.343 DataTime=0.240 Loss=1.759 Prec@1=58.418 Prec@5=81.392 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=01:24 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.343 DataTime=0.240 Loss=1.759 Prec@1=58.418 Prec@5=81.392 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=01:24 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.343 DataTime=0.240 Loss=1.759 Prec@1=58.418 Prec@5=81.392 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=01:25 IST=> training   20.02% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.380 Prec@5=81.388 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=01:25 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.380 Prec@5=81.388 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=01:25 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.380 Prec@5=81.388 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=01:25 IST=> training   24.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.344 DataTime=0.239 Loss=1.760 Prec@1=58.345 Prec@5=81.348 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=01:25 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.344 DataTime=0.239 Loss=1.760 Prec@1=58.345 Prec@5=81.348 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=01:25 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.344 DataTime=0.239 Loss=1.760 Prec@1=58.345 Prec@5=81.348 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=01:26 IST=> training   28.01% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.351 Prec@5=81.360 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=01:26 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.351 Prec@5=81.360 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=01:26 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.237 Loss=1.759 Prec@1=58.351 Prec@5=81.360 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=01:26 IST=> training   32.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.235 Loss=1.759 Prec@1=58.364 Prec@5=81.368 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=01:26 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.235 Loss=1.759 Prec@1=58.364 Prec@5=81.368 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:26 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.235 Loss=1.759 Prec@1=58.364 Prec@5=81.368 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:27 IST=> training   36.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.236 Loss=1.760 Prec@1=58.340 Prec@5=81.344 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=01:27 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.236 Loss=1.760 Prec@1=58.340 Prec@5=81.344 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:27 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.342 DataTime=0.236 Loss=1.760 Prec@1=58.340 Prec@5=81.344 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:27 IST=> training   39.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.761 Prec@1=58.304 Prec@5=81.339 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:27 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.761 Prec@1=58.304 Prec@5=81.339 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=01:27 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.761 Prec@1=58.304 Prec@5=81.339 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=01:28 IST=> training   43.99% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.763 Prec@1=58.258 Prec@5=81.300 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=01:28 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.763 Prec@1=58.258 Prec@5=81.300 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:28 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.763 Prec@1=58.258 Prec@5=81.300 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:29 IST=> training   47.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.764 Prec@1=58.246 Prec@5=81.287 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:29 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.764 Prec@1=58.246 Prec@5=81.287 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=01:29 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.341 DataTime=0.234 Loss=1.764 Prec@1=58.246 Prec@5=81.287 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=01:29 IST=> training   51.98% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.764 Prec@1=58.231 Prec@5=81.289 rate=2.97 Hz, eta=0:06:45, total=0:07:18, wall=01:29 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.764 Prec@1=58.231 Prec@5=81.289 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:29 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.764 Prec@1=58.231 Prec@5=81.289 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:30 IST=> training   55.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.765 Prec@1=58.225 Prec@5=81.278 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:30 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.765 Prec@1=58.225 Prec@5=81.278 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:30 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.765 Prec@1=58.225 Prec@5=81.278 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:30 IST=> training   59.97% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.222 Prec@5=81.279 rate=2.98 Hz, eta=0:05:36, total=0:08:23, wall=01:30 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.222 Prec@5=81.279 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:30 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.222 Prec@5=81.279 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:31 IST=> training   63.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.211 Prec@5=81.260 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:31 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.211 Prec@5=81.260 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:31 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.234 Loss=1.765 Prec@1=58.211 Prec@5=81.260 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:31 IST=> training   67.96% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.766 Prec@1=58.202 Prec@5=81.252 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=01:31 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.766 Prec@1=58.202 Prec@5=81.252 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:31 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.233 Loss=1.766 Prec@1=58.202 Prec@5=81.252 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:32 IST=> training   71.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.767 Prec@1=58.183 Prec@5=81.249 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=01:32 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.767 Prec@1=58.183 Prec@5=81.249 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:32 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.340 DataTime=0.233 Loss=1.767 Prec@1=58.183 Prec@5=81.249 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:32 IST=> training   75.95% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.767 Prec@1=58.177 Prec@5=81.235 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=01:32 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.767 Prec@1=58.177 Prec@5=81.235 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=01:32 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.767 Prec@1=58.177 Prec@5=81.235 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=01:33 IST=> training   79.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.768 Prec@1=58.164 Prec@5=81.223 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=01:33 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.768 Prec@1=58.164 Prec@5=81.223 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:33 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.768 Prec@1=58.164 Prec@5=81.223 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:34 IST=> training   83.94% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.769 Prec@1=58.148 Prec@5=81.209 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=01:34 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.769 Prec@1=58.148 Prec@5=81.209 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:34 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.339 DataTime=0.232 Loss=1.769 Prec@1=58.148 Prec@5=81.209 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:34 IST=> training   87.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.232 Loss=1.770 Prec@1=58.125 Prec@5=81.190 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=01:34 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.232 Loss=1.770 Prec@1=58.125 Prec@5=81.190 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:34 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.232 Loss=1.770 Prec@1=58.125 Prec@5=81.190 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:35 IST=> training   91.93% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.231 Loss=1.770 Prec@1=58.123 Prec@5=81.187 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=01:35 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.231 Loss=1.770 Prec@1=58.123 Prec@5=81.187 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=01:35 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.338 DataTime=0.231 Loss=1.770 Prec@1=58.123 Prec@5=81.187 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=01:35 IST=> training   95.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.337 DataTime=0.231 Loss=1.771 Prec@1=58.108 Prec@5=81.190 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=01:35 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.337 DataTime=0.231 Loss=1.771 Prec@1=58.108 Prec@5=81.190 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=01:35 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.337 DataTime=0.231 Loss=1.771 Prec@1=58.108 Prec@5=81.190 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=01:35 IST=> training   99.92% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.337 DataTime=0.231 Loss=1.771 Prec@1=58.108 Prec@5=81.190 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=01:35 IST=> training   100.00% of 1x2503...Epoch=107/150 LR=0.01977 Time=0.337 DataTime=0.231 Loss=1.771 Prec@1=58.108 Prec@5=81.190 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=01:35 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:35 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:35 IST=> validation 0.00% of 1x98...Epoch=107/150 LR=0.01977 Time=6.038 Loss=1.890 Prec@1=55.273 Prec@5=79.297 rate=0 Hz, eta=?, total=0:00:00, wall=01:35 IST=> validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=6.038 Loss=1.890 Prec@1=55.273 Prec@5=79.297 rate=6449.36 Hz, eta=0:00:00, total=0:00:00, wall=01:35 IST** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=6.038 Loss=1.890 Prec@1=55.273 Prec@5=79.297 rate=6449.36 Hz, eta=0:00:00, total=0:00:00, wall=01:36 IST** validation 1.02% of 1x98...Epoch=107/150 LR=0.01977 Time=0.396 Loss=1.904 Prec@1=55.260 Prec@5=79.440 rate=6449.36 Hz, eta=0:00:00, total=0:00:00, wall=01:36 IST** validation 100.00% of 1x98...Epoch=107/150 LR=0.01977 Time=0.396 Loss=1.904 Prec@1=55.260 Prec@5=79.440 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=01:36 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:36 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:36 IST=> training   0.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=4.902 DataTime=4.704 Loss=1.745 Prec@1=58.789 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=01:36 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=4.902 DataTime=4.704 Loss=1.745 Prec@1=58.789 Prec@5=83.398 rate=4787.48 Hz, eta=0:00:00, total=0:00:00, wall=01:36 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=4.902 DataTime=4.704 Loss=1.745 Prec@1=58.789 Prec@5=83.398 rate=4787.48 Hz, eta=0:00:00, total=0:00:00, wall=01:36 IST=> training   0.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.380 DataTime=0.276 Loss=1.747 Prec@1=58.623 Prec@5=81.459 rate=4787.48 Hz, eta=0:00:00, total=0:00:00, wall=01:36 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.380 DataTime=0.276 Loss=1.747 Prec@1=58.623 Prec@5=81.459 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=01:36 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.380 DataTime=0.276 Loss=1.747 Prec@1=58.623 Prec@5=81.459 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=01:37 IST=> training   4.04% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.352 DataTime=0.247 Loss=1.741 Prec@1=58.825 Prec@5=81.580 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=01:37 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.352 DataTime=0.247 Loss=1.741 Prec@1=58.825 Prec@5=81.580 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:37 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.352 DataTime=0.247 Loss=1.741 Prec@1=58.825 Prec@5=81.580 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:38 IST=> training   8.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.347 DataTime=0.241 Loss=1.745 Prec@1=58.666 Prec@5=81.517 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=01:38 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.347 DataTime=0.241 Loss=1.745 Prec@1=58.666 Prec@5=81.517 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=01:38 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.347 DataTime=0.241 Loss=1.745 Prec@1=58.666 Prec@5=81.517 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=01:38 IST=> training   12.03% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.346 DataTime=0.240 Loss=1.747 Prec@1=58.643 Prec@5=81.523 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=01:38 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.346 DataTime=0.240 Loss=1.747 Prec@1=58.643 Prec@5=81.523 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=01:38 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.346 DataTime=0.240 Loss=1.747 Prec@1=58.643 Prec@5=81.523 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=01:39 IST=> training   16.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.344 DataTime=0.239 Loss=1.750 Prec@1=58.552 Prec@5=81.500 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=01:39 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.344 DataTime=0.239 Loss=1.750 Prec@1=58.552 Prec@5=81.500 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=01:39 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.344 DataTime=0.239 Loss=1.750 Prec@1=58.552 Prec@5=81.500 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=01:39 IST=> training   20.02% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.752 Prec@1=58.490 Prec@5=81.466 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=01:39 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.752 Prec@1=58.490 Prec@5=81.466 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=01:39 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.752 Prec@1=58.490 Prec@5=81.466 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=01:40 IST=> training   24.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.345 DataTime=0.241 Loss=1.755 Prec@1=58.467 Prec@5=81.431 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=01:40 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.345 DataTime=0.241 Loss=1.755 Prec@1=58.467 Prec@5=81.431 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:40 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.345 DataTime=0.241 Loss=1.755 Prec@1=58.467 Prec@5=81.431 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:40 IST=> training   28.01% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.754 Prec@1=58.484 Prec@5=81.439 rate=2.96 Hz, eta=0:10:08, total=0:03:56, wall=01:40 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.754 Prec@1=58.484 Prec@5=81.439 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=01:40 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.343 DataTime=0.239 Loss=1.754 Prec@1=58.484 Prec@5=81.439 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=01:41 IST=> training   32.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.753 Prec@1=58.480 Prec@5=81.439 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=01:41 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.753 Prec@1=58.480 Prec@5=81.439 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:41 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.753 Prec@1=58.480 Prec@5=81.439 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:42 IST=> training   36.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.755 Prec@1=58.453 Prec@5=81.427 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:42 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.755 Prec@1=58.453 Prec@5=81.427 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:42 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.342 DataTime=0.237 Loss=1.755 Prec@1=58.453 Prec@5=81.427 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:42 IST=> training   39.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.754 Prec@1=58.476 Prec@5=81.417 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=01:42 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.754 Prec@1=58.476 Prec@5=81.417 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=01:42 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.754 Prec@1=58.476 Prec@5=81.417 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=01:43 IST=> training   43.99% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.756 Prec@1=58.438 Prec@5=81.386 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=01:43 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.756 Prec@1=58.438 Prec@5=81.386 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:43 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.756 Prec@1=58.438 Prec@5=81.386 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:43 IST=> training   47.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.758 Prec@1=58.407 Prec@5=81.362 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:43 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.758 Prec@1=58.407 Prec@5=81.362 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=01:43 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.341 DataTime=0.235 Loss=1.758 Prec@1=58.407 Prec@5=81.362 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=01:44 IST=> training   51.98% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.376 Prec@5=81.339 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=01:44 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.376 Prec@5=81.339 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:44 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.376 Prec@5=81.339 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:44 IST=> training   55.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.366 Prec@5=81.349 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=01:44 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.366 Prec@5=81.349 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=01:44 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.366 Prec@5=81.349 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=01:45 IST=> training   59.97% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.367 Prec@5=81.359 rate=2.97 Hz, eta=0:05:37, total=0:08:24, wall=01:45 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.367 Prec@5=81.359 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:45 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.340 DataTime=0.234 Loss=1.758 Prec@1=58.367 Prec@5=81.359 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:45 IST=> training   63.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.759 Prec@1=58.347 Prec@5=81.346 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=01:45 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.759 Prec@1=58.347 Prec@5=81.346 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:45 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.759 Prec@1=58.347 Prec@5=81.346 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:46 IST=> training   67.96% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.760 Prec@1=58.336 Prec@5=81.343 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=01:46 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.760 Prec@1=58.336 Prec@5=81.343 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=01:46 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.760 Prec@1=58.336 Prec@5=81.343 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=01:47 IST=> training   71.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.761 Prec@1=58.325 Prec@5=81.333 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=01:47 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.761 Prec@1=58.325 Prec@5=81.333 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=01:47 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.761 Prec@1=58.325 Prec@5=81.333 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=01:47 IST=> training   75.95% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.305 Prec@5=81.324 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=01:47 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.305 Prec@5=81.324 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=01:47 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.305 Prec@5=81.324 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=01:48 IST=> training   79.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.295 Prec@5=81.312 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=01:48 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.295 Prec@5=81.312 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:48 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.295 Prec@5=81.312 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:48 IST=> training   83.94% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.285 Prec@5=81.312 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=01:48 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.285 Prec@5=81.312 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=01:48 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.285 Prec@5=81.312 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=01:49 IST=> training   87.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.282 Prec@5=81.303 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=01:49 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.282 Prec@5=81.303 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=01:49 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.762 Prec@1=58.282 Prec@5=81.303 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=01:49 IST=> training   91.93% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.257 Prec@5=81.298 rate=2.97 Hz, eta=0:01:08, total=0:12:54, wall=01:49 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.257 Prec@5=81.298 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:49 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.257 Prec@5=81.298 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:50 IST=> training   95.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.252 Prec@5=81.305 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=01:50 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.252 Prec@5=81.305 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=01:50 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.252 Prec@5=81.305 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=01:50 IST=> training   99.92% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.250 Prec@5=81.304 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=01:50 IST=> training   100.00% of 1x2503...Epoch=108/150 LR=0.01894 Time=0.339 DataTime=0.233 Loss=1.763 Prec@1=58.250 Prec@5=81.304 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=01:50 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:50 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:50 IST=> validation 0.00% of 1x98...Epoch=108/150 LR=0.01894 Time=6.494 Loss=2.000 Prec@1=55.273 Prec@5=79.492 rate=0 Hz, eta=?, total=0:00:00, wall=01:50 IST=> validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.494 Loss=2.000 Prec@1=55.273 Prec@5=79.492 rate=6613.89 Hz, eta=0:00:00, total=0:00:00, wall=01:50 IST** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=6.494 Loss=2.000 Prec@1=55.273 Prec@5=79.492 rate=6613.89 Hz, eta=0:00:00, total=0:00:00, wall=01:51 IST** validation 1.02% of 1x98...Epoch=108/150 LR=0.01894 Time=0.400 Loss=1.832 Prec@1=56.962 Prec@5=80.606 rate=6613.89 Hz, eta=0:00:00, total=0:00:00, wall=01:51 IST** validation 100.00% of 1x98...Epoch=108/150 LR=0.01894 Time=0.400 Loss=1.832 Prec@1=56.962 Prec@5=80.606 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=01:51 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:51 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:51 IST=> training   0.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.757 DataTime=5.581 Loss=1.601 Prec@1=60.547 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=01:51 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.757 DataTime=5.581 Loss=1.601 Prec@1=60.547 Prec@5=85.156 rate=5982.04 Hz, eta=0:00:00, total=0:00:00, wall=01:51 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=5.757 DataTime=5.581 Loss=1.601 Prec@1=60.547 Prec@5=85.156 rate=5982.04 Hz, eta=0:00:00, total=0:00:00, wall=01:51 IST=> training   0.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.385 DataTime=0.292 Loss=1.738 Prec@1=59.048 Prec@5=81.577 rate=5982.04 Hz, eta=0:00:00, total=0:00:00, wall=01:51 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.385 DataTime=0.292 Loss=1.738 Prec@1=59.048 Prec@5=81.577 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=01:51 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.385 DataTime=0.292 Loss=1.738 Prec@1=59.048 Prec@5=81.577 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=01:52 IST=> training   4.04% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.357 DataTime=0.257 Loss=1.740 Prec@1=58.990 Prec@5=81.684 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=01:52 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.357 DataTime=0.257 Loss=1.740 Prec@1=58.990 Prec@5=81.684 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=01:52 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.357 DataTime=0.257 Loss=1.740 Prec@1=58.990 Prec@5=81.684 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=01:52 IST=> training   8.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.355 DataTime=0.254 Loss=1.740 Prec@1=58.915 Prec@5=81.640 rate=3.05 Hz, eta=0:12:35, total=0:01:06, wall=01:52 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.355 DataTime=0.254 Loss=1.740 Prec@1=58.915 Prec@5=81.640 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=01:52 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.355 DataTime=0.254 Loss=1.740 Prec@1=58.915 Prec@5=81.640 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=01:53 IST=> training   12.03% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.350 DataTime=0.248 Loss=1.739 Prec@1=58.988 Prec@5=81.620 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=01:53 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.350 DataTime=0.248 Loss=1.739 Prec@1=58.988 Prec@5=81.620 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=01:53 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.350 DataTime=0.248 Loss=1.739 Prec@1=58.988 Prec@5=81.620 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=01:54 IST=> training   16.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.345 DataTime=0.242 Loss=1.738 Prec@1=58.911 Prec@5=81.650 rate=2.98 Hz, eta=0:11:45, total=0:02:14, wall=01:54 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.345 DataTime=0.242 Loss=1.738 Prec@1=58.911 Prec@5=81.650 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=01:54 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.345 DataTime=0.242 Loss=1.738 Prec@1=58.911 Prec@5=81.650 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=01:54 IST=> training   20.02% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.346 DataTime=0.242 Loss=1.739 Prec@1=58.800 Prec@5=81.673 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=01:54 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.346 DataTime=0.242 Loss=1.739 Prec@1=58.800 Prec@5=81.673 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=01:54 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.346 DataTime=0.242 Loss=1.739 Prec@1=58.800 Prec@5=81.673 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=01:55 IST=> training   24.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.344 DataTime=0.240 Loss=1.741 Prec@1=58.770 Prec@5=81.626 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=01:55 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.344 DataTime=0.240 Loss=1.741 Prec@1=58.770 Prec@5=81.626 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=01:55 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.344 DataTime=0.240 Loss=1.741 Prec@1=58.770 Prec@5=81.626 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=01:55 IST=> training   28.01% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.342 DataTime=0.237 Loss=1.741 Prec@1=58.740 Prec@5=81.640 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=01:55 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.342 DataTime=0.237 Loss=1.741 Prec@1=58.740 Prec@5=81.640 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=01:55 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.342 DataTime=0.237 Loss=1.741 Prec@1=58.740 Prec@5=81.640 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=01:56 IST=> training   32.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.343 DataTime=0.237 Loss=1.743 Prec@1=58.675 Prec@5=81.606 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=01:56 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.343 DataTime=0.237 Loss=1.743 Prec@1=58.675 Prec@5=81.606 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:56 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.343 DataTime=0.237 Loss=1.743 Prec@1=58.675 Prec@5=81.606 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:56 IST=> training   36.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.745 Prec@1=58.639 Prec@5=81.584 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=01:56 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.745 Prec@1=58.639 Prec@5=81.584 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:56 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.745 Prec@1=58.639 Prec@5=81.584 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:57 IST=> training   39.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.747 Prec@1=58.589 Prec@5=81.557 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=01:57 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.747 Prec@1=58.589 Prec@5=81.557 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=01:57 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.747 Prec@1=58.589 Prec@5=81.557 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=01:58 IST=> training   43.99% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.748 Prec@1=58.554 Prec@5=81.524 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=01:58 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.748 Prec@1=58.554 Prec@5=81.524 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:58 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.235 Loss=1.748 Prec@1=58.554 Prec@5=81.524 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:58 IST=> training   47.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.750 Prec@1=58.534 Prec@5=81.500 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=01:58 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.750 Prec@1=58.534 Prec@5=81.500 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:58 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.750 Prec@1=58.534 Prec@5=81.500 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:59 IST=> training   51.98% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.751 Prec@1=58.510 Prec@5=81.492 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=01:59 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.751 Prec@1=58.510 Prec@5=81.492 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:59 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.751 Prec@1=58.510 Prec@5=81.492 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:59 IST=> training   55.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.751 Prec@1=58.509 Prec@5=81.491 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=01:59 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.751 Prec@1=58.509 Prec@5=81.491 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=01:59 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.234 Loss=1.751 Prec@1=58.509 Prec@5=81.491 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=02:00 IST=> training   59.97% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.752 Prec@1=58.478 Prec@5=81.479 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=02:00 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.752 Prec@1=58.478 Prec@5=81.479 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:00 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.752 Prec@1=58.478 Prec@5=81.479 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:00 IST=> training   63.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.752 Prec@1=58.457 Prec@5=81.468 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:00 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.752 Prec@1=58.457 Prec@5=81.468 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:00 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.752 Prec@1=58.457 Prec@5=81.468 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:01 IST=> training   67.96% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.754 Prec@1=58.431 Prec@5=81.438 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=02:01 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.754 Prec@1=58.431 Prec@5=81.438 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=02:01 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.341 DataTime=0.233 Loss=1.754 Prec@1=58.431 Prec@5=81.438 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=02:01 IST=> training   71.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.755 Prec@1=58.424 Prec@5=81.430 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=02:01 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.755 Prec@1=58.424 Prec@5=81.430 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=02:01 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.755 Prec@1=58.424 Prec@5=81.430 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=02:02 IST=> training   75.95% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.755 Prec@1=58.421 Prec@5=81.419 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=02:02 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.755 Prec@1=58.421 Prec@5=81.419 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=02:02 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.755 Prec@1=58.421 Prec@5=81.419 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=02:03 IST=> training   79.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.756 Prec@1=58.427 Prec@5=81.404 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=02:03 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.756 Prec@1=58.427 Prec@5=81.404 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:03 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.233 Loss=1.756 Prec@1=58.427 Prec@5=81.404 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:03 IST=> training   83.94% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.756 Prec@1=58.418 Prec@5=81.390 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=02:03 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.756 Prec@1=58.418 Prec@5=81.390 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=02:03 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.756 Prec@1=58.418 Prec@5=81.390 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=02:04 IST=> training   87.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.232 Loss=1.756 Prec@1=58.412 Prec@5=81.392 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=02:04 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.232 Loss=1.756 Prec@1=58.412 Prec@5=81.392 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=02:04 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.232 Loss=1.756 Prec@1=58.412 Prec@5=81.392 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=02:04 IST=> training   91.93% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.757 Prec@1=58.397 Prec@5=81.379 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=02:04 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.757 Prec@1=58.397 Prec@5=81.379 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:04 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.340 DataTime=0.232 Loss=1.757 Prec@1=58.397 Prec@5=81.379 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:05 IST=> training   95.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.231 Loss=1.757 Prec@1=58.385 Prec@5=81.373 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.231 Loss=1.757 Prec@1=58.385 Prec@5=81.373 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.231 Loss=1.757 Prec@1=58.385 Prec@5=81.373 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=02:05 IST=> training   99.92% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.231 Loss=1.757 Prec@1=58.382 Prec@5=81.372 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=02:05 IST=> training   100.00% of 1x2503...Epoch=109/150 LR=0.01813 Time=0.339 DataTime=0.231 Loss=1.757 Prec@1=58.382 Prec@5=81.372 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=02:05 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 0.00% of 1x98...Epoch=109/150 LR=0.01813 Time=6.916 Loss=1.788 Prec@1=58.008 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=02:05 IST=> validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.916 Loss=1.788 Prec@1=58.008 Prec@5=81.055 rate=3381.51 Hz, eta=0:00:00, total=0:00:00, wall=02:05 IST** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=6.916 Loss=1.788 Prec@1=58.008 Prec@5=81.055 rate=3381.51 Hz, eta=0:00:00, total=0:00:00, wall=02:05 IST** validation 1.02% of 1x98...Epoch=109/150 LR=0.01813 Time=0.397 Loss=1.769 Prec@1=58.062 Prec@5=81.528 rate=3381.51 Hz, eta=0:00:00, total=0:00:00, wall=02:05 IST** validation 100.00% of 1x98...Epoch=109/150 LR=0.01813 Time=0.397 Loss=1.769 Prec@1=58.062 Prec@5=81.528 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=02:05 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.707 DataTime=5.567 Loss=1.772 Prec@1=55.859 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=02:06 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.707 DataTime=5.567 Loss=1.772 Prec@1=55.859 Prec@5=80.664 rate=7562.41 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=5.707 DataTime=5.567 Loss=1.772 Prec@1=55.859 Prec@5=80.664 rate=7562.41 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST=> training   0.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.373 DataTime=0.275 Loss=1.737 Prec@1=58.745 Prec@5=81.762 rate=7562.41 Hz, eta=0:00:00, total=0:00:00, wall=02:06 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.373 DataTime=0.275 Loss=1.737 Prec@1=58.745 Prec@5=81.762 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=02:06 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.373 DataTime=0.275 Loss=1.737 Prec@1=58.745 Prec@5=81.762 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=02:07 IST=> training   4.04% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.350 DataTime=0.253 Loss=1.736 Prec@1=58.720 Prec@5=81.768 rate=3.17 Hz, eta=0:12:38, total=0:00:31, wall=02:07 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.350 DataTime=0.253 Loss=1.736 Prec@1=58.720 Prec@5=81.768 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=02:07 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.350 DataTime=0.253 Loss=1.736 Prec@1=58.720 Prec@5=81.768 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=02:07 IST=> training   8.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.344 DataTime=0.246 Loss=1.733 Prec@1=58.822 Prec@5=81.828 rate=3.12 Hz, eta=0:12:18, total=0:01:04, wall=02:07 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.344 DataTime=0.246 Loss=1.733 Prec@1=58.822 Prec@5=81.828 rate=3.07 Hz, eta=0:11:56, total=0:01:37, wall=02:07 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.344 DataTime=0.246 Loss=1.733 Prec@1=58.822 Prec@5=81.828 rate=3.07 Hz, eta=0:11:56, total=0:01:37, wall=02:08 IST=> training   12.03% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.737 Prec@1=58.705 Prec@5=81.805 rate=3.07 Hz, eta=0:11:56, total=0:01:37, wall=02:08 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.737 Prec@1=58.705 Prec@5=81.805 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=02:08 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.737 Prec@1=58.705 Prec@5=81.805 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=02:08 IST=> training   16.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.242 Loss=1.738 Prec@1=58.705 Prec@5=81.783 rate=3.05 Hz, eta=0:11:28, total=0:02:11, wall=02:08 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.242 Loss=1.738 Prec@1=58.705 Prec@5=81.783 rate=3.03 Hz, eta=0:10:59, total=0:02:45, wall=02:08 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.242 Loss=1.738 Prec@1=58.705 Prec@5=81.783 rate=3.03 Hz, eta=0:10:59, total=0:02:45, wall=02:09 IST=> training   20.02% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.740 Prec@1=58.691 Prec@5=81.722 rate=3.03 Hz, eta=0:10:59, total=0:02:45, wall=02:09 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.740 Prec@1=58.691 Prec@5=81.722 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=02:09 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.342 DataTime=0.243 Loss=1.740 Prec@1=58.691 Prec@5=81.722 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=02:09 IST=> training   24.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.242 Loss=1.740 Prec@1=58.697 Prec@5=81.707 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=02:09 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.242 Loss=1.740 Prec@1=58.697 Prec@5=81.707 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=02:09 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.242 Loss=1.740 Prec@1=58.697 Prec@5=81.707 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=02:10 IST=> training   28.01% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.240 Loss=1.739 Prec@1=58.737 Prec@5=81.732 rate=3.01 Hz, eta=0:09:58, total=0:03:52, wall=02:10 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.240 Loss=1.739 Prec@1=58.737 Prec@5=81.732 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=02:10 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.240 Loss=1.739 Prec@1=58.737 Prec@5=81.732 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=02:11 IST=> training   32.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.241 Loss=1.739 Prec@1=58.718 Prec@5=81.713 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=02:11 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.241 Loss=1.739 Prec@1=58.718 Prec@5=81.713 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:11 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.341 DataTime=0.241 Loss=1.739 Prec@1=58.718 Prec@5=81.713 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:11 IST=> training   36.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.240 Loss=1.740 Prec@1=58.724 Prec@5=81.685 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:11 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.240 Loss=1.740 Prec@1=58.724 Prec@5=81.685 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=02:11 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.240 Loss=1.740 Prec@1=58.724 Prec@5=81.685 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=02:12 IST=> training   39.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.739 Prec@1=58.728 Prec@5=81.669 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=02:12 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.739 Prec@1=58.728 Prec@5=81.669 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:12 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.739 Prec@1=58.728 Prec@5=81.669 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:12 IST=> training   43.99% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.740 Prec@1=58.688 Prec@5=81.652 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=02:12 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.740 Prec@1=58.688 Prec@5=81.652 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=02:12 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.340 DataTime=0.238 Loss=1.740 Prec@1=58.688 Prec@5=81.652 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=02:13 IST=> training   47.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.236 Loss=1.742 Prec@1=58.666 Prec@5=81.624 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=02:13 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.236 Loss=1.742 Prec@1=58.666 Prec@5=81.624 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=02:13 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.339 DataTime=0.236 Loss=1.742 Prec@1=58.666 Prec@5=81.624 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=02:13 IST=> training   51.98% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.669 Prec@5=81.625 rate=2.99 Hz, eta=0:06:41, total=0:07:15, wall=02:13 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.669 Prec@5=81.625 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:13 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.669 Prec@5=81.625 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:14 IST=> training   55.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.662 Prec@5=81.610 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:14 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.662 Prec@5=81.610 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:14 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.742 Prec@1=58.662 Prec@5=81.610 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:15 IST=> training   59.97% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.743 Prec@1=58.643 Prec@5=81.616 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=02:15 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.743 Prec@1=58.643 Prec@5=81.616 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=02:15 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.236 Loss=1.743 Prec@1=58.643 Prec@5=81.616 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=02:15 IST=> training   63.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.743 Prec@1=58.643 Prec@5=81.594 rate=2.99 Hz, eta=0:05:01, total=0:08:55, wall=02:15 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.743 Prec@1=58.643 Prec@5=81.594 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=02:15 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.743 Prec@1=58.643 Prec@5=81.594 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=02:16 IST=> training   67.96% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.744 Prec@1=58.629 Prec@5=81.579 rate=2.99 Hz, eta=0:04:28, total=0:09:28, wall=02:16 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.744 Prec@1=58.629 Prec@5=81.579 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=02:16 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.744 Prec@1=58.629 Prec@5=81.579 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=02:16 IST=> training   71.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.745 Prec@1=58.614 Prec@5=81.564 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=02:16 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.745 Prec@1=58.614 Prec@5=81.564 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:16 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.745 Prec@1=58.614 Prec@5=81.564 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:17 IST=> training   75.95% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.746 Prec@1=58.605 Prec@5=81.550 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=02:17 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.746 Prec@1=58.605 Prec@5=81.550 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=02:17 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.746 Prec@1=58.605 Prec@5=81.550 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=02:17 IST=> training   79.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.747 Prec@1=58.592 Prec@5=81.545 rate=2.99 Hz, eta=0:02:48, total=0:11:10, wall=02:17 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.747 Prec@1=58.592 Prec@5=81.545 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:17 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.235 Loss=1.747 Prec@1=58.592 Prec@5=81.545 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:18 IST=> training   83.94% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.747 Prec@1=58.583 Prec@5=81.540 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:18 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.747 Prec@1=58.583 Prec@5=81.540 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:18 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.747 Prec@1=58.583 Prec@5=81.540 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:18 IST=> training   87.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.748 Prec@1=58.562 Prec@5=81.530 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:18 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.748 Prec@1=58.562 Prec@5=81.530 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=02:18 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.748 Prec@1=58.562 Prec@5=81.530 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=02:19 IST=> training   91.93% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.749 Prec@1=58.546 Prec@5=81.508 rate=2.98 Hz, eta=0:01:07, total=0:12:50, wall=02:19 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.749 Prec@1=58.546 Prec@5=81.508 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=02:19 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.338 DataTime=0.234 Loss=1.749 Prec@1=58.546 Prec@5=81.508 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=02:20 IST=> training   95.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.337 DataTime=0.233 Loss=1.750 Prec@1=58.533 Prec@5=81.497 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=02:20 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.337 DataTime=0.233 Loss=1.750 Prec@1=58.533 Prec@5=81.497 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=02:20 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.337 DataTime=0.233 Loss=1.750 Prec@1=58.533 Prec@5=81.497 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=02:20 IST=> training   99.92% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.337 DataTime=0.233 Loss=1.750 Prec@1=58.531 Prec@5=81.496 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=02:20 IST=> training   100.00% of 1x2503...Epoch=110/150 LR=0.01733 Time=0.337 DataTime=0.233 Loss=1.750 Prec@1=58.531 Prec@5=81.496 rate=2.99 Hz, eta=0:00:00, total=0:13:57, wall=02:20 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> validation 0.00% of 1x98...Epoch=110/150 LR=0.01733 Time=7.412 Loss=1.887 Prec@1=54.297 Prec@5=79.102 rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=7.412 Loss=1.887 Prec@1=54.297 Prec@5=79.102 rate=5084.92 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=7.412 Loss=1.887 Prec@1=54.297 Prec@5=79.102 rate=5084.92 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST** validation 1.02% of 1x98...Epoch=110/150 LR=0.01733 Time=0.400 Loss=1.784 Prec@1=58.130 Prec@5=81.314 rate=5084.92 Hz, eta=0:00:00, total=0:00:00, wall=02:20 IST** validation 100.00% of 1x98...Epoch=110/150 LR=0.01733 Time=0.400 Loss=1.784 Prec@1=58.130 Prec@5=81.314 rate=3.08 Hz, eta=0:00:00, total=0:00:31, wall=02:20 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.719 DataTime=4.563 Loss=1.849 Prec@1=53.711 Prec@5=78.516 rate=0 Hz, eta=?, total=0:00:00, wall=02:20 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.719 DataTime=4.563 Loss=1.849 Prec@1=53.711 Prec@5=78.516 rate=2437.19 Hz, eta=0:00:01, total=0:00:00, wall=02:20 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=4.719 DataTime=4.563 Loss=1.849 Prec@1=53.711 Prec@5=78.516 rate=2437.19 Hz, eta=0:00:01, total=0:00:00, wall=02:21 IST=> training   0.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.375 DataTime=0.277 Loss=1.730 Prec@1=58.826 Prec@5=81.710 rate=2437.19 Hz, eta=0:00:01, total=0:00:00, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.375 DataTime=0.277 Loss=1.730 Prec@1=58.826 Prec@5=81.710 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.375 DataTime=0.277 Loss=1.730 Prec@1=58.826 Prec@5=81.710 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=02:21 IST=> training   4.04% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.353 DataTime=0.255 Loss=1.731 Prec@1=58.884 Prec@5=81.639 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=02:21 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.353 DataTime=0.255 Loss=1.731 Prec@1=58.884 Prec@5=81.639 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=02:21 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.353 DataTime=0.255 Loss=1.731 Prec@1=58.884 Prec@5=81.639 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=02:22 IST=> training   8.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.346 DataTime=0.248 Loss=1.733 Prec@1=58.854 Prec@5=81.589 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=02:22 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.346 DataTime=0.248 Loss=1.733 Prec@1=58.854 Prec@5=81.589 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=02:22 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.346 DataTime=0.248 Loss=1.733 Prec@1=58.854 Prec@5=81.589 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=02:23 IST=> training   12.03% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.344 DataTime=0.245 Loss=1.731 Prec@1=58.934 Prec@5=81.666 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=02:23 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.344 DataTime=0.245 Loss=1.731 Prec@1=58.934 Prec@5=81.666 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=02:23 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.344 DataTime=0.245 Loss=1.731 Prec@1=58.934 Prec@5=81.666 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=02:23 IST=> training   16.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.242 Loss=1.734 Prec@1=58.865 Prec@5=81.664 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=02:23 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.242 Loss=1.734 Prec@1=58.865 Prec@5=81.664 rate=3.02 Hz, eta=0:11:04, total=0:02:46, wall=02:23 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.242 Loss=1.734 Prec@1=58.865 Prec@5=81.664 rate=3.02 Hz, eta=0:11:04, total=0:02:46, wall=02:24 IST=> training   20.02% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.343 DataTime=0.242 Loss=1.734 Prec@1=58.882 Prec@5=81.679 rate=3.02 Hz, eta=0:11:04, total=0:02:46, wall=02:24 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.343 DataTime=0.242 Loss=1.734 Prec@1=58.882 Prec@5=81.679 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=02:24 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.343 DataTime=0.242 Loss=1.734 Prec@1=58.882 Prec@5=81.679 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=02:24 IST=> training   24.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.239 Loss=1.734 Prec@1=58.874 Prec@5=81.680 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=02:24 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.239 Loss=1.734 Prec@1=58.874 Prec@5=81.680 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=02:24 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.341 DataTime=0.239 Loss=1.734 Prec@1=58.874 Prec@5=81.680 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=02:25 IST=> training   28.01% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.863 Prec@5=81.686 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=02:25 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.863 Prec@5=81.686 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=02:25 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.863 Prec@5=81.686 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=02:25 IST=> training   32.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.845 Prec@5=81.675 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=02:25 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.845 Prec@5=81.675 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=02:25 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.338 DataTime=0.236 Loss=1.734 Prec@1=58.845 Prec@5=81.675 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=02:26 IST=> training   36.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.235 Loss=1.733 Prec@1=58.853 Prec@5=81.683 rate=3.00 Hz, eta=0:08:53, total=0:05:00, wall=02:26 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.235 Loss=1.733 Prec@1=58.853 Prec@5=81.683 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=02:26 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.235 Loss=1.733 Prec@1=58.853 Prec@5=81.683 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=02:26 IST=> training   39.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.234 Loss=1.733 Prec@1=58.842 Prec@5=81.680 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=02:26 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.234 Loss=1.733 Prec@1=58.842 Prec@5=81.680 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:26 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.234 Loss=1.733 Prec@1=58.842 Prec@5=81.680 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:27 IST=> training   43.99% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.234 Loss=1.733 Prec@1=58.848 Prec@5=81.671 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=02:27 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.234 Loss=1.733 Prec@1=58.848 Prec@5=81.671 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=02:27 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.234 Loss=1.733 Prec@1=58.848 Prec@5=81.671 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=02:28 IST=> training   47.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.786 Prec@5=81.640 rate=3.01 Hz, eta=0:07:13, total=0:06:39, wall=02:28 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.786 Prec@5=81.640 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:28 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.786 Prec@5=81.640 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:28 IST=> training   51.98% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.781 Prec@5=81.637 rate=3.01 Hz, eta=0:06:39, total=0:07:12, wall=02:28 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.781 Prec@5=81.637 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:28 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.233 Loss=1.736 Prec@1=58.781 Prec@5=81.637 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:29 IST=> training   55.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.737 Prec@1=58.773 Prec@5=81.641 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=02:29 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.737 Prec@1=58.773 Prec@5=81.641 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=02:29 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.737 Prec@1=58.773 Prec@5=81.641 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=02:29 IST=> training   59.97% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.739 Prec@1=58.727 Prec@5=81.622 rate=3.00 Hz, eta=0:05:33, total=0:08:20, wall=02:29 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.739 Prec@1=58.727 Prec@5=81.622 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:29 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.739 Prec@1=58.727 Prec@5=81.622 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:30 IST=> training   63.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.740 Prec@1=58.722 Prec@5=81.606 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:30 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.740 Prec@1=58.722 Prec@5=81.606 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=02:30 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.232 Loss=1.740 Prec@1=58.722 Prec@5=81.606 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=02:30 IST=> training   67.96% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.232 Loss=1.740 Prec@1=58.717 Prec@5=81.601 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=02:30 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.232 Loss=1.740 Prec@1=58.717 Prec@5=81.601 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=02:30 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.337 DataTime=0.232 Loss=1.740 Prec@1=58.717 Prec@5=81.601 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=02:31 IST=> training   71.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.704 Prec@5=81.596 rate=2.99 Hz, eta=0:03:54, total=0:10:01, wall=02:31 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.704 Prec@5=81.596 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=02:31 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.704 Prec@5=81.596 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=02:31 IST=> training   75.95% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.718 Prec@5=81.600 rate=3.00 Hz, eta=0:03:20, total=0:10:34, wall=02:31 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.718 Prec@5=81.600 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:31 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.740 Prec@1=58.718 Prec@5=81.600 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:32 IST=> training   79.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.703 Prec@5=81.584 rate=3.00 Hz, eta=0:02:47, total=0:11:07, wall=02:32 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.703 Prec@5=81.584 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=02:32 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.703 Prec@5=81.584 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=02:33 IST=> training   83.94% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.697 Prec@5=81.589 rate=2.99 Hz, eta=0:02:14, total=0:11:41, wall=02:33 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.697 Prec@5=81.589 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:33 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.741 Prec@1=58.697 Prec@5=81.589 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:33 IST=> training   87.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.588 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=02:33 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.588 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:33 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.588 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:34 IST=> training   91.93% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.583 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=02:34 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.583 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=02:34 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.336 DataTime=0.231 Loss=1.742 Prec@1=58.700 Prec@5=81.583 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=02:34 IST=> training   95.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.335 DataTime=0.230 Loss=1.743 Prec@1=58.681 Prec@5=81.576 rate=3.00 Hz, eta=0:00:34, total=0:13:21, wall=02:34 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.335 DataTime=0.230 Loss=1.743 Prec@1=58.681 Prec@5=81.576 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=02:34 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.335 DataTime=0.230 Loss=1.743 Prec@1=58.681 Prec@5=81.576 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=02:34 IST=> training   99.92% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.335 DataTime=0.230 Loss=1.743 Prec@1=58.678 Prec@5=81.576 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=02:34 IST=> training   100.00% of 1x2503...Epoch=111/150 LR=0.01654 Time=0.335 DataTime=0.230 Loss=1.743 Prec@1=58.678 Prec@5=81.576 rate=3.00 Hz, eta=0:00:00, total=0:13:54, wall=02:34 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:34 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:34 IST=> validation 0.00% of 1x98...Epoch=111/150 LR=0.01654 Time=7.177 Loss=1.709 Prec@1=58.789 Prec@5=81.836 rate=0 Hz, eta=?, total=0:00:00, wall=02:34 IST=> validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=7.177 Loss=1.709 Prec@1=58.789 Prec@5=81.836 rate=9115.52 Hz, eta=0:00:00, total=0:00:00, wall=02:34 IST** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=7.177 Loss=1.709 Prec@1=58.789 Prec@5=81.836 rate=9115.52 Hz, eta=0:00:00, total=0:00:00, wall=02:35 IST** validation 1.02% of 1x98...Epoch=111/150 LR=0.01654 Time=0.394 Loss=1.825 Prec@1=56.962 Prec@5=80.584 rate=9115.52 Hz, eta=0:00:00, total=0:00:00, wall=02:35 IST** validation 100.00% of 1x98...Epoch=111/150 LR=0.01654 Time=0.394 Loss=1.825 Prec@1=56.962 Prec@5=80.584 rate=3.11 Hz, eta=0:00:00, total=0:00:31, wall=02:35 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:35 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:35 IST=> training   0.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.768 DataTime=5.609 Loss=1.673 Prec@1=58.984 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=02:35 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.768 DataTime=5.609 Loss=1.673 Prec@1=58.984 Prec@5=83.008 rate=6294.30 Hz, eta=0:00:00, total=0:00:00, wall=02:35 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=5.768 DataTime=5.609 Loss=1.673 Prec@1=58.984 Prec@5=83.008 rate=6294.30 Hz, eta=0:00:00, total=0:00:00, wall=02:36 IST=> training   0.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.376 DataTime=0.281 Loss=1.734 Prec@1=59.091 Prec@5=81.704 rate=6294.30 Hz, eta=0:00:00, total=0:00:00, wall=02:36 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.376 DataTime=0.281 Loss=1.734 Prec@1=59.091 Prec@5=81.704 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=02:36 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.376 DataTime=0.281 Loss=1.734 Prec@1=59.091 Prec@5=81.704 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=02:36 IST=> training   4.04% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.353 DataTime=0.259 Loss=1.730 Prec@1=59.103 Prec@5=81.686 rate=3.14 Hz, eta=0:12:45, total=0:00:32, wall=02:36 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.353 DataTime=0.259 Loss=1.730 Prec@1=59.103 Prec@5=81.686 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=02:36 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.353 DataTime=0.259 Loss=1.730 Prec@1=59.103 Prec@5=81.686 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=02:37 IST=> training   8.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.348 DataTime=0.252 Loss=1.727 Prec@1=59.117 Prec@5=81.731 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=02:37 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.348 DataTime=0.252 Loss=1.727 Prec@1=59.117 Prec@5=81.731 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=02:37 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.348 DataTime=0.252 Loss=1.727 Prec@1=59.117 Prec@5=81.731 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=02:37 IST=> training   12.03% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.342 DataTime=0.244 Loss=1.727 Prec@1=59.138 Prec@5=81.773 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=02:37 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.342 DataTime=0.244 Loss=1.727 Prec@1=59.138 Prec@5=81.773 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=02:37 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.342 DataTime=0.244 Loss=1.727 Prec@1=59.138 Prec@5=81.773 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=02:38 IST=> training   16.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.343 DataTime=0.244 Loss=1.723 Prec@1=59.166 Prec@5=81.829 rate=3.05 Hz, eta=0:11:29, total=0:02:11, wall=02:38 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.343 DataTime=0.244 Loss=1.723 Prec@1=59.166 Prec@5=81.829 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=02:38 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.343 DataTime=0.244 Loss=1.723 Prec@1=59.166 Prec@5=81.829 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=02:38 IST=> training   20.02% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.723 Prec@1=59.093 Prec@5=81.832 rate=3.01 Hz, eta=0:11:04, total=0:02:46, wall=02:38 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.723 Prec@1=59.093 Prec@5=81.832 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:38 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.723 Prec@1=59.093 Prec@5=81.832 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:39 IST=> training   24.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.725 Prec@1=59.076 Prec@5=81.786 rate=3.02 Hz, eta=0:10:29, total=0:03:18, wall=02:39 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.725 Prec@1=59.076 Prec@5=81.786 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:39 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.241 Loss=1.725 Prec@1=59.076 Prec@5=81.786 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:39 IST=> training   28.01% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.241 Loss=1.725 Prec@1=59.091 Prec@5=81.790 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=02:39 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.241 Loss=1.725 Prec@1=59.091 Prec@5=81.790 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=02:39 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.241 Loss=1.725 Prec@1=59.091 Prec@5=81.790 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=02:40 IST=> training   32.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.240 Loss=1.726 Prec@1=59.055 Prec@5=81.783 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=02:40 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.240 Loss=1.726 Prec@1=59.055 Prec@5=81.783 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:40 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.341 DataTime=0.240 Loss=1.726 Prec@1=59.055 Prec@5=81.783 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:41 IST=> training   36.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.026 Prec@5=81.776 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=02:41 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.026 Prec@5=81.776 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=02:41 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.026 Prec@5=81.776 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=02:41 IST=> training   39.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.009 Prec@5=81.769 rate=2.99 Hz, eta=0:08:21, total=0:05:34, wall=02:41 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.009 Prec@5=81.769 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:41 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.340 DataTime=0.239 Loss=1.727 Prec@1=59.009 Prec@5=81.769 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:42 IST=> training   43.99% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=58.993 Prec@5=81.749 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:42 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=58.993 Prec@5=81.749 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:42 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=58.993 Prec@5=81.749 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:42 IST=> training   47.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.729 Prec@1=59.005 Prec@5=81.745 rate=3.00 Hz, eta=0:07:14, total=0:06:40, wall=02:42 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.729 Prec@1=59.005 Prec@5=81.745 rate=3.00 Hz, eta=0:06:40, total=0:07:14, wall=02:42 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.729 Prec@1=59.005 Prec@5=81.745 rate=3.00 Hz, eta=0:06:40, total=0:07:14, wall=02:43 IST=> training   51.98% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=59.014 Prec@5=81.761 rate=3.00 Hz, eta=0:06:40, total=0:07:14, wall=02:43 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=59.014 Prec@5=81.761 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:43 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.728 Prec@1=59.014 Prec@5=81.761 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:43 IST=> training   55.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.729 Prec@1=59.016 Prec@5=81.739 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=02:43 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.729 Prec@1=59.016 Prec@5=81.739 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=02:43 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.729 Prec@1=59.016 Prec@5=81.739 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=02:44 IST=> training   59.97% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.729 Prec@1=59.011 Prec@5=81.737 rate=2.99 Hz, eta=0:05:34, total=0:08:21, wall=02:44 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.729 Prec@1=59.011 Prec@5=81.737 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:44 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.729 Prec@1=59.011 Prec@5=81.737 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:44 IST=> training   63.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.731 Prec@1=58.991 Prec@5=81.723 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=02:44 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.731 Prec@1=58.991 Prec@5=81.723 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:44 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.237 Loss=1.731 Prec@1=58.991 Prec@5=81.723 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:45 IST=> training   67.96% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.731 Prec@1=58.983 Prec@5=81.715 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=02:45 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.731 Prec@1=58.983 Prec@5=81.715 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=02:45 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.236 Loss=1.731 Prec@1=58.983 Prec@5=81.715 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=02:46 IST=> training   71.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.732 Prec@1=58.969 Prec@5=81.710 rate=2.99 Hz, eta=0:03:54, total=0:10:02, wall=02:46 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.732 Prec@1=58.969 Prec@5=81.710 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=02:46 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.235 Loss=1.732 Prec@1=58.969 Prec@5=81.710 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=02:46 IST=> training   75.95% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.732 Prec@1=58.970 Prec@5=81.721 rate=2.99 Hz, eta=0:03:21, total=0:10:35, wall=02:46 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.732 Prec@1=58.970 Prec@5=81.721 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:46 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.732 Prec@1=58.970 Prec@5=81.721 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:47 IST=> training   79.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.733 Prec@1=58.953 Prec@5=81.713 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=02:47 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.733 Prec@1=58.953 Prec@5=81.713 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:47 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.235 Loss=1.733 Prec@1=58.953 Prec@5=81.713 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:47 IST=> training   83.94% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.734 Prec@1=58.939 Prec@5=81.698 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=02:47 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.734 Prec@1=58.939 Prec@5=81.698 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:47 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.734 Prec@1=58.939 Prec@5=81.698 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:48 IST=> training   87.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.339 DataTime=0.235 Loss=1.734 Prec@1=58.930 Prec@5=81.690 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=02:48 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.339 DataTime=0.235 Loss=1.734 Prec@1=58.930 Prec@5=81.690 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=02:48 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.339 DataTime=0.235 Loss=1.734 Prec@1=58.930 Prec@5=81.690 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=02:48 IST=> training   91.93% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.735 Prec@1=58.926 Prec@5=81.689 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=02:48 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.735 Prec@1=58.926 Prec@5=81.689 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:48 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.338 DataTime=0.234 Loss=1.735 Prec@1=58.926 Prec@5=81.689 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:49 IST=> training   95.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.233 Loss=1.736 Prec@1=58.889 Prec@5=81.677 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=02:49 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.233 Loss=1.736 Prec@1=58.889 Prec@5=81.677 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=02:49 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.233 Loss=1.736 Prec@1=58.889 Prec@5=81.677 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=02:49 IST=> training   99.92% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.233 Loss=1.736 Prec@1=58.889 Prec@5=81.677 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=02:49 IST=> training   100.00% of 1x2503...Epoch=112/150 LR=0.01577 Time=0.337 DataTime=0.233 Loss=1.736 Prec@1=58.889 Prec@5=81.677 rate=2.99 Hz, eta=0:00:00, total=0:13:58, wall=02:49 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:49 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:49 IST=> validation 0.00% of 1x98...Epoch=112/150 LR=0.01577 Time=7.025 Loss=1.733 Prec@1=59.180 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=02:49 IST=> validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=7.025 Loss=1.733 Prec@1=59.180 Prec@5=81.641 rate=5784.23 Hz, eta=0:00:00, total=0:00:00, wall=02:49 IST** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=7.025 Loss=1.733 Prec@1=59.180 Prec@5=81.641 rate=5784.23 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST** validation 1.02% of 1x98...Epoch=112/150 LR=0.01577 Time=0.407 Loss=1.752 Prec@1=58.526 Prec@5=81.638 rate=5784.23 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST** validation 100.00% of 1x98...Epoch=112/150 LR=0.01577 Time=0.407 Loss=1.752 Prec@1=58.526 Prec@5=81.638 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=02:50 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST=> training   0.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.806 DataTime=5.544 Loss=1.610 Prec@1=60.547 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=02:50 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.806 DataTime=5.544 Loss=1.610 Prec@1=60.547 Prec@5=84.766 rate=5007.16 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=5.806 DataTime=5.544 Loss=1.610 Prec@1=60.547 Prec@5=84.766 rate=5007.16 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST=> training   0.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.375 DataTime=0.269 Loss=1.727 Prec@1=59.164 Prec@5=81.907 rate=5007.16 Hz, eta=0:00:00, total=0:00:00, wall=02:50 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.375 DataTime=0.269 Loss=1.727 Prec@1=59.164 Prec@5=81.907 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:50 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.375 DataTime=0.269 Loss=1.727 Prec@1=59.164 Prec@5=81.907 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:51 IST=> training   4.04% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.354 DataTime=0.253 Loss=1.722 Prec@1=59.213 Prec@5=81.816 rate=3.15 Hz, eta=0:12:41, total=0:00:32, wall=02:51 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.354 DataTime=0.253 Loss=1.722 Prec@1=59.213 Prec@5=81.816 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=02:51 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.354 DataTime=0.253 Loss=1.722 Prec@1=59.213 Prec@5=81.816 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=02:51 IST=> training   8.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.245 Loss=1.719 Prec@1=59.215 Prec@5=81.885 rate=3.07 Hz, eta=0:12:28, total=0:01:05, wall=02:51 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.245 Loss=1.719 Prec@1=59.215 Prec@5=81.885 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:51 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.245 Loss=1.719 Prec@1=59.215 Prec@5=81.885 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:52 IST=> training   12.03% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.347 DataTime=0.245 Loss=1.718 Prec@1=59.231 Prec@5=81.946 rate=3.06 Hz, eta=0:11:59, total=0:01:38, wall=02:52 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.347 DataTime=0.245 Loss=1.718 Prec@1=59.231 Prec@5=81.946 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=02:52 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.347 DataTime=0.245 Loss=1.718 Prec@1=59.231 Prec@5=81.946 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=02:53 IST=> training   16.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.243 Loss=1.718 Prec@1=59.181 Prec@5=81.946 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=02:53 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.243 Loss=1.718 Prec@1=59.181 Prec@5=81.946 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=02:53 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.346 DataTime=0.243 Loss=1.718 Prec@1=59.181 Prec@5=81.946 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=02:53 IST=> training   20.02% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.344 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.952 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=02:53 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.344 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.952 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=02:53 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.344 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.952 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=02:54 IST=> training   24.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.345 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.973 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=02:54 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.345 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.973 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=02:54 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.345 DataTime=0.242 Loss=1.719 Prec@1=59.139 Prec@5=81.973 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=02:54 IST=> training   28.01% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.343 DataTime=0.241 Loss=1.719 Prec@1=59.137 Prec@5=81.967 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=02:54 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.343 DataTime=0.241 Loss=1.719 Prec@1=59.137 Prec@5=81.967 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=02:54 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.343 DataTime=0.241 Loss=1.719 Prec@1=59.137 Prec@5=81.967 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=02:55 IST=> training   32.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.342 DataTime=0.239 Loss=1.720 Prec@1=59.135 Prec@5=81.948 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=02:55 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.342 DataTime=0.239 Loss=1.720 Prec@1=59.135 Prec@5=81.948 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:55 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.342 DataTime=0.239 Loss=1.720 Prec@1=59.135 Prec@5=81.948 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:55 IST=> training   36.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.237 Loss=1.720 Prec@1=59.134 Prec@5=81.956 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=02:55 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.237 Loss=1.720 Prec@1=59.134 Prec@5=81.956 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=02:55 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.237 Loss=1.720 Prec@1=59.134 Prec@5=81.956 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=02:56 IST=> training   39.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.236 Loss=1.721 Prec@1=59.099 Prec@5=81.918 rate=2.98 Hz, eta=0:08:24, total=0:05:35, wall=02:56 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.236 Loss=1.721 Prec@1=59.099 Prec@5=81.918 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:56 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.236 Loss=1.721 Prec@1=59.099 Prec@5=81.918 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:56 IST=> training   43.99% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.720 Prec@1=59.111 Prec@5=81.931 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=02:56 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.720 Prec@1=59.111 Prec@5=81.931 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=02:56 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.720 Prec@1=59.111 Prec@5=81.931 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=02:57 IST=> training   47.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.236 Loss=1.721 Prec@1=59.103 Prec@5=81.909 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=02:57 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.236 Loss=1.721 Prec@1=59.103 Prec@5=81.909 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=02:57 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.236 Loss=1.721 Prec@1=59.103 Prec@5=81.909 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=02:58 IST=> training   51.98% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.722 Prec@1=59.062 Prec@5=81.881 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=02:58 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.722 Prec@1=59.062 Prec@5=81.881 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:58 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.722 Prec@1=59.062 Prec@5=81.881 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:58 IST=> training   55.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.723 Prec@1=59.058 Prec@5=81.866 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=02:58 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.723 Prec@1=59.058 Prec@5=81.866 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=02:58 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.723 Prec@1=59.058 Prec@5=81.866 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=02:59 IST=> training   59.97% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.235 Loss=1.722 Prec@1=59.075 Prec@5=81.880 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=02:59 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.235 Loss=1.722 Prec@1=59.075 Prec@5=81.880 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:59 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.341 DataTime=0.235 Loss=1.722 Prec@1=59.075 Prec@5=81.880 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:59 IST=> training   63.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.046 Prec@5=81.867 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=02:59 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.046 Prec@5=81.867 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=02:59 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.046 Prec@5=81.867 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:00 IST=> training   67.96% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.234 Loss=1.724 Prec@1=59.045 Prec@5=81.858 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:00 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.234 Loss=1.724 Prec@1=59.045 Prec@5=81.858 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:00 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.234 Loss=1.724 Prec@1=59.045 Prec@5=81.858 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:00 IST=> training   71.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.053 Prec@5=81.867 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:00 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.053 Prec@5=81.867 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=03:00 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.340 DataTime=0.235 Loss=1.724 Prec@1=59.053 Prec@5=81.867 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=03:01 IST=> training   75.95% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.725 Prec@1=59.040 Prec@5=81.850 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=03:01 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.725 Prec@1=59.040 Prec@5=81.850 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=03:01 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.725 Prec@1=59.040 Prec@5=81.850 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=03:02 IST=> training   79.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.725 Prec@1=59.035 Prec@5=81.835 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=03:02 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.725 Prec@1=59.035 Prec@5=81.835 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:02 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.725 Prec@1=59.035 Prec@5=81.835 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:02 IST=> training   83.94% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.726 Prec@1=59.029 Prec@5=81.836 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=03:02 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.726 Prec@1=59.029 Prec@5=81.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=03:02 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.234 Loss=1.726 Prec@1=59.029 Prec@5=81.836 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=03:03 IST=> training   87.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.027 Prec@5=81.840 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=03:03 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.027 Prec@5=81.840 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=03:03 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.027 Prec@5=81.840 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=03:03 IST=> training   91.93% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.023 Prec@5=81.837 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=03:03 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.023 Prec@5=81.837 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=03:03 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.339 DataTime=0.233 Loss=1.726 Prec@1=59.023 Prec@5=81.837 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=03:04 IST=> training   95.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.338 DataTime=0.233 Loss=1.727 Prec@1=59.019 Prec@5=81.825 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=03:04 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.338 DataTime=0.233 Loss=1.727 Prec@1=59.019 Prec@5=81.825 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=03:04 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.338 DataTime=0.233 Loss=1.727 Prec@1=59.019 Prec@5=81.825 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=03:04 IST=> training   99.92% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.338 DataTime=0.233 Loss=1.727 Prec@1=59.018 Prec@5=81.824 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=03:04 IST=> training   100.00% of 1x2503...Epoch=113/150 LR=0.01502 Time=0.338 DataTime=0.233 Loss=1.727 Prec@1=59.018 Prec@5=81.824 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=03:04 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:04 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:04 IST=> validation 0.00% of 1x98...Epoch=113/150 LR=0.01502 Time=6.278 Loss=1.821 Prec@1=56.836 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=03:04 IST=> validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=6.278 Loss=1.821 Prec@1=56.836 Prec@5=81.445 rate=6635.04 Hz, eta=0:00:00, total=0:00:00, wall=03:04 IST** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=6.278 Loss=1.821 Prec@1=56.836 Prec@5=81.445 rate=6635.04 Hz, eta=0:00:00, total=0:00:00, wall=03:04 IST** validation 1.02% of 1x98...Epoch=113/150 LR=0.01502 Time=0.401 Loss=1.821 Prec@1=56.944 Prec@5=80.652 rate=6635.04 Hz, eta=0:00:00, total=0:00:00, wall=03:04 IST** validation 100.00% of 1x98...Epoch=113/150 LR=0.01502 Time=0.401 Loss=1.821 Prec@1=56.944 Prec@5=80.652 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=03:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:05 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:05 IST=> training   0.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=5.896 DataTime=5.747 Loss=1.787 Prec@1=57.617 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=03:05 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=5.896 DataTime=5.747 Loss=1.787 Prec@1=57.617 Prec@5=82.422 rate=7133.53 Hz, eta=0:00:00, total=0:00:00, wall=03:05 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=5.896 DataTime=5.747 Loss=1.787 Prec@1=57.617 Prec@5=82.422 rate=7133.53 Hz, eta=0:00:00, total=0:00:00, wall=03:05 IST=> training   0.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.376 DataTime=0.279 Loss=1.715 Prec@1=59.278 Prec@5=82.107 rate=7133.53 Hz, eta=0:00:00, total=0:00:00, wall=03:05 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.376 DataTime=0.279 Loss=1.715 Prec@1=59.278 Prec@5=82.107 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:05 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.376 DataTime=0.279 Loss=1.715 Prec@1=59.278 Prec@5=82.107 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:06 IST=> training   4.04% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.352 DataTime=0.255 Loss=1.707 Prec@1=59.460 Prec@5=82.196 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:06 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.352 DataTime=0.255 Loss=1.707 Prec@1=59.460 Prec@5=82.196 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=03:06 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.352 DataTime=0.255 Loss=1.707 Prec@1=59.460 Prec@5=82.196 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=03:06 IST=> training   8.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.348 DataTime=0.251 Loss=1.706 Prec@1=59.428 Prec@5=82.184 rate=3.10 Hz, eta=0:12:23, total=0:01:04, wall=03:06 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.348 DataTime=0.251 Loss=1.706 Prec@1=59.428 Prec@5=82.184 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=03:06 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.348 DataTime=0.251 Loss=1.706 Prec@1=59.428 Prec@5=82.184 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=03:07 IST=> training   12.03% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.344 DataTime=0.245 Loss=1.706 Prec@1=59.398 Prec@5=82.204 rate=3.04 Hz, eta=0:12:03, total=0:01:38, wall=03:07 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.344 DataTime=0.245 Loss=1.706 Prec@1=59.398 Prec@5=82.204 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=03:07 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.344 DataTime=0.245 Loss=1.706 Prec@1=59.398 Prec@5=82.204 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=03:07 IST=> training   16.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.342 DataTime=0.243 Loss=1.709 Prec@1=59.367 Prec@5=82.125 rate=3.04 Hz, eta=0:11:31, total=0:02:11, wall=03:07 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.342 DataTime=0.243 Loss=1.709 Prec@1=59.367 Prec@5=82.125 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=03:07 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.342 DataTime=0.243 Loss=1.709 Prec@1=59.367 Prec@5=82.125 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=03:08 IST=> training   20.02% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.239 Loss=1.710 Prec@1=59.340 Prec@5=82.097 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=03:08 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.239 Loss=1.710 Prec@1=59.340 Prec@5=82.097 rate=3.05 Hz, eta=0:10:23, total=0:03:17, wall=03:08 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.239 Loss=1.710 Prec@1=59.340 Prec@5=82.097 rate=3.05 Hz, eta=0:10:23, total=0:03:17, wall=03:08 IST=> training   24.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.241 Loss=1.710 Prec@1=59.321 Prec@5=82.072 rate=3.05 Hz, eta=0:10:23, total=0:03:17, wall=03:08 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.241 Loss=1.710 Prec@1=59.321 Prec@5=82.072 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=03:08 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.241 Loss=1.710 Prec@1=59.321 Prec@5=82.072 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=03:09 IST=> training   28.01% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.239 Loss=1.709 Prec@1=59.369 Prec@5=82.086 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=03:09 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.239 Loss=1.709 Prec@1=59.369 Prec@5=82.086 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=03:09 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.340 DataTime=0.239 Loss=1.709 Prec@1=59.369 Prec@5=82.086 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=03:10 IST=> training   32.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.711 Prec@1=59.340 Prec@5=82.051 rate=3.01 Hz, eta=0:09:26, total=0:04:26, wall=03:10 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.711 Prec@1=59.340 Prec@5=82.051 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=03:10 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.711 Prec@1=59.340 Prec@5=82.051 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=03:10 IST=> training   36.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.712 Prec@1=59.317 Prec@5=82.025 rate=3.02 Hz, eta=0:08:50, total=0:04:58, wall=03:10 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.712 Prec@1=59.317 Prec@5=82.025 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=03:10 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.236 Loss=1.712 Prec@1=59.317 Prec@5=82.025 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=03:11 IST=> training   39.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.235 Loss=1.713 Prec@1=59.307 Prec@5=82.023 rate=3.01 Hz, eta=0:08:19, total=0:05:32, wall=03:11 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.235 Loss=1.713 Prec@1=59.307 Prec@5=82.023 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:11 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.338 DataTime=0.235 Loss=1.713 Prec@1=59.307 Prec@5=82.023 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:11 IST=> training   43.99% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.314 Prec@5=82.006 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=03:11 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.314 Prec@5=82.006 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=03:11 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.314 Prec@5=82.006 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=03:12 IST=> training   47.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.234 Loss=1.712 Prec@1=59.319 Prec@5=82.013 rate=3.01 Hz, eta=0:07:11, total=0:06:38, wall=03:12 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.234 Loss=1.712 Prec@1=59.319 Prec@5=82.013 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:12 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.234 Loss=1.712 Prec@1=59.319 Prec@5=82.013 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:12 IST=> training   51.98% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.297 Prec@5=82.020 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=03:12 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.297 Prec@5=82.020 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=03:12 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.337 DataTime=0.233 Loss=1.713 Prec@1=59.297 Prec@5=82.020 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=03:13 IST=> training   55.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.232 Loss=1.714 Prec@1=59.300 Prec@5=81.999 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=03:13 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.232 Loss=1.714 Prec@1=59.300 Prec@5=81.999 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=03:13 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.232 Loss=1.714 Prec@1=59.300 Prec@5=81.999 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=03:13 IST=> training   59.97% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.233 Loss=1.714 Prec@1=59.281 Prec@5=82.001 rate=3.02 Hz, eta=0:05:32, total=0:08:17, wall=03:13 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.233 Loss=1.714 Prec@1=59.281 Prec@5=82.001 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=03:13 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.336 DataTime=0.233 Loss=1.714 Prec@1=59.281 Prec@5=82.001 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=03:14 IST=> training   63.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.715 Prec@1=59.259 Prec@5=81.983 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=03:14 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.715 Prec@1=59.259 Prec@5=81.983 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=03:14 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.715 Prec@1=59.259 Prec@5=81.983 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=03:15 IST=> training   67.96% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.716 Prec@1=59.256 Prec@5=81.969 rate=3.02 Hz, eta=0:04:25, total=0:09:24, wall=03:15 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.716 Prec@1=59.256 Prec@5=81.969 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=03:15 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.232 Loss=1.716 Prec@1=59.256 Prec@5=81.969 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=03:15 IST=> training   71.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.716 Prec@1=59.250 Prec@5=81.970 rate=3.02 Hz, eta=0:03:52, total=0:09:56, wall=03:15 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.716 Prec@1=59.250 Prec@5=81.970 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=03:15 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.716 Prec@1=59.250 Prec@5=81.970 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=03:16 IST=> training   75.95% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.717 Prec@1=59.249 Prec@5=81.955 rate=3.01 Hz, eta=0:03:19, total=0:10:31, wall=03:16 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.717 Prec@1=59.249 Prec@5=81.955 rate=3.01 Hz, eta=0:02:46, total=0:11:03, wall=03:16 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.717 Prec@1=59.249 Prec@5=81.955 rate=3.01 Hz, eta=0:02:46, total=0:11:03, wall=03:16 IST=> training   79.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.717 Prec@1=59.243 Prec@5=81.962 rate=3.01 Hz, eta=0:02:46, total=0:11:03, wall=03:16 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.717 Prec@1=59.243 Prec@5=81.962 rate=3.02 Hz, eta=0:02:13, total=0:11:36, wall=03:16 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.717 Prec@1=59.243 Prec@5=81.962 rate=3.02 Hz, eta=0:02:13, total=0:11:36, wall=03:17 IST=> training   83.94% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.233 Loss=1.717 Prec@1=59.235 Prec@5=81.955 rate=3.02 Hz, eta=0:02:13, total=0:11:36, wall=03:17 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.233 Loss=1.717 Prec@1=59.235 Prec@5=81.955 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=03:17 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.233 Loss=1.717 Prec@1=59.235 Prec@5=81.955 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=03:17 IST=> training   87.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.718 Prec@1=59.226 Prec@5=81.958 rate=3.01 Hz, eta=0:01:40, total=0:12:10, wall=03:17 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.718 Prec@1=59.226 Prec@5=81.958 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=03:17 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.335 DataTime=0.233 Loss=1.718 Prec@1=59.226 Prec@5=81.958 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=03:18 IST=> training   91.93% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.719 Prec@1=59.209 Prec@5=81.942 rate=3.01 Hz, eta=0:01:07, total=0:12:44, wall=03:18 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.719 Prec@1=59.209 Prec@5=81.942 rate=3.02 Hz, eta=0:00:33, total=0:13:16, wall=03:18 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.719 Prec@1=59.209 Prec@5=81.942 rate=3.02 Hz, eta=0:00:33, total=0:13:16, wall=03:18 IST=> training   95.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.720 Prec@1=59.199 Prec@5=81.931 rate=3.02 Hz, eta=0:00:33, total=0:13:16, wall=03:18 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.720 Prec@1=59.199 Prec@5=81.931 rate=3.02 Hz, eta=0:00:00, total=0:13:48, wall=03:18 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.720 Prec@1=59.199 Prec@5=81.931 rate=3.02 Hz, eta=0:00:00, total=0:13:48, wall=03:18 IST=> training   99.92% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.720 Prec@1=59.198 Prec@5=81.931 rate=3.02 Hz, eta=0:00:00, total=0:13:48, wall=03:18 IST=> training   100.00% of 1x2503...Epoch=114/150 LR=0.01428 Time=0.334 DataTime=0.232 Loss=1.720 Prec@1=59.198 Prec@5=81.931 rate=3.02 Hz, eta=0:00:00, total=0:13:48, wall=03:18 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> validation 0.00% of 1x98...Epoch=114/150 LR=0.01428 Time=6.648 Loss=1.701 Prec@1=60.938 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=6.648 Loss=1.701 Prec@1=60.938 Prec@5=84.180 rate=2286.51 Hz, eta=0:00:00, total=0:00:00, wall=03:19 IST** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=6.648 Loss=1.701 Prec@1=60.938 Prec@5=84.180 rate=2286.51 Hz, eta=0:00:00, total=0:00:00, wall=03:19 IST** validation 1.02% of 1x98...Epoch=114/150 LR=0.01428 Time=0.404 Loss=1.772 Prec@1=58.216 Prec@5=81.316 rate=2286.51 Hz, eta=0:00:00, total=0:00:00, wall=03:19 IST** validation 100.00% of 1x98...Epoch=114/150 LR=0.01428 Time=0.404 Loss=1.772 Prec@1=58.216 Prec@5=81.316 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=03:19 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> training   0.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=5.851 DataTime=5.765 Loss=1.797 Prec@1=60.352 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=03:19 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=5.851 DataTime=5.765 Loss=1.797 Prec@1=60.352 Prec@5=81.055 rate=7037.15 Hz, eta=0:00:00, total=0:00:00, wall=03:19 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=5.851 DataTime=5.765 Loss=1.797 Prec@1=60.352 Prec@5=81.055 rate=7037.15 Hz, eta=0:00:00, total=0:00:00, wall=03:20 IST=> training   0.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.378 DataTime=0.284 Loss=1.704 Prec@1=59.694 Prec@5=82.190 rate=7037.15 Hz, eta=0:00:00, total=0:00:00, wall=03:20 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.378 DataTime=0.284 Loss=1.704 Prec@1=59.694 Prec@5=82.190 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=03:20 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.378 DataTime=0.284 Loss=1.704 Prec@1=59.694 Prec@5=82.190 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=03:20 IST=> training   4.04% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.356 DataTime=0.259 Loss=1.706 Prec@1=59.489 Prec@5=82.156 rate=3.12 Hz, eta=0:12:48, total=0:00:32, wall=03:20 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.356 DataTime=0.259 Loss=1.706 Prec@1=59.489 Prec@5=82.156 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=03:20 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.356 DataTime=0.259 Loss=1.706 Prec@1=59.489 Prec@5=82.156 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=03:21 IST=> training   8.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.351 DataTime=0.249 Loss=1.703 Prec@1=59.553 Prec@5=82.183 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=03:21 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.351 DataTime=0.249 Loss=1.703 Prec@1=59.553 Prec@5=82.183 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=03:21 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.351 DataTime=0.249 Loss=1.703 Prec@1=59.553 Prec@5=82.183 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=03:21 IST=> training   12.03% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.348 DataTime=0.244 Loss=1.700 Prec@1=59.639 Prec@5=82.223 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=03:21 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.348 DataTime=0.244 Loss=1.700 Prec@1=59.639 Prec@5=82.223 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=03:21 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.348 DataTime=0.244 Loss=1.700 Prec@1=59.639 Prec@5=82.223 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=03:22 IST=> training   16.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.346 DataTime=0.242 Loss=1.702 Prec@1=59.587 Prec@5=82.202 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=03:22 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.346 DataTime=0.242 Loss=1.702 Prec@1=59.587 Prec@5=82.202 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:22 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.346 DataTime=0.242 Loss=1.702 Prec@1=59.587 Prec@5=82.202 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:23 IST=> training   20.02% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.240 Loss=1.702 Prec@1=59.627 Prec@5=82.198 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=03:23 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.240 Loss=1.702 Prec@1=59.627 Prec@5=82.198 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:23 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.240 Loss=1.702 Prec@1=59.627 Prec@5=82.198 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:23 IST=> training   24.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.345 DataTime=0.240 Loss=1.704 Prec@1=59.570 Prec@5=82.136 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=03:23 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.345 DataTime=0.240 Loss=1.704 Prec@1=59.570 Prec@5=82.136 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=03:23 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.345 DataTime=0.240 Loss=1.704 Prec@1=59.570 Prec@5=82.136 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=03:24 IST=> training   28.01% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.238 Loss=1.707 Prec@1=59.510 Prec@5=82.110 rate=2.97 Hz, eta=0:10:06, total=0:03:56, wall=03:24 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.238 Loss=1.707 Prec@1=59.510 Prec@5=82.110 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:24 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.344 DataTime=0.238 Loss=1.707 Prec@1=59.510 Prec@5=82.110 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:24 IST=> training   32.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.706 Prec@1=59.499 Prec@5=82.124 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:24 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.706 Prec@1=59.499 Prec@5=82.124 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=03:24 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.706 Prec@1=59.499 Prec@5=82.124 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=03:25 IST=> training   36.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.708 Prec@1=59.451 Prec@5=82.099 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=03:25 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.708 Prec@1=59.451 Prec@5=82.099 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=03:25 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.343 DataTime=0.237 Loss=1.708 Prec@1=59.451 Prec@5=82.099 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=03:25 IST=> training   39.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.342 DataTime=0.235 Loss=1.709 Prec@1=59.440 Prec@5=82.080 rate=2.96 Hz, eta=0:08:26, total=0:05:37, wall=03:25 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.342 DataTime=0.235 Loss=1.709 Prec@1=59.440 Prec@5=82.080 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:25 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.342 DataTime=0.235 Loss=1.709 Prec@1=59.440 Prec@5=82.080 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:26 IST=> training   43.99% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.708 Prec@1=59.440 Prec@5=82.115 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=03:26 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.708 Prec@1=59.440 Prec@5=82.115 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=03:26 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.708 Prec@1=59.440 Prec@5=82.115 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=03:26 IST=> training   47.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.709 Prec@1=59.406 Prec@5=82.096 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=03:26 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.709 Prec@1=59.406 Prec@5=82.096 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:26 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.709 Prec@1=59.406 Prec@5=82.096 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:27 IST=> training   51.98% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.380 Prec@5=82.099 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:27 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.380 Prec@5=82.099 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=03:27 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.380 Prec@5=82.099 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=03:28 IST=> training   55.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.710 Prec@1=59.389 Prec@5=82.101 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.710 Prec@1=59.389 Prec@5=82.101 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.341 DataTime=0.234 Loss=1.710 Prec@1=59.389 Prec@5=82.101 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:28 IST=> training   59.97% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.394 Prec@5=82.101 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=03:28 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.394 Prec@5=82.101 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:28 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.710 Prec@1=59.394 Prec@5=82.101 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:29 IST=> training   63.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.383 Prec@5=82.095 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=03:29 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.383 Prec@5=82.095 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:29 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.383 Prec@5=82.095 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:29 IST=> training   67.96% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.711 Prec@1=59.359 Prec@5=82.064 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:29 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.711 Prec@1=59.359 Prec@5=82.064 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:29 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.711 Prec@1=59.359 Prec@5=82.064 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:30 IST=> training   71.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.349 Prec@5=82.073 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=03:30 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.349 Prec@5=82.073 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=03:30 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.710 Prec@1=59.349 Prec@5=82.073 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=03:30 IST=> training   75.95% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.712 Prec@1=59.324 Prec@5=82.051 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=03:30 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.712 Prec@1=59.324 Prec@5=82.051 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=03:30 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.712 Prec@1=59.324 Prec@5=82.051 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=03:31 IST=> training   79.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.713 Prec@1=59.322 Prec@5=82.046 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=03:31 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.713 Prec@1=59.322 Prec@5=82.046 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:31 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.713 Prec@1=59.322 Prec@5=82.046 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:32 IST=> training   83.94% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.713 Prec@1=59.310 Prec@5=82.038 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=03:32 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.713 Prec@1=59.310 Prec@5=82.038 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=03:32 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.234 Loss=1.713 Prec@1=59.310 Prec@5=82.038 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=03:32 IST=> training   87.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.714 Prec@1=59.296 Prec@5=82.032 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=03:32 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.714 Prec@1=59.296 Prec@5=82.032 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=03:32 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.340 DataTime=0.233 Loss=1.714 Prec@1=59.296 Prec@5=82.032 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=03:33 IST=> training   91.93% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.233 Loss=1.714 Prec@1=59.293 Prec@5=82.025 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=03:33 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.233 Loss=1.714 Prec@1=59.293 Prec@5=82.025 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=03:33 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.233 Loss=1.714 Prec@1=59.293 Prec@5=82.025 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=03:33 IST=> training   95.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.232 Loss=1.715 Prec@1=59.272 Prec@5=82.006 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=03:33 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.232 Loss=1.715 Prec@1=59.272 Prec@5=82.006 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=03:33 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.232 Loss=1.715 Prec@1=59.272 Prec@5=82.006 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=03:33 IST=> training   99.92% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.232 Loss=1.715 Prec@1=59.270 Prec@5=82.004 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=03:33 IST=> training   100.00% of 1x2503...Epoch=115/150 LR=0.01355 Time=0.339 DataTime=0.232 Loss=1.715 Prec@1=59.270 Prec@5=82.004 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=03:33 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:33 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:33 IST=> validation 0.00% of 1x98...Epoch=115/150 LR=0.01355 Time=6.483 Loss=1.765 Prec@1=57.617 Prec@5=80.273 rate=0 Hz, eta=?, total=0:00:00, wall=03:33 IST=> validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.483 Loss=1.765 Prec@1=57.617 Prec@5=80.273 rate=4936.88 Hz, eta=0:00:00, total=0:00:00, wall=03:33 IST** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=6.483 Loss=1.765 Prec@1=57.617 Prec@5=80.273 rate=4936.88 Hz, eta=0:00:00, total=0:00:00, wall=03:34 IST** validation 1.02% of 1x98...Epoch=115/150 LR=0.01355 Time=0.394 Loss=1.747 Prec@1=58.526 Prec@5=81.762 rate=4936.88 Hz, eta=0:00:00, total=0:00:00, wall=03:34 IST** validation 100.00% of 1x98...Epoch=115/150 LR=0.01355 Time=0.394 Loss=1.747 Prec@1=58.526 Prec@5=81.762 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=03:34 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST=> training   0.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=4.913 DataTime=4.756 Loss=1.678 Prec@1=59.570 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=03:34 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=4.913 DataTime=4.756 Loss=1.678 Prec@1=59.570 Prec@5=83.594 rate=3864.99 Hz, eta=0:00:00, total=0:00:00, wall=03:34 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=4.913 DataTime=4.756 Loss=1.678 Prec@1=59.570 Prec@5=83.594 rate=3864.99 Hz, eta=0:00:00, total=0:00:00, wall=03:35 IST=> training   0.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.376 DataTime=0.278 Loss=1.690 Prec@1=59.677 Prec@5=82.004 rate=3864.99 Hz, eta=0:00:00, total=0:00:00, wall=03:35 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.376 DataTime=0.278 Loss=1.690 Prec@1=59.677 Prec@5=82.004 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=03:35 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.376 DataTime=0.278 Loss=1.690 Prec@1=59.677 Prec@5=82.004 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=03:35 IST=> training   4.04% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.349 DataTime=0.252 Loss=1.680 Prec@1=59.843 Prec@5=82.354 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=03:35 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.349 DataTime=0.252 Loss=1.680 Prec@1=59.843 Prec@5=82.354 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=03:35 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.349 DataTime=0.252 Loss=1.680 Prec@1=59.843 Prec@5=82.354 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=03:36 IST=> training   8.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.348 DataTime=0.248 Loss=1.682 Prec@1=59.813 Prec@5=82.365 rate=3.08 Hz, eta=0:12:28, total=0:01:05, wall=03:36 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.348 DataTime=0.248 Loss=1.682 Prec@1=59.813 Prec@5=82.365 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=03:36 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.348 DataTime=0.248 Loss=1.682 Prec@1=59.813 Prec@5=82.365 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=03:36 IST=> training   12.03% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.243 Loss=1.686 Prec@1=59.790 Prec@5=82.348 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=03:36 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.243 Loss=1.686 Prec@1=59.790 Prec@5=82.348 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=03:36 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.243 Loss=1.686 Prec@1=59.790 Prec@5=82.348 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=03:37 IST=> training   16.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.346 DataTime=0.244 Loss=1.685 Prec@1=59.809 Prec@5=82.380 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=03:37 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.346 DataTime=0.244 Loss=1.685 Prec@1=59.809 Prec@5=82.380 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=03:37 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.346 DataTime=0.244 Loss=1.685 Prec@1=59.809 Prec@5=82.380 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=03:37 IST=> training   20.02% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.241 Loss=1.687 Prec@1=59.804 Prec@5=82.348 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=03:37 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.241 Loss=1.687 Prec@1=59.804 Prec@5=82.348 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=03:37 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.241 Loss=1.687 Prec@1=59.804 Prec@5=82.348 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=03:38 IST=> training   24.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.750 Prec@5=82.310 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=03:38 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.750 Prec@5=82.310 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=03:38 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.750 Prec@5=82.310 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=03:38 IST=> training   28.01% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.685 Prec@5=82.315 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=03:38 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.685 Prec@5=82.315 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=03:38 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.685 Prec@5=82.315 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=03:39 IST=> training   32.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.237 Loss=1.695 Prec@1=59.644 Prec@5=82.299 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=03:39 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.237 Loss=1.695 Prec@1=59.644 Prec@5=82.299 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=03:39 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.237 Loss=1.695 Prec@1=59.644 Prec@5=82.299 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=03:40 IST=> training   36.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.236 Loss=1.695 Prec@1=59.650 Prec@5=82.297 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=03:40 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.236 Loss=1.695 Prec@1=59.650 Prec@5=82.297 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:40 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.236 Loss=1.695 Prec@1=59.650 Prec@5=82.297 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:40 IST=> training   39.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.236 Loss=1.697 Prec@1=59.608 Prec@5=82.269 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=03:40 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.236 Loss=1.697 Prec@1=59.608 Prec@5=82.269 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=03:40 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.236 Loss=1.697 Prec@1=59.608 Prec@5=82.269 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=03:41 IST=> training   43.99% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.698 Prec@1=59.579 Prec@5=82.258 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=03:41 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.698 Prec@1=59.579 Prec@5=82.258 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=03:41 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.698 Prec@1=59.579 Prec@5=82.258 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=03:41 IST=> training   47.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.698 Prec@1=59.587 Prec@5=82.266 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=03:41 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.698 Prec@1=59.587 Prec@5=82.266 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:41 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.698 Prec@1=59.587 Prec@5=82.266 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:42 IST=> training   51.98% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.699 Prec@1=59.559 Prec@5=82.240 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=03:42 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.699 Prec@1=59.559 Prec@5=82.240 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:42 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.235 Loss=1.699 Prec@1=59.559 Prec@5=82.240 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:42 IST=> training   55.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.699 Prec@1=59.569 Prec@5=82.232 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=03:42 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.699 Prec@1=59.569 Prec@5=82.232 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=03:42 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.234 Loss=1.699 Prec@1=59.569 Prec@5=82.232 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=03:43 IST=> training   59.97% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.700 Prec@1=59.544 Prec@5=82.217 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=03:43 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.700 Prec@1=59.544 Prec@5=82.217 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:43 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.700 Prec@1=59.544 Prec@5=82.217 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:44 IST=> training   63.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.234 Loss=1.701 Prec@1=59.523 Prec@5=82.206 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=03:44 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.234 Loss=1.701 Prec@1=59.523 Prec@5=82.206 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=03:44 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.342 DataTime=0.234 Loss=1.701 Prec@1=59.523 Prec@5=82.206 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=03:44 IST=> training   67.96% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.509 Prec@5=82.197 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=03:44 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.509 Prec@5=82.197 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:44 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.509 Prec@5=82.197 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:45 IST=> training   71.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.496 Prec@5=82.181 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=03:45 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.496 Prec@5=82.181 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:45 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.702 Prec@1=59.496 Prec@5=82.181 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:45 IST=> training   75.95% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.703 Prec@1=59.497 Prec@5=82.175 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=03:45 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.703 Prec@1=59.497 Prec@5=82.175 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=03:45 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.703 Prec@1=59.497 Prec@5=82.175 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=03:46 IST=> training   79.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.480 Prec@5=82.166 rate=2.95 Hz, eta=0:02:50, total=0:11:17, wall=03:46 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.480 Prec@5=82.166 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=03:46 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.480 Prec@5=82.166 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=03:46 IST=> training   83.94% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.703 Prec@1=59.466 Prec@5=82.168 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=03:46 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.703 Prec@1=59.466 Prec@5=82.168 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=03:46 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.703 Prec@1=59.466 Prec@5=82.168 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=03:47 IST=> training   87.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.458 Prec@5=82.165 rate=2.95 Hz, eta=0:01:42, total=0:12:24, wall=03:47 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.458 Prec@5=82.165 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:47 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.233 Loss=1.704 Prec@1=59.458 Prec@5=82.165 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:48 IST=> training   91.93% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.705 Prec@1=59.442 Prec@5=82.146 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=03:48 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.705 Prec@1=59.442 Prec@5=82.146 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:48 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.341 DataTime=0.232 Loss=1.705 Prec@1=59.442 Prec@5=82.146 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:48 IST=> training   95.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.340 DataTime=0.231 Loss=1.706 Prec@1=59.430 Prec@5=82.129 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=03:48 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.340 DataTime=0.231 Loss=1.706 Prec@1=59.430 Prec@5=82.129 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=03:48 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.340 DataTime=0.231 Loss=1.706 Prec@1=59.430 Prec@5=82.129 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=03:48 IST=> training   99.92% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.340 DataTime=0.231 Loss=1.706 Prec@1=59.428 Prec@5=82.128 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=03:48 IST=> training   100.00% of 1x2503...Epoch=116/150 LR=0.01284 Time=0.340 DataTime=0.231 Loss=1.706 Prec@1=59.428 Prec@5=82.128 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=03:48 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:48 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:48 IST=> validation 0.00% of 1x98...Epoch=116/150 LR=0.01284 Time=7.056 Loss=1.602 Prec@1=61.133 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=03:48 IST=> validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=7.056 Loss=1.602 Prec@1=61.133 Prec@5=84.766 rate=3821.59 Hz, eta=0:00:00, total=0:00:00, wall=03:48 IST** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=7.056 Loss=1.602 Prec@1=61.133 Prec@5=84.766 rate=3821.59 Hz, eta=0:00:00, total=0:00:00, wall=03:49 IST** validation 1.02% of 1x98...Epoch=116/150 LR=0.01284 Time=0.398 Loss=1.761 Prec@1=58.424 Prec@5=81.602 rate=3821.59 Hz, eta=0:00:00, total=0:00:00, wall=03:49 IST** validation 100.00% of 1x98...Epoch=116/150 LR=0.01284 Time=0.398 Loss=1.761 Prec@1=58.424 Prec@5=81.602 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=03:49 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:49 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:49 IST=> training   0.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.603 DataTime=5.453 Loss=1.787 Prec@1=58.008 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=03:49 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.603 DataTime=5.453 Loss=1.787 Prec@1=58.008 Prec@5=80.469 rate=6998.39 Hz, eta=0:00:00, total=0:00:00, wall=03:49 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=5.603 DataTime=5.453 Loss=1.787 Prec@1=58.008 Prec@5=80.469 rate=6998.39 Hz, eta=0:00:00, total=0:00:00, wall=03:49 IST=> training   0.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.373 DataTime=0.272 Loss=1.688 Prec@1=60.154 Prec@5=82.493 rate=6998.39 Hz, eta=0:00:00, total=0:00:00, wall=03:49 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.373 DataTime=0.272 Loss=1.688 Prec@1=60.154 Prec@5=82.493 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:49 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.373 DataTime=0.272 Loss=1.688 Prec@1=60.154 Prec@5=82.493 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:50 IST=> training   4.04% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.358 DataTime=0.258 Loss=1.686 Prec@1=59.866 Prec@5=82.537 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=03:50 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.358 DataTime=0.258 Loss=1.686 Prec@1=59.866 Prec@5=82.537 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=03:50 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.358 DataTime=0.258 Loss=1.686 Prec@1=59.866 Prec@5=82.537 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=03:50 IST=> training   8.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.349 DataTime=0.245 Loss=1.690 Prec@1=59.816 Prec@5=82.419 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=03:50 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.349 DataTime=0.245 Loss=1.690 Prec@1=59.816 Prec@5=82.419 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=03:50 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.349 DataTime=0.245 Loss=1.690 Prec@1=59.816 Prec@5=82.419 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=03:51 IST=> training   12.03% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.347 DataTime=0.243 Loss=1.687 Prec@1=59.873 Prec@5=82.439 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=03:51 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.347 DataTime=0.243 Loss=1.687 Prec@1=59.873 Prec@5=82.439 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=03:51 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.347 DataTime=0.243 Loss=1.687 Prec@1=59.873 Prec@5=82.439 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=03:52 IST=> training   16.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.345 DataTime=0.241 Loss=1.690 Prec@1=59.798 Prec@5=82.392 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=03:52 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.345 DataTime=0.241 Loss=1.690 Prec@1=59.798 Prec@5=82.392 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=03:52 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.345 DataTime=0.241 Loss=1.690 Prec@1=59.798 Prec@5=82.392 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=03:52 IST=> training   20.02% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.240 Loss=1.691 Prec@1=59.808 Prec@5=82.362 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=03:52 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.240 Loss=1.691 Prec@1=59.808 Prec@5=82.362 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:52 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.240 Loss=1.691 Prec@1=59.808 Prec@5=82.362 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:53 IST=> training   24.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.786 Prec@5=82.369 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=03:53 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.786 Prec@5=82.369 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=03:53 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.238 Loss=1.691 Prec@1=59.786 Prec@5=82.369 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=03:53 IST=> training   28.01% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.747 Prec@5=82.357 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=03:53 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.747 Prec@5=82.357 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:53 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.343 DataTime=0.239 Loss=1.693 Prec@1=59.747 Prec@5=82.357 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:54 IST=> training   32.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.342 DataTime=0.238 Loss=1.691 Prec@1=59.740 Prec@5=82.383 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=03:54 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.342 DataTime=0.238 Loss=1.691 Prec@1=59.740 Prec@5=82.383 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:54 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.342 DataTime=0.238 Loss=1.691 Prec@1=59.740 Prec@5=82.383 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:54 IST=> training   36.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.758 Prec@5=82.402 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=03:54 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.758 Prec@5=82.402 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=03:54 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.758 Prec@5=82.402 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=03:55 IST=> training   39.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.689 Prec@1=59.779 Prec@5=82.398 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=03:55 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.689 Prec@1=59.779 Prec@5=82.398 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=03:55 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.689 Prec@1=59.779 Prec@5=82.398 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=03:56 IST=> training   43.99% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.755 Prec@5=82.390 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=03:56 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.755 Prec@5=82.390 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:56 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.237 Loss=1.690 Prec@1=59.755 Prec@5=82.390 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:56 IST=> training   47.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.236 Loss=1.692 Prec@1=59.716 Prec@5=82.383 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=03:56 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.236 Loss=1.692 Prec@1=59.716 Prec@5=82.383 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:56 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.341 DataTime=0.236 Loss=1.692 Prec@1=59.716 Prec@5=82.383 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:57 IST=> training   51.98% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.692 Prec@1=59.707 Prec@5=82.377 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=03:57 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.692 Prec@1=59.707 Prec@5=82.377 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=03:57 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.692 Prec@1=59.707 Prec@5=82.377 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=03:57 IST=> training   55.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.694 Prec@1=59.689 Prec@5=82.362 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=03:57 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.694 Prec@1=59.689 Prec@5=82.362 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:57 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.694 Prec@1=59.689 Prec@5=82.362 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:58 IST=> training   59.97% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.694 Prec@1=59.673 Prec@5=82.342 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=03:58 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.694 Prec@1=59.673 Prec@5=82.342 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=03:58 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.694 Prec@1=59.673 Prec@5=82.342 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=03:58 IST=> training   63.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.695 Prec@1=59.659 Prec@5=82.321 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=03:58 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.695 Prec@1=59.659 Prec@5=82.321 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:58 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.340 DataTime=0.235 Loss=1.695 Prec@1=59.659 Prec@5=82.321 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:59 IST=> training   67.96% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.696 Prec@1=59.656 Prec@5=82.304 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=03:59 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.696 Prec@1=59.656 Prec@5=82.304 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=03:59 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.696 Prec@1=59.656 Prec@5=82.304 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=03:59 IST=> training   71.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.696 Prec@1=59.646 Prec@5=82.301 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=03:59 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.696 Prec@1=59.646 Prec@5=82.301 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=03:59 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.696 Prec@1=59.646 Prec@5=82.301 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=04:00 IST=> training   75.95% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.697 Prec@1=59.639 Prec@5=82.293 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=04:00 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.697 Prec@1=59.639 Prec@5=82.293 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=04:00 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.235 Loss=1.697 Prec@1=59.639 Prec@5=82.293 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=04:01 IST=> training   79.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.274 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=04:01 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.274 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=04:01 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.274 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=04:01 IST=> training   83.94% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.259 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=04:01 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.259 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:01 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.698 Prec@1=59.621 Prec@5=82.259 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:02 IST=> training   87.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.606 Prec@5=82.259 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:02 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.606 Prec@5=82.259 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:02 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.606 Prec@5=82.259 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:02 IST=> training   91.93% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.593 Prec@5=82.253 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:02 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.593 Prec@5=82.253 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=04:02 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.593 Prec@5=82.253 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=04:03 IST=> training   95.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.588 Prec@5=82.244 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.588 Prec@5=82.244 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.234 Loss=1.699 Prec@1=59.588 Prec@5=82.244 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=04:03 IST=> training   99.92% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.233 Loss=1.699 Prec@1=59.587 Prec@5=82.243 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=04:03 IST=> training   100.00% of 1x2503...Epoch=117/150 LR=0.01215 Time=0.339 DataTime=0.233 Loss=1.699 Prec@1=59.587 Prec@5=82.243 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=04:03 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 0.00% of 1x98...Epoch=117/150 LR=0.01215 Time=5.896 Loss=1.689 Prec@1=59.375 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=04:03 IST=> validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=5.896 Loss=1.689 Prec@1=59.375 Prec@5=82.617 rate=6552.14 Hz, eta=0:00:00, total=0:00:00, wall=04:03 IST** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=5.896 Loss=1.689 Prec@1=59.375 Prec@5=82.617 rate=6552.14 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST** validation 1.02% of 1x98...Epoch=117/150 LR=0.01215 Time=0.390 Loss=1.731 Prec@1=58.882 Prec@5=81.974 rate=6552.14 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST** validation 100.00% of 1x98...Epoch=117/150 LR=0.01215 Time=0.390 Loss=1.731 Prec@1=58.882 Prec@5=81.974 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=04:04 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.251 DataTime=4.152 Loss=1.704 Prec@1=58.594 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.251 DataTime=4.152 Loss=1.704 Prec@1=58.594 Prec@5=83.594 rate=6508.55 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=4.251 DataTime=4.152 Loss=1.704 Prec@1=58.594 Prec@5=83.594 rate=6508.55 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST=> training   0.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.382 DataTime=0.284 Loss=1.677 Prec@1=60.199 Prec@5=82.561 rate=6508.55 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.382 DataTime=0.284 Loss=1.677 Prec@1=60.199 Prec@5=82.561 rate=2.95 Hz, eta=0:13:35, total=0:00:34, wall=04:04 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.382 DataTime=0.284 Loss=1.677 Prec@1=60.199 Prec@5=82.561 rate=2.95 Hz, eta=0:13:35, total=0:00:34, wall=04:05 IST=> training   4.04% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.355 DataTime=0.255 Loss=1.680 Prec@1=60.101 Prec@5=82.522 rate=2.95 Hz, eta=0:13:35, total=0:00:34, wall=04:05 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.355 DataTime=0.255 Loss=1.680 Prec@1=60.101 Prec@5=82.522 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=04:05 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.355 DataTime=0.255 Loss=1.680 Prec@1=60.101 Prec@5=82.522 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=04:05 IST=> training   8.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.348 DataTime=0.247 Loss=1.680 Prec@1=60.138 Prec@5=82.474 rate=3.00 Hz, eta=0:12:47, total=0:01:07, wall=04:05 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.348 DataTime=0.247 Loss=1.680 Prec@1=60.138 Prec@5=82.474 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=04:05 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.348 DataTime=0.247 Loss=1.680 Prec@1=60.138 Prec@5=82.474 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=04:06 IST=> training   12.03% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.243 Loss=1.683 Prec@1=60.074 Prec@5=82.465 rate=2.99 Hz, eta=0:12:16, total=0:01:40, wall=04:06 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.243 Loss=1.683 Prec@1=60.074 Prec@5=82.465 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=04:06 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.243 Loss=1.683 Prec@1=60.074 Prec@5=82.465 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=04:06 IST=> training   16.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.242 Loss=1.685 Prec@1=60.012 Prec@5=82.407 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=04:06 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.242 Loss=1.685 Prec@1=60.012 Prec@5=82.407 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=04:06 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.344 DataTime=0.242 Loss=1.685 Prec@1=60.012 Prec@5=82.407 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=04:07 IST=> training   20.02% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.343 DataTime=0.241 Loss=1.683 Prec@1=60.024 Prec@5=82.421 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=04:07 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.343 DataTime=0.241 Loss=1.683 Prec@1=60.024 Prec@5=82.421 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:07 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.343 DataTime=0.241 Loss=1.683 Prec@1=60.024 Prec@5=82.421 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:08 IST=> training   24.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.239 Loss=1.681 Prec@1=60.034 Prec@5=82.486 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:08 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.239 Loss=1.681 Prec@1=60.034 Prec@5=82.486 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=04:08 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.239 Loss=1.681 Prec@1=60.034 Prec@5=82.486 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=04:08 IST=> training   28.01% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.235 Loss=1.683 Prec@1=60.021 Prec@5=82.455 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=04:08 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.235 Loss=1.683 Prec@1=60.021 Prec@5=82.455 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=04:08 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.235 Loss=1.683 Prec@1=60.021 Prec@5=82.455 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=04:09 IST=> training   32.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.237 Loss=1.683 Prec@1=60.006 Prec@5=82.469 rate=2.99 Hz, eta=0:09:29, total=0:04:27, wall=04:09 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.237 Loss=1.683 Prec@1=60.006 Prec@5=82.469 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:09 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.342 DataTime=0.237 Loss=1.683 Prec@1=60.006 Prec@5=82.469 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:09 IST=> training   36.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.235 Loss=1.682 Prec@1=60.007 Prec@5=82.483 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:09 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.235 Loss=1.682 Prec@1=60.007 Prec@5=82.483 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=04:09 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.235 Loss=1.682 Prec@1=60.007 Prec@5=82.483 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=04:10 IST=> training   39.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.682 Prec@1=59.996 Prec@5=82.495 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=04:10 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.682 Prec@1=59.996 Prec@5=82.495 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:10 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.682 Prec@1=59.996 Prec@5=82.495 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:10 IST=> training   43.99% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.682 Prec@1=59.973 Prec@5=82.472 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:10 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.682 Prec@1=59.973 Prec@5=82.472 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:10 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.682 Prec@1=59.973 Prec@5=82.472 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:11 IST=> training   47.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.684 Prec@1=59.953 Prec@5=82.448 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:11 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.684 Prec@1=59.953 Prec@5=82.448 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:11 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.234 Loss=1.684 Prec@1=59.953 Prec@5=82.448 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:11 IST=> training   51.98% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.685 Prec@1=59.936 Prec@5=82.415 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:11 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.685 Prec@1=59.936 Prec@5=82.415 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:11 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.685 Prec@1=59.936 Prec@5=82.415 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:12 IST=> training   55.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.686 Prec@1=59.904 Prec@5=82.398 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:12 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.686 Prec@1=59.904 Prec@5=82.398 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:12 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.686 Prec@1=59.904 Prec@5=82.398 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:13 IST=> training   59.97% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.688 Prec@1=59.870 Prec@5=82.361 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:13 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.688 Prec@1=59.870 Prec@5=82.361 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=04:13 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.341 DataTime=0.234 Loss=1.688 Prec@1=59.870 Prec@5=82.361 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=04:13 IST=> training   63.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.688 Prec@1=59.848 Prec@5=82.356 rate=2.96 Hz, eta=0:05:04, total=0:09:01, wall=04:13 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.688 Prec@1=59.848 Prec@5=82.356 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:13 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.688 Prec@1=59.848 Prec@5=82.356 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:14 IST=> training   67.96% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.689 Prec@1=59.825 Prec@5=82.333 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:14 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.689 Prec@1=59.825 Prec@5=82.333 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:14 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.689 Prec@1=59.825 Prec@5=82.333 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:14 IST=> training   71.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.690 Prec@1=59.807 Prec@5=82.331 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:14 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.690 Prec@1=59.807 Prec@5=82.331 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:14 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.690 Prec@1=59.807 Prec@5=82.331 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:15 IST=> training   75.95% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.691 Prec@1=59.795 Prec@5=82.311 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=04:15 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.691 Prec@1=59.795 Prec@5=82.311 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=04:15 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.691 Prec@1=59.795 Prec@5=82.311 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=04:15 IST=> training   79.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.692 Prec@1=59.786 Prec@5=82.305 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=04:15 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.692 Prec@1=59.786 Prec@5=82.305 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:15 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.340 DataTime=0.233 Loss=1.692 Prec@1=59.786 Prec@5=82.305 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:16 IST=> training   83.94% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.692 Prec@1=59.782 Prec@5=82.300 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:16 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.692 Prec@1=59.782 Prec@5=82.300 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=04:16 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.692 Prec@1=59.782 Prec@5=82.300 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=04:17 IST=> training   87.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.231 Loss=1.692 Prec@1=59.775 Prec@5=82.291 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=04:17 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.231 Loss=1.692 Prec@1=59.775 Prec@5=82.291 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:17 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.231 Loss=1.692 Prec@1=59.775 Prec@5=82.291 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:17 IST=> training   91.93% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.693 Prec@1=59.768 Prec@5=82.287 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=04:17 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.693 Prec@1=59.768 Prec@5=82.287 rate=2.96 Hz, eta=0:00:34, total=0:13:29, wall=04:17 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.339 DataTime=0.232 Loss=1.693 Prec@1=59.768 Prec@5=82.287 rate=2.96 Hz, eta=0:00:34, total=0:13:29, wall=04:18 IST=> training   95.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.338 DataTime=0.231 Loss=1.693 Prec@1=59.764 Prec@5=82.280 rate=2.96 Hz, eta=0:00:34, total=0:13:29, wall=04:18 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.338 DataTime=0.231 Loss=1.693 Prec@1=59.764 Prec@5=82.280 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:18 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.338 DataTime=0.231 Loss=1.693 Prec@1=59.764 Prec@5=82.280 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:18 IST=> training   99.92% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.338 DataTime=0.231 Loss=1.693 Prec@1=59.763 Prec@5=82.279 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:18 IST=> training   100.00% of 1x2503...Epoch=118/150 LR=0.01147 Time=0.338 DataTime=0.231 Loss=1.693 Prec@1=59.763 Prec@5=82.279 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=04:18 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> validation 0.00% of 1x98...Epoch=118/150 LR=0.01147 Time=6.463 Loss=1.770 Prec@1=58.594 Prec@5=80.078 rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.463 Loss=1.770 Prec@1=58.594 Prec@5=80.078 rate=4971.07 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=6.463 Loss=1.770 Prec@1=58.594 Prec@5=80.078 rate=4971.07 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST** validation 1.02% of 1x98...Epoch=118/150 LR=0.01147 Time=0.401 Loss=1.759 Prec@1=58.362 Prec@5=81.516 rate=4971.07 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST** validation 100.00% of 1x98...Epoch=118/150 LR=0.01147 Time=0.401 Loss=1.759 Prec@1=58.362 Prec@5=81.516 rate=2.99 Hz, eta=0:00:00, total=0:00:32, wall=04:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.594 DataTime=4.418 Loss=1.592 Prec@1=59.570 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=04:18 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.594 DataTime=4.418 Loss=1.592 Prec@1=59.570 Prec@5=84.766 rate=6964.90 Hz, eta=0:00:00, total=0:00:00, wall=04:18 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=4.594 DataTime=4.418 Loss=1.592 Prec@1=59.570 Prec@5=84.766 rate=6964.90 Hz, eta=0:00:00, total=0:00:00, wall=04:19 IST=> training   0.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.375 DataTime=0.278 Loss=1.677 Prec@1=60.011 Prec@5=82.608 rate=6964.90 Hz, eta=0:00:00, total=0:00:00, wall=04:19 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.375 DataTime=0.278 Loss=1.677 Prec@1=60.011 Prec@5=82.608 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=04:19 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.375 DataTime=0.278 Loss=1.677 Prec@1=60.011 Prec@5=82.608 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=04:20 IST=> training   4.04% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.355 DataTime=0.258 Loss=1.673 Prec@1=60.045 Prec@5=82.675 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=04:20 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.355 DataTime=0.258 Loss=1.673 Prec@1=60.045 Prec@5=82.675 rate=3.01 Hz, eta=0:12:44, total=0:01:06, wall=04:20 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.355 DataTime=0.258 Loss=1.673 Prec@1=60.045 Prec@5=82.675 rate=3.01 Hz, eta=0:12:44, total=0:01:06, wall=04:20 IST=> training   8.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.348 DataTime=0.250 Loss=1.673 Prec@1=60.095 Prec@5=82.659 rate=3.01 Hz, eta=0:12:44, total=0:01:06, wall=04:20 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.348 DataTime=0.250 Loss=1.673 Prec@1=60.095 Prec@5=82.659 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=04:20 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.348 DataTime=0.250 Loss=1.673 Prec@1=60.095 Prec@5=82.659 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=04:21 IST=> training   12.03% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.247 Loss=1.672 Prec@1=60.184 Prec@5=82.639 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=04:21 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.247 Loss=1.672 Prec@1=60.184 Prec@5=82.639 rate=2.99 Hz, eta=0:11:41, total=0:02:13, wall=04:21 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.247 Loss=1.672 Prec@1=60.184 Prec@5=82.639 rate=2.99 Hz, eta=0:11:41, total=0:02:13, wall=04:21 IST=> training   16.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.347 DataTime=0.248 Loss=1.674 Prec@1=60.152 Prec@5=82.585 rate=2.99 Hz, eta=0:11:41, total=0:02:13, wall=04:21 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.347 DataTime=0.248 Loss=1.674 Prec@1=60.152 Prec@5=82.585 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:21 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.347 DataTime=0.248 Loss=1.674 Prec@1=60.152 Prec@5=82.585 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:22 IST=> training   20.02% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.244 Loss=1.676 Prec@1=60.118 Prec@5=82.535 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:22 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.244 Loss=1.676 Prec@1=60.118 Prec@5=82.535 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=04:22 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.345 DataTime=0.244 Loss=1.676 Prec@1=60.118 Prec@5=82.535 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=04:22 IST=> training   24.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.343 DataTime=0.241 Loss=1.677 Prec@1=60.049 Prec@5=82.515 rate=2.97 Hz, eta=0:10:40, total=0:03:22, wall=04:22 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.343 DataTime=0.241 Loss=1.677 Prec@1=60.049 Prec@5=82.515 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=04:22 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.343 DataTime=0.241 Loss=1.677 Prec@1=60.049 Prec@5=82.515 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=04:23 IST=> training   28.01% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.344 DataTime=0.241 Loss=1.678 Prec@1=60.059 Prec@5=82.525 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=04:23 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.344 DataTime=0.241 Loss=1.678 Prec@1=60.059 Prec@5=82.525 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=04:23 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.344 DataTime=0.241 Loss=1.678 Prec@1=60.059 Prec@5=82.525 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=04:23 IST=> training   32.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.239 Loss=1.677 Prec@1=60.049 Prec@5=82.529 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=04:23 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.239 Loss=1.677 Prec@1=60.049 Prec@5=82.529 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=04:23 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.239 Loss=1.677 Prec@1=60.049 Prec@5=82.529 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=04:24 IST=> training   36.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.237 Loss=1.679 Prec@1=60.042 Prec@5=82.497 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=04:24 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.237 Loss=1.679 Prec@1=60.042 Prec@5=82.497 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:24 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.237 Loss=1.679 Prec@1=60.042 Prec@5=82.497 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:25 IST=> training   39.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.237 Loss=1.679 Prec@1=60.036 Prec@5=82.517 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=04:25 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.237 Loss=1.679 Prec@1=60.036 Prec@5=82.517 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:25 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.237 Loss=1.679 Prec@1=60.036 Prec@5=82.517 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:25 IST=> training   43.99% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.236 Loss=1.679 Prec@1=60.042 Prec@5=82.514 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=04:25 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.236 Loss=1.679 Prec@1=60.042 Prec@5=82.514 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:25 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.236 Loss=1.679 Prec@1=60.042 Prec@5=82.514 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:26 IST=> training   47.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.236 Loss=1.680 Prec@1=60.033 Prec@5=82.503 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=04:26 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.236 Loss=1.680 Prec@1=60.033 Prec@5=82.503 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=04:26 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.236 Loss=1.680 Prec@1=60.033 Prec@5=82.503 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=04:26 IST=> training   51.98% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.236 Loss=1.680 Prec@1=59.999 Prec@5=82.506 rate=2.98 Hz, eta=0:06:43, total=0:07:17, wall=04:26 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.236 Loss=1.680 Prec@1=59.999 Prec@5=82.506 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:26 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.341 DataTime=0.236 Loss=1.680 Prec@1=59.999 Prec@5=82.506 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:27 IST=> training   55.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.235 Loss=1.681 Prec@1=59.981 Prec@5=82.497 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:27 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.235 Loss=1.681 Prec@1=59.981 Prec@5=82.497 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=04:27 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.340 DataTime=0.235 Loss=1.681 Prec@1=59.981 Prec@5=82.497 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=04:27 IST=> training   59.97% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.976 Prec@5=82.496 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=04:27 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.976 Prec@5=82.496 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=04:27 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.976 Prec@5=82.496 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=04:28 IST=> training   63.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.235 Loss=1.682 Prec@1=59.956 Prec@5=82.494 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=04:28 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.235 Loss=1.682 Prec@1=59.956 Prec@5=82.494 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=04:28 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.235 Loss=1.682 Prec@1=59.956 Prec@5=82.494 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=04:29 IST=> training   67.96% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.971 Prec@5=82.498 rate=2.97 Hz, eta=0:04:30, total=0:09:32, wall=04:29 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.971 Prec@5=82.498 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=04:29 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.682 Prec@1=59.971 Prec@5=82.498 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=04:29 IST=> training   71.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.682 Prec@1=59.953 Prec@5=82.483 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=04:29 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.682 Prec@1=59.953 Prec@5=82.483 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=04:29 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.682 Prec@1=59.953 Prec@5=82.483 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=04:30 IST=> training   75.95% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.683 Prec@1=59.959 Prec@5=82.486 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=04:30 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.683 Prec@1=59.959 Prec@5=82.486 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=04:30 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.683 Prec@1=59.959 Prec@5=82.486 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=04:30 IST=> training   79.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.683 Prec@1=59.948 Prec@5=82.488 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=04:30 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.683 Prec@1=59.948 Prec@5=82.488 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=04:30 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.683 Prec@1=59.948 Prec@5=82.488 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=04:31 IST=> training   83.94% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.684 Prec@1=59.938 Prec@5=82.487 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=04:31 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.684 Prec@1=59.938 Prec@5=82.487 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:31 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.234 Loss=1.684 Prec@1=59.938 Prec@5=82.487 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:31 IST=> training   87.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.233 Loss=1.684 Prec@1=59.927 Prec@5=82.476 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=04:31 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.233 Loss=1.684 Prec@1=59.927 Prec@5=82.476 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:31 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.339 DataTime=0.233 Loss=1.684 Prec@1=59.927 Prec@5=82.476 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:32 IST=> training   91.93% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.684 Prec@1=59.923 Prec@5=82.475 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=04:32 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.684 Prec@1=59.923 Prec@5=82.475 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:32 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.338 DataTime=0.233 Loss=1.684 Prec@1=59.923 Prec@5=82.475 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:32 IST=> training   95.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.337 DataTime=0.232 Loss=1.685 Prec@1=59.906 Prec@5=82.460 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=04:32 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.337 DataTime=0.232 Loss=1.685 Prec@1=59.906 Prec@5=82.460 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=04:32 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.337 DataTime=0.232 Loss=1.685 Prec@1=59.906 Prec@5=82.460 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=04:32 IST=> training   99.92% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.337 DataTime=0.232 Loss=1.685 Prec@1=59.905 Prec@5=82.459 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=04:32 IST=> training   100.00% of 1x2503...Epoch=119/150 LR=0.01082 Time=0.337 DataTime=0.232 Loss=1.685 Prec@1=59.905 Prec@5=82.459 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=04:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> validation 0.00% of 1x98...Epoch=119/150 LR=0.01082 Time=7.229 Loss=1.915 Prec@1=55.664 Prec@5=79.883 rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=7.229 Loss=1.915 Prec@1=55.664 Prec@5=79.883 rate=5629.30 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=7.229 Loss=1.915 Prec@1=55.664 Prec@5=79.883 rate=5629.30 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST** validation 1.02% of 1x98...Epoch=119/150 LR=0.01082 Time=0.403 Loss=1.797 Prec@1=57.434 Prec@5=81.124 rate=5629.30 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST** validation 100.00% of 1x98...Epoch=119/150 LR=0.01082 Time=0.403 Loss=1.797 Prec@1=57.434 Prec@5=81.124 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=04:33 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> training   0.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=4.939 DataTime=4.736 Loss=1.625 Prec@1=61.133 Prec@5=82.031 rate=0 Hz, eta=?, total=0:00:00, wall=04:33 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=4.939 DataTime=4.736 Loss=1.625 Prec@1=61.133 Prec@5=82.031 rate=2873.64 Hz, eta=0:00:00, total=0:00:00, wall=04:33 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=4.939 DataTime=4.736 Loss=1.625 Prec@1=61.133 Prec@5=82.031 rate=2873.64 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST=> training   0.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.383 DataTime=0.282 Loss=1.677 Prec@1=60.158 Prec@5=82.435 rate=2873.64 Hz, eta=0:00:00, total=0:00:00, wall=04:34 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.383 DataTime=0.282 Loss=1.677 Prec@1=60.158 Prec@5=82.435 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=04:34 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.383 DataTime=0.282 Loss=1.677 Prec@1=60.158 Prec@5=82.435 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=04:34 IST=> training   4.04% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.359 DataTime=0.256 Loss=1.671 Prec@1=60.254 Prec@5=82.533 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=04:34 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.359 DataTime=0.256 Loss=1.671 Prec@1=60.254 Prec@5=82.533 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=04:34 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.359 DataTime=0.256 Loss=1.671 Prec@1=60.254 Prec@5=82.533 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=04:35 IST=> training   8.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.353 DataTime=0.249 Loss=1.672 Prec@1=60.173 Prec@5=82.519 rate=2.99 Hz, eta=0:12:49, total=0:01:07, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.353 DataTime=0.249 Loss=1.672 Prec@1=60.173 Prec@5=82.519 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.353 DataTime=0.249 Loss=1.672 Prec@1=60.173 Prec@5=82.519 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=04:35 IST=> training   12.03% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.352 DataTime=0.248 Loss=1.668 Prec@1=60.223 Prec@5=82.618 rate=2.97 Hz, eta=0:12:20, total=0:01:41, wall=04:35 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.352 DataTime=0.248 Loss=1.668 Prec@1=60.223 Prec@5=82.618 rate=2.95 Hz, eta=0:11:53, total=0:02:16, wall=04:35 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.352 DataTime=0.248 Loss=1.668 Prec@1=60.223 Prec@5=82.618 rate=2.95 Hz, eta=0:11:53, total=0:02:16, wall=04:36 IST=> training   16.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.348 DataTime=0.244 Loss=1.667 Prec@1=60.275 Prec@5=82.635 rate=2.95 Hz, eta=0:11:53, total=0:02:16, wall=04:36 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.348 DataTime=0.244 Loss=1.667 Prec@1=60.275 Prec@5=82.635 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:36 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.348 DataTime=0.244 Loss=1.667 Prec@1=60.275 Prec@5=82.635 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:37 IST=> training   20.02% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.345 DataTime=0.241 Loss=1.669 Prec@1=60.227 Prec@5=82.615 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=04:37 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.345 DataTime=0.241 Loss=1.669 Prec@1=60.227 Prec@5=82.615 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:37 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.345 DataTime=0.241 Loss=1.669 Prec@1=60.227 Prec@5=82.615 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:37 IST=> training   24.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.346 DataTime=0.242 Loss=1.671 Prec@1=60.218 Prec@5=82.564 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=04:37 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.346 DataTime=0.242 Loss=1.671 Prec@1=60.218 Prec@5=82.564 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=04:37 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.346 DataTime=0.242 Loss=1.671 Prec@1=60.218 Prec@5=82.564 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=04:38 IST=> training   28.01% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.240 Loss=1.673 Prec@1=60.190 Prec@5=82.560 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=04:38 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.240 Loss=1.673 Prec@1=60.190 Prec@5=82.560 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=04:38 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.240 Loss=1.673 Prec@1=60.190 Prec@5=82.560 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=04:38 IST=> training   32.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.239 Loss=1.674 Prec@1=60.185 Prec@5=82.546 rate=2.96 Hz, eta=0:09:35, total=0:04:30, wall=04:38 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.239 Loss=1.674 Prec@1=60.185 Prec@5=82.546 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:38 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.239 Loss=1.674 Prec@1=60.185 Prec@5=82.546 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:39 IST=> training   36.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.239 Loss=1.674 Prec@1=60.180 Prec@5=82.541 rate=2.96 Hz, eta=0:09:00, total=0:05:04, wall=04:39 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.239 Loss=1.674 Prec@1=60.180 Prec@5=82.541 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=04:39 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.344 DataTime=0.239 Loss=1.674 Prec@1=60.180 Prec@5=82.541 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=04:39 IST=> training   39.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.238 Loss=1.674 Prec@1=60.166 Prec@5=82.543 rate=2.95 Hz, eta=0:08:28, total=0:05:39, wall=04:39 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.238 Loss=1.674 Prec@1=60.166 Prec@5=82.543 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=04:39 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.343 DataTime=0.238 Loss=1.674 Prec@1=60.166 Prec@5=82.543 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=04:40 IST=> training   43.99% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.196 Prec@5=82.540 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=04:40 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.196 Prec@5=82.540 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=04:40 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.196 Prec@5=82.540 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=04:41 IST=> training   47.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.168 Prec@5=82.529 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=04:41 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.168 Prec@5=82.529 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=04:41 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.237 Loss=1.675 Prec@1=60.168 Prec@5=82.529 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=04:41 IST=> training   51.98% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.159 Prec@5=82.528 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=04:41 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.159 Prec@5=82.528 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:41 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.159 Prec@5=82.528 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:42 IST=> training   55.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.158 Prec@5=82.536 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=04:42 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.158 Prec@5=82.536 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:42 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.158 Prec@5=82.536 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:42 IST=> training   59.97% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.161 Prec@5=82.549 rate=2.96 Hz, eta=0:05:38, total=0:08:27, wall=04:42 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.161 Prec@5=82.549 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=04:42 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.342 DataTime=0.236 Loss=1.675 Prec@1=60.161 Prec@5=82.549 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=04:43 IST=> training   63.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.168 Prec@5=82.548 rate=2.95 Hz, eta=0:05:05, total=0:09:01, wall=04:43 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.168 Prec@5=82.548 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:43 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.236 Loss=1.675 Prec@1=60.168 Prec@5=82.548 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:43 IST=> training   67.96% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.675 Prec@1=60.160 Prec@5=82.540 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=04:43 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.675 Prec@1=60.160 Prec@5=82.540 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:43 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.675 Prec@1=60.160 Prec@5=82.540 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:44 IST=> training   71.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.114 Prec@5=82.516 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=04:44 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.114 Prec@5=82.516 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=04:44 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.114 Prec@5=82.516 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=04:44 IST=> training   75.95% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.089 Prec@5=82.515 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=04:44 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.089 Prec@5=82.515 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=04:44 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.677 Prec@1=60.089 Prec@5=82.515 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=04:45 IST=> training   79.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.084 Prec@5=82.507 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=04:45 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.084 Prec@5=82.507 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:45 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.084 Prec@5=82.507 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:46 IST=> training   83.94% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.076 Prec@5=82.508 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=04:46 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.076 Prec@5=82.508 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=04:46 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.678 Prec@1=60.076 Prec@5=82.508 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=04:46 IST=> training   87.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.679 Prec@1=60.058 Prec@5=82.496 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=04:46 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.679 Prec@1=60.058 Prec@5=82.496 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=04:46 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.679 Prec@1=60.058 Prec@5=82.496 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=04:47 IST=> training   91.93% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.680 Prec@1=60.042 Prec@5=82.495 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=04:47 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.680 Prec@1=60.042 Prec@5=82.495 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=04:47 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.341 DataTime=0.235 Loss=1.680 Prec@1=60.042 Prec@5=82.495 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=04:47 IST=> training   95.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.340 DataTime=0.234 Loss=1.680 Prec@1=60.034 Prec@5=82.492 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=04:47 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.340 DataTime=0.234 Loss=1.680 Prec@1=60.034 Prec@5=82.492 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=04:47 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.340 DataTime=0.234 Loss=1.680 Prec@1=60.034 Prec@5=82.492 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=04:47 IST=> training   99.92% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.340 DataTime=0.234 Loss=1.680 Prec@1=60.033 Prec@5=82.491 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=04:47 IST=> training   100.00% of 1x2503...Epoch=120/150 LR=0.01017 Time=0.340 DataTime=0.234 Loss=1.680 Prec@1=60.033 Prec@5=82.491 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=04:47 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:47 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:47 IST=> validation 0.00% of 1x98...Epoch=120/150 LR=0.01017 Time=6.821 Loss=1.903 Prec@1=54.492 Prec@5=80.469 rate=0 Hz, eta=?, total=0:00:00, wall=04:47 IST=> validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=6.821 Loss=1.903 Prec@1=54.492 Prec@5=80.469 rate=6543.09 Hz, eta=0:00:00, total=0:00:00, wall=04:47 IST** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=6.821 Loss=1.903 Prec@1=54.492 Prec@5=80.469 rate=6543.09 Hz, eta=0:00:00, total=0:00:00, wall=04:48 IST** validation 1.02% of 1x98...Epoch=120/150 LR=0.01017 Time=0.412 Loss=1.732 Prec@1=58.850 Prec@5=81.854 rate=6543.09 Hz, eta=0:00:00, total=0:00:00, wall=04:48 IST** validation 100.00% of 1x98...Epoch=120/150 LR=0.01017 Time=0.412 Loss=1.732 Prec@1=58.850 Prec@5=81.854 rate=2.92 Hz, eta=0:00:00, total=0:00:33, wall=04:48 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:48 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:48 IST=> training   0.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.614 DataTime=5.455 Loss=1.850 Prec@1=55.469 Prec@5=78.125 rate=0 Hz, eta=?, total=0:00:00, wall=04:48 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.614 DataTime=5.455 Loss=1.850 Prec@1=55.469 Prec@5=78.125 rate=6724.00 Hz, eta=0:00:00, total=0:00:00, wall=04:48 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=5.614 DataTime=5.455 Loss=1.850 Prec@1=55.469 Prec@5=78.125 rate=6724.00 Hz, eta=0:00:00, total=0:00:00, wall=04:49 IST=> training   0.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.378 DataTime=0.285 Loss=1.641 Prec@1=60.874 Prec@5=82.900 rate=6724.00 Hz, eta=0:00:00, total=0:00:00, wall=04:49 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.378 DataTime=0.285 Loss=1.641 Prec@1=60.874 Prec@5=82.900 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=04:49 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.378 DataTime=0.285 Loss=1.641 Prec@1=60.874 Prec@5=82.900 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=04:49 IST=> training   4.04% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.356 DataTime=0.256 Loss=1.655 Prec@1=60.591 Prec@5=82.753 rate=3.10 Hz, eta=0:12:53, total=0:00:32, wall=04:49 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.356 DataTime=0.256 Loss=1.655 Prec@1=60.591 Prec@5=82.753 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=04:49 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.356 DataTime=0.256 Loss=1.655 Prec@1=60.591 Prec@5=82.753 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=04:50 IST=> training   8.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.349 DataTime=0.248 Loss=1.657 Prec@1=60.453 Prec@5=82.766 rate=3.05 Hz, eta=0:12:35, total=0:01:05, wall=04:50 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.349 DataTime=0.248 Loss=1.657 Prec@1=60.453 Prec@5=82.766 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:50 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.349 DataTime=0.248 Loss=1.657 Prec@1=60.453 Prec@5=82.766 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:50 IST=> training   12.03% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.244 Loss=1.654 Prec@1=60.524 Prec@5=82.802 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=04:50 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.244 Loss=1.654 Prec@1=60.524 Prec@5=82.802 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=04:50 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.244 Loss=1.654 Prec@1=60.524 Prec@5=82.802 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=04:51 IST=> training   16.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.242 Loss=1.656 Prec@1=60.440 Prec@5=82.776 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=04:51 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.242 Loss=1.656 Prec@1=60.440 Prec@5=82.776 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=04:51 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.345 DataTime=0.242 Loss=1.656 Prec@1=60.440 Prec@5=82.776 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=04:51 IST=> training   20.02% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.343 DataTime=0.241 Loss=1.656 Prec@1=60.433 Prec@5=82.800 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=04:51 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.343 DataTime=0.241 Loss=1.656 Prec@1=60.433 Prec@5=82.800 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=04:51 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.343 DataTime=0.241 Loss=1.656 Prec@1=60.433 Prec@5=82.800 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=04:52 IST=> training   24.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.656 Prec@1=60.427 Prec@5=82.796 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=04:52 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.656 Prec@1=60.427 Prec@5=82.796 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=04:52 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.656 Prec@1=60.427 Prec@5=82.796 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=04:53 IST=> training   28.01% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.657 Prec@1=60.417 Prec@5=82.788 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=04:53 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.657 Prec@1=60.417 Prec@5=82.788 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=04:53 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.238 Loss=1.657 Prec@1=60.417 Prec@5=82.788 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=04:53 IST=> training   32.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.237 Loss=1.658 Prec@1=60.403 Prec@5=82.787 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=04:53 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.237 Loss=1.658 Prec@1=60.403 Prec@5=82.787 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:53 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.237 Loss=1.658 Prec@1=60.403 Prec@5=82.787 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:54 IST=> training   36.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.236 Loss=1.659 Prec@1=60.377 Prec@5=82.769 rate=2.99 Hz, eta=0:08:56, total=0:05:01, wall=04:54 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.236 Loss=1.659 Prec@1=60.377 Prec@5=82.769 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=04:54 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.236 Loss=1.659 Prec@1=60.377 Prec@5=82.769 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=04:54 IST=> training   39.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.660 Prec@1=60.370 Prec@5=82.762 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=04:54 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.660 Prec@1=60.370 Prec@5=82.762 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:54 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.660 Prec@1=60.370 Prec@5=82.762 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:55 IST=> training   43.99% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.661 Prec@1=60.351 Prec@5=82.747 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=04:55 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.661 Prec@1=60.351 Prec@5=82.747 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:55 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.661 Prec@1=60.351 Prec@5=82.747 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:55 IST=> training   47.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.663 Prec@1=60.313 Prec@5=82.717 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=04:55 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.663 Prec@1=60.313 Prec@5=82.717 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:55 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.236 Loss=1.663 Prec@1=60.313 Prec@5=82.717 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:56 IST=> training   51.98% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.236 Loss=1.663 Prec@1=60.314 Prec@5=82.720 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=04:56 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.236 Loss=1.663 Prec@1=60.314 Prec@5=82.720 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:56 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.342 DataTime=0.236 Loss=1.663 Prec@1=60.314 Prec@5=82.720 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:57 IST=> training   55.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.235 Loss=1.665 Prec@1=60.301 Prec@5=82.711 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=04:57 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.235 Loss=1.665 Prec@1=60.301 Prec@5=82.711 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:57 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.235 Loss=1.665 Prec@1=60.301 Prec@5=82.711 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:57 IST=> training   59.97% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.294 Prec@5=82.691 rate=2.97 Hz, eta=0:05:37, total=0:08:26, wall=04:57 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.294 Prec@5=82.691 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:57 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.294 Prec@5=82.691 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:58 IST=> training   63.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.310 Prec@5=82.687 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=04:58 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.310 Prec@5=82.687 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:58 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.666 Prec@1=60.310 Prec@5=82.687 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:58 IST=> training   67.96% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.233 Loss=1.667 Prec@1=60.285 Prec@5=82.677 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=04:58 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.233 Loss=1.667 Prec@1=60.285 Prec@5=82.677 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=04:58 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.233 Loss=1.667 Prec@1=60.285 Prec@5=82.677 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=04:59 IST=> training   71.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.667 Prec@1=60.279 Prec@5=82.673 rate=2.96 Hz, eta=0:03:56, total=0:10:07, wall=04:59 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.667 Prec@1=60.279 Prec@5=82.673 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=04:59 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.667 Prec@1=60.279 Prec@5=82.673 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=04:59 IST=> training   75.95% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.668 Prec@1=60.274 Prec@5=82.662 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=04:59 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.668 Prec@1=60.274 Prec@5=82.662 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=04:59 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.341 DataTime=0.234 Loss=1.668 Prec@1=60.274 Prec@5=82.662 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=05:00 IST=> training   79.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.669 Prec@1=60.249 Prec@5=82.641 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=05:00 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.669 Prec@1=60.249 Prec@5=82.641 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=05:00 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.669 Prec@1=60.249 Prec@5=82.641 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=05:00 IST=> training   83.94% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.669 Prec@1=60.255 Prec@5=82.643 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=05:00 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.669 Prec@1=60.255 Prec@5=82.643 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:00 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.669 Prec@1=60.255 Prec@5=82.643 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:01 IST=> training   87.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.670 Prec@1=60.232 Prec@5=82.629 rate=2.96 Hz, eta=0:01:41, total=0:12:22, wall=05:01 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.670 Prec@1=60.232 Prec@5=82.629 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:01 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.233 Loss=1.670 Prec@1=60.232 Prec@5=82.629 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:02 IST=> training   91.93% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.670 Prec@1=60.216 Prec@5=82.619 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:02 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.670 Prec@1=60.216 Prec@5=82.619 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:02 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.340 DataTime=0.232 Loss=1.670 Prec@1=60.216 Prec@5=82.619 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:02 IST=> training   95.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.339 DataTime=0.232 Loss=1.671 Prec@1=60.188 Prec@5=82.599 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:02 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.339 DataTime=0.232 Loss=1.671 Prec@1=60.188 Prec@5=82.599 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:02 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.339 DataTime=0.232 Loss=1.671 Prec@1=60.188 Prec@5=82.599 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:02 IST=> training   99.92% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.339 DataTime=0.232 Loss=1.671 Prec@1=60.188 Prec@5=82.600 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:02 IST=> training   100.00% of 1x2503...Epoch=121/150 LR=0.00955 Time=0.339 DataTime=0.232 Loss=1.671 Prec@1=60.188 Prec@5=82.600 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:02 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:02 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:02 IST=> validation 0.00% of 1x98...Epoch=121/150 LR=0.00955 Time=7.637 Loss=1.747 Prec@1=60.352 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=05:02 IST=> validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=7.637 Loss=1.747 Prec@1=60.352 Prec@5=81.055 rate=3419.34 Hz, eta=0:00:00, total=0:00:00, wall=05:02 IST** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=7.637 Loss=1.747 Prec@1=60.352 Prec@5=81.055 rate=3419.34 Hz, eta=0:00:00, total=0:00:00, wall=05:03 IST** validation 1.02% of 1x98...Epoch=121/150 LR=0.00955 Time=0.404 Loss=1.680 Prec@1=59.818 Prec@5=82.716 rate=3419.34 Hz, eta=0:00:00, total=0:00:00, wall=05:03 IST** validation 100.00% of 1x98...Epoch=121/150 LR=0.00955 Time=0.404 Loss=1.680 Prec@1=59.818 Prec@5=82.716 rate=3.06 Hz, eta=0:00:00, total=0:00:32, wall=05:03 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:03 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:03 IST=> training   0.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.918 DataTime=5.765 Loss=1.703 Prec@1=59.570 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=05:03 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.918 DataTime=5.765 Loss=1.703 Prec@1=59.570 Prec@5=82.812 rate=6901.60 Hz, eta=0:00:00, total=0:00:00, wall=05:03 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=5.918 DataTime=5.765 Loss=1.703 Prec@1=59.570 Prec@5=82.812 rate=6901.60 Hz, eta=0:00:00, total=0:00:00, wall=05:03 IST=> training   0.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.387 DataTime=0.284 Loss=1.658 Prec@1=60.383 Prec@5=82.983 rate=6901.60 Hz, eta=0:00:00, total=0:00:00, wall=05:03 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.387 DataTime=0.284 Loss=1.658 Prec@1=60.383 Prec@5=82.983 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=05:03 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.387 DataTime=0.284 Loss=1.658 Prec@1=60.383 Prec@5=82.983 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=05:04 IST=> training   4.04% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.357 DataTime=0.252 Loss=1.650 Prec@1=60.504 Prec@5=83.034 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=05:04 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.357 DataTime=0.252 Loss=1.650 Prec@1=60.504 Prec@5=83.034 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=05:04 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.357 DataTime=0.252 Loss=1.650 Prec@1=60.504 Prec@5=83.034 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=05:05 IST=> training   8.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.350 DataTime=0.246 Loss=1.649 Prec@1=60.533 Prec@5=83.036 rate=3.06 Hz, eta=0:12:33, total=0:01:05, wall=05:05 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.350 DataTime=0.246 Loss=1.649 Prec@1=60.533 Prec@5=83.036 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=05:05 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.350 DataTime=0.246 Loss=1.649 Prec@1=60.533 Prec@5=83.036 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=05:05 IST=> training   12.03% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.348 DataTime=0.242 Loss=1.654 Prec@1=60.466 Prec@5=82.914 rate=3.02 Hz, eta=0:12:08, total=0:01:39, wall=05:05 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.348 DataTime=0.242 Loss=1.654 Prec@1=60.466 Prec@5=82.914 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=05:05 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.348 DataTime=0.242 Loss=1.654 Prec@1=60.466 Prec@5=82.914 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=05:06 IST=> training   16.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.347 DataTime=0.241 Loss=1.654 Prec@1=60.467 Prec@5=82.931 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=05:06 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.347 DataTime=0.241 Loss=1.654 Prec@1=60.467 Prec@5=82.931 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:06 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.347 DataTime=0.241 Loss=1.654 Prec@1=60.467 Prec@5=82.931 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:06 IST=> training   20.02% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.344 DataTime=0.238 Loss=1.657 Prec@1=60.421 Prec@5=82.876 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:06 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.344 DataTime=0.238 Loss=1.657 Prec@1=60.421 Prec@5=82.876 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:06 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.344 DataTime=0.238 Loss=1.657 Prec@1=60.421 Prec@5=82.876 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:07 IST=> training   24.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.343 DataTime=0.237 Loss=1.656 Prec@1=60.432 Prec@5=82.900 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:07 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.343 DataTime=0.237 Loss=1.656 Prec@1=60.432 Prec@5=82.900 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=05:07 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.343 DataTime=0.237 Loss=1.656 Prec@1=60.432 Prec@5=82.900 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=05:07 IST=> training   28.01% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.410 Prec@5=82.873 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=05:07 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.410 Prec@5=82.873 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=05:07 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.410 Prec@5=82.873 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=05:08 IST=> training   32.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.432 Prec@5=82.878 rate=2.99 Hz, eta=0:09:28, total=0:04:27, wall=05:08 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.432 Prec@5=82.878 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:08 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.657 Prec@1=60.432 Prec@5=82.878 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:09 IST=> training   36.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.658 Prec@1=60.426 Prec@5=82.875 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=05:09 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.658 Prec@1=60.426 Prec@5=82.875 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=05:09 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.658 Prec@1=60.426 Prec@5=82.875 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=05:09 IST=> training   39.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.659 Prec@1=60.412 Prec@5=82.848 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=05:09 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.659 Prec@1=60.412 Prec@5=82.848 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=05:09 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.341 DataTime=0.235 Loss=1.659 Prec@1=60.412 Prec@5=82.848 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=05:10 IST=> training   43.99% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.823 rate=2.98 Hz, eta=0:07:49, total=0:06:09, wall=05:10 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.823 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=05:10 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.823 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=05:10 IST=> training   47.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.658 Prec@1=60.413 Prec@5=82.832 rate=2.99 Hz, eta=0:07:15, total=0:06:42, wall=05:10 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.658 Prec@1=60.413 Prec@5=82.832 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:10 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.658 Prec@1=60.413 Prec@5=82.832 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:11 IST=> training   51.98% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.397 Prec@5=82.820 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=05:11 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.397 Prec@5=82.820 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=05:11 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.659 Prec@1=60.397 Prec@5=82.820 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=05:11 IST=> training   55.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.396 Prec@5=82.802 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=05:11 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.396 Prec@5=82.802 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=05:11 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.396 Prec@5=82.802 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=05:12 IST=> training   59.97% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.660 Prec@1=60.397 Prec@5=82.791 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=05:12 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.660 Prec@1=60.397 Prec@5=82.791 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:12 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.234 Loss=1.660 Prec@1=60.397 Prec@5=82.791 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:12 IST=> training   63.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.798 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=05:12 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.798 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:12 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.659 Prec@1=60.400 Prec@5=82.798 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:13 IST=> training   67.96% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.371 Prec@5=82.782 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=05:13 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.371 Prec@5=82.782 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=05:13 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.371 Prec@5=82.782 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=05:14 IST=> training   71.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.661 Prec@1=60.365 Prec@5=82.780 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=05:14 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.661 Prec@1=60.365 Prec@5=82.780 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:14 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.340 DataTime=0.235 Loss=1.661 Prec@1=60.365 Prec@5=82.780 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:14 IST=> training   75.95% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.358 Prec@5=82.783 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=05:14 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.358 Prec@5=82.783 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:14 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.661 Prec@1=60.358 Prec@5=82.783 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:15 IST=> training   79.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.352 Prec@5=82.781 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=05:15 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.352 Prec@5=82.781 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:15 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.352 Prec@5=82.781 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:15 IST=> training   83.94% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.349 Prec@5=82.768 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=05:15 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.349 Prec@5=82.768 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:15 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.662 Prec@1=60.349 Prec@5=82.768 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:16 IST=> training   87.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.333 Prec@5=82.755 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=05:16 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.333 Prec@5=82.755 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:16 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.333 Prec@5=82.755 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:16 IST=> training   91.93% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.338 Prec@5=82.754 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=05:16 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.338 Prec@5=82.754 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=05:16 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.234 Loss=1.663 Prec@1=60.338 Prec@5=82.754 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=05:17 IST=> training   95.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.233 Loss=1.663 Prec@1=60.325 Prec@5=82.747 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=05:17 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.233 Loss=1.663 Prec@1=60.325 Prec@5=82.747 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:17 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.233 Loss=1.663 Prec@1=60.325 Prec@5=82.747 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:17 IST=> training   99.92% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.233 Loss=1.663 Prec@1=60.324 Prec@5=82.747 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:17 IST=> training   100.00% of 1x2503...Epoch=122/150 LR=0.00894 Time=0.339 DataTime=0.233 Loss=1.663 Prec@1=60.324 Prec@5=82.747 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=05:17 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST=> validation 0.00% of 1x98...Epoch=122/150 LR=0.00894 Time=7.156 Loss=1.705 Prec@1=59.180 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST=> validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=7.156 Loss=1.705 Prec@1=59.180 Prec@5=84.180 rate=6141.00 Hz, eta=0:00:00, total=0:00:00, wall=05:17 IST** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=7.156 Loss=1.705 Prec@1=59.180 Prec@5=84.180 rate=6141.00 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST** validation 1.02% of 1x98...Epoch=122/150 LR=0.00894 Time=0.402 Loss=1.724 Prec@1=59.082 Prec@5=81.948 rate=6141.00 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST** validation 100.00% of 1x98...Epoch=122/150 LR=0.00894 Time=0.402 Loss=1.724 Prec@1=59.082 Prec@5=81.948 rate=3.04 Hz, eta=0:00:00, total=0:00:32, wall=05:18 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST=> training   0.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.753 DataTime=5.588 Loss=1.561 Prec@1=60.352 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.753 DataTime=5.588 Loss=1.561 Prec@1=60.352 Prec@5=84.375 rate=6516.44 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=5.753 DataTime=5.588 Loss=1.561 Prec@1=60.352 Prec@5=84.375 rate=6516.44 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST=> training   0.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.385 DataTime=0.287 Loss=1.637 Prec@1=60.955 Prec@5=83.021 rate=6516.44 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.385 DataTime=0.287 Loss=1.637 Prec@1=60.955 Prec@5=83.021 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=05:18 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.385 DataTime=0.287 Loss=1.637 Prec@1=60.955 Prec@5=83.021 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=05:19 IST=> training   4.04% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.360 DataTime=0.256 Loss=1.638 Prec@1=60.812 Prec@5=83.081 rate=3.05 Hz, eta=0:13:08, total=0:00:33, wall=05:19 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.360 DataTime=0.256 Loss=1.638 Prec@1=60.812 Prec@5=83.081 rate=3.01 Hz, eta=0:12:43, total=0:01:06, wall=05:19 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.360 DataTime=0.256 Loss=1.638 Prec@1=60.812 Prec@5=83.081 rate=3.01 Hz, eta=0:12:43, total=0:01:06, wall=05:19 IST=> training   8.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.352 DataTime=0.247 Loss=1.639 Prec@1=60.864 Prec@5=83.064 rate=3.01 Hz, eta=0:12:43, total=0:01:06, wall=05:19 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.352 DataTime=0.247 Loss=1.639 Prec@1=60.864 Prec@5=83.064 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=05:19 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.352 DataTime=0.247 Loss=1.639 Prec@1=60.864 Prec@5=83.064 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=05:20 IST=> training   12.03% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.350 DataTime=0.245 Loss=1.641 Prec@1=60.832 Prec@5=83.022 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=05:20 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.350 DataTime=0.245 Loss=1.641 Prec@1=60.832 Prec@5=83.022 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=05:20 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.350 DataTime=0.245 Loss=1.641 Prec@1=60.832 Prec@5=83.022 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=05:21 IST=> training   16.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.347 DataTime=0.241 Loss=1.644 Prec@1=60.801 Prec@5=83.007 rate=2.98 Hz, eta=0:11:44, total=0:02:14, wall=05:21 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.347 DataTime=0.241 Loss=1.644 Prec@1=60.801 Prec@5=83.007 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:21 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.347 DataTime=0.241 Loss=1.644 Prec@1=60.801 Prec@5=83.007 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:21 IST=> training   20.02% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.754 Prec@5=82.986 rate=2.98 Hz, eta=0:11:11, total=0:02:47, wall=05:21 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.754 Prec@5=82.986 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:21 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.754 Prec@5=82.986 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:22 IST=> training   24.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.345 DataTime=0.239 Loss=1.647 Prec@1=60.727 Prec@5=83.001 rate=3.00 Hz, eta=0:10:34, total=0:03:20, wall=05:22 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.345 DataTime=0.239 Loss=1.647 Prec@1=60.727 Prec@5=83.001 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=05:22 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.345 DataTime=0.239 Loss=1.647 Prec@1=60.727 Prec@5=83.001 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=05:22 IST=> training   28.01% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.710 Prec@5=83.006 rate=2.97 Hz, eta=0:10:07, total=0:03:56, wall=05:22 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.710 Prec@5=83.006 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=05:22 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.343 DataTime=0.237 Loss=1.647 Prec@1=60.710 Prec@5=83.006 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=05:23 IST=> training   32.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.690 Prec@5=82.975 rate=2.98 Hz, eta=0:09:31, total=0:04:29, wall=05:23 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.690 Prec@5=82.975 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=05:23 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.690 Prec@5=82.975 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=05:23 IST=> training   36.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.674 Prec@5=82.957 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=05:23 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.674 Prec@5=82.957 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:23 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.674 Prec@5=82.957 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:24 IST=> training   39.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.669 Prec@5=82.955 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:24 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.669 Prec@5=82.955 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:24 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.669 Prec@5=82.955 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:24 IST=> training   43.99% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.640 Prec@5=82.940 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=05:24 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.640 Prec@5=82.940 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:24 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.640 Prec@5=82.940 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:25 IST=> training   47.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.650 Prec@1=60.632 Prec@5=82.937 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=05:25 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.650 Prec@1=60.632 Prec@5=82.937 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=05:25 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.650 Prec@1=60.632 Prec@5=82.937 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=05:26 IST=> training   51.98% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.633 Prec@5=82.948 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=05:26 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.633 Prec@5=82.948 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=05:26 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.650 Prec@1=60.633 Prec@5=82.948 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=05:26 IST=> training   55.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.652 Prec@1=60.588 Prec@5=82.915 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=05:26 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.652 Prec@1=60.588 Prec@5=82.915 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=05:26 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.652 Prec@1=60.588 Prec@5=82.915 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=05:27 IST=> training   59.97% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.653 Prec@1=60.582 Prec@5=82.909 rate=2.96 Hz, eta=0:05:37, total=0:08:26, wall=05:27 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.653 Prec@1=60.582 Prec@5=82.909 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=05:27 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.342 DataTime=0.235 Loss=1.653 Prec@1=60.582 Prec@5=82.909 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=05:27 IST=> training   63.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.653 Prec@1=60.584 Prec@5=82.911 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=05:27 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.653 Prec@1=60.584 Prec@5=82.911 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=05:27 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.653 Prec@1=60.584 Prec@5=82.911 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=05:28 IST=> training   67.96% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.233 Loss=1.654 Prec@1=60.562 Prec@5=82.898 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=05:28 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.233 Loss=1.654 Prec@1=60.562 Prec@5=82.898 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=05:28 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.233 Loss=1.654 Prec@1=60.562 Prec@5=82.898 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=05:28 IST=> training   71.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.655 Prec@1=60.549 Prec@5=82.882 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=05:28 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.655 Prec@1=60.549 Prec@5=82.882 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=05:28 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.341 DataTime=0.234 Loss=1.655 Prec@1=60.549 Prec@5=82.882 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=05:29 IST=> training   75.95% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.544 Prec@5=82.873 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=05:29 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.544 Prec@5=82.873 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=05:29 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.544 Prec@5=82.873 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=05:30 IST=> training   79.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.655 Prec@1=60.541 Prec@5=82.870 rate=2.96 Hz, eta=0:02:49, total=0:11:15, wall=05:30 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.655 Prec@1=60.541 Prec@5=82.870 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=05:30 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.655 Prec@1=60.541 Prec@5=82.870 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=05:30 IST=> training   83.94% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.522 Prec@5=82.863 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=05:30 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.522 Prec@5=82.863 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=05:30 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.655 Prec@1=60.522 Prec@5=82.863 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=05:31 IST=> training   87.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.522 Prec@5=82.871 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=05:31 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.522 Prec@5=82.871 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:31 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.522 Prec@5=82.871 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:31 IST=> training   91.93% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.517 Prec@5=82.869 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=05:31 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.517 Prec@5=82.869 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:31 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.233 Loss=1.656 Prec@1=60.517 Prec@5=82.869 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:32 IST=> training   95.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.657 Prec@1=60.508 Prec@5=82.856 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=05:32 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.657 Prec@1=60.508 Prec@5=82.856 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:32 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.340 DataTime=0.232 Loss=1.657 Prec@1=60.508 Prec@5=82.856 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:32 IST=> training   99.92% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.339 DataTime=0.232 Loss=1.657 Prec@1=60.507 Prec@5=82.856 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:32 IST=> training   100.00% of 1x2503...Epoch=123/150 LR=0.00835 Time=0.339 DataTime=0.232 Loss=1.657 Prec@1=60.507 Prec@5=82.856 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=05:32 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:32 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:32 IST=> validation 0.00% of 1x98...Epoch=123/150 LR=0.00835 Time=6.462 Loss=1.818 Prec@1=57.617 Prec@5=80.859 rate=0 Hz, eta=?, total=0:00:00, wall=05:32 IST=> validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=6.462 Loss=1.818 Prec@1=57.617 Prec@5=80.859 rate=6538.43 Hz, eta=0:00:00, total=0:00:00, wall=05:32 IST** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=6.462 Loss=1.818 Prec@1=57.617 Prec@5=80.859 rate=6538.43 Hz, eta=0:00:00, total=0:00:00, wall=05:32 IST** validation 1.02% of 1x98...Epoch=123/150 LR=0.00835 Time=0.402 Loss=1.716 Prec@1=59.452 Prec@5=82.154 rate=6538.43 Hz, eta=0:00:00, total=0:00:00, wall=05:32 IST** validation 100.00% of 1x98...Epoch=123/150 LR=0.00835 Time=0.402 Loss=1.716 Prec@1=59.452 Prec@5=82.154 rate=2.97 Hz, eta=0:00:00, total=0:00:32, wall=05:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST=> training   0.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=5.049 DataTime=4.870 Loss=1.652 Prec@1=61.523 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=5.049 DataTime=4.870 Loss=1.652 Prec@1=61.523 Prec@5=84.375 rate=2848.38 Hz, eta=0:00:00, total=0:00:00, wall=05:33 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=5.049 DataTime=4.870 Loss=1.652 Prec@1=61.523 Prec@5=84.375 rate=2848.38 Hz, eta=0:00:00, total=0:00:00, wall=05:33 IST=> training   0.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.385 DataTime=0.283 Loss=1.632 Prec@1=60.893 Prec@5=83.193 rate=2848.38 Hz, eta=0:00:00, total=0:00:00, wall=05:33 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.385 DataTime=0.283 Loss=1.632 Prec@1=60.893 Prec@5=83.193 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=05:33 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.385 DataTime=0.283 Loss=1.632 Prec@1=60.893 Prec@5=83.193 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=05:34 IST=> training   4.04% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.365 DataTime=0.262 Loss=1.635 Prec@1=60.892 Prec@5=83.149 rate=2.99 Hz, eta=0:13:23, total=0:00:33, wall=05:34 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.365 DataTime=0.262 Loss=1.635 Prec@1=60.892 Prec@5=83.149 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=05:34 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.365 DataTime=0.262 Loss=1.635 Prec@1=60.892 Prec@5=83.149 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=05:34 IST=> training   8.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.640 Prec@1=60.854 Prec@5=83.042 rate=2.95 Hz, eta=0:13:01, total=0:01:08, wall=05:34 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.640 Prec@1=60.854 Prec@5=83.042 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:34 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.640 Prec@1=60.854 Prec@5=83.042 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:35 IST=> training   12.03% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.645 Prec@1=60.781 Prec@5=82.952 rate=2.98 Hz, eta=0:12:19, total=0:01:41, wall=05:35 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.645 Prec@1=60.781 Prec@5=82.952 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=05:35 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.353 DataTime=0.248 Loss=1.645 Prec@1=60.781 Prec@5=82.952 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=05:35 IST=> training   16.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.242 Loss=1.650 Prec@1=60.702 Prec@5=82.875 rate=2.94 Hz, eta=0:11:55, total=0:02:16, wall=05:35 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.242 Loss=1.650 Prec@1=60.702 Prec@5=82.875 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=05:35 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.242 Loss=1.650 Prec@1=60.702 Prec@5=82.875 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=05:36 IST=> training   20.02% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.348 DataTime=0.243 Loss=1.648 Prec@1=60.745 Prec@5=82.934 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=05:36 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.348 DataTime=0.243 Loss=1.648 Prec@1=60.745 Prec@5=82.934 rate=2.94 Hz, eta=0:10:46, total=0:03:24, wall=05:36 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.348 DataTime=0.243 Loss=1.648 Prec@1=60.745 Prec@5=82.934 rate=2.94 Hz, eta=0:10:46, total=0:03:24, wall=05:37 IST=> training   24.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.241 Loss=1.648 Prec@1=60.765 Prec@5=82.952 rate=2.94 Hz, eta=0:10:46, total=0:03:24, wall=05:37 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.241 Loss=1.648 Prec@1=60.765 Prec@5=82.952 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=05:37 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.241 Loss=1.648 Prec@1=60.765 Prec@5=82.952 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=05:37 IST=> training   28.01% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.724 Prec@5=82.989 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=05:37 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.724 Prec@5=82.989 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=05:37 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.724 Prec@5=82.989 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=05:38 IST=> training   32.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.240 Loss=1.648 Prec@1=60.679 Prec@5=82.960 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=05:38 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.240 Loss=1.648 Prec@1=60.679 Prec@5=82.960 rate=2.93 Hz, eta=0:09:06, total=0:05:07, wall=05:38 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.347 DataTime=0.240 Loss=1.648 Prec@1=60.679 Prec@5=82.960 rate=2.93 Hz, eta=0:09:06, total=0:05:07, wall=05:38 IST=> training   36.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.346 DataTime=0.239 Loss=1.647 Prec@1=60.698 Prec@5=82.969 rate=2.93 Hz, eta=0:09:06, total=0:05:07, wall=05:38 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.346 DataTime=0.239 Loss=1.647 Prec@1=60.698 Prec@5=82.969 rate=2.94 Hz, eta=0:08:31, total=0:05:40, wall=05:38 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.346 DataTime=0.239 Loss=1.647 Prec@1=60.698 Prec@5=82.969 rate=2.94 Hz, eta=0:08:31, total=0:05:40, wall=05:39 IST=> training   39.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.688 Prec@5=82.993 rate=2.94 Hz, eta=0:08:31, total=0:05:40, wall=05:39 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.688 Prec@5=82.993 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=05:39 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.239 Loss=1.646 Prec@1=60.688 Prec@5=82.993 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=05:39 IST=> training   43.99% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.645 Prec@1=60.683 Prec@5=83.018 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=05:39 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.645 Prec@1=60.683 Prec@5=83.018 rate=2.94 Hz, eta=0:07:23, total=0:06:48, wall=05:39 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.645 Prec@1=60.683 Prec@5=83.018 rate=2.94 Hz, eta=0:07:23, total=0:06:48, wall=05:40 IST=> training   47.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.647 Prec@1=60.666 Prec@5=82.996 rate=2.94 Hz, eta=0:07:23, total=0:06:48, wall=05:40 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.647 Prec@1=60.666 Prec@5=82.996 rate=2.93 Hz, eta=0:06:49, total=0:07:23, wall=05:40 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.345 DataTime=0.238 Loss=1.647 Prec@1=60.666 Prec@5=82.996 rate=2.93 Hz, eta=0:06:49, total=0:07:23, wall=05:41 IST=> training   51.98% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.646 Prec@1=60.664 Prec@5=82.999 rate=2.93 Hz, eta=0:06:49, total=0:07:23, wall=05:41 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.646 Prec@1=60.664 Prec@5=82.999 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:41 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.646 Prec@1=60.664 Prec@5=82.999 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:41 IST=> training   55.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.647 Prec@1=60.660 Prec@5=82.994 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:41 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.647 Prec@1=60.660 Prec@5=82.994 rate=2.94 Hz, eta=0:05:41, total=0:08:30, wall=05:41 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.344 DataTime=0.237 Loss=1.647 Prec@1=60.660 Prec@5=82.994 rate=2.94 Hz, eta=0:05:41, total=0:08:30, wall=05:42 IST=> training   59.97% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.237 Loss=1.646 Prec@1=60.683 Prec@5=83.016 rate=2.94 Hz, eta=0:05:41, total=0:08:30, wall=05:42 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.237 Loss=1.646 Prec@1=60.683 Prec@5=83.016 rate=2.94 Hz, eta=0:05:06, total=0:09:04, wall=05:42 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.237 Loss=1.646 Prec@1=60.683 Prec@5=83.016 rate=2.94 Hz, eta=0:05:06, total=0:09:04, wall=05:42 IST=> training   63.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.651 Prec@5=82.994 rate=2.94 Hz, eta=0:05:06, total=0:09:04, wall=05:42 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.651 Prec@5=82.994 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=05:42 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.651 Prec@5=82.994 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=05:43 IST=> training   67.96% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.642 Prec@5=82.994 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.642 Prec@5=82.994 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.642 Prec@5=82.994 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=05:43 IST=> training   71.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.643 Prec@5=82.996 rate=2.94 Hz, eta=0:03:58, total=0:10:12, wall=05:43 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.643 Prec@5=82.996 rate=2.94 Hz, eta=0:03:25, total=0:10:47, wall=05:43 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.343 DataTime=0.236 Loss=1.647 Prec@1=60.643 Prec@5=82.996 rate=2.94 Hz, eta=0:03:25, total=0:10:47, wall=05:44 IST=> training   75.95% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.654 Prec@5=82.984 rate=2.94 Hz, eta=0:03:25, total=0:10:47, wall=05:44 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.654 Prec@5=82.984 rate=2.94 Hz, eta=0:02:50, total=0:11:19, wall=05:44 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.648 Prec@1=60.654 Prec@5=82.984 rate=2.94 Hz, eta=0:02:50, total=0:11:19, wall=05:44 IST=> training   79.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.642 Prec@5=82.993 rate=2.94 Hz, eta=0:02:50, total=0:11:19, wall=05:44 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.642 Prec@5=82.993 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=05:44 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.642 Prec@5=82.993 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=05:45 IST=> training   83.94% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.645 Prec@5=82.985 rate=2.94 Hz, eta=0:02:16, total=0:11:54, wall=05:45 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.645 Prec@5=82.985 rate=2.94 Hz, eta=0:01:42, total=0:12:28, wall=05:45 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.236 Loss=1.648 Prec@1=60.645 Prec@5=82.985 rate=2.94 Hz, eta=0:01:42, total=0:12:28, wall=05:46 IST=> training   87.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.629 Prec@5=82.981 rate=2.94 Hz, eta=0:01:42, total=0:12:28, wall=05:46 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.629 Prec@5=82.981 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=05:46 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.629 Prec@5=82.981 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=05:46 IST=> training   91.93% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.613 Prec@5=82.982 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=05:46 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.613 Prec@5=82.982 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=05:46 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.342 DataTime=0.235 Loss=1.649 Prec@1=60.613 Prec@5=82.982 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=05:47 IST=> training   95.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.341 DataTime=0.235 Loss=1.649 Prec@1=60.616 Prec@5=82.979 rate=2.94 Hz, eta=0:00:34, total=0:13:35, wall=05:47 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.341 DataTime=0.235 Loss=1.649 Prec@1=60.616 Prec@5=82.979 rate=2.95 Hz, eta=0:00:00, total=0:14:07, wall=05:47 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.341 DataTime=0.235 Loss=1.649 Prec@1=60.616 Prec@5=82.979 rate=2.95 Hz, eta=0:00:00, total=0:14:07, wall=05:47 IST=> training   99.92% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.341 DataTime=0.235 Loss=1.649 Prec@1=60.615 Prec@5=82.979 rate=2.95 Hz, eta=0:00:00, total=0:14:07, wall=05:47 IST=> training   100.00% of 1x2503...Epoch=124/150 LR=0.00778 Time=0.341 DataTime=0.235 Loss=1.649 Prec@1=60.615 Prec@5=82.979 rate=2.95 Hz, eta=0:00:00, total=0:14:08, wall=05:47 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> validation 0.00% of 1x98...Epoch=124/150 LR=0.00778 Time=5.924 Loss=1.821 Prec@1=56.445 Prec@5=79.688 rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=5.924 Loss=1.821 Prec@1=56.445 Prec@5=79.688 rate=4680.40 Hz, eta=0:00:00, total=0:00:00, wall=05:47 IST** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=5.924 Loss=1.821 Prec@1=56.445 Prec@5=79.688 rate=4680.40 Hz, eta=0:00:00, total=0:00:00, wall=05:47 IST** validation 1.02% of 1x98...Epoch=124/150 LR=0.00778 Time=0.393 Loss=1.699 Prec@1=59.558 Prec@5=82.384 rate=4680.40 Hz, eta=0:00:00, total=0:00:00, wall=05:47 IST** validation 100.00% of 1x98...Epoch=124/150 LR=0.00778 Time=0.393 Loss=1.699 Prec@1=59.558 Prec@5=82.384 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=05:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> training   0.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.018 DataTime=4.862 Loss=1.675 Prec@1=60.547 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=05:47 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.018 DataTime=4.862 Loss=1.675 Prec@1=60.547 Prec@5=81.445 rate=3653.13 Hz, eta=0:00:00, total=0:00:00, wall=05:47 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=5.018 DataTime=4.862 Loss=1.675 Prec@1=60.547 Prec@5=81.445 rate=3653.13 Hz, eta=0:00:00, total=0:00:00, wall=05:48 IST=> training   0.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.375 DataTime=0.268 Loss=1.643 Prec@1=60.595 Prec@5=82.992 rate=3653.13 Hz, eta=0:00:00, total=0:00:00, wall=05:48 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.375 DataTime=0.268 Loss=1.643 Prec@1=60.595 Prec@5=82.992 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=05:48 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.375 DataTime=0.268 Loss=1.643 Prec@1=60.595 Prec@5=82.992 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=05:49 IST=> training   4.04% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.355 DataTime=0.255 Loss=1.647 Prec@1=60.627 Prec@5=82.960 rate=3.08 Hz, eta=0:13:00, total=0:00:32, wall=05:49 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.355 DataTime=0.255 Loss=1.647 Prec@1=60.627 Prec@5=82.960 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=05:49 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.355 DataTime=0.255 Loss=1.647 Prec@1=60.627 Prec@5=82.960 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=05:49 IST=> training   8.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.348 DataTime=0.247 Loss=1.646 Prec@1=60.696 Prec@5=82.976 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=05:49 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.348 DataTime=0.247 Loss=1.646 Prec@1=60.696 Prec@5=82.976 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=05:49 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.348 DataTime=0.247 Loss=1.646 Prec@1=60.696 Prec@5=82.976 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=05:50 IST=> training   12.03% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.345 DataTime=0.244 Loss=1.647 Prec@1=60.686 Prec@5=82.968 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=05:50 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.345 DataTime=0.244 Loss=1.647 Prec@1=60.686 Prec@5=82.968 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=05:50 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.345 DataTime=0.244 Loss=1.647 Prec@1=60.686 Prec@5=82.968 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=05:50 IST=> training   16.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.346 DataTime=0.245 Loss=1.648 Prec@1=60.709 Prec@5=82.967 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=05:50 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.346 DataTime=0.245 Loss=1.648 Prec@1=60.709 Prec@5=82.967 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:50 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.346 DataTime=0.245 Loss=1.648 Prec@1=60.709 Prec@5=82.967 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:51 IST=> training   20.02% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.244 Loss=1.643 Prec@1=60.794 Prec@5=83.028 rate=2.98 Hz, eta=0:11:12, total=0:02:48, wall=05:51 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.244 Loss=1.643 Prec@1=60.794 Prec@5=83.028 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=05:51 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.244 Loss=1.643 Prec@1=60.794 Prec@5=83.028 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=05:51 IST=> training   24.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.839 Prec@5=83.045 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=05:51 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.839 Prec@5=83.045 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=05:51 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.839 Prec@5=83.045 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=05:52 IST=> training   28.01% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.641 Prec@1=60.875 Prec@5=83.055 rate=2.98 Hz, eta=0:10:04, total=0:03:54, wall=05:52 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.641 Prec@1=60.875 Prec@5=83.055 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=05:52 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.641 Prec@1=60.875 Prec@5=83.055 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=05:53 IST=> training   32.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.881 Prec@5=83.062 rate=2.97 Hz, eta=0:09:33, total=0:04:29, wall=05:53 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.881 Prec@5=83.062 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=05:53 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.242 Loss=1.641 Prec@1=60.881 Prec@5=83.062 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=05:53 IST=> training   36.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.640 Prec@1=60.894 Prec@5=83.052 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=05:53 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.640 Prec@1=60.894 Prec@5=83.052 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:53 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.640 Prec@1=60.894 Prec@5=83.052 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:54 IST=> training   39.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.639 Prec@1=60.906 Prec@5=83.073 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=05:54 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.639 Prec@1=60.906 Prec@5=83.073 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=05:54 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.243 Loss=1.639 Prec@1=60.906 Prec@5=83.073 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=05:54 IST=> training   43.99% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.242 Loss=1.640 Prec@1=60.902 Prec@5=83.045 rate=2.95 Hz, eta=0:07:54, total=0:06:12, wall=05:54 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.242 Loss=1.640 Prec@1=60.902 Prec@5=83.045 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=05:54 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.242 Loss=1.640 Prec@1=60.902 Prec@5=83.045 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=05:55 IST=> training   47.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.241 Loss=1.640 Prec@1=60.885 Prec@5=83.037 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=05:55 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.241 Loss=1.640 Prec@1=60.885 Prec@5=83.037 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=05:55 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.241 Loss=1.640 Prec@1=60.885 Prec@5=83.037 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=05:55 IST=> training   51.98% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.242 Loss=1.641 Prec@1=60.859 Prec@5=83.047 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=05:55 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.242 Loss=1.641 Prec@1=60.859 Prec@5=83.047 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:55 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.344 DataTime=0.242 Loss=1.641 Prec@1=60.859 Prec@5=83.047 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:56 IST=> training   55.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.844 Prec@5=83.049 rate=2.94 Hz, eta=0:06:14, total=0:07:56, wall=05:56 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.844 Prec@5=83.049 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=05:56 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.844 Prec@5=83.049 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=05:57 IST=> training   59.97% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.641 Prec@1=60.834 Prec@5=83.045 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=05:57 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.641 Prec@1=60.834 Prec@5=83.045 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=05:57 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.240 Loss=1.641 Prec@1=60.834 Prec@5=83.045 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=05:57 IST=> training   63.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.829 Prec@5=83.049 rate=2.95 Hz, eta=0:05:05, total=0:09:03, wall=05:57 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.829 Prec@5=83.049 rate=2.94 Hz, eta=0:04:32, total=0:09:38, wall=05:57 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.343 DataTime=0.240 Loss=1.641 Prec@1=60.829 Prec@5=83.049 rate=2.94 Hz, eta=0:04:32, total=0:09:38, wall=05:58 IST=> training   67.96% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.239 Loss=1.641 Prec@1=60.830 Prec@5=83.051 rate=2.94 Hz, eta=0:04:32, total=0:09:38, wall=05:58 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.239 Loss=1.641 Prec@1=60.830 Prec@5=83.051 rate=2.95 Hz, eta=0:03:58, total=0:10:11, wall=05:58 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.239 Loss=1.641 Prec@1=60.830 Prec@5=83.051 rate=2.95 Hz, eta=0:03:58, total=0:10:11, wall=05:58 IST=> training   71.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.642 Prec@1=60.826 Prec@5=83.043 rate=2.95 Hz, eta=0:03:58, total=0:10:11, wall=05:58 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.642 Prec@1=60.826 Prec@5=83.043 rate=2.95 Hz, eta=0:03:24, total=0:10:45, wall=05:58 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.642 Prec@1=60.826 Prec@5=83.043 rate=2.95 Hz, eta=0:03:24, total=0:10:45, wall=05:59 IST=> training   75.95% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.806 Prec@5=83.029 rate=2.95 Hz, eta=0:03:24, total=0:10:45, wall=05:59 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.806 Prec@5=83.029 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=05:59 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.806 Prec@5=83.029 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=05:59 IST=> training   79.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.796 Prec@5=83.020 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=05:59 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.796 Prec@5=83.020 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=05:59 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.238 Loss=1.643 Prec@1=60.796 Prec@5=83.020 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=06:00 IST=> training   83.94% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.237 Loss=1.644 Prec@1=60.776 Prec@5=83.027 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=06:00 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.237 Loss=1.644 Prec@1=60.776 Prec@5=83.027 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=06:00 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.342 DataTime=0.237 Loss=1.644 Prec@1=60.776 Prec@5=83.027 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=06:00 IST=> training   87.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.237 Loss=1.644 Prec@1=60.768 Prec@5=83.019 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=06:00 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.237 Loss=1.644 Prec@1=60.768 Prec@5=83.019 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=06:00 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.237 Loss=1.644 Prec@1=60.768 Prec@5=83.019 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=06:01 IST=> training   91.93% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.236 Loss=1.644 Prec@1=60.773 Prec@5=83.023 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=06:01 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.236 Loss=1.644 Prec@1=60.773 Prec@5=83.023 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=06:01 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.341 DataTime=0.236 Loss=1.644 Prec@1=60.773 Prec@5=83.023 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=06:02 IST=> training   95.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.340 DataTime=0.236 Loss=1.644 Prec@1=60.778 Prec@5=83.025 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=06:02 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.340 DataTime=0.236 Loss=1.644 Prec@1=60.778 Prec@5=83.025 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=06:02 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.340 DataTime=0.236 Loss=1.644 Prec@1=60.778 Prec@5=83.025 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=06:02 IST=> training   99.92% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.340 DataTime=0.236 Loss=1.644 Prec@1=60.776 Prec@5=83.024 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=06:02 IST=> training   100.00% of 1x2503...Epoch=125/150 LR=0.00723 Time=0.340 DataTime=0.236 Loss=1.644 Prec@1=60.776 Prec@5=83.024 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=06:02 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> validation 0.00% of 1x98...Epoch=125/150 LR=0.00723 Time=6.615 Loss=1.543 Prec@1=63.281 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.615 Loss=1.543 Prec@1=63.281 Prec@5=83.984 rate=5516.39 Hz, eta=0:00:00, total=0:00:00, wall=06:02 IST** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=6.615 Loss=1.543 Prec@1=63.281 Prec@5=83.984 rate=5516.39 Hz, eta=0:00:00, total=0:00:00, wall=06:02 IST** validation 1.02% of 1x98...Epoch=125/150 LR=0.00723 Time=0.397 Loss=1.654 Prec@1=60.466 Prec@5=83.134 rate=5516.39 Hz, eta=0:00:00, total=0:00:00, wall=06:02 IST** validation 100.00% of 1x98...Epoch=125/150 LR=0.00723 Time=0.397 Loss=1.654 Prec@1=60.466 Prec@5=83.134 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=06:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> training   0.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.001 DataTime=5.909 Loss=1.741 Prec@1=59.766 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=06:02 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.001 DataTime=5.909 Loss=1.741 Prec@1=59.766 Prec@5=82.227 rate=7658.49 Hz, eta=0:00:00, total=0:00:00, wall=06:02 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=6.001 DataTime=5.909 Loss=1.741 Prec@1=59.766 Prec@5=82.227 rate=7658.49 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST=> training   0.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.377 DataTime=0.289 Loss=1.625 Prec@1=61.013 Prec@5=83.362 rate=7658.49 Hz, eta=0:00:00, total=0:00:00, wall=06:03 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.377 DataTime=0.289 Loss=1.625 Prec@1=61.013 Prec@5=83.362 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=06:03 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.377 DataTime=0.289 Loss=1.625 Prec@1=61.013 Prec@5=83.362 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=06:03 IST=> training   4.04% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.354 DataTime=0.263 Loss=1.622 Prec@1=61.237 Prec@5=83.400 rate=3.15 Hz, eta=0:12:42, total=0:00:32, wall=06:03 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.354 DataTime=0.263 Loss=1.622 Prec@1=61.237 Prec@5=83.400 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=06:03 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.354 DataTime=0.263 Loss=1.622 Prec@1=61.237 Prec@5=83.400 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=06:04 IST=> training   8.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.346 DataTime=0.252 Loss=1.619 Prec@1=61.268 Prec@5=83.392 rate=3.08 Hz, eta=0:12:26, total=0:01:05, wall=06:04 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.346 DataTime=0.252 Loss=1.619 Prec@1=61.268 Prec@5=83.392 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=06:04 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.346 DataTime=0.252 Loss=1.619 Prec@1=61.268 Prec@5=83.392 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=06:05 IST=> training   12.03% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.350 DataTime=0.253 Loss=1.615 Prec@1=61.371 Prec@5=83.452 rate=3.06 Hz, eta=0:11:58, total=0:01:38, wall=06:05 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.350 DataTime=0.253 Loss=1.615 Prec@1=61.371 Prec@5=83.452 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:05 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.350 DataTime=0.253 Loss=1.615 Prec@1=61.371 Prec@5=83.452 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:05 IST=> training   16.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.347 DataTime=0.249 Loss=1.615 Prec@1=61.340 Prec@5=83.474 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=06:05 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.347 DataTime=0.249 Loss=1.615 Prec@1=61.340 Prec@5=83.474 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:05 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.347 DataTime=0.249 Loss=1.615 Prec@1=61.340 Prec@5=83.474 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:06 IST=> training   20.02% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.245 Loss=1.618 Prec@1=61.261 Prec@5=83.437 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=06:06 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.245 Loss=1.618 Prec@1=61.261 Prec@5=83.437 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:06 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.245 Loss=1.618 Prec@1=61.261 Prec@5=83.437 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:06 IST=> training   24.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.243 Loss=1.621 Prec@1=61.236 Prec@5=83.384 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=06:06 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.243 Loss=1.621 Prec@1=61.236 Prec@5=83.384 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=06:06 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.344 DataTime=0.243 Loss=1.621 Prec@1=61.236 Prec@5=83.384 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=06:07 IST=> training   28.01% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.241 Loss=1.623 Prec@1=61.166 Prec@5=83.349 rate=2.98 Hz, eta=0:10:03, total=0:03:54, wall=06:07 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.241 Loss=1.623 Prec@1=61.166 Prec@5=83.349 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=06:07 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.241 Loss=1.623 Prec@1=61.166 Prec@5=83.349 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=06:07 IST=> training   32.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.625 Prec@1=61.146 Prec@5=83.320 rate=2.99 Hz, eta=0:09:30, total=0:04:28, wall=06:07 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.625 Prec@1=61.146 Prec@5=83.320 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=06:07 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.625 Prec@1=61.146 Prec@5=83.320 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=06:08 IST=> training   36.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.343 DataTime=0.241 Loss=1.627 Prec@1=61.145 Prec@5=83.296 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=06:08 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.343 DataTime=0.241 Loss=1.627 Prec@1=61.145 Prec@5=83.296 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:08 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.343 DataTime=0.241 Loss=1.627 Prec@1=61.145 Prec@5=83.296 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:09 IST=> training   39.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.629 Prec@1=61.105 Prec@5=83.250 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:09 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.629 Prec@1=61.105 Prec@5=83.250 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:09 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.342 DataTime=0.240 Loss=1.629 Prec@1=61.105 Prec@5=83.250 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:09 IST=> training   43.99% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.095 Prec@5=83.254 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:09 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.095 Prec@5=83.254 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:09 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.095 Prec@5=83.254 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:10 IST=> training   47.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.073 Prec@5=83.244 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:10 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.073 Prec@5=83.244 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=06:10 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.238 Loss=1.629 Prec@1=61.073 Prec@5=83.244 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=06:10 IST=> training   51.98% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.237 Loss=1.629 Prec@1=61.079 Prec@5=83.248 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=06:10 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.237 Loss=1.629 Prec@1=61.079 Prec@5=83.248 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=06:10 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.341 DataTime=0.237 Loss=1.629 Prec@1=61.079 Prec@5=83.248 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=06:11 IST=> training   55.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.236 Loss=1.630 Prec@1=61.074 Prec@5=83.243 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=06:11 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.236 Loss=1.630 Prec@1=61.074 Prec@5=83.243 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:11 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.236 Loss=1.630 Prec@1=61.074 Prec@5=83.243 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:11 IST=> training   59.97% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.032 Prec@5=83.221 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=06:11 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.032 Prec@5=83.221 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:11 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.032 Prec@5=83.221 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:12 IST=> training   63.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.015 Prec@5=83.214 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:12 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.015 Prec@5=83.214 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=06:12 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.340 DataTime=0.235 Loss=1.632 Prec@1=61.015 Prec@5=83.214 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=06:12 IST=> training   67.96% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.632 Prec@1=61.018 Prec@5=83.215 rate=2.98 Hz, eta=0:04:29, total=0:09:31, wall=06:12 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.632 Prec@1=61.018 Prec@5=83.215 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=06:12 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.632 Prec@1=61.018 Prec@5=83.215 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=06:13 IST=> training   71.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.633 Prec@1=61.004 Prec@5=83.202 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=06:13 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.633 Prec@1=61.004 Prec@5=83.202 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=06:13 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.633 Prec@1=61.004 Prec@5=83.202 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=06:14 IST=> training   75.95% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.994 Prec@5=83.186 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=06:14 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.994 Prec@5=83.186 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=06:14 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.994 Prec@5=83.186 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=06:14 IST=> training   79.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.995 Prec@5=83.181 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=06:14 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.995 Prec@5=83.181 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=06:14 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.634 Prec@1=60.995 Prec@5=83.181 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=06:15 IST=> training   83.94% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.634 Prec@1=60.992 Prec@5=83.186 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=06:15 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.634 Prec@1=60.992 Prec@5=83.186 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:15 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.235 Loss=1.634 Prec@1=60.992 Prec@5=83.186 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:15 IST=> training   87.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.159 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=06:15 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.159 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=06:15 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.159 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=06:16 IST=> training   91.93% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.636 Prec@1=60.958 Prec@5=83.154 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=06:16 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.636 Prec@1=60.958 Prec@5=83.154 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:16 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.339 DataTime=0.234 Loss=1.636 Prec@1=60.958 Prec@5=83.154 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:16 IST=> training   95.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.338 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.161 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=06:16 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.338 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.161 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=06:16 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.338 DataTime=0.234 Loss=1.635 Prec@1=60.959 Prec@5=83.161 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=06:16 IST=> training   99.92% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.338 DataTime=0.234 Loss=1.635 Prec@1=60.960 Prec@5=83.161 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=06:16 IST=> training   100.00% of 1x2503...Epoch=126/150 LR=0.00670 Time=0.338 DataTime=0.234 Loss=1.635 Prec@1=60.960 Prec@5=83.161 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=06:16 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 0.00% of 1x98...Epoch=126/150 LR=0.00670 Time=6.987 Loss=1.642 Prec@1=61.523 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=6.987 Loss=1.642 Prec@1=61.523 Prec@5=82.812 rate=3672.68 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=6.987 Loss=1.642 Prec@1=61.523 Prec@5=82.812 rate=3672.68 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST** validation 1.02% of 1x98...Epoch=126/150 LR=0.00670 Time=0.407 Loss=1.644 Prec@1=60.562 Prec@5=83.322 rate=3672.68 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST** validation 100.00% of 1x98...Epoch=126/150 LR=0.00670 Time=0.407 Loss=1.644 Prec@1=60.562 Prec@5=83.322 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=06:17 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> training   0.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.943 DataTime=4.761 Loss=1.535 Prec@1=63.672 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=06:17 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.943 DataTime=4.761 Loss=1.535 Prec@1=63.672 Prec@5=84.766 rate=4190.18 Hz, eta=0:00:00, total=0:00:00, wall=06:17 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=4.943 DataTime=4.761 Loss=1.535 Prec@1=63.672 Prec@5=84.766 rate=4190.18 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST=> training   0.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.376 DataTime=0.281 Loss=1.626 Prec@1=61.239 Prec@5=83.246 rate=4190.18 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.376 DataTime=0.281 Loss=1.626 Prec@1=61.239 Prec@5=83.246 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=06:18 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.376 DataTime=0.281 Loss=1.626 Prec@1=61.239 Prec@5=83.246 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=06:18 IST=> training   4.04% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.351 DataTime=0.256 Loss=1.621 Prec@1=61.298 Prec@5=83.260 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=06:18 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.351 DataTime=0.256 Loss=1.621 Prec@1=61.298 Prec@5=83.260 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=06:18 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.351 DataTime=0.256 Loss=1.621 Prec@1=61.298 Prec@5=83.260 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=06:19 IST=> training   8.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.350 DataTime=0.253 Loss=1.612 Prec@1=61.514 Prec@5=83.433 rate=3.06 Hz, eta=0:12:31, total=0:01:05, wall=06:19 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.350 DataTime=0.253 Loss=1.612 Prec@1=61.514 Prec@5=83.433 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=06:19 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.350 DataTime=0.253 Loss=1.612 Prec@1=61.514 Prec@5=83.433 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=06:19 IST=> training   12.03% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.347 DataTime=0.247 Loss=1.612 Prec@1=61.537 Prec@5=83.470 rate=2.99 Hz, eta=0:12:15, total=0:01:40, wall=06:19 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.347 DataTime=0.247 Loss=1.612 Prec@1=61.537 Prec@5=83.470 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=06:19 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.347 DataTime=0.247 Loss=1.612 Prec@1=61.537 Prec@5=83.470 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=06:20 IST=> training   16.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.243 Loss=1.615 Prec@1=61.436 Prec@5=83.433 rate=2.99 Hz, eta=0:11:42, total=0:02:14, wall=06:20 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.243 Loss=1.615 Prec@1=61.436 Prec@5=83.433 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=06:20 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.243 Loss=1.615 Prec@1=61.436 Prec@5=83.433 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=06:21 IST=> training   20.02% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.242 Loss=1.618 Prec@1=61.343 Prec@5=83.403 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=06:21 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.242 Loss=1.618 Prec@1=61.343 Prec@5=83.403 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:21 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.343 DataTime=0.242 Loss=1.618 Prec@1=61.343 Prec@5=83.403 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:21 IST=> training   24.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.341 DataTime=0.239 Loss=1.620 Prec@1=61.305 Prec@5=83.345 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:21 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.341 DataTime=0.239 Loss=1.620 Prec@1=61.305 Prec@5=83.345 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:21 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.341 DataTime=0.239 Loss=1.620 Prec@1=61.305 Prec@5=83.345 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:22 IST=> training   28.01% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.238 Loss=1.621 Prec@1=61.269 Prec@5=83.336 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=06:22 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.238 Loss=1.621 Prec@1=61.269 Prec@5=83.336 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=06:22 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.238 Loss=1.621 Prec@1=61.269 Prec@5=83.336 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=06:22 IST=> training   32.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.622 Prec@1=61.235 Prec@5=83.326 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=06:22 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.622 Prec@1=61.235 Prec@5=83.326 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=06:22 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.622 Prec@1=61.235 Prec@5=83.326 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=06:23 IST=> training   36.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.623 Prec@1=61.234 Prec@5=83.310 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=06:23 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.623 Prec@1=61.234 Prec@5=83.310 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=06:23 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.236 Loss=1.623 Prec@1=61.234 Prec@5=83.310 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=06:23 IST=> training   39.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.624 Prec@1=61.213 Prec@5=83.300 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=06:23 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.624 Prec@1=61.213 Prec@5=83.300 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=06:23 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.624 Prec@1=61.213 Prec@5=83.300 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=06:24 IST=> training   43.99% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.338 DataTime=0.234 Loss=1.624 Prec@1=61.214 Prec@5=83.290 rate=2.99 Hz, eta=0:07:48, total=0:06:08, wall=06:24 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.338 DataTime=0.234 Loss=1.624 Prec@1=61.214 Prec@5=83.290 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=06:24 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.338 DataTime=0.234 Loss=1.624 Prec@1=61.214 Prec@5=83.290 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=06:24 IST=> training   47.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.625 Prec@1=61.194 Prec@5=83.288 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=06:24 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.625 Prec@1=61.194 Prec@5=83.288 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:24 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.625 Prec@1=61.194 Prec@5=83.288 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:25 IST=> training   51.98% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.625 Prec@1=61.200 Prec@5=83.299 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=06:25 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.625 Prec@1=61.200 Prec@5=83.299 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=06:25 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.625 Prec@1=61.200 Prec@5=83.299 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=06:26 IST=> training   55.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.182 Prec@5=83.300 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=06:26 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.182 Prec@5=83.300 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=06:26 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.182 Prec@5=83.300 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=06:26 IST=> training   59.97% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.626 Prec@1=61.158 Prec@5=83.285 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=06:26 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.626 Prec@1=61.158 Prec@5=83.285 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:26 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.235 Loss=1.626 Prec@1=61.158 Prec@5=83.285 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:27 IST=> training   63.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.140 Prec@5=83.260 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=06:27 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.140 Prec@5=83.260 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=06:27 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.140 Prec@5=83.260 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=06:27 IST=> training   67.96% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.149 Prec@5=83.278 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=06:27 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.149 Prec@5=83.278 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:27 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.626 Prec@1=61.149 Prec@5=83.278 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:28 IST=> training   71.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.118 Prec@5=83.268 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=06:28 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.118 Prec@5=83.268 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:28 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.234 Loss=1.627 Prec@1=61.118 Prec@5=83.268 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:28 IST=> training   75.95% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.119 Prec@5=83.266 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=06:28 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.119 Prec@5=83.266 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=06:28 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.119 Prec@5=83.266 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=06:29 IST=> training   79.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.117 Prec@5=83.255 rate=2.97 Hz, eta=0:02:49, total=0:11:13, wall=06:29 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.117 Prec@5=83.255 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:29 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.627 Prec@1=61.117 Prec@5=83.255 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:30 IST=> training   83.94% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.235 Loss=1.628 Prec@1=61.101 Prec@5=83.249 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=06:30 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.235 Loss=1.628 Prec@1=61.101 Prec@5=83.249 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=06:30 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.340 DataTime=0.235 Loss=1.628 Prec@1=61.101 Prec@5=83.249 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=06:30 IST=> training   87.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.098 Prec@5=83.237 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=06:30 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.098 Prec@5=83.237 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:30 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.098 Prec@5=83.237 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:31 IST=> training   91.93% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.232 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=06:31 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.232 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=06:31 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.232 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=06:31 IST=> training   95.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.234 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=06:31 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.234 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:31 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.234 Loss=1.628 Prec@1=61.113 Prec@5=83.234 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:31 IST=> training   99.92% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.233 Loss=1.628 Prec@1=61.112 Prec@5=83.233 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:31 IST=> training   100.00% of 1x2503...Epoch=127/150 LR=0.00618 Time=0.339 DataTime=0.233 Loss=1.628 Prec@1=61.112 Prec@5=83.233 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:31 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST=> validation 0.00% of 1x98...Epoch=127/150 LR=0.00618 Time=6.779 Loss=1.794 Prec@1=56.250 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST=> validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.779 Loss=1.794 Prec@1=56.250 Prec@5=82.227 rate=2870.21 Hz, eta=0:00:00, total=0:00:00, wall=06:31 IST** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=6.779 Loss=1.794 Prec@1=56.250 Prec@5=82.227 rate=2870.21 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST** validation 1.02% of 1x98...Epoch=127/150 LR=0.00618 Time=0.399 Loss=1.667 Prec@1=60.354 Prec@5=82.960 rate=2870.21 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST** validation 100.00% of 1x98...Epoch=127/150 LR=0.00618 Time=0.399 Loss=1.667 Prec@1=60.354 Prec@5=82.960 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=06:32 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST=> training   0.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.589 DataTime=5.420 Loss=1.635 Prec@1=58.203 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.589 DataTime=5.420 Loss=1.635 Prec@1=58.203 Prec@5=82.422 rate=5878.20 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=5.589 DataTime=5.420 Loss=1.635 Prec@1=58.203 Prec@5=82.422 rate=5878.20 Hz, eta=0:00:00, total=0:00:00, wall=06:33 IST=> training   0.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.373 DataTime=0.279 Loss=1.616 Prec@1=61.175 Prec@5=83.290 rate=5878.20 Hz, eta=0:00:00, total=0:00:00, wall=06:33 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.373 DataTime=0.279 Loss=1.616 Prec@1=61.175 Prec@5=83.290 rate=3.15 Hz, eta=0:12:43, total=0:00:32, wall=06:33 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.373 DataTime=0.279 Loss=1.616 Prec@1=61.175 Prec@5=83.290 rate=3.15 Hz, eta=0:12:43, total=0:00:32, wall=06:33 IST=> training   4.04% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.256 Loss=1.613 Prec@1=61.253 Prec@5=83.432 rate=3.15 Hz, eta=0:12:43, total=0:00:32, wall=06:33 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.256 Loss=1.613 Prec@1=61.253 Prec@5=83.432 rate=3.10 Hz, eta=0:12:21, total=0:01:04, wall=06:33 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.256 Loss=1.613 Prec@1=61.253 Prec@5=83.432 rate=3.10 Hz, eta=0:12:21, total=0:01:04, wall=06:34 IST=> training   8.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.346 DataTime=0.249 Loss=1.611 Prec@1=61.388 Prec@5=83.432 rate=3.10 Hz, eta=0:12:21, total=0:01:04, wall=06:34 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.346 DataTime=0.249 Loss=1.611 Prec@1=61.388 Prec@5=83.432 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=06:34 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.346 DataTime=0.249 Loss=1.611 Prec@1=61.388 Prec@5=83.432 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=06:34 IST=> training   12.03% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.252 Loss=1.610 Prec@1=61.471 Prec@5=83.430 rate=3.06 Hz, eta=0:12:00, total=0:01:38, wall=06:34 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.252 Loss=1.610 Prec@1=61.471 Prec@5=83.430 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=06:34 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.350 DataTime=0.252 Loss=1.610 Prec@1=61.471 Prec@5=83.430 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=06:35 IST=> training   16.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.445 Prec@5=83.423 rate=2.97 Hz, eta=0:11:46, total=0:02:14, wall=06:35 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.445 Prec@5=83.423 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=06:35 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.445 Prec@5=83.423 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=06:35 IST=> training   20.02% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.242 Loss=1.613 Prec@1=61.449 Prec@5=83.388 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=06:35 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.242 Loss=1.613 Prec@1=61.449 Prec@5=83.388 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:35 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.242 Loss=1.613 Prec@1=61.449 Prec@5=83.388 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:36 IST=> training   24.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.445 Prec@5=83.395 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=06:36 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.445 Prec@5=83.395 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:36 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.445 Prec@5=83.395 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:36 IST=> training   28.01% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.429 Prec@5=83.381 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:36 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.429 Prec@5=83.381 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=06:36 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.344 DataTime=0.241 Loss=1.613 Prec@1=61.429 Prec@5=83.381 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=06:37 IST=> training   32.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.614 Prec@1=61.397 Prec@5=83.393 rate=2.97 Hz, eta=0:09:33, total=0:04:30, wall=06:37 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.614 Prec@1=61.397 Prec@5=83.393 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=06:37 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.614 Prec@1=61.397 Prec@5=83.393 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=06:38 IST=> training   36.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.343 DataTime=0.238 Loss=1.614 Prec@1=61.385 Prec@5=83.387 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=06:38 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.343 DataTime=0.238 Loss=1.614 Prec@1=61.385 Prec@5=83.387 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:38 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.343 DataTime=0.238 Loss=1.614 Prec@1=61.385 Prec@5=83.387 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:38 IST=> training   39.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.615 Prec@1=61.370 Prec@5=83.381 rate=2.97 Hz, eta=0:08:26, total=0:05:37, wall=06:38 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.615 Prec@1=61.370 Prec@5=83.381 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:38 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.238 Loss=1.615 Prec@1=61.370 Prec@5=83.381 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:39 IST=> training   43.99% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.616 Prec@1=61.345 Prec@5=83.360 rate=2.97 Hz, eta=0:07:52, total=0:06:10, wall=06:39 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.616 Prec@1=61.345 Prec@5=83.360 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:39 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.616 Prec@1=61.345 Prec@5=83.360 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:39 IST=> training   47.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.617 Prec@1=61.325 Prec@5=83.349 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=06:39 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.617 Prec@1=61.325 Prec@5=83.349 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=06:39 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.342 DataTime=0.237 Loss=1.617 Prec@1=61.325 Prec@5=83.349 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=06:40 IST=> training   51.98% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.618 Prec@1=61.303 Prec@5=83.334 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=06:40 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.618 Prec@1=61.303 Prec@5=83.334 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:40 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.618 Prec@1=61.303 Prec@5=83.334 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:40 IST=> training   55.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.306 Prec@5=83.333 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=06:40 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.306 Prec@5=83.333 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:40 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.306 Prec@5=83.333 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:41 IST=> training   59.97% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.617 Prec@1=61.318 Prec@5=83.332 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=06:41 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.617 Prec@1=61.318 Prec@5=83.332 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=06:41 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.236 Loss=1.617 Prec@1=61.318 Prec@5=83.332 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=06:42 IST=> training   63.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.235 Loss=1.618 Prec@1=61.332 Prec@5=83.323 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=06:42 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.235 Loss=1.618 Prec@1=61.332 Prec@5=83.323 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=06:42 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.341 DataTime=0.235 Loss=1.618 Prec@1=61.332 Prec@5=83.323 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=06:42 IST=> training   67.96% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.618 Prec@1=61.335 Prec@5=83.320 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=06:42 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.618 Prec@1=61.335 Prec@5=83.320 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:42 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.618 Prec@1=61.335 Prec@5=83.320 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:43 IST=> training   71.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.333 Prec@5=83.326 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=06:43 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.333 Prec@5=83.326 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=06:43 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.618 Prec@1=61.333 Prec@5=83.326 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=06:43 IST=> training   75.95% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.320 Prec@5=83.327 rate=2.97 Hz, eta=0:03:23, total=0:10:41, wall=06:43 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.320 Prec@5=83.327 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:43 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.320 Prec@5=83.327 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:44 IST=> training   79.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.308 Prec@5=83.322 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=06:44 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.308 Prec@5=83.322 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=06:44 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.619 Prec@1=61.308 Prec@5=83.322 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=06:44 IST=> training   83.94% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.619 Prec@1=61.299 Prec@5=83.321 rate=2.97 Hz, eta=0:02:15, total=0:11:48, wall=06:44 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.619 Prec@1=61.299 Prec@5=83.321 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=06:44 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.235 Loss=1.619 Prec@1=61.299 Prec@5=83.321 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=06:45 IST=> training   87.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.620 Prec@1=61.293 Prec@5=83.313 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=06:45 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.620 Prec@1=61.293 Prec@5=83.313 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=06:45 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.340 DataTime=0.234 Loss=1.620 Prec@1=61.293 Prec@5=83.313 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=06:45 IST=> training   91.93% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.620 Prec@1=61.301 Prec@5=83.310 rate=2.96 Hz, eta=0:01:08, total=0:12:56, wall=06:45 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.620 Prec@1=61.301 Prec@5=83.310 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:45 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.620 Prec@1=61.301 Prec@5=83.310 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:46 IST=> training   95.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.621 Prec@1=61.287 Prec@5=83.300 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=06:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.621 Prec@1=61.287 Prec@5=83.300 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.621 Prec@1=61.287 Prec@5=83.300 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:46 IST=> training   99.92% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.621 Prec@1=61.287 Prec@5=83.299 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:46 IST=> training   100.00% of 1x2503...Epoch=128/150 LR=0.00569 Time=0.339 DataTime=0.233 Loss=1.621 Prec@1=61.287 Prec@5=83.299 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=06:46 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST=> validation 0.00% of 1x98...Epoch=128/150 LR=0.00569 Time=6.455 Loss=1.623 Prec@1=59.180 Prec@5=82.031 rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST=> validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.455 Loss=1.623 Prec@1=59.180 Prec@5=82.031 rate=5153.21 Hz, eta=0:00:00, total=0:00:00, wall=06:46 IST** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=6.455 Loss=1.623 Prec@1=59.180 Prec@5=82.031 rate=5153.21 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST** validation 1.02% of 1x98...Epoch=128/150 LR=0.00569 Time=0.404 Loss=1.651 Prec@1=60.506 Prec@5=82.976 rate=5153.21 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST** validation 100.00% of 1x98...Epoch=128/150 LR=0.00569 Time=0.404 Loss=1.651 Prec@1=60.506 Prec@5=82.976 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=06:47 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:47 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:47 IST=> training   0.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.871 DataTime=5.598 Loss=1.522 Prec@1=62.695 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=06:47 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.871 DataTime=5.598 Loss=1.522 Prec@1=62.695 Prec@5=86.328 rate=5863.32 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=5.871 DataTime=5.598 Loss=1.522 Prec@1=62.695 Prec@5=86.328 rate=5863.32 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST=> training   0.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.382 DataTime=0.284 Loss=1.617 Prec@1=61.173 Prec@5=83.466 rate=5863.32 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.382 DataTime=0.284 Loss=1.617 Prec@1=61.173 Prec@5=83.466 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=06:47 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.382 DataTime=0.284 Loss=1.617 Prec@1=61.173 Prec@5=83.466 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=06:48 IST=> training   4.04% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.355 DataTime=0.257 Loss=1.608 Prec@1=61.476 Prec@5=83.564 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=06:48 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.355 DataTime=0.257 Loss=1.608 Prec@1=61.476 Prec@5=83.564 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=06:48 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.355 DataTime=0.257 Loss=1.608 Prec@1=61.476 Prec@5=83.564 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=06:48 IST=> training   8.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.351 DataTime=0.250 Loss=1.608 Prec@1=61.488 Prec@5=83.563 rate=3.07 Hz, eta=0:12:29, total=0:01:05, wall=06:48 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.351 DataTime=0.250 Loss=1.608 Prec@1=61.488 Prec@5=83.563 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=06:48 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.351 DataTime=0.250 Loss=1.608 Prec@1=61.488 Prec@5=83.563 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=06:49 IST=> training   12.03% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.399 Prec@5=83.499 rate=3.02 Hz, eta=0:12:10, total=0:01:39, wall=06:49 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.399 Prec@5=83.499 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=06:49 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.347 DataTime=0.246 Loss=1.610 Prec@1=61.399 Prec@5=83.499 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=06:50 IST=> training   16.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.350 DataTime=0.247 Loss=1.610 Prec@1=61.391 Prec@5=83.464 rate=3.01 Hz, eta=0:11:39, total=0:02:13, wall=06:50 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.350 DataTime=0.247 Loss=1.610 Prec@1=61.391 Prec@5=83.464 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=06:50 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.350 DataTime=0.247 Loss=1.610 Prec@1=61.391 Prec@5=83.464 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=06:50 IST=> training   20.02% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.346 DataTime=0.242 Loss=1.610 Prec@1=61.350 Prec@5=83.487 rate=2.96 Hz, eta=0:11:16, total=0:02:49, wall=06:50 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.346 DataTime=0.242 Loss=1.610 Prec@1=61.350 Prec@5=83.487 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=06:50 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.346 DataTime=0.242 Loss=1.610 Prec@1=61.350 Prec@5=83.487 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=06:51 IST=> training   24.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.240 Loss=1.609 Prec@1=61.393 Prec@5=83.501 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.240 Loss=1.609 Prec@1=61.393 Prec@5=83.501 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.240 Loss=1.609 Prec@1=61.393 Prec@5=83.501 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:51 IST=> training   28.01% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.610 Prec@1=61.395 Prec@5=83.493 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=06:51 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.610 Prec@1=61.395 Prec@5=83.493 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=06:51 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.610 Prec@1=61.395 Prec@5=83.493 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=06:52 IST=> training   32.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.612 Prec@1=61.347 Prec@5=83.455 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=06:52 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.612 Prec@1=61.347 Prec@5=83.455 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=06:52 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.239 Loss=1.612 Prec@1=61.347 Prec@5=83.455 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=06:52 IST=> training   36.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.238 Loss=1.612 Prec@1=61.346 Prec@5=83.422 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=06:52 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.238 Loss=1.612 Prec@1=61.346 Prec@5=83.422 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=06:52 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.238 Loss=1.612 Prec@1=61.346 Prec@5=83.422 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=06:53 IST=> training   39.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.238 Loss=1.612 Prec@1=61.357 Prec@5=83.423 rate=2.96 Hz, eta=0:08:27, total=0:05:38, wall=06:53 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.238 Loss=1.612 Prec@1=61.357 Prec@5=83.423 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=06:53 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.345 DataTime=0.238 Loss=1.612 Prec@1=61.357 Prec@5=83.423 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=06:54 IST=> training   43.99% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.237 Loss=1.612 Prec@1=61.367 Prec@5=83.421 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=06:54 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.237 Loss=1.612 Prec@1=61.367 Prec@5=83.421 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=06:54 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.344 DataTime=0.237 Loss=1.612 Prec@1=61.367 Prec@5=83.421 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=06:54 IST=> training   47.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.611 Prec@1=61.399 Prec@5=83.448 rate=2.95 Hz, eta=0:07:21, total=0:06:47, wall=06:54 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.611 Prec@1=61.399 Prec@5=83.448 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=06:54 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.611 Prec@1=61.399 Prec@5=83.448 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=06:55 IST=> training   51.98% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.237 Loss=1.611 Prec@1=61.392 Prec@5=83.439 rate=2.95 Hz, eta=0:06:46, total=0:07:20, wall=06:55 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.237 Loss=1.611 Prec@1=61.392 Prec@5=83.439 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=06:55 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.237 Loss=1.611 Prec@1=61.392 Prec@5=83.439 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=06:55 IST=> training   55.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.613 Prec@1=61.364 Prec@5=83.431 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=06:55 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.613 Prec@1=61.364 Prec@5=83.431 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=06:55 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.613 Prec@1=61.364 Prec@5=83.431 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=06:56 IST=> training   59.97% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.613 Prec@1=61.347 Prec@5=83.438 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=06:56 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.613 Prec@1=61.347 Prec@5=83.438 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=06:56 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.613 Prec@1=61.347 Prec@5=83.438 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=06:56 IST=> training   63.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.613 Prec@1=61.359 Prec@5=83.436 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=06:56 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.613 Prec@1=61.359 Prec@5=83.436 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=06:56 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.343 DataTime=0.236 Loss=1.613 Prec@1=61.359 Prec@5=83.436 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=06:57 IST=> training   67.96% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.356 Prec@5=83.422 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=06:57 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.356 Prec@5=83.422 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=06:57 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.356 Prec@5=83.422 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=06:58 IST=> training   71.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.366 Prec@5=83.435 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=06:58 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.366 Prec@5=83.435 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=06:58 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.235 Loss=1.614 Prec@1=61.366 Prec@5=83.435 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=06:58 IST=> training   75.95% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.615 Prec@1=61.348 Prec@5=83.433 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=06:58 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.615 Prec@1=61.348 Prec@5=83.433 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=06:58 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.342 DataTime=0.236 Loss=1.615 Prec@1=61.348 Prec@5=83.433 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=06:59 IST=> training   79.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.346 Prec@5=83.434 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=06:59 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.346 Prec@5=83.434 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=06:59 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.346 Prec@5=83.434 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=06:59 IST=> training   83.94% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.234 Loss=1.615 Prec@1=61.350 Prec@5=83.430 rate=2.95 Hz, eta=0:02:16, total=0:11:51, wall=06:59 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.234 Loss=1.615 Prec@1=61.350 Prec@5=83.430 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=06:59 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.234 Loss=1.615 Prec@1=61.350 Prec@5=83.430 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=07:00 IST=> training   87.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.616 Prec@1=61.342 Prec@5=83.416 rate=2.96 Hz, eta=0:01:42, total=0:12:24, wall=07:00 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.616 Prec@1=61.342 Prec@5=83.416 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=07:00 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.616 Prec@1=61.342 Prec@5=83.416 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=07:00 IST=> training   91.93% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.350 Prec@5=83.425 rate=2.95 Hz, eta=0:01:08, total=0:12:59, wall=07:00 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.350 Prec@5=83.425 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=07:00 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.341 DataTime=0.235 Loss=1.615 Prec@1=61.350 Prec@5=83.425 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=07:01 IST=> training   95.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.340 DataTime=0.234 Loss=1.616 Prec@1=61.337 Prec@5=83.419 rate=2.95 Hz, eta=0:00:34, total=0:13:32, wall=07:01 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.340 DataTime=0.234 Loss=1.616 Prec@1=61.337 Prec@5=83.419 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=07:01 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.340 DataTime=0.234 Loss=1.616 Prec@1=61.337 Prec@5=83.419 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=07:01 IST=> training   99.92% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.340 DataTime=0.234 Loss=1.616 Prec@1=61.338 Prec@5=83.419 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=07:01 IST=> training   100.00% of 1x2503...Epoch=129/150 LR=0.00521 Time=0.340 DataTime=0.234 Loss=1.616 Prec@1=61.338 Prec@5=83.419 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=07:01 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:01 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:01 IST=> validation 0.00% of 1x98...Epoch=129/150 LR=0.00521 Time=7.101 Loss=1.725 Prec@1=60.742 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=07:01 IST=> validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=7.101 Loss=1.725 Prec@1=60.742 Prec@5=82.812 rate=3561.68 Hz, eta=0:00:00, total=0:00:00, wall=07:01 IST** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=7.101 Loss=1.725 Prec@1=60.742 Prec@5=82.812 rate=3561.68 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST** validation 1.02% of 1x98...Epoch=129/150 LR=0.00521 Time=0.400 Loss=1.642 Prec@1=60.728 Prec@5=83.248 rate=3561.68 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST** validation 100.00% of 1x98...Epoch=129/150 LR=0.00521 Time=0.400 Loss=1.642 Prec@1=60.728 Prec@5=83.248 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=07:02 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:02 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:02 IST=> training   0.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=4.979 DataTime=4.804 Loss=1.464 Prec@1=64.648 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=07:02 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=4.979 DataTime=4.804 Loss=1.464 Prec@1=64.648 Prec@5=87.109 rate=4016.40 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=4.979 DataTime=4.804 Loss=1.464 Prec@1=64.648 Prec@5=87.109 rate=4016.40 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST=> training   0.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.375 DataTime=0.278 Loss=1.579 Prec@1=62.341 Prec@5=83.924 rate=4016.40 Hz, eta=0:00:00, total=0:00:00, wall=07:02 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.375 DataTime=0.278 Loss=1.579 Prec@1=62.341 Prec@5=83.924 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=07:02 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.375 DataTime=0.278 Loss=1.579 Prec@1=62.341 Prec@5=83.924 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=07:03 IST=> training   4.04% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.354 DataTime=0.258 Loss=1.581 Prec@1=62.112 Prec@5=83.958 rate=3.07 Hz, eta=0:13:01, total=0:00:32, wall=07:03 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.354 DataTime=0.258 Loss=1.581 Prec@1=62.112 Prec@5=83.958 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=07:03 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.354 DataTime=0.258 Loss=1.581 Prec@1=62.112 Prec@5=83.958 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=07:03 IST=> training   8.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.349 DataTime=0.249 Loss=1.590 Prec@1=61.934 Prec@5=83.843 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=07:03 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.349 DataTime=0.249 Loss=1.590 Prec@1=61.934 Prec@5=83.843 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=07:03 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.349 DataTime=0.249 Loss=1.590 Prec@1=61.934 Prec@5=83.843 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=07:04 IST=> training   12.03% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.245 Loss=1.593 Prec@1=61.843 Prec@5=83.792 rate=3.01 Hz, eta=0:12:11, total=0:01:39, wall=07:04 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.245 Loss=1.593 Prec@1=61.843 Prec@5=83.792 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=07:04 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.245 Loss=1.593 Prec@1=61.843 Prec@5=83.792 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=07:04 IST=> training   16.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.244 Loss=1.594 Prec@1=61.846 Prec@5=83.765 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=07:04 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.244 Loss=1.594 Prec@1=61.846 Prec@5=83.765 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=07:04 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.344 DataTime=0.244 Loss=1.594 Prec@1=61.846 Prec@5=83.765 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=07:05 IST=> training   20.02% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.343 DataTime=0.242 Loss=1.597 Prec@1=61.772 Prec@5=83.707 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=07:05 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.343 DataTime=0.242 Loss=1.597 Prec@1=61.772 Prec@5=83.707 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=07:05 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.343 DataTime=0.242 Loss=1.597 Prec@1=61.772 Prec@5=83.707 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=07:06 IST=> training   24.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.342 DataTime=0.241 Loss=1.600 Prec@1=61.723 Prec@5=83.666 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=07:06 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.342 DataTime=0.241 Loss=1.600 Prec@1=61.723 Prec@5=83.666 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=07:06 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.342 DataTime=0.241 Loss=1.600 Prec@1=61.723 Prec@5=83.666 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=07:06 IST=> training   28.01% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.724 Prec@5=83.658 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=07:06 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.724 Prec@5=83.658 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:06 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.724 Prec@5=83.658 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:07 IST=> training   32.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.710 Prec@5=83.671 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:07 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.710 Prec@5=83.671 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=07:07 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.341 DataTime=0.240 Loss=1.600 Prec@1=61.710 Prec@5=83.671 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=07:07 IST=> training   36.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.601 Prec@1=61.706 Prec@5=83.652 rate=2.98 Hz, eta=0:08:58, total=0:05:02, wall=07:07 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.601 Prec@1=61.706 Prec@5=83.652 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=07:07 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.601 Prec@1=61.706 Prec@5=83.652 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=07:08 IST=> training   39.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.603 Prec@1=61.667 Prec@5=83.636 rate=2.98 Hz, eta=0:08:23, total=0:05:35, wall=07:08 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.603 Prec@1=61.667 Prec@5=83.636 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:08 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.238 Loss=1.603 Prec@1=61.667 Prec@5=83.636 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:08 IST=> training   43.99% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.604 Prec@1=61.617 Prec@5=83.620 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:08 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.604 Prec@1=61.617 Prec@5=83.620 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=07:08 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.604 Prec@1=61.617 Prec@5=83.620 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=07:09 IST=> training   47.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.598 Prec@5=83.613 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=07:09 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.598 Prec@5=83.613 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=07:09 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.598 Prec@5=83.613 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=07:10 IST=> training   51.98% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.587 Prec@5=83.603 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=07:10 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.587 Prec@5=83.603 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:10 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.237 Loss=1.605 Prec@1=61.587 Prec@5=83.603 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:10 IST=> training   55.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.237 Loss=1.607 Prec@1=61.552 Prec@5=83.574 rate=2.98 Hz, eta=0:06:10, total=0:07:50, wall=07:10 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.237 Loss=1.607 Prec@1=61.552 Prec@5=83.574 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=07:10 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.237 Loss=1.607 Prec@1=61.552 Prec@5=83.574 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=07:11 IST=> training   59.97% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.538 Prec@5=83.552 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=07:11 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.538 Prec@5=83.552 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:11 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.538 Prec@5=83.552 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:11 IST=> training   63.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.526 Prec@5=83.544 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:11 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.526 Prec@5=83.544 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:11 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.236 Loss=1.608 Prec@1=61.526 Prec@5=83.544 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:12 IST=> training   67.96% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.505 Prec@5=83.530 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:12 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.505 Prec@5=83.530 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=07:12 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.505 Prec@5=83.530 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=07:12 IST=> training   71.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.338 DataTime=0.235 Loss=1.609 Prec@1=61.513 Prec@5=83.529 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=07:12 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.338 DataTime=0.235 Loss=1.609 Prec@1=61.513 Prec@5=83.529 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:12 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.338 DataTime=0.235 Loss=1.609 Prec@1=61.513 Prec@5=83.529 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:13 IST=> training   75.95% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.520 Prec@5=83.521 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=07:13 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.520 Prec@5=83.521 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=07:13 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.609 Prec@1=61.520 Prec@5=83.521 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=07:13 IST=> training   79.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.515 Prec@5=83.504 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=07:13 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.515 Prec@5=83.504 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:13 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.515 Prec@5=83.504 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:14 IST=> training   83.94% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.509 Prec@5=83.506 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=07:14 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.509 Prec@5=83.506 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=07:14 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.509 Prec@5=83.506 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=07:15 IST=> training   87.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.500 Prec@5=83.497 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=07:15 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.500 Prec@5=83.497 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:15 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.235 Loss=1.610 Prec@1=61.500 Prec@5=83.497 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:15 IST=> training   91.93% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.236 Loss=1.611 Prec@1=61.489 Prec@5=83.499 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:15 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.236 Loss=1.611 Prec@1=61.489 Prec@5=83.499 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=07:15 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.340 DataTime=0.236 Loss=1.611 Prec@1=61.489 Prec@5=83.499 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=07:16 IST=> training   95.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.234 Loss=1.611 Prec@1=61.485 Prec@5=83.495 rate=2.96 Hz, eta=0:00:34, total=0:13:30, wall=07:16 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.234 Loss=1.611 Prec@1=61.485 Prec@5=83.495 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=07:16 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.234 Loss=1.611 Prec@1=61.485 Prec@5=83.495 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=07:16 IST=> training   99.92% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.234 Loss=1.611 Prec@1=61.485 Prec@5=83.494 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=07:16 IST=> training   100.00% of 1x2503...Epoch=130/150 LR=0.00476 Time=0.339 DataTime=0.234 Loss=1.611 Prec@1=61.485 Prec@5=83.494 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=07:16 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> validation 0.00% of 1x98...Epoch=130/150 LR=0.00476 Time=7.116 Loss=1.661 Prec@1=58.984 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=7.116 Loss=1.661 Prec@1=58.984 Prec@5=83.398 rate=5271.37 Hz, eta=0:00:00, total=0:00:00, wall=07:16 IST** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=7.116 Loss=1.661 Prec@1=58.984 Prec@5=83.398 rate=5271.37 Hz, eta=0:00:00, total=0:00:00, wall=07:16 IST** validation 1.02% of 1x98...Epoch=130/150 LR=0.00476 Time=0.403 Loss=1.654 Prec@1=60.432 Prec@5=83.162 rate=5271.37 Hz, eta=0:00:00, total=0:00:00, wall=07:16 IST** validation 100.00% of 1x98...Epoch=130/150 LR=0.00476 Time=0.403 Loss=1.654 Prec@1=60.432 Prec@5=83.162 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=07:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> training   0.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.247 DataTime=5.091 Loss=1.539 Prec@1=62.695 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=07:16 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.247 DataTime=5.091 Loss=1.539 Prec@1=62.695 Prec@5=83.398 rate=4198.12 Hz, eta=0:00:00, total=0:00:00, wall=07:16 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=5.247 DataTime=5.091 Loss=1.539 Prec@1=62.695 Prec@5=83.398 rate=4198.12 Hz, eta=0:00:00, total=0:00:00, wall=07:17 IST=> training   0.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.381 DataTime=0.284 Loss=1.607 Prec@1=61.452 Prec@5=83.514 rate=4198.12 Hz, eta=0:00:00, total=0:00:00, wall=07:17 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.381 DataTime=0.284 Loss=1.607 Prec@1=61.452 Prec@5=83.514 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=07:17 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.381 DataTime=0.284 Loss=1.607 Prec@1=61.452 Prec@5=83.514 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=07:18 IST=> training   4.04% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.357 DataTime=0.257 Loss=1.601 Prec@1=61.679 Prec@5=83.608 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=07:18 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.357 DataTime=0.257 Loss=1.601 Prec@1=61.679 Prec@5=83.608 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:18 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.357 DataTime=0.257 Loss=1.601 Prec@1=61.679 Prec@5=83.608 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:18 IST=> training   8.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.347 DataTime=0.245 Loss=1.598 Prec@1=61.656 Prec@5=83.687 rate=3.02 Hz, eta=0:12:42, total=0:01:06, wall=07:18 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.347 DataTime=0.245 Loss=1.598 Prec@1=61.656 Prec@5=83.687 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:18 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.347 DataTime=0.245 Loss=1.598 Prec@1=61.656 Prec@5=83.687 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:19 IST=> training   12.03% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.344 DataTime=0.242 Loss=1.599 Prec@1=61.658 Prec@5=83.692 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=07:19 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.344 DataTime=0.242 Loss=1.599 Prec@1=61.658 Prec@5=83.692 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=07:19 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.344 DataTime=0.242 Loss=1.599 Prec@1=61.658 Prec@5=83.692 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=07:19 IST=> training   16.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.343 DataTime=0.240 Loss=1.597 Prec@1=61.718 Prec@5=83.730 rate=3.02 Hz, eta=0:11:35, total=0:02:12, wall=07:19 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.343 DataTime=0.240 Loss=1.597 Prec@1=61.718 Prec@5=83.730 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=07:19 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.343 DataTime=0.240 Loss=1.597 Prec@1=61.718 Prec@5=83.730 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=07:20 IST=> training   20.02% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.729 Prec@5=83.719 rate=3.01 Hz, eta=0:11:05, total=0:02:46, wall=07:20 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.729 Prec@5=83.719 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:20 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.729 Prec@5=83.719 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:20 IST=> training   24.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.342 DataTime=0.239 Loss=1.597 Prec@1=61.707 Prec@5=83.725 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:20 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.342 DataTime=0.239 Loss=1.597 Prec@1=61.707 Prec@5=83.725 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=07:20 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.342 DataTime=0.239 Loss=1.597 Prec@1=61.707 Prec@5=83.725 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=07:21 IST=> training   28.01% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.719 Prec@5=83.717 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=07:21 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.719 Prec@5=83.717 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:21 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.341 DataTime=0.239 Loss=1.597 Prec@1=61.719 Prec@5=83.717 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:22 IST=> training   32.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.597 Prec@1=61.742 Prec@5=83.707 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=07:22 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.597 Prec@1=61.742 Prec@5=83.707 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=07:22 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.597 Prec@1=61.742 Prec@5=83.707 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=07:22 IST=> training   36.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.598 Prec@1=61.765 Prec@5=83.678 rate=3.00 Hz, eta=0:08:54, total=0:05:00, wall=07:22 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.598 Prec@1=61.765 Prec@5=83.678 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:22 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.598 Prec@1=61.765 Prec@5=83.678 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:23 IST=> training   39.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.598 Prec@1=61.756 Prec@5=83.697 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:23 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.598 Prec@1=61.756 Prec@5=83.697 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:23 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.340 DataTime=0.237 Loss=1.598 Prec@1=61.756 Prec@5=83.697 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:23 IST=> training   43.99% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.735 Prec@5=83.679 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:23 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.735 Prec@5=83.679 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:23 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.735 Prec@5=83.679 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:24 IST=> training   47.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.724 Prec@5=83.688 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=07:24 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.724 Prec@5=83.688 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:24 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.724 Prec@5=83.688 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:24 IST=> training   51.98% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.716 Prec@5=83.672 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:24 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.716 Prec@5=83.672 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=07:24 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.599 Prec@1=61.716 Prec@5=83.672 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=07:25 IST=> training   55.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.687 Prec@5=83.662 rate=2.98 Hz, eta=0:06:09, total=0:07:50, wall=07:25 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.687 Prec@5=83.662 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=07:25 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.687 Prec@5=83.662 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=07:25 IST=> training   59.97% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.600 Prec@1=61.662 Prec@5=83.658 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=07:25 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.600 Prec@1=61.662 Prec@5=83.658 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:25 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.236 Loss=1.600 Prec@1=61.662 Prec@5=83.658 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:26 IST=> training   63.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.667 Prec@5=83.662 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:26 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.667 Prec@5=83.662 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:26 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.667 Prec@5=83.662 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:27 IST=> training   67.96% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.671 Prec@5=83.666 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:27 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.671 Prec@5=83.666 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=07:27 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.235 Loss=1.600 Prec@1=61.671 Prec@5=83.666 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=07:27 IST=> training   71.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.600 Prec@1=61.659 Prec@5=83.666 rate=2.97 Hz, eta=0:03:55, total=0:10:05, wall=07:27 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.600 Prec@1=61.659 Prec@5=83.666 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:27 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.600 Prec@1=61.659 Prec@5=83.666 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:28 IST=> training   75.95% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.601 Prec@1=61.648 Prec@5=83.654 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=07:28 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.601 Prec@1=61.648 Prec@5=83.654 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:28 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.339 DataTime=0.234 Loss=1.601 Prec@1=61.648 Prec@5=83.654 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:28 IST=> training   79.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.601 Prec@1=61.638 Prec@5=83.649 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:28 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.601 Prec@1=61.638 Prec@5=83.649 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:28 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.601 Prec@1=61.638 Prec@5=83.649 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:29 IST=> training   83.94% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.602 Prec@1=61.630 Prec@5=83.641 rate=2.98 Hz, eta=0:02:14, total=0:11:45, wall=07:29 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.602 Prec@1=61.630 Prec@5=83.641 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=07:29 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.602 Prec@1=61.630 Prec@5=83.641 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=07:29 IST=> training   87.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.603 Prec@1=61.613 Prec@5=83.620 rate=2.98 Hz, eta=0:01:41, total=0:12:17, wall=07:29 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.603 Prec@1=61.613 Prec@5=83.620 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=07:29 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.233 Loss=1.603 Prec@1=61.613 Prec@5=83.620 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=07:30 IST=> training   91.93% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.232 Loss=1.603 Prec@1=61.617 Prec@5=83.622 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=07:30 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.232 Loss=1.603 Prec@1=61.617 Prec@5=83.622 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:30 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.338 DataTime=0.232 Loss=1.603 Prec@1=61.617 Prec@5=83.622 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:30 IST=> training   95.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.337 DataTime=0.232 Loss=1.603 Prec@1=61.630 Prec@5=83.621 rate=2.98 Hz, eta=0:00:34, total=0:13:25, wall=07:30 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.337 DataTime=0.232 Loss=1.603 Prec@1=61.630 Prec@5=83.621 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=07:30 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.337 DataTime=0.232 Loss=1.603 Prec@1=61.630 Prec@5=83.621 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=07:30 IST=> training   99.92% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.337 DataTime=0.232 Loss=1.603 Prec@1=61.630 Prec@5=83.622 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=07:30 IST=> training   100.00% of 1x2503...Epoch=131/150 LR=0.00432 Time=0.337 DataTime=0.232 Loss=1.603 Prec@1=61.630 Prec@5=83.622 rate=2.98 Hz, eta=0:00:00, total=0:13:58, wall=07:30 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> validation 0.00% of 1x98...Epoch=131/150 LR=0.00432 Time=6.764 Loss=1.688 Prec@1=59.961 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=6.764 Loss=1.688 Prec@1=59.961 Prec@5=82.227 rate=3740.01 Hz, eta=0:00:00, total=0:00:00, wall=07:31 IST** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=6.764 Loss=1.688 Prec@1=59.961 Prec@5=82.227 rate=3740.01 Hz, eta=0:00:00, total=0:00:00, wall=07:31 IST** validation 1.02% of 1x98...Epoch=131/150 LR=0.00432 Time=0.391 Loss=1.611 Prec@1=61.424 Prec@5=83.656 rate=3740.01 Hz, eta=0:00:00, total=0:00:00, wall=07:31 IST** validation 100.00% of 1x98...Epoch=131/150 LR=0.00432 Time=0.391 Loss=1.611 Prec@1=61.424 Prec@5=83.656 rate=3.11 Hz, eta=0:00:00, total=0:00:31, wall=07:31 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> training   0.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=4.901 DataTime=4.725 Loss=1.614 Prec@1=63.867 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=4.901 DataTime=4.725 Loss=1.614 Prec@1=63.867 Prec@5=83.398 rate=3513.04 Hz, eta=0:00:00, total=0:00:00, wall=07:31 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=4.901 DataTime=4.725 Loss=1.614 Prec@1=63.867 Prec@5=83.398 rate=3513.04 Hz, eta=0:00:00, total=0:00:00, wall=07:32 IST=> training   0.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.375 DataTime=0.278 Loss=1.599 Prec@1=61.755 Prec@5=83.665 rate=3513.04 Hz, eta=0:00:00, total=0:00:00, wall=07:32 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.375 DataTime=0.278 Loss=1.599 Prec@1=61.755 Prec@5=83.665 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=07:32 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.375 DataTime=0.278 Loss=1.599 Prec@1=61.755 Prec@5=83.665 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=07:32 IST=> training   4.04% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.352 DataTime=0.255 Loss=1.585 Prec@1=62.069 Prec@5=83.848 rate=3.06 Hz, eta=0:13:04, total=0:00:33, wall=07:32 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.352 DataTime=0.255 Loss=1.585 Prec@1=62.069 Prec@5=83.848 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=07:32 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.352 DataTime=0.255 Loss=1.585 Prec@1=62.069 Prec@5=83.848 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=07:33 IST=> training   8.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.246 Loss=1.590 Prec@1=61.815 Prec@5=83.799 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=07:33 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.246 Loss=1.590 Prec@1=61.815 Prec@5=83.799 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=07:33 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.246 Loss=1.590 Prec@1=61.815 Prec@5=83.799 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=07:33 IST=> training   12.03% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.342 DataTime=0.243 Loss=1.593 Prec@1=61.767 Prec@5=83.761 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=07:33 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.342 DataTime=0.243 Loss=1.593 Prec@1=61.767 Prec@5=83.761 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=07:33 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.342 DataTime=0.243 Loss=1.593 Prec@1=61.767 Prec@5=83.761 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=07:34 IST=> training   16.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.244 Loss=1.593 Prec@1=61.814 Prec@5=83.766 rate=3.03 Hz, eta=0:11:33, total=0:02:12, wall=07:34 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.244 Loss=1.593 Prec@1=61.814 Prec@5=83.766 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=07:34 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.344 DataTime=0.244 Loss=1.593 Prec@1=61.814 Prec@5=83.766 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=07:35 IST=> training   20.02% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.341 DataTime=0.240 Loss=1.593 Prec@1=61.818 Prec@5=83.768 rate=2.99 Hz, eta=0:11:08, total=0:02:47, wall=07:35 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.341 DataTime=0.240 Loss=1.593 Prec@1=61.818 Prec@5=83.768 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:35 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.341 DataTime=0.240 Loss=1.593 Prec@1=61.818 Prec@5=83.768 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:35 IST=> training   24.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.594 Prec@1=61.808 Prec@5=83.765 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=07:35 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.594 Prec@1=61.808 Prec@5=83.765 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=07:35 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.594 Prec@1=61.808 Prec@5=83.765 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=07:36 IST=> training   28.01% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.835 Prec@5=83.765 rate=3.01 Hz, eta=0:09:57, total=0:03:52, wall=07:36 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.835 Prec@5=83.765 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=07:36 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.835 Prec@5=83.765 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=07:36 IST=> training   32.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.831 Prec@5=83.777 rate=3.00 Hz, eta=0:09:28, total=0:04:27, wall=07:36 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.831 Prec@5=83.777 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=07:36 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.239 Loss=1.593 Prec@1=61.831 Prec@5=83.777 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=07:37 IST=> training   36.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.592 Prec@1=61.844 Prec@5=83.774 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=07:37 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.592 Prec@1=61.844 Prec@5=83.774 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:37 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.238 Loss=1.592 Prec@1=61.844 Prec@5=83.774 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:37 IST=> training   39.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.238 Loss=1.593 Prec@1=61.843 Prec@5=83.756 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=07:37 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.238 Loss=1.593 Prec@1=61.843 Prec@5=83.756 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:37 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.238 Loss=1.593 Prec@1=61.843 Prec@5=83.756 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:38 IST=> training   43.99% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.237 Loss=1.594 Prec@1=61.821 Prec@5=83.733 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=07:38 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.237 Loss=1.594 Prec@1=61.821 Prec@5=83.733 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=07:38 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.237 Loss=1.594 Prec@1=61.821 Prec@5=83.733 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=07:39 IST=> training   47.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.809 Prec@5=83.736 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=07:39 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.809 Prec@5=83.736 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:39 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.809 Prec@5=83.736 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:39 IST=> training   51.98% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.237 Loss=1.594 Prec@1=61.804 Prec@5=83.719 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=07:39 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.237 Loss=1.594 Prec@1=61.804 Prec@5=83.719 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=07:39 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.340 DataTime=0.237 Loss=1.594 Prec@1=61.804 Prec@5=83.719 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=07:40 IST=> training   55.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.803 Prec@5=83.725 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=07:40 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.803 Prec@5=83.725 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=07:40 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.594 Prec@1=61.803 Prec@5=83.725 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=07:40 IST=> training   59.97% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.594 Prec@1=61.804 Prec@5=83.721 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=07:40 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.594 Prec@1=61.804 Prec@5=83.721 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:40 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.594 Prec@1=61.804 Prec@5=83.721 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:41 IST=> training   63.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.784 Prec@5=83.710 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=07:41 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.784 Prec@5=83.710 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:41 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.784 Prec@5=83.710 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:41 IST=> training   67.96% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.776 Prec@5=83.708 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=07:41 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.776 Prec@5=83.708 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=07:41 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.595 Prec@1=61.776 Prec@5=83.708 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=07:42 IST=> training   71.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.234 Loss=1.596 Prec@1=61.753 Prec@5=83.699 rate=2.98 Hz, eta=0:03:55, total=0:10:04, wall=07:42 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.234 Loss=1.596 Prec@1=61.753 Prec@5=83.699 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=07:42 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.234 Loss=1.596 Prec@1=61.753 Prec@5=83.699 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=07:42 IST=> training   75.95% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.596 Prec@1=61.742 Prec@5=83.693 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=07:42 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.596 Prec@1=61.742 Prec@5=83.693 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:42 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.596 Prec@1=61.742 Prec@5=83.693 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:43 IST=> training   79.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.737 Prec@5=83.689 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=07:43 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.737 Prec@5=83.689 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:43 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.737 Prec@5=83.689 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:44 IST=> training   83.94% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.597 Prec@1=61.728 Prec@5=83.684 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=07:44 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.597 Prec@1=61.728 Prec@5=83.684 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:44 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.597 Prec@1=61.728 Prec@5=83.684 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:44 IST=> training   87.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.743 Prec@5=83.698 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=07:44 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.743 Prec@5=83.698 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:44 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.236 Loss=1.596 Prec@1=61.743 Prec@5=83.698 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:45 IST=> training   91.93% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.596 Prec@1=61.746 Prec@5=83.687 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=07:45 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.596 Prec@1=61.746 Prec@5=83.687 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:45 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.339 DataTime=0.235 Loss=1.596 Prec@1=61.746 Prec@5=83.687 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:45 IST=> training   95.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.597 Prec@1=61.735 Prec@5=83.681 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=07:45 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.597 Prec@1=61.735 Prec@5=83.681 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:45 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.235 Loss=1.597 Prec@1=61.735 Prec@5=83.681 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:45 IST=> training   99.92% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.234 Loss=1.597 Prec@1=61.736 Prec@5=83.680 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:45 IST=> training   100.00% of 1x2503...Epoch=132/150 LR=0.00391 Time=0.338 DataTime=0.234 Loss=1.597 Prec@1=61.736 Prec@5=83.680 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=07:45 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST=> validation 0.00% of 1x98...Epoch=132/150 LR=0.00391 Time=6.805 Loss=1.665 Prec@1=61.719 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST=> validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=6.805 Loss=1.665 Prec@1=61.719 Prec@5=82.227 rate=6201.32 Hz, eta=0:00:00, total=0:00:00, wall=07:45 IST** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=6.805 Loss=1.665 Prec@1=61.719 Prec@5=82.227 rate=6201.32 Hz, eta=0:00:00, total=0:00:00, wall=07:46 IST** validation 1.02% of 1x98...Epoch=132/150 LR=0.00391 Time=0.400 Loss=1.614 Prec@1=61.408 Prec@5=83.694 rate=6201.32 Hz, eta=0:00:00, total=0:00:00, wall=07:46 IST** validation 100.00% of 1x98...Epoch=132/150 LR=0.00391 Time=0.400 Loss=1.614 Prec@1=61.408 Prec@5=83.694 rate=3.02 Hz, eta=0:00:00, total=0:00:32, wall=07:46 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:46 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:46 IST=> training   0.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=4.957 DataTime=4.807 Loss=1.543 Prec@1=64.062 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=07:46 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=4.957 DataTime=4.807 Loss=1.543 Prec@1=64.062 Prec@5=84.570 rate=2260.11 Hz, eta=0:00:01, total=0:00:00, wall=07:46 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=4.957 DataTime=4.807 Loss=1.543 Prec@1=64.062 Prec@5=84.570 rate=2260.11 Hz, eta=0:00:01, total=0:00:00, wall=07:47 IST=> training   0.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.375 DataTime=0.272 Loss=1.582 Prec@1=62.227 Prec@5=83.963 rate=2260.11 Hz, eta=0:00:01, total=0:00:00, wall=07:47 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.375 DataTime=0.272 Loss=1.582 Prec@1=62.227 Prec@5=83.963 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=07:47 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.375 DataTime=0.272 Loss=1.582 Prec@1=62.227 Prec@5=83.963 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=07:47 IST=> training   4.04% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.362 DataTime=0.259 Loss=1.589 Prec@1=61.988 Prec@5=83.791 rate=3.07 Hz, eta=0:13:03, total=0:00:32, wall=07:47 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.362 DataTime=0.259 Loss=1.589 Prec@1=61.988 Prec@5=83.791 rate=2.96 Hz, eta=0:12:56, total=0:01:07, wall=07:47 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.362 DataTime=0.259 Loss=1.589 Prec@1=61.988 Prec@5=83.791 rate=2.96 Hz, eta=0:12:56, total=0:01:07, wall=07:48 IST=> training   8.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.355 DataTime=0.252 Loss=1.585 Prec@1=62.134 Prec@5=83.818 rate=2.96 Hz, eta=0:12:56, total=0:01:07, wall=07:48 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.355 DataTime=0.252 Loss=1.585 Prec@1=62.134 Prec@5=83.818 rate=2.96 Hz, eta=0:12:25, total=0:01:41, wall=07:48 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.355 DataTime=0.252 Loss=1.585 Prec@1=62.134 Prec@5=83.818 rate=2.96 Hz, eta=0:12:25, total=0:01:41, wall=07:48 IST=> training   12.03% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.584 Prec@1=62.153 Prec@5=83.865 rate=2.96 Hz, eta=0:12:25, total=0:01:41, wall=07:48 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.584 Prec@1=62.153 Prec@5=83.865 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=07:48 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.584 Prec@1=62.153 Prec@5=83.865 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=07:49 IST=> training   16.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.587 Prec@1=62.065 Prec@5=83.855 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=07:49 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.587 Prec@1=62.065 Prec@5=83.855 rate=2.94 Hz, eta=0:11:21, total=0:02:50, wall=07:49 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.350 DataTime=0.246 Loss=1.587 Prec@1=62.065 Prec@5=83.855 rate=2.94 Hz, eta=0:11:21, total=0:02:50, wall=07:49 IST=> training   20.02% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.591 Prec@1=61.934 Prec@5=83.804 rate=2.94 Hz, eta=0:11:21, total=0:02:50, wall=07:49 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.591 Prec@1=61.934 Prec@5=83.804 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=07:49 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.591 Prec@1=61.934 Prec@5=83.804 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=07:50 IST=> training   24.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.589 Prec@1=61.950 Prec@5=83.833 rate=2.96 Hz, eta=0:10:41, total=0:03:22, wall=07:50 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.589 Prec@1=61.950 Prec@5=83.833 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=07:50 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.346 DataTime=0.242 Loss=1.589 Prec@1=61.950 Prec@5=83.833 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=07:51 IST=> training   28.01% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.242 Loss=1.589 Prec@1=61.959 Prec@5=83.831 rate=2.95 Hz, eta=0:10:10, total=0:03:57, wall=07:51 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.242 Loss=1.589 Prec@1=61.959 Prec@5=83.831 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=07:51 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.242 Loss=1.589 Prec@1=61.959 Prec@5=83.831 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=07:51 IST=> training   32.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.590 Prec@1=61.945 Prec@5=83.797 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=07:51 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.590 Prec@1=61.945 Prec@5=83.797 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=07:51 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.590 Prec@1=61.945 Prec@5=83.797 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=07:52 IST=> training   36.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.240 Loss=1.590 Prec@1=61.960 Prec@5=83.804 rate=2.95 Hz, eta=0:09:03, total=0:05:05, wall=07:52 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.240 Loss=1.590 Prec@1=61.960 Prec@5=83.804 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=07:52 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.240 Loss=1.590 Prec@1=61.960 Prec@5=83.804 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=07:52 IST=> training   39.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.591 Prec@1=61.963 Prec@5=83.788 rate=2.95 Hz, eta=0:08:28, total=0:05:38, wall=07:52 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.591 Prec@1=61.963 Prec@5=83.788 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=07:52 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.345 DataTime=0.241 Loss=1.591 Prec@1=61.963 Prec@5=83.788 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=07:53 IST=> training   43.99% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.592 Prec@1=61.930 Prec@5=83.773 rate=2.94 Hz, eta=0:07:57, total=0:06:14, wall=07:53 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.592 Prec@1=61.930 Prec@5=83.773 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=07:53 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.592 Prec@1=61.930 Prec@5=83.773 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=07:53 IST=> training   47.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.239 Loss=1.592 Prec@1=61.926 Prec@5=83.762 rate=2.94 Hz, eta=0:07:22, total=0:06:48, wall=07:53 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.239 Loss=1.592 Prec@1=61.926 Prec@5=83.762 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=07:53 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.239 Loss=1.592 Prec@1=61.926 Prec@5=83.762 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=07:54 IST=> training   51.98% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.591 Prec@1=61.928 Prec@5=83.762 rate=2.95 Hz, eta=0:06:47, total=0:07:21, wall=07:54 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.591 Prec@1=61.928 Prec@5=83.762 rate=2.94 Hz, eta=0:06:15, total=0:07:57, wall=07:54 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.344 DataTime=0.239 Loss=1.591 Prec@1=61.928 Prec@5=83.762 rate=2.94 Hz, eta=0:06:15, total=0:07:57, wall=07:55 IST=> training   55.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.238 Loss=1.591 Prec@1=61.933 Prec@5=83.758 rate=2.94 Hz, eta=0:06:15, total=0:07:57, wall=07:55 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.238 Loss=1.591 Prec@1=61.933 Prec@5=83.758 rate=2.94 Hz, eta=0:05:40, total=0:08:29, wall=07:55 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.343 DataTime=0.238 Loss=1.591 Prec@1=61.933 Prec@5=83.758 rate=2.94 Hz, eta=0:05:40, total=0:08:29, wall=07:55 IST=> training   59.97% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.938 Prec@5=83.762 rate=2.94 Hz, eta=0:05:40, total=0:08:29, wall=07:55 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.938 Prec@5=83.762 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=07:55 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.938 Prec@5=83.762 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=07:56 IST=> training   63.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.948 Prec@5=83.768 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=07:56 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.948 Prec@5=83.768 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=07:56 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.948 Prec@5=83.768 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=07:56 IST=> training   67.96% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.947 Prec@5=83.767 rate=2.95 Hz, eta=0:04:31, total=0:09:36, wall=07:56 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.947 Prec@5=83.767 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=07:56 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.947 Prec@5=83.767 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=07:57 IST=> training   71.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.933 Prec@5=83.768 rate=2.95 Hz, eta=0:03:57, total=0:10:10, wall=07:57 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.933 Prec@5=83.768 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=07:57 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.933 Prec@5=83.768 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=07:57 IST=> training   75.95% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.939 Prec@5=83.781 rate=2.96 Hz, eta=0:03:23, total=0:10:43, wall=07:57 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.939 Prec@5=83.781 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=07:57 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.590 Prec@1=61.939 Prec@5=83.781 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=07:58 IST=> training   79.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.931 Prec@5=83.778 rate=2.95 Hz, eta=0:02:50, total=0:11:19, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.931 Prec@5=83.778 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.590 Prec@1=61.931 Prec@5=83.778 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=07:58 IST=> training   83.94% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.928 Prec@5=83.774 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=07:58 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.928 Prec@5=83.774 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=07:58 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.928 Prec@5=83.774 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=07:59 IST=> training   87.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.927 Prec@5=83.766 rate=2.95 Hz, eta=0:01:42, total=0:12:26, wall=07:59 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.927 Prec@5=83.766 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=07:59 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.342 DataTime=0.237 Loss=1.591 Prec@1=61.927 Prec@5=83.766 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=08:00 IST=> training   91.93% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.923 Prec@5=83.766 rate=2.95 Hz, eta=0:01:08, total=0:13:01, wall=08:00 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.923 Prec@5=83.766 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:00 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.341 DataTime=0.236 Loss=1.591 Prec@1=61.923 Prec@5=83.766 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:00 IST=> training   95.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.340 DataTime=0.235 Loss=1.591 Prec@1=61.915 Prec@5=83.768 rate=2.95 Hz, eta=0:00:34, total=0:13:34, wall=08:00 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.340 DataTime=0.235 Loss=1.591 Prec@1=61.915 Prec@5=83.768 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=08:00 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.340 DataTime=0.235 Loss=1.591 Prec@1=61.915 Prec@5=83.768 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=08:00 IST=> training   99.92% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.340 DataTime=0.235 Loss=1.592 Prec@1=61.914 Prec@5=83.766 rate=2.95 Hz, eta=0:00:00, total=0:14:06, wall=08:00 IST=> training   100.00% of 1x2503...Epoch=133/150 LR=0.00351 Time=0.340 DataTime=0.235 Loss=1.592 Prec@1=61.914 Prec@5=83.766 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=08:00 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST=> validation 0.00% of 1x98...Epoch=133/150 LR=0.00351 Time=5.865 Loss=1.649 Prec@1=61.133 Prec@5=82.422 rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST=> validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=5.865 Loss=1.649 Prec@1=61.133 Prec@5=82.422 rate=4837.20 Hz, eta=0:00:00, total=0:00:00, wall=08:00 IST** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=5.865 Loss=1.649 Prec@1=61.133 Prec@5=82.422 rate=4837.20 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST** validation 1.02% of 1x98...Epoch=133/150 LR=0.00351 Time=0.398 Loss=1.600 Prec@1=61.690 Prec@5=83.768 rate=4837.20 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST** validation 100.00% of 1x98...Epoch=133/150 LR=0.00351 Time=0.398 Loss=1.600 Prec@1=61.690 Prec@5=83.768 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=08:01 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> training   0.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.070 DataTime=4.909 Loss=1.684 Prec@1=60.352 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.070 DataTime=4.909 Loss=1.684 Prec@1=60.352 Prec@5=84.570 rate=3007.01 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=5.070 DataTime=4.909 Loss=1.684 Prec@1=60.352 Prec@5=84.570 rate=3007.01 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST=> training   0.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.381 DataTime=0.283 Loss=1.590 Prec@1=61.930 Prec@5=83.766 rate=3007.01 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.381 DataTime=0.283 Loss=1.590 Prec@1=61.930 Prec@5=83.766 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=08:01 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.381 DataTime=0.283 Loss=1.590 Prec@1=61.930 Prec@5=83.766 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=08:02 IST=> training   4.04% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.356 DataTime=0.258 Loss=1.579 Prec@1=62.175 Prec@5=83.883 rate=3.03 Hz, eta=0:13:13, total=0:00:33, wall=08:02 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.356 DataTime=0.258 Loss=1.579 Prec@1=62.175 Prec@5=83.883 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=08:02 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.356 DataTime=0.258 Loss=1.579 Prec@1=62.175 Prec@5=83.883 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=08:03 IST=> training   8.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.352 DataTime=0.251 Loss=1.583 Prec@1=62.111 Prec@5=83.786 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=08:03 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.352 DataTime=0.251 Loss=1.583 Prec@1=62.111 Prec@5=83.786 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=08:03 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.352 DataTime=0.251 Loss=1.583 Prec@1=62.111 Prec@5=83.786 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=08:03 IST=> training   12.03% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.350 DataTime=0.248 Loss=1.581 Prec@1=62.127 Prec@5=83.858 rate=2.98 Hz, eta=0:12:18, total=0:01:40, wall=08:03 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.350 DataTime=0.248 Loss=1.581 Prec@1=62.127 Prec@5=83.858 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=08:03 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.350 DataTime=0.248 Loss=1.581 Prec@1=62.127 Prec@5=83.858 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=08:04 IST=> training   16.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.347 DataTime=0.243 Loss=1.579 Prec@1=62.162 Prec@5=83.903 rate=2.96 Hz, eta=0:11:49, total=0:02:15, wall=08:04 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.347 DataTime=0.243 Loss=1.579 Prec@1=62.162 Prec@5=83.903 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:04 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.347 DataTime=0.243 Loss=1.579 Prec@1=62.162 Prec@5=83.903 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:04 IST=> training   20.02% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.239 Loss=1.581 Prec@1=62.115 Prec@5=83.862 rate=2.97 Hz, eta=0:11:14, total=0:02:48, wall=08:04 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.239 Loss=1.581 Prec@1=62.115 Prec@5=83.862 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:04 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.239 Loss=1.581 Prec@1=62.115 Prec@5=83.862 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:05 IST=> training   24.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.238 Loss=1.580 Prec@1=62.141 Prec@5=83.882 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:05 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.238 Loss=1.580 Prec@1=62.141 Prec@5=83.882 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=08:05 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.345 DataTime=0.238 Loss=1.580 Prec@1=62.141 Prec@5=83.882 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=08:05 IST=> training   28.01% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.344 DataTime=0.236 Loss=1.581 Prec@1=62.089 Prec@5=83.863 rate=2.96 Hz, eta=0:10:07, total=0:03:56, wall=08:05 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.344 DataTime=0.236 Loss=1.581 Prec@1=62.089 Prec@5=83.863 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:05 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.344 DataTime=0.236 Loss=1.581 Prec@1=62.089 Prec@5=83.863 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:06 IST=> training   32.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.581 Prec@1=62.052 Prec@5=83.860 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=08:06 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.581 Prec@1=62.052 Prec@5=83.860 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:06 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.581 Prec@1=62.052 Prec@5=83.860 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:07 IST=> training   36.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.580 Prec@1=62.102 Prec@5=83.899 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=08:07 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.580 Prec@1=62.102 Prec@5=83.899 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=08:07 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.235 Loss=1.580 Prec@1=62.102 Prec@5=83.899 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=08:07 IST=> training   39.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.579 Prec@1=62.114 Prec@5=83.921 rate=2.96 Hz, eta=0:08:28, total=0:05:38, wall=08:07 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.579 Prec@1=62.114 Prec@5=83.921 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=08:07 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.579 Prec@1=62.114 Prec@5=83.921 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=08:08 IST=> training   43.99% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.234 Loss=1.580 Prec@1=62.095 Prec@5=83.915 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=08:08 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.234 Loss=1.580 Prec@1=62.095 Prec@5=83.915 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=08:08 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.343 DataTime=0.234 Loss=1.580 Prec@1=62.095 Prec@5=83.915 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=08:08 IST=> training   47.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.581 Prec@1=62.090 Prec@5=83.899 rate=2.96 Hz, eta=0:07:20, total=0:06:46, wall=08:08 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.581 Prec@1=62.090 Prec@5=83.899 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=08:08 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.581 Prec@1=62.090 Prec@5=83.899 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=08:09 IST=> training   51.98% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.580 Prec@1=62.107 Prec@5=83.888 rate=2.96 Hz, eta=0:06:46, total=0:07:20, wall=08:09 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.580 Prec@1=62.107 Prec@5=83.888 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:09 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.234 Loss=1.580 Prec@1=62.107 Prec@5=83.888 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:09 IST=> training   55.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.580 Prec@1=62.118 Prec@5=83.882 rate=2.95 Hz, eta=0:06:13, total=0:07:54, wall=08:09 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.580 Prec@1=62.118 Prec@5=83.882 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:09 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.580 Prec@1=62.118 Prec@5=83.882 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:10 IST=> training   59.97% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.581 Prec@1=62.109 Prec@5=83.876 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=08:10 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.581 Prec@1=62.109 Prec@5=83.876 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=08:10 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.581 Prec@1=62.109 Prec@5=83.876 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=08:11 IST=> training   63.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.107 Prec@5=83.865 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=08:11 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.107 Prec@5=83.865 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:11 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.107 Prec@5=83.865 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:11 IST=> training   67.96% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.085 Prec@5=83.860 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=08:11 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.085 Prec@5=83.860 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:11 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.581 Prec@1=62.085 Prec@5=83.860 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:12 IST=> training   71.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.582 Prec@1=62.083 Prec@5=83.860 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=08:12 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.582 Prec@1=62.083 Prec@5=83.860 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:12 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.342 DataTime=0.233 Loss=1.582 Prec@1=62.083 Prec@5=83.860 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:12 IST=> training   75.95% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.233 Loss=1.582 Prec@1=62.071 Prec@5=83.860 rate=2.95 Hz, eta=0:03:24, total=0:10:44, wall=08:12 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.233 Loss=1.582 Prec@1=62.071 Prec@5=83.860 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=08:12 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.233 Loss=1.582 Prec@1=62.071 Prec@5=83.860 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=08:13 IST=> training   79.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.232 Loss=1.583 Prec@1=62.061 Prec@5=83.850 rate=2.95 Hz, eta=0:02:49, total=0:11:17, wall=08:13 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.232 Loss=1.583 Prec@1=62.061 Prec@5=83.850 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=08:13 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.232 Loss=1.583 Prec@1=62.061 Prec@5=83.850 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=08:13 IST=> training   83.94% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.583 Prec@1=62.080 Prec@5=83.848 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=08:13 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.583 Prec@1=62.080 Prec@5=83.848 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=08:13 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.583 Prec@1=62.080 Prec@5=83.848 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=08:14 IST=> training   87.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.584 Prec@1=62.052 Prec@5=83.833 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=08:14 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.584 Prec@1=62.052 Prec@5=83.833 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=08:14 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.341 DataTime=0.232 Loss=1.584 Prec@1=62.052 Prec@5=83.833 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=08:14 IST=> training   91.93% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.584 Prec@1=62.029 Prec@5=83.828 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=08:14 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.584 Prec@1=62.029 Prec@5=83.828 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=08:14 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.584 Prec@1=62.029 Prec@5=83.828 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=08:15 IST=> training   95.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.585 Prec@1=62.021 Prec@5=83.826 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.585 Prec@1=62.021 Prec@5=83.826 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.231 Loss=1.585 Prec@1=62.021 Prec@5=83.826 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:15 IST=> training   99.92% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.230 Loss=1.585 Prec@1=62.021 Prec@5=83.825 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:15 IST=> training   100.00% of 1x2503...Epoch=134/150 LR=0.00314 Time=0.340 DataTime=0.230 Loss=1.585 Prec@1=62.021 Prec@5=83.825 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=08:15 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 0.00% of 1x98...Epoch=134/150 LR=0.00314 Time=6.654 Loss=1.600 Prec@1=64.258 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=08:15 IST=> validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.654 Loss=1.600 Prec@1=64.258 Prec@5=83.984 rate=7609.07 Hz, eta=0:00:00, total=0:00:00, wall=08:15 IST** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=6.654 Loss=1.600 Prec@1=64.258 Prec@5=83.984 rate=7609.07 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST** validation 1.02% of 1x98...Epoch=134/150 LR=0.00314 Time=0.404 Loss=1.594 Prec@1=61.660 Prec@5=83.958 rate=7609.07 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST** validation 100.00% of 1x98...Epoch=134/150 LR=0.00314 Time=0.404 Loss=1.594 Prec@1=61.660 Prec@5=83.958 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=08:16 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.774 DataTime=4.596 Loss=1.499 Prec@1=63.672 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=08:16 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.774 DataTime=4.596 Loss=1.499 Prec@1=63.672 Prec@5=85.156 rate=4102.04 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=4.774 DataTime=4.596 Loss=1.499 Prec@1=63.672 Prec@5=85.156 rate=4102.04 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST=> training   0.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.375 DataTime=0.273 Loss=1.586 Prec@1=61.906 Prec@5=83.959 rate=4102.04 Hz, eta=0:00:00, total=0:00:00, wall=08:16 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.375 DataTime=0.273 Loss=1.586 Prec@1=61.906 Prec@5=83.959 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=08:16 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.375 DataTime=0.273 Loss=1.586 Prec@1=61.906 Prec@5=83.959 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=08:17 IST=> training   4.04% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.351 DataTime=0.251 Loss=1.582 Prec@1=61.974 Prec@5=83.949 rate=3.05 Hz, eta=0:13:07, total=0:00:33, wall=08:17 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.351 DataTime=0.251 Loss=1.582 Prec@1=61.974 Prec@5=83.949 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=08:17 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.351 DataTime=0.251 Loss=1.582 Prec@1=61.974 Prec@5=83.949 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=08:17 IST=> training   8.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.344 DataTime=0.245 Loss=1.580 Prec@1=62.083 Prec@5=83.986 rate=3.06 Hz, eta=0:12:32, total=0:01:05, wall=08:17 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.344 DataTime=0.245 Loss=1.580 Prec@1=62.083 Prec@5=83.986 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=08:17 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.344 DataTime=0.245 Loss=1.580 Prec@1=62.083 Prec@5=83.986 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=08:18 IST=> training   12.03% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.243 Loss=1.578 Prec@1=62.089 Prec@5=84.042 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=08:18 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.243 Loss=1.578 Prec@1=62.089 Prec@5=84.042 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:18 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.243 Loss=1.578 Prec@1=62.089 Prec@5=84.042 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:19 IST=> training   16.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.242 Loss=1.581 Prec@1=62.101 Prec@5=84.002 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=08:19 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.242 Loss=1.581 Prec@1=62.101 Prec@5=84.002 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=08:19 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.343 DataTime=0.242 Loss=1.581 Prec@1=62.101 Prec@5=84.002 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=08:19 IST=> training   20.02% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.062 Prec@5=83.985 rate=3.00 Hz, eta=0:11:07, total=0:02:46, wall=08:19 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.062 Prec@5=83.985 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=08:19 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.062 Prec@5=83.985 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=08:20 IST=> training   24.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.063 Prec@5=83.987 rate=3.00 Hz, eta=0:10:33, total=0:03:20, wall=08:20 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.063 Prec@5=83.987 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=08:20 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.239 Loss=1.580 Prec@1=62.063 Prec@5=83.987 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=08:20 IST=> training   28.01% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.114 Prec@5=83.989 rate=2.99 Hz, eta=0:10:02, total=0:03:54, wall=08:20 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.114 Prec@5=83.989 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=08:20 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.114 Prec@5=83.989 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=08:21 IST=> training   32.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.235 Loss=1.579 Prec@1=62.076 Prec@5=83.965 rate=2.99 Hz, eta=0:09:29, total=0:04:28, wall=08:21 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.235 Loss=1.579 Prec@1=62.076 Prec@5=83.965 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=08:21 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.235 Loss=1.579 Prec@1=62.076 Prec@5=83.965 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=08:21 IST=> training   36.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.096 Prec@5=83.973 rate=2.99 Hz, eta=0:08:55, total=0:05:00, wall=08:21 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.096 Prec@5=83.973 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=08:21 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.237 Loss=1.579 Prec@1=62.096 Prec@5=83.973 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=08:22 IST=> training   39.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.580 Prec@1=62.096 Prec@5=83.948 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=08:22 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.580 Prec@1=62.096 Prec@5=83.948 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:22 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.580 Prec@1=62.096 Prec@5=83.948 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:22 IST=> training   43.99% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.094 Prec@5=83.961 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:22 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.094 Prec@5=83.961 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=08:22 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.094 Prec@5=83.961 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=08:23 IST=> training   47.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.579 Prec@1=62.083 Prec@5=83.959 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=08:23 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.579 Prec@1=62.083 Prec@5=83.959 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:23 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.236 Loss=1.579 Prec@1=62.083 Prec@5=83.959 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:24 IST=> training   51.98% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.098 Prec@5=83.959 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:24 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.098 Prec@5=83.959 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:24 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.235 Loss=1.579 Prec@1=62.098 Prec@5=83.959 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:24 IST=> training   55.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.580 Prec@1=62.091 Prec@5=83.952 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=08:24 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.580 Prec@1=62.091 Prec@5=83.952 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:24 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.580 Prec@1=62.091 Prec@5=83.952 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:25 IST=> training   59.97% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.235 Loss=1.579 Prec@1=62.103 Prec@5=83.961 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:25 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.235 Loss=1.579 Prec@1=62.103 Prec@5=83.961 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=08:25 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.341 DataTime=0.235 Loss=1.579 Prec@1=62.103 Prec@5=83.961 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=08:25 IST=> training   63.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.579 Prec@1=62.119 Prec@5=83.953 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=08:25 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.579 Prec@1=62.119 Prec@5=83.953 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=08:25 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.234 Loss=1.579 Prec@1=62.119 Prec@5=83.953 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=08:26 IST=> training   67.96% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.579 Prec@1=62.135 Prec@5=83.964 rate=2.97 Hz, eta=0:04:30, total=0:09:33, wall=08:26 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.579 Prec@1=62.135 Prec@5=83.964 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=08:26 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.579 Prec@1=62.135 Prec@5=83.964 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=08:26 IST=> training   71.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.233 Loss=1.580 Prec@1=62.104 Prec@5=83.948 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=08:26 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.233 Loss=1.580 Prec@1=62.104 Prec@5=83.948 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:26 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.340 DataTime=0.233 Loss=1.580 Prec@1=62.104 Prec@5=83.948 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:27 IST=> training   75.95% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.103 Prec@5=83.945 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:27 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.103 Prec@5=83.945 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:27 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.103 Prec@5=83.945 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:28 IST=> training   79.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.091 Prec@5=83.940 rate=2.97 Hz, eta=0:02:49, total=0:11:14, wall=08:28 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.091 Prec@5=83.940 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:28 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.580 Prec@1=62.091 Prec@5=83.940 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:28 IST=> training   83.94% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.092 Prec@5=83.932 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:28 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.092 Prec@5=83.932 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=08:28 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.092 Prec@5=83.932 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=08:29 IST=> training   87.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.094 Prec@5=83.935 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=08:29 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.094 Prec@5=83.935 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=08:29 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.339 DataTime=0.233 Loss=1.581 Prec@1=62.094 Prec@5=83.935 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=08:29 IST=> training   91.93% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.580 Prec@1=62.099 Prec@5=83.934 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=08:29 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.580 Prec@1=62.099 Prec@5=83.934 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=08:29 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.580 Prec@1=62.099 Prec@5=83.934 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=08:30 IST=> training   95.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.581 Prec@1=62.100 Prec@5=83.922 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=08:30 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.581 Prec@1=62.100 Prec@5=83.922 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=08:30 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.232 Loss=1.581 Prec@1=62.100 Prec@5=83.922 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=08:30 IST=> training   99.92% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.231 Loss=1.581 Prec@1=62.102 Prec@5=83.922 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=08:30 IST=> training   100.00% of 1x2503...Epoch=135/150 LR=0.00278 Time=0.338 DataTime=0.231 Loss=1.581 Prec@1=62.102 Prec@5=83.922 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=08:30 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> validation 0.00% of 1x98...Epoch=135/150 LR=0.00278 Time=6.685 Loss=1.719 Prec@1=60.352 Prec@5=80.664 rate=0 Hz, eta=?, total=0:00:00, wall=08:30 IST=> validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=6.685 Loss=1.719 Prec@1=60.352 Prec@5=80.664 rate=4915.58 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=6.685 Loss=1.719 Prec@1=60.352 Prec@5=80.664 rate=4915.58 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST** validation 1.02% of 1x98...Epoch=135/150 LR=0.00278 Time=0.403 Loss=1.590 Prec@1=61.950 Prec@5=84.002 rate=4915.58 Hz, eta=0:00:00, total=0:00:00, wall=08:30 IST** validation 100.00% of 1x98...Epoch=135/150 LR=0.00278 Time=0.403 Loss=1.590 Prec@1=61.950 Prec@5=84.002 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=08:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST=> training   0.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.786 DataTime=5.625 Loss=1.650 Prec@1=58.789 Prec@5=84.375 rate=0 Hz, eta=?, total=0:00:00, wall=08:31 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.786 DataTime=5.625 Loss=1.650 Prec@1=58.789 Prec@5=84.375 rate=4479.16 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=5.786 DataTime=5.625 Loss=1.650 Prec@1=58.789 Prec@5=84.375 rate=4479.16 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST=> training   0.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.373 DataTime=0.272 Loss=1.579 Prec@1=62.338 Prec@5=84.019 rate=4479.16 Hz, eta=0:00:00, total=0:00:00, wall=08:31 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.373 DataTime=0.272 Loss=1.579 Prec@1=62.338 Prec@5=84.019 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=08:31 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.373 DataTime=0.272 Loss=1.579 Prec@1=62.338 Prec@5=84.019 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=08:32 IST=> training   4.04% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.353 DataTime=0.251 Loss=1.574 Prec@1=62.501 Prec@5=84.044 rate=3.17 Hz, eta=0:12:36, total=0:00:31, wall=08:32 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.353 DataTime=0.251 Loss=1.574 Prec@1=62.501 Prec@5=84.044 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=08:32 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.353 DataTime=0.251 Loss=1.574 Prec@1=62.501 Prec@5=84.044 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=08:32 IST=> training   8.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.349 DataTime=0.245 Loss=1.565 Prec@1=62.473 Prec@5=84.156 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=08:32 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.349 DataTime=0.245 Loss=1.565 Prec@1=62.473 Prec@5=84.156 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=08:32 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.349 DataTime=0.245 Loss=1.565 Prec@1=62.473 Prec@5=84.156 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=08:33 IST=> training   12.03% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.351 DataTime=0.245 Loss=1.567 Prec@1=62.447 Prec@5=84.076 rate=3.03 Hz, eta=0:12:05, total=0:01:39, wall=08:33 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.351 DataTime=0.245 Loss=1.567 Prec@1=62.447 Prec@5=84.076 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=08:33 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.351 DataTime=0.245 Loss=1.567 Prec@1=62.447 Prec@5=84.076 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=08:33 IST=> training   16.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.346 DataTime=0.241 Loss=1.565 Prec@1=62.441 Prec@5=84.108 rate=2.97 Hz, eta=0:11:47, total=0:02:14, wall=08:33 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.346 DataTime=0.241 Loss=1.565 Prec@1=62.441 Prec@5=84.108 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=08:33 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.346 DataTime=0.241 Loss=1.565 Prec@1=62.441 Prec@5=84.108 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=08:34 IST=> training   20.02% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.344 DataTime=0.238 Loss=1.569 Prec@1=62.344 Prec@5=84.043 rate=2.99 Hz, eta=0:11:09, total=0:02:47, wall=08:34 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.344 DataTime=0.238 Loss=1.569 Prec@1=62.344 Prec@5=84.043 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=08:34 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.344 DataTime=0.238 Loss=1.569 Prec@1=62.344 Prec@5=84.043 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=08:34 IST=> training   24.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.345 DataTime=0.238 Loss=1.569 Prec@1=62.342 Prec@5=84.064 rate=2.99 Hz, eta=0:10:36, total=0:03:21, wall=08:34 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.345 DataTime=0.238 Loss=1.569 Prec@1=62.342 Prec@5=84.064 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=08:34 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.345 DataTime=0.238 Loss=1.569 Prec@1=62.342 Prec@5=84.064 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=08:35 IST=> training   28.01% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.235 Loss=1.569 Prec@1=62.347 Prec@5=84.058 rate=2.97 Hz, eta=0:10:06, total=0:03:55, wall=08:35 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.235 Loss=1.569 Prec@1=62.347 Prec@5=84.058 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=08:35 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.235 Loss=1.569 Prec@1=62.347 Prec@5=84.058 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=08:36 IST=> training   32.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.234 Loss=1.570 Prec@1=62.313 Prec@5=84.046 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=08:36 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.234 Loss=1.570 Prec@1=62.313 Prec@5=84.046 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=08:36 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.342 DataTime=0.234 Loss=1.570 Prec@1=62.313 Prec@5=84.046 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=08:36 IST=> training   36.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.343 DataTime=0.235 Loss=1.571 Prec@1=62.308 Prec@5=84.039 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=08:36 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.343 DataTime=0.235 Loss=1.571 Prec@1=62.308 Prec@5=84.039 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:36 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.343 DataTime=0.235 Loss=1.571 Prec@1=62.308 Prec@5=84.039 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:37 IST=> training   39.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.312 Prec@5=84.053 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:37 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.312 Prec@5=84.053 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=08:37 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.312 Prec@5=84.053 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=08:37 IST=> training   43.99% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.309 Prec@5=84.053 rate=2.98 Hz, eta=0:07:51, total=0:06:09, wall=08:37 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.309 Prec@5=84.053 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:37 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.571 Prec@1=62.309 Prec@5=84.053 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:38 IST=> training   47.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.572 Prec@1=62.274 Prec@5=84.034 rate=2.97 Hz, eta=0:07:17, total=0:06:43, wall=08:38 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.572 Prec@1=62.274 Prec@5=84.034 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:38 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.572 Prec@1=62.274 Prec@5=84.034 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:38 IST=> training   51.98% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.233 Loss=1.572 Prec@1=62.256 Prec@5=84.022 rate=2.97 Hz, eta=0:06:44, total=0:07:17, wall=08:38 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.233 Loss=1.572 Prec@1=62.256 Prec@5=84.022 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=08:38 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.233 Loss=1.572 Prec@1=62.256 Prec@5=84.022 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=08:39 IST=> training   55.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.027 rate=2.97 Hz, eta=0:06:10, total=0:07:50, wall=08:39 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.027 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=08:39 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.027 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=08:40 IST=> training   59.97% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.573 Prec@1=62.240 Prec@5=84.019 rate=2.98 Hz, eta=0:05:36, total=0:08:24, wall=08:40 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.573 Prec@1=62.240 Prec@5=84.019 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=08:40 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.341 DataTime=0.233 Loss=1.573 Prec@1=62.240 Prec@5=84.019 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=08:40 IST=> training   63.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.230 Prec@5=84.009 rate=2.97 Hz, eta=0:05:04, total=0:08:59, wall=08:40 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.230 Prec@5=84.009 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:40 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.230 Prec@5=84.009 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:41 IST=> training   67.96% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.226 Prec@5=84.007 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:41 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.226 Prec@5=84.007 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=08:41 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.573 Prec@1=62.226 Prec@5=84.007 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=08:41 IST=> training   71.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.574 Prec@1=62.243 Prec@5=83.994 rate=2.97 Hz, eta=0:03:56, total=0:10:05, wall=08:41 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.574 Prec@1=62.243 Prec@5=83.994 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:41 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.574 Prec@1=62.243 Prec@5=83.994 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:42 IST=> training   75.95% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.000 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=08:42 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.000 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=08:42 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.232 Loss=1.573 Prec@1=62.261 Prec@5=84.000 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=08:42 IST=> training   79.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.255 Prec@5=83.989 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=08:42 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.255 Prec@5=83.989 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:42 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.255 Prec@5=83.989 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:43 IST=> training   83.94% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.575 Prec@1=62.239 Prec@5=83.982 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=08:43 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.575 Prec@1=62.239 Prec@5=83.982 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:43 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.232 Loss=1.575 Prec@1=62.239 Prec@5=83.982 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:43 IST=> training   87.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.251 Prec@5=83.990 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:43 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.251 Prec@5=83.990 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:43 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.251 Prec@5=83.990 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:44 IST=> training   91.93% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.231 Loss=1.574 Prec@1=62.244 Prec@5=83.993 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:44 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.231 Loss=1.574 Prec@1=62.244 Prec@5=83.993 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:44 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.340 DataTime=0.231 Loss=1.574 Prec@1=62.244 Prec@5=83.993 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:45 IST=> training   95.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.246 Prec@5=84.003 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:45 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.246 Prec@5=84.003 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:45 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.246 Prec@5=84.003 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:45 IST=> training   99.92% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.247 Prec@5=84.003 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:45 IST=> training   100.00% of 1x2503...Epoch=136/150 LR=0.00245 Time=0.339 DataTime=0.231 Loss=1.574 Prec@1=62.247 Prec@5=84.003 rate=2.97 Hz, eta=0:00:00, total=0:14:02, wall=08:45 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> validation 0.00% of 1x98...Epoch=136/150 LR=0.00245 Time=6.103 Loss=1.581 Prec@1=63.086 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=6.103 Loss=1.581 Prec@1=63.086 Prec@5=83.594 rate=3501.31 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=6.103 Loss=1.581 Prec@1=63.086 Prec@5=83.594 rate=3501.31 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST** validation 1.02% of 1x98...Epoch=136/150 LR=0.00245 Time=0.393 Loss=1.582 Prec@1=61.988 Prec@5=83.972 rate=3501.31 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST** validation 100.00% of 1x98...Epoch=136/150 LR=0.00245 Time=0.393 Loss=1.582 Prec@1=61.988 Prec@5=83.972 rate=3.03 Hz, eta=0:00:00, total=0:00:32, wall=08:45 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> training   0.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=5.007 DataTime=4.765 Loss=1.528 Prec@1=60.742 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=5.007 DataTime=4.765 Loss=1.528 Prec@1=60.742 Prec@5=85.547 rate=3286.46 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=5.007 DataTime=4.765 Loss=1.528 Prec@1=60.742 Prec@5=85.547 rate=3286.46 Hz, eta=0:00:00, total=0:00:00, wall=08:46 IST=> training   0.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.377 DataTime=0.278 Loss=1.561 Prec@1=62.753 Prec@5=84.089 rate=3286.46 Hz, eta=0:00:00, total=0:00:00, wall=08:46 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.377 DataTime=0.278 Loss=1.561 Prec@1=62.753 Prec@5=84.089 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=08:46 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.377 DataTime=0.278 Loss=1.561 Prec@1=62.753 Prec@5=84.089 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=08:46 IST=> training   4.04% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.352 DataTime=0.253 Loss=1.565 Prec@1=62.591 Prec@5=84.070 rate=3.05 Hz, eta=0:13:06, total=0:00:33, wall=08:46 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.352 DataTime=0.253 Loss=1.565 Prec@1=62.591 Prec@5=84.070 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=08:46 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.352 DataTime=0.253 Loss=1.565 Prec@1=62.591 Prec@5=84.070 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=08:47 IST=> training   8.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.350 DataTime=0.249 Loss=1.567 Prec@1=62.538 Prec@5=84.006 rate=3.05 Hz, eta=0:12:34, total=0:01:05, wall=08:47 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.350 DataTime=0.249 Loss=1.567 Prec@1=62.538 Prec@5=84.006 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=08:47 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.350 DataTime=0.249 Loss=1.567 Prec@1=62.538 Prec@5=84.006 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=08:48 IST=> training   12.03% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.347 DataTime=0.246 Loss=1.566 Prec@1=62.496 Prec@5=84.049 rate=3.00 Hz, eta=0:12:13, total=0:01:40, wall=08:48 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.347 DataTime=0.246 Loss=1.566 Prec@1=62.496 Prec@5=84.049 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=08:48 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.347 DataTime=0.246 Loss=1.566 Prec@1=62.496 Prec@5=84.049 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=08:48 IST=> training   16.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.344 DataTime=0.242 Loss=1.566 Prec@1=62.480 Prec@5=84.077 rate=2.99 Hz, eta=0:11:43, total=0:02:14, wall=08:48 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.344 DataTime=0.242 Loss=1.566 Prec@1=62.480 Prec@5=84.077 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:48 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.344 DataTime=0.242 Loss=1.566 Prec@1=62.480 Prec@5=84.077 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:49 IST=> training   20.02% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.345 DataTime=0.243 Loss=1.566 Prec@1=62.439 Prec@5=84.089 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=08:49 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.345 DataTime=0.243 Loss=1.566 Prec@1=62.439 Prec@5=84.089 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:49 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.345 DataTime=0.243 Loss=1.566 Prec@1=62.439 Prec@5=84.089 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:49 IST=> training   24.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.565 Prec@1=62.450 Prec@5=84.104 rate=2.97 Hz, eta=0:10:39, total=0:03:22, wall=08:49 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.565 Prec@1=62.450 Prec@5=84.104 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=08:49 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.565 Prec@1=62.450 Prec@5=84.104 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=08:50 IST=> training   28.01% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.239 Loss=1.565 Prec@1=62.438 Prec@5=84.131 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=08:50 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.239 Loss=1.565 Prec@1=62.438 Prec@5=84.131 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=08:50 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.239 Loss=1.565 Prec@1=62.438 Prec@5=84.131 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=08:50 IST=> training   32.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.567 Prec@1=62.396 Prec@5=84.090 rate=2.97 Hz, eta=0:09:32, total=0:04:29, wall=08:50 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.567 Prec@1=62.396 Prec@5=84.090 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:50 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.343 DataTime=0.240 Loss=1.567 Prec@1=62.396 Prec@5=84.090 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:51 IST=> training   36.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.238 Loss=1.566 Prec@1=62.415 Prec@5=84.102 rate=2.96 Hz, eta=0:09:01, total=0:05:04, wall=08:51 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.238 Loss=1.566 Prec@1=62.415 Prec@5=84.102 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:51 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.238 Loss=1.566 Prec@1=62.415 Prec@5=84.102 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:52 IST=> training   39.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.567 Prec@1=62.398 Prec@5=84.095 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=08:52 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.567 Prec@1=62.398 Prec@5=84.095 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:52 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.567 Prec@1=62.398 Prec@5=84.095 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:52 IST=> training   43.99% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.237 Loss=1.568 Prec@1=62.375 Prec@5=84.086 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=08:52 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.237 Loss=1.568 Prec@1=62.375 Prec@5=84.086 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:52 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.342 DataTime=0.237 Loss=1.568 Prec@1=62.375 Prec@5=84.086 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:53 IST=> training   47.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.569 Prec@1=62.358 Prec@5=84.090 rate=2.96 Hz, eta=0:07:19, total=0:06:45, wall=08:53 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.569 Prec@1=62.358 Prec@5=84.090 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:53 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.341 DataTime=0.237 Loss=1.569 Prec@1=62.358 Prec@5=84.090 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:53 IST=> training   51.98% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.334 Prec@5=84.072 rate=2.96 Hz, eta=0:06:45, total=0:07:18, wall=08:53 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.334 Prec@5=84.072 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=08:53 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.334 Prec@5=84.072 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=08:54 IST=> training   55.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.570 Prec@1=62.337 Prec@5=84.074 rate=2.97 Hz, eta=0:06:11, total=0:07:51, wall=08:54 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.570 Prec@1=62.337 Prec@5=84.074 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:54 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.570 Prec@1=62.337 Prec@5=84.074 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:54 IST=> training   59.97% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.331 Prec@5=84.062 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=08:54 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.331 Prec@5=84.062 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=08:54 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.236 Loss=1.570 Prec@1=62.331 Prec@5=84.062 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=08:55 IST=> training   63.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.335 Prec@5=84.049 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=08:55 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.335 Prec@5=84.049 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:55 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.335 Prec@5=84.049 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:55 IST=> training   67.96% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.571 Prec@1=62.341 Prec@5=84.052 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=08:55 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.571 Prec@1=62.341 Prec@5=84.052 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=08:55 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.340 DataTime=0.235 Loss=1.571 Prec@1=62.341 Prec@5=84.052 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=08:56 IST=> training   71.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.349 Prec@5=84.050 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=08:56 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.349 Prec@5=84.050 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=08:56 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.349 Prec@5=84.050 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=08:57 IST=> training   75.95% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.343 Prec@5=84.048 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=08:57 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.343 Prec@5=84.048 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=08:57 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.234 Loss=1.571 Prec@1=62.343 Prec@5=84.048 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=08:57 IST=> training   79.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.330 Prec@5=84.047 rate=2.97 Hz, eta=0:02:48, total=0:11:12, wall=08:57 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.330 Prec@5=84.047 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=08:57 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.330 Prec@5=84.047 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=08:58 IST=> training   83.94% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.327 Prec@5=84.045 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=08:58 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.327 Prec@5=84.045 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:58 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.327 Prec@5=84.045 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:58 IST=> training   87.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.329 Prec@5=84.039 rate=2.97 Hz, eta=0:01:41, total=0:12:21, wall=08:58 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.329 Prec@5=84.039 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:58 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.329 Prec@5=84.039 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:59 IST=> training   91.93% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.333 Prec@5=84.044 rate=2.97 Hz, eta=0:01:08, total=0:12:55, wall=08:59 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.333 Prec@5=84.044 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:59 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.339 DataTime=0.235 Loss=1.571 Prec@1=62.333 Prec@5=84.044 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:59 IST=> training   95.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.338 DataTime=0.234 Loss=1.571 Prec@1=62.336 Prec@5=84.044 rate=2.97 Hz, eta=0:00:34, total=0:13:29, wall=08:59 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.338 DataTime=0.234 Loss=1.571 Prec@1=62.336 Prec@5=84.044 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=08:59 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.338 DataTime=0.234 Loss=1.571 Prec@1=62.336 Prec@5=84.044 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=08:59 IST=> training   99.92% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.338 DataTime=0.234 Loss=1.571 Prec@1=62.334 Prec@5=84.043 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=08:59 IST=> training   100.00% of 1x2503...Epoch=137/150 LR=0.00213 Time=0.338 DataTime=0.234 Loss=1.571 Prec@1=62.334 Prec@5=84.043 rate=2.97 Hz, eta=0:00:00, total=0:14:01, wall=08:59 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> validation 0.00% of 1x98...Epoch=137/150 LR=0.00213 Time=6.577 Loss=1.563 Prec@1=62.109 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=6.577 Loss=1.563 Prec@1=62.109 Prec@5=83.594 rate=4307.04 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=6.577 Loss=1.563 Prec@1=62.109 Prec@5=83.594 rate=4307.04 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST** validation 1.02% of 1x98...Epoch=137/150 LR=0.00213 Time=0.404 Loss=1.592 Prec@1=61.668 Prec@5=83.986 rate=4307.04 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST** validation 100.00% of 1x98...Epoch=137/150 LR=0.00213 Time=0.404 Loss=1.592 Prec@1=61.668 Prec@5=83.986 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=09:00 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> training   0.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.592 DataTime=5.345 Loss=1.609 Prec@1=60.742 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=09:00 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.592 DataTime=5.345 Loss=1.609 Prec@1=60.742 Prec@5=83.008 rate=4276.52 Hz, eta=0:00:00, total=0:00:00, wall=09:00 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=5.592 DataTime=5.345 Loss=1.609 Prec@1=60.742 Prec@5=83.008 rate=4276.52 Hz, eta=0:00:00, total=0:00:00, wall=09:01 IST=> training   0.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.381 DataTime=0.283 Loss=1.577 Prec@1=62.539 Prec@5=83.837 rate=4276.52 Hz, eta=0:00:00, total=0:00:00, wall=09:01 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.381 DataTime=0.283 Loss=1.577 Prec@1=62.539 Prec@5=83.837 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=09:01 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.381 DataTime=0.283 Loss=1.577 Prec@1=62.539 Prec@5=83.837 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=09:01 IST=> training   4.04% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.357 DataTime=0.256 Loss=1.561 Prec@1=62.739 Prec@5=84.203 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=09:01 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.357 DataTime=0.256 Loss=1.561 Prec@1=62.739 Prec@5=84.203 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=09:01 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.357 DataTime=0.256 Loss=1.561 Prec@1=62.739 Prec@5=84.203 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=09:02 IST=> training   8.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.246 Loss=1.560 Prec@1=62.647 Prec@5=84.222 rate=3.04 Hz, eta=0:12:38, total=0:01:06, wall=09:02 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.246 Loss=1.560 Prec@1=62.647 Prec@5=84.222 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:02 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.246 Loss=1.560 Prec@1=62.647 Prec@5=84.222 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:02 IST=> training   12.03% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.345 DataTime=0.243 Loss=1.565 Prec@1=62.466 Prec@5=84.098 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:02 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.345 DataTime=0.243 Loss=1.565 Prec@1=62.466 Prec@5=84.098 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=09:02 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.345 DataTime=0.243 Loss=1.565 Prec@1=62.466 Prec@5=84.098 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=09:03 IST=> training   16.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.245 Loss=1.565 Prec@1=62.419 Prec@5=84.100 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=09:03 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.245 Loss=1.565 Prec@1=62.419 Prec@5=84.100 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=09:03 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.348 DataTime=0.245 Loss=1.565 Prec@1=62.419 Prec@5=84.100 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=09:04 IST=> training   20.02% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.346 DataTime=0.242 Loss=1.565 Prec@1=62.413 Prec@5=84.077 rate=2.97 Hz, eta=0:11:15, total=0:02:48, wall=09:04 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.346 DataTime=0.242 Loss=1.565 Prec@1=62.413 Prec@5=84.077 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=09:04 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.346 DataTime=0.242 Loss=1.565 Prec@1=62.413 Prec@5=84.077 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=09:04 IST=> training   24.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.239 Loss=1.563 Prec@1=62.457 Prec@5=84.102 rate=2.97 Hz, eta=0:10:41, total=0:03:22, wall=09:04 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.239 Loss=1.563 Prec@1=62.457 Prec@5=84.102 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:04 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.239 Loss=1.563 Prec@1=62.457 Prec@5=84.102 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:05 IST=> training   28.01% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.240 Loss=1.563 Prec@1=62.433 Prec@5=84.105 rate=2.98 Hz, eta=0:10:04, total=0:03:55, wall=09:05 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.240 Loss=1.563 Prec@1=62.433 Prec@5=84.105 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=09:05 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.344 DataTime=0.240 Loss=1.563 Prec@1=62.433 Prec@5=84.105 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=09:05 IST=> training   32.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.239 Loss=1.563 Prec@1=62.438 Prec@5=84.113 rate=2.96 Hz, eta=0:09:34, total=0:04:30, wall=09:05 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.239 Loss=1.563 Prec@1=62.438 Prec@5=84.113 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=09:05 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.239 Loss=1.563 Prec@1=62.438 Prec@5=84.113 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=09:06 IST=> training   36.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.237 Loss=1.564 Prec@1=62.441 Prec@5=84.112 rate=2.97 Hz, eta=0:08:59, total=0:05:03, wall=09:06 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.237 Loss=1.564 Prec@1=62.441 Prec@5=84.112 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=09:06 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.237 Loss=1.564 Prec@1=62.441 Prec@5=84.112 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=09:06 IST=> training   39.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.238 Loss=1.565 Prec@1=62.416 Prec@5=84.090 rate=2.97 Hz, eta=0:08:25, total=0:05:36, wall=09:06 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.238 Loss=1.565 Prec@1=62.416 Prec@5=84.090 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=09:06 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.343 DataTime=0.238 Loss=1.565 Prec@1=62.416 Prec@5=84.090 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=09:07 IST=> training   43.99% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.413 Prec@5=84.084 rate=2.96 Hz, eta=0:07:53, total=0:06:11, wall=09:07 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.413 Prec@5=84.084 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:07 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.413 Prec@5=84.084 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:07 IST=> training   47.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.402 Prec@5=84.079 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:07 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.402 Prec@5=84.079 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=09:07 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.402 Prec@5=84.079 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=09:08 IST=> training   51.98% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.434 Prec@5=84.094 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=09:08 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.434 Prec@5=84.094 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=09:08 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.565 Prec@1=62.434 Prec@5=84.094 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=09:09 IST=> training   55.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.565 Prec@1=62.448 Prec@5=84.094 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=09:09 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.565 Prec@1=62.448 Prec@5=84.094 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:09 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.565 Prec@1=62.448 Prec@5=84.094 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:09 IST=> training   59.97% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.236 Loss=1.565 Prec@1=62.447 Prec@5=84.096 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:09 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.236 Loss=1.565 Prec@1=62.447 Prec@5=84.096 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:09 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.236 Loss=1.565 Prec@1=62.447 Prec@5=84.096 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:10 IST=> training   63.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.566 Prec@1=62.442 Prec@5=84.091 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:10 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.566 Prec@1=62.442 Prec@5=84.091 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:10 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.342 DataTime=0.236 Loss=1.566 Prec@1=62.442 Prec@5=84.091 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:10 IST=> training   67.96% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.437 Prec@5=84.091 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=09:10 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.437 Prec@5=84.091 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:10 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.437 Prec@5=84.091 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:11 IST=> training   71.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.102 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=09:11 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.102 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:11 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.102 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:11 IST=> training   75.95% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.105 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:11 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.105 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:11 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.435 Prec@5=84.105 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:12 IST=> training   79.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.452 Prec@5=84.108 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:12 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.452 Prec@5=84.108 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=09:12 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.452 Prec@5=84.108 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=09:13 IST=> training   83.94% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.566 Prec@1=62.433 Prec@5=84.112 rate=2.96 Hz, eta=0:02:15, total=0:11:50, wall=09:13 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.566 Prec@1=62.433 Prec@5=84.112 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=09:13 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.566 Prec@1=62.433 Prec@5=84.112 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=09:13 IST=> training   87.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.422 Prec@5=84.119 rate=2.96 Hz, eta=0:01:42, total=0:12:23, wall=09:13 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.422 Prec@5=84.119 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:13 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.341 DataTime=0.235 Loss=1.566 Prec@1=62.422 Prec@5=84.119 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:14 IST=> training   91.93% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.565 Prec@1=62.416 Prec@5=84.124 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:14 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.565 Prec@1=62.416 Prec@5=84.124 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:14 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.565 Prec@1=62.416 Prec@5=84.124 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:14 IST=> training   95.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.339 DataTime=0.233 Loss=1.565 Prec@1=62.420 Prec@5=84.119 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:14 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.339 DataTime=0.233 Loss=1.565 Prec@1=62.420 Prec@5=84.119 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=09:14 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.339 DataTime=0.233 Loss=1.565 Prec@1=62.420 Prec@5=84.119 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=09:14 IST=> training   99.92% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.565 Prec@1=62.421 Prec@5=84.120 rate=2.97 Hz, eta=0:00:00, total=0:14:03, wall=09:14 IST=> training   100.00% of 1x2503...Epoch=138/150 LR=0.00184 Time=0.340 DataTime=0.234 Loss=1.565 Prec@1=62.421 Prec@5=84.120 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=09:14 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST=> validation 0.00% of 1x98...Epoch=138/150 LR=0.00184 Time=6.093 Loss=1.533 Prec@1=65.625 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST=> validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=6.093 Loss=1.533 Prec@1=65.625 Prec@5=84.766 rate=6026.49 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=6.093 Loss=1.533 Prec@1=65.625 Prec@5=84.766 rate=6026.49 Hz, eta=0:00:00, total=0:00:00, wall=09:15 IST** validation 1.02% of 1x98...Epoch=138/150 LR=0.00184 Time=0.400 Loss=1.573 Prec@1=62.174 Prec@5=84.242 rate=6026.49 Hz, eta=0:00:00, total=0:00:00, wall=09:15 IST** validation 100.00% of 1x98...Epoch=138/150 LR=0.00184 Time=0.400 Loss=1.573 Prec@1=62.174 Prec@5=84.242 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=09:15 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:15 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:15 IST=> training   0.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.709 DataTime=5.544 Loss=1.662 Prec@1=59.570 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=09:15 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.709 DataTime=5.544 Loss=1.662 Prec@1=59.570 Prec@5=82.617 rate=6884.07 Hz, eta=0:00:00, total=0:00:00, wall=09:15 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=5.709 DataTime=5.544 Loss=1.662 Prec@1=59.570 Prec@5=82.617 rate=6884.07 Hz, eta=0:00:00, total=0:00:00, wall=09:16 IST=> training   0.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.381 DataTime=0.284 Loss=1.553 Prec@1=62.796 Prec@5=84.447 rate=6884.07 Hz, eta=0:00:00, total=0:00:00, wall=09:16 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.381 DataTime=0.284 Loss=1.553 Prec@1=62.796 Prec@5=84.447 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=09:16 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.381 DataTime=0.284 Loss=1.553 Prec@1=62.796 Prec@5=84.447 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=09:16 IST=> training   4.04% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.357 DataTime=0.261 Loss=1.551 Prec@1=62.690 Prec@5=84.328 rate=3.09 Hz, eta=0:12:58, total=0:00:32, wall=09:16 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.357 DataTime=0.261 Loss=1.551 Prec@1=62.690 Prec@5=84.328 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=09:16 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.357 DataTime=0.261 Loss=1.551 Prec@1=62.690 Prec@5=84.328 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=09:17 IST=> training   8.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.350 DataTime=0.253 Loss=1.552 Prec@1=62.680 Prec@5=84.280 rate=3.04 Hz, eta=0:12:37, total=0:01:06, wall=09:17 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.350 DataTime=0.253 Loss=1.552 Prec@1=62.680 Prec@5=84.280 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=09:17 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.350 DataTime=0.253 Loss=1.552 Prec@1=62.680 Prec@5=84.280 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=09:17 IST=> training   12.03% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.347 DataTime=0.249 Loss=1.555 Prec@1=62.613 Prec@5=84.211 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=09:17 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.347 DataTime=0.249 Loss=1.555 Prec@1=62.613 Prec@5=84.211 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:17 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.347 DataTime=0.249 Loss=1.555 Prec@1=62.613 Prec@5=84.211 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:18 IST=> training   16.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.247 Loss=1.555 Prec@1=62.648 Prec@5=84.222 rate=3.00 Hz, eta=0:11:40, total=0:02:13, wall=09:18 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.247 Loss=1.555 Prec@1=62.648 Prec@5=84.222 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:18 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.247 Loss=1.555 Prec@1=62.648 Prec@5=84.222 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:18 IST=> training   20.02% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.246 Loss=1.556 Prec@1=62.613 Prec@5=84.200 rate=3.00 Hz, eta=0:11:08, total=0:02:47, wall=09:18 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.246 Loss=1.556 Prec@1=62.613 Prec@5=84.200 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=09:18 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.345 DataTime=0.246 Loss=1.556 Prec@1=62.613 Prec@5=84.200 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=09:19 IST=> training   24.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.343 DataTime=0.243 Loss=1.555 Prec@1=62.624 Prec@5=84.235 rate=2.98 Hz, eta=0:10:37, total=0:03:21, wall=09:19 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.343 DataTime=0.243 Loss=1.555 Prec@1=62.624 Prec@5=84.235 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=09:19 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.343 DataTime=0.243 Loss=1.555 Prec@1=62.624 Prec@5=84.235 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=09:20 IST=> training   28.01% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.342 DataTime=0.242 Loss=1.557 Prec@1=62.579 Prec@5=84.223 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=09:20 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.342 DataTime=0.242 Loss=1.557 Prec@1=62.579 Prec@5=84.223 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=09:20 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.342 DataTime=0.242 Loss=1.557 Prec@1=62.579 Prec@5=84.223 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=09:20 IST=> training   32.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.240 Loss=1.557 Prec@1=62.569 Prec@5=84.227 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=09:20 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.240 Loss=1.557 Prec@1=62.569 Prec@5=84.227 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=09:20 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.240 Loss=1.557 Prec@1=62.569 Prec@5=84.227 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=09:21 IST=> training   36.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.238 Loss=1.558 Prec@1=62.567 Prec@5=84.224 rate=2.98 Hz, eta=0:08:56, total=0:05:01, wall=09:21 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.238 Loss=1.558 Prec@1=62.567 Prec@5=84.224 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=09:21 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.238 Loss=1.558 Prec@1=62.567 Prec@5=84.224 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=09:21 IST=> training   39.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.239 Loss=1.558 Prec@1=62.554 Prec@5=84.228 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=09:21 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.239 Loss=1.558 Prec@1=62.554 Prec@5=84.228 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=09:21 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.341 DataTime=0.239 Loss=1.558 Prec@1=62.554 Prec@5=84.228 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=09:22 IST=> training   43.99% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.237 Loss=1.559 Prec@1=62.549 Prec@5=84.201 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=09:22 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.237 Loss=1.559 Prec@1=62.549 Prec@5=84.201 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:22 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.340 DataTime=0.237 Loss=1.559 Prec@1=62.549 Prec@5=84.201 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:22 IST=> training   47.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.560 Prec@1=62.537 Prec@5=84.192 rate=2.98 Hz, eta=0:07:16, total=0:06:42, wall=09:22 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.560 Prec@1=62.537 Prec@5=84.192 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=09:22 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.560 Prec@1=62.537 Prec@5=84.192 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=09:23 IST=> training   51.98% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.561 Prec@1=62.505 Prec@5=84.168 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=09:23 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.561 Prec@1=62.505 Prec@5=84.168 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:23 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.561 Prec@1=62.505 Prec@5=84.168 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:23 IST=> training   55.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.497 Prec@5=84.148 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=09:23 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.497 Prec@5=84.148 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=09:23 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.497 Prec@5=84.148 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=09:24 IST=> training   59.97% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.495 Prec@5=84.158 rate=2.99 Hz, eta=0:05:35, total=0:08:22, wall=09:24 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.495 Prec@5=84.158 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=09:24 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.495 Prec@5=84.158 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=09:25 IST=> training   63.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.484 Prec@5=84.151 rate=2.98 Hz, eta=0:05:02, total=0:08:56, wall=09:25 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.484 Prec@5=84.151 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=09:25 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.484 Prec@5=84.151 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=09:25 IST=> training   67.96% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.486 Prec@5=84.157 rate=2.99 Hz, eta=0:04:28, total=0:09:29, wall=09:25 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.486 Prec@5=84.157 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=09:25 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.486 Prec@5=84.157 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=09:26 IST=> training   71.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.562 Prec@1=62.489 Prec@5=84.155 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=09:26 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.562 Prec@1=62.489 Prec@5=84.155 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=09:26 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.562 Prec@1=62.489 Prec@5=84.155 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=09:26 IST=> training   75.95% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.503 Prec@5=84.160 rate=2.98 Hz, eta=0:03:22, total=0:10:37, wall=09:26 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.503 Prec@5=84.160 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=09:26 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.503 Prec@5=84.160 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=09:27 IST=> training   79.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.496 Prec@5=84.150 rate=2.98 Hz, eta=0:02:48, total=0:11:11, wall=09:27 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.496 Prec@5=84.150 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=09:27 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.562 Prec@1=62.496 Prec@5=84.150 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=09:27 IST=> training   83.94% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.563 Prec@1=62.479 Prec@5=84.151 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=09:27 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.563 Prec@1=62.479 Prec@5=84.151 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=09:27 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.236 Loss=1.563 Prec@1=62.479 Prec@5=84.151 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=09:28 IST=> training   87.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.495 Prec@5=84.151 rate=2.98 Hz, eta=0:01:41, total=0:12:19, wall=09:28 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.495 Prec@5=84.151 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=09:28 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.495 Prec@5=84.151 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=09:29 IST=> training   91.93% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.563 Prec@1=62.473 Prec@5=84.141 rate=2.98 Hz, eta=0:01:07, total=0:12:53, wall=09:29 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.563 Prec@1=62.473 Prec@5=84.141 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:29 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.339 DataTime=0.237 Loss=1.563 Prec@1=62.473 Prec@5=84.141 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:29 IST=> training   95.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.462 Prec@5=84.146 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=09:29 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.462 Prec@5=84.146 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=09:29 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.562 Prec@1=62.462 Prec@5=84.146 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=09:29 IST=> training   99.92% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.563 Prec@1=62.461 Prec@5=84.145 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=09:29 IST=> training   100.00% of 1x2503...Epoch=139/150 LR=0.00157 Time=0.338 DataTime=0.236 Loss=1.563 Prec@1=62.461 Prec@5=84.145 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=09:29 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:29 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:29 IST=> validation 0.00% of 1x98...Epoch=139/150 LR=0.00157 Time=5.611 Loss=1.544 Prec@1=64.062 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=09:29 IST=> validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=5.611 Loss=1.544 Prec@1=64.062 Prec@5=83.984 rate=7074.29 Hz, eta=0:00:00, total=0:00:00, wall=09:29 IST** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=5.611 Loss=1.544 Prec@1=64.062 Prec@5=83.984 rate=7074.29 Hz, eta=0:00:00, total=0:00:00, wall=09:30 IST** validation 1.02% of 1x98...Epoch=139/150 LR=0.00157 Time=0.396 Loss=1.568 Prec@1=62.242 Prec@5=84.348 rate=7074.29 Hz, eta=0:00:00, total=0:00:00, wall=09:30 IST** validation 100.00% of 1x98...Epoch=139/150 LR=0.00157 Time=0.396 Loss=1.568 Prec@1=62.242 Prec@5=84.348 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=09:30 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:30 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:30 IST=> training   0.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.939 DataTime=4.565 Loss=1.387 Prec@1=66.016 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=09:30 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.939 DataTime=4.565 Loss=1.387 Prec@1=66.016 Prec@5=85.547 rate=5973.43 Hz, eta=0:00:00, total=0:00:00, wall=09:30 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=4.939 DataTime=4.565 Loss=1.387 Prec@1=66.016 Prec@5=85.547 rate=5973.43 Hz, eta=0:00:00, total=0:00:00, wall=09:30 IST=> training   0.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.382 DataTime=0.283 Loss=1.566 Prec@1=62.655 Prec@5=84.073 rate=5973.43 Hz, eta=0:00:00, total=0:00:00, wall=09:30 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.382 DataTime=0.283 Loss=1.566 Prec@1=62.655 Prec@5=84.073 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=09:30 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.382 DataTime=0.283 Loss=1.566 Prec@1=62.655 Prec@5=84.073 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=09:31 IST=> training   4.04% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.354 DataTime=0.256 Loss=1.563 Prec@1=62.482 Prec@5=84.155 rate=3.00 Hz, eta=0:13:20, total=0:00:33, wall=09:31 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.354 DataTime=0.256 Loss=1.563 Prec@1=62.482 Prec@5=84.155 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=09:31 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.354 DataTime=0.256 Loss=1.563 Prec@1=62.482 Prec@5=84.155 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=09:31 IST=> training   8.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.350 DataTime=0.253 Loss=1.561 Prec@1=62.495 Prec@5=84.181 rate=3.03 Hz, eta=0:12:39, total=0:01:06, wall=09:31 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.350 DataTime=0.253 Loss=1.561 Prec@1=62.495 Prec@5=84.181 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=09:31 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.350 DataTime=0.253 Loss=1.561 Prec@1=62.495 Prec@5=84.181 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=09:32 IST=> training   12.03% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.345 DataTime=0.246 Loss=1.560 Prec@1=62.524 Prec@5=84.160 rate=3.00 Hz, eta=0:12:14, total=0:01:40, wall=09:32 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.345 DataTime=0.246 Loss=1.560 Prec@1=62.524 Prec@5=84.160 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=09:32 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.345 DataTime=0.246 Loss=1.560 Prec@1=62.524 Prec@5=84.160 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=09:33 IST=> training   16.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.343 DataTime=0.243 Loss=1.557 Prec@1=62.587 Prec@5=84.219 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=09:33 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.343 DataTime=0.243 Loss=1.557 Prec@1=62.587 Prec@5=84.219 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=09:33 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.343 DataTime=0.243 Loss=1.557 Prec@1=62.587 Prec@5=84.219 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=09:33 IST=> training   20.02% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.341 DataTime=0.239 Loss=1.556 Prec@1=62.558 Prec@5=84.226 rate=3.00 Hz, eta=0:11:06, total=0:02:46, wall=09:33 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.341 DataTime=0.239 Loss=1.556 Prec@1=62.558 Prec@5=84.226 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=09:33 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.341 DataTime=0.239 Loss=1.556 Prec@1=62.558 Prec@5=84.226 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=09:34 IST=> training   24.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.340 DataTime=0.238 Loss=1.556 Prec@1=62.598 Prec@5=84.235 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=09:34 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.340 DataTime=0.238 Loss=1.556 Prec@1=62.598 Prec@5=84.235 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:34 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.340 DataTime=0.238 Loss=1.556 Prec@1=62.598 Prec@5=84.235 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:34 IST=> training   28.01% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.339 DataTime=0.236 Loss=1.556 Prec@1=62.591 Prec@5=84.235 rate=3.00 Hz, eta=0:10:00, total=0:03:53, wall=09:34 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.339 DataTime=0.236 Loss=1.556 Prec@1=62.591 Prec@5=84.235 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=09:34 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.339 DataTime=0.236 Loss=1.556 Prec@1=62.591 Prec@5=84.235 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=09:35 IST=> training   32.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.235 Loss=1.556 Prec@1=62.613 Prec@5=84.246 rate=3.01 Hz, eta=0:09:25, total=0:04:26, wall=09:35 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.235 Loss=1.556 Prec@1=62.613 Prec@5=84.246 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=09:35 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.235 Loss=1.556 Prec@1=62.613 Prec@5=84.246 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=09:35 IST=> training   36.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.236 Loss=1.556 Prec@1=62.612 Prec@5=84.244 rate=3.01 Hz, eta=0:08:52, total=0:04:59, wall=09:35 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.236 Loss=1.556 Prec@1=62.612 Prec@5=84.244 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=09:35 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.338 DataTime=0.236 Loss=1.556 Prec@1=62.612 Prec@5=84.244 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=09:36 IST=> training   39.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.236 Loss=1.557 Prec@1=62.582 Prec@5=84.228 rate=3.00 Hz, eta=0:08:19, total=0:05:33, wall=09:36 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.236 Loss=1.557 Prec@1=62.582 Prec@5=84.228 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=09:36 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.236 Loss=1.557 Prec@1=62.582 Prec@5=84.228 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=09:36 IST=> training   43.99% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.235 Loss=1.557 Prec@1=62.570 Prec@5=84.219 rate=3.01 Hz, eta=0:07:46, total=0:06:06, wall=09:36 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.235 Loss=1.557 Prec@1=62.570 Prec@5=84.219 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=09:36 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.235 Loss=1.557 Prec@1=62.570 Prec@5=84.219 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=09:37 IST=> training   47.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.557 Prec@1=62.572 Prec@5=84.221 rate=3.01 Hz, eta=0:07:12, total=0:06:39, wall=09:37 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.557 Prec@1=62.572 Prec@5=84.221 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=09:37 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.557 Prec@1=62.572 Prec@5=84.221 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=09:38 IST=> training   51.98% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.558 Prec@1=62.574 Prec@5=84.217 rate=3.00 Hz, eta=0:06:40, total=0:07:13, wall=09:38 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.558 Prec@1=62.574 Prec@5=84.217 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=09:38 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.235 Loss=1.558 Prec@1=62.574 Prec@5=84.217 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=09:38 IST=> training   55.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.594 Prec@5=84.207 rate=3.00 Hz, eta=0:06:06, total=0:07:46, wall=09:38 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.594 Prec@5=84.207 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=09:38 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.594 Prec@5=84.207 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=09:39 IST=> training   59.97% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.573 Prec@5=84.206 rate=3.00 Hz, eta=0:05:33, total=0:08:19, wall=09:39 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.573 Prec@5=84.206 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=09:39 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.573 Prec@5=84.206 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=09:39 IST=> training   63.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.204 rate=3.00 Hz, eta=0:05:00, total=0:08:53, wall=09:39 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.204 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=09:39 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.204 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=09:40 IST=> training   67.96% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.578 Prec@5=84.203 rate=3.00 Hz, eta=0:04:27, total=0:09:27, wall=09:40 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.578 Prec@5=84.203 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=09:40 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.234 Loss=1.558 Prec@1=62.578 Prec@5=84.203 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=09:40 IST=> training   71.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=3.00 Hz, eta=0:03:54, total=0:10:00, wall=09:40 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=2.99 Hz, eta=0:03:21, total=0:10:34, wall=09:40 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.337 DataTime=0.234 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=2.99 Hz, eta=0:03:21, total=0:10:34, wall=09:41 IST=> training   75.95% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=2.99 Hz, eta=0:03:21, total=0:10:34, wall=09:41 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=09:41 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.558 Prec@1=62.572 Prec@5=84.205 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=09:41 IST=> training   79.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.597 Prec@5=84.221 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=09:41 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.597 Prec@5=84.221 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=09:41 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.597 Prec@5=84.221 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=09:42 IST=> training   83.94% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.227 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=09:42 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.227 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=09:42 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.336 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.227 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=09:43 IST=> training   87.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.219 rate=3.00 Hz, eta=0:01:40, total=0:12:14, wall=09:43 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.219 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=09:43 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.605 Prec@5=84.219 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=09:43 IST=> training   91.93% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.607 Prec@5=84.220 rate=3.00 Hz, eta=0:01:07, total=0:12:46, wall=09:43 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.607 Prec@5=84.220 rate=3.00 Hz, eta=0:00:33, total=0:13:20, wall=09:43 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.607 Prec@5=84.220 rate=3.00 Hz, eta=0:00:33, total=0:13:20, wall=09:44 IST=> training   95.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.618 Prec@5=84.227 rate=3.00 Hz, eta=0:00:33, total=0:13:20, wall=09:44 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.618 Prec@5=84.227 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=09:44 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.233 Loss=1.557 Prec@1=62.618 Prec@5=84.227 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=09:44 IST=> training   99.92% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.232 Loss=1.557 Prec@1=62.620 Prec@5=84.227 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=09:44 IST=> training   100.00% of 1x2503...Epoch=140/150 LR=0.00132 Time=0.335 DataTime=0.232 Loss=1.557 Prec@1=62.620 Prec@5=84.227 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=09:44 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> validation 0.00% of 1x98...Epoch=140/150 LR=0.00132 Time=6.816 Loss=1.484 Prec@1=64.453 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=09:44 IST=> validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.816 Loss=1.484 Prec@1=64.453 Prec@5=84.766 rate=2632.68 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=6.816 Loss=1.484 Prec@1=64.453 Prec@5=84.766 rate=2632.68 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST** validation 1.02% of 1x98...Epoch=140/150 LR=0.00132 Time=0.405 Loss=1.561 Prec@1=62.456 Prec@5=84.354 rate=2632.68 Hz, eta=0:00:00, total=0:00:00, wall=09:44 IST** validation 100.00% of 1x98...Epoch=140/150 LR=0.00132 Time=0.405 Loss=1.561 Prec@1=62.456 Prec@5=84.354 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=09:44 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:45 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:45 IST=> training   0.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=5.759 DataTime=5.598 Loss=1.505 Prec@1=65.234 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=09:45 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=5.759 DataTime=5.598 Loss=1.505 Prec@1=65.234 Prec@5=84.961 rate=6500.64 Hz, eta=0:00:00, total=0:00:00, wall=09:45 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=5.759 DataTime=5.598 Loss=1.505 Prec@1=65.234 Prec@5=84.961 rate=6500.64 Hz, eta=0:00:00, total=0:00:00, wall=09:45 IST=> training   0.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.386 DataTime=0.293 Loss=1.561 Prec@1=62.635 Prec@5=84.251 rate=6500.64 Hz, eta=0:00:00, total=0:00:00, wall=09:45 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.386 DataTime=0.293 Loss=1.561 Prec@1=62.635 Prec@5=84.251 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=09:45 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.386 DataTime=0.293 Loss=1.561 Prec@1=62.635 Prec@5=84.251 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=09:46 IST=> training   4.04% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.353 DataTime=0.258 Loss=1.559 Prec@1=62.578 Prec@5=84.273 rate=3.04 Hz, eta=0:13:10, total=0:00:33, wall=09:46 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.353 DataTime=0.258 Loss=1.559 Prec@1=62.578 Prec@5=84.273 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=09:46 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.353 DataTime=0.258 Loss=1.559 Prec@1=62.578 Prec@5=84.273 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=09:46 IST=> training   8.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.349 DataTime=0.252 Loss=1.556 Prec@1=62.708 Prec@5=84.296 rate=3.08 Hz, eta=0:12:27, total=0:01:05, wall=09:46 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.349 DataTime=0.252 Loss=1.556 Prec@1=62.708 Prec@5=84.296 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:46 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.349 DataTime=0.252 Loss=1.556 Prec@1=62.708 Prec@5=84.296 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:47 IST=> training   12.03% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.348 DataTime=0.250 Loss=1.557 Prec@1=62.694 Prec@5=84.243 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=09:47 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.348 DataTime=0.250 Loss=1.557 Prec@1=62.694 Prec@5=84.243 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=09:47 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.348 DataTime=0.250 Loss=1.557 Prec@1=62.694 Prec@5=84.243 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=09:47 IST=> training   16.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.245 Loss=1.555 Prec@1=62.699 Prec@5=84.219 rate=2.99 Hz, eta=0:11:42, total=0:02:13, wall=09:47 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.245 Loss=1.555 Prec@1=62.699 Prec@5=84.219 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=09:47 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.245 Loss=1.555 Prec@1=62.699 Prec@5=84.219 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=09:48 IST=> training   20.02% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.344 DataTime=0.243 Loss=1.555 Prec@1=62.730 Prec@5=84.242 rate=3.00 Hz, eta=0:11:07, total=0:02:47, wall=09:48 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.344 DataTime=0.243 Loss=1.555 Prec@1=62.730 Prec@5=84.242 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:48 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.344 DataTime=0.243 Loss=1.555 Prec@1=62.730 Prec@5=84.242 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:48 IST=> training   24.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.243 Loss=1.554 Prec@1=62.701 Prec@5=84.259 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=09:48 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.243 Loss=1.554 Prec@1=62.701 Prec@5=84.259 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=09:48 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.345 DataTime=0.243 Loss=1.554 Prec@1=62.701 Prec@5=84.259 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=09:49 IST=> training   28.01% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.553 Prec@1=62.730 Prec@5=84.264 rate=2.97 Hz, eta=0:10:05, total=0:03:55, wall=09:49 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.553 Prec@1=62.730 Prec@5=84.264 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:49 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.553 Prec@1=62.730 Prec@5=84.264 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:50 IST=> training   32.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.551 Prec@1=62.735 Prec@5=84.303 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=09:50 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.551 Prec@1=62.735 Prec@5=84.303 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=09:50 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.343 DataTime=0.240 Loss=1.551 Prec@1=62.735 Prec@5=84.303 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=09:50 IST=> training   36.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.239 Loss=1.551 Prec@1=62.752 Prec@5=84.303 rate=2.97 Hz, eta=0:08:58, total=0:05:03, wall=09:50 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.239 Loss=1.551 Prec@1=62.752 Prec@5=84.303 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=09:50 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.239 Loss=1.551 Prec@1=62.752 Prec@5=84.303 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=09:51 IST=> training   39.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.238 Loss=1.552 Prec@1=62.733 Prec@5=84.299 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=09:51 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.238 Loss=1.552 Prec@1=62.733 Prec@5=84.299 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:51 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.238 Loss=1.552 Prec@1=62.733 Prec@5=84.299 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:51 IST=> training   43.99% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.718 Prec@5=84.259 rate=2.97 Hz, eta=0:07:52, total=0:06:11, wall=09:51 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.718 Prec@5=84.259 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:51 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.718 Prec@5=84.259 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:52 IST=> training   47.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.719 Prec@5=84.260 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=09:52 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.719 Prec@5=84.260 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:52 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.719 Prec@5=84.260 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:52 IST=> training   51.98% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.713 Prec@5=84.248 rate=2.96 Hz, eta=0:06:45, total=0:07:19, wall=09:52 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.713 Prec@5=84.248 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=09:52 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.713 Prec@5=84.248 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=09:53 IST=> training   55.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.703 Prec@5=84.238 rate=2.96 Hz, eta=0:06:11, total=0:07:52, wall=09:53 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.703 Prec@5=84.238 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:53 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.342 DataTime=0.237 Loss=1.554 Prec@1=62.703 Prec@5=84.238 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:54 IST=> training   59.97% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.675 Prec@5=84.236 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=09:54 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.675 Prec@5=84.236 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:54 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.675 Prec@5=84.236 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:54 IST=> training   63.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.666 Prec@5=84.239 rate=2.96 Hz, eta=0:05:04, total=0:09:00, wall=09:54 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.666 Prec@5=84.239 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=09:54 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.236 Loss=1.554 Prec@1=62.666 Prec@5=84.239 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=09:55 IST=> training   67.96% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.235 Loss=1.554 Prec@1=62.685 Prec@5=84.237 rate=2.96 Hz, eta=0:04:30, total=0:09:33, wall=09:55 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.235 Loss=1.554 Prec@1=62.685 Prec@5=84.237 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=09:55 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.235 Loss=1.554 Prec@1=62.685 Prec@5=84.237 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=09:55 IST=> training   71.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.554 Prec@1=62.665 Prec@5=84.249 rate=2.97 Hz, eta=0:03:56, total=0:10:07, wall=09:55 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.554 Prec@1=62.665 Prec@5=84.249 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:55 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.554 Prec@1=62.665 Prec@5=84.249 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:56 IST=> training   75.95% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.556 Prec@1=62.658 Prec@5=84.234 rate=2.96 Hz, eta=0:03:23, total=0:10:41, wall=09:56 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.556 Prec@1=62.658 Prec@5=84.234 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:56 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.556 Prec@1=62.658 Prec@5=84.234 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:56 IST=> training   79.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.657 Prec@5=84.248 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=09:56 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.657 Prec@5=84.248 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=09:56 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.657 Prec@5=84.248 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=09:57 IST=> training   83.94% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.656 Prec@5=84.247 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=09:57 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.656 Prec@5=84.247 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:57 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.656 Prec@5=84.247 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:57 IST=> training   87.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.636 Prec@5=84.249 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=09:57 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.636 Prec@5=84.249 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:57 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.341 DataTime=0.235 Loss=1.555 Prec@1=62.636 Prec@5=84.249 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:58 IST=> training   91.93% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.640 Prec@5=84.252 rate=2.96 Hz, eta=0:01:08, total=0:12:57, wall=09:58 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.640 Prec@5=84.252 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:58 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.640 Prec@5=84.252 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:59 IST=> training   95.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.637 Prec@5=84.248 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=09:59 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.637 Prec@5=84.248 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=09:59 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.637 Prec@5=84.248 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=09:59 IST=> training   99.92% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.637 Prec@5=84.247 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=09:59 IST=> training   100.00% of 1x2503...Epoch=141/150 LR=0.00109 Time=0.340 DataTime=0.234 Loss=1.555 Prec@1=62.637 Prec@5=84.247 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=09:59 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> validation 0.00% of 1x98...Epoch=141/150 LR=0.00109 Time=6.754 Loss=1.597 Prec@1=60.547 Prec@5=83.984 rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=6.754 Loss=1.597 Prec@1=60.547 Prec@5=83.984 rate=5728.59 Hz, eta=0:00:00, total=0:00:00, wall=09:59 IST** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=6.754 Loss=1.597 Prec@1=60.547 Prec@5=83.984 rate=5728.59 Hz, eta=0:00:00, total=0:00:00, wall=09:59 IST** validation 1.02% of 1x98...Epoch=141/150 LR=0.00109 Time=0.395 Loss=1.560 Prec@1=62.474 Prec@5=84.414 rate=5728.59 Hz, eta=0:00:00, total=0:00:00, wall=09:59 IST** validation 100.00% of 1x98...Epoch=141/150 LR=0.00109 Time=0.395 Loss=1.560 Prec@1=62.474 Prec@5=84.414 rate=3.06 Hz, eta=0:00:00, total=0:00:31, wall=09:59 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> training   0.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.239 DataTime=5.088 Loss=1.522 Prec@1=63.477 Prec@5=84.180 rate=0 Hz, eta=?, total=0:00:00, wall=09:59 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.239 DataTime=5.088 Loss=1.522 Prec@1=63.477 Prec@5=84.180 rate=3565.48 Hz, eta=0:00:00, total=0:00:00, wall=09:59 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=5.239 DataTime=5.088 Loss=1.522 Prec@1=63.477 Prec@5=84.180 rate=3565.48 Hz, eta=0:00:00, total=0:00:00, wall=10:00 IST=> training   0.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.371 DataTime=0.274 Loss=1.525 Prec@1=63.210 Prec@5=84.845 rate=3565.48 Hz, eta=0:00:00, total=0:00:00, wall=10:00 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.371 DataTime=0.274 Loss=1.525 Prec@1=63.210 Prec@5=84.845 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=10:00 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.371 DataTime=0.274 Loss=1.525 Prec@1=63.210 Prec@5=84.845 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=10:00 IST=> training   4.04% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.348 DataTime=0.248 Loss=1.538 Prec@1=63.045 Prec@5=84.639 rate=3.13 Hz, eta=0:12:46, total=0:00:32, wall=10:00 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.348 DataTime=0.248 Loss=1.538 Prec@1=63.045 Prec@5=84.639 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=10:00 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.348 DataTime=0.248 Loss=1.538 Prec@1=63.045 Prec@5=84.639 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=10:01 IST=> training   8.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.347 DataTime=0.247 Loss=1.541 Prec@1=62.990 Prec@5=84.513 rate=3.11 Hz, eta=0:12:20, total=0:01:04, wall=10:01 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.347 DataTime=0.247 Loss=1.541 Prec@1=62.990 Prec@5=84.513 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:01 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.347 DataTime=0.247 Loss=1.541 Prec@1=62.990 Prec@5=84.513 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:02 IST=> training   12.03% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.345 DataTime=0.245 Loss=1.544 Prec@1=62.939 Prec@5=84.458 rate=3.04 Hz, eta=0:12:04, total=0:01:39, wall=10:02 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.345 DataTime=0.245 Loss=1.544 Prec@1=62.939 Prec@5=84.458 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:02 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.345 DataTime=0.245 Loss=1.544 Prec@1=62.939 Prec@5=84.458 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:02 IST=> training   16.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.342 DataTime=0.242 Loss=1.544 Prec@1=62.918 Prec@5=84.425 rate=3.01 Hz, eta=0:11:37, total=0:02:13, wall=10:02 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.342 DataTime=0.242 Loss=1.544 Prec@1=62.918 Prec@5=84.425 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=10:02 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.342 DataTime=0.242 Loss=1.544 Prec@1=62.918 Prec@5=84.425 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=10:03 IST=> training   20.02% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.344 DataTime=0.243 Loss=1.546 Prec@1=62.870 Prec@5=84.366 rate=3.02 Hz, eta=0:11:03, total=0:02:46, wall=10:03 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.344 DataTime=0.243 Loss=1.546 Prec@1=62.870 Prec@5=84.366 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:03 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.344 DataTime=0.243 Loss=1.546 Prec@1=62.870 Prec@5=84.366 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:03 IST=> training   24.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.341 DataTime=0.239 Loss=1.549 Prec@1=62.820 Prec@5=84.322 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=10:03 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.341 DataTime=0.239 Loss=1.549 Prec@1=62.820 Prec@5=84.322 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=10:03 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.341 DataTime=0.239 Loss=1.549 Prec@1=62.820 Prec@5=84.322 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=10:04 IST=> training   28.01% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.812 Prec@5=84.327 rate=3.00 Hz, eta=0:10:01, total=0:03:53, wall=10:04 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.812 Prec@5=84.327 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=10:04 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.812 Prec@5=84.327 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=10:04 IST=> training   32.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.784 Prec@5=84.340 rate=3.00 Hz, eta=0:09:27, total=0:04:27, wall=10:04 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.784 Prec@5=84.340 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:04 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.784 Prec@5=84.340 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:05 IST=> training   36.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.806 Prec@5=84.346 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:05 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.806 Prec@5=84.346 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=10:05 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.806 Prec@5=84.346 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=10:05 IST=> training   39.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.792 Prec@5=84.340 rate=3.00 Hz, eta=0:08:21, total=0:05:34, wall=10:05 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.792 Prec@5=84.340 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=10:05 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.792 Prec@5=84.340 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=10:06 IST=> training   43.99% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.780 Prec@5=84.324 rate=3.00 Hz, eta=0:07:47, total=0:06:07, wall=10:06 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.780 Prec@5=84.324 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=10:06 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.780 Prec@5=84.324 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=10:07 IST=> training   47.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.745 Prec@5=84.302 rate=2.99 Hz, eta=0:07:16, total=0:06:42, wall=10:07 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.745 Prec@5=84.302 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=10:07 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.745 Prec@5=84.302 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=10:07 IST=> training   51.98% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.550 Prec@1=62.745 Prec@5=84.313 rate=2.99 Hz, eta=0:06:42, total=0:07:15, wall=10:07 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.550 Prec@1=62.745 Prec@5=84.313 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=10:07 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.550 Prec@1=62.745 Prec@5=84.313 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=10:08 IST=> training   55.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.236 Loss=1.550 Prec@1=62.741 Prec@5=84.324 rate=2.99 Hz, eta=0:06:08, total=0:07:48, wall=10:08 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.236 Loss=1.550 Prec@1=62.741 Prec@5=84.324 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=10:08 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.340 DataTime=0.236 Loss=1.550 Prec@1=62.741 Prec@5=84.324 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=10:08 IST=> training   59.97% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.719 Prec@5=84.328 rate=2.97 Hz, eta=0:05:36, total=0:08:24, wall=10:08 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.719 Prec@5=84.328 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=10:08 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.719 Prec@5=84.328 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=10:09 IST=> training   63.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.234 Loss=1.551 Prec@1=62.703 Prec@5=84.317 rate=2.98 Hz, eta=0:05:02, total=0:08:57, wall=10:09 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.234 Loss=1.551 Prec@1=62.703 Prec@5=84.317 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:09 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.234 Loss=1.551 Prec@1=62.703 Prec@5=84.317 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:09 IST=> training   67.96% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.701 Prec@5=84.320 rate=2.98 Hz, eta=0:04:29, total=0:09:30, wall=10:09 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.701 Prec@5=84.320 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:09 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.701 Prec@5=84.320 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:10 IST=> training   71.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.552 Prec@1=62.679 Prec@5=84.313 rate=2.98 Hz, eta=0:03:55, total=0:10:05, wall=10:10 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.552 Prec@1=62.679 Prec@5=84.313 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:10 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.552 Prec@1=62.679 Prec@5=84.313 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:11 IST=> training   75.95% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.552 Prec@1=62.682 Prec@5=84.312 rate=2.97 Hz, eta=0:03:22, total=0:10:39, wall=10:11 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.552 Prec@1=62.682 Prec@5=84.312 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=10:11 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.552 Prec@1=62.682 Prec@5=84.312 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=10:11 IST=> training   79.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.682 Prec@5=84.318 rate=2.98 Hz, eta=0:02:48, total=0:11:12, wall=10:11 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.682 Prec@5=84.318 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:11 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.682 Prec@5=84.318 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:12 IST=> training   83.94% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.693 Prec@5=84.322 rate=2.97 Hz, eta=0:02:15, total=0:11:47, wall=10:12 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.693 Prec@5=84.322 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:12 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.693 Prec@5=84.322 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:12 IST=> training   87.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.551 Prec@1=62.694 Prec@5=84.316 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:12 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.551 Prec@1=62.694 Prec@5=84.316 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=10:12 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.234 Loss=1.551 Prec@1=62.694 Prec@5=84.316 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=10:13 IST=> training   91.93% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.697 Prec@5=84.319 rate=2.97 Hz, eta=0:01:07, total=0:12:53, wall=10:13 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.697 Prec@5=84.319 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=10:13 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.339 DataTime=0.235 Loss=1.551 Prec@1=62.697 Prec@5=84.319 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=10:13 IST=> training   95.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.233 Loss=1.551 Prec@1=62.689 Prec@5=84.323 rate=2.97 Hz, eta=0:00:34, total=0:13:28, wall=10:13 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.233 Loss=1.551 Prec@1=62.689 Prec@5=84.323 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=10:13 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.233 Loss=1.551 Prec@1=62.689 Prec@5=84.323 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=10:13 IST=> training   99.92% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.233 Loss=1.551 Prec@1=62.689 Prec@5=84.323 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=10:13 IST=> training   100.00% of 1x2503...Epoch=142/150 LR=0.00089 Time=0.338 DataTime=0.233 Loss=1.551 Prec@1=62.689 Prec@5=84.323 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=10:13 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:13 IST=> validation 0.00% of 1x98...Epoch=142/150 LR=0.00089 Time=6.705 Loss=1.513 Prec@1=63.867 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=10:13 IST=> validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=6.705 Loss=1.513 Prec@1=63.867 Prec@5=84.766 rate=6319.03 Hz, eta=0:00:00, total=0:00:00, wall=10:13 IST** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=6.705 Loss=1.513 Prec@1=63.867 Prec@5=84.766 rate=6319.03 Hz, eta=0:00:00, total=0:00:00, wall=10:14 IST** validation 1.02% of 1x98...Epoch=142/150 LR=0.00089 Time=0.408 Loss=1.557 Prec@1=62.568 Prec@5=84.436 rate=6319.03 Hz, eta=0:00:00, total=0:00:00, wall=10:14 IST** validation 100.00% of 1x98...Epoch=142/150 LR=0.00089 Time=0.408 Loss=1.557 Prec@1=62.568 Prec@5=84.436 rate=2.95 Hz, eta=0:00:00, total=0:00:33, wall=10:14 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:14 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:14 IST=> training   0.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.774 DataTime=4.589 Loss=1.543 Prec@1=60.938 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=10:14 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.774 DataTime=4.589 Loss=1.543 Prec@1=60.938 Prec@5=83.008 rate=5460.66 Hz, eta=0:00:00, total=0:00:00, wall=10:14 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=4.774 DataTime=4.589 Loss=1.543 Prec@1=60.938 Prec@5=83.008 rate=5460.66 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST=> training   0.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.374 DataTime=0.277 Loss=1.547 Prec@1=63.074 Prec@5=84.410 rate=5460.66 Hz, eta=0:00:00, total=0:00:00, wall=10:15 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.374 DataTime=0.277 Loss=1.547 Prec@1=63.074 Prec@5=84.410 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=10:15 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.374 DataTime=0.277 Loss=1.547 Prec@1=63.074 Prec@5=84.410 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=10:15 IST=> training   4.04% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.357 DataTime=0.257 Loss=1.540 Prec@1=63.039 Prec@5=84.502 rate=3.06 Hz, eta=0:13:04, total=0:00:32, wall=10:15 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.357 DataTime=0.257 Loss=1.540 Prec@1=63.039 Prec@5=84.502 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=10:15 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.357 DataTime=0.257 Loss=1.540 Prec@1=63.039 Prec@5=84.502 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=10:16 IST=> training   8.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.346 DataTime=0.245 Loss=1.546 Prec@1=62.928 Prec@5=84.431 rate=3.00 Hz, eta=0:12:46, total=0:01:06, wall=10:16 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.346 DataTime=0.245 Loss=1.546 Prec@1=62.928 Prec@5=84.431 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=10:16 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.346 DataTime=0.245 Loss=1.546 Prec@1=62.928 Prec@5=84.431 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=10:16 IST=> training   12.03% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.342 DataTime=0.241 Loss=1.547 Prec@1=62.841 Prec@5=84.431 rate=3.03 Hz, eta=0:12:06, total=0:01:39, wall=10:16 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.342 DataTime=0.241 Loss=1.547 Prec@1=62.841 Prec@5=84.431 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=10:16 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.342 DataTime=0.241 Loss=1.547 Prec@1=62.841 Prec@5=84.431 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=10:17 IST=> training   16.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.345 DataTime=0.244 Loss=1.545 Prec@1=62.884 Prec@5=84.438 rate=3.03 Hz, eta=0:11:34, total=0:02:12, wall=10:17 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.345 DataTime=0.244 Loss=1.545 Prec@1=62.884 Prec@5=84.438 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:17 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.345 DataTime=0.244 Loss=1.545 Prec@1=62.884 Prec@5=84.438 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:18 IST=> training   20.02% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.343 DataTime=0.241 Loss=1.544 Prec@1=62.899 Prec@5=84.448 rate=2.98 Hz, eta=0:11:11, total=0:02:48, wall=10:18 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.343 DataTime=0.241 Loss=1.544 Prec@1=62.899 Prec@5=84.448 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=10:18 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.343 DataTime=0.241 Loss=1.544 Prec@1=62.899 Prec@5=84.448 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=10:18 IST=> training   24.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.545 Prec@1=62.882 Prec@5=84.437 rate=2.98 Hz, eta=0:10:38, total=0:03:21, wall=10:18 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.545 Prec@1=62.882 Prec@5=84.437 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:18 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.545 Prec@1=62.882 Prec@5=84.437 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:19 IST=> training   28.01% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.239 Loss=1.545 Prec@1=62.877 Prec@5=84.446 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:19 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.239 Loss=1.545 Prec@1=62.877 Prec@5=84.446 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=10:19 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.239 Loss=1.545 Prec@1=62.877 Prec@5=84.446 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=10:19 IST=> training   32.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.238 Loss=1.547 Prec@1=62.821 Prec@5=84.419 rate=2.98 Hz, eta=0:09:30, total=0:04:28, wall=10:19 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.238 Loss=1.547 Prec@1=62.821 Prec@5=84.419 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:19 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.238 Loss=1.547 Prec@1=62.821 Prec@5=84.419 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:20 IST=> training   36.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.237 Loss=1.548 Prec@1=62.777 Prec@5=84.391 rate=2.99 Hz, eta=0:08:55, total=0:05:01, wall=10:20 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.237 Loss=1.548 Prec@1=62.777 Prec@5=84.391 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=10:20 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.237 Loss=1.548 Prec@1=62.777 Prec@5=84.391 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=10:20 IST=> training   39.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.549 Prec@1=62.765 Prec@5=84.376 rate=2.99 Hz, eta=0:08:22, total=0:05:34, wall=10:20 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.549 Prec@1=62.765 Prec@5=84.376 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=10:20 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.341 DataTime=0.238 Loss=1.549 Prec@1=62.765 Prec@5=84.376 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=10:21 IST=> training   43.99% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.780 Prec@5=84.390 rate=2.97 Hz, eta=0:07:51, total=0:06:10, wall=10:21 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.780 Prec@5=84.390 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:21 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.549 Prec@1=62.780 Prec@5=84.390 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:21 IST=> training   47.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.773 Prec@5=84.373 rate=2.98 Hz, eta=0:07:17, total=0:06:43, wall=10:21 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.773 Prec@5=84.373 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=10:21 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.773 Prec@5=84.373 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=10:22 IST=> training   51.98% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.764 Prec@5=84.355 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=10:22 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.764 Prec@5=84.355 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=10:22 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.764 Prec@5=84.355 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=10:23 IST=> training   55.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.794 Prec@5=84.355 rate=2.97 Hz, eta=0:06:10, total=0:07:51, wall=10:23 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.794 Prec@5=84.355 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:23 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.340 DataTime=0.237 Loss=1.550 Prec@1=62.794 Prec@5=84.355 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:23 IST=> training   59.97% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.780 Prec@5=84.348 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:23 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.780 Prec@5=84.348 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:23 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.780 Prec@5=84.348 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:24 IST=> training   63.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.778 Prec@5=84.343 rate=2.97 Hz, eta=0:05:03, total=0:08:58, wall=10:24 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.778 Prec@5=84.343 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:24 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.550 Prec@1=62.778 Prec@5=84.343 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:24 IST=> training   67.96% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.793 Prec@5=84.336 rate=2.97 Hz, eta=0:04:29, total=0:09:32, wall=10:24 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.793 Prec@5=84.336 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=10:24 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.793 Prec@5=84.336 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=10:25 IST=> training   71.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.799 Prec@5=84.336 rate=2.97 Hz, eta=0:03:56, total=0:10:06, wall=10:25 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.799 Prec@5=84.336 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:25 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.799 Prec@5=84.336 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:25 IST=> training   75.95% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.794 Prec@5=84.332 rate=2.98 Hz, eta=0:03:22, total=0:10:38, wall=10:25 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.794 Prec@5=84.332 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=10:25 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.236 Loss=1.549 Prec@1=62.794 Prec@5=84.332 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=10:26 IST=> training   79.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.807 Prec@5=84.338 rate=2.97 Hz, eta=0:02:48, total=0:11:13, wall=10:26 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.807 Prec@5=84.338 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=10:26 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.807 Prec@5=84.338 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=10:26 IST=> training   83.94% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.793 Prec@5=84.335 rate=2.97 Hz, eta=0:02:15, total=0:11:46, wall=10:26 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.793 Prec@5=84.335 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:26 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.549 Prec@1=62.793 Prec@5=84.335 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:27 IST=> training   87.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.775 Prec@5=84.333 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=10:27 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.775 Prec@5=84.333 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=10:27 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.339 DataTime=0.235 Loss=1.550 Prec@1=62.775 Prec@5=84.333 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=10:28 IST=> training   91.93% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.779 Prec@5=84.340 rate=2.97 Hz, eta=0:01:07, total=0:12:54, wall=10:28 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.779 Prec@5=84.340 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=10:28 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.235 Loss=1.549 Prec@1=62.779 Prec@5=84.340 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=10:28 IST=> training   95.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.781 Prec@5=84.338 rate=2.97 Hz, eta=0:00:34, total=0:13:27, wall=10:28 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.781 Prec@5=84.338 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:28 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.781 Prec@5=84.338 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:28 IST=> training   99.92% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.781 Prec@5=84.338 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=10:28 IST=> training   100.00% of 1x2503...Epoch=143/150 LR=0.00070 Time=0.338 DataTime=0.234 Loss=1.549 Prec@1=62.781 Prec@5=84.338 rate=2.98 Hz, eta=0:00:00, total=0:14:00, wall=10:28 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> validation 0.00% of 1x98...Epoch=143/150 LR=0.00070 Time=7.484 Loss=1.406 Prec@1=65.234 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=10:28 IST=> validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.484 Loss=1.406 Prec@1=65.234 Prec@5=86.328 rate=6343.48 Hz, eta=0:00:00, total=0:00:00, wall=10:28 IST** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=7.484 Loss=1.406 Prec@1=65.234 Prec@5=86.328 rate=6343.48 Hz, eta=0:00:00, total=0:00:00, wall=10:29 IST** validation 1.02% of 1x98...Epoch=143/150 LR=0.00070 Time=0.409 Loss=1.554 Prec@1=62.508 Prec@5=84.546 rate=6343.48 Hz, eta=0:00:00, total=0:00:00, wall=10:29 IST** validation 100.00% of 1x98...Epoch=143/150 LR=0.00070 Time=0.409 Loss=1.554 Prec@1=62.508 Prec@5=84.546 rate=3.01 Hz, eta=0:00:00, total=0:00:32, wall=10:29 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:29 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:29 IST=> training   0.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.647 DataTime=5.486 Loss=1.594 Prec@1=62.305 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=10:29 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.647 DataTime=5.486 Loss=1.594 Prec@1=62.305 Prec@5=84.570 rate=2159.00 Hz, eta=0:00:01, total=0:00:00, wall=10:29 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=5.647 DataTime=5.486 Loss=1.594 Prec@1=62.305 Prec@5=84.570 rate=2159.00 Hz, eta=0:00:01, total=0:00:00, wall=10:29 IST=> training   0.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.372 DataTime=0.271 Loss=1.545 Prec@1=62.898 Prec@5=84.342 rate=2159.00 Hz, eta=0:00:01, total=0:00:00, wall=10:29 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.372 DataTime=0.271 Loss=1.545 Prec@1=62.898 Prec@5=84.342 rate=3.16 Hz, eta=0:12:39, total=0:00:31, wall=10:29 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.372 DataTime=0.271 Loss=1.545 Prec@1=62.898 Prec@5=84.342 rate=3.16 Hz, eta=0:12:39, total=0:00:31, wall=10:30 IST=> training   4.04% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.349 DataTime=0.248 Loss=1.536 Prec@1=62.944 Prec@5=84.547 rate=3.16 Hz, eta=0:12:39, total=0:00:31, wall=10:30 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.349 DataTime=0.248 Loss=1.536 Prec@1=62.944 Prec@5=84.547 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=10:30 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.349 DataTime=0.248 Loss=1.536 Prec@1=62.944 Prec@5=84.547 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=10:31 IST=> training   8.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.347 DataTime=0.245 Loss=1.538 Prec@1=63.004 Prec@5=84.541 rate=3.11 Hz, eta=0:12:19, total=0:01:04, wall=10:31 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.347 DataTime=0.245 Loss=1.538 Prec@1=63.004 Prec@5=84.541 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=10:31 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.347 DataTime=0.245 Loss=1.538 Prec@1=63.004 Prec@5=84.541 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=10:31 IST=> training   12.03% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.239 Loss=1.543 Prec@1=62.932 Prec@5=84.463 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=10:31 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.239 Loss=1.543 Prec@1=62.932 Prec@5=84.463 rate=3.06 Hz, eta=0:11:28, total=0:02:11, wall=10:31 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.239 Loss=1.543 Prec@1=62.932 Prec@5=84.463 rate=3.06 Hz, eta=0:11:28, total=0:02:11, wall=10:32 IST=> training   16.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.238 Loss=1.543 Prec@1=62.956 Prec@5=84.433 rate=3.06 Hz, eta=0:11:28, total=0:02:11, wall=10:32 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.238 Loss=1.543 Prec@1=62.956 Prec@5=84.433 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:32 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.341 DataTime=0.238 Loss=1.543 Prec@1=62.956 Prec@5=84.433 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:32 IST=> training   20.02% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.340 DataTime=0.236 Loss=1.547 Prec@1=62.866 Prec@5=84.383 rate=3.03 Hz, eta=0:11:00, total=0:02:45, wall=10:32 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.340 DataTime=0.236 Loss=1.547 Prec@1=62.866 Prec@5=84.383 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=10:32 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.340 DataTime=0.236 Loss=1.547 Prec@1=62.866 Prec@5=84.383 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=10:33 IST=> training   24.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.548 Prec@1=62.838 Prec@5=84.391 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=10:33 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.548 Prec@1=62.838 Prec@5=84.391 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=10:33 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.548 Prec@1=62.838 Prec@5=84.391 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=10:33 IST=> training   28.01% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.547 Prec@1=62.853 Prec@5=84.428 rate=3.03 Hz, eta=0:09:55, total=0:03:51, wall=10:33 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.547 Prec@1=62.853 Prec@5=84.428 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=10:33 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.339 DataTime=0.235 Loss=1.547 Prec@1=62.853 Prec@5=84.428 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=10:34 IST=> training   32.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.546 Prec@1=62.878 Prec@5=84.430 rate=3.02 Hz, eta=0:09:24, total=0:04:25, wall=10:34 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.546 Prec@1=62.878 Prec@5=84.430 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=10:34 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.546 Prec@1=62.878 Prec@5=84.430 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=10:34 IST=> training   36.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.545 Prec@1=62.898 Prec@5=84.433 rate=3.01 Hz, eta=0:08:51, total=0:04:58, wall=10:34 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.545 Prec@1=62.898 Prec@5=84.433 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=10:34 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.338 DataTime=0.234 Loss=1.545 Prec@1=62.898 Prec@5=84.433 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=10:35 IST=> training   39.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.882 Prec@5=84.414 rate=3.01 Hz, eta=0:08:18, total=0:05:32, wall=10:35 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.882 Prec@5=84.414 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:35 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.882 Prec@5=84.414 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:36 IST=> training   43.99% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.905 Prec@5=84.422 rate=3.01 Hz, eta=0:07:45, total=0:06:05, wall=10:36 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.905 Prec@5=84.422 rate=3.01 Hz, eta=0:07:12, total=0:06:38, wall=10:36 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.545 Prec@1=62.905 Prec@5=84.422 rate=3.01 Hz, eta=0:07:12, total=0:06:38, wall=10:36 IST=> training   47.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.892 Prec@5=84.427 rate=3.01 Hz, eta=0:07:12, total=0:06:38, wall=10:36 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.892 Prec@5=84.427 rate=3.01 Hz, eta=0:06:39, total=0:07:11, wall=10:36 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.892 Prec@5=84.427 rate=3.01 Hz, eta=0:06:39, total=0:07:11, wall=10:37 IST=> training   51.98% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.544 Prec@1=62.900 Prec@5=84.433 rate=3.01 Hz, eta=0:06:39, total=0:07:11, wall=10:37 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.544 Prec@1=62.900 Prec@5=84.433 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=10:37 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.337 DataTime=0.233 Loss=1.544 Prec@1=62.900 Prec@5=84.433 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=10:37 IST=> training   55.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.883 Prec@5=84.421 rate=3.01 Hz, eta=0:06:06, total=0:07:45, wall=10:37 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.883 Prec@5=84.421 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=10:37 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.883 Prec@5=84.421 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=10:38 IST=> training   59.97% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.893 Prec@5=84.421 rate=3.01 Hz, eta=0:05:33, total=0:08:18, wall=10:38 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.893 Prec@5=84.421 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=10:38 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.893 Prec@5=84.421 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=10:38 IST=> training   63.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.899 Prec@5=84.423 rate=3.01 Hz, eta=0:04:59, total=0:08:51, wall=10:38 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.899 Prec@5=84.423 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=10:38 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.899 Prec@5=84.423 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=10:39 IST=> training   67.96% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.891 Prec@5=84.429 rate=3.01 Hz, eta=0:04:26, total=0:09:25, wall=10:39 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.891 Prec@5=84.429 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=10:39 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.544 Prec@1=62.891 Prec@5=84.429 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=10:39 IST=> training   71.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.887 Prec@5=84.436 rate=3.01 Hz, eta=0:03:53, total=0:09:59, wall=10:39 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.887 Prec@5=84.436 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=10:39 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.887 Prec@5=84.436 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=10:40 IST=> training   75.95% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.890 Prec@5=84.433 rate=3.01 Hz, eta=0:03:20, total=0:10:32, wall=10:40 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.890 Prec@5=84.433 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=10:40 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.232 Loss=1.545 Prec@1=62.890 Prec@5=84.433 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=10:41 IST=> training   79.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.545 Prec@1=62.883 Prec@5=84.440 rate=3.00 Hz, eta=0:02:47, total=0:11:06, wall=10:41 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.545 Prec@1=62.883 Prec@5=84.440 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=10:41 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.545 Prec@1=62.883 Prec@5=84.440 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=10:41 IST=> training   83.94% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.545 Prec@1=62.885 Prec@5=84.437 rate=3.00 Hz, eta=0:02:13, total=0:11:39, wall=10:41 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.545 Prec@1=62.885 Prec@5=84.437 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=10:41 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.545 Prec@1=62.885 Prec@5=84.437 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=10:42 IST=> training   87.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.544 Prec@1=62.908 Prec@5=84.437 rate=3.01 Hz, eta=0:01:40, total=0:12:12, wall=10:42 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.544 Prec@1=62.908 Prec@5=84.437 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=10:42 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.231 Loss=1.544 Prec@1=62.908 Prec@5=84.437 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=10:42 IST=> training   91.93% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.230 Loss=1.544 Prec@1=62.902 Prec@5=84.430 rate=3.00 Hz, eta=0:01:07, total=0:12:47, wall=10:42 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.230 Loss=1.544 Prec@1=62.902 Prec@5=84.430 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=10:42 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.336 DataTime=0.230 Loss=1.544 Prec@1=62.902 Prec@5=84.430 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=10:43 IST=> training   95.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.544 Prec@1=62.911 Prec@5=84.435 rate=3.00 Hz, eta=0:00:34, total=0:13:20, wall=10:43 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.544 Prec@1=62.911 Prec@5=84.435 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=10:43 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.544 Prec@1=62.911 Prec@5=84.435 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=10:43 IST=> training   99.92% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.544 Prec@1=62.912 Prec@5=84.435 rate=3.00 Hz, eta=0:00:00, total=0:13:52, wall=10:43 IST=> training   100.00% of 1x2503...Epoch=144/150 LR=0.00054 Time=0.335 DataTime=0.230 Loss=1.544 Prec@1=62.912 Prec@5=84.435 rate=3.00 Hz, eta=0:00:00, total=0:13:53, wall=10:43 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:43 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:43 IST=> validation 0.00% of 1x98...Epoch=144/150 LR=0.00054 Time=6.178 Loss=1.581 Prec@1=63.086 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=10:43 IST=> validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=6.178 Loss=1.581 Prec@1=63.086 Prec@5=82.227 rate=6272.38 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=6.178 Loss=1.581 Prec@1=63.086 Prec@5=82.227 rate=6272.38 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST** validation 1.02% of 1x98...Epoch=144/150 LR=0.00054 Time=0.398 Loss=1.553 Prec@1=62.614 Prec@5=84.520 rate=6272.38 Hz, eta=0:00:00, total=0:00:00, wall=10:43 IST** validation 100.00% of 1x98...Epoch=144/150 LR=0.00054 Time=0.398 Loss=1.553 Prec@1=62.614 Prec@5=84.520 rate=2.98 Hz, eta=0:00:00, total=0:00:32, wall=10:43 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:44 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:44 IST=> training   0.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.674 DataTime=5.485 Loss=1.516 Prec@1=61.719 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=10:44 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.674 DataTime=5.485 Loss=1.516 Prec@1=61.719 Prec@5=85.547 rate=3840.91 Hz, eta=0:00:00, total=0:00:00, wall=10:44 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=5.674 DataTime=5.485 Loss=1.516 Prec@1=61.719 Prec@5=85.547 rate=3840.91 Hz, eta=0:00:00, total=0:00:00, wall=10:44 IST=> training   0.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.383 DataTime=0.287 Loss=1.538 Prec@1=63.076 Prec@5=84.555 rate=3840.91 Hz, eta=0:00:00, total=0:00:00, wall=10:44 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.383 DataTime=0.287 Loss=1.538 Prec@1=63.076 Prec@5=84.555 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=10:44 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.383 DataTime=0.287 Loss=1.538 Prec@1=63.076 Prec@5=84.555 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=10:45 IST=> training   4.04% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.358 DataTime=0.256 Loss=1.545 Prec@1=62.874 Prec@5=84.497 rate=3.06 Hz, eta=0:13:03, total=0:00:32, wall=10:45 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.358 DataTime=0.256 Loss=1.545 Prec@1=62.874 Prec@5=84.497 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=10:45 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.358 DataTime=0.256 Loss=1.545 Prec@1=62.874 Prec@5=84.497 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=10:45 IST=> training   8.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.350 DataTime=0.246 Loss=1.539 Prec@1=62.992 Prec@5=84.573 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=10:45 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.350 DataTime=0.246 Loss=1.539 Prec@1=62.992 Prec@5=84.573 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=10:45 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.350 DataTime=0.246 Loss=1.539 Prec@1=62.992 Prec@5=84.573 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=10:46 IST=> training   12.03% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.540 Prec@1=63.002 Prec@5=84.528 rate=3.02 Hz, eta=0:12:09, total=0:01:39, wall=10:46 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.540 Prec@1=63.002 Prec@5=84.528 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=10:46 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.540 Prec@1=63.002 Prec@5=84.528 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=10:46 IST=> training   16.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.541 Prec@1=62.947 Prec@5=84.485 rate=3.02 Hz, eta=0:11:37, total=0:02:12, wall=10:46 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.541 Prec@1=62.947 Prec@5=84.485 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=10:46 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.346 DataTime=0.242 Loss=1.541 Prec@1=62.947 Prec@5=84.485 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=10:47 IST=> training   20.02% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.239 Loss=1.542 Prec@1=62.934 Prec@5=84.468 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=10:47 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.239 Loss=1.542 Prec@1=62.934 Prec@5=84.468 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=10:47 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.239 Loss=1.542 Prec@1=62.934 Prec@5=84.468 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=10:48 IST=> training   24.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.237 Loss=1.542 Prec@1=62.925 Prec@5=84.459 rate=2.99 Hz, eta=0:10:35, total=0:03:20, wall=10:48 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.237 Loss=1.542 Prec@1=62.925 Prec@5=84.459 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:48 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.237 Loss=1.542 Prec@1=62.925 Prec@5=84.459 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:48 IST=> training   28.01% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.237 Loss=1.542 Prec@1=62.931 Prec@5=84.466 rate=2.99 Hz, eta=0:10:01, total=0:03:54, wall=10:48 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.237 Loss=1.542 Prec@1=62.931 Prec@5=84.466 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=10:48 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.343 DataTime=0.237 Loss=1.542 Prec@1=62.931 Prec@5=84.466 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=10:49 IST=> training   32.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.950 Prec@5=84.497 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=10:49 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.950 Prec@5=84.497 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=10:49 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.950 Prec@5=84.497 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=10:49 IST=> training   36.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.922 Prec@5=84.497 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=10:49 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.922 Prec@5=84.497 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=10:49 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.922 Prec@5=84.497 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=10:50 IST=> training   39.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.540 Prec@1=62.936 Prec@5=84.502 rate=2.98 Hz, eta=0:08:24, total=0:05:36, wall=10:50 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.540 Prec@1=62.936 Prec@5=84.502 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=10:50 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.236 Loss=1.540 Prec@1=62.936 Prec@5=84.502 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=10:50 IST=> training   43.99% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.235 Loss=1.541 Prec@1=62.949 Prec@5=84.495 rate=2.96 Hz, eta=0:07:52, total=0:06:11, wall=10:50 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.235 Loss=1.541 Prec@1=62.949 Prec@5=84.495 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=10:50 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.235 Loss=1.541 Prec@1=62.949 Prec@5=84.495 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=10:51 IST=> training   47.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.234 Loss=1.540 Prec@1=62.981 Prec@5=84.501 rate=2.97 Hz, eta=0:07:18, total=0:06:44, wall=10:51 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.234 Loss=1.540 Prec@1=62.981 Prec@5=84.501 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=10:51 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.234 Loss=1.540 Prec@1=62.981 Prec@5=84.501 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=10:52 IST=> training   51.98% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=62.988 Prec@5=84.506 rate=2.97 Hz, eta=0:06:44, total=0:07:18, wall=10:52 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=62.988 Prec@5=84.506 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=10:52 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=62.988 Prec@5=84.506 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=10:52 IST=> training   55.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.540 Prec@1=62.977 Prec@5=84.497 rate=2.96 Hz, eta=0:06:12, total=0:07:52, wall=10:52 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.540 Prec@1=62.977 Prec@5=84.497 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:52 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.540 Prec@1=62.977 Prec@5=84.497 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:53 IST=> training   59.97% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.540 Prec@1=62.968 Prec@5=84.502 rate=2.97 Hz, eta=0:05:37, total=0:08:25, wall=10:53 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.540 Prec@1=62.968 Prec@5=84.502 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=10:53 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.540 Prec@1=62.968 Prec@5=84.502 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=10:53 IST=> training   63.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.954 Prec@5=84.492 rate=2.97 Hz, eta=0:05:03, total=0:08:59, wall=10:53 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.954 Prec@5=84.492 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:53 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.954 Prec@5=84.492 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:54 IST=> training   67.96% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.962 Prec@5=84.493 rate=2.96 Hz, eta=0:04:30, total=0:09:34, wall=10:54 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.962 Prec@5=84.493 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=10:54 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.541 Prec@1=62.962 Prec@5=84.493 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=10:54 IST=> training   71.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.948 Prec@5=84.486 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=10:54 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.948 Prec@5=84.486 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=10:54 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.948 Prec@5=84.486 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=10:55 IST=> training   75.95% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.945 Prec@5=84.482 rate=2.97 Hz, eta=0:03:22, total=0:10:40, wall=10:55 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.945 Prec@5=84.482 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=10:55 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.945 Prec@5=84.482 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=10:55 IST=> training   79.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.934 Prec@5=84.483 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=10:55 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.934 Prec@5=84.483 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:55 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.541 Prec@1=62.934 Prec@5=84.483 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:56 IST=> training   83.94% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.935 Prec@5=84.480 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=10:56 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.935 Prec@5=84.480 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:56 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.935 Prec@5=84.480 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:57 IST=> training   87.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.927 Prec@5=84.470 rate=2.96 Hz, eta=0:01:41, total=0:12:23, wall=10:57 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.927 Prec@5=84.470 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=10:57 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.341 DataTime=0.233 Loss=1.542 Prec@1=62.927 Prec@5=84.470 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=10:57 IST=> training   91.93% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.233 Loss=1.542 Prec@1=62.914 Prec@5=84.459 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=10:57 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.233 Loss=1.542 Prec@1=62.914 Prec@5=84.459 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=10:57 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.233 Loss=1.542 Prec@1=62.914 Prec@5=84.459 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=10:58 IST=> training   95.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.898 Prec@5=84.462 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=10:58 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.898 Prec@5=84.462 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:58 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.340 DataTime=0.232 Loss=1.542 Prec@1=62.898 Prec@5=84.462 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:58 IST=> training   99.92% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.339 DataTime=0.232 Loss=1.542 Prec@1=62.896 Prec@5=84.462 rate=2.96 Hz, eta=0:00:00, total=0:14:03, wall=10:58 IST=> training   100.00% of 1x2503...Epoch=145/150 LR=0.00039 Time=0.339 DataTime=0.232 Loss=1.542 Prec@1=62.896 Prec@5=84.462 rate=2.97 Hz, eta=0:00:00, total=0:14:04, wall=10:58 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> validation 0.00% of 1x98...Epoch=145/150 LR=0.00039 Time=7.065 Loss=1.583 Prec@1=60.547 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=7.065 Loss=1.583 Prec@1=60.547 Prec@5=82.812 rate=7129.82 Hz, eta=0:00:00, total=0:00:00, wall=10:58 IST** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=7.065 Loss=1.583 Prec@1=60.547 Prec@5=82.812 rate=7129.82 Hz, eta=0:00:00, total=0:00:00, wall=10:58 IST** validation 1.02% of 1x98...Epoch=145/150 LR=0.00039 Time=0.400 Loss=1.550 Prec@1=62.630 Prec@5=84.532 rate=7129.82 Hz, eta=0:00:00, total=0:00:00, wall=10:58 IST** validation 100.00% of 1x98...Epoch=145/150 LR=0.00039 Time=0.400 Loss=1.550 Prec@1=62.630 Prec@5=84.532 rate=3.05 Hz, eta=0:00:00, total=0:00:32, wall=10:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> training   0.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.324 DataTime=5.093 Loss=1.541 Prec@1=63.477 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=10:58 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.324 DataTime=5.093 Loss=1.541 Prec@1=63.477 Prec@5=84.961 rate=6191.07 Hz, eta=0:00:00, total=0:00:00, wall=10:58 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=5.324 DataTime=5.093 Loss=1.541 Prec@1=63.477 Prec@5=84.961 rate=6191.07 Hz, eta=0:00:00, total=0:00:00, wall=10:59 IST=> training   0.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.376 DataTime=0.281 Loss=1.554 Prec@1=62.521 Prec@5=84.193 rate=6191.07 Hz, eta=0:00:00, total=0:00:00, wall=10:59 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.376 DataTime=0.281 Loss=1.554 Prec@1=62.521 Prec@5=84.193 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=10:59 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.376 DataTime=0.281 Loss=1.554 Prec@1=62.521 Prec@5=84.193 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=11:00 IST=> training   4.04% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.357 DataTime=0.260 Loss=1.552 Prec@1=62.794 Prec@5=84.207 rate=3.08 Hz, eta=0:12:58, total=0:00:32, wall=11:00 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.357 DataTime=0.260 Loss=1.552 Prec@1=62.794 Prec@5=84.207 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=11:00 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.357 DataTime=0.260 Loss=1.552 Prec@1=62.794 Prec@5=84.207 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=11:00 IST=> training   8.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.346 DataTime=0.246 Loss=1.546 Prec@1=62.930 Prec@5=84.339 rate=3.03 Hz, eta=0:12:40, total=0:01:06, wall=11:00 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.346 DataTime=0.246 Loss=1.546 Prec@1=62.930 Prec@5=84.339 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:00 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.346 DataTime=0.246 Loss=1.546 Prec@1=62.930 Prec@5=84.339 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:01 IST=> training   12.03% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.345 DataTime=0.244 Loss=1.543 Prec@1=63.000 Prec@5=84.385 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:01 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.345 DataTime=0.244 Loss=1.543 Prec@1=63.000 Prec@5=84.385 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:01 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.345 DataTime=0.244 Loss=1.543 Prec@1=63.000 Prec@5=84.385 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:01 IST=> training   16.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.342 DataTime=0.241 Loss=1.543 Prec@1=63.005 Prec@5=84.396 rate=3.02 Hz, eta=0:11:36, total=0:02:12, wall=11:01 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.342 DataTime=0.241 Loss=1.543 Prec@1=63.005 Prec@5=84.396 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=11:01 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.342 DataTime=0.241 Loss=1.543 Prec@1=63.005 Prec@5=84.396 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=11:02 IST=> training   20.02% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.045 Prec@5=84.441 rate=3.02 Hz, eta=0:11:02, total=0:02:45, wall=11:02 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.045 Prec@5=84.441 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=11:02 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.045 Prec@5=84.441 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=11:02 IST=> training   24.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.240 Loss=1.539 Prec@1=63.042 Prec@5=84.474 rate=3.03 Hz, eta=0:10:27, total=0:03:18, wall=11:02 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.240 Loss=1.539 Prec@1=63.042 Prec@5=84.474 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:02 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.240 Loss=1.539 Prec@1=63.042 Prec@5=84.474 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:03 IST=> training   28.01% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.239 Loss=1.539 Prec@1=63.036 Prec@5=84.475 rate=3.02 Hz, eta=0:09:57, total=0:03:52, wall=11:03 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.239 Loss=1.539 Prec@1=63.036 Prec@5=84.475 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=11:03 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.239 Loss=1.539 Prec@1=63.036 Prec@5=84.475 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=11:03 IST=> training   32.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.009 Prec@5=84.477 rate=3.01 Hz, eta=0:09:24, total=0:04:25, wall=11:03 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.009 Prec@5=84.477 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=11:03 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.239 Loss=1.539 Prec@1=63.009 Prec@5=84.477 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=11:04 IST=> training   36.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.239 Loss=1.541 Prec@1=62.994 Prec@5=84.451 rate=3.00 Hz, eta=0:08:53, total=0:04:59, wall=11:04 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.239 Loss=1.541 Prec@1=62.994 Prec@5=84.451 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=11:04 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.239 Loss=1.541 Prec@1=62.994 Prec@5=84.451 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=11:05 IST=> training   39.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.238 Loss=1.542 Prec@1=62.941 Prec@5=84.438 rate=2.99 Hz, eta=0:08:22, total=0:05:35, wall=11:05 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.238 Loss=1.542 Prec@1=62.941 Prec@5=84.438 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=11:05 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.238 Loss=1.542 Prec@1=62.941 Prec@5=84.438 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=11:05 IST=> training   43.99% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.957 Prec@5=84.452 rate=2.99 Hz, eta=0:07:49, total=0:06:08, wall=11:05 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.957 Prec@5=84.452 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:05 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.957 Prec@5=84.452 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:06 IST=> training   47.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.238 Loss=1.541 Prec@1=62.962 Prec@5=84.455 rate=2.99 Hz, eta=0:07:15, total=0:06:41, wall=11:06 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.238 Loss=1.541 Prec@1=62.962 Prec@5=84.455 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=11:06 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.340 DataTime=0.238 Loss=1.541 Prec@1=62.962 Prec@5=84.455 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=11:06 IST=> training   51.98% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.959 Prec@5=84.442 rate=2.98 Hz, eta=0:06:43, total=0:07:16, wall=11:06 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.959 Prec@5=84.442 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:06 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.959 Prec@5=84.442 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:07 IST=> training   55.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.967 Prec@5=84.438 rate=2.98 Hz, eta=0:06:09, total=0:07:49, wall=11:07 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.967 Prec@5=84.438 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=11:07 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.542 Prec@1=62.967 Prec@5=84.438 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=11:07 IST=> training   59.97% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.541 Prec@1=62.986 Prec@5=84.447 rate=2.98 Hz, eta=0:05:35, total=0:08:23, wall=11:07 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.541 Prec@1=62.986 Prec@5=84.447 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=11:07 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.237 Loss=1.541 Prec@1=62.986 Prec@5=84.447 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=11:08 IST=> training   63.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.993 Prec@5=84.439 rate=2.98 Hz, eta=0:05:03, total=0:08:57, wall=11:08 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.993 Prec@5=84.439 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:08 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.993 Prec@5=84.439 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:09 IST=> training   67.96% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=63.005 Prec@5=84.454 rate=2.98 Hz, eta=0:04:28, total=0:09:30, wall=11:09 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=63.005 Prec@5=84.454 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=11:09 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=63.005 Prec@5=84.454 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=11:09 IST=> training   71.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.013 Prec@5=84.460 rate=2.99 Hz, eta=0:03:55, total=0:10:03, wall=11:09 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.013 Prec@5=84.460 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=11:09 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.013 Prec@5=84.460 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=11:10 IST=> training   75.95% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=63.002 Prec@5=84.445 rate=2.98 Hz, eta=0:03:21, total=0:10:37, wall=11:10 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=63.002 Prec@5=84.445 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=11:10 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=63.002 Prec@5=84.445 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=11:10 IST=> training   79.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.009 Prec@5=84.445 rate=2.98 Hz, eta=0:02:48, total=0:11:10, wall=11:10 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.009 Prec@5=84.445 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=11:10 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.540 Prec@1=63.009 Prec@5=84.445 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=11:11 IST=> training   83.94% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.991 Prec@5=84.441 rate=2.98 Hz, eta=0:02:14, total=0:11:44, wall=11:11 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.991 Prec@5=84.441 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=11:11 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.339 DataTime=0.236 Loss=1.541 Prec@1=62.991 Prec@5=84.441 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=11:11 IST=> training   87.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=62.995 Prec@5=84.448 rate=2.97 Hz, eta=0:01:41, total=0:12:20, wall=11:11 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=62.995 Prec@5=84.448 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:11 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.236 Loss=1.541 Prec@1=62.995 Prec@5=84.448 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:12 IST=> training   91.93% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.540 Prec@1=62.994 Prec@5=84.449 rate=2.98 Hz, eta=0:01:07, total=0:12:52, wall=11:12 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.540 Prec@1=62.994 Prec@5=84.449 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=11:12 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.540 Prec@1=62.994 Prec@5=84.449 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=11:12 IST=> training   95.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=62.988 Prec@5=84.447 rate=2.98 Hz, eta=0:00:34, total=0:13:26, wall=11:12 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=62.988 Prec@5=84.447 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=11:12 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=62.988 Prec@5=84.447 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=11:12 IST=> training   99.92% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=62.987 Prec@5=84.447 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=11:12 IST=> training   100.00% of 1x2503...Epoch=146/150 LR=0.00027 Time=0.338 DataTime=0.235 Loss=1.541 Prec@1=62.987 Prec@5=84.447 rate=2.98 Hz, eta=0:00:00, total=0:13:59, wall=11:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> validation 0.00% of 1x98...Epoch=146/150 LR=0.00027 Time=7.012 Loss=1.501 Prec@1=64.844 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=7.012 Loss=1.501 Prec@1=64.844 Prec@5=85.156 rate=87.08 Hz, eta=0:00:01, total=0:00:00, wall=11:13 IST** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=7.012 Loss=1.501 Prec@1=64.844 Prec@5=85.156 rate=87.08 Hz, eta=0:00:01, total=0:00:00, wall=11:13 IST** validation 1.02% of 1x98...Epoch=146/150 LR=0.00027 Time=0.393 Loss=1.550 Prec@1=62.662 Prec@5=84.572 rate=87.08 Hz, eta=0:00:01, total=0:00:00, wall=11:13 IST** validation 100.00% of 1x98...Epoch=146/150 LR=0.00027 Time=0.393 Loss=1.550 Prec@1=62.662 Prec@5=84.572 rate=3.11 Hz, eta=0:00:00, total=0:00:31, wall=11:13 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> training   0.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=6.035 DataTime=5.935 Loss=1.500 Prec@1=66.016 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=11:13 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=6.035 DataTime=5.935 Loss=1.500 Prec@1=66.016 Prec@5=86.328 rate=7768.74 Hz, eta=0:00:00, total=0:00:00, wall=11:13 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=6.035 DataTime=5.935 Loss=1.500 Prec@1=66.016 Prec@5=86.328 rate=7768.74 Hz, eta=0:00:00, total=0:00:00, wall=11:14 IST=> training   0.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.378 DataTime=0.291 Loss=1.554 Prec@1=62.591 Prec@5=84.216 rate=7768.74 Hz, eta=0:00:00, total=0:00:00, wall=11:14 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.378 DataTime=0.291 Loss=1.554 Prec@1=62.591 Prec@5=84.216 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=11:14 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.378 DataTime=0.291 Loss=1.554 Prec@1=62.591 Prec@5=84.216 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=11:14 IST=> training   4.04% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.353 DataTime=0.263 Loss=1.546 Prec@1=62.793 Prec@5=84.345 rate=3.14 Hz, eta=0:12:44, total=0:00:32, wall=11:14 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.353 DataTime=0.263 Loss=1.546 Prec@1=62.793 Prec@5=84.345 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=11:14 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.353 DataTime=0.263 Loss=1.546 Prec@1=62.793 Prec@5=84.345 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=11:15 IST=> training   8.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.348 DataTime=0.255 Loss=1.541 Prec@1=62.883 Prec@5=84.505 rate=3.09 Hz, eta=0:12:24, total=0:01:04, wall=11:15 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.348 DataTime=0.255 Loss=1.541 Prec@1=62.883 Prec@5=84.505 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:15 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.348 DataTime=0.255 Loss=1.541 Prec@1=62.883 Prec@5=84.505 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:15 IST=> training   12.03% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.347 DataTime=0.251 Loss=1.537 Prec@1=62.936 Prec@5=84.560 rate=3.05 Hz, eta=0:12:02, total=0:01:38, wall=11:15 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.347 DataTime=0.251 Loss=1.537 Prec@1=62.936 Prec@5=84.560 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:15 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.347 DataTime=0.251 Loss=1.537 Prec@1=62.936 Prec@5=84.560 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:16 IST=> training   16.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.345 DataTime=0.245 Loss=1.536 Prec@1=63.010 Prec@5=84.552 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:16 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.345 DataTime=0.245 Loss=1.536 Prec@1=63.010 Prec@5=84.552 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=11:16 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.345 DataTime=0.245 Loss=1.536 Prec@1=63.010 Prec@5=84.552 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=11:17 IST=> training   20.02% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.538 Prec@1=62.978 Prec@5=84.541 rate=3.01 Hz, eta=0:11:06, total=0:02:46, wall=11:17 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.538 Prec@1=62.978 Prec@5=84.541 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=11:17 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.538 Prec@1=62.978 Prec@5=84.541 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=11:17 IST=> training   24.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.537 Prec@1=62.991 Prec@5=84.551 rate=3.01 Hz, eta=0:10:32, total=0:03:19, wall=11:17 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.537 Prec@1=62.991 Prec@5=84.551 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=11:17 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.537 Prec@1=62.991 Prec@5=84.551 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=11:18 IST=> training   28.01% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.536 Prec@1=63.020 Prec@5=84.555 rate=2.99 Hz, eta=0:10:03, total=0:03:54, wall=11:18 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.536 Prec@1=63.020 Prec@5=84.555 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=11:18 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.242 Loss=1.536 Prec@1=63.020 Prec@5=84.555 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=11:18 IST=> training   32.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.240 Loss=1.535 Prec@1=63.040 Prec@5=84.561 rate=2.98 Hz, eta=0:09:31, total=0:04:28, wall=11:18 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.240 Loss=1.535 Prec@1=63.040 Prec@5=84.561 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:18 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.240 Loss=1.535 Prec@1=63.040 Prec@5=84.561 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:19 IST=> training   36.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.240 Loss=1.535 Prec@1=63.041 Prec@5=84.558 rate=2.98 Hz, eta=0:08:57, total=0:05:02, wall=11:19 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.240 Loss=1.535 Prec@1=63.041 Prec@5=84.558 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=11:19 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.343 DataTime=0.240 Loss=1.535 Prec@1=63.041 Prec@5=84.558 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=11:19 IST=> training   39.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.535 Prec@1=63.063 Prec@5=84.550 rate=2.97 Hz, eta=0:08:25, total=0:05:37, wall=11:19 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.535 Prec@1=63.063 Prec@5=84.550 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=11:19 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.535 Prec@1=63.063 Prec@5=84.550 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=11:20 IST=> training   43.99% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.535 Prec@1=63.065 Prec@5=84.525 rate=2.98 Hz, eta=0:07:50, total=0:06:09, wall=11:20 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.535 Prec@1=63.065 Prec@5=84.525 rate=2.98 Hz, eta=0:07:16, total=0:06:43, wall=11:20 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.535 Prec@1=63.065 Prec@5=84.525 rate=2.98 Hz, eta=0:07:16, total=0:06:43, wall=11:21 IST=> training   47.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.239 Loss=1.536 Prec@1=63.051 Prec@5=84.510 rate=2.98 Hz, eta=0:07:16, total=0:06:43, wall=11:21 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.239 Loss=1.536 Prec@1=63.051 Prec@5=84.510 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=11:21 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.239 Loss=1.536 Prec@1=63.051 Prec@5=84.510 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=11:21 IST=> training   51.98% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.537 Prec@1=63.030 Prec@5=84.502 rate=2.96 Hz, eta=0:06:46, total=0:07:19, wall=11:21 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.537 Prec@1=63.030 Prec@5=84.502 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=11:21 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.238 Loss=1.537 Prec@1=63.030 Prec@5=84.502 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=11:22 IST=> training   55.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.537 Prec@1=63.013 Prec@5=84.498 rate=2.97 Hz, eta=0:06:11, total=0:07:52, wall=11:22 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.537 Prec@1=63.013 Prec@5=84.498 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=11:22 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.537 Prec@1=63.013 Prec@5=84.498 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=11:22 IST=> training   59.97% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.538 Prec@1=63.007 Prec@5=84.486 rate=2.96 Hz, eta=0:05:38, total=0:08:26, wall=11:22 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.538 Prec@1=63.007 Prec@5=84.486 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=11:22 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.238 Loss=1.538 Prec@1=63.007 Prec@5=84.486 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=11:23 IST=> training   63.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.237 Loss=1.538 Prec@1=63.027 Prec@5=84.479 rate=2.96 Hz, eta=0:05:05, total=0:09:01, wall=11:23 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.237 Loss=1.538 Prec@1=63.027 Prec@5=84.479 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=11:23 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.342 DataTime=0.237 Loss=1.538 Prec@1=63.027 Prec@5=84.479 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=11:23 IST=> training   67.96% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.015 Prec@5=84.469 rate=2.96 Hz, eta=0:04:31, total=0:09:35, wall=11:23 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.015 Prec@5=84.469 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=11:23 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.015 Prec@5=84.469 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=11:24 IST=> training   71.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.024 Prec@5=84.472 rate=2.96 Hz, eta=0:03:57, total=0:10:08, wall=11:24 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.024 Prec@5=84.472 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:24 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.024 Prec@5=84.472 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:25 IST=> training   75.95% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.025 Prec@5=84.469 rate=2.96 Hz, eta=0:03:23, total=0:10:42, wall=11:25 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.025 Prec@5=84.469 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:25 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.025 Prec@5=84.469 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:25 IST=> training   79.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.018 Prec@5=84.473 rate=2.96 Hz, eta=0:02:49, total=0:11:16, wall=11:25 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.018 Prec@5=84.473 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:25 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.538 Prec@1=63.018 Prec@5=84.473 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:26 IST=> training   83.94% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.019 Prec@5=84.476 rate=2.96 Hz, eta=0:02:15, total=0:11:49, wall=11:26 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.019 Prec@5=84.476 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:26 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.237 Loss=1.538 Prec@1=63.019 Prec@5=84.476 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:26 IST=> training   87.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.539 Prec@1=63.008 Prec@5=84.461 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:26 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.539 Prec@1=63.008 Prec@5=84.461 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=11:26 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.341 DataTime=0.236 Loss=1.539 Prec@1=63.008 Prec@5=84.461 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=11:27 IST=> training   91.93% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.236 Loss=1.539 Prec@1=63.007 Prec@5=84.470 rate=2.96 Hz, eta=0:01:08, total=0:12:58, wall=11:27 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.236 Loss=1.539 Prec@1=63.007 Prec@5=84.470 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:27 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.236 Loss=1.539 Prec@1=63.007 Prec@5=84.470 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:27 IST=> training   95.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.235 Loss=1.539 Prec@1=63.013 Prec@5=84.479 rate=2.96 Hz, eta=0:00:34, total=0:13:31, wall=11:27 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.235 Loss=1.539 Prec@1=63.013 Prec@5=84.479 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=11:27 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.235 Loss=1.539 Prec@1=63.013 Prec@5=84.479 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=11:27 IST=> training   99.92% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.235 Loss=1.539 Prec@1=63.012 Prec@5=84.478 rate=2.96 Hz, eta=0:00:00, total=0:14:04, wall=11:27 IST=> training   100.00% of 1x2503...Epoch=147/150 LR=0.00018 Time=0.340 DataTime=0.235 Loss=1.539 Prec@1=63.012 Prec@5=84.478 rate=2.96 Hz, eta=0:00:00, total=0:14:05, wall=11:27 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:27 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:27 IST=> validation 0.00% of 1x98...Epoch=147/150 LR=0.00018 Time=6.624 Loss=1.542 Prec@1=63.672 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=11:27 IST=> validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=6.624 Loss=1.542 Prec@1=63.672 Prec@5=84.766 rate=5714.15 Hz, eta=0:00:00, total=0:00:00, wall=11:27 IST** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=6.624 Loss=1.542 Prec@1=63.672 Prec@5=84.766 rate=5714.15 Hz, eta=0:00:00, total=0:00:00, wall=11:28 IST** validation 1.02% of 1x98...Epoch=147/150 LR=0.00018 Time=0.401 Loss=1.549 Prec@1=62.630 Prec@5=84.516 rate=5714.15 Hz, eta=0:00:00, total=0:00:00, wall=11:28 IST** validation 100.00% of 1x98...Epoch=147/150 LR=0.00018 Time=0.401 Loss=1.549 Prec@1=62.630 Prec@5=84.516 rate=3.00 Hz, eta=0:00:00, total=0:00:32, wall=11:28 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:28 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:28 IST=> training   0.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.881 DataTime=5.733 Loss=1.484 Prec@1=63.672 Prec@5=85.352 rate=0 Hz, eta=?, total=0:00:00, wall=11:28 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.881 DataTime=5.733 Loss=1.484 Prec@1=63.672 Prec@5=85.352 rate=5081.12 Hz, eta=0:00:00, total=0:00:00, wall=11:28 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=5.881 DataTime=5.733 Loss=1.484 Prec@1=63.672 Prec@5=85.352 rate=5081.12 Hz, eta=0:00:00, total=0:00:00, wall=11:29 IST=> training   0.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.381 DataTime=0.288 Loss=1.527 Prec@1=63.208 Prec@5=84.739 rate=5081.12 Hz, eta=0:00:00, total=0:00:00, wall=11:29 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.381 DataTime=0.288 Loss=1.527 Prec@1=63.208 Prec@5=84.739 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=11:29 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.381 DataTime=0.288 Loss=1.527 Prec@1=63.208 Prec@5=84.739 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=11:29 IST=> training   4.04% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.353 DataTime=0.258 Loss=1.534 Prec@1=63.127 Prec@5=84.525 rate=3.10 Hz, eta=0:12:55, total=0:00:32, wall=11:29 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.353 DataTime=0.258 Loss=1.534 Prec@1=63.127 Prec@5=84.525 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=11:29 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.353 DataTime=0.258 Loss=1.534 Prec@1=63.127 Prec@5=84.525 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=11:30 IST=> training   8.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=1.533 Prec@1=63.118 Prec@5=84.572 rate=3.09 Hz, eta=0:12:26, total=0:01:05, wall=11:30 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=1.533 Prec@1=63.118 Prec@5=84.572 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=11:30 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.352 DataTime=0.254 Loss=1.533 Prec@1=63.118 Prec@5=84.572 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=11:30 IST=> training   12.03% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.248 Loss=1.534 Prec@1=63.155 Prec@5=84.574 rate=3.01 Hz, eta=0:12:11, total=0:01:40, wall=11:30 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.248 Loss=1.534 Prec@1=63.155 Prec@5=84.574 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:30 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.248 Loss=1.534 Prec@1=63.155 Prec@5=84.574 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:31 IST=> training   16.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.247 Loss=1.536 Prec@1=63.067 Prec@5=84.544 rate=3.01 Hz, eta=0:11:38, total=0:02:13, wall=11:31 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.247 Loss=1.536 Prec@1=63.067 Prec@5=84.544 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=11:31 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.347 DataTime=0.247 Loss=1.536 Prec@1=63.067 Prec@5=84.544 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=11:31 IST=> training   20.02% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.243 Loss=1.536 Prec@1=63.051 Prec@5=84.526 rate=2.99 Hz, eta=0:11:10, total=0:02:47, wall=11:31 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.243 Loss=1.536 Prec@1=63.051 Prec@5=84.526 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=11:31 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.243 Loss=1.536 Prec@1=63.051 Prec@5=84.526 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=11:32 IST=> training   24.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.242 Loss=1.536 Prec@1=63.046 Prec@5=84.534 rate=2.99 Hz, eta=0:10:37, total=0:03:21, wall=11:32 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.242 Loss=1.536 Prec@1=63.046 Prec@5=84.534 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=11:32 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.242 Loss=1.536 Prec@1=63.046 Prec@5=84.534 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=11:33 IST=> training   28.01% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.346 DataTime=0.242 Loss=1.536 Prec@1=63.047 Prec@5=84.563 rate=2.98 Hz, eta=0:10:05, total=0:03:55, wall=11:33 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.346 DataTime=0.242 Loss=1.536 Prec@1=63.047 Prec@5=84.563 rate=2.95 Hz, eta=0:09:37, total=0:04:31, wall=11:33 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.346 DataTime=0.242 Loss=1.536 Prec@1=63.047 Prec@5=84.563 rate=2.95 Hz, eta=0:09:37, total=0:04:31, wall=11:33 IST=> training   32.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.240 Loss=1.536 Prec@1=63.082 Prec@5=84.576 rate=2.95 Hz, eta=0:09:37, total=0:04:31, wall=11:33 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.240 Loss=1.536 Prec@1=63.082 Prec@5=84.576 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=11:33 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.240 Loss=1.536 Prec@1=63.082 Prec@5=84.576 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=11:34 IST=> training   36.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.535 Prec@1=63.106 Prec@5=84.584 rate=2.95 Hz, eta=0:09:02, total=0:05:05, wall=11:34 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.535 Prec@1=63.106 Prec@5=84.584 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:34 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.535 Prec@1=63.106 Prec@5=84.584 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:34 IST=> training   39.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.537 Prec@1=63.078 Prec@5=84.559 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:34 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.537 Prec@1=63.078 Prec@5=84.559 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=11:34 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.239 Loss=1.537 Prec@1=63.078 Prec@5=84.559 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=11:35 IST=> training   43.99% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.238 Loss=1.537 Prec@1=63.044 Prec@5=84.547 rate=2.94 Hz, eta=0:07:56, total=0:06:14, wall=11:35 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.238 Loss=1.537 Prec@1=63.044 Prec@5=84.547 rate=2.94 Hz, eta=0:07:22, total=0:06:47, wall=11:35 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.345 DataTime=0.238 Loss=1.537 Prec@1=63.044 Prec@5=84.547 rate=2.94 Hz, eta=0:07:22, total=0:06:47, wall=11:35 IST=> training   47.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.237 Loss=1.537 Prec@1=63.060 Prec@5=84.546 rate=2.94 Hz, eta=0:07:22, total=0:06:47, wall=11:35 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.237 Loss=1.537 Prec@1=63.060 Prec@5=84.546 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:35 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.237 Loss=1.537 Prec@1=63.060 Prec@5=84.546 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:36 IST=> training   51.98% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.344 DataTime=0.237 Loss=1.537 Prec@1=63.066 Prec@5=84.542 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:36 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.344 DataTime=0.237 Loss=1.537 Prec@1=63.066 Prec@5=84.542 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=11:36 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.344 DataTime=0.237 Loss=1.537 Prec@1=63.066 Prec@5=84.542 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=11:37 IST=> training   55.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.236 Loss=1.537 Prec@1=63.065 Prec@5=84.535 rate=2.95 Hz, eta=0:06:13, total=0:07:55, wall=11:37 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.236 Loss=1.537 Prec@1=63.065 Prec@5=84.535 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=11:37 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.236 Loss=1.537 Prec@1=63.065 Prec@5=84.535 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=11:37 IST=> training   59.97% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.537 Prec@1=63.054 Prec@5=84.535 rate=2.95 Hz, eta=0:05:39, total=0:08:28, wall=11:37 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.537 Prec@1=63.054 Prec@5=84.535 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=11:37 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.537 Prec@1=63.054 Prec@5=84.535 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=11:38 IST=> training   63.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.060 Prec@5=84.532 rate=2.95 Hz, eta=0:05:05, total=0:09:02, wall=11:38 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.060 Prec@5=84.532 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=11:38 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.060 Prec@5=84.532 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=11:38 IST=> training   67.96% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.051 Prec@5=84.514 rate=2.95 Hz, eta=0:04:32, total=0:09:37, wall=11:38 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.051 Prec@5=84.514 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=11:38 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.051 Prec@5=84.514 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=11:39 IST=> training   71.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.040 Prec@5=84.518 rate=2.95 Hz, eta=0:03:57, total=0:10:09, wall=11:39 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.040 Prec@5=84.518 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=11:39 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.040 Prec@5=84.518 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=11:39 IST=> training   75.95% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.043 Prec@5=84.519 rate=2.95 Hz, eta=0:03:23, total=0:10:43, wall=11:39 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.043 Prec@5=84.519 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=11:39 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.043 Prec@5=84.519 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=11:40 IST=> training   79.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.027 Prec@5=84.514 rate=2.95 Hz, eta=0:02:50, total=0:11:18, wall=11:40 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.027 Prec@5=84.514 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=11:40 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.027 Prec@5=84.514 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=11:41 IST=> training   83.94% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.022 Prec@5=84.507 rate=2.95 Hz, eta=0:02:16, total=0:11:52, wall=11:41 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.022 Prec@5=84.507 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:41 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.022 Prec@5=84.507 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:41 IST=> training   87.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=63.018 Prec@5=84.501 rate=2.95 Hz, eta=0:01:42, total=0:12:25, wall=11:41 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=63.018 Prec@5=84.501 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=11:41 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.342 DataTime=0.234 Loss=1.539 Prec@1=63.018 Prec@5=84.501 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=11:42 IST=> training   91.93% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.016 Prec@5=84.499 rate=2.95 Hz, eta=0:01:08, total=0:13:00, wall=11:42 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.016 Prec@5=84.499 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=11:42 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.234 Loss=1.539 Prec@1=63.016 Prec@5=84.499 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=11:42 IST=> training   95.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.233 Loss=1.539 Prec@1=63.018 Prec@5=84.493 rate=2.95 Hz, eta=0:00:34, total=0:13:33, wall=11:42 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.233 Loss=1.539 Prec@1=63.018 Prec@5=84.493 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=11:42 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.233 Loss=1.539 Prec@1=63.018 Prec@5=84.493 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=11:42 IST=> training   99.92% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.233 Loss=1.539 Prec@1=63.018 Prec@5=84.493 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=11:42 IST=> training   100.00% of 1x2503...Epoch=148/150 LR=0.00010 Time=0.341 DataTime=0.233 Loss=1.539 Prec@1=63.018 Prec@5=84.493 rate=2.96 Hz, eta=0:00:00, total=0:14:06, wall=11:42 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:42 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:42 IST=> validation 0.00% of 1x98...Epoch=148/150 LR=0.00010 Time=5.942 Loss=1.557 Prec@1=63.086 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=11:42 IST=> validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=5.942 Loss=1.557 Prec@1=63.086 Prec@5=84.766 rate=121.70 Hz, eta=0:00:00, total=0:00:00, wall=11:42 IST** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=5.942 Loss=1.557 Prec@1=63.086 Prec@5=84.766 rate=121.70 Hz, eta=0:00:00, total=0:00:00, wall=11:43 IST** validation 1.02% of 1x98...Epoch=148/150 LR=0.00010 Time=0.399 Loss=1.548 Prec@1=62.682 Prec@5=84.604 rate=121.70 Hz, eta=0:00:00, total=0:00:00, wall=11:43 IST** validation 100.00% of 1x98...Epoch=148/150 LR=0.00010 Time=0.399 Loss=1.548 Prec@1=62.682 Prec@5=84.604 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=11:43 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST=> training   0.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=4.772 DataTime=4.590 Loss=1.516 Prec@1=62.695 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=11:43 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=4.772 DataTime=4.590 Loss=1.516 Prec@1=62.695 Prec@5=85.156 rate=2297.83 Hz, eta=0:00:01, total=0:00:00, wall=11:43 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=4.772 DataTime=4.590 Loss=1.516 Prec@1=62.695 Prec@5=85.156 rate=2297.83 Hz, eta=0:00:01, total=0:00:00, wall=11:44 IST=> training   0.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.379 DataTime=0.280 Loss=1.536 Prec@1=62.912 Prec@5=84.539 rate=2297.83 Hz, eta=0:00:01, total=0:00:00, wall=11:44 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.379 DataTime=0.280 Loss=1.536 Prec@1=62.912 Prec@5=84.539 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=11:44 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.379 DataTime=0.280 Loss=1.536 Prec@1=62.912 Prec@5=84.539 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=11:44 IST=> training   4.04% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.355 DataTime=0.255 Loss=1.538 Prec@1=62.912 Prec@5=84.504 rate=3.02 Hz, eta=0:13:15, total=0:00:33, wall=11:44 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.355 DataTime=0.255 Loss=1.538 Prec@1=62.912 Prec@5=84.504 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=11:44 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.355 DataTime=0.255 Loss=1.538 Prec@1=62.912 Prec@5=84.504 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=11:45 IST=> training   8.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.354 DataTime=0.253 Loss=1.539 Prec@1=62.933 Prec@5=84.459 rate=3.02 Hz, eta=0:12:43, total=0:01:06, wall=11:45 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.354 DataTime=0.253 Loss=1.539 Prec@1=62.933 Prec@5=84.459 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=11:45 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.354 DataTime=0.253 Loss=1.539 Prec@1=62.933 Prec@5=84.459 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=11:45 IST=> training   12.03% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.351 DataTime=0.248 Loss=1.542 Prec@1=62.844 Prec@5=84.434 rate=2.96 Hz, eta=0:12:23, total=0:01:41, wall=11:45 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.351 DataTime=0.248 Loss=1.542 Prec@1=62.844 Prec@5=84.434 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=11:45 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.351 DataTime=0.248 Loss=1.542 Prec@1=62.844 Prec@5=84.434 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=11:46 IST=> training   16.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.244 Loss=1.542 Prec@1=62.886 Prec@5=84.384 rate=2.95 Hz, eta=0:11:52, total=0:02:15, wall=11:46 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.244 Loss=1.542 Prec@1=62.886 Prec@5=84.384 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=11:46 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.244 Loss=1.542 Prec@1=62.886 Prec@5=84.384 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=11:46 IST=> training   20.02% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.245 Loss=1.542 Prec@1=62.904 Prec@5=84.411 rate=2.96 Hz, eta=0:11:17, total=0:02:49, wall=11:46 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.245 Loss=1.542 Prec@1=62.904 Prec@5=84.411 rate=2.94 Hz, eta=0:10:47, total=0:03:24, wall=11:46 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.348 DataTime=0.245 Loss=1.542 Prec@1=62.904 Prec@5=84.411 rate=2.94 Hz, eta=0:10:47, total=0:03:24, wall=11:47 IST=> training   24.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.347 DataTime=0.243 Loss=1.541 Prec@1=62.943 Prec@5=84.428 rate=2.94 Hz, eta=0:10:47, total=0:03:24, wall=11:47 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.347 DataTime=0.243 Loss=1.541 Prec@1=62.943 Prec@5=84.428 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=11:47 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.347 DataTime=0.243 Loss=1.541 Prec@1=62.943 Prec@5=84.428 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=11:48 IST=> training   28.01% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.239 Loss=1.541 Prec@1=62.923 Prec@5=84.419 rate=2.94 Hz, eta=0:10:12, total=0:03:58, wall=11:48 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.239 Loss=1.541 Prec@1=62.923 Prec@5=84.419 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=11:48 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.239 Loss=1.541 Prec@1=62.923 Prec@5=84.419 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=11:48 IST=> training   32.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.346 DataTime=0.240 Loss=1.541 Prec@1=62.926 Prec@5=84.428 rate=2.95 Hz, eta=0:09:36, total=0:04:31, wall=11:48 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.346 DataTime=0.240 Loss=1.541 Prec@1=62.926 Prec@5=84.428 rate=2.94 Hz, eta=0:09:05, total=0:05:06, wall=11:48 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.346 DataTime=0.240 Loss=1.541 Prec@1=62.926 Prec@5=84.428 rate=2.94 Hz, eta=0:09:05, total=0:05:06, wall=11:49 IST=> training   36.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.238 Loss=1.540 Prec@1=62.941 Prec@5=84.448 rate=2.94 Hz, eta=0:09:05, total=0:05:06, wall=11:49 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.238 Loss=1.540 Prec@1=62.941 Prec@5=84.448 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:49 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.238 Loss=1.540 Prec@1=62.941 Prec@5=84.448 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:49 IST=> training   39.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.541 Prec@1=62.942 Prec@5=84.444 rate=2.95 Hz, eta=0:08:29, total=0:05:39, wall=11:49 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.541 Prec@1=62.942 Prec@5=84.444 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=11:49 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.541 Prec@1=62.942 Prec@5=84.444 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=11:50 IST=> training   43.99% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.540 Prec@1=62.964 Prec@5=84.447 rate=2.95 Hz, eta=0:07:55, total=0:06:13, wall=11:50 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.540 Prec@1=62.964 Prec@5=84.447 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=11:50 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.237 Loss=1.540 Prec@1=62.964 Prec@5=84.447 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=11:50 IST=> training   47.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.983 Prec@5=84.438 rate=2.95 Hz, eta=0:07:21, total=0:06:46, wall=11:50 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.983 Prec@5=84.438 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:50 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.236 Loss=1.541 Prec@1=62.983 Prec@5=84.438 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:51 IST=> training   51.98% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.341 DataTime=0.235 Loss=1.540 Prec@1=63.002 Prec@5=84.457 rate=2.95 Hz, eta=0:06:47, total=0:07:20, wall=11:51 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.341 DataTime=0.235 Loss=1.540 Prec@1=63.002 Prec@5=84.457 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=11:51 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.341 DataTime=0.235 Loss=1.540 Prec@1=63.002 Prec@5=84.457 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=11:52 IST=> training   55.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.035 Prec@5=84.474 rate=2.96 Hz, eta=0:06:12, total=0:07:53, wall=11:52 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.035 Prec@5=84.474 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=11:52 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.035 Prec@5=84.474 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=11:52 IST=> training   59.97% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.015 Prec@5=84.463 rate=2.95 Hz, eta=0:05:40, total=0:08:29, wall=11:52 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.015 Prec@5=84.463 rate=2.94 Hz, eta=0:05:06, total=0:09:03, wall=11:52 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.539 Prec@1=63.015 Prec@5=84.463 rate=2.94 Hz, eta=0:05:06, total=0:09:03, wall=11:53 IST=> training   63.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.538 Prec@1=63.033 Prec@5=84.481 rate=2.94 Hz, eta=0:05:06, total=0:09:03, wall=11:53 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.538 Prec@1=63.033 Prec@5=84.481 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=11:53 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.236 Loss=1.538 Prec@1=63.033 Prec@5=84.481 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=11:53 IST=> training   67.96% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.021 Prec@5=84.474 rate=2.94 Hz, eta=0:04:32, total=0:09:37, wall=11:53 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.021 Prec@5=84.474 rate=2.94 Hz, eta=0:03:58, total=0:10:11, wall=11:53 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.235 Loss=1.538 Prec@1=63.021 Prec@5=84.474 rate=2.94 Hz, eta=0:03:58, total=0:10:11, wall=11:54 IST=> training   71.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.016 Prec@5=84.461 rate=2.94 Hz, eta=0:03:58, total=0:10:11, wall=11:54 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.016 Prec@5=84.461 rate=2.94 Hz, eta=0:03:24, total=0:10:46, wall=11:54 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.016 Prec@5=84.461 rate=2.94 Hz, eta=0:03:24, total=0:10:46, wall=11:54 IST=> training   75.95% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.017 Prec@5=84.468 rate=2.94 Hz, eta=0:03:24, total=0:10:46, wall=11:54 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.017 Prec@5=84.468 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=11:54 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.539 Prec@1=63.017 Prec@5=84.468 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=11:55 IST=> training   79.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.018 Prec@5=84.478 rate=2.94 Hz, eta=0:02:50, total=0:11:20, wall=11:55 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.018 Prec@5=84.478 rate=2.94 Hz, eta=0:02:16, total=0:11:53, wall=11:55 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.342 DataTime=0.234 Loss=1.538 Prec@1=63.018 Prec@5=84.478 rate=2.94 Hz, eta=0:02:16, total=0:11:53, wall=11:56 IST=> training   83.94% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.022 Prec@5=84.476 rate=2.94 Hz, eta=0:02:16, total=0:11:53, wall=11:56 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.022 Prec@5=84.476 rate=2.93 Hz, eta=0:01:42, total=0:12:30, wall=11:56 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.022 Prec@5=84.476 rate=2.93 Hz, eta=0:01:42, total=0:12:30, wall=11:56 IST=> training   87.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.010 Prec@5=84.477 rate=2.93 Hz, eta=0:01:42, total=0:12:30, wall=11:56 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.010 Prec@5=84.477 rate=2.93 Hz, eta=0:01:08, total=0:13:05, wall=11:56 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.010 Prec@5=84.477 rate=2.93 Hz, eta=0:01:08, total=0:13:05, wall=11:57 IST=> training   91.93% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.017 Prec@5=84.486 rate=2.93 Hz, eta=0:01:08, total=0:13:05, wall=11:57 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.017 Prec@5=84.486 rate=2.93 Hz, eta=0:00:34, total=0:13:39, wall=11:57 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.017 Prec@5=84.486 rate=2.93 Hz, eta=0:00:34, total=0:13:39, wall=11:57 IST=> training   95.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.013 Prec@5=84.477 rate=2.93 Hz, eta=0:00:34, total=0:13:39, wall=11:57 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.013 Prec@5=84.477 rate=2.93 Hz, eta=0:00:00, total=0:14:14, wall=11:57 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.343 DataTime=0.235 Loss=1.538 Prec@1=63.013 Prec@5=84.477 rate=2.93 Hz, eta=0:00:00, total=0:14:14, wall=11:57 IST=> training   99.92% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.234 Loss=1.538 Prec@1=63.012 Prec@5=84.478 rate=2.93 Hz, eta=0:00:00, total=0:14:14, wall=11:57 IST=> training   100.00% of 1x2503...Epoch=149/150 LR=0.00004 Time=0.344 DataTime=0.234 Loss=1.538 Prec@1=63.012 Prec@5=84.478 rate=2.93 Hz, eta=0:00:00, total=0:14:14, wall=11:57 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:57 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:57 IST=> validation 0.00% of 1x98...Epoch=149/150 LR=0.00004 Time=10.868 Loss=1.707 Prec@1=59.766 Prec@5=81.445 rate=0 Hz, eta=?, total=0:00:00, wall=11:57 IST=> validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=10.868 Loss=1.707 Prec@1=59.766 Prec@5=81.445 rate=5762.79 Hz, eta=0:00:00, total=0:00:00, wall=11:57 IST** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=10.868 Loss=1.707 Prec@1=59.766 Prec@5=81.445 rate=5762.79 Hz, eta=0:00:00, total=0:00:00, wall=11:58 IST** validation 1.02% of 1x98...Epoch=149/150 LR=0.00004 Time=0.449 Loss=1.549 Prec@1=62.666 Prec@5=84.580 rate=5762.79 Hz, eta=0:00:00, total=0:00:00, wall=11:58 IST** validation 100.00% of 1x98...Epoch=149/150 LR=0.00004 Time=0.449 Loss=1.549 Prec@1=62.666 Prec@5=84.580 rate=2.96 Hz, eta=0:00:00, total=0:00:33, wall=11:58 IST
[39m[37m=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:58 IST=> training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:58 IST=> training   0.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.986 DataTime=4.822 Loss=1.576 Prec@1=64.258 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=11:58 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.986 DataTime=4.822 Loss=1.576 Prec@1=64.258 Prec@5=83.789 rate=4514.59 Hz, eta=0:00:00, total=0:00:00, wall=11:58 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=4.986 DataTime=4.822 Loss=1.576 Prec@1=64.258 Prec@5=83.789 rate=4514.59 Hz, eta=0:00:00, total=0:00:00, wall=11:59 IST=> training   0.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.383 DataTime=0.288 Loss=1.532 Prec@1=63.040 Prec@5=84.700 rate=4514.59 Hz, eta=0:00:00, total=0:00:00, wall=11:59 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.383 DataTime=0.288 Loss=1.532 Prec@1=63.040 Prec@5=84.700 rate=3.00 Hz, eta=0:13:21, total=0:00:33, wall=11:59 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.383 DataTime=0.288 Loss=1.532 Prec@1=63.040 Prec@5=84.700 rate=3.00 Hz, eta=0:13:21, total=0:00:33, wall=11:59 IST=> training   4.04% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.362 DataTime=0.262 Loss=1.537 Prec@1=62.973 Prec@5=84.618 rate=3.00 Hz, eta=0:13:21, total=0:00:33, wall=11:59 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.362 DataTime=0.262 Loss=1.537 Prec@1=62.973 Prec@5=84.618 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=11:59 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.362 DataTime=0.262 Loss=1.537 Prec@1=62.973 Prec@5=84.618 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=12:00 IST=> training   8.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.356 DataTime=0.253 Loss=1.537 Prec@1=62.985 Prec@5=84.575 rate=2.97 Hz, eta=0:12:56, total=0:01:07, wall=12:00 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.356 DataTime=0.253 Loss=1.537 Prec@1=62.985 Prec@5=84.575 rate=2.95 Hz, eta=0:12:26, total=0:01:42, wall=12:00 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.356 DataTime=0.253 Loss=1.537 Prec@1=62.985 Prec@5=84.575 rate=2.95 Hz, eta=0:12:26, total=0:01:42, wall=12:00 IST=> training   12.03% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.353 DataTime=0.249 Loss=1.537 Prec@1=63.005 Prec@5=84.549 rate=2.95 Hz, eta=0:12:26, total=0:01:42, wall=12:00 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.353 DataTime=0.249 Loss=1.537 Prec@1=63.005 Prec@5=84.549 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=12:00 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.353 DataTime=0.249 Loss=1.537 Prec@1=63.005 Prec@5=84.549 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=12:01 IST=> training   16.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.351 DataTime=0.246 Loss=1.539 Prec@1=62.974 Prec@5=84.516 rate=2.94 Hz, eta=0:11:54, total=0:02:16, wall=12:01 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.351 DataTime=0.246 Loss=1.539 Prec@1=62.974 Prec@5=84.516 rate=2.93 Hz, eta=0:11:22, total=0:02:50, wall=12:01 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.351 DataTime=0.246 Loss=1.539 Prec@1=62.974 Prec@5=84.516 rate=2.93 Hz, eta=0:11:22, total=0:02:50, wall=12:02 IST=> training   20.02% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.538 Prec@1=63.006 Prec@5=84.522 rate=2.93 Hz, eta=0:11:22, total=0:02:50, wall=12:02 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.538 Prec@1=63.006 Prec@5=84.522 rate=2.94 Hz, eta=0:10:45, total=0:03:24, wall=12:02 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.538 Prec@1=63.006 Prec@5=84.522 rate=2.94 Hz, eta=0:10:45, total=0:03:24, wall=12:02 IST=> training   24.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.537 Prec@1=63.027 Prec@5=84.517 rate=2.94 Hz, eta=0:10:45, total=0:03:24, wall=12:02 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.537 Prec@1=63.027 Prec@5=84.517 rate=2.93 Hz, eta=0:10:14, total=0:03:58, wall=12:02 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.243 Loss=1.537 Prec@1=63.027 Prec@5=84.517 rate=2.93 Hz, eta=0:10:14, total=0:03:58, wall=12:03 IST=> training   28.01% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.349 DataTime=0.243 Loss=1.538 Prec@1=63.034 Prec@5=84.484 rate=2.93 Hz, eta=0:10:14, total=0:03:58, wall=12:03 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.349 DataTime=0.243 Loss=1.538 Prec@1=63.034 Prec@5=84.484 rate=2.91 Hz, eta=0:09:44, total=0:04:34, wall=12:03 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.349 DataTime=0.243 Loss=1.538 Prec@1=63.034 Prec@5=84.484 rate=2.91 Hz, eta=0:09:44, total=0:04:34, wall=12:03 IST=> training   32.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.005 Prec@5=84.463 rate=2.91 Hz, eta=0:09:44, total=0:04:34, wall=12:03 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.005 Prec@5=84.463 rate=2.92 Hz, eta=0:09:08, total=0:05:08, wall=12:03 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.005 Prec@5=84.463 rate=2.92 Hz, eta=0:09:08, total=0:05:08, wall=12:04 IST=> training   36.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.240 Loss=1.539 Prec@1=63.010 Prec@5=84.470 rate=2.92 Hz, eta=0:09:08, total=0:05:08, wall=12:04 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.240 Loss=1.539 Prec@1=63.010 Prec@5=84.470 rate=2.92 Hz, eta=0:08:33, total=0:05:42, wall=12:04 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.240 Loss=1.539 Prec@1=63.010 Prec@5=84.470 rate=2.92 Hz, eta=0:08:33, total=0:05:42, wall=12:04 IST=> training   39.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.034 Prec@5=84.477 rate=2.92 Hz, eta=0:08:33, total=0:05:42, wall=12:04 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.034 Prec@5=84.477 rate=2.91 Hz, eta=0:08:01, total=0:06:18, wall=12:04 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.348 DataTime=0.241 Loss=1.539 Prec@1=63.034 Prec@5=84.477 rate=2.91 Hz, eta=0:08:01, total=0:06:18, wall=12:05 IST=> training   43.99% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.239 Loss=1.538 Prec@1=63.065 Prec@5=84.486 rate=2.91 Hz, eta=0:08:01, total=0:06:18, wall=12:05 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.239 Loss=1.538 Prec@1=63.065 Prec@5=84.486 rate=2.92 Hz, eta=0:07:26, total=0:06:51, wall=12:05 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.239 Loss=1.538 Prec@1=63.065 Prec@5=84.486 rate=2.92 Hz, eta=0:07:26, total=0:06:51, wall=12:06 IST=> training   47.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.238 Loss=1.537 Prec@1=63.069 Prec@5=84.486 rate=2.92 Hz, eta=0:07:26, total=0:06:51, wall=12:06 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.238 Loss=1.537 Prec@1=63.069 Prec@5=84.486 rate=2.92 Hz, eta=0:06:51, total=0:07:25, wall=12:06 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.238 Loss=1.537 Prec@1=63.069 Prec@5=84.486 rate=2.92 Hz, eta=0:06:51, total=0:07:25, wall=12:06 IST=> training   51.98% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.238 Loss=1.538 Prec@1=63.048 Prec@5=84.476 rate=2.92 Hz, eta=0:06:51, total=0:07:25, wall=12:06 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.238 Loss=1.538 Prec@1=63.048 Prec@5=84.476 rate=2.91 Hz, eta=0:06:18, total=0:08:00, wall=12:06 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.347 DataTime=0.238 Loss=1.538 Prec@1=63.048 Prec@5=84.476 rate=2.91 Hz, eta=0:06:18, total=0:08:00, wall=12:07 IST=> training   55.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.486 rate=2.91 Hz, eta=0:06:18, total=0:08:00, wall=12:07 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.486 rate=2.92 Hz, eta=0:05:43, total=0:08:33, wall=12:07 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.486 rate=2.92 Hz, eta=0:05:43, total=0:08:33, wall=12:07 IST=> training   59.97% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.539 Prec@1=63.025 Prec@5=84.470 rate=2.92 Hz, eta=0:05:43, total=0:08:33, wall=12:07 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.539 Prec@1=63.025 Prec@5=84.470 rate=2.92 Hz, eta=0:05:08, total=0:09:07, wall=12:07 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.539 Prec@1=63.025 Prec@5=84.470 rate=2.92 Hz, eta=0:05:08, total=0:09:07, wall=12:08 IST=> training   63.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.031 Prec@5=84.473 rate=2.92 Hz, eta=0:05:08, total=0:09:07, wall=12:08 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.031 Prec@5=84.473 rate=2.92 Hz, eta=0:04:34, total=0:09:43, wall=12:08 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.346 DataTime=0.237 Loss=1.538 Prec@1=63.031 Prec@5=84.473 rate=2.92 Hz, eta=0:04:34, total=0:09:43, wall=12:08 IST=> training   67.96% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.469 rate=2.92 Hz, eta=0:04:34, total=0:09:43, wall=12:08 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.469 rate=2.92 Hz, eta=0:04:00, total=0:10:16, wall=12:08 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.237 Loss=1.538 Prec@1=63.032 Prec@5=84.469 rate=2.92 Hz, eta=0:04:00, total=0:10:16, wall=12:09 IST=> training   71.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.471 rate=2.92 Hz, eta=0:04:00, total=0:10:16, wall=12:09 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.471 rate=2.92 Hz, eta=0:03:25, total=0:10:50, wall=12:09 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.471 rate=2.92 Hz, eta=0:03:25, total=0:10:50, wall=12:10 IST=> training   75.95% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.034 Prec@5=84.477 rate=2.92 Hz, eta=0:03:25, total=0:10:50, wall=12:10 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.034 Prec@5=84.477 rate=2.92 Hz, eta=0:02:51, total=0:11:25, wall=12:10 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.034 Prec@5=84.477 rate=2.92 Hz, eta=0:02:51, total=0:11:25, wall=12:10 IST=> training   79.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.477 rate=2.92 Hz, eta=0:02:51, total=0:11:25, wall=12:10 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.477 rate=2.92 Hz, eta=0:02:17, total=0:11:59, wall=12:10 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.538 Prec@1=63.037 Prec@5=84.477 rate=2.92 Hz, eta=0:02:17, total=0:11:59, wall=12:11 IST=> training   83.94% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.538 Prec@1=63.040 Prec@5=84.483 rate=2.92 Hz, eta=0:02:17, total=0:11:59, wall=12:11 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.538 Prec@1=63.040 Prec@5=84.483 rate=2.93 Hz, eta=0:01:43, total=0:12:32, wall=12:11 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.538 Prec@1=63.040 Prec@5=84.483 rate=2.93 Hz, eta=0:01:43, total=0:12:32, wall=12:11 IST=> training   87.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.537 Prec@1=63.041 Prec@5=84.488 rate=2.93 Hz, eta=0:01:43, total=0:12:32, wall=12:11 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.537 Prec@1=63.041 Prec@5=84.488 rate=2.92 Hz, eta=0:01:09, total=0:13:07, wall=12:11 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.345 DataTime=0.236 Loss=1.537 Prec@1=63.041 Prec@5=84.488 rate=2.92 Hz, eta=0:01:09, total=0:13:07, wall=12:12 IST=> training   91.93% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.053 Prec@5=84.489 rate=2.92 Hz, eta=0:01:09, total=0:13:07, wall=12:12 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.053 Prec@5=84.489 rate=2.92 Hz, eta=0:00:34, total=0:13:41, wall=12:12 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.344 DataTime=0.235 Loss=1.537 Prec@1=63.053 Prec@5=84.489 rate=2.92 Hz, eta=0:00:34, total=0:13:41, wall=12:12 IST=> training   95.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.043 Prec@5=84.492 rate=2.92 Hz, eta=0:00:34, total=0:13:41, wall=12:12 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.043 Prec@5=84.492 rate=2.93 Hz, eta=0:00:00, total=0:14:13, wall=12:12 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.343 DataTime=0.235 Loss=1.537 Prec@1=63.043 Prec@5=84.492 rate=2.93 Hz, eta=0:00:00, total=0:14:13, wall=12:12 IST=> training   99.92% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.343 DataTime=0.234 Loss=1.537 Prec@1=63.044 Prec@5=84.493 rate=2.93 Hz, eta=0:00:00, total=0:14:13, wall=12:12 IST=> training   100.00% of 1x2503...Epoch=150/150 LR=0.00001 Time=0.343 DataTime=0.234 Loss=1.537 Prec@1=63.044 Prec@5=84.493 rate=2.93 Hz, eta=0:00:00, total=0:14:14, wall=12:12 IST
[39m[32m=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:12 IST=> validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:12 IST=> validation 0.00% of 1x98...Epoch=150/150 LR=0.00001 Time=6.947 Loss=1.622 Prec@1=60.352 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=12:12 IST=> validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=6.947 Loss=1.622 Prec@1=60.352 Prec@5=83.203 rate=1494.86 Hz, eta=0:00:00, total=0:00:00, wall=12:12 IST** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=6.947 Loss=1.622 Prec@1=60.352 Prec@5=83.203 rate=1494.86 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST** validation 1.02% of 1x98...Epoch=150/150 LR=0.00001 Time=0.408 Loss=1.548 Prec@1=62.688 Prec@5=84.588 rate=1494.86 Hz, eta=0:00:00, total=0:00:00, wall=12:13 IST** validation 100.00% of 1x98...Epoch=150/150 LR=0.00001 Time=0.408 Loss=1.548 Prec@1=62.688 Prec@5=84.588 rate=2.97 Hz, eta=0:00:00, total=0:00:33, wall=12:13 IST
[39m