[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'num_classes': 1000, 'strides': (2, 2, 2, 2, 2)}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv1_x1', 'dataset_name': 'image_folder_classification', 'workers': 8, 'logger': <modules.xtensor.xnn.utils.logger.TeeLogger object at 0x7f4e2f96c048>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': 0.0001, 'print_freq': 100, 'resume': '', 'evaluate': False, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'date': None, 'save_path': './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224', 'generate_onnx': False, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': [0.485, 0.456, 0.406], 'image_scale': [4.366812227074235, 4.464285714285714, 4.444444444444445], 'quantize': False, 'model_surgery': None, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'channelwise_q': False, 'bias_calibration': False, 'solver': 'sgd', 'num_inputs': 1, 'distributed': False}
=> creating model 'mobilenetv1_x1'
=> feature size is:  torch.Size([1, 1024, 7, 7])
=> Size = 224, GFLOPs = 1.135432704, GMACs = 0.567716352
MobileNetV1(
  (classifier): Sequential(
    (0): BypassBlock()
    (1): Linear(in_features=1024, out_features=1000, bias=True)
  )
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
    )
    (1): Sequential(
      (0): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (2): Sequential(
      (0): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (3): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (4): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (5): Sequential(
      (0): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (6): Sequential(
      (0): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (7): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (8): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (9): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (10): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (11): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (12): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (13): Sequential(
      (0): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'num_classes': 1000, 'strides': (2, 2, 2, 2, 2)}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv1_x1', 'dataset_name': 'image_folder_classification', 'workers': 8, 'logger': <modules.xtensor.xnn.utils.logger.TeeLogger object at 0x7f4e2f96c048>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': 0.0001, 'print_freq': 100, 'resume': '', 'evaluate': False, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'date': None, 'save_path': './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224', 'generate_onnx': False, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': [0.485, 0.456, 0.406], 'image_scale': [4.366812227074235, 4.464285714285714, 4.444444444444445], 'quantize': False, 'model_surgery': None, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'channelwise_q': False, 'bias_calibration': False, 'solver': 'sgd', 'num_inputs': 1, 'distributed': False}
=> optimizer type   : sgd
=> learning rate    : 0.1
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 150
[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:25 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:25 IST
=> Validation 0.00% of 1x98...Epoch=1/150 LR=0.0200 Time=13.395 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=0 Hz, eta=?, total=0:00:00, wall=15:25 IST
=> Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=13.395 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=3397.72 Hz, eta=0:00:00, total=0:00:00, wall=15:25 IST
** Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=13.395 Loss=6.908 Prec@1=0.000 Prec@5=0.000 rate=3397.72 Hz, eta=0:00:00, total=0:00:00, wall=15:26 IST
** Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=0.622 Loss=6.908 Prec@1=0.100 Prec@5=0.488 rate=3397.72 Hz, eta=0:00:00, total=0:00:00, wall=15:26 IST
** Validation 100.00% of 1x98...Epoch=1/150 LR=0.0200 Time=0.622 Loss=6.908 Prec@1=0.100 Prec@5=0.488 rate=2.06 Hz, eta=0:00:00, total=0:00:47, wall=15:26 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:26 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:26 IST
=> Training   0.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=6.601 DataTime=3.829 Loss=6.921 Prec@1=0.000 Prec@5=0.586 rate=0 Hz, eta=?, total=0:00:00, wall=15:26 IST
=> Training   0.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=6.601 DataTime=3.829 Loss=6.921 Prec@1=0.000 Prec@5=0.586 rate=3234.49 Hz, eta=0:00:00, total=0:00:00, wall=15:26 IST
=> Training   0.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=6.601 DataTime=3.829 Loss=6.921 Prec@1=0.000 Prec@5=0.586 rate=3234.49 Hz, eta=0:00:00, total=0:00:00, wall=15:27 IST
=> Training   0.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.585 DataTime=0.400 Loss=6.874 Prec@1=0.215 Prec@5=1.033 rate=3234.49 Hz, eta=0:00:00, total=0:00:00, wall=15:27 IST
=> Training   4.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.585 DataTime=0.400 Loss=6.874 Prec@1=0.215 Prec@5=1.033 rate=1.92 Hz, eta=0:20:52, total=0:00:52, wall=15:27 IST
=> Training   4.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.585 DataTime=0.400 Loss=6.874 Prec@1=0.215 Prec@5=1.033 rate=1.92 Hz, eta=0:20:52, total=0:00:52, wall=15:28 IST
=> Training   4.04% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.566 DataTime=0.393 Loss=6.702 Prec@1=0.537 Prec@5=2.195 rate=1.92 Hz, eta=0:20:52, total=0:00:52, wall=15:28 IST
=> Training   8.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.566 DataTime=0.393 Loss=6.702 Prec@1=0.537 Prec@5=2.195 rate=1.87 Hz, eta=0:20:28, total=0:01:47, wall=15:28 IST
=> Training   8.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.566 DataTime=0.393 Loss=6.702 Prec@1=0.537 Prec@5=2.195 rate=1.87 Hz, eta=0:20:28, total=0:01:47, wall=15:29 IST
=> Training   8.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.560 DataTime=0.390 Loss=6.544 Prec@1=0.899 Prec@5=3.466 rate=1.87 Hz, eta=0:20:28, total=0:01:47, wall=15:29 IST
=> Training   12.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.560 DataTime=0.390 Loss=6.544 Prec@1=0.899 Prec@5=3.466 rate=1.86 Hz, eta=0:19:46, total=0:02:42, wall=15:29 IST
=> Training   12.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.560 DataTime=0.390 Loss=6.544 Prec@1=0.899 Prec@5=3.466 rate=1.86 Hz, eta=0:19:46, total=0:02:42, wall=15:30 IST
=> Training   12.03% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.389 Loss=6.394 Prec@1=1.350 Prec@5=4.861 rate=1.86 Hz, eta=0:19:46, total=0:02:42, wall=15:30 IST
=> Training   16.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.389 Loss=6.394 Prec@1=1.350 Prec@5=4.861 rate=1.85 Hz, eta=0:18:57, total=0:03:36, wall=15:30 IST
=> Training   16.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.389 Loss=6.394 Prec@1=1.350 Prec@5=4.861 rate=1.85 Hz, eta=0:18:57, total=0:03:36, wall=15:31 IST
=> Training   16.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.258 Prec@1=1.837 Prec@5=6.265 rate=1.85 Hz, eta=0:18:57, total=0:03:36, wall=15:31 IST
=> Training   20.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.258 Prec@1=1.837 Prec@5=6.265 rate=1.83 Hz, eta=0:18:11, total=0:04:33, wall=15:31 IST
=> Training   20.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.258 Prec@1=1.837 Prec@5=6.265 rate=1.83 Hz, eta=0:18:11, total=0:04:33, wall=15:32 IST
=> Training   20.02% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.390 Loss=6.134 Prec@1=2.328 Prec@5=7.694 rate=1.83 Hz, eta=0:18:11, total=0:04:33, wall=15:32 IST
=> Training   24.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.390 Loss=6.134 Prec@1=2.328 Prec@5=7.694 rate=1.83 Hz, eta=0:17:20, total=0:05:28, wall=15:32 IST
=> Training   24.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.390 Loss=6.134 Prec@1=2.328 Prec@5=7.694 rate=1.83 Hz, eta=0:17:20, total=0:05:28, wall=15:33 IST
=> Training   24.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.019 Prec@1=2.879 Prec@5=9.126 rate=1.83 Hz, eta=0:17:20, total=0:05:28, wall=15:33 IST
=> Training   28.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.019 Prec@1=2.879 Prec@5=9.126 rate=1.82 Hz, eta=0:16:29, total=0:06:25, wall=15:33 IST
=> Training   28.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.391 Loss=6.019 Prec@1=2.879 Prec@5=9.126 rate=1.82 Hz, eta=0:16:29, total=0:06:25, wall=15:34 IST
=> Training   28.01% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.389 Loss=5.913 Prec@1=3.453 Prec@5=10.519 rate=1.82 Hz, eta=0:16:29, total=0:06:25, wall=15:34 IST
=> Training   32.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.389 Loss=5.913 Prec@1=3.453 Prec@5=10.519 rate=1.82 Hz, eta=0:15:35, total=0:07:20, wall=15:34 IST
=> Training   32.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.558 DataTime=0.389 Loss=5.913 Prec@1=3.453 Prec@5=10.519 rate=1.82 Hz, eta=0:15:35, total=0:07:20, wall=15:35 IST
=> Training   32.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.388 Loss=5.815 Prec@1=4.002 Prec@5=11.873 rate=1.82 Hz, eta=0:15:35, total=0:07:20, wall=15:35 IST
=> Training   36.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.388 Loss=5.815 Prec@1=4.002 Prec@5=11.873 rate=1.82 Hz, eta=0:14:41, total=0:08:15, wall=15:35 IST
=> Training   36.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.388 Loss=5.815 Prec@1=4.002 Prec@5=11.873 rate=1.82 Hz, eta=0:14:41, total=0:08:15, wall=15:36 IST
=> Training   36.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.725 Prec@1=4.563 Prec@5=13.169 rate=1.82 Hz, eta=0:14:41, total=0:08:15, wall=15:36 IST
=> Training   39.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.725 Prec@1=4.563 Prec@5=13.169 rate=1.82 Hz, eta=0:13:45, total=0:09:10, wall=15:36 IST
=> Training   39.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.725 Prec@1=4.563 Prec@5=13.169 rate=1.82 Hz, eta=0:13:45, total=0:09:10, wall=15:36 IST
=> Training   39.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.640 Prec@1=5.141 Prec@5=14.449 rate=1.82 Hz, eta=0:13:45, total=0:09:10, wall=15:36 IST
=> Training   43.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.640 Prec@1=5.141 Prec@5=14.449 rate=1.82 Hz, eta=0:12:51, total=0:10:05, wall=15:36 IST
=> Training   43.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.386 Loss=5.640 Prec@1=5.141 Prec@5=14.449 rate=1.82 Hz, eta=0:12:51, total=0:10:05, wall=15:37 IST
=> Training   43.99% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.385 Loss=5.560 Prec@1=5.706 Prec@5=15.685 rate=1.82 Hz, eta=0:12:51, total=0:10:05, wall=15:37 IST
=> Training   47.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.385 Loss=5.560 Prec@1=5.706 Prec@5=15.685 rate=1.82 Hz, eta=0:11:56, total=0:11:01, wall=15:37 IST
=> Training   47.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.556 DataTime=0.385 Loss=5.560 Prec@1=5.706 Prec@5=15.685 rate=1.82 Hz, eta=0:11:56, total=0:11:01, wall=15:38 IST
=> Training   47.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.485 Prec@1=6.292 Prec@5=16.897 rate=1.82 Hz, eta=0:11:56, total=0:11:01, wall=15:38 IST
=> Training   51.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.485 Prec@1=6.292 Prec@5=16.897 rate=1.81 Hz, eta=0:11:03, total=0:11:57, wall=15:38 IST
=> Training   51.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.485 Prec@1=6.292 Prec@5=16.897 rate=1.81 Hz, eta=0:11:03, total=0:11:57, wall=15:39 IST
=> Training   51.98% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.413 Prec@1=6.850 Prec@5=18.047 rate=1.81 Hz, eta=0:11:03, total=0:11:57, wall=15:39 IST
=> Training   55.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.413 Prec@1=6.850 Prec@5=18.047 rate=1.81 Hz, eta=0:10:08, total=0:12:53, wall=15:39 IST
=> Training   55.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.557 DataTime=0.386 Loss=5.413 Prec@1=6.850 Prec@5=18.047 rate=1.81 Hz, eta=0:10:08, total=0:12:53, wall=15:40 IST
=> Training   55.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.559 DataTime=0.387 Loss=5.345 Prec@1=7.419 Prec@5=19.169 rate=1.81 Hz, eta=0:10:08, total=0:12:53, wall=15:40 IST
=> Training   59.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.559 DataTime=0.387 Loss=5.345 Prec@1=7.419 Prec@5=19.169 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=15:40 IST
=> Training   59.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.559 DataTime=0.387 Loss=5.345 Prec@1=7.419 Prec@5=19.169 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=15:41 IST
=> Training   59.97% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.389 Loss=5.279 Prec@1=7.979 Prec@5=20.263 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=15:41 IST
=> Training   63.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.389 Loss=5.279 Prec@1=7.979 Prec@5=20.263 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=15:41 IST
=> Training   63.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.389 Loss=5.279 Prec@1=7.979 Prec@5=20.263 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=15:42 IST
=> Training   63.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.390 Loss=5.217 Prec@1=8.540 Prec@5=21.322 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=15:42 IST
=> Training   67.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.390 Loss=5.217 Prec@1=8.540 Prec@5=21.322 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=15:42 IST
=> Training   67.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.561 DataTime=0.390 Loss=5.217 Prec@1=8.540 Prec@5=21.322 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=15:43 IST
=> Training   67.96% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.562 DataTime=0.391 Loss=5.157 Prec@1=9.089 Prec@5=22.345 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=15:43 IST
=> Training   71.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.562 DataTime=0.391 Loss=5.157 Prec@1=9.089 Prec@5=22.345 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=15:43 IST
=> Training   71.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.562 DataTime=0.391 Loss=5.157 Prec@1=9.089 Prec@5=22.345 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=15:44 IST
=> Training   71.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.101 Prec@1=9.623 Prec@5=23.339 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=15:44 IST
=> Training   75.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.101 Prec@1=9.623 Prec@5=23.339 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:44 IST
=> Training   75.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.101 Prec@1=9.623 Prec@5=23.339 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:45 IST
=> Training   75.95% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.047 Prec@1=10.137 Prec@5=24.267 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:45 IST
=> Training   79.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.047 Prec@1=10.137 Prec@5=24.267 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=15:45 IST
=> Training   79.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=5.047 Prec@1=10.137 Prec@5=24.267 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=15:46 IST
=> Training   79.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.393 Loss=4.995 Prec@1=10.641 Prec@5=25.167 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=15:46 IST
=> Training   83.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.393 Loss=4.995 Prec@1=10.641 Prec@5=25.167 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:46 IST
=> Training   83.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.393 Loss=4.995 Prec@1=10.641 Prec@5=25.167 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:47 IST
=> Training   83.94% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.945 Prec@1=11.147 Prec@5=26.061 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:47 IST
=> Training   87.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.945 Prec@1=11.147 Prec@5=26.061 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=15:47 IST
=> Training   87.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.945 Prec@1=11.147 Prec@5=26.061 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=15:48 IST
=> Training   87.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.897 Prec@1=11.650 Prec@5=26.921 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=15:48 IST
=> Training   91.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.897 Prec@1=11.650 Prec@5=26.921 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=15:48 IST
=> Training   91.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.897 Prec@1=11.650 Prec@5=26.921 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=15:49 IST
=> Training   91.93% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.851 Prec@1=12.124 Prec@5=27.727 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=15:49 IST
=> Training   95.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.851 Prec@1=12.124 Prec@5=27.727 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=15:49 IST
=> Training   95.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.851 Prec@1=12.124 Prec@5=27.727 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=15:50 IST
=> Training   95.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.807 Prec@1=12.593 Prec@5=28.511 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=15:50 IST
=> Training   99.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.807 Prec@1=12.593 Prec@5=28.511 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=15:50 IST
=> Training   99.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.563 DataTime=0.392 Loss=4.807 Prec@1=12.593 Prec@5=28.511 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=15:50 IST
=> Training   99.92% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=4.807 Prec@1=12.596 Prec@5=28.520 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=15:50 IST
=> Training   100.00% of 1x2503...Epoch=1/150 LR=0.0200 Time=0.564 DataTime=0.392 Loss=4.807 Prec@1=12.596 Prec@5=28.520 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=15:50 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> Validation 0.00% of 1x98...Epoch=1/150 LR=0.0200 Time=5.807 Loss=3.441 Prec@1=26.367 Prec@5=56.445 rate=0 Hz, eta=?, total=0:00:00, wall=15:50 IST
=> Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=5.807 Loss=3.441 Prec@1=26.367 Prec@5=56.445 rate=2091.88 Hz, eta=0:00:00, total=0:00:00, wall=15:50 IST
** Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=5.807 Loss=3.441 Prec@1=26.367 Prec@5=56.445 rate=2091.88 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
** Validation 1.02% of 1x98...Epoch=1/150 LR=0.0200 Time=0.628 Loss=3.800 Prec@1=22.436 Prec@5=46.250 rate=2091.88 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
** Validation 100.00% of 1x98...Epoch=1/150 LR=0.0200 Time=0.628 Loss=3.800 Prec@1=22.436 Prec@5=46.250 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=15:51 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:51 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:51 IST
=> Training   0.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=5.294 DataTime=4.830 Loss=3.721 Prec@1=25.977 Prec@5=50.000 rate=0 Hz, eta=?, total=0:00:00, wall=15:51 IST
=> Training   0.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=5.294 DataTime=4.830 Loss=3.721 Prec@1=25.977 Prec@5=50.000 rate=7967.43 Hz, eta=0:00:00, total=0:00:00, wall=15:51 IST
=> Training   0.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=5.294 DataTime=4.830 Loss=3.721 Prec@1=25.977 Prec@5=50.000 rate=7967.43 Hz, eta=0:00:00, total=0:00:00, wall=15:52 IST
=> Training   0.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.594 DataTime=0.424 Loss=3.662 Prec@1=25.066 Prec@5=48.954 rate=7967.43 Hz, eta=0:00:00, total=0:00:00, wall=15:52 IST
=> Training   4.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.594 DataTime=0.424 Loss=3.662 Prec@1=25.066 Prec@5=48.954 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=15:52 IST
=> Training   4.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.594 DataTime=0.424 Loss=3.662 Prec@1=25.066 Prec@5=48.954 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=15:53 IST
=> Training   4.04% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.579 DataTime=0.410 Loss=3.646 Prec@1=25.368 Prec@5=49.278 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=15:53 IST
=> Training   8.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.579 DataTime=0.410 Loss=3.646 Prec@1=25.368 Prec@5=49.278 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=15:53 IST
=> Training   8.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.579 DataTime=0.410 Loss=3.646 Prec@1=25.368 Prec@5=49.278 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=15:54 IST
=> Training   8.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.569 DataTime=0.400 Loss=3.624 Prec@1=25.633 Prec@5=49.683 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=15:54 IST
=> Training   12.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.569 DataTime=0.400 Loss=3.624 Prec@1=25.633 Prec@5=49.683 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=15:54 IST
=> Training   12.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.569 DataTime=0.400 Loss=3.624 Prec@1=25.633 Prec@5=49.683 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=15:55 IST
=> Training   12.03% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.570 DataTime=0.400 Loss=3.608 Prec@1=25.883 Prec@5=49.954 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=15:55 IST
=> Training   16.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.570 DataTime=0.400 Loss=3.608 Prec@1=25.883 Prec@5=49.954 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=15:55 IST
=> Training   16.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.570 DataTime=0.400 Loss=3.608 Prec@1=25.883 Prec@5=49.954 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=15:56 IST
=> Training   16.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.589 Prec@1=26.232 Prec@5=50.329 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=15:56 IST
=> Training   20.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.589 Prec@1=26.232 Prec@5=50.329 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=15:56 IST
=> Training   20.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.589 Prec@1=26.232 Prec@5=50.329 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=15:56 IST
=> Training   20.02% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.571 Prec@1=26.456 Prec@5=50.684 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=15:56 IST
=> Training   24.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.571 Prec@1=26.456 Prec@5=50.684 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=15:56 IST
=> Training   24.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.565 DataTime=0.393 Loss=3.571 Prec@1=26.456 Prec@5=50.684 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=15:57 IST
=> Training   24.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.389 Loss=3.554 Prec@1=26.723 Prec@5=51.017 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=15:57 IST
=> Training   28.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.389 Loss=3.554 Prec@1=26.723 Prec@5=51.017 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=15:57 IST
=> Training   28.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.389 Loss=3.554 Prec@1=26.723 Prec@5=51.017 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=15:58 IST
=> Training   28.01% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.387 Loss=3.538 Prec@1=26.984 Prec@5=51.335 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=15:58 IST
=> Training   32.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.387 Loss=3.538 Prec@1=26.984 Prec@5=51.335 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=15:58 IST
=> Training   32.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.562 DataTime=0.387 Loss=3.538 Prec@1=26.984 Prec@5=51.335 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=15:59 IST
=> Training   32.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.560 DataTime=0.385 Loss=3.523 Prec@1=27.202 Prec@5=51.615 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=15:59 IST
=> Training   36.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.560 DataTime=0.385 Loss=3.523 Prec@1=27.202 Prec@5=51.615 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=15:59 IST
=> Training   36.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.560 DataTime=0.385 Loss=3.523 Prec@1=27.202 Prec@5=51.615 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=16:00 IST
=> Training   36.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.504 Prec@1=27.488 Prec@5=51.965 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=16:00 IST
=> Training   39.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.504 Prec@1=27.488 Prec@5=51.965 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=16:00 IST
=> Training   39.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.504 Prec@1=27.488 Prec@5=51.965 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=16:01 IST
=> Training   39.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.487 Prec@1=27.734 Prec@5=52.277 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=16:01 IST
=> Training   43.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.487 Prec@1=27.734 Prec@5=52.277 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=16:01 IST
=> Training   43.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.487 Prec@1=27.734 Prec@5=52.277 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=16:02 IST
=> Training   43.99% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.473 Prec@1=27.985 Prec@5=52.551 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=16:02 IST
=> Training   47.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.473 Prec@1=27.985 Prec@5=52.551 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:02 IST
=> Training   47.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.473 Prec@1=27.985 Prec@5=52.551 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:03 IST
=> Training   47.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.458 Prec@1=28.236 Prec@5=52.835 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:03 IST
=> Training   51.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.458 Prec@1=28.236 Prec@5=52.835 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=16:03 IST
=> Training   51.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.458 Prec@1=28.236 Prec@5=52.835 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=16:04 IST
=> Training   51.98% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.442 Prec@1=28.489 Prec@5=53.111 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=16:04 IST
=> Training   55.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.442 Prec@1=28.489 Prec@5=53.111 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=16:04 IST
=> Training   55.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.442 Prec@1=28.489 Prec@5=53.111 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=16:05 IST
=> Training   55.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.428 Prec@1=28.702 Prec@5=53.367 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=16:05 IST
=> Training   59.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.428 Prec@1=28.702 Prec@5=53.367 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=16:05 IST
=> Training   59.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.428 Prec@1=28.702 Prec@5=53.367 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=16:06 IST
=> Training   59.97% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.414 Prec@1=28.913 Prec@5=53.634 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=16:06 IST
=> Training   63.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.414 Prec@1=28.913 Prec@5=53.634 rate=1.80 Hz, eta=0:08:20, total=0:14:47, wall=16:06 IST
=> Training   63.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.414 Prec@1=28.913 Prec@5=53.634 rate=1.80 Hz, eta=0:08:20, total=0:14:47, wall=16:07 IST
=> Training   63.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.399 Prec@1=29.137 Prec@5=53.902 rate=1.80 Hz, eta=0:08:20, total=0:14:47, wall=16:07 IST
=> Training   67.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.399 Prec@1=29.137 Prec@5=53.902 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=16:07 IST
=> Training   67.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.557 DataTime=0.383 Loss=3.399 Prec@1=29.137 Prec@5=53.902 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=16:08 IST
=> Training   67.96% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.385 Prec@1=29.360 Prec@5=54.150 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=16:08 IST
=> Training   71.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.385 Prec@1=29.360 Prec@5=54.150 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:08 IST
=> Training   71.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.385 Prec@1=29.360 Prec@5=54.150 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:08 IST
=> Training   71.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.373 Prec@1=29.561 Prec@5=54.399 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:08 IST
=> Training   75.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.373 Prec@1=29.561 Prec@5=54.399 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=16:08 IST
=> Training   75.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.373 Prec@1=29.561 Prec@5=54.399 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=16:09 IST
=> Training   75.95% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.360 Prec@1=29.768 Prec@5=54.626 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=16:09 IST
=> Training   79.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.360 Prec@1=29.768 Prec@5=54.626 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=16:09 IST
=> Training   79.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.360 Prec@1=29.768 Prec@5=54.626 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=16:10 IST
=> Training   79.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.347 Prec@1=29.985 Prec@5=54.863 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=16:10 IST
=> Training   83.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.347 Prec@1=29.985 Prec@5=54.863 rate=1.80 Hz, eta=0:03:43, total=0:19:26, wall=16:10 IST
=> Training   83.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.347 Prec@1=29.985 Prec@5=54.863 rate=1.80 Hz, eta=0:03:43, total=0:19:26, wall=16:11 IST
=> Training   83.94% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.335 Prec@1=30.195 Prec@5=55.096 rate=1.80 Hz, eta=0:03:43, total=0:19:26, wall=16:11 IST
=> Training   87.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.335 Prec@1=30.195 Prec@5=55.096 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=16:11 IST
=> Training   87.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.335 Prec@1=30.195 Prec@5=55.096 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=16:12 IST
=> Training   87.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.322 Prec@1=30.398 Prec@5=55.316 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=16:12 IST
=> Training   91.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.322 Prec@1=30.398 Prec@5=55.316 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=16:12 IST
=> Training   91.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.385 Loss=3.322 Prec@1=30.398 Prec@5=55.316 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=16:13 IST
=> Training   91.93% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.310 Prec@1=30.614 Prec@5=55.547 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=16:13 IST
=> Training   95.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.310 Prec@1=30.614 Prec@5=55.547 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=16:13 IST
=> Training   95.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.559 DataTime=0.385 Loss=3.310 Prec@1=30.614 Prec@5=55.547 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=16:14 IST
=> Training   95.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.298 Prec@1=30.818 Prec@5=55.760 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=16:14 IST
=> Training   99.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.298 Prec@1=30.818 Prec@5=55.760 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=16:14 IST
=> Training   99.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.298 Prec@1=30.818 Prec@5=55.760 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=16:14 IST
=> Training   99.92% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.298 Prec@1=30.819 Prec@5=55.762 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=16:14 IST
=> Training   100.00% of 1x2503...Epoch=2/150 LR=0.0400 Time=0.558 DataTime=0.384 Loss=3.298 Prec@1=30.819 Prec@5=55.762 rate=1.80 Hz, eta=0:00:00, total=0:23:10, wall=16:14 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> Validation 0.00% of 1x98...Epoch=2/150 LR=0.0400 Time=5.894 Loss=2.620 Prec@1=41.992 Prec@5=71.484 rate=0 Hz, eta=?, total=0:00:00, wall=16:14 IST
=> Validation 1.02% of 1x98...Epoch=2/150 LR=0.0400 Time=5.894 Loss=2.620 Prec@1=41.992 Prec@5=71.484 rate=5775.97 Hz, eta=0:00:00, total=0:00:00, wall=16:14 IST
** Validation 1.02% of 1x98...Epoch=2/150 LR=0.0400 Time=5.894 Loss=2.620 Prec@1=41.992 Prec@5=71.484 rate=5775.97 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
** Validation 1.02% of 1x98...Epoch=2/150 LR=0.0400 Time=0.626 Loss=3.038 Prec@1=34.446 Prec@5=60.376 rate=5775.97 Hz, eta=0:00:00, total=0:00:00, wall=16:15 IST
** Validation 100.00% of 1x98...Epoch=2/150 LR=0.0400 Time=0.626 Loss=3.038 Prec@1=34.446 Prec@5=60.376 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=16:15 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> Training   0.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=4.606 DataTime=4.229 Loss=2.897 Prec@1=39.258 Prec@5=62.695 rate=0 Hz, eta=?, total=0:00:00, wall=16:15 IST
=> Training   0.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=4.606 DataTime=4.229 Loss=2.897 Prec@1=39.258 Prec@5=62.695 rate=1670.45 Hz, eta=0:00:01, total=0:00:00, wall=16:15 IST
=> Training   0.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=4.606 DataTime=4.229 Loss=2.897 Prec@1=39.258 Prec@5=62.695 rate=1670.45 Hz, eta=0:00:01, total=0:00:00, wall=16:16 IST
=> Training   0.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.580 DataTime=0.416 Loss=2.958 Prec@1=36.409 Prec@5=61.899 rate=1670.45 Hz, eta=0:00:01, total=0:00:00, wall=16:16 IST
=> Training   4.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.580 DataTime=0.416 Loss=2.958 Prec@1=36.409 Prec@5=61.899 rate=1.87 Hz, eta=0:21:24, total=0:00:54, wall=16:16 IST
=> Training   4.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.580 DataTime=0.416 Loss=2.958 Prec@1=36.409 Prec@5=61.899 rate=1.87 Hz, eta=0:21:24, total=0:00:54, wall=16:17 IST
=> Training   4.04% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.568 DataTime=0.401 Loss=2.953 Prec@1=36.423 Prec@5=62.038 rate=1.87 Hz, eta=0:21:24, total=0:00:54, wall=16:17 IST
=> Training   8.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.568 DataTime=0.401 Loss=2.953 Prec@1=36.423 Prec@5=62.038 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=16:17 IST
=> Training   8.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.568 DataTime=0.401 Loss=2.953 Prec@1=36.423 Prec@5=62.038 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=16:18 IST
=> Training   8.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.564 DataTime=0.391 Loss=2.950 Prec@1=36.490 Prec@5=62.112 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=16:18 IST
=> Training   12.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.564 DataTime=0.391 Loss=2.950 Prec@1=36.490 Prec@5=62.112 rate=1.82 Hz, eta=0:20:08, total=0:02:45, wall=16:18 IST
=> Training   12.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.564 DataTime=0.391 Loss=2.950 Prec@1=36.490 Prec@5=62.112 rate=1.82 Hz, eta=0:20:08, total=0:02:45, wall=16:19 IST
=> Training   12.03% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.388 Loss=2.938 Prec@1=36.692 Prec@5=62.340 rate=1.82 Hz, eta=0:20:08, total=0:02:45, wall=16:19 IST
=> Training   16.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.388 Loss=2.938 Prec@1=36.692 Prec@5=62.340 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=16:19 IST
=> Training   16.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.388 Loss=2.938 Prec@1=36.692 Prec@5=62.340 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=16:20 IST
=> Training   16.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.386 Loss=2.933 Prec@1=36.826 Prec@5=62.413 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=16:20 IST
=> Training   20.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.386 Loss=2.933 Prec@1=36.826 Prec@5=62.413 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=16:20 IST
=> Training   20.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.386 Loss=2.933 Prec@1=36.826 Prec@5=62.413 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=16:21 IST
=> Training   20.02% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.385 Loss=2.925 Prec@1=36.977 Prec@5=62.543 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=16:21 IST
=> Training   24.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.385 Loss=2.925 Prec@1=36.977 Prec@5=62.543 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=16:21 IST
=> Training   24.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.385 Loss=2.925 Prec@1=36.977 Prec@5=62.543 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=16:22 IST
=> Training   24.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.918 Prec@1=37.127 Prec@5=62.662 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=16:22 IST
=> Training   28.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.918 Prec@1=37.127 Prec@5=62.662 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=16:22 IST
=> Training   28.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.918 Prec@1=37.127 Prec@5=62.662 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=16:23 IST
=> Training   28.01% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.912 Prec@1=37.224 Prec@5=62.765 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=16:23 IST
=> Training   32.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.912 Prec@1=37.224 Prec@5=62.765 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=16:23 IST
=> Training   32.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.912 Prec@1=37.224 Prec@5=62.765 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=16:24 IST
=> Training   32.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.904 Prec@1=37.392 Prec@5=62.911 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=16:24 IST
=> Training   36.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.904 Prec@1=37.392 Prec@5=62.911 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=16:24 IST
=> Training   36.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.904 Prec@1=37.392 Prec@5=62.911 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=16:24 IST
=> Training   36.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.899 Prec@1=37.459 Prec@5=62.981 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=16:24 IST
=> Training   39.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.899 Prec@1=37.459 Prec@5=62.981 rate=1.81 Hz, eta=0:13:51, total=0:09:13, wall=16:24 IST
=> Training   39.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.899 Prec@1=37.459 Prec@5=62.981 rate=1.81 Hz, eta=0:13:51, total=0:09:13, wall=16:25 IST
=> Training   39.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.891 Prec@1=37.592 Prec@5=63.111 rate=1.81 Hz, eta=0:13:51, total=0:09:13, wall=16:25 IST
=> Training   43.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.891 Prec@1=37.592 Prec@5=63.111 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=16:25 IST
=> Training   43.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.891 Prec@1=37.592 Prec@5=63.111 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=16:26 IST
=> Training   43.99% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.885 Prec@1=37.702 Prec@5=63.207 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=16:26 IST
=> Training   47.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.885 Prec@1=37.702 Prec@5=63.207 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:26 IST
=> Training   47.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.885 Prec@1=37.702 Prec@5=63.207 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:27 IST
=> Training   47.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.878 Prec@1=37.843 Prec@5=63.331 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=16:27 IST
=> Training   51.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.878 Prec@1=37.843 Prec@5=63.331 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=16:27 IST
=> Training   51.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.385 Loss=2.878 Prec@1=37.843 Prec@5=63.331 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=16:28 IST
=> Training   51.98% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.871 Prec@1=37.959 Prec@5=63.449 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=16:28 IST
=> Training   55.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.871 Prec@1=37.959 Prec@5=63.449 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=16:28 IST
=> Training   55.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.384 Loss=2.871 Prec@1=37.959 Prec@5=63.449 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=16:29 IST
=> Training   55.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.863 Prec@1=38.088 Prec@5=63.592 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=16:29 IST
=> Training   59.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.863 Prec@1=38.088 Prec@5=63.592 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=16:29 IST
=> Training   59.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.863 Prec@1=38.088 Prec@5=63.592 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=16:30 IST
=> Training   59.97% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.857 Prec@1=38.201 Prec@5=63.685 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=16:30 IST
=> Training   63.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.857 Prec@1=38.201 Prec@5=63.685 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=16:30 IST
=> Training   63.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.857 Prec@1=38.201 Prec@5=63.685 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=16:31 IST
=> Training   63.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.557 DataTime=0.384 Loss=2.850 Prec@1=38.338 Prec@5=63.810 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=16:31 IST
=> Training   67.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.557 DataTime=0.384 Loss=2.850 Prec@1=38.338 Prec@5=63.810 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=16:31 IST
=> Training   67.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.557 DataTime=0.384 Loss=2.850 Prec@1=38.338 Prec@5=63.810 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=16:32 IST
=> Training   67.96% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.844 Prec@1=38.428 Prec@5=63.912 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=16:32 IST
=> Training   71.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.844 Prec@1=38.428 Prec@5=63.912 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:32 IST
=> Training   71.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.558 DataTime=0.385 Loss=2.844 Prec@1=38.428 Prec@5=63.912 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:33 IST
=> Training   71.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.838 Prec@1=38.544 Prec@5=64.026 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=16:33 IST
=> Training   75.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.838 Prec@1=38.544 Prec@5=64.026 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=16:33 IST
=> Training   75.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.838 Prec@1=38.544 Prec@5=64.026 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=16:34 IST
=> Training   75.95% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.831 Prec@1=38.659 Prec@5=64.140 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=16:34 IST
=> Training   79.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.831 Prec@1=38.659 Prec@5=64.140 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=16:34 IST
=> Training   79.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.559 DataTime=0.386 Loss=2.831 Prec@1=38.659 Prec@5=64.140 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=16:35 IST
=> Training   79.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.826 Prec@1=38.755 Prec@5=64.237 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=16:35 IST
=> Training   83.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.826 Prec@1=38.755 Prec@5=64.237 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=16:35 IST
=> Training   83.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.826 Prec@1=38.755 Prec@5=64.237 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=16:36 IST
=> Training   83.94% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.820 Prec@1=38.861 Prec@5=64.346 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=16:36 IST
=> Training   87.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.820 Prec@1=38.861 Prec@5=64.346 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=16:36 IST
=> Training   87.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.386 Loss=2.820 Prec@1=38.861 Prec@5=64.346 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=16:37 IST
=> Training   87.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.814 Prec@1=38.964 Prec@5=64.448 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=16:37 IST
=> Training   91.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.814 Prec@1=38.964 Prec@5=64.448 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=16:37 IST
=> Training   91.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.814 Prec@1=38.964 Prec@5=64.448 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=16:38 IST
=> Training   91.93% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.808 Prec@1=39.067 Prec@5=64.554 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=16:38 IST
=> Training   95.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.808 Prec@1=39.067 Prec@5=64.554 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=16:38 IST
=> Training   95.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.561 DataTime=0.387 Loss=2.808 Prec@1=39.067 Prec@5=64.554 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=16:38 IST
=> Training   95.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.803 Prec@1=39.165 Prec@5=64.650 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=16:38 IST
=> Training   99.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.803 Prec@1=39.165 Prec@5=64.650 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=16:38 IST
=> Training   99.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.803 Prec@1=39.165 Prec@5=64.650 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=16:38 IST
=> Training   99.92% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.802 Prec@1=39.167 Prec@5=64.652 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=16:38 IST
=> Training   100.00% of 1x2503...Epoch=3/150 LR=0.0600 Time=0.560 DataTime=0.387 Loss=2.802 Prec@1=39.167 Prec@5=64.652 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=16:38 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST
=> Validation 0.00% of 1x98...Epoch=3/150 LR=0.0600 Time=5.967 Loss=1.746 Prec@1=58.789 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=16:39 IST
=> Validation 1.02% of 1x98...Epoch=3/150 LR=0.0600 Time=5.967 Loss=1.746 Prec@1=58.789 Prec@5=85.156 rate=7026.42 Hz, eta=0:00:00, total=0:00:00, wall=16:39 IST
** Validation 1.02% of 1x98...Epoch=3/150 LR=0.0600 Time=5.967 Loss=1.746 Prec@1=58.789 Prec@5=85.156 rate=7026.42 Hz, eta=0:00:00, total=0:00:00, wall=16:40 IST
** Validation 1.02% of 1x98...Epoch=3/150 LR=0.0600 Time=0.632 Loss=2.586 Prec@1=42.378 Prec@5=68.528 rate=7026.42 Hz, eta=0:00:00, total=0:00:00, wall=16:40 IST
** Validation 100.00% of 1x98...Epoch=3/150 LR=0.0600 Time=0.632 Loss=2.586 Prec@1=42.378 Prec@5=68.528 rate=1.75 Hz, eta=0:00:00, total=0:00:55, wall=16:40 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:40 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:40 IST
=> Training   0.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=4.822 DataTime=4.290 Loss=2.796 Prec@1=38.867 Prec@5=64.258 rate=0 Hz, eta=?, total=0:00:00, wall=16:40 IST
=> Training   0.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=4.822 DataTime=4.290 Loss=2.796 Prec@1=38.867 Prec@5=64.258 rate=5887.41 Hz, eta=0:00:00, total=0:00:00, wall=16:40 IST
=> Training   0.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=4.822 DataTime=4.290 Loss=2.796 Prec@1=38.867 Prec@5=64.258 rate=5887.41 Hz, eta=0:00:00, total=0:00:00, wall=16:41 IST
=> Training   0.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.597 DataTime=0.438 Loss=2.616 Prec@1=42.545 Prec@5=67.888 rate=5887.41 Hz, eta=0:00:00, total=0:00:00, wall=16:41 IST
=> Training   4.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.597 DataTime=0.438 Loss=2.616 Prec@1=42.545 Prec@5=67.888 rate=1.82 Hz, eta=0:22:01, total=0:00:55, wall=16:41 IST
=> Training   4.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.597 DataTime=0.438 Loss=2.616 Prec@1=42.545 Prec@5=67.888 rate=1.82 Hz, eta=0:22:01, total=0:00:55, wall=16:42 IST
=> Training   4.04% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.592 DataTime=0.429 Loss=2.598 Prec@1=42.754 Prec@5=68.206 rate=1.82 Hz, eta=0:22:01, total=0:00:55, wall=16:42 IST
=> Training   8.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.592 DataTime=0.429 Loss=2.598 Prec@1=42.754 Prec@5=68.206 rate=1.76 Hz, eta=0:21:48, total=0:01:54, wall=16:42 IST
=> Training   8.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.592 DataTime=0.429 Loss=2.598 Prec@1=42.754 Prec@5=68.206 rate=1.76 Hz, eta=0:21:48, total=0:01:54, wall=16:42 IST
=> Training   8.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.593 DataTime=0.427 Loss=2.599 Prec@1=42.687 Prec@5=68.178 rate=1.76 Hz, eta=0:21:48, total=0:01:54, wall=16:42 IST
=> Training   12.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.593 DataTime=0.427 Loss=2.599 Prec@1=42.687 Prec@5=68.178 rate=1.73 Hz, eta=0:21:10, total=0:02:53, wall=16:42 IST
=> Training   12.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.593 DataTime=0.427 Loss=2.599 Prec@1=42.687 Prec@5=68.178 rate=1.73 Hz, eta=0:21:10, total=0:02:53, wall=16:43 IST
=> Training   12.03% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.420 Loss=2.595 Prec@1=42.740 Prec@5=68.299 rate=1.73 Hz, eta=0:21:10, total=0:02:53, wall=16:43 IST
=> Training   16.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.420 Loss=2.595 Prec@1=42.740 Prec@5=68.299 rate=1.74 Hz, eta=0:20:07, total=0:03:50, wall=16:43 IST
=> Training   16.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.420 Loss=2.595 Prec@1=42.740 Prec@5=68.299 rate=1.74 Hz, eta=0:20:07, total=0:03:50, wall=16:44 IST
=> Training   16.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.416 Loss=2.594 Prec@1=42.774 Prec@5=68.318 rate=1.74 Hz, eta=0:20:07, total=0:03:50, wall=16:44 IST
=> Training   20.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.416 Loss=2.594 Prec@1=42.774 Prec@5=68.318 rate=1.74 Hz, eta=0:19:09, total=0:04:47, wall=16:44 IST
=> Training   20.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.416 Loss=2.594 Prec@1=42.774 Prec@5=68.318 rate=1.74 Hz, eta=0:19:09, total=0:04:47, wall=16:45 IST
=> Training   20.02% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.414 Loss=2.591 Prec@1=42.808 Prec@5=68.332 rate=1.74 Hz, eta=0:19:09, total=0:04:47, wall=16:45 IST
=> Training   24.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.414 Loss=2.591 Prec@1=42.808 Prec@5=68.332 rate=1.74 Hz, eta=0:18:11, total=0:05:44, wall=16:45 IST
=> Training   24.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.414 Loss=2.591 Prec@1=42.808 Prec@5=68.332 rate=1.74 Hz, eta=0:18:11, total=0:05:44, wall=16:46 IST
=> Training   24.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.412 Loss=2.589 Prec@1=42.846 Prec@5=68.379 rate=1.74 Hz, eta=0:18:11, total=0:05:44, wall=16:46 IST
=> Training   28.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.412 Loss=2.589 Prec@1=42.846 Prec@5=68.379 rate=1.74 Hz, eta=0:17:15, total=0:06:42, wall=16:46 IST
=> Training   28.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.412 Loss=2.589 Prec@1=42.846 Prec@5=68.379 rate=1.74 Hz, eta=0:17:15, total=0:06:42, wall=16:47 IST
=> Training   28.01% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.413 Loss=2.587 Prec@1=42.888 Prec@5=68.388 rate=1.74 Hz, eta=0:17:15, total=0:06:42, wall=16:47 IST
=> Training   32.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.413 Loss=2.587 Prec@1=42.888 Prec@5=68.388 rate=1.74 Hz, eta=0:16:20, total=0:07:41, wall=16:47 IST
=> Training   32.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.582 DataTime=0.413 Loss=2.587 Prec@1=42.888 Prec@5=68.388 rate=1.74 Hz, eta=0:16:20, total=0:07:41, wall=16:48 IST
=> Training   32.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.415 Loss=2.583 Prec@1=42.965 Prec@5=68.460 rate=1.74 Hz, eta=0:16:20, total=0:07:41, wall=16:48 IST
=> Training   36.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.415 Loss=2.583 Prec@1=42.965 Prec@5=68.460 rate=1.72 Hz, eta=0:15:29, total=0:08:42, wall=16:48 IST
=> Training   36.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.415 Loss=2.583 Prec@1=42.965 Prec@5=68.460 rate=1.72 Hz, eta=0:15:29, total=0:08:42, wall=16:49 IST
=> Training   36.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.415 Loss=2.579 Prec@1=43.036 Prec@5=68.517 rate=1.72 Hz, eta=0:15:29, total=0:08:42, wall=16:49 IST
=> Training   39.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.415 Loss=2.579 Prec@1=43.036 Prec@5=68.517 rate=1.72 Hz, eta=0:14:33, total=0:09:42, wall=16:49 IST
=> Training   39.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.586 DataTime=0.415 Loss=2.579 Prec@1=43.036 Prec@5=68.517 rate=1.72 Hz, eta=0:14:33, total=0:09:42, wall=16:50 IST
=> Training   39.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.414 Loss=2.574 Prec@1=43.137 Prec@5=68.610 rate=1.72 Hz, eta=0:14:33, total=0:09:42, wall=16:50 IST
=> Training   43.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.414 Loss=2.574 Prec@1=43.137 Prec@5=68.610 rate=1.72 Hz, eta=0:13:34, total=0:10:39, wall=16:50 IST
=> Training   43.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.414 Loss=2.574 Prec@1=43.137 Prec@5=68.610 rate=1.72 Hz, eta=0:13:34, total=0:10:39, wall=16:51 IST
=> Training   43.99% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.412 Loss=2.571 Prec@1=43.195 Prec@5=68.646 rate=1.72 Hz, eta=0:13:34, total=0:10:39, wall=16:51 IST
=> Training   47.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.412 Loss=2.571 Prec@1=43.195 Prec@5=68.646 rate=1.72 Hz, eta=0:12:34, total=0:11:36, wall=16:51 IST
=> Training   47.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.412 Loss=2.571 Prec@1=43.195 Prec@5=68.646 rate=1.72 Hz, eta=0:12:34, total=0:11:36, wall=16:52 IST
=> Training   47.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.412 Loss=2.568 Prec@1=43.281 Prec@5=68.683 rate=1.72 Hz, eta=0:12:34, total=0:11:36, wall=16:52 IST
=> Training   51.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.412 Loss=2.568 Prec@1=43.281 Prec@5=68.683 rate=1.72 Hz, eta=0:11:38, total=0:12:35, wall=16:52 IST
=> Training   51.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.585 DataTime=0.412 Loss=2.568 Prec@1=43.281 Prec@5=68.683 rate=1.72 Hz, eta=0:11:38, total=0:12:35, wall=16:53 IST
=> Training   51.98% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.411 Loss=2.564 Prec@1=43.352 Prec@5=68.761 rate=1.72 Hz, eta=0:11:38, total=0:12:35, wall=16:53 IST
=> Training   55.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.411 Loss=2.564 Prec@1=43.352 Prec@5=68.761 rate=1.72 Hz, eta=0:10:39, total=0:13:33, wall=16:53 IST
=> Training   55.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.584 DataTime=0.411 Loss=2.564 Prec@1=43.352 Prec@5=68.761 rate=1.72 Hz, eta=0:10:39, total=0:13:33, wall=16:54 IST
=> Training   55.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.583 DataTime=0.410 Loss=2.560 Prec@1=43.414 Prec@5=68.848 rate=1.72 Hz, eta=0:10:39, total=0:13:33, wall=16:54 IST
=> Training   59.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.583 DataTime=0.410 Loss=2.560 Prec@1=43.414 Prec@5=68.848 rate=1.73 Hz, eta=0:09:40, total=0:14:30, wall=16:54 IST
=> Training   59.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.583 DataTime=0.410 Loss=2.560 Prec@1=43.414 Prec@5=68.848 rate=1.73 Hz, eta=0:09:40, total=0:14:30, wall=16:55 IST
=> Training   59.97% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.581 DataTime=0.408 Loss=2.556 Prec@1=43.478 Prec@5=68.891 rate=1.73 Hz, eta=0:09:40, total=0:14:30, wall=16:55 IST
=> Training   63.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.581 DataTime=0.408 Loss=2.556 Prec@1=43.478 Prec@5=68.891 rate=1.73 Hz, eta=0:08:41, total=0:15:25, wall=16:55 IST
=> Training   63.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.581 DataTime=0.408 Loss=2.556 Prec@1=43.478 Prec@5=68.891 rate=1.73 Hz, eta=0:08:41, total=0:15:25, wall=16:56 IST
=> Training   63.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.580 DataTime=0.406 Loss=2.553 Prec@1=43.540 Prec@5=68.937 rate=1.73 Hz, eta=0:08:41, total=0:15:25, wall=16:56 IST
=> Training   67.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.580 DataTime=0.406 Loss=2.553 Prec@1=43.540 Prec@5=68.937 rate=1.73 Hz, eta=0:07:42, total=0:16:21, wall=16:56 IST
=> Training   67.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.580 DataTime=0.406 Loss=2.553 Prec@1=43.540 Prec@5=68.937 rate=1.73 Hz, eta=0:07:42, total=0:16:21, wall=16:57 IST
=> Training   67.96% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.550 Prec@1=43.593 Prec@5=68.985 rate=1.73 Hz, eta=0:07:42, total=0:16:21, wall=16:57 IST
=> Training   71.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.550 Prec@1=43.593 Prec@5=68.985 rate=1.74 Hz, eta=0:06:43, total=0:17:15, wall=16:57 IST
=> Training   71.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.550 Prec@1=43.593 Prec@5=68.985 rate=1.74 Hz, eta=0:06:43, total=0:17:15, wall=16:58 IST
=> Training   71.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.547 Prec@1=43.662 Prec@5=69.037 rate=1.74 Hz, eta=0:06:43, total=0:17:15, wall=16:58 IST
=> Training   75.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.547 Prec@1=43.662 Prec@5=69.037 rate=1.74 Hz, eta=0:05:46, total=0:18:13, wall=16:58 IST
=> Training   75.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.578 DataTime=0.404 Loss=2.547 Prec@1=43.662 Prec@5=69.037 rate=1.74 Hz, eta=0:05:46, total=0:18:13, wall=16:59 IST
=> Training   75.95% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.576 DataTime=0.402 Loss=2.544 Prec@1=43.723 Prec@5=69.090 rate=1.74 Hz, eta=0:05:46, total=0:18:13, wall=16:59 IST
=> Training   79.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.576 DataTime=0.402 Loss=2.544 Prec@1=43.723 Prec@5=69.090 rate=1.74 Hz, eta=0:04:47, total=0:19:07, wall=16:59 IST
=> Training   79.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.576 DataTime=0.402 Loss=2.544 Prec@1=43.723 Prec@5=69.090 rate=1.74 Hz, eta=0:04:47, total=0:19:07, wall=17:00 IST
=> Training   79.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.575 DataTime=0.401 Loss=2.540 Prec@1=43.793 Prec@5=69.146 rate=1.74 Hz, eta=0:04:47, total=0:19:07, wall=17:00 IST
=> Training   83.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.575 DataTime=0.401 Loss=2.540 Prec@1=43.793 Prec@5=69.146 rate=1.75 Hz, eta=0:03:50, total=0:20:03, wall=17:00 IST
=> Training   83.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.575 DataTime=0.401 Loss=2.540 Prec@1=43.793 Prec@5=69.146 rate=1.75 Hz, eta=0:03:50, total=0:20:03, wall=17:01 IST
=> Training   83.94% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.536 Prec@1=43.858 Prec@5=69.202 rate=1.75 Hz, eta=0:03:50, total=0:20:03, wall=17:01 IST
=> Training   87.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.536 Prec@1=43.858 Prec@5=69.202 rate=1.75 Hz, eta=0:02:52, total=0:20:59, wall=17:01 IST
=> Training   87.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.536 Prec@1=43.858 Prec@5=69.202 rate=1.75 Hz, eta=0:02:52, total=0:20:59, wall=17:02 IST
=> Training   87.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.533 Prec@1=43.916 Prec@5=69.255 rate=1.75 Hz, eta=0:02:52, total=0:20:59, wall=17:02 IST
=> Training   91.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.533 Prec@1=43.916 Prec@5=69.255 rate=1.75 Hz, eta=0:01:55, total=0:21:55, wall=17:02 IST
=> Training   91.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.401 Loss=2.533 Prec@1=43.916 Prec@5=69.255 rate=1.75 Hz, eta=0:01:55, total=0:21:55, wall=17:02 IST
=> Training   91.93% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.400 Loss=2.530 Prec@1=43.976 Prec@5=69.300 rate=1.75 Hz, eta=0:01:55, total=0:21:55, wall=17:02 IST
=> Training   95.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.400 Loss=2.530 Prec@1=43.976 Prec@5=69.300 rate=1.75 Hz, eta=0:00:58, total=0:22:52, wall=17:02 IST
=> Training   95.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.574 DataTime=0.400 Loss=2.530 Prec@1=43.976 Prec@5=69.300 rate=1.75 Hz, eta=0:00:58, total=0:22:52, wall=17:03 IST
=> Training   95.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.572 DataTime=0.399 Loss=2.528 Prec@1=44.033 Prec@5=69.346 rate=1.75 Hz, eta=0:00:58, total=0:22:52, wall=17:03 IST
=> Training   99.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.572 DataTime=0.399 Loss=2.528 Prec@1=44.033 Prec@5=69.346 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:03 IST
=> Training   99.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.572 DataTime=0.399 Loss=2.528 Prec@1=44.033 Prec@5=69.346 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:03 IST
=> Training   99.92% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.572 DataTime=0.399 Loss=2.528 Prec@1=44.034 Prec@5=69.346 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:03 IST
=> Training   100.00% of 1x2503...Epoch=4/150 LR=0.0800 Time=0.572 DataTime=0.399 Loss=2.528 Prec@1=44.034 Prec@5=69.346 rate=1.75 Hz, eta=0:00:00, total=0:23:47, wall=17:03 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST
=> Validation 0.00% of 1x98...Epoch=4/150 LR=0.0800 Time=6.899 Loss=1.700 Prec@1=58.008 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=17:04 IST
=> Validation 1.02% of 1x98...Epoch=4/150 LR=0.0800 Time=6.899 Loss=1.700 Prec@1=58.008 Prec@5=84.570 rate=960.71 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST
** Validation 1.02% of 1x98...Epoch=4/150 LR=0.0800 Time=6.899 Loss=1.700 Prec@1=58.008 Prec@5=84.570 rate=960.71 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST
** Validation 1.02% of 1x98...Epoch=4/150 LR=0.0800 Time=0.630 Loss=2.441 Prec@1=44.898 Prec@5=70.726 rate=960.71 Hz, eta=0:00:00, total=0:00:00, wall=17:04 IST
** Validation 100.00% of 1x98...Epoch=4/150 LR=0.0800 Time=0.630 Loss=2.441 Prec@1=44.898 Prec@5=70.726 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=17:04 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST
=> Training   0.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=5.287 DataTime=4.987 Loss=2.474 Prec@1=44.336 Prec@5=69.336 rate=0 Hz, eta=?, total=0:00:00, wall=17:05 IST
=> Training   0.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=5.287 DataTime=4.987 Loss=2.474 Prec@1=44.336 Prec@5=69.336 rate=8014.17 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST
=> Training   0.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=5.287 DataTime=4.987 Loss=2.474 Prec@1=44.336 Prec@5=69.336 rate=8014.17 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST
=> Training   0.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.577 DataTime=0.405 Loss=2.382 Prec@1=46.745 Prec@5=71.931 rate=8014.17 Hz, eta=0:00:00, total=0:00:00, wall=17:05 IST
=> Training   4.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.577 DataTime=0.405 Loss=2.382 Prec@1=46.745 Prec@5=71.931 rate=1.90 Hz, eta=0:21:00, total=0:00:53, wall=17:05 IST
=> Training   4.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.577 DataTime=0.405 Loss=2.382 Prec@1=46.745 Prec@5=71.931 rate=1.90 Hz, eta=0:21:00, total=0:00:53, wall=17:06 IST
=> Training   4.04% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.401 Loss=2.384 Prec@1=46.768 Prec@5=71.826 rate=1.90 Hz, eta=0:21:00, total=0:00:53, wall=17:06 IST
=> Training   8.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.401 Loss=2.384 Prec@1=46.768 Prec@5=71.826 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=17:06 IST
=> Training   8.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.401 Loss=2.384 Prec@1=46.768 Prec@5=71.826 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=17:07 IST
=> Training   8.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.386 Prec@1=46.625 Prec@5=71.726 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=17:07 IST
=> Training   12.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.386 Prec@1=46.625 Prec@5=71.726 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=17:07 IST
=> Training   12.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.386 Prec@1=46.625 Prec@5=71.726 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=17:08 IST
=> Training   12.03% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.392 Loss=2.383 Prec@1=46.621 Prec@5=71.742 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=17:08 IST
=> Training   16.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.392 Loss=2.383 Prec@1=46.621 Prec@5=71.742 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=17:08 IST
=> Training   16.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.392 Loss=2.383 Prec@1=46.621 Prec@5=71.742 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=17:09 IST
=> Training   16.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.393 Loss=2.383 Prec@1=46.659 Prec@5=71.738 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=17:09 IST
=> Training   20.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.393 Loss=2.383 Prec@1=46.659 Prec@5=71.738 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=17:09 IST
=> Training   20.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.562 DataTime=0.393 Loss=2.383 Prec@1=46.659 Prec@5=71.738 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=17:10 IST
=> Training   20.02% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.726 Prec@5=71.793 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=17:10 IST
=> Training   24.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.726 Prec@5=71.793 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=17:10 IST
=> Training   24.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.726 Prec@5=71.793 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=17:11 IST
=> Training   24.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.753 Prec@5=71.785 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=17:11 IST
=> Training   28.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.753 Prec@5=71.785 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=17:11 IST
=> Training   28.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.393 Loss=2.381 Prec@1=46.753 Prec@5=71.785 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=17:12 IST
=> Training   28.01% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.383 Prec@1=46.712 Prec@5=71.771 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=17:12 IST
=> Training   32.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.383 Prec@1=46.712 Prec@5=71.771 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:12 IST
=> Training   32.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.383 Prec@1=46.712 Prec@5=71.771 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:13 IST
=> Training   32.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.395 Loss=2.380 Prec@1=46.731 Prec@5=71.804 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:13 IST
=> Training   36.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.395 Loss=2.380 Prec@1=46.731 Prec@5=71.804 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=17:13 IST
=> Training   36.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.395 Loss=2.380 Prec@1=46.731 Prec@5=71.804 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=17:14 IST
=> Training   36.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.379 Prec@1=46.734 Prec@5=71.840 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=17:14 IST
=> Training   39.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.379 Prec@1=46.734 Prec@5=71.840 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:14 IST
=> Training   39.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.564 DataTime=0.395 Loss=2.379 Prec@1=46.734 Prec@5=71.840 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:15 IST
=> Training   39.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.394 Loss=2.378 Prec@1=46.756 Prec@5=71.844 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:15 IST
=> Training   43.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.394 Loss=2.378 Prec@1=46.756 Prec@5=71.844 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:15 IST
=> Training   43.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.563 DataTime=0.394 Loss=2.378 Prec@1=46.756 Prec@5=71.844 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:16 IST
=> Training   43.99% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.393 Loss=2.376 Prec@1=46.802 Prec@5=71.866 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:16 IST
=> Training   47.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.393 Loss=2.376 Prec@1=46.802 Prec@5=71.866 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=17:16 IST
=> Training   47.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.393 Loss=2.376 Prec@1=46.802 Prec@5=71.866 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=17:17 IST
=> Training   47.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.394 Loss=2.374 Prec@1=46.823 Prec@5=71.912 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=17:17 IST
=> Training   51.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.394 Loss=2.374 Prec@1=46.823 Prec@5=71.912 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=17:17 IST
=> Training   51.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.565 DataTime=0.394 Loss=2.374 Prec@1=46.823 Prec@5=71.912 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=17:18 IST
=> Training   51.98% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.373 Prec@1=46.832 Prec@5=71.937 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=17:18 IST
=> Training   55.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.373 Prec@1=46.832 Prec@5=71.937 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=17:18 IST
=> Training   55.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.373 Prec@1=46.832 Prec@5=71.937 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=17:19 IST
=> Training   55.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.566 DataTime=0.394 Loss=2.370 Prec@1=46.883 Prec@5=71.975 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=17:19 IST
=> Training   59.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.566 DataTime=0.394 Loss=2.370 Prec@1=46.883 Prec@5=71.975 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=17:19 IST
=> Training   59.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.566 DataTime=0.394 Loss=2.370 Prec@1=46.883 Prec@5=71.975 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=17:20 IST
=> Training   59.97% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.368 Prec@1=46.923 Prec@5=72.012 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=17:20 IST
=> Training   63.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.368 Prec@1=46.923 Prec@5=72.012 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=17:20 IST
=> Training   63.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.567 DataTime=0.395 Loss=2.368 Prec@1=46.923 Prec@5=72.012 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=17:21 IST
=> Training   63.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.367 Prec@1=46.949 Prec@5=72.031 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=17:21 IST
=> Training   67.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.367 Prec@1=46.949 Prec@5=72.031 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=17:21 IST
=> Training   67.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.367 Prec@1=46.949 Prec@5=72.031 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=17:21 IST
=> Training   67.96% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.365 Prec@1=46.986 Prec@5=72.051 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=17:21 IST
=> Training   71.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.365 Prec@1=46.986 Prec@5=72.051 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=17:21 IST
=> Training   71.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.568 DataTime=0.395 Loss=2.365 Prec@1=46.986 Prec@5=72.051 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=17:22 IST
=> Training   71.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.363 Prec@1=47.022 Prec@5=72.087 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=17:23 IST
=> Training   75.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.363 Prec@1=47.022 Prec@5=72.087 rate=1.76 Hz, eta=0:05:41, total=0:17:58, wall=17:23 IST
=> Training   75.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.363 Prec@1=47.022 Prec@5=72.087 rate=1.76 Hz, eta=0:05:41, total=0:17:58, wall=17:23 IST
=> Training   75.95% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.362 Prec@1=47.061 Prec@5=72.104 rate=1.76 Hz, eta=0:05:41, total=0:17:58, wall=17:23 IST
=> Training   79.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.362 Prec@1=47.061 Prec@5=72.104 rate=1.76 Hz, eta=0:04:45, total=0:18:56, wall=17:23 IST
=> Training   79.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.362 Prec@1=47.061 Prec@5=72.104 rate=1.76 Hz, eta=0:04:45, total=0:18:56, wall=17:24 IST
=> Training   79.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.358 Prec@1=47.121 Prec@5=72.154 rate=1.76 Hz, eta=0:04:45, total=0:18:56, wall=17:24 IST
=> Training   83.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.358 Prec@1=47.121 Prec@5=72.154 rate=1.76 Hz, eta=0:03:48, total=0:19:52, wall=17:24 IST
=> Training   83.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.570 DataTime=0.396 Loss=2.358 Prec@1=47.121 Prec@5=72.154 rate=1.76 Hz, eta=0:03:48, total=0:19:52, wall=17:25 IST
=> Training   83.94% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.357 Prec@1=47.160 Prec@5=72.173 rate=1.76 Hz, eta=0:03:48, total=0:19:52, wall=17:25 IST
=> Training   87.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.357 Prec@1=47.160 Prec@5=72.173 rate=1.76 Hz, eta=0:02:51, total=0:20:50, wall=17:25 IST
=> Training   87.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.397 Loss=2.357 Prec@1=47.160 Prec@5=72.173 rate=1.76 Hz, eta=0:02:51, total=0:20:50, wall=17:26 IST
=> Training   87.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.398 Loss=2.356 Prec@1=47.188 Prec@5=72.195 rate=1.76 Hz, eta=0:02:51, total=0:20:50, wall=17:26 IST
=> Training   91.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.398 Loss=2.356 Prec@1=47.188 Prec@5=72.195 rate=1.76 Hz, eta=0:01:54, total=0:21:49, wall=17:26 IST
=> Training   91.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.571 DataTime=0.398 Loss=2.356 Prec@1=47.188 Prec@5=72.195 rate=1.76 Hz, eta=0:01:54, total=0:21:49, wall=17:27 IST
=> Training   91.93% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.572 DataTime=0.399 Loss=2.354 Prec@1=47.227 Prec@5=72.224 rate=1.76 Hz, eta=0:01:54, total=0:21:49, wall=17:27 IST
=> Training   95.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.572 DataTime=0.399 Loss=2.354 Prec@1=47.227 Prec@5=72.224 rate=1.75 Hz, eta=0:00:58, total=0:22:48, wall=17:27 IST
=> Training   95.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.572 DataTime=0.399 Loss=2.354 Prec@1=47.227 Prec@5=72.224 rate=1.75 Hz, eta=0:00:58, total=0:22:48, wall=17:28 IST
=> Training   95.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.400 Loss=2.352 Prec@1=47.274 Prec@5=72.261 rate=1.75 Hz, eta=0:00:58, total=0:22:48, wall=17:28 IST
=> Training   99.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.400 Loss=2.352 Prec@1=47.274 Prec@5=72.261 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:28 IST
=> Training   99.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.400 Loss=2.352 Prec@1=47.274 Prec@5=72.261 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:28 IST
=> Training   99.92% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.400 Loss=2.352 Prec@1=47.274 Prec@5=72.262 rate=1.75 Hz, eta=0:00:01, total=0:23:47, wall=17:28 IST
=> Training   100.00% of 1x2503...Epoch=5/150 LR=0.0998 Time=0.573 DataTime=0.400 Loss=2.352 Prec@1=47.274 Prec@5=72.262 rate=1.75 Hz, eta=0:00:00, total=0:23:47, wall=17:28 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST
=> Validation 0.00% of 1x98...Epoch=5/150 LR=0.0998 Time=9.254 Loss=1.398 Prec@1=65.039 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=17:28 IST
=> Validation 1.02% of 1x98...Epoch=5/150 LR=0.0998 Time=9.254 Loss=1.398 Prec@1=65.039 Prec@5=88.281 rate=6214.69 Hz, eta=0:00:00, total=0:00:00, wall=17:28 IST
** Validation 1.02% of 1x98...Epoch=5/150 LR=0.0998 Time=9.254 Loss=1.398 Prec@1=65.039 Prec@5=88.281 rate=6214.69 Hz, eta=0:00:00, total=0:00:00, wall=17:29 IST
** Validation 1.02% of 1x98...Epoch=5/150 LR=0.0998 Time=0.702 Loss=2.286 Prec@1=47.924 Prec@5=73.726 rate=6214.69 Hz, eta=0:00:00, total=0:00:00, wall=17:29 IST
** Validation 100.00% of 1x98...Epoch=5/150 LR=0.0998 Time=0.702 Loss=2.286 Prec@1=47.924 Prec@5=73.726 rate=1.65 Hz, eta=0:00:00, total=0:00:59, wall=17:29 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST
=> Training   0.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=5.214 DataTime=5.038 Loss=2.427 Prec@1=47.266 Prec@5=71.875 rate=0 Hz, eta=?, total=0:00:00, wall=17:30 IST
=> Training   0.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=5.214 DataTime=5.038 Loss=2.427 Prec@1=47.266 Prec@5=71.875 rate=950.98 Hz, eta=0:00:02, total=0:00:00, wall=17:30 IST
=> Training   0.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=5.214 DataTime=5.038 Loss=2.427 Prec@1=47.266 Prec@5=71.875 rate=950.98 Hz, eta=0:00:02, total=0:00:00, wall=17:31 IST
=> Training   0.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.614 DataTime=0.447 Loss=2.235 Prec@1=49.547 Prec@5=74.080 rate=950.98 Hz, eta=0:00:02, total=0:00:00, wall=17:31 IST
=> Training   4.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.614 DataTime=0.447 Loss=2.235 Prec@1=49.547 Prec@5=74.080 rate=1.78 Hz, eta=0:22:29, total=0:00:56, wall=17:31 IST
=> Training   4.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.614 DataTime=0.447 Loss=2.235 Prec@1=49.547 Prec@5=74.080 rate=1.78 Hz, eta=0:22:29, total=0:00:56, wall=17:32 IST
=> Training   4.04% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.603 DataTime=0.435 Loss=2.232 Prec@1=49.582 Prec@5=74.096 rate=1.78 Hz, eta=0:22:29, total=0:00:56, wall=17:32 IST
=> Training   8.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.603 DataTime=0.435 Loss=2.232 Prec@1=49.582 Prec@5=74.096 rate=1.73 Hz, eta=0:22:09, total=0:01:56, wall=17:32 IST
=> Training   8.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.603 DataTime=0.435 Loss=2.232 Prec@1=49.582 Prec@5=74.096 rate=1.73 Hz, eta=0:22:09, total=0:01:56, wall=17:33 IST
=> Training   8.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.600 DataTime=0.428 Loss=2.236 Prec@1=49.537 Prec@5=74.067 rate=1.73 Hz, eta=0:22:09, total=0:01:56, wall=17:33 IST
=> Training   12.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.600 DataTime=0.428 Loss=2.236 Prec@1=49.537 Prec@5=74.067 rate=1.72 Hz, eta=0:21:23, total=0:02:55, wall=17:33 IST
=> Training   12.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.600 DataTime=0.428 Loss=2.236 Prec@1=49.537 Prec@5=74.067 rate=1.72 Hz, eta=0:21:23, total=0:02:55, wall=17:33 IST
=> Training   12.03% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.597 DataTime=0.424 Loss=2.236 Prec@1=49.499 Prec@5=74.070 rate=1.72 Hz, eta=0:21:23, total=0:02:55, wall=17:33 IST
=> Training   16.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.597 DataTime=0.424 Loss=2.236 Prec@1=49.499 Prec@5=74.070 rate=1.71 Hz, eta=0:20:27, total=0:03:54, wall=17:33 IST
=> Training   16.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.597 DataTime=0.424 Loss=2.236 Prec@1=49.499 Prec@5=74.070 rate=1.71 Hz, eta=0:20:27, total=0:03:54, wall=17:34 IST
=> Training   16.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.237 Prec@1=49.464 Prec@5=74.104 rate=1.71 Hz, eta=0:20:27, total=0:03:54, wall=17:34 IST
=> Training   20.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.237 Prec@1=49.464 Prec@5=74.104 rate=1.71 Hz, eta=0:19:28, total=0:04:52, wall=17:34 IST
=> Training   20.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.237 Prec@1=49.464 Prec@5=74.104 rate=1.71 Hz, eta=0:19:28, total=0:04:52, wall=17:35 IST
=> Training   20.02% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.422 Loss=2.237 Prec@1=49.423 Prec@5=74.126 rate=1.71 Hz, eta=0:19:28, total=0:04:52, wall=17:35 IST
=> Training   24.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.422 Loss=2.237 Prec@1=49.423 Prec@5=74.126 rate=1.70 Hz, eta=0:18:35, total=0:05:52, wall=17:35 IST
=> Training   24.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.422 Loss=2.237 Prec@1=49.423 Prec@5=74.126 rate=1.70 Hz, eta=0:18:35, total=0:05:52, wall=17:36 IST
=> Training   24.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.421 Loss=2.237 Prec@1=49.407 Prec@5=74.101 rate=1.70 Hz, eta=0:18:35, total=0:05:52, wall=17:36 IST
=> Training   28.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.421 Loss=2.237 Prec@1=49.407 Prec@5=74.101 rate=1.70 Hz, eta=0:17:37, total=0:06:51, wall=17:36 IST
=> Training   28.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.421 Loss=2.237 Prec@1=49.407 Prec@5=74.101 rate=1.70 Hz, eta=0:17:37, total=0:06:51, wall=17:37 IST
=> Training   28.01% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.239 Prec@1=49.315 Prec@5=74.068 rate=1.70 Hz, eta=0:17:37, total=0:06:51, wall=17:37 IST
=> Training   32.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.239 Prec@1=49.315 Prec@5=74.068 rate=1.70 Hz, eta=0:16:39, total=0:07:50, wall=17:37 IST
=> Training   32.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.420 Loss=2.239 Prec@1=49.315 Prec@5=74.068 rate=1.70 Hz, eta=0:16:39, total=0:07:50, wall=17:38 IST
=> Training   32.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.421 Loss=2.238 Prec@1=49.305 Prec@5=74.072 rate=1.70 Hz, eta=0:16:39, total=0:07:50, wall=17:38 IST
=> Training   36.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.421 Loss=2.238 Prec@1=49.305 Prec@5=74.072 rate=1.70 Hz, eta=0:15:43, total=0:08:50, wall=17:38 IST
=> Training   36.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.595 DataTime=0.421 Loss=2.238 Prec@1=49.305 Prec@5=74.072 rate=1.70 Hz, eta=0:15:43, total=0:08:50, wall=17:39 IST
=> Training   36.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.317 Prec@5=74.072 rate=1.70 Hz, eta=0:15:43, total=0:08:50, wall=17:39 IST
=> Training   39.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.317 Prec@5=74.072 rate=1.70 Hz, eta=0:14:44, total=0:09:49, wall=17:39 IST
=> Training   39.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.317 Prec@5=74.072 rate=1.70 Hz, eta=0:14:44, total=0:09:49, wall=17:40 IST
=> Training   39.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.337 Prec@5=74.052 rate=1.70 Hz, eta=0:14:44, total=0:09:49, wall=17:40 IST
=> Training   43.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.337 Prec@5=74.052 rate=1.70 Hz, eta=0:13:45, total=0:10:48, wall=17:40 IST
=> Training   43.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.594 DataTime=0.419 Loss=2.238 Prec@1=49.337 Prec@5=74.052 rate=1.70 Hz, eta=0:13:45, total=0:10:48, wall=17:41 IST
=> Training   43.99% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.592 DataTime=0.418 Loss=2.239 Prec@1=49.323 Prec@5=74.029 rate=1.70 Hz, eta=0:13:45, total=0:10:48, wall=17:41 IST
=> Training   47.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.592 DataTime=0.418 Loss=2.239 Prec@1=49.323 Prec@5=74.029 rate=1.70 Hz, eta=0:12:45, total=0:11:46, wall=17:41 IST
=> Training   47.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.592 DataTime=0.418 Loss=2.239 Prec@1=49.323 Prec@5=74.029 rate=1.70 Hz, eta=0:12:45, total=0:11:46, wall=17:42 IST
=> Training   47.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.333 Prec@5=74.053 rate=1.70 Hz, eta=0:12:45, total=0:11:46, wall=17:42 IST
=> Training   51.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.333 Prec@5=74.053 rate=1.70 Hz, eta=0:11:46, total=0:12:44, wall=17:42 IST
=> Training   51.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.333 Prec@5=74.053 rate=1.70 Hz, eta=0:11:46, total=0:12:44, wall=17:43 IST
=> Training   51.98% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.359 Prec@5=74.063 rate=1.70 Hz, eta=0:11:46, total=0:12:44, wall=17:43 IST
=> Training   55.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.359 Prec@5=74.063 rate=1.70 Hz, eta=0:10:47, total=0:13:42, wall=17:43 IST
=> Training   55.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.591 DataTime=0.417 Loss=2.238 Prec@1=49.359 Prec@5=74.063 rate=1.70 Hz, eta=0:10:47, total=0:13:42, wall=17:44 IST
=> Training   55.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.236 Prec@1=49.388 Prec@5=74.096 rate=1.70 Hz, eta=0:10:47, total=0:13:42, wall=17:44 IST
=> Training   59.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.236 Prec@1=49.388 Prec@5=74.096 rate=1.71 Hz, eta=0:09:47, total=0:14:39, wall=17:44 IST
=> Training   59.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.236 Prec@1=49.388 Prec@5=74.096 rate=1.71 Hz, eta=0:09:47, total=0:14:39, wall=17:45 IST
=> Training   59.97% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.235 Prec@1=49.408 Prec@5=74.107 rate=1.71 Hz, eta=0:09:47, total=0:14:39, wall=17:45 IST
=> Training   63.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.235 Prec@1=49.408 Prec@5=74.107 rate=1.71 Hz, eta=0:08:48, total=0:15:37, wall=17:45 IST
=> Training   63.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.589 DataTime=0.416 Loss=2.235 Prec@1=49.408 Prec@5=74.107 rate=1.71 Hz, eta=0:08:48, total=0:15:37, wall=17:46 IST
=> Training   63.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.588 DataTime=0.414 Loss=2.232 Prec@1=49.481 Prec@5=74.150 rate=1.71 Hz, eta=0:08:48, total=0:15:37, wall=17:46 IST
=> Training   67.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.588 DataTime=0.414 Loss=2.232 Prec@1=49.481 Prec@5=74.150 rate=1.71 Hz, eta=0:07:48, total=0:16:34, wall=17:46 IST
=> Training   67.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.588 DataTime=0.414 Loss=2.232 Prec@1=49.481 Prec@5=74.150 rate=1.71 Hz, eta=0:07:48, total=0:16:34, wall=17:47 IST
=> Training   67.96% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.587 DataTime=0.414 Loss=2.233 Prec@1=49.494 Prec@5=74.143 rate=1.71 Hz, eta=0:07:48, total=0:16:34, wall=17:47 IST
=> Training   71.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.587 DataTime=0.414 Loss=2.233 Prec@1=49.494 Prec@5=74.143 rate=1.71 Hz, eta=0:06:50, total=0:17:32, wall=17:47 IST
=> Training   71.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.587 DataTime=0.414 Loss=2.233 Prec@1=49.494 Prec@5=74.143 rate=1.71 Hz, eta=0:06:50, total=0:17:32, wall=17:48 IST
=> Training   71.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.586 DataTime=0.412 Loss=2.232 Prec@1=49.501 Prec@5=74.152 rate=1.71 Hz, eta=0:06:50, total=0:17:32, wall=17:48 IST
=> Training   75.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.586 DataTime=0.412 Loss=2.232 Prec@1=49.501 Prec@5=74.152 rate=1.72 Hz, eta=0:05:51, total=0:18:28, wall=17:48 IST
=> Training   75.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.586 DataTime=0.412 Loss=2.232 Prec@1=49.501 Prec@5=74.152 rate=1.72 Hz, eta=0:05:51, total=0:18:28, wall=17:49 IST
=> Training   75.95% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.585 DataTime=0.411 Loss=2.232 Prec@1=49.524 Prec@5=74.170 rate=1.72 Hz, eta=0:05:51, total=0:18:28, wall=17:49 IST
=> Training   79.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.585 DataTime=0.411 Loss=2.232 Prec@1=49.524 Prec@5=74.170 rate=1.72 Hz, eta=0:04:52, total=0:19:25, wall=17:49 IST
=> Training   79.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.585 DataTime=0.411 Loss=2.232 Prec@1=49.524 Prec@5=74.170 rate=1.72 Hz, eta=0:04:52, total=0:19:25, wall=17:50 IST
=> Training   79.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.411 Loss=2.231 Prec@1=49.554 Prec@5=74.183 rate=1.72 Hz, eta=0:04:52, total=0:19:25, wall=17:50 IST
=> Training   83.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.411 Loss=2.231 Prec@1=49.554 Prec@5=74.183 rate=1.72 Hz, eta=0:03:53, total=0:20:22, wall=17:50 IST
=> Training   83.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.411 Loss=2.231 Prec@1=49.554 Prec@5=74.183 rate=1.72 Hz, eta=0:03:53, total=0:20:22, wall=17:51 IST
=> Training   83.94% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.410 Loss=2.229 Prec@1=49.580 Prec@5=74.200 rate=1.72 Hz, eta=0:03:53, total=0:20:22, wall=17:51 IST
=> Training   87.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.410 Loss=2.229 Prec@1=49.580 Prec@5=74.200 rate=1.72 Hz, eta=0:02:55, total=0:21:19, wall=17:51 IST
=> Training   87.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.584 DataTime=0.410 Loss=2.229 Prec@1=49.580 Prec@5=74.200 rate=1.72 Hz, eta=0:02:55, total=0:21:19, wall=17:52 IST
=> Training   87.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.583 DataTime=0.409 Loss=2.228 Prec@1=49.603 Prec@5=74.228 rate=1.72 Hz, eta=0:02:55, total=0:21:19, wall=17:52 IST
=> Training   91.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.583 DataTime=0.409 Loss=2.228 Prec@1=49.603 Prec@5=74.228 rate=1.72 Hz, eta=0:01:57, total=0:22:15, wall=17:52 IST
=> Training   91.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.583 DataTime=0.409 Loss=2.228 Prec@1=49.603 Prec@5=74.228 rate=1.72 Hz, eta=0:01:57, total=0:22:15, wall=17:53 IST
=> Training   91.93% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.582 DataTime=0.408 Loss=2.226 Prec@1=49.633 Prec@5=74.264 rate=1.72 Hz, eta=0:01:57, total=0:22:15, wall=17:53 IST
=> Training   95.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.582 DataTime=0.408 Loss=2.226 Prec@1=49.633 Prec@5=74.264 rate=1.72 Hz, eta=0:00:59, total=0:23:12, wall=17:53 IST
=> Training   95.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.582 DataTime=0.408 Loss=2.226 Prec@1=49.633 Prec@5=74.264 rate=1.72 Hz, eta=0:00:59, total=0:23:12, wall=17:54 IST
=> Training   95.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.581 DataTime=0.407 Loss=2.225 Prec@1=49.667 Prec@5=74.278 rate=1.72 Hz, eta=0:00:59, total=0:23:12, wall=17:54 IST
=> Training   99.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.581 DataTime=0.407 Loss=2.225 Prec@1=49.667 Prec@5=74.278 rate=1.73 Hz, eta=0:00:01, total=0:24:07, wall=17:54 IST
=> Training   99.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.581 DataTime=0.407 Loss=2.225 Prec@1=49.667 Prec@5=74.278 rate=1.73 Hz, eta=0:00:01, total=0:24:07, wall=17:54 IST
=> Training   99.92% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.580 DataTime=0.407 Loss=2.225 Prec@1=49.666 Prec@5=74.277 rate=1.73 Hz, eta=0:00:01, total=0:24:07, wall=17:54 IST
=> Training   100.00% of 1x2503...Epoch=6/150 LR=0.0997 Time=0.580 DataTime=0.407 Loss=2.225 Prec@1=49.666 Prec@5=74.277 rate=1.73 Hz, eta=0:00:00, total=0:24:07, wall=17:54 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:54 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:54 IST
=> Validation 0.00% of 1x98...Epoch=6/150 LR=0.0997 Time=7.287 Loss=1.404 Prec@1=64.258 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=17:54 IST
=> Validation 1.02% of 1x98...Epoch=6/150 LR=0.0997 Time=7.287 Loss=1.404 Prec@1=64.258 Prec@5=87.500 rate=7313.63 Hz, eta=0:00:00, total=0:00:00, wall=17:54 IST
** Validation 1.02% of 1x98...Epoch=6/150 LR=0.0997 Time=7.287 Loss=1.404 Prec@1=64.258 Prec@5=87.500 rate=7313.63 Hz, eta=0:00:00, total=0:00:00, wall=17:55 IST
** Validation 1.02% of 1x98...Epoch=6/150 LR=0.0997 Time=0.639 Loss=2.145 Prec@1=50.368 Prec@5=76.046 rate=7313.63 Hz, eta=0:00:00, total=0:00:00, wall=17:55 IST
** Validation 100.00% of 1x98...Epoch=6/150 LR=0.0997 Time=0.639 Loss=2.145 Prec@1=50.368 Prec@5=76.046 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=17:55 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:55 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:55 IST
=> Training   0.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=5.443 DataTime=4.972 Loss=2.048 Prec@1=52.734 Prec@5=77.148 rate=0 Hz, eta=?, total=0:00:00, wall=17:55 IST
=> Training   0.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=5.443 DataTime=4.972 Loss=2.048 Prec@1=52.734 Prec@5=77.148 rate=9000.41 Hz, eta=0:00:00, total=0:00:00, wall=17:55 IST
=> Training   0.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=5.443 DataTime=4.972 Loss=2.048 Prec@1=52.734 Prec@5=77.148 rate=9000.41 Hz, eta=0:00:00, total=0:00:00, wall=17:56 IST
=> Training   0.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.598 DataTime=0.436 Loss=2.123 Prec@1=51.564 Prec@5=75.882 rate=9000.41 Hz, eta=0:00:00, total=0:00:00, wall=17:56 IST
=> Training   4.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.598 DataTime=0.436 Loss=2.123 Prec@1=51.564 Prec@5=75.882 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=17:56 IST
=> Training   4.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.598 DataTime=0.436 Loss=2.123 Prec@1=51.564 Prec@5=75.882 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=17:57 IST
=> Training   4.04% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.578 DataTime=0.415 Loss=2.129 Prec@1=51.467 Prec@5=75.842 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=17:57 IST
=> Training   8.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.578 DataTime=0.415 Loss=2.129 Prec@1=51.467 Prec@5=75.842 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=17:57 IST
=> Training   8.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.578 DataTime=0.415 Loss=2.129 Prec@1=51.467 Prec@5=75.842 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=17:58 IST
=> Training   8.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.573 DataTime=0.409 Loss=2.130 Prec@1=51.524 Prec@5=75.837 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=17:58 IST
=> Training   12.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.573 DataTime=0.409 Loss=2.130 Prec@1=51.524 Prec@5=75.837 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=17:58 IST
=> Training   12.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.573 DataTime=0.409 Loss=2.130 Prec@1=51.524 Prec@5=75.837 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=17:59 IST
=> Training   12.03% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.401 Loss=2.131 Prec@1=51.544 Prec@5=75.820 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=17:59 IST
=> Training   16.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.401 Loss=2.131 Prec@1=51.544 Prec@5=75.820 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=17:59 IST
=> Training   16.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.401 Loss=2.131 Prec@1=51.544 Prec@5=75.820 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=18:00 IST
=> Training   16.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.399 Loss=2.134 Prec@1=51.444 Prec@5=75.779 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=18:00 IST
=> Training   20.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.399 Loss=2.134 Prec@1=51.444 Prec@5=75.779 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=18:00 IST
=> Training   20.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.566 DataTime=0.399 Loss=2.134 Prec@1=51.444 Prec@5=75.779 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=18:00 IST
=> Training   20.02% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.563 DataTime=0.396 Loss=2.133 Prec@1=51.461 Prec@5=75.788 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=18:00 IST
=> Training   24.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.563 DataTime=0.396 Loss=2.133 Prec@1=51.461 Prec@5=75.788 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=18:00 IST
=> Training   24.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.563 DataTime=0.396 Loss=2.133 Prec@1=51.461 Prec@5=75.788 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=18:01 IST
=> Training   24.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.562 DataTime=0.394 Loss=2.131 Prec@1=51.478 Prec@5=75.784 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=18:01 IST
=> Training   28.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.562 DataTime=0.394 Loss=2.131 Prec@1=51.478 Prec@5=75.784 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=18:01 IST
=> Training   28.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.562 DataTime=0.394 Loss=2.131 Prec@1=51.478 Prec@5=75.784 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=18:02 IST
=> Training   28.01% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.561 DataTime=0.393 Loss=2.132 Prec@1=51.466 Prec@5=75.751 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=18:02 IST
=> Training   32.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.561 DataTime=0.393 Loss=2.132 Prec@1=51.466 Prec@5=75.751 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=18:02 IST
=> Training   32.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.561 DataTime=0.393 Loss=2.132 Prec@1=51.466 Prec@5=75.751 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=18:03 IST
=> Training   32.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.133 Prec@1=51.421 Prec@5=75.728 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=18:03 IST
=> Training   36.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.133 Prec@1=51.421 Prec@5=75.728 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=18:03 IST
=> Training   36.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.133 Prec@1=51.421 Prec@5=75.728 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=18:04 IST
=> Training   36.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.132 Prec@1=51.436 Prec@5=75.753 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=18:04 IST
=> Training   39.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.132 Prec@1=51.436 Prec@5=75.753 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=18:04 IST
=> Training   39.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.392 Loss=2.132 Prec@1=51.436 Prec@5=75.753 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=18:05 IST
=> Training   39.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.132 Prec@1=51.443 Prec@5=75.748 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=18:05 IST
=> Training   43.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.132 Prec@1=51.443 Prec@5=75.748 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=18:05 IST
=> Training   43.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.132 Prec@1=51.443 Prec@5=75.748 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=18:06 IST
=> Training   43.99% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.130 Prec@1=51.465 Prec@5=75.788 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=18:06 IST
=> Training   47.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.130 Prec@1=51.465 Prec@5=75.788 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=18:06 IST
=> Training   47.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.391 Loss=2.130 Prec@1=51.465 Prec@5=75.788 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=18:07 IST
=> Training   47.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.390 Loss=2.131 Prec@1=51.427 Prec@5=75.775 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=18:07 IST
=> Training   51.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.390 Loss=2.131 Prec@1=51.427 Prec@5=75.775 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=18:07 IST
=> Training   51.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.390 Loss=2.131 Prec@1=51.427 Prec@5=75.775 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=18:08 IST
=> Training   51.98% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.389 Loss=2.131 Prec@1=51.452 Prec@5=75.785 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=18:08 IST
=> Training   55.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.389 Loss=2.131 Prec@1=51.452 Prec@5=75.785 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=18:08 IST
=> Training   55.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.389 Loss=2.131 Prec@1=51.452 Prec@5=75.785 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=18:09 IST
=> Training   55.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.132 Prec@1=51.444 Prec@5=75.783 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=18:09 IST
=> Training   59.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.132 Prec@1=51.444 Prec@5=75.783 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=18:09 IST
=> Training   59.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.132 Prec@1=51.444 Prec@5=75.783 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=18:10 IST
=> Training   59.97% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.790 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=18:10 IST
=> Training   63.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.790 rate=1.80 Hz, eta=0:08:20, total=0:14:49, wall=18:10 IST
=> Training   63.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.790 rate=1.80 Hz, eta=0:08:20, total=0:14:49, wall=18:11 IST
=> Training   63.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.795 rate=1.80 Hz, eta=0:08:20, total=0:14:49, wall=18:11 IST
=> Training   67.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.795 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=18:11 IST
=> Training   67.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.447 Prec@5=75.795 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=18:12 IST
=> Training   67.96% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.388 Loss=2.131 Prec@1=51.465 Prec@5=75.783 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=18:12 IST
=> Training   71.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.388 Loss=2.131 Prec@1=51.465 Prec@5=75.783 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=18:12 IST
=> Training   71.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.558 DataTime=0.388 Loss=2.131 Prec@1=51.465 Prec@5=75.783 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=18:12 IST
=> Training   71.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.472 Prec@5=75.783 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=18:12 IST
=> Training   75.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.472 Prec@5=75.783 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=18:12 IST
=> Training   75.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.131 Prec@1=51.472 Prec@5=75.783 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=18:13 IST
=> Training   75.95% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.130 Prec@1=51.466 Prec@5=75.796 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=18:13 IST
=> Training   79.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.130 Prec@1=51.466 Prec@5=75.796 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=18:13 IST
=> Training   79.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.130 Prec@1=51.466 Prec@5=75.796 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=18:14 IST
=> Training   79.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.130 Prec@1=51.463 Prec@5=75.809 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=18:14 IST
=> Training   83.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.130 Prec@1=51.463 Prec@5=75.809 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=18:14 IST
=> Training   83.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.130 Prec@1=51.463 Prec@5=75.809 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=18:15 IST
=> Training   83.94% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.129 Prec@1=51.493 Prec@5=75.826 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=18:15 IST
=> Training   87.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.129 Prec@1=51.493 Prec@5=75.826 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=18:15 IST
=> Training   87.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.388 Loss=2.129 Prec@1=51.493 Prec@5=75.826 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=18:16 IST
=> Training   87.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.389 Loss=2.128 Prec@1=51.510 Prec@5=75.834 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=18:16 IST
=> Training   91.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.389 Loss=2.128 Prec@1=51.510 Prec@5=75.834 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=18:16 IST
=> Training   91.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.560 DataTime=0.389 Loss=2.128 Prec@1=51.510 Prec@5=75.834 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=18:17 IST
=> Training   91.93% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.128 Prec@1=51.502 Prec@5=75.836 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=18:17 IST
=> Training   95.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.128 Prec@1=51.502 Prec@5=75.836 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=18:17 IST
=> Training   95.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.128 Prec@1=51.502 Prec@5=75.836 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=18:18 IST
=> Training   95.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.127 Prec@1=51.525 Prec@5=75.854 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=18:18 IST
=> Training   99.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.127 Prec@1=51.525 Prec@5=75.854 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=18:18 IST
=> Training   99.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.127 Prec@1=51.525 Prec@5=75.854 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=18:18 IST
=> Training   99.92% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.127 Prec@1=51.525 Prec@5=75.855 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=18:18 IST
=> Training   100.00% of 1x2503...Epoch=7/150 LR=0.0996 Time=0.559 DataTime=0.389 Loss=2.127 Prec@1=51.525 Prec@5=75.855 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=18:18 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> Validation 0.00% of 1x98...Epoch=7/150 LR=0.0996 Time=7.058 Loss=1.531 Prec@1=62.500 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=18:18 IST
=> Validation 1.02% of 1x98...Epoch=7/150 LR=0.0996 Time=7.058 Loss=1.531 Prec@1=62.500 Prec@5=85.938 rate=1063.07 Hz, eta=0:00:00, total=0:00:00, wall=18:18 IST
** Validation 1.02% of 1x98...Epoch=7/150 LR=0.0996 Time=7.058 Loss=1.531 Prec@1=62.500 Prec@5=85.938 rate=1063.07 Hz, eta=0:00:00, total=0:00:00, wall=18:19 IST
** Validation 1.02% of 1x98...Epoch=7/150 LR=0.0996 Time=0.641 Loss=2.060 Prec@1=52.546 Prec@5=77.190 rate=1063.07 Hz, eta=0:00:00, total=0:00:00, wall=18:19 IST
** Validation 100.00% of 1x98...Epoch=7/150 LR=0.0996 Time=0.641 Loss=2.060 Prec@1=52.546 Prec@5=77.190 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=18:19 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:19 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:19 IST
=> Training   0.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=4.557 DataTime=4.237 Loss=2.062 Prec@1=51.367 Prec@5=75.977 rate=0 Hz, eta=?, total=0:00:00, wall=18:19 IST
=> Training   0.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=4.557 DataTime=4.237 Loss=2.062 Prec@1=51.367 Prec@5=75.977 rate=1722.98 Hz, eta=0:00:01, total=0:00:00, wall=18:19 IST
=> Training   0.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=4.557 DataTime=4.237 Loss=2.062 Prec@1=51.367 Prec@5=75.977 rate=1722.98 Hz, eta=0:00:01, total=0:00:00, wall=18:20 IST
=> Training   0.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.581 DataTime=0.419 Loss=2.034 Prec@1=53.274 Prec@5=77.185 rate=1722.98 Hz, eta=0:00:01, total=0:00:00, wall=18:20 IST
=> Training   4.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.581 DataTime=0.419 Loss=2.034 Prec@1=53.274 Prec@5=77.185 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=18:20 IST
=> Training   4.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.581 DataTime=0.419 Loss=2.034 Prec@1=53.274 Prec@5=77.185 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=18:21 IST
=> Training   4.04% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.403 Loss=2.046 Prec@1=53.006 Prec@5=77.096 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=18:21 IST
=> Training   8.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.403 Loss=2.046 Prec@1=53.006 Prec@5=77.096 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:21 IST
=> Training   8.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.403 Loss=2.046 Prec@1=53.006 Prec@5=77.096 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:22 IST
=> Training   8.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.402 Loss=2.045 Prec@1=53.190 Prec@5=77.103 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:22 IST
=> Training   12.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.402 Loss=2.045 Prec@1=53.190 Prec@5=77.103 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=18:22 IST
=> Training   12.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.402 Loss=2.045 Prec@1=53.190 Prec@5=77.103 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=18:23 IST
=> Training   12.03% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.570 DataTime=0.401 Loss=2.044 Prec@1=53.201 Prec@5=77.113 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=18:23 IST
=> Training   16.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.570 DataTime=0.401 Loss=2.044 Prec@1=53.201 Prec@5=77.113 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=18:23 IST
=> Training   16.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.570 DataTime=0.401 Loss=2.044 Prec@1=53.201 Prec@5=77.113 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=18:24 IST
=> Training   16.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.400 Loss=2.046 Prec@1=53.115 Prec@5=77.127 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=18:24 IST
=> Training   20.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.400 Loss=2.046 Prec@1=53.115 Prec@5=77.127 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=18:24 IST
=> Training   20.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.568 DataTime=0.400 Loss=2.046 Prec@1=53.115 Prec@5=77.127 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=18:25 IST
=> Training   20.02% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.567 DataTime=0.398 Loss=2.048 Prec@1=53.100 Prec@5=77.084 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=18:25 IST
=> Training   24.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.567 DataTime=0.398 Loss=2.048 Prec@1=53.100 Prec@5=77.084 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:25 IST
=> Training   24.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.567 DataTime=0.398 Loss=2.048 Prec@1=53.100 Prec@5=77.084 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:26 IST
=> Training   24.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.396 Loss=2.048 Prec@1=53.087 Prec@5=77.092 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:26 IST
=> Training   28.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.396 Loss=2.048 Prec@1=53.087 Prec@5=77.092 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=18:26 IST
=> Training   28.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.396 Loss=2.048 Prec@1=53.087 Prec@5=77.092 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=18:27 IST
=> Training   28.01% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.048 Prec@1=53.083 Prec@5=77.057 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=18:27 IST
=> Training   32.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.048 Prec@1=53.083 Prec@5=77.057 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=18:27 IST
=> Training   32.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.048 Prec@1=53.083 Prec@5=77.057 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=18:28 IST
=> Training   32.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.051 Prec@1=53.003 Prec@5=77.024 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=18:28 IST
=> Training   36.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.051 Prec@1=53.003 Prec@5=77.024 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=18:28 IST
=> Training   36.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.395 Loss=2.051 Prec@1=53.003 Prec@5=77.024 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=18:29 IST
=> Training   36.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.394 Loss=2.054 Prec@1=52.940 Prec@5=76.978 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=18:29 IST
=> Training   39.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.394 Loss=2.054 Prec@1=52.940 Prec@5=76.978 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:29 IST
=> Training   39.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.565 DataTime=0.394 Loss=2.054 Prec@1=52.940 Prec@5=76.978 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:29 IST
=> Training   39.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.056 Prec@1=52.920 Prec@5=76.936 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:29 IST
=> Training   43.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.056 Prec@1=52.920 Prec@5=76.936 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:29 IST
=> Training   43.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.056 Prec@1=52.920 Prec@5=76.936 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:30 IST
=> Training   43.99% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.054 Prec@1=52.952 Prec@5=76.957 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:30 IST
=> Training   47.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.054 Prec@1=52.952 Prec@5=76.957 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=18:30 IST
=> Training   47.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.563 DataTime=0.392 Loss=2.054 Prec@1=52.952 Prec@5=76.957 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=18:31 IST
=> Training   47.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.054 Prec@1=52.963 Prec@5=76.978 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=18:31 IST
=> Training   51.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.054 Prec@1=52.963 Prec@5=76.978 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:31 IST
=> Training   51.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.054 Prec@1=52.963 Prec@5=76.978 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:32 IST
=> Training   51.98% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.389 Loss=2.055 Prec@1=52.933 Prec@5=76.965 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:32 IST
=> Training   55.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.389 Loss=2.055 Prec@1=52.933 Prec@5=76.965 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=18:32 IST
=> Training   55.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.389 Loss=2.055 Prec@1=52.933 Prec@5=76.965 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=18:33 IST
=> Training   55.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.056 Prec@1=52.914 Prec@5=76.967 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=18:33 IST
=> Training   59.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.056 Prec@1=52.914 Prec@5=76.967 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=18:33 IST
=> Training   59.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.390 Loss=2.056 Prec@1=52.914 Prec@5=76.967 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=18:34 IST
=> Training   59.97% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.057 Prec@1=52.887 Prec@5=76.959 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=18:34 IST
=> Training   63.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.057 Prec@1=52.887 Prec@5=76.959 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=18:34 IST
=> Training   63.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.057 Prec@1=52.887 Prec@5=76.959 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=18:35 IST
=> Training   63.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.870 Prec@5=76.968 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=18:35 IST
=> Training   67.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.870 Prec@5=76.968 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=18:35 IST
=> Training   67.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.870 Prec@5=76.968 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=18:36 IST
=> Training   67.96% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.873 Prec@5=76.963 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=18:36 IST
=> Training   71.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.873 Prec@5=76.963 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=18:36 IST
=> Training   71.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.056 Prec@1=52.873 Prec@5=76.963 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=18:37 IST
=> Training   71.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.891 Prec@5=76.980 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=18:37 IST
=> Training   75.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.891 Prec@5=76.980 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=18:37 IST
=> Training   75.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.891 Prec@5=76.980 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=18:38 IST
=> Training   75.95% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.901 Prec@5=77.001 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=18:38 IST
=> Training   79.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.901 Prec@5=77.001 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=18:38 IST
=> Training   79.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.055 Prec@1=52.901 Prec@5=77.001 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=18:39 IST
=> Training   79.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.054 Prec@1=52.902 Prec@5=77.010 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=18:39 IST
=> Training   83.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.054 Prec@1=52.902 Prec@5=77.010 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=18:39 IST
=> Training   83.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.561 DataTime=0.391 Loss=2.054 Prec@1=52.902 Prec@5=77.010 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=18:40 IST
=> Training   83.94% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.391 Loss=2.054 Prec@1=52.897 Prec@5=77.012 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=18:40 IST
=> Training   87.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.391 Loss=2.054 Prec@1=52.897 Prec@5=77.012 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=18:40 IST
=> Training   87.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.391 Loss=2.054 Prec@1=52.897 Prec@5=77.012 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=18:41 IST
=> Training   87.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.923 Prec@5=77.022 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=18:41 IST
=> Training   91.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.923 Prec@5=77.022 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=18:41 IST
=> Training   91.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.923 Prec@5=77.022 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=18:42 IST
=> Training   91.93% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.919 Prec@5=77.021 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=18:42 IST
=> Training   95.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.919 Prec@5=77.021 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:42 IST
=> Training   95.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.560 DataTime=0.390 Loss=2.054 Prec@1=52.919 Prec@5=77.021 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:42 IST
=> Training   95.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.559 DataTime=0.390 Loss=2.054 Prec@1=52.922 Prec@5=77.023 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:42 IST
=> Training   99.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.559 DataTime=0.390 Loss=2.054 Prec@1=52.922 Prec@5=77.023 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:42 IST
=> Training   99.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.559 DataTime=0.390 Loss=2.054 Prec@1=52.922 Prec@5=77.023 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:42 IST
=> Training   99.92% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.559 DataTime=0.390 Loss=2.054 Prec@1=52.921 Prec@5=77.023 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:42 IST
=> Training   100.00% of 1x2503...Epoch=8/150 LR=0.0995 Time=0.559 DataTime=0.390 Loss=2.054 Prec@1=52.921 Prec@5=77.023 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=18:42 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST
=> Validation 0.00% of 1x98...Epoch=8/150 LR=0.0995 Time=7.056 Loss=1.476 Prec@1=64.648 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=18:43 IST
=> Validation 1.02% of 1x98...Epoch=8/150 LR=0.0995 Time=7.056 Loss=1.476 Prec@1=64.648 Prec@5=86.914 rate=5928.60 Hz, eta=0:00:00, total=0:00:00, wall=18:43 IST
** Validation 1.02% of 1x98...Epoch=8/150 LR=0.0995 Time=7.056 Loss=1.476 Prec@1=64.648 Prec@5=86.914 rate=5928.60 Hz, eta=0:00:00, total=0:00:00, wall=18:44 IST
** Validation 1.02% of 1x98...Epoch=8/150 LR=0.0995 Time=0.631 Loss=2.093 Prec@1=51.762 Prec@5=76.548 rate=5928.60 Hz, eta=0:00:00, total=0:00:00, wall=18:44 IST
** Validation 100.00% of 1x98...Epoch=8/150 LR=0.0995 Time=0.631 Loss=2.093 Prec@1=51.762 Prec@5=76.548 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=18:44 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:44 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:44 IST
=> Training   0.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=5.110 DataTime=4.905 Loss=1.940 Prec@1=55.273 Prec@5=78.320 rate=0 Hz, eta=?, total=0:00:00, wall=18:44 IST
=> Training   0.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=5.110 DataTime=4.905 Loss=1.940 Prec@1=55.273 Prec@5=78.320 rate=2009.38 Hz, eta=0:00:01, total=0:00:00, wall=18:44 IST
=> Training   0.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=5.110 DataTime=4.905 Loss=1.940 Prec@1=55.273 Prec@5=78.320 rate=2009.38 Hz, eta=0:00:01, total=0:00:00, wall=18:45 IST
=> Training   0.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.598 DataTime=0.439 Loss=1.992 Prec@1=54.250 Prec@5=77.767 rate=2009.38 Hz, eta=0:00:01, total=0:00:00, wall=18:45 IST
=> Training   4.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.598 DataTime=0.439 Loss=1.992 Prec@1=54.250 Prec@5=77.767 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=18:45 IST
=> Training   4.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.598 DataTime=0.439 Loss=1.992 Prec@1=54.250 Prec@5=77.767 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=18:45 IST
=> Training   4.04% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.585 DataTime=0.419 Loss=1.981 Prec@1=54.481 Prec@5=78.045 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=18:45 IST
=> Training   8.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.585 DataTime=0.419 Loss=1.981 Prec@1=54.481 Prec@5=78.045 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=18:45 IST
=> Training   8.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.585 DataTime=0.419 Loss=1.981 Prec@1=54.481 Prec@5=78.045 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=18:46 IST
=> Training   8.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.574 DataTime=0.405 Loss=1.982 Prec@1=54.434 Prec@5=78.029 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=18:46 IST
=> Training   12.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.574 DataTime=0.405 Loss=1.982 Prec@1=54.434 Prec@5=78.029 rate=1.80 Hz, eta=0:20:26, total=0:02:47, wall=18:46 IST
=> Training   12.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.574 DataTime=0.405 Loss=1.982 Prec@1=54.434 Prec@5=78.029 rate=1.80 Hz, eta=0:20:26, total=0:02:47, wall=18:47 IST
=> Training   12.03% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.572 DataTime=0.401 Loss=1.984 Prec@1=54.399 Prec@5=77.989 rate=1.80 Hz, eta=0:20:26, total=0:02:47, wall=18:47 IST
=> Training   16.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.572 DataTime=0.401 Loss=1.984 Prec@1=54.399 Prec@5=77.989 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=18:47 IST
=> Training   16.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.572 DataTime=0.401 Loss=1.984 Prec@1=54.399 Prec@5=77.989 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=18:48 IST
=> Training   16.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.569 DataTime=0.396 Loss=1.986 Prec@1=54.327 Prec@5=77.963 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=18:48 IST
=> Training   20.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.569 DataTime=0.396 Loss=1.986 Prec@1=54.327 Prec@5=77.963 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=18:48 IST
=> Training   20.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.569 DataTime=0.396 Loss=1.986 Prec@1=54.327 Prec@5=77.963 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=18:49 IST
=> Training   20.02% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.568 DataTime=0.394 Loss=1.989 Prec@1=54.293 Prec@5=77.917 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=18:49 IST
=> Training   24.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.568 DataTime=0.394 Loss=1.989 Prec@1=54.293 Prec@5=77.917 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:49 IST
=> Training   24.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.568 DataTime=0.394 Loss=1.989 Prec@1=54.293 Prec@5=77.917 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:50 IST
=> Training   24.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.390 Loss=1.989 Prec@1=54.257 Prec@5=77.915 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:50 IST
=> Training   28.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.390 Loss=1.989 Prec@1=54.257 Prec@5=77.915 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=18:50 IST
=> Training   28.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.390 Loss=1.989 Prec@1=54.257 Prec@5=77.915 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=18:51 IST
=> Training   28.01% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.391 Loss=1.990 Prec@1=54.186 Prec@5=77.914 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=18:51 IST
=> Training   32.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.391 Loss=1.990 Prec@1=54.186 Prec@5=77.914 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=18:51 IST
=> Training   32.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.565 DataTime=0.391 Loss=1.990 Prec@1=54.186 Prec@5=77.914 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=18:52 IST
=> Training   32.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.990 Prec@1=54.177 Prec@5=77.941 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=18:52 IST
=> Training   36.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.990 Prec@1=54.177 Prec@5=77.941 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=18:52 IST
=> Training   36.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.990 Prec@1=54.177 Prec@5=77.941 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=18:53 IST
=> Training   36.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.991 Prec@1=54.139 Prec@5=77.937 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=18:53 IST
=> Training   39.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.991 Prec@1=54.139 Prec@5=77.937 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=18:53 IST
=> Training   39.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.563 DataTime=0.388 Loss=1.991 Prec@1=54.139 Prec@5=77.937 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=18:54 IST
=> Training   39.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.387 Loss=1.990 Prec@1=54.162 Prec@5=77.952 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=18:54 IST
=> Training   43.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.387 Loss=1.990 Prec@1=54.162 Prec@5=77.952 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=18:54 IST
=> Training   43.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.387 Loss=1.990 Prec@1=54.162 Prec@5=77.952 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=18:55 IST
=> Training   43.99% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.990 Prec@1=54.152 Prec@5=77.962 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=18:55 IST
=> Training   47.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.990 Prec@1=54.152 Prec@5=77.962 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=18:55 IST
=> Training   47.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.990 Prec@1=54.152 Prec@5=77.962 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=18:56 IST
=> Training   47.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.992 Prec@1=54.125 Prec@5=77.942 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=18:56 IST
=> Training   51.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.992 Prec@1=54.125 Prec@5=77.942 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:56 IST
=> Training   51.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.992 Prec@1=54.125 Prec@5=77.942 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:57 IST
=> Training   51.98% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.994 Prec@1=54.102 Prec@5=77.936 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=18:57 IST
=> Training   55.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.994 Prec@1=54.102 Prec@5=77.936 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=18:57 IST
=> Training   55.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.562 DataTime=0.389 Loss=1.994 Prec@1=54.102 Prec@5=77.936 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=18:58 IST
=> Training   55.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.389 Loss=1.994 Prec@1=54.107 Prec@5=77.917 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=18:58 IST
=> Training   59.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.389 Loss=1.994 Prec@1=54.107 Prec@5=77.917 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=18:58 IST
=> Training   59.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.561 DataTime=0.389 Loss=1.994 Prec@1=54.107 Prec@5=77.917 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=18:58 IST
=> Training   59.97% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.387 Loss=1.992 Prec@1=54.123 Prec@5=77.954 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=18:58 IST
=> Training   63.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.387 Loss=1.992 Prec@1=54.123 Prec@5=77.954 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=18:58 IST
=> Training   63.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.387 Loss=1.992 Prec@1=54.123 Prec@5=77.954 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=18:59 IST
=> Training   63.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.124 Prec@5=77.952 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=18:59 IST
=> Training   67.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.124 Prec@5=77.952 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=18:59 IST
=> Training   67.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.124 Prec@5=77.952 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=19:00 IST
=> Training   67.96% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.136 Prec@5=77.965 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=19:00 IST
=> Training   71.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.136 Prec@5=77.965 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=19:00 IST
=> Training   71.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.992 Prec@1=54.136 Prec@5=77.965 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=19:01 IST
=> Training   71.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.388 Loss=1.993 Prec@1=54.117 Prec@5=77.949 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=19:01 IST
=> Training   75.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.388 Loss=1.993 Prec@1=54.117 Prec@5=77.949 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=19:01 IST
=> Training   75.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.388 Loss=1.993 Prec@1=54.117 Prec@5=77.949 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=19:02 IST
=> Training   75.95% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.994 Prec@1=54.118 Prec@5=77.936 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=19:02 IST
=> Training   79.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.994 Prec@1=54.118 Prec@5=77.936 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=19:02 IST
=> Training   79.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.994 Prec@1=54.118 Prec@5=77.936 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=19:03 IST
=> Training   79.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.995 Prec@1=54.096 Prec@5=77.928 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=19:03 IST
=> Training   83.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.995 Prec@1=54.096 Prec@5=77.928 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=19:03 IST
=> Training   83.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.388 Loss=1.995 Prec@1=54.096 Prec@5=77.928 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=19:04 IST
=> Training   83.94% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.095 Prec@5=77.926 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=19:04 IST
=> Training   87.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.095 Prec@5=77.926 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:04 IST
=> Training   87.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.095 Prec@5=77.926 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:05 IST
=> Training   87.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.103 Prec@5=77.931 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:05 IST
=> Training   91.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.103 Prec@5=77.931 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:05 IST
=> Training   91.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.103 Prec@5=77.931 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:06 IST
=> Training   91.93% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.092 Prec@5=77.917 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:06 IST
=> Training   95.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.092 Prec@5=77.917 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:06 IST
=> Training   95.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.560 DataTime=0.389 Loss=1.995 Prec@1=54.092 Prec@5=77.917 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:07 IST
=> Training   95.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.387 Loss=1.995 Prec@1=54.086 Prec@5=77.918 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:07 IST
=> Training   99.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.387 Loss=1.995 Prec@1=54.086 Prec@5=77.918 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:07 IST
=> Training   99.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.387 Loss=1.995 Prec@1=54.086 Prec@5=77.918 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:07 IST
=> Training   99.92% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.387 Loss=1.995 Prec@1=54.087 Prec@5=77.917 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:07 IST
=> Training   100.00% of 1x2503...Epoch=9/150 LR=0.0993 Time=0.559 DataTime=0.387 Loss=1.995 Prec@1=54.087 Prec@5=77.917 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=19:07 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:07 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:07 IST
=> Validation 0.00% of 1x98...Epoch=9/150 LR=0.0993 Time=7.297 Loss=1.413 Prec@1=63.672 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=19:07 IST
=> Validation 1.02% of 1x98...Epoch=9/150 LR=0.0993 Time=7.297 Loss=1.413 Prec@1=63.672 Prec@5=86.523 rate=3830.87 Hz, eta=0:00:00, total=0:00:00, wall=19:07 IST
** Validation 1.02% of 1x98...Epoch=9/150 LR=0.0993 Time=7.297 Loss=1.413 Prec@1=63.672 Prec@5=86.523 rate=3830.87 Hz, eta=0:00:00, total=0:00:00, wall=19:08 IST
** Validation 1.02% of 1x98...Epoch=9/150 LR=0.0993 Time=0.639 Loss=1.998 Prec@1=53.588 Prec@5=78.218 rate=3830.87 Hz, eta=0:00:00, total=0:00:00, wall=19:08 IST
** Validation 100.00% of 1x98...Epoch=9/150 LR=0.0993 Time=0.639 Loss=1.998 Prec@1=53.588 Prec@5=78.218 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=19:08 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:08 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:08 IST
=> Training   0.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=4.820 DataTime=4.208 Loss=1.950 Prec@1=57.422 Prec@5=77.344 rate=0 Hz, eta=?, total=0:00:00, wall=19:08 IST
=> Training   0.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=4.820 DataTime=4.208 Loss=1.950 Prec@1=57.422 Prec@5=77.344 rate=6455.11 Hz, eta=0:00:00, total=0:00:00, wall=19:08 IST
=> Training   0.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=4.820 DataTime=4.208 Loss=1.950 Prec@1=57.422 Prec@5=77.344 rate=6455.11 Hz, eta=0:00:00, total=0:00:00, wall=19:09 IST
=> Training   0.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.595 DataTime=0.432 Loss=1.926 Prec@1=55.297 Prec@5=79.113 rate=6455.11 Hz, eta=0:00:00, total=0:00:00, wall=19:09 IST
=> Training   4.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.595 DataTime=0.432 Loss=1.926 Prec@1=55.297 Prec@5=79.113 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=19:09 IST
=> Training   4.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.595 DataTime=0.432 Loss=1.926 Prec@1=55.297 Prec@5=79.113 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=19:10 IST
=> Training   4.04% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.583 DataTime=0.417 Loss=1.934 Prec@1=55.222 Prec@5=78.862 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=19:10 IST
=> Training   8.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.583 DataTime=0.417 Loss=1.934 Prec@1=55.222 Prec@5=78.862 rate=1.79 Hz, eta=0:21:28, total=0:01:52, wall=19:10 IST
=> Training   8.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.583 DataTime=0.417 Loss=1.934 Prec@1=55.222 Prec@5=78.862 rate=1.79 Hz, eta=0:21:28, total=0:01:52, wall=19:11 IST
=> Training   8.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.576 DataTime=0.408 Loss=1.934 Prec@1=55.261 Prec@5=78.800 rate=1.79 Hz, eta=0:21:28, total=0:01:52, wall=19:11 IST
=> Training   12.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.576 DataTime=0.408 Loss=1.934 Prec@1=55.261 Prec@5=78.800 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=19:11 IST
=> Training   12.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.576 DataTime=0.408 Loss=1.934 Prec@1=55.261 Prec@5=78.800 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=19:12 IST
=> Training   12.03% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.574 DataTime=0.402 Loss=1.927 Prec@1=55.323 Prec@5=78.882 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=19:12 IST
=> Training   16.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.574 DataTime=0.402 Loss=1.927 Prec@1=55.323 Prec@5=78.882 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=19:12 IST
=> Training   16.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.574 DataTime=0.402 Loss=1.927 Prec@1=55.323 Prec@5=78.882 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=19:13 IST
=> Training   16.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.570 DataTime=0.398 Loss=1.929 Prec@1=55.305 Prec@5=78.831 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=19:13 IST
=> Training   20.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.570 DataTime=0.398 Loss=1.929 Prec@1=55.305 Prec@5=78.831 rate=1.78 Hz, eta=0:18:42, total=0:04:40, wall=19:13 IST
=> Training   20.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.570 DataTime=0.398 Loss=1.929 Prec@1=55.305 Prec@5=78.831 rate=1.78 Hz, eta=0:18:42, total=0:04:40, wall=19:14 IST
=> Training   20.02% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.569 DataTime=0.396 Loss=1.932 Prec@1=55.276 Prec@5=78.793 rate=1.78 Hz, eta=0:18:42, total=0:04:40, wall=19:14 IST
=> Training   24.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.569 DataTime=0.396 Loss=1.932 Prec@1=55.276 Prec@5=78.793 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=19:14 IST
=> Training   24.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.569 DataTime=0.396 Loss=1.932 Prec@1=55.276 Prec@5=78.793 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=19:15 IST
=> Training   24.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.566 DataTime=0.394 Loss=1.934 Prec@1=55.245 Prec@5=78.795 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=19:15 IST
=> Training   28.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.566 DataTime=0.394 Loss=1.934 Prec@1=55.245 Prec@5=78.795 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=19:15 IST
=> Training   28.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.566 DataTime=0.394 Loss=1.934 Prec@1=55.245 Prec@5=78.795 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=19:15 IST
=> Training   28.01% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.567 DataTime=0.396 Loss=1.936 Prec@1=55.221 Prec@5=78.784 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=19:15 IST
=> Training   32.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.567 DataTime=0.396 Loss=1.936 Prec@1=55.221 Prec@5=78.784 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=19:15 IST
=> Training   32.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.567 DataTime=0.396 Loss=1.936 Prec@1=55.221 Prec@5=78.784 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=19:16 IST
=> Training   32.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.394 Loss=1.938 Prec@1=55.200 Prec@5=78.774 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=19:16 IST
=> Training   36.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.394 Loss=1.938 Prec@1=55.200 Prec@5=78.774 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:16 IST
=> Training   36.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.394 Loss=1.938 Prec@1=55.200 Prec@5=78.774 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:17 IST
=> Training   36.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.393 Loss=1.938 Prec@1=55.218 Prec@5=78.775 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:17 IST
=> Training   39.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.393 Loss=1.938 Prec@1=55.218 Prec@5=78.775 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:17 IST
=> Training   39.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.565 DataTime=0.393 Loss=1.938 Prec@1=55.218 Prec@5=78.775 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:18 IST
=> Training   39.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.392 Loss=1.938 Prec@1=55.205 Prec@5=78.773 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:18 IST
=> Training   43.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.392 Loss=1.938 Prec@1=55.205 Prec@5=78.773 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=19:18 IST
=> Training   43.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.392 Loss=1.938 Prec@1=55.205 Prec@5=78.773 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=19:19 IST
=> Training   43.99% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.391 Loss=1.937 Prec@1=55.195 Prec@5=78.783 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=19:19 IST
=> Training   47.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.391 Loss=1.937 Prec@1=55.195 Prec@5=78.783 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:19 IST
=> Training   47.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.564 DataTime=0.391 Loss=1.937 Prec@1=55.195 Prec@5=78.783 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:20 IST
=> Training   47.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.938 Prec@1=55.186 Prec@5=78.777 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:20 IST
=> Training   51.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.938 Prec@1=55.186 Prec@5=78.777 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=19:20 IST
=> Training   51.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.938 Prec@1=55.186 Prec@5=78.777 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=19:21 IST
=> Training   51.98% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.563 DataTime=0.390 Loss=1.940 Prec@1=55.142 Prec@5=78.754 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=19:21 IST
=> Training   55.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.563 DataTime=0.390 Loss=1.940 Prec@1=55.142 Prec@5=78.754 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=19:21 IST
=> Training   55.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.563 DataTime=0.390 Loss=1.940 Prec@1=55.142 Prec@5=78.754 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=19:22 IST
=> Training   55.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.148 Prec@5=78.752 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=19:22 IST
=> Training   59.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.148 Prec@5=78.752 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=19:22 IST
=> Training   59.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.148 Prec@5=78.752 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=19:23 IST
=> Training   59.97% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.158 Prec@5=78.751 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=19:23 IST
=> Training   63.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.158 Prec@5=78.751 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:23 IST
=> Training   63.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.562 DataTime=0.389 Loss=1.940 Prec@1=55.158 Prec@5=78.751 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:24 IST
=> Training   63.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.389 Loss=1.941 Prec@1=55.135 Prec@5=78.742 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:24 IST
=> Training   67.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.389 Loss=1.941 Prec@1=55.135 Prec@5=78.742 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=19:24 IST
=> Training   67.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.389 Loss=1.941 Prec@1=55.135 Prec@5=78.742 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=19:25 IST
=> Training   67.96% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.107 Prec@5=78.736 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=19:25 IST
=> Training   71.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.107 Prec@5=78.736 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=19:25 IST
=> Training   71.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.107 Prec@5=78.736 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=19:26 IST
=> Training   71.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.096 Prec@5=78.739 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=19:26 IST
=> Training   75.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.096 Prec@5=78.739 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:26 IST
=> Training   75.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.942 Prec@1=55.096 Prec@5=78.739 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:27 IST
=> Training   75.95% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.943 Prec@1=55.088 Prec@5=78.739 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:27 IST
=> Training   79.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.943 Prec@1=55.088 Prec@5=78.739 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=19:27 IST
=> Training   79.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.561 DataTime=0.388 Loss=1.943 Prec@1=55.088 Prec@5=78.739 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=19:28 IST
=> Training   79.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.087 Prec@5=78.749 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=19:28 IST
=> Training   83.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.087 Prec@5=78.749 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:28 IST
=> Training   83.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.087 Prec@5=78.749 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:28 IST
=> Training   83.94% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.078 Prec@5=78.740 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:28 IST
=> Training   87.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.078 Prec@5=78.740 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:28 IST
=> Training   87.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.078 Prec@5=78.740 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:29 IST
=> Training   87.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.091 Prec@5=78.745 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=19:29 IST
=> Training   91.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.091 Prec@5=78.745 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:29 IST
=> Training   91.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.091 Prec@5=78.745 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:30 IST
=> Training   91.93% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.075 Prec@5=78.745 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:30 IST
=> Training   95.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.075 Prec@5=78.745 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:30 IST
=> Training   95.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.943 Prec@1=55.075 Prec@5=78.745 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:31 IST
=> Training   95.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.944 Prec@1=55.066 Prec@5=78.737 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:31 IST
=> Training   99.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.944 Prec@1=55.066 Prec@5=78.737 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:31 IST
=> Training   99.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.944 Prec@1=55.066 Prec@5=78.737 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:31 IST
=> Training   99.92% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.944 Prec@1=55.066 Prec@5=78.736 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:31 IST
=> Training   100.00% of 1x2503...Epoch=10/150 LR=0.0991 Time=0.560 DataTime=0.387 Loss=1.944 Prec@1=55.066 Prec@5=78.736 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=19:31 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:31 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:31 IST
=> Validation 0.00% of 1x98...Epoch=10/150 LR=0.0991 Time=6.796 Loss=1.695 Prec@1=61.133 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=19:31 IST
=> Validation 1.02% of 1x98...Epoch=10/150 LR=0.0991 Time=6.796 Loss=1.695 Prec@1=61.133 Prec@5=82.812 rate=2947.49 Hz, eta=0:00:00, total=0:00:00, wall=19:31 IST
** Validation 1.02% of 1x98...Epoch=10/150 LR=0.0991 Time=6.796 Loss=1.695 Prec@1=61.133 Prec@5=82.812 rate=2947.49 Hz, eta=0:00:00, total=0:00:00, wall=19:32 IST
** Validation 1.02% of 1x98...Epoch=10/150 LR=0.0991 Time=0.622 Loss=2.079 Prec@1=51.982 Prec@5=77.050 rate=2947.49 Hz, eta=0:00:00, total=0:00:00, wall=19:32 IST
** Validation 100.00% of 1x98...Epoch=10/150 LR=0.0991 Time=0.622 Loss=2.079 Prec@1=51.982 Prec@5=77.050 rate=1.81 Hz, eta=0:00:00, total=0:00:54, wall=19:32 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:32 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:32 IST
=> Training   0.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=5.310 DataTime=5.022 Loss=1.783 Prec@1=58.203 Prec@5=79.102 rate=0 Hz, eta=?, total=0:00:00, wall=19:32 IST
=> Training   0.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=5.310 DataTime=5.022 Loss=1.783 Prec@1=58.203 Prec@5=79.102 rate=7036.46 Hz, eta=0:00:00, total=0:00:00, wall=19:32 IST
=> Training   0.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=5.310 DataTime=5.022 Loss=1.783 Prec@1=58.203 Prec@5=79.102 rate=7036.46 Hz, eta=0:00:00, total=0:00:00, wall=19:33 IST
=> Training   0.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.415 Loss=1.870 Prec@1=56.432 Prec@5=79.771 rate=7036.46 Hz, eta=0:00:00, total=0:00:00, wall=19:33 IST
=> Training   4.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.415 Loss=1.870 Prec@1=56.432 Prec@5=79.771 rate=1.91 Hz, eta=0:20:59, total=0:00:52, wall=19:33 IST
=> Training   4.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.415 Loss=1.870 Prec@1=56.432 Prec@5=79.771 rate=1.91 Hz, eta=0:20:59, total=0:00:52, wall=19:34 IST
=> Training   4.04% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.413 Loss=1.884 Prec@1=56.212 Prec@5=79.588 rate=1.91 Hz, eta=0:20:59, total=0:00:52, wall=19:34 IST
=> Training   8.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.413 Loss=1.884 Prec@1=56.212 Prec@5=79.588 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=19:34 IST
=> Training   8.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.577 DataTime=0.413 Loss=1.884 Prec@1=56.212 Prec@5=79.588 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=19:35 IST
=> Training   8.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.572 DataTime=0.403 Loss=1.888 Prec@1=56.021 Prec@5=79.554 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=19:35 IST
=> Training   12.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.572 DataTime=0.403 Loss=1.888 Prec@1=56.021 Prec@5=79.554 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=19:35 IST
=> Training   12.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.572 DataTime=0.403 Loss=1.888 Prec@1=56.021 Prec@5=79.554 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=19:36 IST
=> Training   12.03% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.567 DataTime=0.399 Loss=1.892 Prec@1=55.927 Prec@5=79.463 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=19:36 IST
=> Training   16.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.567 DataTime=0.399 Loss=1.892 Prec@1=55.927 Prec@5=79.463 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:36 IST
=> Training   16.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.567 DataTime=0.399 Loss=1.892 Prec@1=55.927 Prec@5=79.463 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:37 IST
=> Training   16.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.562 DataTime=0.394 Loss=1.891 Prec@1=56.011 Prec@5=79.485 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:37 IST
=> Training   20.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.562 DataTime=0.394 Loss=1.891 Prec@1=56.011 Prec@5=79.485 rate=1.81 Hz, eta=0:18:23, total=0:04:36, wall=19:37 IST
=> Training   20.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.562 DataTime=0.394 Loss=1.891 Prec@1=56.011 Prec@5=79.485 rate=1.81 Hz, eta=0:18:23, total=0:04:36, wall=19:38 IST
=> Training   20.02% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.393 Loss=1.895 Prec@1=55.945 Prec@5=79.398 rate=1.81 Hz, eta=0:18:23, total=0:04:36, wall=19:38 IST
=> Training   24.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.393 Loss=1.895 Prec@1=55.945 Prec@5=79.398 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=19:38 IST
=> Training   24.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.393 Loss=1.895 Prec@1=55.945 Prec@5=79.398 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=19:39 IST
=> Training   24.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.392 Loss=1.894 Prec@1=56.018 Prec@5=79.418 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=19:39 IST
=> Training   28.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.392 Loss=1.894 Prec@1=56.018 Prec@5=79.418 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:39 IST
=> Training   28.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.392 Loss=1.894 Prec@1=56.018 Prec@5=79.418 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:40 IST
=> Training   28.01% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.896 Prec@1=56.005 Prec@5=79.403 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:40 IST
=> Training   32.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.896 Prec@1=56.005 Prec@5=79.403 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=19:40 IST
=> Training   32.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.896 Prec@1=56.005 Prec@5=79.403 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=19:41 IST
=> Training   32.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.979 Prec@5=79.393 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=19:41 IST
=> Training   36.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.979 Prec@5=79.393 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=19:41 IST
=> Training   36.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.979 Prec@5=79.393 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=19:42 IST
=> Training   36.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.391 Loss=1.896 Prec@1=55.984 Prec@5=79.433 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=19:42 IST
=> Training   39.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.391 Loss=1.896 Prec@1=55.984 Prec@5=79.433 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:42 IST
=> Training   39.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.561 DataTime=0.391 Loss=1.896 Prec@1=55.984 Prec@5=79.433 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:43 IST
=> Training   39.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.978 Prec@5=79.399 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:43 IST
=> Training   43.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.978 Prec@5=79.399 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=19:43 IST
=> Training   43.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.391 Loss=1.897 Prec@1=55.978 Prec@5=79.399 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=19:44 IST
=> Training   43.99% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.390 Loss=1.898 Prec@1=55.958 Prec@5=79.394 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=19:44 IST
=> Training   47.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.390 Loss=1.898 Prec@1=55.958 Prec@5=79.394 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=19:44 IST
=> Training   47.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.560 DataTime=0.390 Loss=1.898 Prec@1=55.958 Prec@5=79.394 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=19:44 IST
=> Training   47.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.899 Prec@1=55.926 Prec@5=79.392 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=19:44 IST
=> Training   51.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.899 Prec@1=55.926 Prec@5=79.392 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=19:44 IST
=> Training   51.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.899 Prec@1=55.926 Prec@5=79.392 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=19:45 IST
=> Training   51.98% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.900 Prec@1=55.916 Prec@5=79.389 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=19:45 IST
=> Training   55.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.900 Prec@1=55.916 Prec@5=79.389 rate=1.81 Hz, eta=0:10:10, total=0:12:55, wall=19:45 IST
=> Training   55.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.900 Prec@1=55.916 Prec@5=79.389 rate=1.81 Hz, eta=0:10:10, total=0:12:55, wall=19:46 IST
=> Training   55.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.388 Loss=1.900 Prec@1=55.917 Prec@5=79.400 rate=1.81 Hz, eta=0:10:10, total=0:12:55, wall=19:46 IST
=> Training   59.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.388 Loss=1.900 Prec@1=55.917 Prec@5=79.400 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=19:46 IST
=> Training   59.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.388 Loss=1.900 Prec@1=55.917 Prec@5=79.400 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=19:47 IST
=> Training   59.97% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.915 Prec@5=79.395 rate=1.81 Hz, eta=0:09:14, total=0:13:50, wall=19:47 IST
=> Training   63.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.915 Prec@5=79.395 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=19:47 IST
=> Training   63.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.915 Prec@5=79.395 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=19:48 IST
=> Training   63.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.907 Prec@5=79.391 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=19:48 IST
=> Training   67.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.907 Prec@5=79.391 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=19:48 IST
=> Training   67.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.900 Prec@1=55.907 Prec@5=79.391 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=19:49 IST
=> Training   67.96% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.390 Loss=1.901 Prec@1=55.890 Prec@5=79.378 rate=1.80 Hz, eta=0:07:24, total=0:15:43, wall=19:49 IST
=> Training   71.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.390 Loss=1.901 Prec@1=55.890 Prec@5=79.378 rate=1.80 Hz, eta=0:06:30, total=0:16:40, wall=19:49 IST
=> Training   71.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.390 Loss=1.901 Prec@1=55.890 Prec@5=79.378 rate=1.80 Hz, eta=0:06:30, total=0:16:40, wall=19:50 IST
=> Training   71.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.901 Prec@1=55.890 Prec@5=79.373 rate=1.80 Hz, eta=0:06:30, total=0:16:40, wall=19:50 IST
=> Training   75.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.901 Prec@1=55.890 Prec@5=79.373 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=19:50 IST
=> Training   75.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.557 DataTime=0.389 Loss=1.901 Prec@1=55.890 Prec@5=79.373 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=19:51 IST
=> Training   75.95% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.893 Prec@5=79.388 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=19:51 IST
=> Training   79.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.893 Prec@5=79.388 rate=1.80 Hz, eta=0:04:38, total=0:18:30, wall=19:51 IST
=> Training   79.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.893 Prec@5=79.388 rate=1.80 Hz, eta=0:04:38, total=0:18:30, wall=19:52 IST
=> Training   79.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.871 Prec@5=79.373 rate=1.80 Hz, eta=0:04:38, total=0:18:30, wall=19:52 IST
=> Training   83.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.871 Prec@5=79.373 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=19:52 IST
=> Training   83.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.871 Prec@5=79.373 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=19:53 IST
=> Training   83.94% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.559 DataTime=0.390 Loss=1.902 Prec@1=55.863 Prec@5=79.369 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=19:53 IST
=> Training   87.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.559 DataTime=0.390 Loss=1.902 Prec@1=55.863 Prec@5=79.369 rate=1.80 Hz, eta=0:02:47, total=0:20:24, wall=19:53 IST
=> Training   87.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.559 DataTime=0.390 Loss=1.902 Prec@1=55.863 Prec@5=79.369 rate=1.80 Hz, eta=0:02:47, total=0:20:24, wall=19:54 IST
=> Training   87.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.869 Prec@5=79.372 rate=1.80 Hz, eta=0:02:47, total=0:20:24, wall=19:54 IST
=> Training   91.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.869 Prec@5=79.372 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=19:54 IST
=> Training   91.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.901 Prec@1=55.869 Prec@5=79.372 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=19:55 IST
=> Training   91.93% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.847 Prec@5=79.369 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=19:55 IST
=> Training   95.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.847 Prec@5=79.369 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=19:55 IST
=> Training   95.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.847 Prec@5=79.369 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=19:56 IST
=> Training   95.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.841 Prec@5=79.364 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=19:56 IST
=> Training   99.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.841 Prec@5=79.364 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=19:56 IST
=> Training   99.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.841 Prec@5=79.364 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=19:56 IST
=> Training   99.92% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.840 Prec@5=79.364 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=19:56 IST
=> Training   100.00% of 1x2503...Epoch=11/150 LR=0.0989 Time=0.558 DataTime=0.389 Loss=1.902 Prec@1=55.840 Prec@5=79.364 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=19:56 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST
=> Validation 0.00% of 1x98...Epoch=11/150 LR=0.0989 Time=8.209 Loss=1.413 Prec@1=65.820 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=19:56 IST
=> Validation 1.02% of 1x98...Epoch=11/150 LR=0.0989 Time=8.209 Loss=1.413 Prec@1=65.820 Prec@5=86.719 rate=6231.31 Hz, eta=0:00:00, total=0:00:00, wall=19:56 IST
** Validation 1.02% of 1x98...Epoch=11/150 LR=0.0989 Time=8.209 Loss=1.413 Prec@1=65.820 Prec@5=86.719 rate=6231.31 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST
** Validation 1.02% of 1x98...Epoch=11/150 LR=0.0989 Time=0.647 Loss=2.050 Prec@1=52.432 Prec@5=77.248 rate=6231.31 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST
** Validation 100.00% of 1x98...Epoch=11/150 LR=0.0989 Time=0.647 Loss=2.050 Prec@1=52.432 Prec@5=77.248 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=19:57 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:57 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:57 IST
=> Training   0.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=5.390 DataTime=5.139 Loss=1.914 Prec@1=56.445 Prec@5=78.711 rate=0 Hz, eta=?, total=0:00:00, wall=19:57 IST
=> Training   0.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=5.390 DataTime=5.139 Loss=1.914 Prec@1=56.445 Prec@5=78.711 rate=4775.16 Hz, eta=0:00:00, total=0:00:00, wall=19:57 IST
=> Training   0.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=5.390 DataTime=5.139 Loss=1.914 Prec@1=56.445 Prec@5=78.711 rate=4775.16 Hz, eta=0:00:00, total=0:00:00, wall=19:58 IST
=> Training   0.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.599 DataTime=0.437 Loss=1.841 Prec@1=57.132 Prec@5=80.254 rate=4775.16 Hz, eta=0:00:00, total=0:00:00, wall=19:58 IST
=> Training   4.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.599 DataTime=0.437 Loss=1.841 Prec@1=57.132 Prec@5=80.254 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=19:58 IST
=> Training   4.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.599 DataTime=0.437 Loss=1.841 Prec@1=57.132 Prec@5=80.254 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=19:59 IST
=> Training   4.04% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.578 DataTime=0.414 Loss=1.845 Prec@1=57.092 Prec@5=80.212 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=19:59 IST
=> Training   8.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.578 DataTime=0.414 Loss=1.845 Prec@1=57.092 Prec@5=80.212 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=19:59 IST
=> Training   8.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.578 DataTime=0.414 Loss=1.845 Prec@1=57.092 Prec@5=80.212 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=20:00 IST
=> Training   8.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.569 DataTime=0.397 Loss=1.845 Prec@1=57.038 Prec@5=80.227 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=20:00 IST
=> Training   12.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.569 DataTime=0.397 Loss=1.845 Prec@1=57.038 Prec@5=80.227 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=20:00 IST
=> Training   12.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.569 DataTime=0.397 Loss=1.845 Prec@1=57.038 Prec@5=80.227 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=20:00 IST
=> Training   12.03% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.568 DataTime=0.395 Loss=1.844 Prec@1=57.077 Prec@5=80.194 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=20:00 IST
=> Training   16.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.568 DataTime=0.395 Loss=1.844 Prec@1=57.077 Prec@5=80.194 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=20:00 IST
=> Training   16.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.568 DataTime=0.395 Loss=1.844 Prec@1=57.077 Prec@5=80.194 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=20:01 IST
=> Training   16.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.564 DataTime=0.389 Loss=1.849 Prec@1=56.968 Prec@5=80.133 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=20:01 IST
=> Training   20.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.564 DataTime=0.389 Loss=1.849 Prec@1=56.968 Prec@5=80.133 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=20:01 IST
=> Training   20.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.564 DataTime=0.389 Loss=1.849 Prec@1=56.968 Prec@5=80.133 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=20:02 IST
=> Training   20.02% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.562 DataTime=0.386 Loss=1.849 Prec@1=56.983 Prec@5=80.122 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=20:02 IST
=> Training   24.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.562 DataTime=0.386 Loss=1.849 Prec@1=56.983 Prec@5=80.122 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=20:02 IST
=> Training   24.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.562 DataTime=0.386 Loss=1.849 Prec@1=56.983 Prec@5=80.122 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=20:03 IST
=> Training   24.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.560 DataTime=0.385 Loss=1.853 Prec@1=56.887 Prec@5=80.074 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=20:03 IST
=> Training   28.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.560 DataTime=0.385 Loss=1.853 Prec@1=56.887 Prec@5=80.074 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:03 IST
=> Training   28.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.560 DataTime=0.385 Loss=1.853 Prec@1=56.887 Prec@5=80.074 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:04 IST
=> Training   28.01% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.855 Prec@1=56.843 Prec@5=80.059 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:04 IST
=> Training   32.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.855 Prec@1=56.843 Prec@5=80.059 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=20:04 IST
=> Training   32.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.855 Prec@1=56.843 Prec@5=80.059 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=20:05 IST
=> Training   32.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.857 Prec@1=56.785 Prec@5=80.025 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=20:05 IST
=> Training   36.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.857 Prec@1=56.785 Prec@5=80.025 rate=1.81 Hz, eta=0:14:45, total=0:08:18, wall=20:05 IST
=> Training   36.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.559 DataTime=0.385 Loss=1.857 Prec@1=56.785 Prec@5=80.025 rate=1.81 Hz, eta=0:14:45, total=0:08:18, wall=20:06 IST
=> Training   36.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.759 Prec@5=80.026 rate=1.81 Hz, eta=0:14:45, total=0:08:18, wall=20:06 IST
=> Training   39.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.759 Prec@5=80.026 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=20:06 IST
=> Training   39.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.759 Prec@5=80.026 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=20:07 IST
=> Training   39.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.773 Prec@5=80.053 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=20:07 IST
=> Training   43.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.773 Prec@5=80.053 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=20:07 IST
=> Training   43.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.384 Loss=1.857 Prec@1=56.773 Prec@5=80.053 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=20:08 IST
=> Training   43.99% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.724 Prec@5=80.022 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=20:08 IST
=> Training   47.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.724 Prec@5=80.022 rate=1.81 Hz, eta=0:11:59, total=0:11:03, wall=20:08 IST
=> Training   47.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.724 Prec@5=80.022 rate=1.81 Hz, eta=0:11:59, total=0:11:03, wall=20:09 IST
=> Training   47.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.716 Prec@5=80.006 rate=1.81 Hz, eta=0:11:59, total=0:11:03, wall=20:09 IST
=> Training   51.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.716 Prec@5=80.006 rate=1.81 Hz, eta=0:11:04, total=0:11:59, wall=20:09 IST
=> Training   51.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.859 Prec@1=56.716 Prec@5=80.006 rate=1.81 Hz, eta=0:11:04, total=0:11:59, wall=20:10 IST
=> Training   51.98% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.860 Prec@1=56.725 Prec@5=80.016 rate=1.81 Hz, eta=0:11:04, total=0:11:59, wall=20:10 IST
=> Training   55.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.860 Prec@1=56.725 Prec@5=80.016 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=20:10 IST
=> Training   55.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.860 Prec@1=56.725 Prec@5=80.016 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=20:11 IST
=> Training   55.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.726 Prec@5=80.006 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=20:11 IST
=> Training   59.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.726 Prec@5=80.006 rate=1.80 Hz, eta=0:09:15, total=0:13:51, wall=20:11 IST
=> Training   59.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.726 Prec@5=80.006 rate=1.80 Hz, eta=0:09:15, total=0:13:51, wall=20:12 IST
=> Training   59.97% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.859 Prec@1=56.745 Prec@5=80.027 rate=1.80 Hz, eta=0:09:15, total=0:13:51, wall=20:12 IST
=> Training   63.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.859 Prec@1=56.745 Prec@5=80.027 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:12 IST
=> Training   63.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.859 Prec@1=56.745 Prec@5=80.027 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:13 IST
=> Training   63.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.708 Prec@5=80.012 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:13 IST
=> Training   67.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.708 Prec@5=80.012 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=20:13 IST
=> Training   67.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.860 Prec@1=56.708 Prec@5=80.012 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=20:13 IST
=> Training   67.96% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.861 Prec@1=56.704 Prec@5=79.998 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=20:13 IST
=> Training   71.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.861 Prec@1=56.704 Prec@5=79.998 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=20:13 IST
=> Training   71.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.558 DataTime=0.385 Loss=1.861 Prec@1=56.704 Prec@5=79.998 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=20:14 IST
=> Training   71.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.862 Prec@1=56.686 Prec@5=79.979 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=20:14 IST
=> Training   75.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.862 Prec@1=56.686 Prec@5=79.979 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=20:14 IST
=> Training   75.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.862 Prec@1=56.686 Prec@5=79.979 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=20:15 IST
=> Training   75.95% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.863 Prec@1=56.659 Prec@5=79.965 rate=1.80 Hz, eta=0:05:33, total=0:17:34, wall=20:15 IST
=> Training   79.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.863 Prec@1=56.659 Prec@5=79.965 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=20:15 IST
=> Training   79.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.863 Prec@1=56.659 Prec@5=79.965 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=20:16 IST
=> Training   79.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.649 Prec@5=79.951 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=20:16 IST
=> Training   83.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.649 Prec@5=79.951 rate=1.81 Hz, eta=0:03:42, total=0:19:23, wall=20:16 IST
=> Training   83.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.649 Prec@5=79.951 rate=1.81 Hz, eta=0:03:42, total=0:19:23, wall=20:17 IST
=> Training   83.94% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.640 Prec@5=79.954 rate=1.81 Hz, eta=0:03:42, total=0:19:23, wall=20:17 IST
=> Training   87.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.640 Prec@5=79.954 rate=1.80 Hz, eta=0:02:47, total=0:20:19, wall=20:17 IST
=> Training   87.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.384 Loss=1.864 Prec@1=56.640 Prec@5=79.954 rate=1.80 Hz, eta=0:02:47, total=0:20:19, wall=20:18 IST
=> Training   87.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.623 Prec@5=79.947 rate=1.80 Hz, eta=0:02:47, total=0:20:19, wall=20:18 IST
=> Training   91.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.623 Prec@5=79.947 rate=1.80 Hz, eta=0:01:51, total=0:21:15, wall=20:18 IST
=> Training   91.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.623 Prec@5=79.947 rate=1.80 Hz, eta=0:01:51, total=0:21:15, wall=20:19 IST
=> Training   91.93% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.622 Prec@5=79.946 rate=1.80 Hz, eta=0:01:51, total=0:21:15, wall=20:19 IST
=> Training   95.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.622 Prec@5=79.946 rate=1.80 Hz, eta=0:00:56, total=0:22:11, wall=20:19 IST
=> Training   95.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.557 DataTime=0.385 Loss=1.864 Prec@1=56.622 Prec@5=79.946 rate=1.80 Hz, eta=0:00:56, total=0:22:11, wall=20:20 IST
=> Training   95.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.556 DataTime=0.384 Loss=1.864 Prec@1=56.628 Prec@5=79.947 rate=1.80 Hz, eta=0:00:56, total=0:22:11, wall=20:20 IST
=> Training   99.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.556 DataTime=0.384 Loss=1.864 Prec@1=56.628 Prec@5=79.947 rate=1.80 Hz, eta=0:00:01, total=0:23:06, wall=20:20 IST
=> Training   99.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.556 DataTime=0.384 Loss=1.864 Prec@1=56.628 Prec@5=79.947 rate=1.80 Hz, eta=0:00:01, total=0:23:06, wall=20:20 IST
=> Training   99.92% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.556 DataTime=0.384 Loss=1.864 Prec@1=56.629 Prec@5=79.947 rate=1.80 Hz, eta=0:00:01, total=0:23:06, wall=20:20 IST
=> Training   100.00% of 1x2503...Epoch=12/150 LR=0.0987 Time=0.556 DataTime=0.384 Loss=1.864 Prec@1=56.629 Prec@5=79.947 rate=1.81 Hz, eta=0:00:00, total=0:23:06, wall=20:20 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> Validation 0.00% of 1x98...Epoch=12/150 LR=0.0987 Time=7.397 Loss=1.259 Prec@1=69.141 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=20:20 IST
=> Validation 1.02% of 1x98...Epoch=12/150 LR=0.0987 Time=7.397 Loss=1.259 Prec@1=69.141 Prec@5=88.086 rate=7594.80 Hz, eta=0:00:00, total=0:00:00, wall=20:20 IST
** Validation 1.02% of 1x98...Epoch=12/150 LR=0.0987 Time=7.397 Loss=1.259 Prec@1=69.141 Prec@5=88.086 rate=7594.80 Hz, eta=0:00:00, total=0:00:00, wall=20:21 IST
** Validation 1.02% of 1x98...Epoch=12/150 LR=0.0987 Time=0.641 Loss=1.897 Prec@1=55.812 Prec@5=79.820 rate=7594.80 Hz, eta=0:00:00, total=0:00:00, wall=20:21 IST
** Validation 100.00% of 1x98...Epoch=12/150 LR=0.0987 Time=0.641 Loss=1.897 Prec@1=55.812 Prec@5=79.820 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=20:21 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> Training   0.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=5.124 DataTime=4.831 Loss=1.745 Prec@1=56.445 Prec@5=83.398 rate=0 Hz, eta=?, total=0:00:00, wall=20:21 IST
=> Training   0.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=5.124 DataTime=4.831 Loss=1.745 Prec@1=56.445 Prec@5=83.398 rate=2140.84 Hz, eta=0:00:01, total=0:00:00, wall=20:21 IST
=> Training   0.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=5.124 DataTime=4.831 Loss=1.745 Prec@1=56.445 Prec@5=83.398 rate=2140.84 Hz, eta=0:00:01, total=0:00:00, wall=20:22 IST
=> Training   0.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.589 DataTime=0.426 Loss=1.807 Prec@1=57.749 Prec@5=80.799 rate=2140.84 Hz, eta=0:00:01, total=0:00:00, wall=20:22 IST
=> Training   4.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.589 DataTime=0.426 Loss=1.807 Prec@1=57.749 Prec@5=80.799 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=20:22 IST
=> Training   4.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.589 DataTime=0.426 Loss=1.807 Prec@1=57.749 Prec@5=80.799 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=20:23 IST
=> Training   4.04% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.575 DataTime=0.409 Loss=1.812 Prec@1=57.663 Prec@5=80.772 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=20:23 IST
=> Training   8.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.575 DataTime=0.409 Loss=1.812 Prec@1=57.663 Prec@5=80.772 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:23 IST
=> Training   8.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.575 DataTime=0.409 Loss=1.812 Prec@1=57.663 Prec@5=80.772 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:24 IST
=> Training   8.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.565 DataTime=0.400 Loss=1.808 Prec@1=57.792 Prec@5=80.827 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:24 IST
=> Training   12.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.565 DataTime=0.400 Loss=1.808 Prec@1=57.792 Prec@5=80.827 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=20:24 IST
=> Training   12.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.565 DataTime=0.400 Loss=1.808 Prec@1=57.792 Prec@5=80.827 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=20:25 IST
=> Training   12.03% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.568 DataTime=0.403 Loss=1.811 Prec@1=57.759 Prec@5=80.748 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=20:25 IST
=> Training   16.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.568 DataTime=0.403 Loss=1.811 Prec@1=57.759 Prec@5=80.748 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:25 IST
=> Training   16.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.568 DataTime=0.403 Loss=1.811 Prec@1=57.759 Prec@5=80.748 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:26 IST
=> Training   16.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.564 DataTime=0.399 Loss=1.814 Prec@1=57.717 Prec@5=80.709 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:26 IST
=> Training   20.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.564 DataTime=0.399 Loss=1.814 Prec@1=57.717 Prec@5=80.709 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=20:26 IST
=> Training   20.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.564 DataTime=0.399 Loss=1.814 Prec@1=57.717 Prec@5=80.709 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=20:27 IST
=> Training   20.02% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.816 Prec@1=57.662 Prec@5=80.691 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=20:27 IST
=> Training   24.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.816 Prec@1=57.662 Prec@5=80.691 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=20:27 IST
=> Training   24.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.816 Prec@1=57.662 Prec@5=80.691 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=20:28 IST
=> Training   24.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.817 Prec@1=57.603 Prec@5=80.685 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=20:28 IST
=> Training   28.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.817 Prec@1=57.603 Prec@5=80.685 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:28 IST
=> Training   28.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.563 DataTime=0.396 Loss=1.817 Prec@1=57.603 Prec@5=80.685 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:28 IST
=> Training   28.01% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.561 DataTime=0.393 Loss=1.819 Prec@1=57.547 Prec@5=80.653 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:28 IST
=> Training   32.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.561 DataTime=0.393 Loss=1.819 Prec@1=57.547 Prec@5=80.653 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:28 IST
=> Training   32.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.561 DataTime=0.393 Loss=1.819 Prec@1=57.547 Prec@5=80.653 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:29 IST
=> Training   32.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.504 Prec@5=80.640 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:29 IST
=> Training   36.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.504 Prec@5=80.640 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=20:29 IST
=> Training   36.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.504 Prec@5=80.640 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=20:30 IST
=> Training   36.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.488 Prec@5=80.626 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=20:30 IST
=> Training   39.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.488 Prec@5=80.626 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=20:30 IST
=> Training   39.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.560 DataTime=0.391 Loss=1.820 Prec@1=57.488 Prec@5=80.626 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=20:31 IST
=> Training   39.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.390 Loss=1.821 Prec@1=57.491 Prec@5=80.611 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=20:31 IST
=> Training   43.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.390 Loss=1.821 Prec@1=57.491 Prec@5=80.611 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=20:31 IST
=> Training   43.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.390 Loss=1.821 Prec@1=57.491 Prec@5=80.611 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=20:32 IST
=> Training   43.99% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.822 Prec@1=57.484 Prec@5=80.591 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=20:32 IST
=> Training   47.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.822 Prec@1=57.484 Prec@5=80.591 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=20:32 IST
=> Training   47.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.822 Prec@1=57.484 Prec@5=80.591 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=20:33 IST
=> Training   47.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.824 Prec@1=57.441 Prec@5=80.556 rate=1.80 Hz, eta=0:12:01, total=0:11:05, wall=20:33 IST
=> Training   51.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.824 Prec@1=57.441 Prec@5=80.556 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=20:33 IST
=> Training   51.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.824 Prec@1=57.441 Prec@5=80.556 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=20:34 IST
=> Training   51.98% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.388 Loss=1.825 Prec@1=57.418 Prec@5=80.557 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=20:34 IST
=> Training   55.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.388 Loss=1.825 Prec@1=57.418 Prec@5=80.557 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=20:34 IST
=> Training   55.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.388 Loss=1.825 Prec@1=57.418 Prec@5=80.557 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=20:35 IST
=> Training   55.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.825 Prec@1=57.400 Prec@5=80.554 rate=1.80 Hz, eta=0:10:10, total=0:12:56, wall=20:35 IST
=> Training   59.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.825 Prec@1=57.400 Prec@5=80.554 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=20:35 IST
=> Training   59.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.559 DataTime=0.388 Loss=1.825 Prec@1=57.400 Prec@5=80.554 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=20:36 IST
=> Training   59.97% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.387 Loss=1.826 Prec@1=57.391 Prec@5=80.535 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=20:36 IST
=> Training   63.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.387 Loss=1.826 Prec@1=57.391 Prec@5=80.535 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:36 IST
=> Training   63.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.387 Loss=1.826 Prec@1=57.391 Prec@5=80.535 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:37 IST
=> Training   63.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.826 Prec@1=57.387 Prec@5=80.525 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=20:37 IST
=> Training   67.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.826 Prec@1=57.387 Prec@5=80.525 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=20:37 IST
=> Training   67.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.826 Prec@1=57.387 Prec@5=80.525 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=20:38 IST
=> Training   67.96% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.827 Prec@1=57.371 Prec@5=80.512 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=20:38 IST
=> Training   71.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.827 Prec@1=57.371 Prec@5=80.512 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=20:38 IST
=> Training   71.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.557 DataTime=0.386 Loss=1.827 Prec@1=57.371 Prec@5=80.512 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=20:39 IST
=> Training   71.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.827 Prec@1=57.366 Prec@5=80.514 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=20:39 IST
=> Training   75.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.827 Prec@1=57.366 Prec@5=80.514 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=20:39 IST
=> Training   75.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.827 Prec@1=57.366 Prec@5=80.514 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=20:40 IST
=> Training   75.95% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.827 Prec@1=57.365 Prec@5=80.504 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=20:40 IST
=> Training   79.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.827 Prec@1=57.365 Prec@5=80.504 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=20:40 IST
=> Training   79.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.827 Prec@1=57.365 Prec@5=80.504 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=20:40 IST
=> Training   79.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.828 Prec@1=57.377 Prec@5=80.498 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=20:40 IST
=> Training   83.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.828 Prec@1=57.377 Prec@5=80.498 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=20:40 IST
=> Training   83.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.828 Prec@1=57.377 Prec@5=80.498 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=20:41 IST
=> Training   83.94% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.353 Prec@5=80.471 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=20:41 IST
=> Training   87.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.353 Prec@5=80.471 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=20:41 IST
=> Training   87.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.353 Prec@5=80.471 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=20:42 IST
=> Training   87.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.347 Prec@5=80.469 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=20:42 IST
=> Training   91.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.347 Prec@5=80.469 rate=1.80 Hz, eta=0:01:52, total=0:21:18, wall=20:42 IST
=> Training   91.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.386 Loss=1.829 Prec@1=57.347 Prec@5=80.469 rate=1.80 Hz, eta=0:01:52, total=0:21:18, wall=20:43 IST
=> Training   91.93% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.830 Prec@1=57.326 Prec@5=80.469 rate=1.80 Hz, eta=0:01:52, total=0:21:18, wall=20:43 IST
=> Training   95.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.830 Prec@1=57.326 Prec@5=80.469 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=20:43 IST
=> Training   95.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.830 Prec@1=57.326 Prec@5=80.469 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=20:44 IST
=> Training   95.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.831 Prec@1=57.305 Prec@5=80.462 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=20:44 IST
=> Training   99.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.831 Prec@1=57.305 Prec@5=80.462 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=20:44 IST
=> Training   99.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.831 Prec@1=57.305 Prec@5=80.462 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=20:44 IST
=> Training   99.92% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.831 Prec@1=57.306 Prec@5=80.463 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=20:44 IST
=> Training   100.00% of 1x2503...Epoch=13/150 LR=0.0984 Time=0.558 DataTime=0.387 Loss=1.831 Prec@1=57.306 Prec@5=80.463 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=20:44 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:44 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:44 IST
=> Validation 0.00% of 1x98...Epoch=13/150 LR=0.0984 Time=6.906 Loss=1.373 Prec@1=65.625 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=20:44 IST
=> Validation 1.02% of 1x98...Epoch=13/150 LR=0.0984 Time=6.906 Loss=1.373 Prec@1=65.625 Prec@5=88.086 rate=6523.24 Hz, eta=0:00:00, total=0:00:00, wall=20:44 IST
** Validation 1.02% of 1x98...Epoch=13/150 LR=0.0984 Time=6.906 Loss=1.373 Prec@1=65.625 Prec@5=88.086 rate=6523.24 Hz, eta=0:00:00, total=0:00:00, wall=20:45 IST
** Validation 1.02% of 1x98...Epoch=13/150 LR=0.0984 Time=0.635 Loss=1.839 Prec@1=56.790 Prec@5=80.724 rate=6523.24 Hz, eta=0:00:00, total=0:00:00, wall=20:45 IST
** Validation 100.00% of 1x98...Epoch=13/150 LR=0.0984 Time=0.635 Loss=1.839 Prec@1=56.790 Prec@5=80.724 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=20:45 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:45 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:45 IST
=> Training   0.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=5.453 DataTime=5.216 Loss=1.576 Prec@1=61.719 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=20:45 IST
=> Training   0.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=5.453 DataTime=5.216 Loss=1.576 Prec@1=61.719 Prec@5=85.156 rate=8077.22 Hz, eta=0:00:00, total=0:00:00, wall=20:45 IST
=> Training   0.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=5.453 DataTime=5.216 Loss=1.576 Prec@1=61.719 Prec@5=85.156 rate=8077.22 Hz, eta=0:00:00, total=0:00:00, wall=20:46 IST
=> Training   0.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.596 DataTime=0.428 Loss=1.764 Prec@1=58.671 Prec@5=81.519 rate=8077.22 Hz, eta=0:00:00, total=0:00:00, wall=20:46 IST
=> Training   4.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.596 DataTime=0.428 Loss=1.764 Prec@1=58.671 Prec@5=81.519 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=20:46 IST
=> Training   4.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.596 DataTime=0.428 Loss=1.764 Prec@1=58.671 Prec@5=81.519 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=20:47 IST
=> Training   4.04% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.573 DataTime=0.405 Loss=1.770 Prec@1=58.437 Prec@5=81.363 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=20:47 IST
=> Training   8.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.573 DataTime=0.405 Loss=1.770 Prec@1=58.437 Prec@5=81.363 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=20:47 IST
=> Training   8.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.573 DataTime=0.405 Loss=1.770 Prec@1=58.437 Prec@5=81.363 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=20:48 IST
=> Training   8.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.571 DataTime=0.399 Loss=1.776 Prec@1=58.341 Prec@5=81.349 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=20:48 IST
=> Training   12.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.571 DataTime=0.399 Loss=1.776 Prec@1=58.341 Prec@5=81.349 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=20:48 IST
=> Training   12.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.571 DataTime=0.399 Loss=1.776 Prec@1=58.341 Prec@5=81.349 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=20:49 IST
=> Training   12.03% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.567 DataTime=0.394 Loss=1.778 Prec@1=58.313 Prec@5=81.246 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=20:49 IST
=> Training   16.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.567 DataTime=0.394 Loss=1.778 Prec@1=58.313 Prec@5=81.246 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=20:49 IST
=> Training   16.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.567 DataTime=0.394 Loss=1.778 Prec@1=58.313 Prec@5=81.246 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=20:50 IST
=> Training   16.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.565 DataTime=0.392 Loss=1.776 Prec@1=58.352 Prec@5=81.271 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=20:50 IST
=> Training   20.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.565 DataTime=0.392 Loss=1.776 Prec@1=58.352 Prec@5=81.271 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:50 IST
=> Training   20.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.565 DataTime=0.392 Loss=1.776 Prec@1=58.352 Prec@5=81.271 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:51 IST
=> Training   20.02% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.781 Prec@1=58.248 Prec@5=81.190 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:51 IST
=> Training   24.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.781 Prec@1=58.248 Prec@5=81.190 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=20:51 IST
=> Training   24.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.781 Prec@1=58.248 Prec@5=81.190 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=20:52 IST
=> Training   24.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.783 Prec@1=58.174 Prec@5=81.179 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=20:52 IST
=> Training   28.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.783 Prec@1=58.174 Prec@5=81.179 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:52 IST
=> Training   28.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.783 Prec@1=58.174 Prec@5=81.179 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:53 IST
=> Training   28.01% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.785 Prec@1=58.156 Prec@5=81.126 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=20:53 IST
=> Training   32.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.785 Prec@1=58.156 Prec@5=81.126 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:53 IST
=> Training   32.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.785 Prec@1=58.156 Prec@5=81.126 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:54 IST
=> Training   32.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.563 DataTime=0.390 Loss=1.787 Prec@1=58.129 Prec@5=81.091 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=20:54 IST
=> Training   36.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.563 DataTime=0.390 Loss=1.787 Prec@1=58.129 Prec@5=81.091 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=20:54 IST
=> Training   36.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.563 DataTime=0.390 Loss=1.787 Prec@1=58.129 Prec@5=81.091 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=20:55 IST
=> Training   36.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.789 Prec@1=58.085 Prec@5=81.070 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=20:55 IST
=> Training   39.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.789 Prec@1=58.085 Prec@5=81.070 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:55 IST
=> Training   39.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.789 Prec@1=58.085 Prec@5=81.070 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:56 IST
=> Training   39.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.791 Prec@1=58.063 Prec@5=81.035 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:56 IST
=> Training   43.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.791 Prec@1=58.063 Prec@5=81.035 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=20:56 IST
=> Training   43.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.562 DataTime=0.390 Loss=1.791 Prec@1=58.063 Prec@5=81.035 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=20:56 IST
=> Training   43.99% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.792 Prec@1=58.045 Prec@5=81.023 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=20:56 IST
=> Training   47.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.792 Prec@1=58.045 Prec@5=81.023 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=20:56 IST
=> Training   47.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.792 Prec@1=58.045 Prec@5=81.023 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=20:57 IST
=> Training   47.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.793 Prec@1=58.022 Prec@5=81.003 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=20:57 IST
=> Training   51.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.793 Prec@1=58.022 Prec@5=81.003 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=20:57 IST
=> Training   51.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.793 Prec@1=58.022 Prec@5=81.003 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=20:58 IST
=> Training   51.98% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.794 Prec@1=57.988 Prec@5=80.969 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=20:58 IST
=> Training   55.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.794 Prec@1=57.988 Prec@5=80.969 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:58 IST
=> Training   55.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.794 Prec@1=57.988 Prec@5=80.969 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:59 IST
=> Training   55.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.796 Prec@1=57.966 Prec@5=80.946 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:59 IST
=> Training   59.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.796 Prec@1=57.966 Prec@5=80.946 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=20:59 IST
=> Training   59.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.389 Loss=1.796 Prec@1=57.966 Prec@5=80.946 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=21:00 IST
=> Training   59.97% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.797 Prec@1=57.963 Prec@5=80.941 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=21:00 IST
=> Training   63.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.797 Prec@1=57.963 Prec@5=80.941 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:00 IST
=> Training   63.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.797 Prec@1=57.963 Prec@5=80.941 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:01 IST
=> Training   63.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.798 Prec@1=57.943 Prec@5=80.928 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:01 IST
=> Training   67.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.798 Prec@1=57.943 Prec@5=80.928 rate=1.79 Hz, eta=0:07:26, total=0:15:48, wall=21:01 IST
=> Training   67.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.388 Loss=1.798 Prec@1=57.943 Prec@5=80.928 rate=1.79 Hz, eta=0:07:26, total=0:15:48, wall=21:02 IST
=> Training   67.96% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.387 Loss=1.798 Prec@1=57.937 Prec@5=80.925 rate=1.79 Hz, eta=0:07:26, total=0:15:48, wall=21:02 IST
=> Training   71.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.387 Loss=1.798 Prec@1=57.937 Prec@5=80.925 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=21:02 IST
=> Training   71.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.387 Loss=1.798 Prec@1=57.937 Prec@5=80.925 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=21:03 IST
=> Training   71.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.799 Prec@1=57.921 Prec@5=80.923 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=21:03 IST
=> Training   75.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.799 Prec@1=57.921 Prec@5=80.923 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:03 IST
=> Training   75.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.799 Prec@1=57.921 Prec@5=80.923 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:04 IST
=> Training   75.95% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.800 Prec@1=57.917 Prec@5=80.915 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:04 IST
=> Training   79.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.800 Prec@1=57.917 Prec@5=80.915 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:04 IST
=> Training   79.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.388 Loss=1.800 Prec@1=57.917 Prec@5=80.915 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:05 IST
=> Training   79.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.387 Loss=1.800 Prec@1=57.909 Prec@5=80.907 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:05 IST
=> Training   83.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.387 Loss=1.800 Prec@1=57.909 Prec@5=80.907 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:05 IST
=> Training   83.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.561 DataTime=0.387 Loss=1.800 Prec@1=57.909 Prec@5=80.907 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:06 IST
=> Training   83.94% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.882 Prec@5=80.879 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:06 IST
=> Training   87.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.882 Prec@5=80.879 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=21:06 IST
=> Training   87.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.882 Prec@5=80.879 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=21:07 IST
=> Training   87.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.880 Prec@5=80.875 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=21:07 IST
=> Training   91.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.880 Prec@5=80.875 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=21:07 IST
=> Training   91.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.560 DataTime=0.386 Loss=1.802 Prec@1=57.880 Prec@5=80.875 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=21:08 IST
=> Training   91.93% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.802 Prec@1=57.868 Prec@5=80.871 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=21:08 IST
=> Training   95.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.802 Prec@1=57.868 Prec@5=80.871 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=21:08 IST
=> Training   95.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.802 Prec@1=57.868 Prec@5=80.871 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=21:09 IST
=> Training   95.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.803 Prec@1=57.858 Prec@5=80.868 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=21:09 IST
=> Training   99.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.803 Prec@1=57.858 Prec@5=80.868 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=21:09 IST
=> Training   99.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.803 Prec@1=57.858 Prec@5=80.868 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=21:09 IST
=> Training   99.92% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.803 Prec@1=57.857 Prec@5=80.866 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=21:09 IST
=> Training   100.00% of 1x2503...Epoch=14/150 LR=0.0982 Time=0.559 DataTime=0.385 Loss=1.803 Prec@1=57.857 Prec@5=80.866 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=21:09 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:09 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:09 IST
=> Validation 0.00% of 1x98...Epoch=14/150 LR=0.0982 Time=7.076 Loss=1.120 Prec@1=72.461 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=21:09 IST
=> Validation 1.02% of 1x98...Epoch=14/150 LR=0.0982 Time=7.076 Loss=1.120 Prec@1=72.461 Prec@5=91.406 rate=5356.04 Hz, eta=0:00:00, total=0:00:00, wall=21:09 IST
** Validation 1.02% of 1x98...Epoch=14/150 LR=0.0982 Time=7.076 Loss=1.120 Prec@1=72.461 Prec@5=91.406 rate=5356.04 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST
** Validation 1.02% of 1x98...Epoch=14/150 LR=0.0982 Time=0.630 Loss=1.845 Prec@1=56.706 Prec@5=80.800 rate=5356.04 Hz, eta=0:00:00, total=0:00:00, wall=21:10 IST
** Validation 100.00% of 1x98...Epoch=14/150 LR=0.0982 Time=0.630 Loss=1.845 Prec@1=56.706 Prec@5=80.800 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=21:10 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST
=> Training   0.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=4.837 DataTime=4.352 Loss=1.674 Prec@1=61.328 Prec@5=83.789 rate=0 Hz, eta=?, total=0:00:00, wall=21:10 IST
=> Training   0.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=4.837 DataTime=4.352 Loss=1.674 Prec@1=61.328 Prec@5=83.789 rate=337.20 Hz, eta=0:00:07, total=0:00:00, wall=21:10 IST
=> Training   0.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=4.837 DataTime=4.352 Loss=1.674 Prec@1=61.328 Prec@5=83.789 rate=337.20 Hz, eta=0:00:07, total=0:00:00, wall=21:11 IST
=> Training   0.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.592 DataTime=0.425 Loss=1.737 Prec@1=59.222 Prec@5=81.919 rate=337.20 Hz, eta=0:00:07, total=0:00:00, wall=21:11 IST
=> Training   4.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.592 DataTime=0.425 Loss=1.737 Prec@1=59.222 Prec@5=81.919 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=21:11 IST
=> Training   4.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.592 DataTime=0.425 Loss=1.737 Prec@1=59.222 Prec@5=81.919 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=21:12 IST
=> Training   4.04% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.583 DataTime=0.415 Loss=1.745 Prec@1=59.005 Prec@5=81.795 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=21:12 IST
=> Training   8.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.583 DataTime=0.415 Loss=1.745 Prec@1=59.005 Prec@5=81.795 rate=1.79 Hz, eta=0:21:27, total=0:01:52, wall=21:12 IST
=> Training   8.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.583 DataTime=0.415 Loss=1.745 Prec@1=59.005 Prec@5=81.795 rate=1.79 Hz, eta=0:21:27, total=0:01:52, wall=21:12 IST
=> Training   8.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.568 DataTime=0.397 Loss=1.748 Prec@1=58.890 Prec@5=81.781 rate=1.79 Hz, eta=0:21:27, total=0:01:52, wall=21:12 IST
=> Training   12.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.568 DataTime=0.397 Loss=1.748 Prec@1=58.890 Prec@5=81.781 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=21:12 IST
=> Training   12.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.568 DataTime=0.397 Loss=1.748 Prec@1=58.890 Prec@5=81.781 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=21:13 IST
=> Training   12.03% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.566 DataTime=0.395 Loss=1.755 Prec@1=58.772 Prec@5=81.615 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=21:13 IST
=> Training   16.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.566 DataTime=0.395 Loss=1.755 Prec@1=58.772 Prec@5=81.615 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=21:13 IST
=> Training   16.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.566 DataTime=0.395 Loss=1.755 Prec@1=58.772 Prec@5=81.615 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=21:14 IST
=> Training   16.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.757 Prec@1=58.696 Prec@5=81.608 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=21:14 IST
=> Training   20.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.757 Prec@1=58.696 Prec@5=81.608 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=21:14 IST
=> Training   20.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.757 Prec@1=58.696 Prec@5=81.608 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=21:15 IST
=> Training   20.02% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.759 Prec@1=58.645 Prec@5=81.574 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=21:15 IST
=> Training   24.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.759 Prec@1=58.645 Prec@5=81.574 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=21:15 IST
=> Training   24.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.562 DataTime=0.392 Loss=1.759 Prec@1=58.645 Prec@5=81.574 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=21:16 IST
=> Training   24.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.561 DataTime=0.392 Loss=1.761 Prec@1=58.637 Prec@5=81.542 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=21:16 IST
=> Training   28.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.561 DataTime=0.392 Loss=1.761 Prec@1=58.637 Prec@5=81.542 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=21:16 IST
=> Training   28.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.561 DataTime=0.392 Loss=1.761 Prec@1=58.637 Prec@5=81.542 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=21:17 IST
=> Training   28.01% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.390 Loss=1.761 Prec@1=58.625 Prec@5=81.509 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=21:17 IST
=> Training   32.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.390 Loss=1.761 Prec@1=58.625 Prec@5=81.509 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=21:17 IST
=> Training   32.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.390 Loss=1.761 Prec@1=58.625 Prec@5=81.509 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=21:18 IST
=> Training   32.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.764 Prec@1=58.579 Prec@5=81.444 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=21:18 IST
=> Training   36.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.764 Prec@1=58.579 Prec@5=81.444 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=21:18 IST
=> Training   36.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.764 Prec@1=58.579 Prec@5=81.444 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=21:19 IST
=> Training   36.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.765 Prec@1=58.544 Prec@5=81.422 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=21:19 IST
=> Training   39.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.765 Prec@1=58.544 Prec@5=81.422 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:19 IST
=> Training   39.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.765 Prec@1=58.544 Prec@5=81.422 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:20 IST
=> Training   39.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.767 Prec@1=58.536 Prec@5=81.395 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:20 IST
=> Training   43.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.767 Prec@1=58.536 Prec@5=81.395 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=21:20 IST
=> Training   43.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.767 Prec@1=58.536 Prec@5=81.395 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=21:21 IST
=> Training   43.99% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.768 Prec@1=58.480 Prec@5=81.385 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=21:21 IST
=> Training   47.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.768 Prec@1=58.480 Prec@5=81.385 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:21 IST
=> Training   47.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.389 Loss=1.768 Prec@1=58.480 Prec@5=81.385 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:22 IST
=> Training   47.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.769 Prec@1=58.444 Prec@5=81.378 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:22 IST
=> Training   51.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.769 Prec@1=58.444 Prec@5=81.378 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=21:22 IST
=> Training   51.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.769 Prec@1=58.444 Prec@5=81.378 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=21:23 IST
=> Training   51.98% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.770 Prec@1=58.431 Prec@5=81.377 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=21:23 IST
=> Training   55.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.770 Prec@1=58.431 Prec@5=81.377 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=21:23 IST
=> Training   55.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.770 Prec@1=58.431 Prec@5=81.377 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=21:24 IST
=> Training   55.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.771 Prec@1=58.426 Prec@5=81.362 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=21:24 IST
=> Training   59.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.771 Prec@1=58.426 Prec@5=81.362 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=21:24 IST
=> Training   59.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.388 Loss=1.771 Prec@1=58.426 Prec@5=81.362 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=21:25 IST
=> Training   59.97% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.398 Prec@5=81.340 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=21:25 IST
=> Training   63.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.398 Prec@5=81.340 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=21:25 IST
=> Training   63.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.398 Prec@5=81.340 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=21:25 IST
=> Training   63.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.395 Prec@5=81.331 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=21:25 IST
=> Training   67.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.395 Prec@5=81.331 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:25 IST
=> Training   67.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.387 Loss=1.772 Prec@1=58.395 Prec@5=81.331 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:26 IST
=> Training   67.96% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.774 Prec@1=58.361 Prec@5=81.311 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:26 IST
=> Training   71.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.774 Prec@1=58.361 Prec@5=81.311 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=21:26 IST
=> Training   71.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.774 Prec@1=58.361 Prec@5=81.311 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=21:27 IST
=> Training   71.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.774 Prec@1=58.361 Prec@5=81.303 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=21:27 IST
=> Training   75.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.774 Prec@1=58.361 Prec@5=81.303 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=21:27 IST
=> Training   75.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.774 Prec@1=58.361 Prec@5=81.303 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=21:28 IST
=> Training   75.95% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.776 Prec@1=58.336 Prec@5=81.273 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=21:28 IST
=> Training   79.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.776 Prec@1=58.336 Prec@5=81.273 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=21:28 IST
=> Training   79.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.776 Prec@1=58.336 Prec@5=81.273 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=21:29 IST
=> Training   79.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.776 Prec@1=58.318 Prec@5=81.272 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=21:29 IST
=> Training   83.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.776 Prec@1=58.318 Prec@5=81.272 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=21:29 IST
=> Training   83.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.776 Prec@1=58.318 Prec@5=81.272 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=21:30 IST
=> Training   83.94% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.777 Prec@1=58.310 Prec@5=81.256 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=21:30 IST
=> Training   87.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.777 Prec@1=58.310 Prec@5=81.256 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=21:30 IST
=> Training   87.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.386 Loss=1.777 Prec@1=58.310 Prec@5=81.256 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=21:31 IST
=> Training   87.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.778 Prec@1=58.303 Prec@5=81.248 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=21:31 IST
=> Training   91.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.778 Prec@1=58.303 Prec@5=81.248 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=21:31 IST
=> Training   91.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.559 DataTime=0.385 Loss=1.778 Prec@1=58.303 Prec@5=81.248 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=21:32 IST
=> Training   91.93% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.778 Prec@1=58.306 Prec@5=81.241 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=21:32 IST
=> Training   95.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.778 Prec@1=58.306 Prec@5=81.241 rate=1.80 Hz, eta=0:00:56, total=0:22:14, wall=21:32 IST
=> Training   95.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.778 Prec@1=58.306 Prec@5=81.241 rate=1.80 Hz, eta=0:00:56, total=0:22:14, wall=21:33 IST
=> Training   95.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.779 Prec@1=58.294 Prec@5=81.230 rate=1.80 Hz, eta=0:00:56, total=0:22:14, wall=21:33 IST
=> Training   99.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.779 Prec@1=58.294 Prec@5=81.230 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=21:33 IST
=> Training   99.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.779 Prec@1=58.294 Prec@5=81.230 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=21:33 IST
=> Training   99.92% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.779 Prec@1=58.293 Prec@5=81.229 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=21:33 IST
=> Training   100.00% of 1x2503...Epoch=15/150 LR=0.0979 Time=0.558 DataTime=0.385 Loss=1.779 Prec@1=58.293 Prec@5=81.229 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=21:33 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:33 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:33 IST
=> Validation 0.00% of 1x98...Epoch=15/150 LR=0.0979 Time=6.916 Loss=1.124 Prec@1=70.312 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=21:33 IST
=> Validation 1.02% of 1x98...Epoch=15/150 LR=0.0979 Time=6.916 Loss=1.124 Prec@1=70.312 Prec@5=90.820 rate=6239.12 Hz, eta=0:00:00, total=0:00:00, wall=21:33 IST
** Validation 1.02% of 1x98...Epoch=15/150 LR=0.0979 Time=6.916 Loss=1.124 Prec@1=70.312 Prec@5=90.820 rate=6239.12 Hz, eta=0:00:00, total=0:00:00, wall=21:34 IST
** Validation 1.02% of 1x98...Epoch=15/150 LR=0.0979 Time=0.638 Loss=1.765 Prec@1=58.132 Prec@5=81.852 rate=6239.12 Hz, eta=0:00:00, total=0:00:00, wall=21:34 IST
** Validation 100.00% of 1x98...Epoch=15/150 LR=0.0979 Time=0.638 Loss=1.765 Prec@1=58.132 Prec@5=81.852 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=21:34 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:34 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:34 IST
=> Training   0.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=5.242 DataTime=5.035 Loss=1.747 Prec@1=59.570 Prec@5=81.250 rate=0 Hz, eta=?, total=0:00:00, wall=21:34 IST
=> Training   0.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=5.242 DataTime=5.035 Loss=1.747 Prec@1=59.570 Prec@5=81.250 rate=7356.73 Hz, eta=0:00:00, total=0:00:00, wall=21:34 IST
=> Training   0.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=5.242 DataTime=5.035 Loss=1.747 Prec@1=59.570 Prec@5=81.250 rate=7356.73 Hz, eta=0:00:00, total=0:00:00, wall=21:35 IST
=> Training   0.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.577 DataTime=0.414 Loss=1.717 Prec@1=59.479 Prec@5=82.186 rate=7356.73 Hz, eta=0:00:00, total=0:00:00, wall=21:35 IST
=> Training   4.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.577 DataTime=0.414 Loss=1.717 Prec@1=59.479 Prec@5=82.186 rate=1.90 Hz, eta=0:21:01, total=0:00:53, wall=21:35 IST
=> Training   4.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.577 DataTime=0.414 Loss=1.717 Prec@1=59.479 Prec@5=82.186 rate=1.90 Hz, eta=0:21:01, total=0:00:53, wall=21:36 IST
=> Training   4.04% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.399 Loss=1.720 Prec@1=59.610 Prec@5=82.102 rate=1.90 Hz, eta=0:21:01, total=0:00:53, wall=21:36 IST
=> Training   8.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.399 Loss=1.720 Prec@1=59.610 Prec@5=82.102 rate=1.86 Hz, eta=0:20:39, total=0:01:48, wall=21:36 IST
=> Training   8.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.399 Loss=1.720 Prec@1=59.610 Prec@5=82.102 rate=1.86 Hz, eta=0:20:39, total=0:01:48, wall=21:37 IST
=> Training   8.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.558 DataTime=0.392 Loss=1.723 Prec@1=59.549 Prec@5=82.069 rate=1.86 Hz, eta=0:20:39, total=0:01:48, wall=21:37 IST
=> Training   12.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.558 DataTime=0.392 Loss=1.723 Prec@1=59.549 Prec@5=82.069 rate=1.85 Hz, eta=0:19:51, total=0:02:42, wall=21:37 IST
=> Training   12.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.558 DataTime=0.392 Loss=1.723 Prec@1=59.549 Prec@5=82.069 rate=1.85 Hz, eta=0:19:51, total=0:02:42, wall=21:38 IST
=> Training   12.03% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.392 Loss=1.724 Prec@1=59.470 Prec@5=82.075 rate=1.85 Hz, eta=0:19:51, total=0:02:42, wall=21:38 IST
=> Training   16.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.392 Loss=1.724 Prec@1=59.470 Prec@5=82.075 rate=1.83 Hz, eta=0:19:07, total=0:03:38, wall=21:38 IST
=> Training   16.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.392 Loss=1.724 Prec@1=59.470 Prec@5=82.075 rate=1.83 Hz, eta=0:19:07, total=0:03:38, wall=21:39 IST
=> Training   16.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.394 Loss=1.726 Prec@1=59.401 Prec@5=82.036 rate=1.83 Hz, eta=0:19:07, total=0:03:38, wall=21:39 IST
=> Training   20.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.394 Loss=1.726 Prec@1=59.401 Prec@5=82.036 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=21:39 IST
=> Training   20.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.394 Loss=1.726 Prec@1=59.401 Prec@5=82.036 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=21:40 IST
=> Training   20.02% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.393 Loss=1.729 Prec@1=59.338 Prec@5=81.987 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=21:40 IST
=> Training   24.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.393 Loss=1.729 Prec@1=59.338 Prec@5=81.987 rate=1.81 Hz, eta=0:17:29, total=0:05:31, wall=21:40 IST
=> Training   24.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.393 Loss=1.729 Prec@1=59.338 Prec@5=81.987 rate=1.81 Hz, eta=0:17:29, total=0:05:31, wall=21:41 IST
=> Training   24.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.396 Loss=1.731 Prec@1=59.289 Prec@5=81.957 rate=1.81 Hz, eta=0:17:29, total=0:05:31, wall=21:41 IST
=> Training   28.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.396 Loss=1.731 Prec@1=59.289 Prec@5=81.957 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:41 IST
=> Training   28.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.564 DataTime=0.396 Loss=1.731 Prec@1=59.289 Prec@5=81.957 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:41 IST
=> Training   28.01% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.732 Prec@1=59.245 Prec@5=81.946 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:41 IST
=> Training   32.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.732 Prec@1=59.245 Prec@5=81.946 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=21:41 IST
=> Training   32.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.732 Prec@1=59.245 Prec@5=81.946 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=21:42 IST
=> Training   32.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.735 Prec@1=59.193 Prec@5=81.886 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=21:42 IST
=> Training   36.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.735 Prec@1=59.193 Prec@5=81.886 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:42 IST
=> Training   36.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.562 DataTime=0.393 Loss=1.735 Prec@1=59.193 Prec@5=81.886 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:43 IST
=> Training   36.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.737 Prec@1=59.153 Prec@5=81.848 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:43 IST
=> Training   39.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.737 Prec@1=59.153 Prec@5=81.848 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:43 IST
=> Training   39.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.737 Prec@1=59.153 Prec@5=81.848 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:44 IST
=> Training   39.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.739 Prec@1=59.110 Prec@5=81.830 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=21:44 IST
=> Training   43.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.739 Prec@1=59.110 Prec@5=81.830 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=21:44 IST
=> Training   43.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.739 Prec@1=59.110 Prec@5=81.830 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=21:45 IST
=> Training   43.99% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.740 Prec@1=59.072 Prec@5=81.820 rate=1.80 Hz, eta=0:12:57, total=0:10:10, wall=21:45 IST
=> Training   47.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.740 Prec@1=59.072 Prec@5=81.820 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:45 IST
=> Training   47.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.740 Prec@1=59.072 Prec@5=81.820 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:46 IST
=> Training   47.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.742 Prec@1=59.027 Prec@5=81.784 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=21:46 IST
=> Training   51.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.742 Prec@1=59.027 Prec@5=81.784 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:46 IST
=> Training   51.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.742 Prec@1=59.027 Prec@5=81.784 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:47 IST
=> Training   51.98% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.561 DataTime=0.392 Loss=1.744 Prec@1=58.986 Prec@5=81.754 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:47 IST
=> Training   55.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.561 DataTime=0.392 Loss=1.744 Prec@1=58.986 Prec@5=81.754 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:47 IST
=> Training   55.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.561 DataTime=0.392 Loss=1.744 Prec@1=58.986 Prec@5=81.754 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:48 IST
=> Training   55.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.745 Prec@1=58.980 Prec@5=81.730 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:48 IST
=> Training   59.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.745 Prec@1=58.980 Prec@5=81.730 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=21:48 IST
=> Training   59.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.745 Prec@1=58.980 Prec@5=81.730 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=21:49 IST
=> Training   59.97% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.745 Prec@1=58.961 Prec@5=81.726 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=21:49 IST
=> Training   63.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.745 Prec@1=58.961 Prec@5=81.726 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:49 IST
=> Training   63.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.745 Prec@1=58.961 Prec@5=81.726 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:50 IST
=> Training   63.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.391 Loss=1.747 Prec@1=58.931 Prec@5=81.710 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=21:50 IST
=> Training   67.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.391 Loss=1.747 Prec@1=58.931 Prec@5=81.710 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:50 IST
=> Training   67.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.391 Loss=1.747 Prec@1=58.931 Prec@5=81.710 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:51 IST
=> Training   67.96% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.748 Prec@1=58.903 Prec@5=81.692 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=21:51 IST
=> Training   71.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.748 Prec@1=58.903 Prec@5=81.692 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=21:51 IST
=> Training   71.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.391 Loss=1.748 Prec@1=58.903 Prec@5=81.692 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=21:52 IST
=> Training   71.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.750 Prec@1=58.894 Prec@5=81.671 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=21:52 IST
=> Training   75.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.750 Prec@1=58.894 Prec@5=81.671 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=21:52 IST
=> Training   75.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.750 Prec@1=58.894 Prec@5=81.671 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=21:53 IST
=> Training   75.95% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.750 Prec@1=58.885 Prec@5=81.651 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=21:53 IST
=> Training   79.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.750 Prec@1=58.885 Prec@5=81.651 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:53 IST
=> Training   79.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.390 Loss=1.750 Prec@1=58.885 Prec@5=81.651 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:54 IST
=> Training   79.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.751 Prec@1=58.867 Prec@5=81.636 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=21:54 IST
=> Training   83.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.751 Prec@1=58.867 Prec@5=81.636 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=21:54 IST
=> Training   83.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.751 Prec@1=58.867 Prec@5=81.636 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=21:55 IST
=> Training   83.94% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.752 Prec@1=58.853 Prec@5=81.615 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=21:55 IST
=> Training   87.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.752 Prec@1=58.853 Prec@5=81.615 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=21:55 IST
=> Training   87.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.752 Prec@1=58.853 Prec@5=81.615 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=21:55 IST
=> Training   87.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.754 Prec@1=58.828 Prec@5=81.592 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=21:55 IST
=> Training   91.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.754 Prec@1=58.828 Prec@5=81.592 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=21:55 IST
=> Training   91.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.389 Loss=1.754 Prec@1=58.828 Prec@5=81.592 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=21:56 IST
=> Training   91.93% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.388 Loss=1.754 Prec@1=58.823 Prec@5=81.582 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=21:56 IST
=> Training   95.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.388 Loss=1.754 Prec@1=58.823 Prec@5=81.582 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=21:56 IST
=> Training   95.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.560 DataTime=0.388 Loss=1.754 Prec@1=58.823 Prec@5=81.582 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=21:57 IST
=> Training   95.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.388 Loss=1.755 Prec@1=58.813 Prec@5=81.565 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=21:57 IST
=> Training   99.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.388 Loss=1.755 Prec@1=58.813 Prec@5=81.565 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=21:57 IST
=> Training   99.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.388 Loss=1.755 Prec@1=58.813 Prec@5=81.565 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=21:57 IST
=> Training   99.92% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.388 Loss=1.755 Prec@1=58.813 Prec@5=81.565 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=21:57 IST
=> Training   100.00% of 1x2503...Epoch=16/150 LR=0.0976 Time=0.559 DataTime=0.388 Loss=1.755 Prec@1=58.813 Prec@5=81.565 rate=1.80 Hz, eta=0:00:00, total=0:23:14, wall=21:57 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:57 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:57 IST
=> Validation 0.00% of 1x98...Epoch=16/150 LR=0.0976 Time=7.126 Loss=1.239 Prec@1=67.969 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=21:57 IST
=> Validation 1.02% of 1x98...Epoch=16/150 LR=0.0976 Time=7.126 Loss=1.239 Prec@1=67.969 Prec@5=88.086 rate=7995.08 Hz, eta=0:00:00, total=0:00:00, wall=21:57 IST
** Validation 1.02% of 1x98...Epoch=16/150 LR=0.0976 Time=7.126 Loss=1.239 Prec@1=67.969 Prec@5=88.086 rate=7995.08 Hz, eta=0:00:00, total=0:00:00, wall=21:58 IST
** Validation 1.02% of 1x98...Epoch=16/150 LR=0.0976 Time=0.638 Loss=1.823 Prec@1=57.374 Prec@5=80.994 rate=7995.08 Hz, eta=0:00:00, total=0:00:00, wall=21:58 IST
** Validation 100.00% of 1x98...Epoch=16/150 LR=0.0976 Time=0.638 Loss=1.823 Prec@1=57.374 Prec@5=80.994 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=21:58 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> Training   0.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=5.373 DataTime=5.163 Loss=1.720 Prec@1=60.156 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=21:58 IST
=> Training   0.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=5.373 DataTime=5.163 Loss=1.720 Prec@1=60.156 Prec@5=83.203 rate=10384.76 Hz, eta=0:00:00, total=0:00:00, wall=21:58 IST
=> Training   0.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=5.373 DataTime=5.163 Loss=1.720 Prec@1=60.156 Prec@5=83.203 rate=10384.76 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
=> Training   0.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.594 DataTime=0.428 Loss=1.696 Prec@1=59.996 Prec@5=82.464 rate=10384.76 Hz, eta=0:00:00, total=0:00:00, wall=21:59 IST
=> Training   4.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.594 DataTime=0.428 Loss=1.696 Prec@1=59.996 Prec@5=82.464 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=21:59 IST
=> Training   4.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.594 DataTime=0.428 Loss=1.696 Prec@1=59.996 Prec@5=82.464 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=22:00 IST
=> Training   4.04% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.399 Loss=1.698 Prec@1=59.799 Prec@5=82.393 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=22:00 IST
=> Training   8.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.399 Loss=1.698 Prec@1=59.799 Prec@5=82.393 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=22:00 IST
=> Training   8.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.399 Loss=1.698 Prec@1=59.799 Prec@5=82.393 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=22:01 IST
=> Training   8.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.394 Loss=1.703 Prec@1=59.747 Prec@5=82.324 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=22:01 IST
=> Training   12.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.394 Loss=1.703 Prec@1=59.747 Prec@5=82.324 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=22:01 IST
=> Training   12.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.571 DataTime=0.394 Loss=1.703 Prec@1=59.747 Prec@5=82.324 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=22:02 IST
=> Training   12.03% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.389 Loss=1.703 Prec@1=59.756 Prec@5=82.326 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=22:02 IST
=> Training   16.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.389 Loss=1.703 Prec@1=59.756 Prec@5=82.326 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=22:02 IST
=> Training   16.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.389 Loss=1.703 Prec@1=59.756 Prec@5=82.326 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=22:03 IST
=> Training   16.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.390 Loss=1.706 Prec@1=59.647 Prec@5=82.289 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=22:03 IST
=> Training   20.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.390 Loss=1.706 Prec@1=59.647 Prec@5=82.289 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=22:03 IST
=> Training   20.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.565 DataTime=0.390 Loss=1.706 Prec@1=59.647 Prec@5=82.289 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=22:04 IST
=> Training   20.02% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.708 Prec@1=59.647 Prec@5=82.295 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=22:04 IST
=> Training   24.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.708 Prec@1=59.647 Prec@5=82.295 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=22:04 IST
=> Training   24.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.708 Prec@1=59.647 Prec@5=82.295 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=22:05 IST
=> Training   24.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.710 Prec@1=59.581 Prec@5=82.268 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=22:05 IST
=> Training   28.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.710 Prec@1=59.581 Prec@5=82.268 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=22:05 IST
=> Training   28.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.710 Prec@1=59.581 Prec@5=82.268 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=22:06 IST
=> Training   28.01% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.713 Prec@1=59.513 Prec@5=82.213 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=22:06 IST
=> Training   32.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.713 Prec@1=59.513 Prec@5=82.213 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=22:06 IST
=> Training   32.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.713 Prec@1=59.513 Prec@5=82.213 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=22:07 IST
=> Training   32.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.716 Prec@1=59.500 Prec@5=82.198 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=22:07 IST
=> Training   36.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.716 Prec@1=59.500 Prec@5=82.198 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=22:07 IST
=> Training   36.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.716 Prec@1=59.500 Prec@5=82.198 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=22:08 IST
=> Training   36.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.718 Prec@1=59.462 Prec@5=82.169 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=22:08 IST
=> Training   39.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.718 Prec@1=59.462 Prec@5=82.169 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=22:08 IST
=> Training   39.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.391 Loss=1.718 Prec@1=59.462 Prec@5=82.169 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=22:09 IST
=> Training   39.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.390 Loss=1.718 Prec@1=59.456 Prec@5=82.173 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=22:09 IST
=> Training   43.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.390 Loss=1.718 Prec@1=59.456 Prec@5=82.173 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=22:09 IST
=> Training   43.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.564 DataTime=0.390 Loss=1.718 Prec@1=59.456 Prec@5=82.173 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=22:10 IST
=> Training   43.99% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.427 Prec@5=82.149 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=22:10 IST
=> Training   47.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.427 Prec@5=82.149 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=22:10 IST
=> Training   47.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.427 Prec@5=82.149 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=22:11 IST
=> Training   47.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.429 Prec@5=82.154 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=22:11 IST
=> Training   51.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.429 Prec@5=82.154 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=22:11 IST
=> Training   51.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.563 DataTime=0.390 Loss=1.720 Prec@1=59.429 Prec@5=82.154 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=22:11 IST
=> Training   51.98% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.389 Loss=1.722 Prec@1=59.410 Prec@5=82.132 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=22:11 IST
=> Training   55.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.389 Loss=1.722 Prec@1=59.410 Prec@5=82.132 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:11 IST
=> Training   55.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.389 Loss=1.722 Prec@1=59.410 Prec@5=82.132 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:12 IST
=> Training   55.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.724 Prec@1=59.387 Prec@5=82.110 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:12 IST
=> Training   59.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.724 Prec@1=59.387 Prec@5=82.110 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=22:12 IST
=> Training   59.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.724 Prec@1=59.387 Prec@5=82.110 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=22:13 IST
=> Training   59.97% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.388 Loss=1.724 Prec@1=59.379 Prec@5=82.107 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=22:13 IST
=> Training   63.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.388 Loss=1.724 Prec@1=59.379 Prec@5=82.107 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=22:13 IST
=> Training   63.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.562 DataTime=0.388 Loss=1.724 Prec@1=59.379 Prec@5=82.107 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=22:14 IST
=> Training   63.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.725 Prec@1=59.355 Prec@5=82.090 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=22:14 IST
=> Training   67.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.725 Prec@1=59.355 Prec@5=82.090 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:14 IST
=> Training   67.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.725 Prec@1=59.355 Prec@5=82.090 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:15 IST
=> Training   67.96% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.726 Prec@1=59.334 Prec@5=82.073 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:15 IST
=> Training   71.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.726 Prec@1=59.334 Prec@5=82.073 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=22:15 IST
=> Training   71.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.388 Loss=1.726 Prec@1=59.334 Prec@5=82.073 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=22:16 IST
=> Training   71.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.387 Loss=1.727 Prec@1=59.327 Prec@5=82.047 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=22:16 IST
=> Training   75.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.387 Loss=1.727 Prec@1=59.327 Prec@5=82.047 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:16 IST
=> Training   75.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.561 DataTime=0.387 Loss=1.727 Prec@1=59.327 Prec@5=82.047 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:17 IST
=> Training   75.95% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.728 Prec@1=59.300 Prec@5=82.016 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:17 IST
=> Training   79.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.728 Prec@1=59.300 Prec@5=82.016 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=22:17 IST
=> Training   79.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.728 Prec@1=59.300 Prec@5=82.016 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=22:18 IST
=> Training   79.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.730 Prec@1=59.279 Prec@5=81.991 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=22:18 IST
=> Training   83.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.730 Prec@1=59.279 Prec@5=81.991 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=22:18 IST
=> Training   83.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.730 Prec@1=59.279 Prec@5=81.991 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=22:19 IST
=> Training   83.94% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.732 Prec@1=59.257 Prec@5=81.967 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=22:19 IST
=> Training   87.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.732 Prec@1=59.257 Prec@5=81.967 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=22:19 IST
=> Training   87.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.560 DataTime=0.387 Loss=1.732 Prec@1=59.257 Prec@5=81.967 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=22:20 IST
=> Training   87.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.732 Prec@1=59.240 Prec@5=81.953 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=22:20 IST
=> Training   91.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.732 Prec@1=59.240 Prec@5=81.953 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=22:20 IST
=> Training   91.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.732 Prec@1=59.240 Prec@5=81.953 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=22:21 IST
=> Training   91.93% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.385 Loss=1.733 Prec@1=59.217 Prec@5=81.944 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=22:21 IST
=> Training   95.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.385 Loss=1.733 Prec@1=59.217 Prec@5=81.944 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=22:21 IST
=> Training   95.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.385 Loss=1.733 Prec@1=59.217 Prec@5=81.944 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=22:22 IST
=> Training   95.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.733 Prec@1=59.205 Prec@5=81.927 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=22:22 IST
=> Training   99.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.733 Prec@1=59.205 Prec@5=81.927 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=22:22 IST
=> Training   99.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.733 Prec@1=59.205 Prec@5=81.927 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=22:22 IST
=> Training   99.92% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.733 Prec@1=59.205 Prec@5=81.925 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=22:22 IST
=> Training   100.00% of 1x2503...Epoch=17/150 LR=0.0972 Time=0.559 DataTime=0.386 Loss=1.733 Prec@1=59.205 Prec@5=81.925 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=22:22 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST
=> Validation 0.00% of 1x98...Epoch=17/150 LR=0.0972 Time=7.317 Loss=1.072 Prec@1=71.484 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=22:22 IST
=> Validation 1.02% of 1x98...Epoch=17/150 LR=0.0972 Time=7.317 Loss=1.072 Prec@1=71.484 Prec@5=91.211 rate=5064.60 Hz, eta=0:00:00, total=0:00:00, wall=22:22 IST
** Validation 1.02% of 1x98...Epoch=17/150 LR=0.0972 Time=7.317 Loss=1.072 Prec@1=71.484 Prec@5=91.211 rate=5064.60 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST
** Validation 1.02% of 1x98...Epoch=17/150 LR=0.0972 Time=0.635 Loss=1.791 Prec@1=57.716 Prec@5=81.332 rate=5064.60 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST
** Validation 100.00% of 1x98...Epoch=17/150 LR=0.0972 Time=0.635 Loss=1.791 Prec@1=57.716 Prec@5=81.332 rate=1.78 Hz, eta=0:00:00, total=0:00:54, wall=22:23 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> Training   0.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=5.317 DataTime=5.096 Loss=1.801 Prec@1=58.203 Prec@5=81.055 rate=0 Hz, eta=?, total=0:00:00, wall=22:23 IST
=> Training   0.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=5.317 DataTime=5.096 Loss=1.801 Prec@1=58.203 Prec@5=81.055 rate=7685.16 Hz, eta=0:00:00, total=0:00:00, wall=22:23 IST
=> Training   0.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=5.317 DataTime=5.096 Loss=1.801 Prec@1=58.203 Prec@5=81.055 rate=7685.16 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST
=> Training   0.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.590 DataTime=0.423 Loss=1.683 Prec@1=60.027 Prec@5=82.745 rate=7685.16 Hz, eta=0:00:00, total=0:00:00, wall=22:24 IST
=> Training   4.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.590 DataTime=0.423 Loss=1.683 Prec@1=60.027 Prec@5=82.745 rate=1.86 Hz, eta=0:21:32, total=0:00:54, wall=22:24 IST
=> Training   4.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.590 DataTime=0.423 Loss=1.683 Prec@1=60.027 Prec@5=82.745 rate=1.86 Hz, eta=0:21:32, total=0:00:54, wall=22:25 IST
=> Training   4.04% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.576 DataTime=0.407 Loss=1.689 Prec@1=59.952 Prec@5=82.505 rate=1.86 Hz, eta=0:21:32, total=0:00:54, wall=22:25 IST
=> Training   8.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.576 DataTime=0.407 Loss=1.689 Prec@1=59.952 Prec@5=82.505 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=22:25 IST
=> Training   8.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.576 DataTime=0.407 Loss=1.689 Prec@1=59.952 Prec@5=82.505 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=22:26 IST
=> Training   8.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.570 DataTime=0.400 Loss=1.688 Prec@1=60.015 Prec@5=82.498 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=22:26 IST
=> Training   12.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.570 DataTime=0.400 Loss=1.688 Prec@1=60.015 Prec@5=82.498 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=22:26 IST
=> Training   12.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.570 DataTime=0.400 Loss=1.688 Prec@1=60.015 Prec@5=82.498 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=22:27 IST
=> Training   12.03% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.571 DataTime=0.400 Loss=1.688 Prec@1=60.041 Prec@5=82.552 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=22:27 IST
=> Training   16.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.571 DataTime=0.400 Loss=1.688 Prec@1=60.041 Prec@5=82.552 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=22:27 IST
=> Training   16.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.571 DataTime=0.400 Loss=1.688 Prec@1=60.041 Prec@5=82.552 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=22:27 IST
=> Training   16.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.567 DataTime=0.394 Loss=1.691 Prec@1=60.000 Prec@5=82.503 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=22:27 IST
=> Training   20.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.567 DataTime=0.394 Loss=1.691 Prec@1=60.000 Prec@5=82.503 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:27 IST
=> Training   20.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.567 DataTime=0.394 Loss=1.691 Prec@1=60.000 Prec@5=82.503 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:28 IST
=> Training   20.02% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.568 DataTime=0.395 Loss=1.695 Prec@1=59.939 Prec@5=82.446 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:28 IST
=> Training   24.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.568 DataTime=0.395 Loss=1.695 Prec@1=59.939 Prec@5=82.446 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:28 IST
=> Training   24.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.568 DataTime=0.395 Loss=1.695 Prec@1=59.939 Prec@5=82.446 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:29 IST
=> Training   24.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.393 Loss=1.696 Prec@1=59.933 Prec@5=82.448 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:29 IST
=> Training   28.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.393 Loss=1.696 Prec@1=59.933 Prec@5=82.448 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=22:29 IST
=> Training   28.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.393 Loss=1.696 Prec@1=59.933 Prec@5=82.448 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=22:30 IST
=> Training   28.01% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.392 Loss=1.697 Prec@1=59.915 Prec@5=82.435 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=22:30 IST
=> Training   32.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.392 Loss=1.697 Prec@1=59.915 Prec@5=82.435 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=22:30 IST
=> Training   32.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.566 DataTime=0.392 Loss=1.697 Prec@1=59.915 Prec@5=82.435 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=22:31 IST
=> Training   32.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.564 DataTime=0.390 Loss=1.699 Prec@1=59.848 Prec@5=82.399 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=22:31 IST
=> Training   36.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.564 DataTime=0.390 Loss=1.699 Prec@1=59.848 Prec@5=82.399 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=22:31 IST
=> Training   36.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.564 DataTime=0.390 Loss=1.699 Prec@1=59.848 Prec@5=82.399 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=22:32 IST
=> Training   36.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.389 Loss=1.700 Prec@1=59.855 Prec@5=82.397 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=22:32 IST
=> Training   39.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.389 Loss=1.700 Prec@1=59.855 Prec@5=82.397 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:32 IST
=> Training   39.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.389 Loss=1.700 Prec@1=59.855 Prec@5=82.397 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:33 IST
=> Training   39.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.388 Loss=1.703 Prec@1=59.788 Prec@5=82.356 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:33 IST
=> Training   43.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.388 Loss=1.703 Prec@1=59.788 Prec@5=82.356 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=22:33 IST
=> Training   43.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.388 Loss=1.703 Prec@1=59.788 Prec@5=82.356 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=22:34 IST
=> Training   43.99% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.705 Prec@1=59.772 Prec@5=82.325 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=22:34 IST
=> Training   47.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.705 Prec@1=59.772 Prec@5=82.325 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=22:34 IST
=> Training   47.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.705 Prec@1=59.772 Prec@5=82.325 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=22:35 IST
=> Training   47.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.388 Loss=1.706 Prec@1=59.740 Prec@5=82.311 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=22:35 IST
=> Training   51.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.388 Loss=1.706 Prec@1=59.740 Prec@5=82.311 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=22:35 IST
=> Training   51.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.388 Loss=1.706 Prec@1=59.740 Prec@5=82.311 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=22:36 IST
=> Training   51.98% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.707 Prec@1=59.695 Prec@5=82.277 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=22:36 IST
=> Training   55.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.707 Prec@1=59.695 Prec@5=82.277 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=22:36 IST
=> Training   55.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.389 Loss=1.707 Prec@1=59.695 Prec@5=82.277 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=22:37 IST
=> Training   55.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.389 Loss=1.707 Prec@1=59.713 Prec@5=82.285 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=22:37 IST
=> Training   59.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.389 Loss=1.707 Prec@1=59.713 Prec@5=82.285 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=22:37 IST
=> Training   59.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.389 Loss=1.707 Prec@1=59.713 Prec@5=82.285 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=22:38 IST
=> Training   59.97% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.708 Prec@1=59.704 Prec@5=82.271 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=22:38 IST
=> Training   63.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.708 Prec@1=59.704 Prec@5=82.271 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=22:38 IST
=> Training   63.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.708 Prec@1=59.704 Prec@5=82.271 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=22:39 IST
=> Training   63.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.709 Prec@1=59.696 Prec@5=82.261 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=22:39 IST
=> Training   67.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.709 Prec@1=59.696 Prec@5=82.261 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:39 IST
=> Training   67.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.561 DataTime=0.390 Loss=1.709 Prec@1=59.696 Prec@5=82.261 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:40 IST
=> Training   67.96% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.710 Prec@1=59.681 Prec@5=82.243 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=22:40 IST
=> Training   71.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.710 Prec@1=59.681 Prec@5=82.243 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=22:40 IST
=> Training   71.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.710 Prec@1=59.681 Prec@5=82.243 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=22:41 IST
=> Training   71.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.710 Prec@1=59.663 Prec@5=82.244 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=22:41 IST
=> Training   75.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.710 Prec@1=59.663 Prec@5=82.244 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=22:41 IST
=> Training   75.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.710 Prec@1=59.663 Prec@5=82.244 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=22:41 IST
=> Training   75.95% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.391 Loss=1.712 Prec@1=59.642 Prec@5=82.223 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=22:41 IST
=> Training   79.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.391 Loss=1.712 Prec@1=59.642 Prec@5=82.223 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:41 IST
=> Training   79.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.391 Loss=1.712 Prec@1=59.642 Prec@5=82.223 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:42 IST
=> Training   79.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.638 Prec@5=82.225 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:42 IST
=> Training   83.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.638 Prec@5=82.225 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:42 IST
=> Training   83.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.638 Prec@5=82.225 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:43 IST
=> Training   83.94% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.627 Prec@5=82.220 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:43 IST
=> Training   87.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.627 Prec@5=82.220 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=22:43 IST
=> Training   87.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.712 Prec@1=59.627 Prec@5=82.220 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=22:44 IST
=> Training   87.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.609 Prec@5=82.207 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=22:44 IST
=> Training   91.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.609 Prec@5=82.207 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:44 IST
=> Training   91.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.609 Prec@5=82.207 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:45 IST
=> Training   91.93% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.602 Prec@5=82.194 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:45 IST
=> Training   95.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.602 Prec@5=82.194 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=22:45 IST
=> Training   95.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.562 DataTime=0.391 Loss=1.713 Prec@1=59.602 Prec@5=82.194 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=22:46 IST
=> Training   95.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.714 Prec@1=59.586 Prec@5=82.182 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=22:46 IST
=> Training   99.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.714 Prec@1=59.586 Prec@5=82.182 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:46 IST
=> Training   99.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.714 Prec@1=59.586 Prec@5=82.182 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:46 IST
=> Training   99.92% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.714 Prec@1=59.586 Prec@5=82.182 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:46 IST
=> Training   100.00% of 1x2503...Epoch=18/150 LR=0.0969 Time=0.563 DataTime=0.392 Loss=1.714 Prec@1=59.586 Prec@5=82.182 rate=1.78 Hz, eta=0:00:00, total=0:23:22, wall=22:46 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:46 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:46 IST
=> Validation 0.00% of 1x98...Epoch=18/150 LR=0.0969 Time=7.096 Loss=1.169 Prec@1=69.141 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=22:46 IST
=> Validation 1.02% of 1x98...Epoch=18/150 LR=0.0969 Time=7.096 Loss=1.169 Prec@1=69.141 Prec@5=91.602 rate=5440.31 Hz, eta=0:00:00, total=0:00:00, wall=22:46 IST
** Validation 1.02% of 1x98...Epoch=18/150 LR=0.0969 Time=7.096 Loss=1.169 Prec@1=69.141 Prec@5=91.602 rate=5440.31 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST
** Validation 1.02% of 1x98...Epoch=18/150 LR=0.0969 Time=0.637 Loss=1.764 Prec@1=58.400 Prec@5=81.956 rate=5440.31 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST
** Validation 100.00% of 1x98...Epoch=18/150 LR=0.0969 Time=0.637 Loss=1.764 Prec@1=58.400 Prec@5=81.956 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=22:47 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST
=> Training   0.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=5.197 DataTime=5.026 Loss=1.560 Prec@1=64.648 Prec@5=85.352 rate=0 Hz, eta=?, total=0:00:00, wall=22:47 IST
=> Training   0.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=5.197 DataTime=5.026 Loss=1.560 Prec@1=64.648 Prec@5=85.352 rate=6599.96 Hz, eta=0:00:00, total=0:00:00, wall=22:47 IST
=> Training   0.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=5.197 DataTime=5.026 Loss=1.560 Prec@1=64.648 Prec@5=85.352 rate=6599.96 Hz, eta=0:00:00, total=0:00:00, wall=22:48 IST
=> Training   0.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.601 DataTime=0.432 Loss=1.667 Prec@1=60.642 Prec@5=83.006 rate=6599.96 Hz, eta=0:00:00, total=0:00:00, wall=22:48 IST
=> Training   4.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.601 DataTime=0.432 Loss=1.667 Prec@1=60.642 Prec@5=83.006 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=22:48 IST
=> Training   4.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.601 DataTime=0.432 Loss=1.667 Prec@1=60.642 Prec@5=83.006 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=22:49 IST
=> Training   4.04% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.583 DataTime=0.413 Loss=1.667 Prec@1=60.603 Prec@5=82.933 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=22:49 IST
=> Training   8.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.583 DataTime=0.413 Loss=1.667 Prec@1=60.603 Prec@5=82.933 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=22:49 IST
=> Training   8.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.583 DataTime=0.413 Loss=1.667 Prec@1=60.603 Prec@5=82.933 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=22:50 IST
=> Training   8.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.576 DataTime=0.403 Loss=1.678 Prec@1=60.340 Prec@5=82.792 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=22:50 IST
=> Training   12.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.576 DataTime=0.403 Loss=1.678 Prec@1=60.340 Prec@5=82.792 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=22:50 IST
=> Training   12.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.576 DataTime=0.403 Loss=1.678 Prec@1=60.340 Prec@5=82.792 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=22:51 IST
=> Training   12.03% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.572 DataTime=0.398 Loss=1.678 Prec@1=60.325 Prec@5=82.799 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=22:51 IST
=> Training   16.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.572 DataTime=0.398 Loss=1.678 Prec@1=60.325 Prec@5=82.799 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=22:51 IST
=> Training   16.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.572 DataTime=0.398 Loss=1.678 Prec@1=60.325 Prec@5=82.799 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=22:52 IST
=> Training   16.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.567 DataTime=0.391 Loss=1.681 Prec@1=60.282 Prec@5=82.728 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=22:52 IST
=> Training   20.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.567 DataTime=0.391 Loss=1.681 Prec@1=60.282 Prec@5=82.728 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=22:52 IST
=> Training   20.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.567 DataTime=0.391 Loss=1.681 Prec@1=60.282 Prec@5=82.728 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=22:53 IST
=> Training   20.02% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.564 DataTime=0.388 Loss=1.682 Prec@1=60.240 Prec@5=82.698 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=22:53 IST
=> Training   24.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.564 DataTime=0.388 Loss=1.682 Prec@1=60.240 Prec@5=82.698 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=22:53 IST
=> Training   24.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.564 DataTime=0.388 Loss=1.682 Prec@1=60.240 Prec@5=82.698 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=22:54 IST
=> Training   24.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.388 Loss=1.684 Prec@1=60.224 Prec@5=82.669 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=22:54 IST
=> Training   28.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.388 Loss=1.684 Prec@1=60.224 Prec@5=82.669 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=22:54 IST
=> Training   28.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.388 Loss=1.684 Prec@1=60.224 Prec@5=82.669 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=22:55 IST
=> Training   28.01% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.389 Loss=1.686 Prec@1=60.146 Prec@5=82.657 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=22:55 IST
=> Training   32.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.389 Loss=1.686 Prec@1=60.146 Prec@5=82.657 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=22:55 IST
=> Training   32.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.389 Loss=1.686 Prec@1=60.146 Prec@5=82.657 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=22:56 IST
=> Training   32.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.388 Loss=1.686 Prec@1=60.136 Prec@5=82.647 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=22:56 IST
=> Training   36.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.388 Loss=1.686 Prec@1=60.136 Prec@5=82.647 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=22:56 IST
=> Training   36.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.388 Loss=1.686 Prec@1=60.136 Prec@5=82.647 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=22:57 IST
=> Training   36.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.391 Loss=1.685 Prec@1=60.149 Prec@5=82.644 rate=1.81 Hz, eta=0:14:47, total=0:08:18, wall=22:57 IST
=> Training   39.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.391 Loss=1.685 Prec@1=60.149 Prec@5=82.644 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=22:57 IST
=> Training   39.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.561 DataTime=0.391 Loss=1.685 Prec@1=60.149 Prec@5=82.644 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=22:58 IST
=> Training   39.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.124 Prec@5=82.628 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=22:58 IST
=> Training   43.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.124 Prec@5=82.628 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=22:58 IST
=> Training   43.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.124 Prec@5=82.628 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=22:58 IST
=> Training   43.99% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.136 Prec@5=82.634 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=22:58 IST
=> Training   47.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.136 Prec@5=82.634 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=22:58 IST
=> Training   47.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.560 DataTime=0.389 Loss=1.686 Prec@1=60.136 Prec@5=82.634 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=22:59 IST
=> Training   47.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.687 Prec@1=60.129 Prec@5=82.610 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=22:59 IST
=> Training   51.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.687 Prec@1=60.129 Prec@5=82.610 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=22:59 IST
=> Training   51.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.687 Prec@1=60.129 Prec@5=82.610 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=23:00 IST
=> Training   51.98% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.687 Prec@1=60.128 Prec@5=82.605 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=23:00 IST
=> Training   55.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.687 Prec@1=60.128 Prec@5=82.605 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=23:00 IST
=> Training   55.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.687 Prec@1=60.128 Prec@5=82.605 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=23:01 IST
=> Training   55.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.688 Prec@1=60.109 Prec@5=82.609 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=23:01 IST
=> Training   59.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.688 Prec@1=60.109 Prec@5=82.609 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=23:01 IST
=> Training   59.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.688 Prec@1=60.109 Prec@5=82.609 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=23:02 IST
=> Training   59.97% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.689 Prec@1=60.090 Prec@5=82.574 rate=1.80 Hz, eta=0:09:15, total=0:13:52, wall=23:02 IST
=> Training   63.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.689 Prec@1=60.090 Prec@5=82.574 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=23:02 IST
=> Training   63.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.689 Prec@1=60.090 Prec@5=82.574 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=23:03 IST
=> Training   63.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.691 Prec@1=60.072 Prec@5=82.552 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=23:03 IST
=> Training   67.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.691 Prec@1=60.072 Prec@5=82.552 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=23:03 IST
=> Training   67.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.691 Prec@1=60.072 Prec@5=82.552 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=23:04 IST
=> Training   67.96% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.692 Prec@1=60.054 Prec@5=82.541 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=23:04 IST
=> Training   71.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.692 Prec@1=60.054 Prec@5=82.541 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=23:04 IST
=> Training   71.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.692 Prec@1=60.054 Prec@5=82.541 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=23:05 IST
=> Training   71.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.067 Prec@5=82.537 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=23:05 IST
=> Training   75.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.067 Prec@5=82.537 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=23:05 IST
=> Training   75.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.067 Prec@5=82.537 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=23:06 IST
=> Training   75.95% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.048 Prec@5=82.520 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=23:06 IST
=> Training   79.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.048 Prec@5=82.520 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=23:06 IST
=> Training   79.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.692 Prec@1=60.048 Prec@5=82.520 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=23:07 IST
=> Training   79.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.693 Prec@1=60.024 Prec@5=82.510 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=23:07 IST
=> Training   83.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.693 Prec@1=60.024 Prec@5=82.510 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=23:07 IST
=> Training   83.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.389 Loss=1.693 Prec@1=60.024 Prec@5=82.510 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=23:08 IST
=> Training   83.94% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.694 Prec@1=60.015 Prec@5=82.498 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=23:08 IST
=> Training   87.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.694 Prec@1=60.015 Prec@5=82.498 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=23:08 IST
=> Training   87.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.694 Prec@1=60.015 Prec@5=82.498 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=23:09 IST
=> Training   87.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=60.008 Prec@5=82.488 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=23:09 IST
=> Training   91.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=60.008 Prec@5=82.488 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=23:09 IST
=> Training   91.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=60.008 Prec@5=82.488 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=23:10 IST
=> Training   91.93% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=59.994 Prec@5=82.478 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=23:10 IST
=> Training   95.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=59.994 Prec@5=82.478 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=23:10 IST
=> Training   95.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.559 DataTime=0.388 Loss=1.695 Prec@1=59.994 Prec@5=82.478 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=23:11 IST
=> Training   95.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.696 Prec@1=59.978 Prec@5=82.463 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=23:11 IST
=> Training   99.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.696 Prec@1=59.978 Prec@5=82.463 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=23:11 IST
=> Training   99.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.696 Prec@1=59.978 Prec@5=82.463 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=23:11 IST
=> Training   99.92% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.696 Prec@1=59.978 Prec@5=82.463 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=23:11 IST
=> Training   100.00% of 1x2503...Epoch=19/150 LR=0.0965 Time=0.558 DataTime=0.388 Loss=1.696 Prec@1=59.978 Prec@5=82.463 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=23:11 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:11 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:11 IST
=> Validation 0.00% of 1x98...Epoch=19/150 LR=0.0965 Time=6.877 Loss=1.044 Prec@1=73.242 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=23:11 IST
=> Validation 1.02% of 1x98...Epoch=19/150 LR=0.0965 Time=6.877 Loss=1.044 Prec@1=73.242 Prec@5=91.406 rate=2765.31 Hz, eta=0:00:00, total=0:00:00, wall=23:11 IST
** Validation 1.02% of 1x98...Epoch=19/150 LR=0.0965 Time=6.877 Loss=1.044 Prec@1=73.242 Prec@5=91.406 rate=2765.31 Hz, eta=0:00:00, total=0:00:00, wall=23:12 IST
** Validation 1.02% of 1x98...Epoch=19/150 LR=0.0965 Time=0.630 Loss=1.718 Prec@1=59.368 Prec@5=82.622 rate=2765.31 Hz, eta=0:00:00, total=0:00:00, wall=23:12 IST
** Validation 100.00% of 1x98...Epoch=19/150 LR=0.0965 Time=0.630 Loss=1.718 Prec@1=59.368 Prec@5=82.622 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=23:12 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:12 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:12 IST
=> Training   0.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=4.907 DataTime=4.606 Loss=1.591 Prec@1=63.281 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=23:12 IST
=> Training   0.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=4.907 DataTime=4.606 Loss=1.591 Prec@1=63.281 Prec@5=83.203 rate=1725.49 Hz, eta=0:00:01, total=0:00:00, wall=23:12 IST
=> Training   0.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=4.907 DataTime=4.606 Loss=1.591 Prec@1=63.281 Prec@5=83.203 rate=1725.49 Hz, eta=0:00:01, total=0:00:00, wall=23:13 IST
=> Training   0.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.592 DataTime=0.422 Loss=1.661 Prec@1=60.723 Prec@5=83.058 rate=1725.49 Hz, eta=0:00:01, total=0:00:00, wall=23:13 IST
=> Training   4.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.592 DataTime=0.422 Loss=1.661 Prec@1=60.723 Prec@5=83.058 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=23:13 IST
=> Training   4.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.592 DataTime=0.422 Loss=1.661 Prec@1=60.723 Prec@5=83.058 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=23:13 IST
=> Training   4.04% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.571 DataTime=0.403 Loss=1.643 Prec@1=61.077 Prec@5=83.376 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=23:13 IST
=> Training   8.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.571 DataTime=0.403 Loss=1.643 Prec@1=61.077 Prec@5=83.376 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=23:13 IST
=> Training   8.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.571 DataTime=0.403 Loss=1.643 Prec@1=61.077 Prec@5=83.376 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=23:14 IST
=> Training   8.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.574 DataTime=0.408 Loss=1.645 Prec@1=60.923 Prec@5=83.232 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=23:14 IST
=> Training   12.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.574 DataTime=0.408 Loss=1.645 Prec@1=60.923 Prec@5=83.232 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=23:14 IST
=> Training   12.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.574 DataTime=0.408 Loss=1.645 Prec@1=60.923 Prec@5=83.232 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=23:15 IST
=> Training   12.03% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.569 DataTime=0.403 Loss=1.646 Prec@1=60.927 Prec@5=83.235 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=23:15 IST
=> Training   16.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.569 DataTime=0.403 Loss=1.646 Prec@1=60.927 Prec@5=83.235 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=23:15 IST
=> Training   16.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.569 DataTime=0.403 Loss=1.646 Prec@1=60.927 Prec@5=83.235 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=23:16 IST
=> Training   16.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.396 Loss=1.646 Prec@1=60.902 Prec@5=83.220 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=23:16 IST
=> Training   20.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.396 Loss=1.646 Prec@1=60.902 Prec@5=83.220 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=23:16 IST
=> Training   20.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.396 Loss=1.646 Prec@1=60.902 Prec@5=83.220 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=23:17 IST
=> Training   20.02% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.395 Loss=1.652 Prec@1=60.766 Prec@5=83.125 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=23:17 IST
=> Training   24.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.395 Loss=1.652 Prec@1=60.766 Prec@5=83.125 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:17 IST
=> Training   24.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.395 Loss=1.652 Prec@1=60.766 Prec@5=83.125 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:18 IST
=> Training   24.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.393 Loss=1.655 Prec@1=60.684 Prec@5=83.089 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:18 IST
=> Training   28.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.393 Loss=1.655 Prec@1=60.684 Prec@5=83.089 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=23:18 IST
=> Training   28.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.393 Loss=1.655 Prec@1=60.684 Prec@5=83.089 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=23:19 IST
=> Training   28.01% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.393 Loss=1.658 Prec@1=60.634 Prec@5=83.062 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=23:19 IST
=> Training   32.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.393 Loss=1.658 Prec@1=60.634 Prec@5=83.062 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=23:19 IST
=> Training   32.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.393 Loss=1.658 Prec@1=60.634 Prec@5=83.062 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=23:20 IST
=> Training   32.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.392 Loss=1.660 Prec@1=60.605 Prec@5=82.993 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=23:20 IST
=> Training   36.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.392 Loss=1.660 Prec@1=60.605 Prec@5=82.993 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:20 IST
=> Training   36.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.561 DataTime=0.392 Loss=1.660 Prec@1=60.605 Prec@5=82.993 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:21 IST
=> Training   36.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.392 Loss=1.662 Prec@1=60.565 Prec@5=82.976 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:21 IST
=> Training   39.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.392 Loss=1.662 Prec@1=60.565 Prec@5=82.976 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=23:21 IST
=> Training   39.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.392 Loss=1.662 Prec@1=60.565 Prec@5=82.976 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=23:22 IST
=> Training   39.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.664 Prec@1=60.524 Prec@5=82.935 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=23:22 IST
=> Training   43.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.664 Prec@1=60.524 Prec@5=82.935 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=23:22 IST
=> Training   43.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.664 Prec@1=60.524 Prec@5=82.935 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=23:23 IST
=> Training   43.99% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.392 Loss=1.667 Prec@1=60.479 Prec@5=82.895 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=23:23 IST
=> Training   47.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.392 Loss=1.667 Prec@1=60.479 Prec@5=82.895 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=23:23 IST
=> Training   47.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.392 Loss=1.667 Prec@1=60.479 Prec@5=82.895 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=23:24 IST
=> Training   47.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.668 Prec@1=60.450 Prec@5=82.889 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=23:24 IST
=> Training   51.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.668 Prec@1=60.450 Prec@5=82.889 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=23:24 IST
=> Training   51.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.391 Loss=1.668 Prec@1=60.450 Prec@5=82.889 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=23:25 IST
=> Training   51.98% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.670 Prec@1=60.422 Prec@5=82.868 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=23:25 IST
=> Training   55.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.670 Prec@1=60.422 Prec@5=82.868 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:25 IST
=> Training   55.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.670 Prec@1=60.422 Prec@5=82.868 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:26 IST
=> Training   55.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.671 Prec@1=60.399 Prec@5=82.856 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:26 IST
=> Training   59.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.671 Prec@1=60.399 Prec@5=82.856 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:26 IST
=> Training   59.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.671 Prec@1=60.399 Prec@5=82.856 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:27 IST
=> Training   59.97% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.673 Prec@1=60.361 Prec@5=82.823 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:27 IST
=> Training   63.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.673 Prec@1=60.361 Prec@5=82.823 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:27 IST
=> Training   63.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.673 Prec@1=60.361 Prec@5=82.823 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:28 IST
=> Training   63.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.674 Prec@1=60.338 Prec@5=82.792 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:28 IST
=> Training   67.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.674 Prec@1=60.338 Prec@5=82.792 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=23:28 IST
=> Training   67.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.674 Prec@1=60.338 Prec@5=82.792 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=23:28 IST
=> Training   67.96% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.676 Prec@1=60.336 Prec@5=82.772 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=23:28 IST
=> Training   71.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.676 Prec@1=60.336 Prec@5=82.772 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:28 IST
=> Training   71.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.676 Prec@1=60.336 Prec@5=82.772 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:29 IST
=> Training   71.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.676 Prec@1=60.315 Prec@5=82.762 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:29 IST
=> Training   75.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.676 Prec@1=60.315 Prec@5=82.762 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=23:29 IST
=> Training   75.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.676 Prec@1=60.315 Prec@5=82.762 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=23:30 IST
=> Training   75.95% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.677 Prec@1=60.306 Prec@5=82.750 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=23:30 IST
=> Training   79.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.677 Prec@1=60.306 Prec@5=82.750 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=23:30 IST
=> Training   79.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.562 DataTime=0.390 Loss=1.677 Prec@1=60.306 Prec@5=82.750 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=23:31 IST
=> Training   79.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.678 Prec@1=60.287 Prec@5=82.727 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=23:31 IST
=> Training   83.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.678 Prec@1=60.287 Prec@5=82.727 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=23:31 IST
=> Training   83.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.678 Prec@1=60.287 Prec@5=82.727 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=23:32 IST
=> Training   83.94% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.679 Prec@1=60.281 Prec@5=82.717 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=23:32 IST
=> Training   87.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.679 Prec@1=60.281 Prec@5=82.717 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:32 IST
=> Training   87.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.390 Loss=1.679 Prec@1=60.281 Prec@5=82.717 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:33 IST
=> Training   87.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.679 Prec@1=60.270 Prec@5=82.708 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:33 IST
=> Training   91.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.679 Prec@1=60.270 Prec@5=82.708 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=23:33 IST
=> Training   91.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.679 Prec@1=60.270 Prec@5=82.708 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=23:34 IST
=> Training   91.93% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.256 Prec@5=82.699 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=23:34 IST
=> Training   95.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.256 Prec@5=82.699 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=23:34 IST
=> Training   95.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.256 Prec@5=82.699 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=23:35 IST
=> Training   95.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.251 Prec@5=82.684 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=23:35 IST
=> Training   99.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.251 Prec@5=82.684 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=23:35 IST
=> Training   99.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.251 Prec@5=82.684 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=23:35 IST
=> Training   99.92% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.252 Prec@5=82.684 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=23:35 IST
=> Training   100.00% of 1x2503...Epoch=20/150 LR=0.0961 Time=0.563 DataTime=0.391 Loss=1.680 Prec@1=60.252 Prec@5=82.684 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=23:35 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:35 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:35 IST
=> Validation 0.00% of 1x98...Epoch=20/150 LR=0.0961 Time=7.447 Loss=1.198 Prec@1=67.383 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=23:35 IST
=> Validation 1.02% of 1x98...Epoch=20/150 LR=0.0961 Time=7.447 Loss=1.198 Prec@1=67.383 Prec@5=90.430 rate=7342.09 Hz, eta=0:00:00, total=0:00:00, wall=23:35 IST
** Validation 1.02% of 1x98...Epoch=20/150 LR=0.0961 Time=7.447 Loss=1.198 Prec@1=67.383 Prec@5=90.430 rate=7342.09 Hz, eta=0:00:00, total=0:00:00, wall=23:36 IST
** Validation 1.02% of 1x98...Epoch=20/150 LR=0.0961 Time=0.639 Loss=1.752 Prec@1=58.642 Prec@5=82.132 rate=7342.09 Hz, eta=0:00:00, total=0:00:00, wall=23:36 IST
** Validation 100.00% of 1x98...Epoch=20/150 LR=0.0961 Time=0.639 Loss=1.752 Prec@1=58.642 Prec@5=82.132 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=23:36 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:36 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:36 IST
=> Training   0.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=4.851 DataTime=4.355 Loss=1.660 Prec@1=60.352 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=23:36 IST
=> Training   0.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=4.851 DataTime=4.355 Loss=1.660 Prec@1=60.352 Prec@5=84.766 rate=6158.05 Hz, eta=0:00:00, total=0:00:00, wall=23:36 IST
=> Training   0.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=4.851 DataTime=4.355 Loss=1.660 Prec@1=60.352 Prec@5=84.766 rate=6158.05 Hz, eta=0:00:00, total=0:00:00, wall=23:37 IST
=> Training   0.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.600 DataTime=0.433 Loss=1.620 Prec@1=61.386 Prec@5=83.621 rate=6158.05 Hz, eta=0:00:00, total=0:00:00, wall=23:37 IST
=> Training   4.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.600 DataTime=0.433 Loss=1.620 Prec@1=61.386 Prec@5=83.621 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=23:37 IST
=> Training   4.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.600 DataTime=0.433 Loss=1.620 Prec@1=61.386 Prec@5=83.621 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=23:38 IST
=> Training   4.04% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.580 DataTime=0.405 Loss=1.631 Prec@1=61.204 Prec@5=83.425 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=23:38 IST
=> Training   8.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.580 DataTime=0.405 Loss=1.631 Prec@1=61.204 Prec@5=83.425 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=23:38 IST
=> Training   8.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.580 DataTime=0.405 Loss=1.631 Prec@1=61.204 Prec@5=83.425 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=23:39 IST
=> Training   8.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.581 DataTime=0.405 Loss=1.637 Prec@1=61.085 Prec@5=83.335 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=23:39 IST
=> Training   12.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.581 DataTime=0.405 Loss=1.637 Prec@1=61.085 Prec@5=83.335 rate=1.77 Hz, eta=0:20:42, total=0:02:49, wall=23:39 IST
=> Training   12.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.581 DataTime=0.405 Loss=1.637 Prec@1=61.085 Prec@5=83.335 rate=1.77 Hz, eta=0:20:42, total=0:02:49, wall=23:40 IST
=> Training   12.03% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.576 DataTime=0.399 Loss=1.640 Prec@1=61.067 Prec@5=83.236 rate=1.77 Hz, eta=0:20:42, total=0:02:49, wall=23:40 IST
=> Training   16.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.576 DataTime=0.399 Loss=1.640 Prec@1=61.067 Prec@5=83.236 rate=1.77 Hz, eta=0:19:45, total=0:03:46, wall=23:40 IST
=> Training   16.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.576 DataTime=0.399 Loss=1.640 Prec@1=61.067 Prec@5=83.236 rate=1.77 Hz, eta=0:19:45, total=0:03:46, wall=23:41 IST
=> Training   16.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.573 DataTime=0.396 Loss=1.643 Prec@1=61.044 Prec@5=83.178 rate=1.77 Hz, eta=0:19:45, total=0:03:46, wall=23:41 IST
=> Training   20.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.573 DataTime=0.396 Loss=1.643 Prec@1=61.044 Prec@5=83.178 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=23:41 IST
=> Training   20.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.573 DataTime=0.396 Loss=1.643 Prec@1=61.044 Prec@5=83.178 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=23:42 IST
=> Training   20.02% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.394 Loss=1.642 Prec@1=61.066 Prec@5=83.199 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=23:42 IST
=> Training   24.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.394 Loss=1.642 Prec@1=61.066 Prec@5=83.199 rate=1.79 Hz, eta=0:17:45, total=0:05:36, wall=23:42 IST
=> Training   24.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.394 Loss=1.642 Prec@1=61.066 Prec@5=83.199 rate=1.79 Hz, eta=0:17:45, total=0:05:36, wall=23:43 IST
=> Training   24.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.570 DataTime=0.396 Loss=1.643 Prec@1=61.067 Prec@5=83.182 rate=1.79 Hz, eta=0:17:45, total=0:05:36, wall=23:43 IST
=> Training   28.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.570 DataTime=0.396 Loss=1.643 Prec@1=61.067 Prec@5=83.182 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=23:43 IST
=> Training   28.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.570 DataTime=0.396 Loss=1.643 Prec@1=61.067 Prec@5=83.182 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=23:44 IST
=> Training   28.01% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.645 Prec@1=61.047 Prec@5=83.200 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=23:44 IST
=> Training   32.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.645 Prec@1=61.047 Prec@5=83.200 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:44 IST
=> Training   32.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.645 Prec@1=61.047 Prec@5=83.200 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:45 IST
=> Training   32.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.398 Loss=1.646 Prec@1=60.987 Prec@5=83.210 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:45 IST
=> Training   36.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.398 Loss=1.646 Prec@1=60.987 Prec@5=83.210 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=23:45 IST
=> Training   36.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.398 Loss=1.646 Prec@1=60.987 Prec@5=83.210 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=23:46 IST
=> Training   36.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.397 Loss=1.647 Prec@1=60.948 Prec@5=83.194 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=23:46 IST
=> Training   39.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.397 Loss=1.647 Prec@1=60.948 Prec@5=83.194 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=23:46 IST
=> Training   39.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.397 Loss=1.647 Prec@1=60.948 Prec@5=83.194 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=23:47 IST
=> Training   39.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.649 Prec@1=60.909 Prec@5=83.175 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=23:47 IST
=> Training   43.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.649 Prec@1=60.909 Prec@5=83.175 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=23:47 IST
=> Training   43.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.569 DataTime=0.396 Loss=1.649 Prec@1=60.909 Prec@5=83.175 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=23:47 IST
=> Training   43.99% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.649 Prec@1=60.890 Prec@5=83.164 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=23:47 IST
=> Training   47.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.649 Prec@1=60.890 Prec@5=83.164 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=23:47 IST
=> Training   47.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.649 Prec@1=60.890 Prec@5=83.164 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=23:48 IST
=> Training   47.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.395 Loss=1.651 Prec@1=60.859 Prec@5=83.133 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=23:48 IST
=> Training   51.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.395 Loss=1.651 Prec@1=60.859 Prec@5=83.133 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=23:48 IST
=> Training   51.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.395 Loss=1.651 Prec@1=60.859 Prec@5=83.133 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=23:49 IST
=> Training   51.98% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.653 Prec@1=60.823 Prec@5=83.102 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=23:49 IST
=> Training   55.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.653 Prec@1=60.823 Prec@5=83.102 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=23:49 IST
=> Training   55.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.568 DataTime=0.396 Loss=1.653 Prec@1=60.823 Prec@5=83.102 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=23:50 IST
=> Training   55.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.396 Loss=1.654 Prec@1=60.788 Prec@5=83.078 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=23:50 IST
=> Training   59.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.396 Loss=1.654 Prec@1=60.788 Prec@5=83.078 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=23:50 IST
=> Training   59.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.396 Loss=1.654 Prec@1=60.788 Prec@5=83.078 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=23:51 IST
=> Training   59.97% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.395 Loss=1.655 Prec@1=60.774 Prec@5=83.054 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=23:51 IST
=> Training   63.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.395 Loss=1.655 Prec@1=60.774 Prec@5=83.054 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=23:51 IST
=> Training   63.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.567 DataTime=0.395 Loss=1.655 Prec@1=60.774 Prec@5=83.054 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=23:52 IST
=> Training   63.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.658 Prec@1=60.739 Prec@5=83.023 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=23:52 IST
=> Training   67.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.658 Prec@1=60.739 Prec@5=83.023 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=23:52 IST
=> Training   67.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.658 Prec@1=60.739 Prec@5=83.023 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=23:53 IST
=> Training   67.96% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.659 Prec@1=60.725 Prec@5=83.008 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=23:53 IST
=> Training   71.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.659 Prec@1=60.725 Prec@5=83.008 rate=1.78 Hz, eta=0:06:35, total=0:16:54, wall=23:53 IST
=> Training   71.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.659 Prec@1=60.725 Prec@5=83.008 rate=1.78 Hz, eta=0:06:35, total=0:16:54, wall=23:54 IST
=> Training   71.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.660 Prec@1=60.698 Prec@5=82.994 rate=1.78 Hz, eta=0:06:35, total=0:16:54, wall=23:54 IST
=> Training   75.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.660 Prec@1=60.698 Prec@5=82.994 rate=1.78 Hz, eta=0:05:39, total=0:17:50, wall=23:54 IST
=> Training   75.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.394 Loss=1.660 Prec@1=60.698 Prec@5=82.994 rate=1.78 Hz, eta=0:05:39, total=0:17:50, wall=23:55 IST
=> Training   75.95% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.660 Prec@1=60.692 Prec@5=82.982 rate=1.78 Hz, eta=0:05:39, total=0:17:50, wall=23:55 IST
=> Training   79.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.660 Prec@1=60.692 Prec@5=82.982 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=23:55 IST
=> Training   79.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.660 Prec@1=60.692 Prec@5=82.982 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=23:56 IST
=> Training   79.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.661 Prec@1=60.682 Prec@5=82.972 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=23:56 IST
=> Training   83.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.661 Prec@1=60.682 Prec@5=82.972 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=23:56 IST
=> Training   83.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.661 Prec@1=60.682 Prec@5=82.972 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=23:57 IST
=> Training   83.94% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.662 Prec@1=60.662 Prec@5=82.947 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=23:57 IST
=> Training   87.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.662 Prec@1=60.662 Prec@5=82.947 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=23:57 IST
=> Training   87.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.662 Prec@1=60.662 Prec@5=82.947 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=23:58 IST
=> Training   87.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.663 Prec@1=60.640 Prec@5=82.938 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=23:58 IST
=> Training   91.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.663 Prec@1=60.640 Prec@5=82.938 rate=1.77 Hz, eta=0:01:53, total=0:21:36, wall=23:58 IST
=> Training   91.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.566 DataTime=0.395 Loss=1.663 Prec@1=60.640 Prec@5=82.938 rate=1.77 Hz, eta=0:01:53, total=0:21:36, wall=23:59 IST
=> Training   91.93% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.663 Prec@1=60.630 Prec@5=82.927 rate=1.77 Hz, eta=0:01:53, total=0:21:36, wall=23:59 IST
=> Training   95.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.663 Prec@1=60.630 Prec@5=82.927 rate=1.77 Hz, eta=0:00:57, total=0:22:32, wall=23:59 IST
=> Training   95.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.663 Prec@1=60.630 Prec@5=82.927 rate=1.77 Hz, eta=0:00:57, total=0:22:32, wall=00:00 IST
=> Training   95.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.664 Prec@1=60.622 Prec@5=82.908 rate=1.77 Hz, eta=0:00:57, total=0:22:32, wall=00:00 IST
=> Training   99.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.664 Prec@1=60.622 Prec@5=82.908 rate=1.77 Hz, eta=0:00:01, total=0:23:29, wall=00:00 IST
=> Training   99.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.395 Loss=1.664 Prec@1=60.622 Prec@5=82.908 rate=1.77 Hz, eta=0:00:01, total=0:23:29, wall=00:00 IST
=> Training   99.92% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.394 Loss=1.664 Prec@1=60.621 Prec@5=82.908 rate=1.77 Hz, eta=0:00:01, total=0:23:29, wall=00:00 IST
=> Training   100.00% of 1x2503...Epoch=21/150 LR=0.0957 Time=0.565 DataTime=0.394 Loss=1.664 Prec@1=60.621 Prec@5=82.908 rate=1.78 Hz, eta=0:00:00, total=0:23:29, wall=00:00 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST
=> Validation 0.00% of 1x98...Epoch=21/150 LR=0.0957 Time=6.836 Loss=1.163 Prec@1=70.508 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=00:00 IST
=> Validation 1.02% of 1x98...Epoch=21/150 LR=0.0957 Time=6.836 Loss=1.163 Prec@1=70.508 Prec@5=90.820 rate=6347.23 Hz, eta=0:00:00, total=0:00:00, wall=00:00 IST
** Validation 1.02% of 1x98...Epoch=21/150 LR=0.0957 Time=6.836 Loss=1.163 Prec@1=70.508 Prec@5=90.820 rate=6347.23 Hz, eta=0:00:00, total=0:00:00, wall=00:01 IST
** Validation 1.02% of 1x98...Epoch=21/150 LR=0.0957 Time=0.637 Loss=1.716 Prec@1=59.300 Prec@5=82.590 rate=6347.23 Hz, eta=0:00:00, total=0:00:00, wall=00:01 IST
** Validation 100.00% of 1x98...Epoch=21/150 LR=0.0957 Time=0.637 Loss=1.716 Prec@1=59.300 Prec@5=82.590 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=00:01 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:01 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:01 IST
=> Training   0.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=5.148 DataTime=4.887 Loss=1.704 Prec@1=58.594 Prec@5=81.641 rate=0 Hz, eta=?, total=0:00:00, wall=00:01 IST
=> Training   0.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=5.148 DataTime=4.887 Loss=1.704 Prec@1=58.594 Prec@5=81.641 rate=5810.84 Hz, eta=0:00:00, total=0:00:00, wall=00:01 IST
=> Training   0.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=5.148 DataTime=4.887 Loss=1.704 Prec@1=58.594 Prec@5=81.641 rate=5810.84 Hz, eta=0:00:00, total=0:00:00, wall=00:02 IST
=> Training   0.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.599 DataTime=0.430 Loss=1.609 Prec@1=61.707 Prec@5=83.716 rate=5810.84 Hz, eta=0:00:00, total=0:00:00, wall=00:02 IST
=> Training   4.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.599 DataTime=0.430 Loss=1.609 Prec@1=61.707 Prec@5=83.716 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=00:02 IST
=> Training   4.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.599 DataTime=0.430 Loss=1.609 Prec@1=61.707 Prec@5=83.716 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=00:03 IST
=> Training   4.04% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.575 DataTime=0.407 Loss=1.610 Prec@1=61.697 Prec@5=83.684 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=00:03 IST
=> Training   8.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.575 DataTime=0.407 Loss=1.610 Prec@1=61.697 Prec@5=83.684 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=00:03 IST
=> Training   8.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.575 DataTime=0.407 Loss=1.610 Prec@1=61.697 Prec@5=83.684 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=00:04 IST
=> Training   8.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.619 Prec@1=61.529 Prec@5=83.533 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=00:04 IST
=> Training   12.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.619 Prec@1=61.529 Prec@5=83.533 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=00:04 IST
=> Training   12.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.619 Prec@1=61.529 Prec@5=83.533 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=00:05 IST
=> Training   12.03% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.622 Prec@1=61.504 Prec@5=83.525 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=00:05 IST
=> Training   16.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.622 Prec@1=61.504 Prec@5=83.525 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=00:05 IST
=> Training   16.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.572 DataTime=0.404 Loss=1.622 Prec@1=61.504 Prec@5=83.525 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=00:06 IST
=> Training   16.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.574 DataTime=0.407 Loss=1.623 Prec@1=61.447 Prec@5=83.529 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=00:06 IST
=> Training   20.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.574 DataTime=0.407 Loss=1.623 Prec@1=61.447 Prec@5=83.529 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=00:06 IST
=> Training   20.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.574 DataTime=0.407 Loss=1.623 Prec@1=61.447 Prec@5=83.529 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=00:06 IST
=> Training   20.02% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.570 DataTime=0.403 Loss=1.623 Prec@1=61.494 Prec@5=83.514 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=00:06 IST
=> Training   24.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.570 DataTime=0.403 Loss=1.623 Prec@1=61.494 Prec@5=83.514 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=00:06 IST
=> Training   24.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.570 DataTime=0.403 Loss=1.623 Prec@1=61.494 Prec@5=83.514 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=00:07 IST
=> Training   24.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.569 DataTime=0.400 Loss=1.623 Prec@1=61.481 Prec@5=83.512 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=00:07 IST
=> Training   28.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.569 DataTime=0.400 Loss=1.623 Prec@1=61.481 Prec@5=83.512 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=00:07 IST
=> Training   28.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.569 DataTime=0.400 Loss=1.623 Prec@1=61.481 Prec@5=83.512 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=00:08 IST
=> Training   28.01% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.400 Loss=1.624 Prec@1=61.447 Prec@5=83.500 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=00:08 IST
=> Training   32.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.400 Loss=1.624 Prec@1=61.447 Prec@5=83.500 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=00:08 IST
=> Training   32.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.400 Loss=1.624 Prec@1=61.447 Prec@5=83.500 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=00:09 IST
=> Training   32.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.398 Loss=1.627 Prec@1=61.388 Prec@5=83.457 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=00:09 IST
=> Training   36.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.398 Loss=1.627 Prec@1=61.388 Prec@5=83.457 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=00:09 IST
=> Training   36.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.398 Loss=1.627 Prec@1=61.388 Prec@5=83.457 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=00:10 IST
=> Training   36.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.397 Loss=1.629 Prec@1=61.351 Prec@5=83.425 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=00:10 IST
=> Training   39.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.397 Loss=1.629 Prec@1=61.351 Prec@5=83.425 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=00:10 IST
=> Training   39.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.568 DataTime=0.397 Loss=1.629 Prec@1=61.351 Prec@5=83.425 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=00:11 IST
=> Training   39.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.567 DataTime=0.395 Loss=1.632 Prec@1=61.256 Prec@5=83.370 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=00:11 IST
=> Training   43.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.567 DataTime=0.395 Loss=1.632 Prec@1=61.256 Prec@5=83.370 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:11 IST
=> Training   43.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.567 DataTime=0.395 Loss=1.632 Prec@1=61.256 Prec@5=83.370 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:12 IST
=> Training   43.99% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.634 Prec@1=61.213 Prec@5=83.351 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:12 IST
=> Training   47.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.634 Prec@1=61.213 Prec@5=83.351 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:12 IST
=> Training   47.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.634 Prec@1=61.213 Prec@5=83.351 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:13 IST
=> Training   47.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.393 Loss=1.636 Prec@1=61.169 Prec@5=83.314 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:13 IST
=> Training   51.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.393 Loss=1.636 Prec@1=61.169 Prec@5=83.314 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=00:13 IST
=> Training   51.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.393 Loss=1.636 Prec@1=61.169 Prec@5=83.314 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=00:14 IST
=> Training   51.98% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.638 Prec@1=61.118 Prec@5=83.291 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=00:14 IST
=> Training   55.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.638 Prec@1=61.118 Prec@5=83.291 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=00:14 IST
=> Training   55.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.638 Prec@1=61.118 Prec@5=83.291 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=00:15 IST
=> Training   55.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.639 Prec@1=61.085 Prec@5=83.274 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=00:15 IST
=> Training   59.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.639 Prec@1=61.085 Prec@5=83.274 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=00:15 IST
=> Training   59.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.639 Prec@1=61.085 Prec@5=83.274 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=00:16 IST
=> Training   59.97% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.395 Loss=1.641 Prec@1=61.048 Prec@5=83.266 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=00:16 IST
=> Training   63.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.395 Loss=1.641 Prec@1=61.048 Prec@5=83.266 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=00:16 IST
=> Training   63.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.395 Loss=1.641 Prec@1=61.048 Prec@5=83.266 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=00:17 IST
=> Training   63.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.029 Prec@5=83.253 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=00:17 IST
=> Training   67.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.029 Prec@5=83.253 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=00:17 IST
=> Training   67.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.029 Prec@5=83.253 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=00:18 IST
=> Training   67.96% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.018 Prec@5=83.240 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=00:18 IST
=> Training   71.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.018 Prec@5=83.240 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=00:18 IST
=> Training   71.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.566 DataTime=0.394 Loss=1.642 Prec@1=61.018 Prec@5=83.240 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=00:19 IST
=> Training   71.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.394 Loss=1.644 Prec@1=60.983 Prec@5=83.211 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=00:19 IST
=> Training   75.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.394 Loss=1.644 Prec@1=60.983 Prec@5=83.211 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:19 IST
=> Training   75.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.394 Loss=1.644 Prec@1=60.983 Prec@5=83.211 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:20 IST
=> Training   75.95% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.393 Loss=1.645 Prec@1=60.966 Prec@5=83.212 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:20 IST
=> Training   79.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.393 Loss=1.645 Prec@1=60.966 Prec@5=83.212 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=00:20 IST
=> Training   79.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.565 DataTime=0.393 Loss=1.645 Prec@1=60.966 Prec@5=83.212 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=00:20 IST
=> Training   79.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.646 Prec@1=60.934 Prec@5=83.191 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=00:20 IST
=> Training   83.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.646 Prec@1=60.934 Prec@5=83.191 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=00:20 IST
=> Training   83.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.646 Prec@1=60.934 Prec@5=83.191 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=00:21 IST
=> Training   83.94% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.648 Prec@1=60.903 Prec@5=83.162 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=00:21 IST
=> Training   87.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.648 Prec@1=60.903 Prec@5=83.162 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=00:21 IST
=> Training   87.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.648 Prec@1=60.903 Prec@5=83.162 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=00:22 IST
=> Training   87.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.649 Prec@1=60.876 Prec@5=83.144 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=00:22 IST
=> Training   91.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.649 Prec@1=60.876 Prec@5=83.144 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=00:22 IST
=> Training   91.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.649 Prec@1=60.876 Prec@5=83.144 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=00:23 IST
=> Training   91.93% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.650 Prec@1=60.869 Prec@5=83.137 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=00:23 IST
=> Training   95.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.650 Prec@1=60.869 Prec@5=83.137 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=00:23 IST
=> Training   95.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.564 DataTime=0.392 Loss=1.650 Prec@1=60.869 Prec@5=83.137 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=00:24 IST
=> Training   95.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.563 DataTime=0.391 Loss=1.650 Prec@1=60.852 Prec@5=83.125 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=00:24 IST
=> Training   99.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.563 DataTime=0.391 Loss=1.650 Prec@1=60.852 Prec@5=83.125 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=00:24 IST
=> Training   99.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.563 DataTime=0.391 Loss=1.650 Prec@1=60.852 Prec@5=83.125 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=00:24 IST
=> Training   99.92% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.563 DataTime=0.391 Loss=1.650 Prec@1=60.851 Prec@5=83.126 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=00:24 IST
=> Training   100.00% of 1x2503...Epoch=22/150 LR=0.0952 Time=0.563 DataTime=0.391 Loss=1.650 Prec@1=60.851 Prec@5=83.126 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=00:24 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> Validation 0.00% of 1x98...Epoch=22/150 LR=0.0952 Time=7.016 Loss=1.122 Prec@1=68.750 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=00:24 IST
=> Validation 1.02% of 1x98...Epoch=22/150 LR=0.0952 Time=7.016 Loss=1.122 Prec@1=68.750 Prec@5=90.820 rate=6390.92 Hz, eta=0:00:00, total=0:00:00, wall=00:24 IST
** Validation 1.02% of 1x98...Epoch=22/150 LR=0.0952 Time=7.016 Loss=1.122 Prec@1=68.750 Prec@5=90.820 rate=6390.92 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
** Validation 1.02% of 1x98...Epoch=22/150 LR=0.0952 Time=0.641 Loss=1.700 Prec@1=59.790 Prec@5=82.982 rate=6390.92 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
** Validation 100.00% of 1x98...Epoch=22/150 LR=0.0952 Time=0.641 Loss=1.700 Prec@1=59.790 Prec@5=82.982 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=00:25 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> Training   0.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=5.131 DataTime=4.825 Loss=1.653 Prec@1=61.133 Prec@5=83.008 rate=0 Hz, eta=?, total=0:00:00, wall=00:25 IST
=> Training   0.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=5.131 DataTime=4.825 Loss=1.653 Prec@1=61.133 Prec@5=83.008 rate=4398.25 Hz, eta=0:00:00, total=0:00:00, wall=00:25 IST
=> Training   0.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=5.131 DataTime=4.825 Loss=1.653 Prec@1=61.133 Prec@5=83.008 rate=4398.25 Hz, eta=0:00:00, total=0:00:00, wall=00:26 IST
=> Training   0.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.594 DataTime=0.426 Loss=1.608 Prec@1=61.535 Prec@5=83.745 rate=4398.25 Hz, eta=0:00:00, total=0:00:00, wall=00:26 IST
=> Training   4.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.594 DataTime=0.426 Loss=1.608 Prec@1=61.535 Prec@5=83.745 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=00:26 IST
=> Training   4.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.594 DataTime=0.426 Loss=1.608 Prec@1=61.535 Prec@5=83.745 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=00:27 IST
=> Training   4.04% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.573 DataTime=0.404 Loss=1.604 Prec@1=61.665 Prec@5=83.889 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=00:27 IST
=> Training   8.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.573 DataTime=0.404 Loss=1.604 Prec@1=61.665 Prec@5=83.889 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=00:27 IST
=> Training   8.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.573 DataTime=0.404 Loss=1.604 Prec@1=61.665 Prec@5=83.889 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=00:28 IST
=> Training   8.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.569 DataTime=0.400 Loss=1.609 Prec@1=61.663 Prec@5=83.777 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=00:28 IST
=> Training   12.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.569 DataTime=0.400 Loss=1.609 Prec@1=61.663 Prec@5=83.777 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=00:28 IST
=> Training   12.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.569 DataTime=0.400 Loss=1.609 Prec@1=61.663 Prec@5=83.777 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=00:29 IST
=> Training   12.03% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.568 DataTime=0.397 Loss=1.610 Prec@1=61.612 Prec@5=83.764 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=00:29 IST
=> Training   16.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.568 DataTime=0.397 Loss=1.610 Prec@1=61.612 Prec@5=83.764 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:29 IST
=> Training   16.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.568 DataTime=0.397 Loss=1.610 Prec@1=61.612 Prec@5=83.764 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:30 IST
=> Training   16.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.550 Prec@5=83.709 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:30 IST
=> Training   20.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.550 Prec@5=83.709 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=00:30 IST
=> Training   20.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.550 Prec@5=83.709 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=00:31 IST
=> Training   20.02% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.613 Prec@1=61.577 Prec@5=83.685 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=00:31 IST
=> Training   24.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.613 Prec@1=61.577 Prec@5=83.685 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:31 IST
=> Training   24.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.613 Prec@1=61.577 Prec@5=83.685 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:32 IST
=> Training   24.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.554 Prec@5=83.683 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:32 IST
=> Training   28.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.554 Prec@5=83.683 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=00:32 IST
=> Training   28.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.567 DataTime=0.396 Loss=1.614 Prec@1=61.554 Prec@5=83.683 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=00:33 IST
=> Training   28.01% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.617 Prec@1=61.502 Prec@5=83.643 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=00:33 IST
=> Training   32.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.617 Prec@1=61.502 Prec@5=83.643 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:33 IST
=> Training   32.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.565 DataTime=0.394 Loss=1.617 Prec@1=61.502 Prec@5=83.643 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:34 IST
=> Training   32.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.564 DataTime=0.392 Loss=1.620 Prec@1=61.482 Prec@5=83.611 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:34 IST
=> Training   36.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.564 DataTime=0.392 Loss=1.620 Prec@1=61.482 Prec@5=83.611 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=00:34 IST
=> Training   36.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.564 DataTime=0.392 Loss=1.620 Prec@1=61.482 Prec@5=83.611 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=00:35 IST
=> Training   36.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.388 Loss=1.622 Prec@1=61.425 Prec@5=83.583 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=00:35 IST
=> Training   39.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.388 Loss=1.622 Prec@1=61.425 Prec@5=83.583 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=00:35 IST
=> Training   39.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.388 Loss=1.622 Prec@1=61.425 Prec@5=83.583 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=00:36 IST
=> Training   39.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.410 Prec@5=83.585 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=00:36 IST
=> Training   43.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.410 Prec@5=83.585 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:36 IST
=> Training   43.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.410 Prec@5=83.585 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:37 IST
=> Training   43.99% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.406 Prec@5=83.583 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:37 IST
=> Training   47.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.406 Prec@5=83.583 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=00:37 IST
=> Training   47.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.563 DataTime=0.390 Loss=1.622 Prec@1=61.406 Prec@5=83.583 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=00:37 IST
=> Training   47.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.623 Prec@1=61.391 Prec@5=83.564 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=00:37 IST
=> Training   51.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.623 Prec@1=61.391 Prec@5=83.564 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=00:37 IST
=> Training   51.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.623 Prec@1=61.391 Prec@5=83.564 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=00:38 IST
=> Training   51.98% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.624 Prec@1=61.377 Prec@5=83.544 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=00:38 IST
=> Training   55.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.624 Prec@1=61.377 Prec@5=83.544 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:38 IST
=> Training   55.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.562 DataTime=0.389 Loss=1.624 Prec@1=61.377 Prec@5=83.544 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:39 IST
=> Training   55.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.355 Prec@5=83.511 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:39 IST
=> Training   59.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.355 Prec@5=83.511 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=00:39 IST
=> Training   59.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.355 Prec@5=83.511 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=00:40 IST
=> Training   59.97% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.349 Prec@5=83.510 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=00:40 IST
=> Training   63.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.349 Prec@5=83.510 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:40 IST
=> Training   63.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.625 Prec@1=61.349 Prec@5=83.510 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:41 IST
=> Training   63.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.627 Prec@1=61.312 Prec@5=83.488 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:41 IST
=> Training   67.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.627 Prec@1=61.312 Prec@5=83.488 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:41 IST
=> Training   67.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.627 Prec@1=61.312 Prec@5=83.488 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:42 IST
=> Training   67.96% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.628 Prec@1=61.293 Prec@5=83.467 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:42 IST
=> Training   71.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.628 Prec@1=61.293 Prec@5=83.467 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=00:42 IST
=> Training   71.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.628 Prec@1=61.293 Prec@5=83.467 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=00:43 IST
=> Training   71.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.629 Prec@1=61.259 Prec@5=83.459 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=00:43 IST
=> Training   75.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.629 Prec@1=61.259 Prec@5=83.459 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:43 IST
=> Training   75.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.629 Prec@1=61.259 Prec@5=83.459 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:44 IST
=> Training   75.95% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.630 Prec@1=61.243 Prec@5=83.445 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:44 IST
=> Training   79.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.630 Prec@1=61.243 Prec@5=83.445 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:44 IST
=> Training   79.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.630 Prec@1=61.243 Prec@5=83.445 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:45 IST
=> Training   79.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.631 Prec@1=61.231 Prec@5=83.429 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:45 IST
=> Training   83.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.631 Prec@1=61.231 Prec@5=83.429 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:45 IST
=> Training   83.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.389 Loss=1.631 Prec@1=61.231 Prec@5=83.429 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:46 IST
=> Training   83.94% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.631 Prec@1=61.230 Prec@5=83.419 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:46 IST
=> Training   87.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.631 Prec@1=61.230 Prec@5=83.419 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=00:46 IST
=> Training   87.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.631 Prec@1=61.230 Prec@5=83.419 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=00:47 IST
=> Training   87.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.632 Prec@1=61.224 Prec@5=83.403 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=00:47 IST
=> Training   91.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.632 Prec@1=61.224 Prec@5=83.403 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:47 IST
=> Training   91.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.632 Prec@1=61.224 Prec@5=83.403 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:48 IST
=> Training   91.93% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.634 Prec@1=61.198 Prec@5=83.377 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:48 IST
=> Training   95.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.634 Prec@1=61.198 Prec@5=83.377 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=00:48 IST
=> Training   95.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.561 DataTime=0.389 Loss=1.634 Prec@1=61.198 Prec@5=83.377 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=00:49 IST
=> Training   95.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.388 Loss=1.635 Prec@1=61.171 Prec@5=83.357 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=00:49 IST
=> Training   99.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.388 Loss=1.635 Prec@1=61.171 Prec@5=83.357 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=00:49 IST
=> Training   99.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.388 Loss=1.635 Prec@1=61.171 Prec@5=83.357 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=00:49 IST
=> Training   99.92% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.388 Loss=1.635 Prec@1=61.170 Prec@5=83.357 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=00:49 IST
=> Training   100.00% of 1x2503...Epoch=23/150 LR=0.0948 Time=0.560 DataTime=0.388 Loss=1.635 Prec@1=61.170 Prec@5=83.357 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=00:49 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:49 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:49 IST
=> Validation 0.00% of 1x98...Epoch=23/150 LR=0.0948 Time=7.056 Loss=1.016 Prec@1=74.805 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=00:49 IST
=> Validation 1.02% of 1x98...Epoch=23/150 LR=0.0948 Time=7.056 Loss=1.016 Prec@1=74.805 Prec@5=92.578 rate=5068.09 Hz, eta=0:00:00, total=0:00:00, wall=00:49 IST
** Validation 1.02% of 1x98...Epoch=23/150 LR=0.0948 Time=7.056 Loss=1.016 Prec@1=74.805 Prec@5=92.578 rate=5068.09 Hz, eta=0:00:00, total=0:00:00, wall=00:50 IST
** Validation 1.02% of 1x98...Epoch=23/150 LR=0.0948 Time=0.632 Loss=1.657 Prec@1=60.746 Prec@5=83.606 rate=5068.09 Hz, eta=0:00:00, total=0:00:00, wall=00:50 IST
** Validation 100.00% of 1x98...Epoch=23/150 LR=0.0948 Time=0.632 Loss=1.657 Prec@1=60.746 Prec@5=83.606 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=00:50 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:50 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:50 IST
=> Training   0.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=5.203 DataTime=4.831 Loss=1.520 Prec@1=64.062 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=00:50 IST
=> Training   0.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=5.203 DataTime=4.831 Loss=1.520 Prec@1=64.062 Prec@5=84.766 rate=5651.44 Hz, eta=0:00:00, total=0:00:00, wall=00:50 IST
=> Training   0.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=5.203 DataTime=4.831 Loss=1.520 Prec@1=64.062 Prec@5=84.766 rate=5651.44 Hz, eta=0:00:00, total=0:00:00, wall=00:51 IST
=> Training   0.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.592 DataTime=0.428 Loss=1.594 Prec@1=61.924 Prec@5=83.791 rate=5651.44 Hz, eta=0:00:00, total=0:00:00, wall=00:51 IST
=> Training   4.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.592 DataTime=0.428 Loss=1.594 Prec@1=61.924 Prec@5=83.791 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=00:51 IST
=> Training   4.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.592 DataTime=0.428 Loss=1.594 Prec@1=61.924 Prec@5=83.791 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=00:52 IST
=> Training   4.04% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.571 DataTime=0.400 Loss=1.590 Prec@1=62.016 Prec@5=83.914 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=00:52 IST
=> Training   8.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.571 DataTime=0.400 Loss=1.590 Prec@1=62.016 Prec@5=83.914 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=00:52 IST
=> Training   8.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.571 DataTime=0.400 Loss=1.590 Prec@1=62.016 Prec@5=83.914 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=00:53 IST
=> Training   8.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.566 DataTime=0.394 Loss=1.594 Prec@1=61.917 Prec@5=83.943 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=00:53 IST
=> Training   12.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.566 DataTime=0.394 Loss=1.594 Prec@1=61.917 Prec@5=83.943 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=00:53 IST
=> Training   12.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.566 DataTime=0.394 Loss=1.594 Prec@1=61.917 Prec@5=83.943 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=00:53 IST
=> Training   12.03% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.597 Prec@1=61.906 Prec@5=83.889 rate=1.82 Hz, eta=0:20:07, total=0:02:45, wall=00:53 IST
=> Training   16.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.597 Prec@1=61.906 Prec@5=83.889 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=00:53 IST
=> Training   16.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.597 Prec@1=61.906 Prec@5=83.889 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=00:54 IST
=> Training   16.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.601 Prec@1=61.799 Prec@5=83.833 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=00:54 IST
=> Training   20.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.601 Prec@1=61.799 Prec@5=83.833 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=00:54 IST
=> Training   20.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.601 Prec@1=61.799 Prec@5=83.833 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=00:55 IST
=> Training   20.02% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.561 DataTime=0.390 Loss=1.601 Prec@1=61.809 Prec@5=83.830 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=00:55 IST
=> Training   24.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.561 DataTime=0.390 Loss=1.601 Prec@1=61.809 Prec@5=83.830 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=00:55 IST
=> Training   24.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.561 DataTime=0.390 Loss=1.601 Prec@1=61.809 Prec@5=83.830 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=00:56 IST
=> Training   24.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.603 Prec@1=61.776 Prec@5=83.790 rate=1.81 Hz, eta=0:17:30, total=0:05:32, wall=00:56 IST
=> Training   28.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.603 Prec@1=61.776 Prec@5=83.790 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=00:56 IST
=> Training   28.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.562 DataTime=0.391 Loss=1.603 Prec@1=61.776 Prec@5=83.790 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=00:57 IST
=> Training   28.01% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.560 DataTime=0.390 Loss=1.604 Prec@1=61.760 Prec@5=83.760 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=00:57 IST
=> Training   32.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.560 DataTime=0.390 Loss=1.604 Prec@1=61.760 Prec@5=83.760 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=00:57 IST
=> Training   32.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.560 DataTime=0.390 Loss=1.604 Prec@1=61.760 Prec@5=83.760 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=00:58 IST
=> Training   32.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.605 Prec@1=61.686 Prec@5=83.768 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=00:58 IST
=> Training   36.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.605 Prec@1=61.686 Prec@5=83.768 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=00:58 IST
=> Training   36.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.605 Prec@1=61.686 Prec@5=83.768 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=00:59 IST
=> Training   36.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.608 Prec@1=61.647 Prec@5=83.730 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=00:59 IST
=> Training   39.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.608 Prec@1=61.647 Prec@5=83.730 rate=1.81 Hz, eta=0:13:52, total=0:09:14, wall=00:59 IST
=> Training   39.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.559 DataTime=0.388 Loss=1.608 Prec@1=61.647 Prec@5=83.730 rate=1.81 Hz, eta=0:13:52, total=0:09:14, wall=01:00 IST
=> Training   39.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.611 Prec@1=61.614 Prec@5=83.688 rate=1.81 Hz, eta=0:13:52, total=0:09:14, wall=01:00 IST
=> Training   43.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.611 Prec@1=61.614 Prec@5=83.688 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=01:00 IST
=> Training   43.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.611 Prec@1=61.614 Prec@5=83.688 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=01:01 IST
=> Training   43.99% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.610 Prec@1=61.622 Prec@5=83.710 rate=1.81 Hz, eta=0:12:55, total=0:10:09, wall=01:01 IST
=> Training   47.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.610 Prec@1=61.622 Prec@5=83.710 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=01:01 IST
=> Training   47.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.387 Loss=1.610 Prec@1=61.622 Prec@5=83.710 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=01:02 IST
=> Training   47.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.386 Loss=1.614 Prec@1=61.553 Prec@5=83.657 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=01:02 IST
=> Training   51.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.386 Loss=1.614 Prec@1=61.553 Prec@5=83.657 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=01:02 IST
=> Training   51.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.558 DataTime=0.386 Loss=1.614 Prec@1=61.553 Prec@5=83.657 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=01:03 IST
=> Training   51.98% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.615 Prec@1=61.533 Prec@5=83.643 rate=1.81 Hz, eta=0:11:05, total=0:12:00, wall=01:03 IST
=> Training   55.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.615 Prec@1=61.533 Prec@5=83.643 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=01:03 IST
=> Training   55.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.615 Prec@1=61.533 Prec@5=83.643 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=01:04 IST
=> Training   55.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.617 Prec@1=61.501 Prec@5=83.612 rate=1.81 Hz, eta=0:10:09, total=0:12:55, wall=01:04 IST
=> Training   59.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.617 Prec@1=61.501 Prec@5=83.612 rate=1.81 Hz, eta=0:09:14, total=0:13:51, wall=01:04 IST
=> Training   59.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.617 Prec@1=61.501 Prec@5=83.612 rate=1.81 Hz, eta=0:09:14, total=0:13:51, wall=01:05 IST
=> Training   59.97% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.618 Prec@1=61.494 Prec@5=83.602 rate=1.81 Hz, eta=0:09:14, total=0:13:51, wall=01:05 IST
=> Training   63.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.618 Prec@1=61.494 Prec@5=83.602 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=01:05 IST
=> Training   63.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.618 Prec@1=61.494 Prec@5=83.602 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=01:06 IST
=> Training   63.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.619 Prec@1=61.451 Prec@5=83.583 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=01:06 IST
=> Training   67.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.619 Prec@1=61.451 Prec@5=83.583 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=01:06 IST
=> Training   67.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.619 Prec@1=61.451 Prec@5=83.583 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=01:06 IST
=> Training   67.96% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.620 Prec@1=61.437 Prec@5=83.552 rate=1.80 Hz, eta=0:07:24, total=0:15:42, wall=01:06 IST
=> Training   71.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.620 Prec@1=61.437 Prec@5=83.552 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=01:06 IST
=> Training   71.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.386 Loss=1.620 Prec@1=61.437 Prec@5=83.552 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=01:07 IST
=> Training   71.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.620 Prec@1=61.438 Prec@5=83.550 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=01:07 IST
=> Training   75.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.620 Prec@1=61.438 Prec@5=83.550 rate=1.80 Hz, eta=0:05:33, total=0:17:33, wall=01:07 IST
=> Training   75.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.620 Prec@1=61.438 Prec@5=83.550 rate=1.80 Hz, eta=0:05:33, total=0:17:33, wall=01:08 IST
=> Training   75.95% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.621 Prec@1=61.428 Prec@5=83.549 rate=1.80 Hz, eta=0:05:33, total=0:17:33, wall=01:08 IST
=> Training   79.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.621 Prec@1=61.428 Prec@5=83.549 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=01:08 IST
=> Training   79.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.621 Prec@1=61.428 Prec@5=83.549 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=01:09 IST
=> Training   79.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.416 Prec@5=83.538 rate=1.80 Hz, eta=0:04:38, total=0:18:28, wall=01:09 IST
=> Training   83.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.416 Prec@5=83.538 rate=1.80 Hz, eta=0:03:42, total=0:19:25, wall=01:09 IST
=> Training   83.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.416 Prec@5=83.538 rate=1.80 Hz, eta=0:03:42, total=0:19:25, wall=01:10 IST
=> Training   83.94% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.399 Prec@5=83.535 rate=1.80 Hz, eta=0:03:42, total=0:19:25, wall=01:10 IST
=> Training   87.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.399 Prec@5=83.535 rate=1.80 Hz, eta=0:02:47, total=0:20:21, wall=01:10 IST
=> Training   87.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.622 Prec@1=61.399 Prec@5=83.535 rate=1.80 Hz, eta=0:02:47, total=0:20:21, wall=01:11 IST
=> Training   87.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.381 Prec@5=83.528 rate=1.80 Hz, eta=0:02:47, total=0:20:21, wall=01:11 IST
=> Training   91.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.381 Prec@5=83.528 rate=1.80 Hz, eta=0:01:52, total=0:21:16, wall=01:11 IST
=> Training   91.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.381 Prec@5=83.528 rate=1.80 Hz, eta=0:01:52, total=0:21:16, wall=01:12 IST
=> Training   91.93% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.384 Prec@5=83.516 rate=1.80 Hz, eta=0:01:52, total=0:21:16, wall=01:12 IST
=> Training   95.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.384 Prec@5=83.516 rate=1.80 Hz, eta=0:00:56, total=0:22:13, wall=01:12 IST
=> Training   95.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.623 Prec@1=61.384 Prec@5=83.516 rate=1.80 Hz, eta=0:00:56, total=0:22:13, wall=01:13 IST
=> Training   95.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.624 Prec@1=61.377 Prec@5=83.518 rate=1.80 Hz, eta=0:00:56, total=0:22:13, wall=01:13 IST
=> Training   99.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.624 Prec@1=61.377 Prec@5=83.518 rate=1.80 Hz, eta=0:00:01, total=0:23:07, wall=01:13 IST
=> Training   99.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.385 Loss=1.624 Prec@1=61.377 Prec@5=83.518 rate=1.80 Hz, eta=0:00:01, total=0:23:07, wall=01:13 IST
=> Training   99.92% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.384 Loss=1.624 Prec@1=61.376 Prec@5=83.518 rate=1.80 Hz, eta=0:00:01, total=0:23:07, wall=01:13 IST
=> Training   100.00% of 1x2503...Epoch=24/150 LR=0.0943 Time=0.557 DataTime=0.384 Loss=1.624 Prec@1=61.376 Prec@5=83.518 rate=1.80 Hz, eta=0:00:00, total=0:23:08, wall=01:13 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST
=> Validation 0.00% of 1x98...Epoch=24/150 LR=0.0943 Time=7.146 Loss=0.834 Prec@1=80.469 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=01:13 IST
=> Validation 1.02% of 1x98...Epoch=24/150 LR=0.0943 Time=7.146 Loss=0.834 Prec@1=80.469 Prec@5=94.336 rate=6908.46 Hz, eta=0:00:00, total=0:00:00, wall=01:13 IST
** Validation 1.02% of 1x98...Epoch=24/150 LR=0.0943 Time=7.146 Loss=0.834 Prec@1=80.469 Prec@5=94.336 rate=6908.46 Hz, eta=0:00:00, total=0:00:00, wall=01:14 IST
** Validation 1.02% of 1x98...Epoch=24/150 LR=0.0943 Time=0.638 Loss=1.682 Prec@1=60.210 Prec@5=83.162 rate=6908.46 Hz, eta=0:00:00, total=0:00:00, wall=01:14 IST
** Validation 100.00% of 1x98...Epoch=24/150 LR=0.0943 Time=0.638 Loss=1.682 Prec@1=60.210 Prec@5=83.162 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=01:14 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST
=> Training   0.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=5.149 DataTime=4.901 Loss=1.491 Prec@1=59.766 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=01:14 IST
=> Training   0.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=5.149 DataTime=4.901 Loss=1.491 Prec@1=59.766 Prec@5=84.766 rate=8942.62 Hz, eta=0:00:00, total=0:00:00, wall=01:14 IST
=> Training   0.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=5.149 DataTime=4.901 Loss=1.491 Prec@1=59.766 Prec@5=84.766 rate=8942.62 Hz, eta=0:00:00, total=0:00:00, wall=01:15 IST
=> Training   0.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.598 DataTime=0.436 Loss=1.568 Prec@1=62.537 Prec@5=84.448 rate=8942.62 Hz, eta=0:00:00, total=0:00:00, wall=01:15 IST
=> Training   4.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.598 DataTime=0.436 Loss=1.568 Prec@1=62.537 Prec@5=84.448 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=01:15 IST
=> Training   4.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.598 DataTime=0.436 Loss=1.568 Prec@1=62.537 Prec@5=84.448 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=01:16 IST
=> Training   4.04% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.592 DataTime=0.426 Loss=1.587 Prec@1=62.266 Prec@5=84.109 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=01:16 IST
=> Training   8.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.592 DataTime=0.426 Loss=1.587 Prec@1=62.266 Prec@5=84.109 rate=1.76 Hz, eta=0:21:44, total=0:01:53, wall=01:16 IST
=> Training   8.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.592 DataTime=0.426 Loss=1.587 Prec@1=62.266 Prec@5=84.109 rate=1.76 Hz, eta=0:21:44, total=0:01:53, wall=01:17 IST
=> Training   8.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.582 DataTime=0.415 Loss=1.592 Prec@1=62.141 Prec@5=84.042 rate=1.76 Hz, eta=0:21:44, total=0:01:53, wall=01:17 IST
=> Training   12.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.582 DataTime=0.415 Loss=1.592 Prec@1=62.141 Prec@5=84.042 rate=1.77 Hz, eta=0:20:44, total=0:02:50, wall=01:17 IST
=> Training   12.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.582 DataTime=0.415 Loss=1.592 Prec@1=62.141 Prec@5=84.042 rate=1.77 Hz, eta=0:20:44, total=0:02:50, wall=01:18 IST
=> Training   12.03% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.580 DataTime=0.412 Loss=1.595 Prec@1=62.031 Prec@5=83.973 rate=1.77 Hz, eta=0:20:44, total=0:02:50, wall=01:18 IST
=> Training   16.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.580 DataTime=0.412 Loss=1.595 Prec@1=62.031 Prec@5=83.973 rate=1.76 Hz, eta=0:19:53, total=0:03:47, wall=01:18 IST
=> Training   16.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.580 DataTime=0.412 Loss=1.595 Prec@1=62.031 Prec@5=83.973 rate=1.76 Hz, eta=0:19:53, total=0:03:47, wall=01:19 IST
=> Training   16.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.574 DataTime=0.404 Loss=1.595 Prec@1=62.052 Prec@5=83.970 rate=1.76 Hz, eta=0:19:53, total=0:03:47, wall=01:19 IST
=> Training   20.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.574 DataTime=0.404 Loss=1.595 Prec@1=62.052 Prec@5=83.970 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=01:19 IST
=> Training   20.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.574 DataTime=0.404 Loss=1.595 Prec@1=62.052 Prec@5=83.970 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=01:20 IST
=> Training   20.02% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.573 DataTime=0.402 Loss=1.592 Prec@1=62.105 Prec@5=83.994 rate=1.77 Hz, eta=0:18:48, total=0:04:42, wall=01:20 IST
=> Training   24.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.573 DataTime=0.402 Loss=1.592 Prec@1=62.105 Prec@5=83.994 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=01:20 IST
=> Training   24.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.573 DataTime=0.402 Loss=1.592 Prec@1=62.105 Prec@5=83.994 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=01:21 IST
=> Training   24.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.571 DataTime=0.400 Loss=1.594 Prec@1=62.068 Prec@5=83.973 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=01:21 IST
=> Training   28.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.571 DataTime=0.400 Loss=1.594 Prec@1=62.068 Prec@5=83.973 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=01:21 IST
=> Training   28.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.571 DataTime=0.400 Loss=1.594 Prec@1=62.068 Prec@5=83.973 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=01:22 IST
=> Training   28.01% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.569 DataTime=0.398 Loss=1.597 Prec@1=61.961 Prec@5=83.919 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=01:22 IST
=> Training   32.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.569 DataTime=0.398 Loss=1.597 Prec@1=61.961 Prec@5=83.919 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=01:22 IST
=> Training   32.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.569 DataTime=0.398 Loss=1.597 Prec@1=61.961 Prec@5=83.919 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=01:23 IST
=> Training   32.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.599 Prec@1=61.929 Prec@5=83.902 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=01:23 IST
=> Training   36.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.599 Prec@1=61.929 Prec@5=83.902 rate=1.78 Hz, eta=0:14:57, total=0:08:25, wall=01:23 IST
=> Training   36.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.599 Prec@1=61.929 Prec@5=83.902 rate=1.78 Hz, eta=0:14:57, total=0:08:25, wall=01:23 IST
=> Training   36.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.396 Loss=1.600 Prec@1=61.907 Prec@5=83.884 rate=1.78 Hz, eta=0:14:57, total=0:08:25, wall=01:23 IST
=> Training   39.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.396 Loss=1.600 Prec@1=61.907 Prec@5=83.884 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=01:23 IST
=> Training   39.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.396 Loss=1.600 Prec@1=61.907 Prec@5=83.884 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=01:24 IST
=> Training   39.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.395 Loss=1.600 Prec@1=61.873 Prec@5=83.891 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=01:24 IST
=> Training   43.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.395 Loss=1.600 Prec@1=61.873 Prec@5=83.891 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:24 IST
=> Training   43.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.567 DataTime=0.395 Loss=1.600 Prec@1=61.873 Prec@5=83.891 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:25 IST
=> Training   43.99% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.600 Prec@1=61.889 Prec@5=83.883 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:25 IST
=> Training   47.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.600 Prec@1=61.889 Prec@5=83.883 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:25 IST
=> Training   47.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.395 Loss=1.600 Prec@1=61.889 Prec@5=83.883 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:26 IST
=> Training   47.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.393 Loss=1.602 Prec@1=61.845 Prec@5=83.847 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:26 IST
=> Training   51.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.393 Loss=1.602 Prec@1=61.845 Prec@5=83.847 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=01:26 IST
=> Training   51.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.566 DataTime=0.393 Loss=1.602 Prec@1=61.845 Prec@5=83.847 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=01:27 IST
=> Training   51.98% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.564 DataTime=0.392 Loss=1.602 Prec@1=61.838 Prec@5=83.841 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=01:27 IST
=> Training   55.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.564 DataTime=0.392 Loss=1.602 Prec@1=61.838 Prec@5=83.841 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:27 IST
=> Training   55.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.564 DataTime=0.392 Loss=1.602 Prec@1=61.838 Prec@5=83.841 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:28 IST
=> Training   55.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.603 Prec@1=61.824 Prec@5=83.820 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:28 IST
=> Training   59.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.603 Prec@1=61.824 Prec@5=83.820 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=01:28 IST
=> Training   59.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.603 Prec@1=61.824 Prec@5=83.820 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=01:29 IST
=> Training   59.97% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.604 Prec@1=61.801 Prec@5=83.792 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=01:29 IST
=> Training   63.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.604 Prec@1=61.801 Prec@5=83.792 rate=1.78 Hz, eta=0:08:25, total=0:14:56, wall=01:29 IST
=> Training   63.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.604 Prec@1=61.801 Prec@5=83.792 rate=1.78 Hz, eta=0:08:25, total=0:14:56, wall=01:30 IST
=> Training   63.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.605 Prec@1=61.778 Prec@5=83.794 rate=1.78 Hz, eta=0:08:25, total=0:14:56, wall=01:30 IST
=> Training   67.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.605 Prec@1=61.778 Prec@5=83.794 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=01:30 IST
=> Training   67.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.391 Loss=1.605 Prec@1=61.778 Prec@5=83.794 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=01:31 IST
=> Training   67.96% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.392 Loss=1.606 Prec@1=61.765 Prec@5=83.780 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=01:31 IST
=> Training   71.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.392 Loss=1.606 Prec@1=61.765 Prec@5=83.780 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:31 IST
=> Training   71.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.563 DataTime=0.392 Loss=1.606 Prec@1=61.765 Prec@5=83.780 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:32 IST
=> Training   71.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.606 Prec@1=61.748 Prec@5=83.776 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:32 IST
=> Training   75.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.606 Prec@1=61.748 Prec@5=83.776 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=01:32 IST
=> Training   75.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.606 Prec@1=61.748 Prec@5=83.776 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=01:33 IST
=> Training   75.95% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.607 Prec@1=61.737 Prec@5=83.763 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=01:33 IST
=> Training   79.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.607 Prec@1=61.737 Prec@5=83.763 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=01:33 IST
=> Training   79.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.390 Loss=1.607 Prec@1=61.737 Prec@5=83.763 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=01:34 IST
=> Training   79.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.609 Prec@1=61.705 Prec@5=83.739 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=01:34 IST
=> Training   83.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.609 Prec@1=61.705 Prec@5=83.739 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=01:34 IST
=> Training   83.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.609 Prec@1=61.705 Prec@5=83.739 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=01:35 IST
=> Training   83.94% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.610 Prec@1=61.685 Prec@5=83.725 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=01:35 IST
=> Training   87.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.610 Prec@1=61.685 Prec@5=83.725 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=01:35 IST
=> Training   87.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.610 Prec@1=61.685 Prec@5=83.725 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=01:36 IST
=> Training   87.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.611 Prec@1=61.652 Prec@5=83.699 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=01:36 IST
=> Training   91.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.611 Prec@1=61.652 Prec@5=83.699 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=01:36 IST
=> Training   91.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.611 Prec@1=61.652 Prec@5=83.699 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=01:36 IST
=> Training   91.93% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.612 Prec@1=61.638 Prec@5=83.699 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=01:36 IST
=> Training   95.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.612 Prec@1=61.638 Prec@5=83.699 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:36 IST
=> Training   95.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.562 DataTime=0.389 Loss=1.612 Prec@1=61.638 Prec@5=83.699 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:37 IST
=> Training   95.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.612 Prec@1=61.625 Prec@5=83.688 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:37 IST
=> Training   99.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.612 Prec@1=61.625 Prec@5=83.688 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=01:37 IST
=> Training   99.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.612 Prec@1=61.625 Prec@5=83.688 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=01:37 IST
=> Training   99.92% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.612 Prec@1=61.625 Prec@5=83.689 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=01:37 IST
=> Training   100.00% of 1x2503...Epoch=25/150 LR=0.0938 Time=0.561 DataTime=0.389 Loss=1.612 Prec@1=61.625 Prec@5=83.689 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=01:37 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:38 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:38 IST
=> Validation 0.00% of 1x98...Epoch=25/150 LR=0.0938 Time=6.938 Loss=1.155 Prec@1=70.508 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=01:38 IST
=> Validation 1.02% of 1x98...Epoch=25/150 LR=0.0938 Time=6.938 Loss=1.155 Prec@1=70.508 Prec@5=89.453 rate=6366.75 Hz, eta=0:00:00, total=0:00:00, wall=01:38 IST
** Validation 1.02% of 1x98...Epoch=25/150 LR=0.0938 Time=6.938 Loss=1.155 Prec@1=70.508 Prec@5=89.453 rate=6366.75 Hz, eta=0:00:00, total=0:00:00, wall=01:38 IST
** Validation 1.02% of 1x98...Epoch=25/150 LR=0.0938 Time=0.634 Loss=1.664 Prec@1=60.524 Prec@5=83.600 rate=6366.75 Hz, eta=0:00:00, total=0:00:00, wall=01:38 IST
** Validation 100.00% of 1x98...Epoch=25/150 LR=0.0938 Time=0.634 Loss=1.664 Prec@1=60.524 Prec@5=83.600 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=01:38 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST
=> Training   0.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=5.304 DataTime=5.070 Loss=1.620 Prec@1=62.500 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=01:39 IST
=> Training   0.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=5.304 DataTime=5.070 Loss=1.620 Prec@1=62.500 Prec@5=83.594 rate=7222.46 Hz, eta=0:00:00, total=0:00:00, wall=01:39 IST
=> Training   0.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=5.304 DataTime=5.070 Loss=1.620 Prec@1=62.500 Prec@5=83.594 rate=7222.46 Hz, eta=0:00:00, total=0:00:00, wall=01:39 IST
=> Training   0.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.601 DataTime=0.436 Loss=1.571 Prec@1=62.382 Prec@5=84.410 rate=7222.46 Hz, eta=0:00:00, total=0:00:00, wall=01:39 IST
=> Training   4.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.601 DataTime=0.436 Loss=1.571 Prec@1=62.382 Prec@5=84.410 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=01:39 IST
=> Training   4.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.601 DataTime=0.436 Loss=1.571 Prec@1=62.382 Prec@5=84.410 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=01:40 IST
=> Training   4.04% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.592 DataTime=0.426 Loss=1.567 Prec@1=62.506 Prec@5=84.302 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=01:40 IST
=> Training   8.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.592 DataTime=0.426 Loss=1.567 Prec@1=62.506 Prec@5=84.302 rate=1.77 Hz, eta=0:21:43, total=0:01:53, wall=01:40 IST
=> Training   8.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.592 DataTime=0.426 Loss=1.567 Prec@1=62.506 Prec@5=84.302 rate=1.77 Hz, eta=0:21:43, total=0:01:53, wall=01:41 IST
=> Training   8.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.577 DataTime=0.407 Loss=1.571 Prec@1=62.523 Prec@5=84.211 rate=1.77 Hz, eta=0:21:43, total=0:01:53, wall=01:41 IST
=> Training   12.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.577 DataTime=0.407 Loss=1.571 Prec@1=62.523 Prec@5=84.211 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=01:41 IST
=> Training   12.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.577 DataTime=0.407 Loss=1.571 Prec@1=62.523 Prec@5=84.211 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=01:42 IST
=> Training   12.03% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.576 DataTime=0.405 Loss=1.573 Prec@1=62.400 Prec@5=84.202 rate=1.79 Hz, eta=0:20:30, total=0:02:48, wall=01:42 IST
=> Training   16.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.576 DataTime=0.405 Loss=1.573 Prec@1=62.400 Prec@5=84.202 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=01:42 IST
=> Training   16.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.576 DataTime=0.405 Loss=1.573 Prec@1=62.400 Prec@5=84.202 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=01:43 IST
=> Training   16.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.571 DataTime=0.401 Loss=1.575 Prec@1=62.340 Prec@5=84.187 rate=1.78 Hz, eta=0:19:42, total=0:03:45, wall=01:43 IST
=> Training   20.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.571 DataTime=0.401 Loss=1.575 Prec@1=62.340 Prec@5=84.187 rate=1.78 Hz, eta=0:18:42, total=0:04:41, wall=01:43 IST
=> Training   20.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.571 DataTime=0.401 Loss=1.575 Prec@1=62.340 Prec@5=84.187 rate=1.78 Hz, eta=0:18:42, total=0:04:41, wall=01:44 IST
=> Training   20.02% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.570 DataTime=0.400 Loss=1.574 Prec@1=62.357 Prec@5=84.207 rate=1.78 Hz, eta=0:18:42, total=0:04:41, wall=01:44 IST
=> Training   24.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.570 DataTime=0.400 Loss=1.574 Prec@1=62.357 Prec@5=84.207 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=01:44 IST
=> Training   24.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.570 DataTime=0.400 Loss=1.574 Prec@1=62.357 Prec@5=84.207 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=01:45 IST
=> Training   24.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.577 Prec@1=62.310 Prec@5=84.157 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=01:45 IST
=> Training   28.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.577 Prec@1=62.310 Prec@5=84.157 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=01:45 IST
=> Training   28.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.577 Prec@1=62.310 Prec@5=84.157 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=01:46 IST
=> Training   28.01% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.577 Prec@1=62.293 Prec@5=84.149 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=01:46 IST
=> Training   32.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.577 Prec@1=62.293 Prec@5=84.149 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=01:46 IST
=> Training   32.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.577 Prec@1=62.293 Prec@5=84.149 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=01:47 IST
=> Training   32.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.579 Prec@1=62.245 Prec@5=84.126 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=01:47 IST
=> Training   36.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.579 Prec@1=62.245 Prec@5=84.126 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=01:47 IST
=> Training   36.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.568 DataTime=0.398 Loss=1.579 Prec@1=62.245 Prec@5=84.126 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=01:48 IST
=> Training   36.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.582 Prec@1=62.204 Prec@5=84.098 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=01:48 IST
=> Training   39.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.582 Prec@1=62.204 Prec@5=84.098 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=01:48 IST
=> Training   39.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.567 DataTime=0.397 Loss=1.582 Prec@1=62.204 Prec@5=84.098 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=01:49 IST
=> Training   39.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.395 Loss=1.584 Prec@1=62.187 Prec@5=84.079 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=01:49 IST
=> Training   43.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.395 Loss=1.584 Prec@1=62.187 Prec@5=84.079 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=01:49 IST
=> Training   43.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.395 Loss=1.584 Prec@1=62.187 Prec@5=84.079 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=01:50 IST
=> Training   43.99% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.150 Prec@5=84.047 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=01:50 IST
=> Training   47.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.150 Prec@5=84.047 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:50 IST
=> Training   47.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.150 Prec@5=84.047 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:51 IST
=> Training   47.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.588 Prec@1=62.097 Prec@5=84.023 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:51 IST
=> Training   51.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.588 Prec@1=62.097 Prec@5=84.023 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=01:51 IST
=> Training   51.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.588 Prec@1=62.097 Prec@5=84.023 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=01:52 IST
=> Training   51.98% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.589 Prec@1=62.078 Prec@5=84.014 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=01:52 IST
=> Training   55.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.589 Prec@1=62.078 Prec@5=84.014 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=01:52 IST
=> Training   55.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.589 Prec@1=62.078 Prec@5=84.014 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=01:53 IST
=> Training   55.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.591 Prec@1=62.039 Prec@5=83.979 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=01:53 IST
=> Training   59.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.591 Prec@1=62.039 Prec@5=83.979 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=01:53 IST
=> Training   59.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.591 Prec@1=62.039 Prec@5=83.979 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=01:54 IST
=> Training   59.97% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.592 Prec@1=62.032 Prec@5=83.961 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=01:54 IST
=> Training   63.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.592 Prec@1=62.032 Prec@5=83.961 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=01:54 IST
=> Training   63.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.565 DataTime=0.394 Loss=1.592 Prec@1=62.032 Prec@5=83.961 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=01:54 IST
=> Training   63.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.593 Prec@1=61.997 Prec@5=83.941 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=01:54 IST
=> Training   67.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.593 Prec@1=61.997 Prec@5=83.941 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:54 IST
=> Training   67.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.393 Loss=1.593 Prec@1=61.997 Prec@5=83.941 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:55 IST
=> Training   67.96% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.394 Loss=1.595 Prec@1=61.958 Prec@5=83.906 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:55 IST
=> Training   71.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.394 Loss=1.595 Prec@1=61.958 Prec@5=83.906 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=01:55 IST
=> Training   71.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.564 DataTime=0.394 Loss=1.595 Prec@1=61.958 Prec@5=83.906 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=01:56 IST
=> Training   71.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.596 Prec@1=61.936 Prec@5=83.892 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=01:56 IST
=> Training   75.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.596 Prec@1=61.936 Prec@5=83.892 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=01:56 IST
=> Training   75.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.596 Prec@1=61.936 Prec@5=83.892 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=01:57 IST
=> Training   75.95% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.597 Prec@1=61.918 Prec@5=83.867 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=01:57 IST
=> Training   79.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.597 Prec@1=61.918 Prec@5=83.867 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:57 IST
=> Training   79.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.597 Prec@1=61.918 Prec@5=83.867 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:58 IST
=> Training   79.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.392 Loss=1.599 Prec@1=61.882 Prec@5=83.851 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:58 IST
=> Training   83.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.392 Loss=1.599 Prec@1=61.882 Prec@5=83.851 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:58 IST
=> Training   83.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.392 Loss=1.599 Prec@1=61.882 Prec@5=83.851 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:59 IST
=> Training   83.94% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.599 Prec@1=61.879 Prec@5=83.851 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:59 IST
=> Training   87.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.599 Prec@1=61.879 Prec@5=83.851 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=01:59 IST
=> Training   87.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.599 Prec@1=61.879 Prec@5=83.851 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=02:00 IST
=> Training   87.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.391 Loss=1.600 Prec@1=61.864 Prec@5=83.839 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=02:00 IST
=> Training   91.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.391 Loss=1.600 Prec@1=61.864 Prec@5=83.839 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=02:00 IST
=> Training   91.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.391 Loss=1.600 Prec@1=61.864 Prec@5=83.839 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=02:01 IST
=> Training   91.93% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.601 Prec@1=61.850 Prec@5=83.817 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=02:01 IST
=> Training   95.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.601 Prec@1=61.850 Prec@5=83.817 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=02:01 IST
=> Training   95.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.563 DataTime=0.392 Loss=1.601 Prec@1=61.850 Prec@5=83.817 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=02:02 IST
=> Training   95.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.390 Loss=1.602 Prec@1=61.853 Prec@5=83.813 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=02:02 IST
=> Training   99.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.390 Loss=1.602 Prec@1=61.853 Prec@5=83.813 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=02:02 IST
=> Training   99.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.390 Loss=1.602 Prec@1=61.853 Prec@5=83.813 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=02:02 IST
=> Training   99.92% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.390 Loss=1.602 Prec@1=61.850 Prec@5=83.812 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=02:02 IST
=> Training   100.00% of 1x2503...Epoch=26/150 LR=0.0933 Time=0.562 DataTime=0.390 Loss=1.602 Prec@1=61.850 Prec@5=83.812 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=02:02 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:02 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:02 IST
=> Validation 0.00% of 1x98...Epoch=26/150 LR=0.0933 Time=6.896 Loss=0.953 Prec@1=74.805 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=02:02 IST
=> Validation 1.02% of 1x98...Epoch=26/150 LR=0.0933 Time=6.896 Loss=0.953 Prec@1=74.805 Prec@5=93.164 rate=6787.67 Hz, eta=0:00:00, total=0:00:00, wall=02:02 IST
** Validation 1.02% of 1x98...Epoch=26/150 LR=0.0933 Time=6.896 Loss=0.953 Prec@1=74.805 Prec@5=93.164 rate=6787.67 Hz, eta=0:00:00, total=0:00:00, wall=02:03 IST
** Validation 1.02% of 1x98...Epoch=26/150 LR=0.0933 Time=0.632 Loss=1.698 Prec@1=59.674 Prec@5=82.794 rate=6787.67 Hz, eta=0:00:00, total=0:00:00, wall=02:03 IST
** Validation 100.00% of 1x98...Epoch=26/150 LR=0.0933 Time=0.632 Loss=1.698 Prec@1=59.674 Prec@5=82.794 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=02:03 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:03 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:03 IST
=> Training   0.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=4.785 DataTime=4.281 Loss=1.590 Prec@1=62.500 Prec@5=83.594 rate=0 Hz, eta=?, total=0:00:00, wall=02:03 IST
=> Training   0.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=4.785 DataTime=4.281 Loss=1.590 Prec@1=62.500 Prec@5=83.594 rate=20.02 Hz, eta=0:02:04, total=0:00:00, wall=02:03 IST
=> Training   0.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=4.785 DataTime=4.281 Loss=1.590 Prec@1=62.500 Prec@5=83.594 rate=20.02 Hz, eta=0:02:04, total=0:00:00, wall=02:04 IST
=> Training   0.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.591 DataTime=0.425 Loss=1.550 Prec@1=62.958 Prec@5=84.621 rate=20.02 Hz, eta=0:02:04, total=0:00:00, wall=02:04 IST
=> Training   4.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.591 DataTime=0.425 Loss=1.550 Prec@1=62.958 Prec@5=84.621 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=02:04 IST
=> Training   4.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.591 DataTime=0.425 Loss=1.550 Prec@1=62.958 Prec@5=84.621 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=02:05 IST
=> Training   4.04% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.406 Loss=1.557 Prec@1=62.733 Prec@5=84.513 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=02:05 IST
=> Training   8.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.406 Loss=1.557 Prec@1=62.733 Prec@5=84.513 rate=1.83 Hz, eta=0:20:59, total=0:01:50, wall=02:05 IST
=> Training   8.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.406 Loss=1.557 Prec@1=62.733 Prec@5=84.513 rate=1.83 Hz, eta=0:20:59, total=0:01:50, wall=02:06 IST
=> Training   8.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.561 Prec@1=62.662 Prec@5=84.371 rate=1.83 Hz, eta=0:20:59, total=0:01:50, wall=02:06 IST
=> Training   12.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.561 Prec@1=62.662 Prec@5=84.371 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:06 IST
=> Training   12.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.561 Prec@1=62.662 Prec@5=84.371 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:07 IST
=> Training   12.03% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.406 Loss=1.560 Prec@1=62.667 Prec@5=84.420 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:07 IST
=> Training   16.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.406 Loss=1.560 Prec@1=62.667 Prec@5=84.420 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=02:07 IST
=> Training   16.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.406 Loss=1.560 Prec@1=62.667 Prec@5=84.420 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=02:08 IST
=> Training   16.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.560 Prec@1=62.652 Prec@5=84.435 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=02:08 IST
=> Training   20.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.560 Prec@1=62.652 Prec@5=84.435 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=02:08 IST
=> Training   20.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.404 Loss=1.560 Prec@1=62.652 Prec@5=84.435 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=02:09 IST
=> Training   20.02% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.404 Loss=1.564 Prec@1=62.586 Prec@5=84.384 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=02:09 IST
=> Training   24.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.404 Loss=1.564 Prec@1=62.586 Prec@5=84.384 rate=1.77 Hz, eta=0:17:52, total=0:05:39, wall=02:09 IST
=> Training   24.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.572 DataTime=0.404 Loss=1.564 Prec@1=62.586 Prec@5=84.384 rate=1.77 Hz, eta=0:17:52, total=0:05:39, wall=02:10 IST
=> Training   24.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.401 Loss=1.565 Prec@1=62.553 Prec@5=84.361 rate=1.77 Hz, eta=0:17:52, total=0:05:39, wall=02:10 IST
=> Training   28.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.401 Loss=1.565 Prec@1=62.553 Prec@5=84.361 rate=1.77 Hz, eta=0:16:55, total=0:06:34, wall=02:10 IST
=> Training   28.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.401 Loss=1.565 Prec@1=62.553 Prec@5=84.361 rate=1.77 Hz, eta=0:16:55, total=0:06:34, wall=02:11 IST
=> Training   28.01% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.569 DataTime=0.400 Loss=1.566 Prec@1=62.550 Prec@5=84.335 rate=1.77 Hz, eta=0:16:55, total=0:06:34, wall=02:11 IST
=> Training   32.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.569 DataTime=0.400 Loss=1.566 Prec@1=62.550 Prec@5=84.335 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=02:11 IST
=> Training   32.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.569 DataTime=0.400 Loss=1.566 Prec@1=62.550 Prec@5=84.335 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=02:11 IST
=> Training   32.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.399 Loss=1.569 Prec@1=62.485 Prec@5=84.274 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=02:11 IST
=> Training   36.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.399 Loss=1.569 Prec@1=62.485 Prec@5=84.274 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=02:11 IST
=> Training   36.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.399 Loss=1.569 Prec@1=62.485 Prec@5=84.274 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=02:12 IST
=> Training   36.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.400 Loss=1.572 Prec@1=62.424 Prec@5=84.236 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=02:12 IST
=> Training   39.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.400 Loss=1.572 Prec@1=62.424 Prec@5=84.236 rate=1.77 Hz, eta=0:14:08, total=0:09:25, wall=02:12 IST
=> Training   39.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.570 DataTime=0.400 Loss=1.572 Prec@1=62.424 Prec@5=84.236 rate=1.77 Hz, eta=0:14:08, total=0:09:25, wall=02:13 IST
=> Training   39.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.397 Loss=1.574 Prec@1=62.397 Prec@5=84.207 rate=1.77 Hz, eta=0:14:08, total=0:09:25, wall=02:13 IST
=> Training   43.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.397 Loss=1.574 Prec@1=62.397 Prec@5=84.207 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=02:13 IST
=> Training   43.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.397 Loss=1.574 Prec@1=62.397 Prec@5=84.207 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=02:14 IST
=> Training   43.99% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.398 Loss=1.576 Prec@1=62.335 Prec@5=84.180 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=02:14 IST
=> Training   47.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.398 Loss=1.576 Prec@1=62.335 Prec@5=84.180 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=02:14 IST
=> Training   47.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.398 Loss=1.576 Prec@1=62.335 Prec@5=84.180 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=02:15 IST
=> Training   47.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.397 Loss=1.577 Prec@1=62.312 Prec@5=84.171 rate=1.77 Hz, eta=0:12:14, total=0:11:17, wall=02:15 IST
=> Training   51.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.397 Loss=1.577 Prec@1=62.312 Prec@5=84.171 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=02:15 IST
=> Training   51.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.568 DataTime=0.397 Loss=1.577 Prec@1=62.312 Prec@5=84.171 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=02:16 IST
=> Training   51.98% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.580 Prec@1=62.241 Prec@5=84.114 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=02:16 IST
=> Training   55.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.580 Prec@1=62.241 Prec@5=84.114 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=02:16 IST
=> Training   55.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.580 Prec@1=62.241 Prec@5=84.114 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=02:17 IST
=> Training   55.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.582 Prec@1=62.212 Prec@5=84.087 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=02:17 IST
=> Training   59.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.582 Prec@1=62.212 Prec@5=84.087 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:17 IST
=> Training   59.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.582 Prec@1=62.212 Prec@5=84.087 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:18 IST
=> Training   59.97% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.583 Prec@1=62.199 Prec@5=84.079 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:18 IST
=> Training   63.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.583 Prec@1=62.199 Prec@5=84.079 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:18 IST
=> Training   63.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.567 DataTime=0.396 Loss=1.583 Prec@1=62.199 Prec@5=84.079 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:19 IST
=> Training   63.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.584 Prec@1=62.163 Prec@5=84.066 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:19 IST
=> Training   67.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.584 Prec@1=62.163 Prec@5=84.066 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=02:19 IST
=> Training   67.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.584 Prec@1=62.163 Prec@5=84.066 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=02:20 IST
=> Training   67.96% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.585 Prec@1=62.146 Prec@5=84.055 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=02:20 IST
=> Training   71.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.585 Prec@1=62.146 Prec@5=84.055 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=02:20 IST
=> Training   71.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.585 Prec@1=62.146 Prec@5=84.055 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=02:21 IST
=> Training   71.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.121 Prec@5=84.044 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=02:21 IST
=> Training   75.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.121 Prec@5=84.044 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:21 IST
=> Training   75.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.586 Prec@1=62.121 Prec@5=84.044 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:22 IST
=> Training   75.95% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.587 Prec@1=62.102 Prec@5=84.041 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:22 IST
=> Training   79.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.587 Prec@1=62.102 Prec@5=84.041 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=02:22 IST
=> Training   79.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.395 Loss=1.587 Prec@1=62.102 Prec@5=84.041 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=02:23 IST
=> Training   79.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.394 Loss=1.588 Prec@1=62.067 Prec@5=84.025 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=02:23 IST
=> Training   83.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.394 Loss=1.588 Prec@1=62.067 Prec@5=84.025 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:23 IST
=> Training   83.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.394 Loss=1.588 Prec@1=62.067 Prec@5=84.025 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:24 IST
=> Training   83.94% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.393 Loss=1.589 Prec@1=62.050 Prec@5=84.005 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:24 IST
=> Training   87.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.393 Loss=1.589 Prec@1=62.050 Prec@5=84.005 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=02:24 IST
=> Training   87.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.566 DataTime=0.393 Loss=1.589 Prec@1=62.050 Prec@5=84.005 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=02:25 IST
=> Training   87.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.392 Loss=1.590 Prec@1=62.030 Prec@5=83.997 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=02:25 IST
=> Training   91.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.392 Loss=1.590 Prec@1=62.030 Prec@5=83.997 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:25 IST
=> Training   91.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.392 Loss=1.590 Prec@1=62.030 Prec@5=83.997 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:26 IST
=> Training   91.93% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.393 Loss=1.591 Prec@1=62.017 Prec@5=83.980 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:26 IST
=> Training   95.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.393 Loss=1.591 Prec@1=62.017 Prec@5=83.980 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=02:26 IST
=> Training   95.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.565 DataTime=0.393 Loss=1.591 Prec@1=62.017 Prec@5=83.980 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=02:26 IST
=> Training   95.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.564 DataTime=0.392 Loss=1.592 Prec@1=62.014 Prec@5=83.972 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=02:26 IST
=> Training   99.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.564 DataTime=0.392 Loss=1.592 Prec@1=62.014 Prec@5=83.972 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:26 IST
=> Training   99.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.564 DataTime=0.392 Loss=1.592 Prec@1=62.014 Prec@5=83.972 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:26 IST
=> Training   99.92% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.564 DataTime=0.392 Loss=1.592 Prec@1=62.013 Prec@5=83.970 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:26 IST
=> Training   100.00% of 1x2503...Epoch=27/150 LR=0.0928 Time=0.564 DataTime=0.392 Loss=1.592 Prec@1=62.013 Prec@5=83.970 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=02:26 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST
=> Validation 0.00% of 1x98...Epoch=27/150 LR=0.0928 Time=6.916 Loss=1.219 Prec@1=68.164 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=02:27 IST
=> Validation 1.02% of 1x98...Epoch=27/150 LR=0.0928 Time=6.916 Loss=1.219 Prec@1=68.164 Prec@5=91.211 rate=6198.71 Hz, eta=0:00:00, total=0:00:00, wall=02:27 IST
** Validation 1.02% of 1x98...Epoch=27/150 LR=0.0928 Time=6.916 Loss=1.219 Prec@1=68.164 Prec@5=91.211 rate=6198.71 Hz, eta=0:00:00, total=0:00:00, wall=02:28 IST
** Validation 1.02% of 1x98...Epoch=27/150 LR=0.0928 Time=0.642 Loss=1.644 Prec@1=60.486 Prec@5=83.556 rate=6198.71 Hz, eta=0:00:00, total=0:00:00, wall=02:28 IST
** Validation 100.00% of 1x98...Epoch=27/150 LR=0.0928 Time=0.642 Loss=1.644 Prec@1=60.486 Prec@5=83.556 rate=1.75 Hz, eta=0:00:00, total=0:00:56, wall=02:28 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:28 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:28 IST
=> Training   0.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=5.178 DataTime=4.832 Loss=1.680 Prec@1=60.547 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=02:28 IST
=> Training   0.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=5.178 DataTime=4.832 Loss=1.680 Prec@1=60.547 Prec@5=83.203 rate=4862.99 Hz, eta=0:00:00, total=0:00:00, wall=02:28 IST
=> Training   0.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=5.178 DataTime=4.832 Loss=1.680 Prec@1=60.547 Prec@5=83.203 rate=4862.99 Hz, eta=0:00:00, total=0:00:00, wall=02:29 IST
=> Training   0.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.595 DataTime=0.436 Loss=1.535 Prec@1=63.138 Prec@5=85.002 rate=4862.99 Hz, eta=0:00:00, total=0:00:00, wall=02:29 IST
=> Training   4.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.595 DataTime=0.436 Loss=1.535 Prec@1=63.138 Prec@5=85.002 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:29 IST
=> Training   4.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.595 DataTime=0.436 Loss=1.535 Prec@1=63.138 Prec@5=85.002 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:29 IST
=> Training   4.04% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.584 DataTime=0.423 Loss=1.542 Prec@1=63.055 Prec@5=84.858 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:29 IST
=> Training   8.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.584 DataTime=0.423 Loss=1.542 Prec@1=63.055 Prec@5=84.858 rate=1.79 Hz, eta=0:21:25, total=0:01:52, wall=02:29 IST
=> Training   8.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.584 DataTime=0.423 Loss=1.542 Prec@1=63.055 Prec@5=84.858 rate=1.79 Hz, eta=0:21:25, total=0:01:52, wall=02:30 IST
=> Training   8.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.576 DataTime=0.412 Loss=1.546 Prec@1=62.989 Prec@5=84.729 rate=1.79 Hz, eta=0:21:25, total=0:01:52, wall=02:30 IST
=> Training   12.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.576 DataTime=0.412 Loss=1.546 Prec@1=62.989 Prec@5=84.729 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=02:30 IST
=> Training   12.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.576 DataTime=0.412 Loss=1.546 Prec@1=62.989 Prec@5=84.729 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=02:31 IST
=> Training   12.03% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.573 DataTime=0.407 Loss=1.549 Prec@1=62.936 Prec@5=84.649 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=02:31 IST
=> Training   16.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.573 DataTime=0.407 Loss=1.549 Prec@1=62.936 Prec@5=84.649 rate=1.79 Hz, eta=0:19:37, total=0:03:44, wall=02:31 IST
=> Training   16.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.573 DataTime=0.407 Loss=1.549 Prec@1=62.936 Prec@5=84.649 rate=1.79 Hz, eta=0:19:37, total=0:03:44, wall=02:32 IST
=> Training   16.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.574 DataTime=0.407 Loss=1.551 Prec@1=62.858 Prec@5=84.624 rate=1.79 Hz, eta=0:19:37, total=0:03:44, wall=02:32 IST
=> Training   20.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.574 DataTime=0.407 Loss=1.551 Prec@1=62.858 Prec@5=84.624 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=02:32 IST
=> Training   20.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.574 DataTime=0.407 Loss=1.551 Prec@1=62.858 Prec@5=84.624 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=02:33 IST
=> Training   20.02% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.403 Loss=1.550 Prec@1=62.869 Prec@5=84.643 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=02:33 IST
=> Training   24.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.403 Loss=1.550 Prec@1=62.869 Prec@5=84.643 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=02:33 IST
=> Training   24.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.403 Loss=1.550 Prec@1=62.869 Prec@5=84.643 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=02:34 IST
=> Training   24.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.571 DataTime=0.402 Loss=1.554 Prec@1=62.820 Prec@5=84.566 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=02:34 IST
=> Training   28.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.571 DataTime=0.402 Loss=1.554 Prec@1=62.820 Prec@5=84.566 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=02:34 IST
=> Training   28.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.571 DataTime=0.402 Loss=1.554 Prec@1=62.820 Prec@5=84.566 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=02:35 IST
=> Training   28.01% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.555 Prec@1=62.762 Prec@5=84.558 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=02:35 IST
=> Training   32.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.555 Prec@1=62.762 Prec@5=84.558 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=02:35 IST
=> Training   32.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.555 Prec@1=62.762 Prec@5=84.558 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=02:36 IST
=> Training   32.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.400 Loss=1.558 Prec@1=62.713 Prec@5=84.498 rate=1.78 Hz, eta=0:15:58, total=0:07:30, wall=02:36 IST
=> Training   36.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.400 Loss=1.558 Prec@1=62.713 Prec@5=84.498 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=02:36 IST
=> Training   36.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.570 DataTime=0.400 Loss=1.558 Prec@1=62.713 Prec@5=84.498 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=02:37 IST
=> Training   36.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.561 Prec@1=62.649 Prec@5=84.452 rate=1.77 Hz, eta=0:15:03, total=0:08:28, wall=02:37 IST
=> Training   39.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.561 Prec@1=62.649 Prec@5=84.452 rate=1.77 Hz, eta=0:14:07, total=0:09:24, wall=02:37 IST
=> Training   39.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.561 Prec@1=62.649 Prec@5=84.452 rate=1.77 Hz, eta=0:14:07, total=0:09:24, wall=02:38 IST
=> Training   39.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.564 Prec@1=62.582 Prec@5=84.404 rate=1.77 Hz, eta=0:14:07, total=0:09:24, wall=02:38 IST
=> Training   43.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.564 Prec@1=62.582 Prec@5=84.404 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=02:38 IST
=> Training   43.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.569 DataTime=0.400 Loss=1.564 Prec@1=62.582 Prec@5=84.404 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=02:39 IST
=> Training   43.99% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.565 Prec@1=62.545 Prec@5=84.383 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=02:39 IST
=> Training   47.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.565 Prec@1=62.545 Prec@5=84.383 rate=1.77 Hz, eta=0:12:13, total=0:11:16, wall=02:39 IST
=> Training   47.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.565 Prec@1=62.545 Prec@5=84.383 rate=1.77 Hz, eta=0:12:13, total=0:11:16, wall=02:40 IST
=> Training   47.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.567 Prec@1=62.512 Prec@5=84.362 rate=1.77 Hz, eta=0:12:13, total=0:11:16, wall=02:40 IST
=> Training   51.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.567 Prec@1=62.512 Prec@5=84.362 rate=1.77 Hz, eta=0:11:18, total=0:12:14, wall=02:40 IST
=> Training   51.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.398 Loss=1.567 Prec@1=62.512 Prec@5=84.362 rate=1.77 Hz, eta=0:11:18, total=0:12:14, wall=02:41 IST
=> Training   51.98% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.568 Prec@1=62.493 Prec@5=84.342 rate=1.77 Hz, eta=0:11:18, total=0:12:14, wall=02:41 IST
=> Training   55.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.568 Prec@1=62.493 Prec@5=84.342 rate=1.77 Hz, eta=0:10:21, total=0:13:09, wall=02:41 IST
=> Training   55.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.568 Prec@1=62.493 Prec@5=84.342 rate=1.77 Hz, eta=0:10:21, total=0:13:09, wall=02:42 IST
=> Training   55.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.569 Prec@1=62.486 Prec@5=84.320 rate=1.77 Hz, eta=0:10:21, total=0:13:09, wall=02:42 IST
=> Training   59.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.569 Prec@1=62.486 Prec@5=84.320 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=02:42 IST
=> Training   59.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.568 DataTime=0.397 Loss=1.569 Prec@1=62.486 Prec@5=84.320 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=02:43 IST
=> Training   59.97% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.570 Prec@1=62.461 Prec@5=84.308 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=02:43 IST
=> Training   63.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.570 Prec@1=62.461 Prec@5=84.308 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:43 IST
=> Training   63.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.570 Prec@1=62.461 Prec@5=84.308 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:44 IST
=> Training   63.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.572 Prec@1=62.400 Prec@5=84.269 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:44 IST
=> Training   67.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.572 Prec@1=62.400 Prec@5=84.269 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:44 IST
=> Training   67.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.572 Prec@1=62.400 Prec@5=84.269 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:45 IST
=> Training   67.96% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.574 Prec@1=62.375 Prec@5=84.245 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:45 IST
=> Training   71.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.574 Prec@1=62.375 Prec@5=84.245 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=02:45 IST
=> Training   71.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.574 Prec@1=62.375 Prec@5=84.245 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=02:45 IST
=> Training   71.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.575 Prec@1=62.355 Prec@5=84.236 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=02:45 IST
=> Training   75.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.575 Prec@1=62.355 Prec@5=84.236 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:45 IST
=> Training   75.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.567 DataTime=0.396 Loss=1.575 Prec@1=62.355 Prec@5=84.236 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:46 IST
=> Training   75.95% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.576 Prec@1=62.337 Prec@5=84.219 rate=1.77 Hz, eta=0:05:39, total=0:17:52, wall=02:46 IST
=> Training   79.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.576 Prec@1=62.337 Prec@5=84.219 rate=1.78 Hz, eta=0:04:42, total=0:18:46, wall=02:46 IST
=> Training   79.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.576 Prec@1=62.337 Prec@5=84.219 rate=1.78 Hz, eta=0:04:42, total=0:18:46, wall=02:47 IST
=> Training   79.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.395 Loss=1.576 Prec@1=62.314 Prec@5=84.207 rate=1.78 Hz, eta=0:04:42, total=0:18:46, wall=02:47 IST
=> Training   83.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.395 Loss=1.576 Prec@1=62.314 Prec@5=84.207 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:47 IST
=> Training   83.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.395 Loss=1.576 Prec@1=62.314 Prec@5=84.207 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:48 IST
=> Training   83.94% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.577 Prec@1=62.309 Prec@5=84.204 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=02:48 IST
=> Training   87.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.577 Prec@1=62.309 Prec@5=84.204 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=02:48 IST
=> Training   87.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.566 DataTime=0.394 Loss=1.577 Prec@1=62.309 Prec@5=84.204 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=02:49 IST
=> Training   87.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.565 DataTime=0.394 Loss=1.578 Prec@1=62.301 Prec@5=84.195 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=02:49 IST
=> Training   91.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.565 DataTime=0.394 Loss=1.578 Prec@1=62.301 Prec@5=84.195 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:49 IST
=> Training   91.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.565 DataTime=0.394 Loss=1.578 Prec@1=62.301 Prec@5=84.195 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:50 IST
=> Training   91.93% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.281 Prec@5=84.178 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=02:50 IST
=> Training   95.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.281 Prec@5=84.178 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=02:50 IST
=> Training   95.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.281 Prec@5=84.178 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=02:51 IST
=> Training   95.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.270 Prec@5=84.174 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=02:51 IST
=> Training   99.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.270 Prec@5=84.174 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:51 IST
=> Training   99.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.270 Prec@5=84.174 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:51 IST
=> Training   99.92% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.268 Prec@5=84.173 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=02:51 IST
=> Training   100.00% of 1x2503...Epoch=28/150 LR=0.0922 Time=0.564 DataTime=0.393 Loss=1.579 Prec@1=62.268 Prec@5=84.173 rate=1.78 Hz, eta=0:00:00, total=0:23:25, wall=02:51 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> Validation 0.00% of 1x98...Epoch=28/150 LR=0.0922 Time=7.628 Loss=1.145 Prec@1=70.508 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=02:51 IST
=> Validation 1.02% of 1x98...Epoch=28/150 LR=0.0922 Time=7.628 Loss=1.145 Prec@1=70.508 Prec@5=91.016 rate=6937.89 Hz, eta=0:00:00, total=0:00:00, wall=02:51 IST
** Validation 1.02% of 1x98...Epoch=28/150 LR=0.0922 Time=7.628 Loss=1.145 Prec@1=70.508 Prec@5=91.016 rate=6937.89 Hz, eta=0:00:00, total=0:00:00, wall=02:52 IST
** Validation 1.02% of 1x98...Epoch=28/150 LR=0.0922 Time=0.638 Loss=1.645 Prec@1=60.634 Prec@5=83.516 rate=6937.89 Hz, eta=0:00:00, total=0:00:00, wall=02:52 IST
** Validation 100.00% of 1x98...Epoch=28/150 LR=0.0922 Time=0.638 Loss=1.645 Prec@1=60.634 Prec@5=83.516 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=02:52 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> Training   0.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=5.460 DataTime=5.261 Loss=1.509 Prec@1=62.109 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=02:52 IST
=> Training   0.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=5.460 DataTime=5.261 Loss=1.509 Prec@1=62.109 Prec@5=85.156 rate=1705.21 Hz, eta=0:00:01, total=0:00:00, wall=02:52 IST
=> Training   0.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=5.460 DataTime=5.261 Loss=1.509 Prec@1=62.109 Prec@5=85.156 rate=1705.21 Hz, eta=0:00:01, total=0:00:00, wall=02:53 IST
=> Training   0.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.578 DataTime=0.415 Loss=1.527 Prec@1=63.297 Prec@5=84.971 rate=1705.21 Hz, eta=0:00:01, total=0:00:00, wall=02:53 IST
=> Training   4.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.578 DataTime=0.415 Loss=1.527 Prec@1=63.297 Prec@5=84.971 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=02:53 IST
=> Training   4.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.578 DataTime=0.415 Loss=1.527 Prec@1=63.297 Prec@5=84.971 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=02:54 IST
=> Training   4.04% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.575 DataTime=0.408 Loss=1.533 Prec@1=63.275 Prec@5=84.804 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=02:54 IST
=> Training   8.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.575 DataTime=0.408 Loss=1.533 Prec@1=63.275 Prec@5=84.804 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=02:54 IST
=> Training   8.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.575 DataTime=0.408 Loss=1.533 Prec@1=63.275 Prec@5=84.804 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=02:55 IST
=> Training   8.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.403 Loss=1.539 Prec@1=63.177 Prec@5=84.710 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=02:55 IST
=> Training   12.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.403 Loss=1.539 Prec@1=63.177 Prec@5=84.710 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:55 IST
=> Training   12.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.403 Loss=1.539 Prec@1=63.177 Prec@5=84.710 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:56 IST
=> Training   12.03% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.402 Loss=1.544 Prec@1=63.104 Prec@5=84.639 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=02:56 IST
=> Training   16.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.402 Loss=1.544 Prec@1=63.104 Prec@5=84.639 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=02:56 IST
=> Training   16.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.573 DataTime=0.402 Loss=1.544 Prec@1=63.104 Prec@5=84.639 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=02:57 IST
=> Training   16.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.569 DataTime=0.397 Loss=1.546 Prec@1=63.028 Prec@5=84.612 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=02:57 IST
=> Training   20.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.569 DataTime=0.397 Loss=1.546 Prec@1=63.028 Prec@5=84.612 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=02:57 IST
=> Training   20.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.569 DataTime=0.397 Loss=1.546 Prec@1=63.028 Prec@5=84.612 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=02:58 IST
=> Training   20.02% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.567 DataTime=0.394 Loss=1.547 Prec@1=62.994 Prec@5=84.640 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=02:58 IST
=> Training   24.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.567 DataTime=0.394 Loss=1.547 Prec@1=62.994 Prec@5=84.640 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=02:58 IST
=> Training   24.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.567 DataTime=0.394 Loss=1.547 Prec@1=62.994 Prec@5=84.640 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=02:59 IST
=> Training   24.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.550 Prec@1=62.895 Prec@5=84.565 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=02:59 IST
=> Training   28.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.550 Prec@1=62.895 Prec@5=84.565 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=02:59 IST
=> Training   28.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.550 Prec@1=62.895 Prec@5=84.565 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=03:00 IST
=> Training   28.01% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.550 Prec@1=62.917 Prec@5=84.566 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=03:00 IST
=> Training   32.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.550 Prec@1=62.917 Prec@5=84.566 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=03:00 IST
=> Training   32.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.550 Prec@1=62.917 Prec@5=84.566 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=03:01 IST
=> Training   32.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.389 Loss=1.551 Prec@1=62.904 Prec@5=84.558 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=03:01 IST
=> Training   36.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.389 Loss=1.551 Prec@1=62.904 Prec@5=84.558 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:01 IST
=> Training   36.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.389 Loss=1.551 Prec@1=62.904 Prec@5=84.558 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:01 IST
=> Training   36.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.389 Loss=1.553 Prec@1=62.876 Prec@5=84.543 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:01 IST
=> Training   39.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.389 Loss=1.553 Prec@1=62.876 Prec@5=84.543 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:01 IST
=> Training   39.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.389 Loss=1.553 Prec@1=62.876 Prec@5=84.543 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:02 IST
=> Training   39.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.556 Prec@1=62.802 Prec@5=84.498 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:02 IST
=> Training   43.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.556 Prec@1=62.802 Prec@5=84.498 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=03:02 IST
=> Training   43.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.556 Prec@1=62.802 Prec@5=84.498 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=03:03 IST
=> Training   43.99% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.774 Prec@5=84.484 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=03:03 IST
=> Training   47.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.774 Prec@5=84.484 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:03 IST
=> Training   47.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.774 Prec@5=84.484 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:04 IST
=> Training   47.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.757 Prec@5=84.466 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:04 IST
=> Training   51.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.757 Prec@5=84.466 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:04 IST
=> Training   51.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.564 DataTime=0.390 Loss=1.557 Prec@1=62.757 Prec@5=84.466 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:05 IST
=> Training   51.98% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.560 Prec@1=62.716 Prec@5=84.421 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:05 IST
=> Training   55.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.560 Prec@1=62.716 Prec@5=84.421 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=03:05 IST
=> Training   55.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.391 Loss=1.560 Prec@1=62.716 Prec@5=84.421 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=03:06 IST
=> Training   55.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.566 DataTime=0.391 Loss=1.561 Prec@1=62.698 Prec@5=84.385 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=03:06 IST
=> Training   59.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.566 DataTime=0.391 Loss=1.561 Prec@1=62.698 Prec@5=84.385 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=03:06 IST
=> Training   59.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.566 DataTime=0.391 Loss=1.561 Prec@1=62.698 Prec@5=84.385 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=03:07 IST
=> Training   59.97% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.562 Prec@1=62.690 Prec@5=84.360 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=03:07 IST
=> Training   63.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.562 Prec@1=62.690 Prec@5=84.360 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=03:07 IST
=> Training   63.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.562 Prec@1=62.690 Prec@5=84.360 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=03:08 IST
=> Training   63.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.563 Prec@1=62.653 Prec@5=84.341 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=03:08 IST
=> Training   67.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.563 Prec@1=62.653 Prec@5=84.341 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=03:08 IST
=> Training   67.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.565 DataTime=0.390 Loss=1.563 Prec@1=62.653 Prec@5=84.341 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=03:09 IST
=> Training   67.96% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.564 Prec@1=62.622 Prec@5=84.335 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=03:09 IST
=> Training   71.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.564 Prec@1=62.622 Prec@5=84.335 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:09 IST
=> Training   71.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.564 Prec@1=62.622 Prec@5=84.335 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:10 IST
=> Training   71.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.566 Prec@1=62.594 Prec@5=84.309 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:10 IST
=> Training   75.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.566 Prec@1=62.594 Prec@5=84.309 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=03:10 IST
=> Training   75.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.566 Prec@1=62.594 Prec@5=84.309 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=03:11 IST
=> Training   75.95% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.567 Prec@1=62.576 Prec@5=84.304 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=03:11 IST
=> Training   79.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.567 Prec@1=62.576 Prec@5=84.304 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:11 IST
=> Training   79.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.567 Prec@1=62.576 Prec@5=84.304 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:12 IST
=> Training   79.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.567 Prec@1=62.576 Prec@5=84.296 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:12 IST
=> Training   83.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.567 Prec@1=62.576 Prec@5=84.296 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:12 IST
=> Training   83.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.563 DataTime=0.388 Loss=1.567 Prec@1=62.576 Prec@5=84.296 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:13 IST
=> Training   83.94% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.568 Prec@1=62.548 Prec@5=84.283 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:13 IST
=> Training   87.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.568 Prec@1=62.548 Prec@5=84.283 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=03:13 IST
=> Training   87.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.568 Prec@1=62.548 Prec@5=84.283 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=03:14 IST
=> Training   87.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.570 Prec@1=62.516 Prec@5=84.264 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=03:14 IST
=> Training   91.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.570 Prec@1=62.516 Prec@5=84.264 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:14 IST
=> Training   91.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.570 Prec@1=62.516 Prec@5=84.264 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:15 IST
=> Training   91.93% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.571 Prec@1=62.487 Prec@5=84.251 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:15 IST
=> Training   95.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.571 Prec@1=62.487 Prec@5=84.251 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=03:15 IST
=> Training   95.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.387 Loss=1.571 Prec@1=62.487 Prec@5=84.251 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=03:16 IST
=> Training   95.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.388 Loss=1.571 Prec@1=62.472 Prec@5=84.240 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=03:16 IST
=> Training   99.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.388 Loss=1.571 Prec@1=62.472 Prec@5=84.240 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:16 IST
=> Training   99.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.562 DataTime=0.388 Loss=1.571 Prec@1=62.472 Prec@5=84.240 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:16 IST
=> Training   99.92% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.561 DataTime=0.387 Loss=1.571 Prec@1=62.471 Prec@5=84.240 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:16 IST
=> Training   100.00% of 1x2503...Epoch=29/150 LR=0.0916 Time=0.561 DataTime=0.387 Loss=1.571 Prec@1=62.471 Prec@5=84.240 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=03:16 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:16 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:16 IST
=> Validation 0.00% of 1x98...Epoch=29/150 LR=0.0916 Time=6.966 Loss=1.147 Prec@1=71.875 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=03:16 IST
=> Validation 1.02% of 1x98...Epoch=29/150 LR=0.0916 Time=6.966 Loss=1.147 Prec@1=71.875 Prec@5=90.430 rate=6997.66 Hz, eta=0:00:00, total=0:00:00, wall=03:16 IST
** Validation 1.02% of 1x98...Epoch=29/150 LR=0.0916 Time=6.966 Loss=1.147 Prec@1=71.875 Prec@5=90.430 rate=6997.66 Hz, eta=0:00:00, total=0:00:00, wall=03:17 IST
** Validation 1.02% of 1x98...Epoch=29/150 LR=0.0916 Time=0.636 Loss=1.631 Prec@1=61.072 Prec@5=83.820 rate=6997.66 Hz, eta=0:00:00, total=0:00:00, wall=03:17 IST
** Validation 100.00% of 1x98...Epoch=29/150 LR=0.0916 Time=0.636 Loss=1.631 Prec@1=61.072 Prec@5=83.820 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=03:17 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:17 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:17 IST
=> Training   0.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=4.420 DataTime=4.045 Loss=1.527 Prec@1=64.258 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=03:17 IST
=> Training   0.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=4.420 DataTime=4.045 Loss=1.527 Prec@1=64.258 Prec@5=84.961 rate=14.18 Hz, eta=0:02:56, total=0:00:00, wall=03:17 IST
=> Training   0.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=4.420 DataTime=4.045 Loss=1.527 Prec@1=64.258 Prec@5=84.961 rate=14.18 Hz, eta=0:02:56, total=0:00:00, wall=03:18 IST
=> Training   0.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.588 DataTime=0.416 Loss=1.529 Prec@1=63.314 Prec@5=84.951 rate=14.18 Hz, eta=0:02:56, total=0:00:00, wall=03:18 IST
=> Training   4.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.588 DataTime=0.416 Loss=1.529 Prec@1=63.314 Prec@5=84.951 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=03:18 IST
=> Training   4.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.588 DataTime=0.416 Loss=1.529 Prec@1=63.314 Prec@5=84.951 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=03:18 IST
=> Training   4.04% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.571 DataTime=0.402 Loss=1.515 Prec@1=63.585 Prec@5=85.147 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=03:18 IST
=> Training   8.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.571 DataTime=0.402 Loss=1.515 Prec@1=63.585 Prec@5=85.147 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=03:18 IST
=> Training   8.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.571 DataTime=0.402 Loss=1.515 Prec@1=63.585 Prec@5=85.147 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=03:19 IST
=> Training   8.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.573 DataTime=0.402 Loss=1.522 Prec@1=63.458 Prec@5=85.029 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=03:19 IST
=> Training   12.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.573 DataTime=0.402 Loss=1.522 Prec@1=63.458 Prec@5=85.029 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=03:19 IST
=> Training   12.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.573 DataTime=0.402 Loss=1.522 Prec@1=63.458 Prec@5=85.029 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=03:20 IST
=> Training   12.03% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.568 DataTime=0.398 Loss=1.523 Prec@1=63.450 Prec@5=84.990 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=03:20 IST
=> Training   16.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.568 DataTime=0.398 Loss=1.523 Prec@1=63.450 Prec@5=84.990 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=03:20 IST
=> Training   16.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.568 DataTime=0.398 Loss=1.523 Prec@1=63.450 Prec@5=84.990 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=03:21 IST
=> Training   16.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.567 DataTime=0.397 Loss=1.526 Prec@1=63.385 Prec@5=84.924 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=03:21 IST
=> Training   20.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.567 DataTime=0.397 Loss=1.526 Prec@1=63.385 Prec@5=84.924 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=03:21 IST
=> Training   20.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.567 DataTime=0.397 Loss=1.526 Prec@1=63.385 Prec@5=84.924 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=03:22 IST
=> Training   20.02% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.566 DataTime=0.396 Loss=1.529 Prec@1=63.321 Prec@5=84.880 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=03:22 IST
=> Training   24.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.566 DataTime=0.396 Loss=1.529 Prec@1=63.321 Prec@5=84.880 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=03:22 IST
=> Training   24.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.566 DataTime=0.396 Loss=1.529 Prec@1=63.321 Prec@5=84.880 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=03:23 IST
=> Training   24.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.564 DataTime=0.393 Loss=1.531 Prec@1=63.261 Prec@5=84.828 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=03:23 IST
=> Training   28.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.564 DataTime=0.393 Loss=1.531 Prec@1=63.261 Prec@5=84.828 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=03:23 IST
=> Training   28.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.564 DataTime=0.393 Loss=1.531 Prec@1=63.261 Prec@5=84.828 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=03:24 IST
=> Training   28.01% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.391 Loss=1.534 Prec@1=63.216 Prec@5=84.814 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=03:24 IST
=> Training   32.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.391 Loss=1.534 Prec@1=63.216 Prec@5=84.814 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:24 IST
=> Training   32.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.391 Loss=1.534 Prec@1=63.216 Prec@5=84.814 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:25 IST
=> Training   32.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.392 Loss=1.539 Prec@1=63.108 Prec@5=84.753 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:25 IST
=> Training   36.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.392 Loss=1.539 Prec@1=63.108 Prec@5=84.753 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=03:25 IST
=> Training   36.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.563 DataTime=0.392 Loss=1.539 Prec@1=63.108 Prec@5=84.753 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=03:26 IST
=> Training   36.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.541 Prec@1=63.024 Prec@5=84.736 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=03:26 IST
=> Training   39.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.541 Prec@1=63.024 Prec@5=84.736 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=03:26 IST
=> Training   39.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.541 Prec@1=63.024 Prec@5=84.736 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=03:27 IST
=> Training   39.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.543 Prec@1=62.983 Prec@5=84.713 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=03:27 IST
=> Training   43.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.543 Prec@1=62.983 Prec@5=84.713 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:27 IST
=> Training   43.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.543 Prec@1=62.983 Prec@5=84.713 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:28 IST
=> Training   43.99% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.545 Prec@1=62.939 Prec@5=84.684 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:28 IST
=> Training   47.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.545 Prec@1=62.939 Prec@5=84.684 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=03:28 IST
=> Training   47.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.545 Prec@1=62.939 Prec@5=84.684 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=03:29 IST
=> Training   47.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.547 Prec@1=62.901 Prec@5=84.645 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=03:29 IST
=> Training   51.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.547 Prec@1=62.901 Prec@5=84.645 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=03:29 IST
=> Training   51.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.391 Loss=1.547 Prec@1=62.901 Prec@5=84.645 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=03:30 IST
=> Training   51.98% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.550 Prec@1=62.875 Prec@5=84.597 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=03:30 IST
=> Training   55.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.550 Prec@1=62.875 Prec@5=84.597 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:30 IST
=> Training   55.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.550 Prec@1=62.875 Prec@5=84.597 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:31 IST
=> Training   55.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.390 Loss=1.550 Prec@1=62.881 Prec@5=84.576 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:31 IST
=> Training   59.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.390 Loss=1.550 Prec@1=62.881 Prec@5=84.576 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:31 IST
=> Training   59.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.562 DataTime=0.390 Loss=1.550 Prec@1=62.881 Prec@5=84.576 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:32 IST
=> Training   59.97% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.552 Prec@1=62.852 Prec@5=84.555 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:32 IST
=> Training   63.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.552 Prec@1=62.852 Prec@5=84.555 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=03:32 IST
=> Training   63.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.552 Prec@1=62.852 Prec@5=84.555 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=03:32 IST
=> Training   63.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.553 Prec@1=62.828 Prec@5=84.549 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=03:32 IST
=> Training   67.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.553 Prec@1=62.828 Prec@5=84.549 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=03:32 IST
=> Training   67.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.561 DataTime=0.390 Loss=1.553 Prec@1=62.828 Prec@5=84.549 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=03:33 IST
=> Training   67.96% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.553 Prec@1=62.817 Prec@5=84.542 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=03:33 IST
=> Training   71.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.553 Prec@1=62.817 Prec@5=84.542 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:33 IST
=> Training   71.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.553 Prec@1=62.817 Prec@5=84.542 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:34 IST
=> Training   71.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.554 Prec@1=62.794 Prec@5=84.525 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:34 IST
=> Training   75.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.554 Prec@1=62.794 Prec@5=84.525 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=03:34 IST
=> Training   75.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.554 Prec@1=62.794 Prec@5=84.525 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=03:35 IST
=> Training   75.95% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.555 Prec@1=62.788 Prec@5=84.515 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=03:35 IST
=> Training   79.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.555 Prec@1=62.788 Prec@5=84.515 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:35 IST
=> Training   79.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.555 Prec@1=62.788 Prec@5=84.515 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:36 IST
=> Training   79.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.556 Prec@1=62.757 Prec@5=84.493 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:36 IST
=> Training   83.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.556 Prec@1=62.757 Prec@5=84.493 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:36 IST
=> Training   83.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.556 Prec@1=62.757 Prec@5=84.493 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:37 IST
=> Training   83.94% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.557 Prec@1=62.731 Prec@5=84.480 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:37 IST
=> Training   87.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.557 Prec@1=62.731 Prec@5=84.480 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:37 IST
=> Training   87.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.557 Prec@1=62.731 Prec@5=84.480 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:38 IST
=> Training   87.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.559 Prec@1=62.704 Prec@5=84.456 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:38 IST
=> Training   91.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.559 Prec@1=62.704 Prec@5=84.456 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:38 IST
=> Training   91.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.559 Prec@1=62.704 Prec@5=84.456 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:39 IST
=> Training   91.93% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.559 Prec@1=62.683 Prec@5=84.451 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:39 IST
=> Training   95.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.559 Prec@1=62.683 Prec@5=84.451 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:39 IST
=> Training   95.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.390 Loss=1.559 Prec@1=62.683 Prec@5=84.451 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:40 IST
=> Training   95.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.560 Prec@1=62.666 Prec@5=84.443 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:40 IST
=> Training   99.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.560 Prec@1=62.666 Prec@5=84.443 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=03:40 IST
=> Training   99.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.560 Prec@1=62.666 Prec@5=84.443 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=03:40 IST
=> Training   99.92% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.560 Prec@1=62.667 Prec@5=84.443 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=03:40 IST
=> Training   100.00% of 1x2503...Epoch=30/150 LR=0.0911 Time=0.560 DataTime=0.389 Loss=1.560 Prec@1=62.667 Prec@5=84.443 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=03:40 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:40 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:40 IST
=> Validation 0.00% of 1x98...Epoch=30/150 LR=0.0911 Time=7.136 Loss=1.026 Prec@1=71.094 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=03:40 IST
=> Validation 1.02% of 1x98...Epoch=30/150 LR=0.0911 Time=7.136 Loss=1.026 Prec@1=71.094 Prec@5=91.406 rate=5137.27 Hz, eta=0:00:00, total=0:00:00, wall=03:40 IST
** Validation 1.02% of 1x98...Epoch=30/150 LR=0.0911 Time=7.136 Loss=1.026 Prec@1=71.094 Prec@5=91.406 rate=5137.27 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST
** Validation 1.02% of 1x98...Epoch=30/150 LR=0.0911 Time=0.630 Loss=1.595 Prec@1=61.912 Prec@5=84.300 rate=5137.27 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST
** Validation 100.00% of 1x98...Epoch=30/150 LR=0.0911 Time=0.630 Loss=1.595 Prec@1=61.912 Prec@5=84.300 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=03:41 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST
=> Training   0.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=5.114 DataTime=4.751 Loss=1.613 Prec@1=61.719 Prec@5=83.203 rate=0 Hz, eta=?, total=0:00:00, wall=03:41 IST
=> Training   0.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=5.114 DataTime=4.751 Loss=1.613 Prec@1=61.719 Prec@5=83.203 rate=9193.20 Hz, eta=0:00:00, total=0:00:00, wall=03:41 IST
=> Training   0.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=5.114 DataTime=4.751 Loss=1.613 Prec@1=61.719 Prec@5=83.203 rate=9193.20 Hz, eta=0:00:00, total=0:00:00, wall=03:42 IST
=> Training   0.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.573 DataTime=0.407 Loss=1.517 Prec@1=63.598 Prec@5=85.212 rate=9193.20 Hz, eta=0:00:00, total=0:00:00, wall=03:42 IST
=> Training   4.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.573 DataTime=0.407 Loss=1.517 Prec@1=63.598 Prec@5=85.212 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=03:42 IST
=> Training   4.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.573 DataTime=0.407 Loss=1.517 Prec@1=63.598 Prec@5=85.212 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=03:43 IST
=> Training   4.04% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.399 Loss=1.513 Prec@1=63.696 Prec@5=85.173 rate=1.91 Hz, eta=0:20:57, total=0:00:52, wall=03:43 IST
=> Training   8.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.399 Loss=1.513 Prec@1=63.696 Prec@5=85.173 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=03:43 IST
=> Training   8.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.399 Loss=1.513 Prec@1=63.696 Prec@5=85.173 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=03:44 IST
=> Training   8.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.396 Loss=1.515 Prec@1=63.560 Prec@5=85.104 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=03:44 IST
=> Training   12.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.396 Loss=1.515 Prec@1=63.560 Prec@5=85.104 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=03:44 IST
=> Training   12.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.396 Loss=1.515 Prec@1=63.560 Prec@5=85.104 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=03:45 IST
=> Training   12.03% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.395 Loss=1.521 Prec@1=63.482 Prec@5=85.015 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=03:45 IST
=> Training   16.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.395 Loss=1.521 Prec@1=63.482 Prec@5=85.015 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=03:45 IST
=> Training   16.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.566 DataTime=0.395 Loss=1.521 Prec@1=63.482 Prec@5=85.015 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=03:46 IST
=> Training   16.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.392 Loss=1.523 Prec@1=63.428 Prec@5=85.008 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=03:46 IST
=> Training   20.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.392 Loss=1.523 Prec@1=63.428 Prec@5=85.008 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=03:46 IST
=> Training   20.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.392 Loss=1.523 Prec@1=63.428 Prec@5=85.008 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=03:47 IST
=> Training   20.02% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.392 Loss=1.527 Prec@1=63.360 Prec@5=84.937 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=03:47 IST
=> Training   24.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.392 Loss=1.527 Prec@1=63.360 Prec@5=84.937 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=03:47 IST
=> Training   24.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.392 Loss=1.527 Prec@1=63.360 Prec@5=84.937 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=03:48 IST
=> Training   24.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.391 Loss=1.530 Prec@1=63.299 Prec@5=84.878 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=03:48 IST
=> Training   28.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.391 Loss=1.530 Prec@1=63.299 Prec@5=84.878 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=03:48 IST
=> Training   28.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.562 DataTime=0.391 Loss=1.530 Prec@1=63.299 Prec@5=84.878 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=03:48 IST
=> Training   28.01% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.393 Loss=1.531 Prec@1=63.291 Prec@5=84.885 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=03:48 IST
=> Training   32.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.393 Loss=1.531 Prec@1=63.291 Prec@5=84.885 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=03:48 IST
=> Training   32.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.563 DataTime=0.393 Loss=1.531 Prec@1=63.291 Prec@5=84.885 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=03:49 IST
=> Training   32.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.532 Prec@1=63.285 Prec@5=84.855 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=03:49 IST
=> Training   36.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.532 Prec@1=63.285 Prec@5=84.855 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=03:49 IST
=> Training   36.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.532 Prec@1=63.285 Prec@5=84.855 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=03:50 IST
=> Training   36.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.534 Prec@1=63.269 Prec@5=84.823 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=03:50 IST
=> Training   39.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.534 Prec@1=63.269 Prec@5=84.823 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=03:50 IST
=> Training   39.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.390 Loss=1.534 Prec@1=63.269 Prec@5=84.823 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=03:51 IST
=> Training   39.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.215 Prec@5=84.768 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=03:51 IST
=> Training   43.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.215 Prec@5=84.768 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=03:51 IST
=> Training   43.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.215 Prec@5=84.768 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=03:52 IST
=> Training   43.99% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.537 Prec@1=63.176 Prec@5=84.766 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=03:52 IST
=> Training   47.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.537 Prec@1=63.176 Prec@5=84.766 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=03:52 IST
=> Training   47.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.537 Prec@1=63.176 Prec@5=84.766 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=03:53 IST
=> Training   47.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.540 Prec@1=63.113 Prec@5=84.737 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=03:53 IST
=> Training   51.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.540 Prec@1=63.113 Prec@5=84.737 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=03:53 IST
=> Training   51.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.540 Prec@1=63.113 Prec@5=84.737 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=03:54 IST
=> Training   51.98% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.542 Prec@1=63.076 Prec@5=84.705 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=03:54 IST
=> Training   55.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.542 Prec@1=63.076 Prec@5=84.705 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=03:54 IST
=> Training   55.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.389 Loss=1.542 Prec@1=63.076 Prec@5=84.705 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=03:55 IST
=> Training   55.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.390 Loss=1.543 Prec@1=63.062 Prec@5=84.700 rate=1.80 Hz, eta=0:10:12, total=0:12:58, wall=03:55 IST
=> Training   59.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.390 Loss=1.543 Prec@1=63.062 Prec@5=84.700 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=03:55 IST
=> Training   59.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.560 DataTime=0.390 Loss=1.543 Prec@1=63.062 Prec@5=84.700 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=03:56 IST
=> Training   59.97% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.543 Prec@1=63.050 Prec@5=84.680 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=03:56 IST
=> Training   63.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.543 Prec@1=63.050 Prec@5=84.680 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=03:56 IST
=> Training   63.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.543 Prec@1=63.050 Prec@5=84.680 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=03:57 IST
=> Training   63.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.544 Prec@1=63.029 Prec@5=84.670 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=03:57 IST
=> Training   67.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.544 Prec@1=63.029 Prec@5=84.670 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=03:57 IST
=> Training   67.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.544 Prec@1=63.029 Prec@5=84.670 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=03:58 IST
=> Training   67.96% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.546 Prec@1=62.984 Prec@5=84.644 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=03:58 IST
=> Training   71.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.546 Prec@1=62.984 Prec@5=84.644 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=03:58 IST
=> Training   71.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.546 Prec@1=62.984 Prec@5=84.644 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=03:59 IST
=> Training   71.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.548 Prec@1=62.953 Prec@5=84.613 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=03:59 IST
=> Training   75.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.548 Prec@1=62.953 Prec@5=84.613 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=03:59 IST
=> Training   75.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.548 Prec@1=62.953 Prec@5=84.613 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=04:00 IST
=> Training   75.95% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.549 Prec@1=62.938 Prec@5=84.604 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=04:00 IST
=> Training   79.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.549 Prec@1=62.938 Prec@5=84.604 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=04:00 IST
=> Training   79.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.388 Loss=1.549 Prec@1=62.938 Prec@5=84.604 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=04:01 IST
=> Training   79.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.550 Prec@1=62.916 Prec@5=84.583 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=04:01 IST
=> Training   83.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.550 Prec@1=62.916 Prec@5=84.583 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=04:01 IST
=> Training   83.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.550 Prec@1=62.916 Prec@5=84.583 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=04:01 IST
=> Training   83.94% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.551 Prec@1=62.890 Prec@5=84.571 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=04:01 IST
=> Training   87.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.551 Prec@1=62.890 Prec@5=84.571 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=04:01 IST
=> Training   87.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.551 Prec@1=62.890 Prec@5=84.571 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=04:02 IST
=> Training   87.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.552 Prec@1=62.868 Prec@5=84.549 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=04:02 IST
=> Training   91.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.552 Prec@1=62.868 Prec@5=84.549 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=04:02 IST
=> Training   91.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.559 DataTime=0.389 Loss=1.552 Prec@1=62.868 Prec@5=84.549 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=04:03 IST
=> Training   91.93% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.388 Loss=1.553 Prec@1=62.835 Prec@5=84.531 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=04:03 IST
=> Training   95.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.388 Loss=1.553 Prec@1=62.835 Prec@5=84.531 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:03 IST
=> Training   95.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.388 Loss=1.553 Prec@1=62.835 Prec@5=84.531 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:04 IST
=> Training   95.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.387 Loss=1.553 Prec@1=62.834 Prec@5=84.534 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:04 IST
=> Training   99.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.387 Loss=1.553 Prec@1=62.834 Prec@5=84.534 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=04:04 IST
=> Training   99.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.558 DataTime=0.387 Loss=1.553 Prec@1=62.834 Prec@5=84.534 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=04:04 IST
=> Training   99.92% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.557 DataTime=0.386 Loss=1.553 Prec@1=62.834 Prec@5=84.534 rate=1.80 Hz, eta=0:00:01, total=0:23:09, wall=04:04 IST
=> Training   100.00% of 1x2503...Epoch=31/150 LR=0.0905 Time=0.557 DataTime=0.386 Loss=1.553 Prec@1=62.834 Prec@5=84.534 rate=1.80 Hz, eta=0:00:00, total=0:23:09, wall=04:04 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST
=> Validation 0.00% of 1x98...Epoch=31/150 LR=0.0905 Time=7.197 Loss=1.052 Prec@1=73.242 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=04:04 IST
=> Validation 1.02% of 1x98...Epoch=31/150 LR=0.0905 Time=7.197 Loss=1.052 Prec@1=73.242 Prec@5=90.820 rate=6758.54 Hz, eta=0:00:00, total=0:00:00, wall=04:04 IST
** Validation 1.02% of 1x98...Epoch=31/150 LR=0.0905 Time=7.197 Loss=1.052 Prec@1=73.242 Prec@5=90.820 rate=6758.54 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST
** Validation 1.02% of 1x98...Epoch=31/150 LR=0.0905 Time=0.640 Loss=1.607 Prec@1=61.734 Prec@5=84.190 rate=6758.54 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST
** Validation 100.00% of 1x98...Epoch=31/150 LR=0.0905 Time=0.640 Loss=1.607 Prec@1=61.734 Prec@5=84.190 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=04:05 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST
=> Training   0.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=5.125 DataTime=4.904 Loss=1.345 Prec@1=66.797 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=04:05 IST
=> Training   0.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=5.125 DataTime=4.904 Loss=1.345 Prec@1=66.797 Prec@5=86.719 rate=3371.86 Hz, eta=0:00:00, total=0:00:00, wall=04:05 IST
=> Training   0.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=5.125 DataTime=4.904 Loss=1.345 Prec@1=66.797 Prec@5=86.719 rate=3371.86 Hz, eta=0:00:00, total=0:00:00, wall=04:06 IST
=> Training   0.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.585 DataTime=0.427 Loss=1.517 Prec@1=63.289 Prec@5=84.986 rate=3371.86 Hz, eta=0:00:00, total=0:00:00, wall=04:06 IST
=> Training   4.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.585 DataTime=0.427 Loss=1.517 Prec@1=63.289 Prec@5=84.986 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:06 IST
=> Training   4.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.585 DataTime=0.427 Loss=1.517 Prec@1=63.289 Prec@5=84.986 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:07 IST
=> Training   4.04% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.576 DataTime=0.416 Loss=1.513 Prec@1=63.547 Prec@5=85.063 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:07 IST
=> Training   8.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.576 DataTime=0.416 Loss=1.513 Prec@1=63.547 Prec@5=85.063 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=04:07 IST
=> Training   8.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.576 DataTime=0.416 Loss=1.513 Prec@1=63.547 Prec@5=85.063 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=04:08 IST
=> Training   8.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.567 DataTime=0.405 Loss=1.514 Prec@1=63.522 Prec@5=85.079 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=04:08 IST
=> Training   12.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.567 DataTime=0.405 Loss=1.514 Prec@1=63.522 Prec@5=85.079 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=04:08 IST
=> Training   12.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.567 DataTime=0.405 Loss=1.514 Prec@1=63.522 Prec@5=85.079 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=04:09 IST
=> Training   12.03% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.403 Loss=1.518 Prec@1=63.464 Prec@5=85.014 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=04:09 IST
=> Training   16.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.403 Loss=1.518 Prec@1=63.464 Prec@5=85.014 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=04:09 IST
=> Training   16.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.403 Loss=1.518 Prec@1=63.464 Prec@5=85.014 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=04:10 IST
=> Training   16.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.566 DataTime=0.401 Loss=1.518 Prec@1=63.470 Prec@5=85.035 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=04:10 IST
=> Training   20.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.566 DataTime=0.401 Loss=1.518 Prec@1=63.470 Prec@5=85.035 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=04:10 IST
=> Training   20.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.566 DataTime=0.401 Loss=1.518 Prec@1=63.470 Prec@5=85.035 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=04:11 IST
=> Training   20.02% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.400 Loss=1.521 Prec@1=63.424 Prec@5=85.022 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=04:11 IST
=> Training   24.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.400 Loss=1.521 Prec@1=63.424 Prec@5=85.022 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:11 IST
=> Training   24.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.400 Loss=1.521 Prec@1=63.424 Prec@5=85.022 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:12 IST
=> Training   24.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.399 Loss=1.524 Prec@1=63.393 Prec@5=84.988 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:12 IST
=> Training   28.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.399 Loss=1.524 Prec@1=63.393 Prec@5=84.988 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=04:12 IST
=> Training   28.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.399 Loss=1.524 Prec@1=63.393 Prec@5=84.988 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=04:13 IST
=> Training   28.01% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.398 Loss=1.526 Prec@1=63.376 Prec@5=84.954 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=04:13 IST
=> Training   32.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.398 Loss=1.526 Prec@1=63.376 Prec@5=84.954 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=04:13 IST
=> Training   32.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.398 Loss=1.526 Prec@1=63.376 Prec@5=84.954 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=04:14 IST
=> Training   32.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.398 Loss=1.526 Prec@1=63.364 Prec@5=84.948 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=04:14 IST
=> Training   36.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.398 Loss=1.526 Prec@1=63.364 Prec@5=84.948 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:14 IST
=> Training   36.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.398 Loss=1.526 Prec@1=63.364 Prec@5=84.948 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:15 IST
=> Training   36.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.397 Loss=1.527 Prec@1=63.358 Prec@5=84.939 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:15 IST
=> Training   39.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.397 Loss=1.527 Prec@1=63.358 Prec@5=84.939 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=04:15 IST
=> Training   39.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.397 Loss=1.527 Prec@1=63.358 Prec@5=84.939 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=04:16 IST
=> Training   39.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.397 Loss=1.528 Prec@1=63.314 Prec@5=84.914 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=04:16 IST
=> Training   43.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.397 Loss=1.528 Prec@1=63.314 Prec@5=84.914 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=04:16 IST
=> Training   43.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.565 DataTime=0.397 Loss=1.528 Prec@1=63.314 Prec@5=84.914 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=04:17 IST
=> Training   43.99% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.529 Prec@1=63.296 Prec@5=84.911 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=04:17 IST
=> Training   47.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.529 Prec@1=63.296 Prec@5=84.911 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=04:17 IST
=> Training   47.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.529 Prec@1=63.296 Prec@5=84.911 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=04:18 IST
=> Training   47.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.530 Prec@1=63.258 Prec@5=84.874 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=04:18 IST
=> Training   51.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.530 Prec@1=63.258 Prec@5=84.874 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:18 IST
=> Training   51.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.564 DataTime=0.394 Loss=1.530 Prec@1=63.258 Prec@5=84.874 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:18 IST
=> Training   51.98% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.393 Loss=1.532 Prec@1=63.251 Prec@5=84.843 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:18 IST
=> Training   55.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.393 Loss=1.532 Prec@1=63.251 Prec@5=84.843 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=04:18 IST
=> Training   55.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.393 Loss=1.532 Prec@1=63.251 Prec@5=84.843 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=04:19 IST
=> Training   55.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.392 Loss=1.534 Prec@1=63.209 Prec@5=84.812 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=04:19 IST
=> Training   59.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.392 Loss=1.534 Prec@1=63.209 Prec@5=84.812 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=04:19 IST
=> Training   59.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.563 DataTime=0.392 Loss=1.534 Prec@1=63.209 Prec@5=84.812 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=04:20 IST
=> Training   59.97% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.202 Prec@5=84.816 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=04:20 IST
=> Training   63.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.202 Prec@5=84.816 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:20 IST
=> Training   63.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.202 Prec@5=84.816 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:21 IST
=> Training   63.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.197 Prec@5=84.805 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:21 IST
=> Training   67.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.197 Prec@5=84.805 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:21 IST
=> Training   67.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.562 DataTime=0.390 Loss=1.534 Prec@1=63.197 Prec@5=84.805 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:22 IST
=> Training   67.96% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.177 Prec@5=84.766 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:22 IST
=> Training   71.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.177 Prec@5=84.766 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=04:22 IST
=> Training   71.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.536 Prec@1=63.177 Prec@5=84.766 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=04:23 IST
=> Training   71.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.538 Prec@1=63.145 Prec@5=84.748 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=04:23 IST
=> Training   75.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.538 Prec@1=63.145 Prec@5=84.748 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=04:23 IST
=> Training   75.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.538 Prec@1=63.145 Prec@5=84.748 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=04:24 IST
=> Training   75.95% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.539 Prec@1=63.117 Prec@5=84.728 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=04:24 IST
=> Training   79.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.539 Prec@1=63.117 Prec@5=84.728 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=04:24 IST
=> Training   79.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.539 Prec@1=63.117 Prec@5=84.728 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=04:25 IST
=> Training   79.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.540 Prec@1=63.084 Prec@5=84.716 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=04:25 IST
=> Training   83.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.540 Prec@1=63.084 Prec@5=84.716 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:25 IST
=> Training   83.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.540 Prec@1=63.084 Prec@5=84.716 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:26 IST
=> Training   83.94% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.541 Prec@1=63.059 Prec@5=84.691 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:26 IST
=> Training   87.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.541 Prec@1=63.059 Prec@5=84.691 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:26 IST
=> Training   87.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.389 Loss=1.541 Prec@1=63.059 Prec@5=84.691 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:27 IST
=> Training   87.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.388 Loss=1.543 Prec@1=63.027 Prec@5=84.672 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:27 IST
=> Training   91.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.388 Loss=1.543 Prec@1=63.027 Prec@5=84.672 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:27 IST
=> Training   91.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.561 DataTime=0.388 Loss=1.543 Prec@1=63.027 Prec@5=84.672 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:28 IST
=> Training   91.93% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.544 Prec@1=63.013 Prec@5=84.670 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:28 IST
=> Training   95.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.544 Prec@1=63.013 Prec@5=84.670 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:28 IST
=> Training   95.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.544 Prec@1=63.013 Prec@5=84.670 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:29 IST
=> Training   95.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.545 Prec@1=62.981 Prec@5=84.652 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:29 IST
=> Training   99.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.545 Prec@1=62.981 Prec@5=84.652 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:29 IST
=> Training   99.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.545 Prec@1=62.981 Prec@5=84.652 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:29 IST
=> Training   99.92% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.545 Prec@1=62.981 Prec@5=84.651 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:29 IST
=> Training   100.00% of 1x2503...Epoch=32/150 LR=0.0898 Time=0.560 DataTime=0.388 Loss=1.545 Prec@1=62.981 Prec@5=84.651 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=04:29 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:29 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:29 IST
=> Validation 0.00% of 1x98...Epoch=32/150 LR=0.0898 Time=6.735 Loss=0.962 Prec@1=75.391 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=04:29 IST
=> Validation 1.02% of 1x98...Epoch=32/150 LR=0.0898 Time=6.735 Loss=0.962 Prec@1=75.391 Prec@5=92.578 rate=6767.04 Hz, eta=0:00:00, total=0:00:00, wall=04:29 IST
** Validation 1.02% of 1x98...Epoch=32/150 LR=0.0898 Time=6.735 Loss=0.962 Prec@1=75.391 Prec@5=92.578 rate=6767.04 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST
** Validation 1.02% of 1x98...Epoch=32/150 LR=0.0898 Time=0.633 Loss=1.618 Prec@1=61.530 Prec@5=84.216 rate=6767.04 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST
** Validation 100.00% of 1x98...Epoch=32/150 LR=0.0898 Time=0.633 Loss=1.618 Prec@1=61.530 Prec@5=84.216 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=04:30 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST
=> Training   0.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=5.029 DataTime=4.472 Loss=1.628 Prec@1=59.570 Prec@5=82.227 rate=0 Hz, eta=?, total=0:00:00, wall=04:30 IST
=> Training   0.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=5.029 DataTime=4.472 Loss=1.628 Prec@1=59.570 Prec@5=82.227 rate=3690.11 Hz, eta=0:00:00, total=0:00:00, wall=04:30 IST
=> Training   0.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=5.029 DataTime=4.472 Loss=1.628 Prec@1=59.570 Prec@5=82.227 rate=3690.11 Hz, eta=0:00:00, total=0:00:00, wall=04:31 IST
=> Training   0.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.587 DataTime=0.424 Loss=1.496 Prec@1=63.962 Prec@5=85.282 rate=3690.11 Hz, eta=0:00:00, total=0:00:00, wall=04:31 IST
=> Training   4.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.587 DataTime=0.424 Loss=1.496 Prec@1=63.962 Prec@5=85.282 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=04:31 IST
=> Training   4.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.587 DataTime=0.424 Loss=1.496 Prec@1=63.962 Prec@5=85.282 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=04:32 IST
=> Training   4.04% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.568 DataTime=0.407 Loss=1.498 Prec@1=63.879 Prec@5=85.356 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=04:32 IST
=> Training   8.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.568 DataTime=0.407 Loss=1.498 Prec@1=63.879 Prec@5=85.356 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=04:32 IST
=> Training   8.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.568 DataTime=0.407 Loss=1.498 Prec@1=63.879 Prec@5=85.356 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=04:33 IST
=> Training   8.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.570 DataTime=0.404 Loss=1.501 Prec@1=63.809 Prec@5=85.303 rate=1.84 Hz, eta=0:20:51, total=0:01:49, wall=04:33 IST
=> Training   12.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.570 DataTime=0.404 Loss=1.501 Prec@1=63.809 Prec@5=85.303 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:33 IST
=> Training   12.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.570 DataTime=0.404 Loss=1.501 Prec@1=63.809 Prec@5=85.303 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:33 IST
=> Training   12.03% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.565 DataTime=0.395 Loss=1.498 Prec@1=63.919 Prec@5=85.328 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:33 IST
=> Training   16.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.565 DataTime=0.395 Loss=1.498 Prec@1=63.919 Prec@5=85.328 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:33 IST
=> Training   16.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.565 DataTime=0.395 Loss=1.498 Prec@1=63.919 Prec@5=85.328 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:34 IST
=> Training   16.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.393 Loss=1.500 Prec@1=63.840 Prec@5=85.306 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:34 IST
=> Training   20.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.393 Loss=1.500 Prec@1=63.840 Prec@5=85.306 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=04:34 IST
=> Training   20.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.393 Loss=1.500 Prec@1=63.840 Prec@5=85.306 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=04:35 IST
=> Training   20.02% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.391 Loss=1.502 Prec@1=63.766 Prec@5=85.278 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=04:35 IST
=> Training   24.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.391 Loss=1.502 Prec@1=63.766 Prec@5=85.278 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=04:35 IST
=> Training   24.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.391 Loss=1.502 Prec@1=63.766 Prec@5=85.278 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=04:36 IST
=> Training   24.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.392 Loss=1.505 Prec@1=63.726 Prec@5=85.223 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=04:36 IST
=> Training   28.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.392 Loss=1.505 Prec@1=63.726 Prec@5=85.223 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=04:36 IST
=> Training   28.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.392 Loss=1.505 Prec@1=63.726 Prec@5=85.223 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=04:37 IST
=> Training   28.01% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.507 Prec@1=63.674 Prec@5=85.188 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=04:37 IST
=> Training   32.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.507 Prec@1=63.674 Prec@5=85.188 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=04:37 IST
=> Training   32.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.507 Prec@1=63.674 Prec@5=85.188 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=04:38 IST
=> Training   32.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.391 Loss=1.511 Prec@1=63.621 Prec@5=85.143 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=04:38 IST
=> Training   36.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.391 Loss=1.511 Prec@1=63.621 Prec@5=85.143 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:38 IST
=> Training   36.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.391 Loss=1.511 Prec@1=63.621 Prec@5=85.143 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:39 IST
=> Training   36.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.514 Prec@1=63.549 Prec@5=85.082 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:39 IST
=> Training   39.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.514 Prec@1=63.549 Prec@5=85.082 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:39 IST
=> Training   39.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.562 DataTime=0.390 Loss=1.514 Prec@1=63.549 Prec@5=85.082 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:40 IST
=> Training   39.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.390 Loss=1.518 Prec@1=63.466 Prec@5=85.037 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:40 IST
=> Training   43.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.390 Loss=1.518 Prec@1=63.466 Prec@5=85.037 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:40 IST
=> Training   43.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.563 DataTime=0.390 Loss=1.518 Prec@1=63.466 Prec@5=85.037 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:41 IST
=> Training   43.99% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.388 Loss=1.519 Prec@1=63.444 Prec@5=85.006 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:41 IST
=> Training   47.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.388 Loss=1.519 Prec@1=63.444 Prec@5=85.006 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:41 IST
=> Training   47.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.388 Loss=1.519 Prec@1=63.444 Prec@5=85.006 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:42 IST
=> Training   47.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.521 Prec@1=63.418 Prec@5=84.997 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:42 IST
=> Training   51.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.521 Prec@1=63.418 Prec@5=84.997 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=04:42 IST
=> Training   51.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.521 Prec@1=63.418 Prec@5=84.997 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=04:43 IST
=> Training   51.98% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.386 Loss=1.522 Prec@1=63.389 Prec@5=84.975 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=04:43 IST
=> Training   55.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.386 Loss=1.522 Prec@1=63.389 Prec@5=84.975 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=04:43 IST
=> Training   55.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.386 Loss=1.522 Prec@1=63.389 Prec@5=84.975 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=04:44 IST
=> Training   55.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.523 Prec@1=63.389 Prec@5=84.957 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=04:44 IST
=> Training   59.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.523 Prec@1=63.389 Prec@5=84.957 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=04:44 IST
=> Training   59.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.561 DataTime=0.387 Loss=1.523 Prec@1=63.389 Prec@5=84.957 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=04:45 IST
=> Training   59.97% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.387 Loss=1.525 Prec@1=63.339 Prec@5=84.939 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=04:45 IST
=> Training   63.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.387 Loss=1.525 Prec@1=63.339 Prec@5=84.939 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=04:45 IST
=> Training   63.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.387 Loss=1.525 Prec@1=63.339 Prec@5=84.939 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=04:46 IST
=> Training   63.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.387 Loss=1.527 Prec@1=63.291 Prec@5=84.912 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=04:46 IST
=> Training   67.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.387 Loss=1.527 Prec@1=63.291 Prec@5=84.912 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=04:46 IST
=> Training   67.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.560 DataTime=0.387 Loss=1.527 Prec@1=63.291 Prec@5=84.912 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=04:46 IST
=> Training   67.96% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.528 Prec@1=63.265 Prec@5=84.902 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=04:46 IST
=> Training   71.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.528 Prec@1=63.265 Prec@5=84.902 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=04:46 IST
=> Training   71.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.528 Prec@1=63.265 Prec@5=84.902 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=04:47 IST
=> Training   71.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.529 Prec@1=63.254 Prec@5=84.896 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=04:47 IST
=> Training   75.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.529 Prec@1=63.254 Prec@5=84.896 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=04:47 IST
=> Training   75.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.559 DataTime=0.386 Loss=1.529 Prec@1=63.254 Prec@5=84.896 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=04:48 IST
=> Training   75.95% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.530 Prec@1=63.240 Prec@5=84.881 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=04:48 IST
=> Training   79.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.530 Prec@1=63.240 Prec@5=84.881 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=04:48 IST
=> Training   79.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.530 Prec@1=63.240 Prec@5=84.881 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=04:49 IST
=> Training   79.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.532 Prec@1=63.220 Prec@5=84.855 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=04:49 IST
=> Training   83.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.532 Prec@1=63.220 Prec@5=84.855 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=04:49 IST
=> Training   83.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.532 Prec@1=63.220 Prec@5=84.855 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=04:50 IST
=> Training   83.94% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.533 Prec@1=63.204 Prec@5=84.845 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=04:50 IST
=> Training   87.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.533 Prec@1=63.204 Prec@5=84.845 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=04:50 IST
=> Training   87.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.533 Prec@1=63.204 Prec@5=84.845 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=04:51 IST
=> Training   87.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.535 Prec@1=63.181 Prec@5=84.819 rate=1.80 Hz, eta=0:02:47, total=0:20:23, wall=04:51 IST
=> Training   91.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.535 Prec@1=63.181 Prec@5=84.819 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=04:51 IST
=> Training   91.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.535 Prec@1=63.181 Prec@5=84.819 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=04:52 IST
=> Training   91.93% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.536 Prec@1=63.157 Prec@5=84.805 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=04:52 IST
=> Training   95.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.536 Prec@1=63.157 Prec@5=84.805 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:52 IST
=> Training   95.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.536 Prec@1=63.157 Prec@5=84.805 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:53 IST
=> Training   95.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.537 Prec@1=63.128 Prec@5=84.789 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=04:53 IST
=> Training   99.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.537 Prec@1=63.128 Prec@5=84.789 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=04:53 IST
=> Training   99.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.537 Prec@1=63.128 Prec@5=84.789 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=04:53 IST
=> Training   99.92% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.537 Prec@1=63.128 Prec@5=84.789 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=04:53 IST
=> Training   100.00% of 1x2503...Epoch=33/150 LR=0.0892 Time=0.558 DataTime=0.386 Loss=1.537 Prec@1=63.128 Prec@5=84.789 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=04:53 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> Validation 0.00% of 1x98...Epoch=33/150 LR=0.0892 Time=7.006 Loss=1.070 Prec@1=73.047 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=04:53 IST
=> Validation 1.02% of 1x98...Epoch=33/150 LR=0.0892 Time=7.006 Loss=1.070 Prec@1=73.047 Prec@5=91.016 rate=5613.34 Hz, eta=0:00:00, total=0:00:00, wall=04:53 IST
** Validation 1.02% of 1x98...Epoch=33/150 LR=0.0892 Time=7.006 Loss=1.070 Prec@1=73.047 Prec@5=91.016 rate=5613.34 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST
** Validation 1.02% of 1x98...Epoch=33/150 LR=0.0892 Time=0.634 Loss=1.579 Prec@1=61.918 Prec@5=84.772 rate=5613.34 Hz, eta=0:00:00, total=0:00:00, wall=04:54 IST
** Validation 100.00% of 1x98...Epoch=33/150 LR=0.0892 Time=0.634 Loss=1.579 Prec@1=61.918 Prec@5=84.772 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=04:54 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> Training   0.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=4.633 DataTime=4.152 Loss=1.621 Prec@1=60.352 Prec@5=82.617 rate=0 Hz, eta=?, total=0:00:00, wall=04:54 IST
=> Training   0.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=4.633 DataTime=4.152 Loss=1.621 Prec@1=60.352 Prec@5=82.617 rate=1071.35 Hz, eta=0:00:02, total=0:00:00, wall=04:54 IST
=> Training   0.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=4.633 DataTime=4.152 Loss=1.621 Prec@1=60.352 Prec@5=82.617 rate=1071.35 Hz, eta=0:00:02, total=0:00:00, wall=04:55 IST
=> Training   0.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.592 DataTime=0.428 Loss=1.488 Prec@1=64.252 Prec@5=85.471 rate=1071.35 Hz, eta=0:00:02, total=0:00:00, wall=04:55 IST
=> Training   4.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.592 DataTime=0.428 Loss=1.488 Prec@1=64.252 Prec@5=85.471 rate=1.83 Hz, eta=0:21:50, total=0:00:55, wall=04:55 IST
=> Training   4.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.592 DataTime=0.428 Loss=1.488 Prec@1=64.252 Prec@5=85.471 rate=1.83 Hz, eta=0:21:50, total=0:00:55, wall=04:56 IST
=> Training   4.04% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.403 Loss=1.489 Prec@1=64.217 Prec@5=85.426 rate=1.83 Hz, eta=0:21:50, total=0:00:55, wall=04:56 IST
=> Training   8.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.403 Loss=1.489 Prec@1=64.217 Prec@5=85.426 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:56 IST
=> Training   8.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.403 Loss=1.489 Prec@1=64.217 Prec@5=85.426 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:57 IST
=> Training   8.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.399 Loss=1.489 Prec@1=64.196 Prec@5=85.409 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:57 IST
=> Training   12.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.399 Loss=1.489 Prec@1=64.196 Prec@5=85.409 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=04:57 IST
=> Training   12.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.570 DataTime=0.399 Loss=1.489 Prec@1=64.196 Prec@5=85.409 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=04:58 IST
=> Training   12.03% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.566 DataTime=0.396 Loss=1.490 Prec@1=64.120 Prec@5=85.422 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=04:58 IST
=> Training   16.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.566 DataTime=0.396 Loss=1.490 Prec@1=64.120 Prec@5=85.422 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=04:58 IST
=> Training   16.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.566 DataTime=0.396 Loss=1.490 Prec@1=64.120 Prec@5=85.422 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=04:59 IST
=> Training   16.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.565 DataTime=0.397 Loss=1.493 Prec@1=64.040 Prec@5=85.377 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=04:59 IST
=> Training   20.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.565 DataTime=0.397 Loss=1.493 Prec@1=64.040 Prec@5=85.377 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=04:59 IST
=> Training   20.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.565 DataTime=0.397 Loss=1.493 Prec@1=64.040 Prec@5=85.377 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=05:00 IST
=> Training   20.02% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.395 Loss=1.496 Prec@1=63.974 Prec@5=85.341 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=05:00 IST
=> Training   24.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.395 Loss=1.496 Prec@1=63.974 Prec@5=85.341 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:00 IST
=> Training   24.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.395 Loss=1.496 Prec@1=63.974 Prec@5=85.341 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:01 IST
=> Training   24.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.394 Loss=1.500 Prec@1=63.885 Prec@5=85.296 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:01 IST
=> Training   28.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.394 Loss=1.500 Prec@1=63.885 Prec@5=85.296 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=05:01 IST
=> Training   28.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.563 DataTime=0.394 Loss=1.500 Prec@1=63.885 Prec@5=85.296 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=05:02 IST
=> Training   28.01% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.564 DataTime=0.394 Loss=1.502 Prec@1=63.863 Prec@5=85.270 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=05:02 IST
=> Training   32.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.564 DataTime=0.394 Loss=1.502 Prec@1=63.863 Prec@5=85.270 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=05:02 IST
=> Training   32.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.564 DataTime=0.394 Loss=1.502 Prec@1=63.863 Prec@5=85.270 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=05:02 IST
=> Training   32.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.393 Loss=1.506 Prec@1=63.761 Prec@5=85.219 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=05:02 IST
=> Training   36.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.393 Loss=1.506 Prec@1=63.761 Prec@5=85.219 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=05:02 IST
=> Training   36.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.393 Loss=1.506 Prec@1=63.761 Prec@5=85.219 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=05:03 IST
=> Training   36.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.391 Loss=1.508 Prec@1=63.717 Prec@5=85.195 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=05:03 IST
=> Training   39.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.391 Loss=1.508 Prec@1=63.717 Prec@5=85.195 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=05:03 IST
=> Training   39.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.562 DataTime=0.391 Loss=1.508 Prec@1=63.717 Prec@5=85.195 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=05:04 IST
=> Training   39.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.561 DataTime=0.390 Loss=1.509 Prec@1=63.710 Prec@5=85.177 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=05:04 IST
=> Training   43.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.561 DataTime=0.390 Loss=1.509 Prec@1=63.710 Prec@5=85.177 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=05:04 IST
=> Training   43.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.561 DataTime=0.390 Loss=1.509 Prec@1=63.710 Prec@5=85.177 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=05:05 IST
=> Training   43.99% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.511 Prec@1=63.666 Prec@5=85.138 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=05:05 IST
=> Training   47.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.511 Prec@1=63.666 Prec@5=85.138 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=05:05 IST
=> Training   47.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.511 Prec@1=63.666 Prec@5=85.138 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=05:06 IST
=> Training   47.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.513 Prec@1=63.620 Prec@5=85.114 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=05:06 IST
=> Training   51.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.513 Prec@1=63.620 Prec@5=85.114 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=05:06 IST
=> Training   51.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.390 Loss=1.513 Prec@1=63.620 Prec@5=85.114 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=05:07 IST
=> Training   51.98% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.389 Loss=1.514 Prec@1=63.603 Prec@5=85.092 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=05:07 IST
=> Training   55.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.389 Loss=1.514 Prec@1=63.603 Prec@5=85.092 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=05:07 IST
=> Training   55.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.389 Loss=1.514 Prec@1=63.603 Prec@5=85.092 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=05:08 IST
=> Training   55.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.564 Prec@5=85.071 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=05:08 IST
=> Training   59.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.564 Prec@5=85.071 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=05:08 IST
=> Training   59.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.564 Prec@5=85.071 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=05:09 IST
=> Training   59.97% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.534 Prec@5=85.059 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=05:09 IST
=> Training   63.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.534 Prec@5=85.059 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=05:09 IST
=> Training   63.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.516 Prec@1=63.534 Prec@5=85.059 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=05:10 IST
=> Training   63.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.519 Prec@1=63.498 Prec@5=85.025 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=05:10 IST
=> Training   67.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.519 Prec@1=63.498 Prec@5=85.025 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=05:10 IST
=> Training   67.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.519 Prec@1=63.498 Prec@5=85.025 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=05:11 IST
=> Training   67.96% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.520 Prec@1=63.471 Prec@5=84.996 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=05:11 IST
=> Training   71.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.520 Prec@1=63.471 Prec@5=84.996 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=05:11 IST
=> Training   71.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.520 Prec@1=63.471 Prec@5=84.996 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=05:12 IST
=> Training   71.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.521 Prec@1=63.442 Prec@5=84.988 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=05:12 IST
=> Training   75.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.521 Prec@1=63.442 Prec@5=84.988 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=05:12 IST
=> Training   75.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.560 DataTime=0.388 Loss=1.521 Prec@1=63.442 Prec@5=84.988 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=05:13 IST
=> Training   75.95% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.387 Loss=1.523 Prec@1=63.405 Prec@5=84.976 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=05:13 IST
=> Training   79.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.387 Loss=1.523 Prec@1=63.405 Prec@5=84.976 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=05:13 IST
=> Training   79.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.387 Loss=1.523 Prec@1=63.405 Prec@5=84.976 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=05:14 IST
=> Training   79.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.524 Prec@1=63.387 Prec@5=84.962 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=05:14 IST
=> Training   83.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.524 Prec@1=63.387 Prec@5=84.962 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=05:14 IST
=> Training   83.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.524 Prec@1=63.387 Prec@5=84.962 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=05:15 IST
=> Training   83.94% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.525 Prec@1=63.357 Prec@5=84.942 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=05:15 IST
=> Training   87.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.525 Prec@1=63.357 Prec@5=84.942 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=05:15 IST
=> Training   87.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.525 Prec@1=63.357 Prec@5=84.942 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=05:15 IST
=> Training   87.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.527 Prec@1=63.334 Prec@5=84.924 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=05:15 IST
=> Training   91.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.527 Prec@1=63.334 Prec@5=84.924 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=05:15 IST
=> Training   91.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.559 DataTime=0.388 Loss=1.527 Prec@1=63.334 Prec@5=84.924 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=05:16 IST
=> Training   91.93% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.308 Prec@5=84.913 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=05:16 IST
=> Training   95.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.308 Prec@5=84.913 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=05:16 IST
=> Training   95.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.308 Prec@5=84.913 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=05:17 IST
=> Training   95.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.304 Prec@5=84.904 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=05:17 IST
=> Training   99.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.304 Prec@5=84.904 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=05:17 IST
=> Training   99.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.304 Prec@5=84.904 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=05:17 IST
=> Training   99.92% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.304 Prec@5=84.904 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=05:17 IST
=> Training   100.00% of 1x2503...Epoch=34/150 LR=0.0885 Time=0.558 DataTime=0.387 Loss=1.528 Prec@1=63.304 Prec@5=84.904 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=05:17 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST
=> Validation 0.00% of 1x98...Epoch=34/150 LR=0.0885 Time=7.308 Loss=1.065 Prec@1=72.656 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=05:17 IST
=> Validation 1.02% of 1x98...Epoch=34/150 LR=0.0885 Time=7.308 Loss=1.065 Prec@1=72.656 Prec@5=91.797 rate=6720.84 Hz, eta=0:00:00, total=0:00:00, wall=05:17 IST
** Validation 1.02% of 1x98...Epoch=34/150 LR=0.0885 Time=7.308 Loss=1.065 Prec@1=72.656 Prec@5=91.797 rate=6720.84 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST
** Validation 1.02% of 1x98...Epoch=34/150 LR=0.0885 Time=0.626 Loss=1.632 Prec@1=61.070 Prec@5=84.060 rate=6720.84 Hz, eta=0:00:00, total=0:00:00, wall=05:18 IST
** Validation 100.00% of 1x98...Epoch=34/150 LR=0.0885 Time=0.626 Loss=1.632 Prec@1=61.070 Prec@5=84.060 rate=1.81 Hz, eta=0:00:00, total=0:00:54, wall=05:18 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST
=> Training   0.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=4.436 DataTime=4.097 Loss=1.608 Prec@1=62.500 Prec@5=82.812 rate=0 Hz, eta=?, total=0:00:00, wall=05:18 IST
=> Training   0.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=4.436 DataTime=4.097 Loss=1.608 Prec@1=62.500 Prec@5=82.812 rate=331.99 Hz, eta=0:00:07, total=0:00:00, wall=05:18 IST
=> Training   0.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=4.436 DataTime=4.097 Loss=1.608 Prec@1=62.500 Prec@5=82.812 rate=331.99 Hz, eta=0:00:07, total=0:00:00, wall=05:19 IST
=> Training   0.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.586 DataTime=0.415 Loss=1.485 Prec@1=64.018 Prec@5=85.651 rate=331.99 Hz, eta=0:00:07, total=0:00:00, wall=05:19 IST
=> Training   4.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.586 DataTime=0.415 Loss=1.485 Prec@1=64.018 Prec@5=85.651 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:19 IST
=> Training   4.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.586 DataTime=0.415 Loss=1.485 Prec@1=64.018 Prec@5=85.651 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:20 IST
=> Training   4.04% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.572 DataTime=0.399 Loss=1.481 Prec@1=64.135 Prec@5=85.585 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:20 IST
=> Training   8.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.572 DataTime=0.399 Loss=1.481 Prec@1=64.135 Prec@5=85.585 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=05:20 IST
=> Training   8.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.572 DataTime=0.399 Loss=1.481 Prec@1=64.135 Prec@5=85.585 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=05:21 IST
=> Training   8.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.574 DataTime=0.402 Loss=1.488 Prec@1=63.958 Prec@5=85.545 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=05:21 IST
=> Training   12.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.574 DataTime=0.402 Loss=1.488 Prec@1=63.958 Prec@5=85.545 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=05:21 IST
=> Training   12.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.574 DataTime=0.402 Loss=1.488 Prec@1=63.958 Prec@5=85.545 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=05:22 IST
=> Training   12.03% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.570 DataTime=0.397 Loss=1.489 Prec@1=63.961 Prec@5=85.472 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=05:22 IST
=> Training   16.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.570 DataTime=0.397 Loss=1.489 Prec@1=63.961 Prec@5=85.472 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=05:22 IST
=> Training   16.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.570 DataTime=0.397 Loss=1.489 Prec@1=63.961 Prec@5=85.472 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=05:23 IST
=> Training   16.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.569 DataTime=0.397 Loss=1.492 Prec@1=63.921 Prec@5=85.454 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=05:23 IST
=> Training   20.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.569 DataTime=0.397 Loss=1.492 Prec@1=63.921 Prec@5=85.454 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=05:23 IST
=> Training   20.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.569 DataTime=0.397 Loss=1.492 Prec@1=63.921 Prec@5=85.454 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=05:24 IST
=> Training   20.02% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.496 Prec@1=63.882 Prec@5=85.405 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=05:24 IST
=> Training   24.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.496 Prec@1=63.882 Prec@5=85.405 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:24 IST
=> Training   24.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.496 Prec@1=63.882 Prec@5=85.405 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:25 IST
=> Training   24.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.396 Loss=1.499 Prec@1=63.816 Prec@5=85.340 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:25 IST
=> Training   28.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.396 Loss=1.499 Prec@1=63.816 Prec@5=85.340 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=05:25 IST
=> Training   28.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.396 Loss=1.499 Prec@1=63.816 Prec@5=85.340 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=05:26 IST
=> Training   28.01% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.395 Loss=1.500 Prec@1=63.805 Prec@5=85.307 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=05:26 IST
=> Training   32.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.395 Loss=1.500 Prec@1=63.805 Prec@5=85.307 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=05:26 IST
=> Training   32.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.566 DataTime=0.395 Loss=1.500 Prec@1=63.805 Prec@5=85.307 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=05:27 IST
=> Training   32.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.395 Loss=1.503 Prec@1=63.772 Prec@5=85.266 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=05:27 IST
=> Training   36.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.395 Loss=1.503 Prec@1=63.772 Prec@5=85.266 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=05:27 IST
=> Training   36.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.395 Loss=1.503 Prec@1=63.772 Prec@5=85.266 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=05:28 IST
=> Training   36.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.504 Prec@1=63.739 Prec@5=85.236 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=05:28 IST
=> Training   39.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.504 Prec@1=63.739 Prec@5=85.236 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=05:28 IST
=> Training   39.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.504 Prec@1=63.739 Prec@5=85.236 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=05:29 IST
=> Training   39.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.505 Prec@1=63.723 Prec@5=85.218 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=05:29 IST
=> Training   43.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.505 Prec@1=63.723 Prec@5=85.218 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=05:29 IST
=> Training   43.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.565 DataTime=0.394 Loss=1.505 Prec@1=63.723 Prec@5=85.218 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=05:30 IST
=> Training   43.99% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.506 Prec@1=63.697 Prec@5=85.199 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=05:30 IST
=> Training   47.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.506 Prec@1=63.697 Prec@5=85.199 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:30 IST
=> Training   47.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.506 Prec@1=63.697 Prec@5=85.199 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:31 IST
=> Training   47.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.508 Prec@1=63.662 Prec@5=85.171 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:31 IST
=> Training   51.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.508 Prec@1=63.662 Prec@5=85.171 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=05:31 IST
=> Training   51.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.394 Loss=1.508 Prec@1=63.662 Prec@5=85.171 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=05:32 IST
=> Training   51.98% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.509 Prec@1=63.630 Prec@5=85.155 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=05:32 IST
=> Training   55.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.509 Prec@1=63.630 Prec@5=85.155 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=05:32 IST
=> Training   55.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.509 Prec@1=63.630 Prec@5=85.155 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=05:32 IST
=> Training   55.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.510 Prec@1=63.598 Prec@5=85.136 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=05:32 IST
=> Training   59.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.510 Prec@1=63.598 Prec@5=85.136 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=05:32 IST
=> Training   59.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.393 Loss=1.510 Prec@1=63.598 Prec@5=85.136 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=05:33 IST
=> Training   59.97% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.392 Loss=1.511 Prec@1=63.600 Prec@5=85.119 rate=1.78 Hz, eta=0:09:22, total=0:14:01, wall=05:33 IST
=> Training   63.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.392 Loss=1.511 Prec@1=63.600 Prec@5=85.119 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=05:33 IST
=> Training   63.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.564 DataTime=0.392 Loss=1.511 Prec@1=63.600 Prec@5=85.119 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=05:34 IST
=> Training   63.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.512 Prec@1=63.594 Prec@5=85.108 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=05:34 IST
=> Training   67.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.512 Prec@1=63.594 Prec@5=85.108 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=05:34 IST
=> Training   67.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.512 Prec@1=63.594 Prec@5=85.108 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=05:35 IST
=> Training   67.96% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.563 DataTime=0.392 Loss=1.512 Prec@1=63.590 Prec@5=85.105 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=05:35 IST
=> Training   71.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.563 DataTime=0.392 Loss=1.512 Prec@1=63.590 Prec@5=85.105 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:35 IST
=> Training   71.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.563 DataTime=0.392 Loss=1.512 Prec@1=63.590 Prec@5=85.105 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:36 IST
=> Training   71.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.514 Prec@1=63.565 Prec@5=85.084 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:36 IST
=> Training   75.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.514 Prec@1=63.565 Prec@5=85.084 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:36 IST
=> Training   75.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.514 Prec@1=63.565 Prec@5=85.084 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:37 IST
=> Training   75.95% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.515 Prec@1=63.528 Prec@5=85.064 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:37 IST
=> Training   79.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.515 Prec@1=63.528 Prec@5=85.064 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:37 IST
=> Training   79.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.515 Prec@1=63.528 Prec@5=85.064 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:38 IST
=> Training   79.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.516 Prec@1=63.510 Prec@5=85.062 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:38 IST
=> Training   83.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.516 Prec@1=63.510 Prec@5=85.062 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=05:38 IST
=> Training   83.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.516 Prec@1=63.510 Prec@5=85.062 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=05:39 IST
=> Training   83.94% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.517 Prec@1=63.503 Prec@5=85.046 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=05:39 IST
=> Training   87.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.517 Prec@1=63.503 Prec@5=85.046 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=05:39 IST
=> Training   87.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.517 Prec@1=63.503 Prec@5=85.046 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=05:40 IST
=> Training   87.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.518 Prec@1=63.494 Prec@5=85.028 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=05:40 IST
=> Training   91.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.518 Prec@1=63.494 Prec@5=85.028 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:40 IST
=> Training   91.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.391 Loss=1.518 Prec@1=63.494 Prec@5=85.028 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:41 IST
=> Training   91.93% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.518 Prec@1=63.483 Prec@5=85.023 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:41 IST
=> Training   95.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.518 Prec@1=63.483 Prec@5=85.023 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:41 IST
=> Training   95.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.562 DataTime=0.390 Loss=1.518 Prec@1=63.483 Prec@5=85.023 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:42 IST
=> Training   95.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.520 Prec@1=63.459 Prec@5=85.003 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:42 IST
=> Training   99.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.520 Prec@1=63.459 Prec@5=85.003 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=05:42 IST
=> Training   99.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.390 Loss=1.520 Prec@1=63.459 Prec@5=85.003 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=05:42 IST
=> Training   99.92% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.389 Loss=1.520 Prec@1=63.457 Prec@5=85.002 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=05:42 IST
=> Training   100.00% of 1x2503...Epoch=35/150 LR=0.0878 Time=0.561 DataTime=0.389 Loss=1.520 Prec@1=63.457 Prec@5=85.002 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=05:42 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> Validation 0.00% of 1x98...Epoch=35/150 LR=0.0878 Time=8.420 Loss=0.999 Prec@1=72.852 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=05:42 IST
=> Validation 1.02% of 1x98...Epoch=35/150 LR=0.0878 Time=8.420 Loss=0.999 Prec@1=72.852 Prec@5=91.797 rate=8148.56 Hz, eta=0:00:00, total=0:00:00, wall=05:42 IST
** Validation 1.02% of 1x98...Epoch=35/150 LR=0.0878 Time=8.420 Loss=0.999 Prec@1=72.852 Prec@5=91.797 rate=8148.56 Hz, eta=0:00:00, total=0:00:00, wall=05:43 IST
** Validation 1.02% of 1x98...Epoch=35/150 LR=0.0878 Time=0.649 Loss=1.656 Prec@1=60.880 Prec@5=83.618 rate=8148.56 Hz, eta=0:00:00, total=0:00:00, wall=05:43 IST
** Validation 100.00% of 1x98...Epoch=35/150 LR=0.0878 Time=0.649 Loss=1.656 Prec@1=60.880 Prec@5=83.618 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=05:43 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:43 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:43 IST
=> Training   0.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=4.149 DataTime=3.972 Loss=1.384 Prec@1=67.383 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=05:43 IST
=> Training   0.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=4.149 DataTime=3.972 Loss=1.384 Prec@1=67.383 Prec@5=88.086 rate=110.62 Hz, eta=0:00:22, total=0:00:00, wall=05:43 IST
=> Training   0.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=4.149 DataTime=3.972 Loss=1.384 Prec@1=67.383 Prec@5=88.086 rate=110.62 Hz, eta=0:00:22, total=0:00:00, wall=05:44 IST
=> Training   0.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.593 DataTime=0.430 Loss=1.469 Prec@1=64.585 Prec@5=85.779 rate=110.62 Hz, eta=0:00:22, total=0:00:00, wall=05:44 IST
=> Training   4.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.593 DataTime=0.430 Loss=1.469 Prec@1=64.585 Prec@5=85.779 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:44 IST
=> Training   4.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.593 DataTime=0.430 Loss=1.469 Prec@1=64.585 Prec@5=85.779 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:45 IST
=> Training   4.04% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.683 Prec@5=85.755 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:45 IST
=> Training   8.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.683 Prec@5=85.755 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=05:45 IST
=> Training   8.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.683 Prec@5=85.755 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=05:46 IST
=> Training   8.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.400 Loss=1.472 Prec@1=64.532 Prec@5=85.677 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=05:46 IST
=> Training   12.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.400 Loss=1.472 Prec@1=64.532 Prec@5=85.677 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=05:46 IST
=> Training   12.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.569 DataTime=0.400 Loss=1.472 Prec@1=64.532 Prec@5=85.677 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=05:47 IST
=> Training   12.03% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.473 Prec@1=64.477 Prec@5=85.661 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=05:47 IST
=> Training   16.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.473 Prec@1=64.477 Prec@5=85.661 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=05:47 IST
=> Training   16.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.473 Prec@1=64.477 Prec@5=85.661 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=05:48 IST
=> Training   16.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.566 DataTime=0.394 Loss=1.478 Prec@1=64.329 Prec@5=85.612 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=05:48 IST
=> Training   20.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.566 DataTime=0.394 Loss=1.478 Prec@1=64.329 Prec@5=85.612 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:48 IST
=> Training   20.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.566 DataTime=0.394 Loss=1.478 Prec@1=64.329 Prec@5=85.612 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:48 IST
=> Training   20.02% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.267 Prec@5=85.534 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:48 IST
=> Training   24.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.267 Prec@5=85.534 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=05:48 IST
=> Training   24.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.267 Prec@5=85.534 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=05:49 IST
=> Training   24.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.271 Prec@5=85.527 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=05:49 IST
=> Training   28.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.271 Prec@5=85.527 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=05:49 IST
=> Training   28.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.564 DataTime=0.391 Loss=1.482 Prec@1=64.271 Prec@5=85.527 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=05:50 IST
=> Training   28.01% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.486 Prec@1=64.184 Prec@5=85.489 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=05:50 IST
=> Training   32.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.486 Prec@1=64.184 Prec@5=85.489 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=05:50 IST
=> Training   32.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.486 Prec@1=64.184 Prec@5=85.489 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=05:51 IST
=> Training   32.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.488 Prec@1=64.151 Prec@5=85.457 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=05:51 IST
=> Training   36.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.488 Prec@1=64.151 Prec@5=85.457 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=05:51 IST
=> Training   36.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.488 Prec@1=64.151 Prec@5=85.457 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=05:52 IST
=> Training   36.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.490 Prec@1=64.101 Prec@5=85.409 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=05:52 IST
=> Training   39.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.490 Prec@1=64.101 Prec@5=85.409 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:52 IST
=> Training   39.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.390 Loss=1.490 Prec@1=64.101 Prec@5=85.409 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:53 IST
=> Training   39.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.563 DataTime=0.391 Loss=1.492 Prec@1=64.043 Prec@5=85.396 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:53 IST
=> Training   43.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.563 DataTime=0.391 Loss=1.492 Prec@1=64.043 Prec@5=85.396 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=05:53 IST
=> Training   43.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.563 DataTime=0.391 Loss=1.492 Prec@1=64.043 Prec@5=85.396 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=05:54 IST
=> Training   43.99% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.495 Prec@1=63.970 Prec@5=85.351 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=05:54 IST
=> Training   47.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.495 Prec@1=63.970 Prec@5=85.351 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=05:54 IST
=> Training   47.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.562 DataTime=0.389 Loss=1.495 Prec@1=63.970 Prec@5=85.351 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=05:55 IST
=> Training   47.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.388 Loss=1.497 Prec@1=63.936 Prec@5=85.335 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=05:55 IST
=> Training   51.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.388 Loss=1.497 Prec@1=63.936 Prec@5=85.335 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=05:55 IST
=> Training   51.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.388 Loss=1.497 Prec@1=63.936 Prec@5=85.335 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=05:56 IST
=> Training   51.98% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.499 Prec@1=63.907 Prec@5=85.309 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=05:56 IST
=> Training   55.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.499 Prec@1=63.907 Prec@5=85.309 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=05:56 IST
=> Training   55.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.499 Prec@1=63.907 Prec@5=85.309 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=05:57 IST
=> Training   55.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.389 Loss=1.500 Prec@1=63.882 Prec@5=85.300 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=05:57 IST
=> Training   59.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.389 Loss=1.500 Prec@1=63.882 Prec@5=85.300 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=05:57 IST
=> Training   59.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.561 DataTime=0.389 Loss=1.500 Prec@1=63.882 Prec@5=85.300 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=05:58 IST
=> Training   59.97% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.502 Prec@1=63.819 Prec@5=85.265 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=05:58 IST
=> Training   63.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.502 Prec@1=63.819 Prec@5=85.265 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=05:58 IST
=> Training   63.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.502 Prec@1=63.819 Prec@5=85.265 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=05:59 IST
=> Training   63.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.504 Prec@1=63.788 Prec@5=85.236 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=05:59 IST
=> Training   67.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.504 Prec@1=63.788 Prec@5=85.236 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=05:59 IST
=> Training   67.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.388 Loss=1.504 Prec@1=63.788 Prec@5=85.236 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=06:00 IST
=> Training   67.96% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.505 Prec@1=63.763 Prec@5=85.220 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=06:00 IST
=> Training   71.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.505 Prec@1=63.763 Prec@5=85.220 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=06:00 IST
=> Training   71.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.505 Prec@1=63.763 Prec@5=85.220 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=06:01 IST
=> Training   71.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.506 Prec@1=63.746 Prec@5=85.209 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=06:01 IST
=> Training   75.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.506 Prec@1=63.746 Prec@5=85.209 rate=1.80 Hz, eta=0:05:35, total=0:17:39, wall=06:01 IST
=> Training   75.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.506 Prec@1=63.746 Prec@5=85.209 rate=1.80 Hz, eta=0:05:35, total=0:17:39, wall=06:01 IST
=> Training   75.95% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.507 Prec@1=63.733 Prec@5=85.196 rate=1.80 Hz, eta=0:05:35, total=0:17:39, wall=06:01 IST
=> Training   79.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.507 Prec@1=63.733 Prec@5=85.196 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=06:01 IST
=> Training   79.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.507 Prec@1=63.733 Prec@5=85.196 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=06:02 IST
=> Training   79.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.507 Prec@1=63.732 Prec@5=85.195 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=06:02 IST
=> Training   83.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.507 Prec@1=63.732 Prec@5=85.195 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:02 IST
=> Training   83.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.387 Loss=1.507 Prec@1=63.732 Prec@5=85.195 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:03 IST
=> Training   83.94% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.508 Prec@1=63.708 Prec@5=85.180 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:03 IST
=> Training   87.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.508 Prec@1=63.708 Prec@5=85.180 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=06:03 IST
=> Training   87.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.508 Prec@1=63.708 Prec@5=85.180 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=06:04 IST
=> Training   87.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.510 Prec@1=63.669 Prec@5=85.150 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=06:04 IST
=> Training   91.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.510 Prec@1=63.669 Prec@5=85.150 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=06:04 IST
=> Training   91.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.560 DataTime=0.387 Loss=1.510 Prec@1=63.669 Prec@5=85.150 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=06:05 IST
=> Training   91.93% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.512 Prec@1=63.636 Prec@5=85.135 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=06:05 IST
=> Training   95.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.512 Prec@1=63.636 Prec@5=85.135 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=06:05 IST
=> Training   95.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.386 Loss=1.512 Prec@1=63.636 Prec@5=85.135 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=06:06 IST
=> Training   95.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.385 Loss=1.513 Prec@1=63.614 Prec@5=85.119 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=06:06 IST
=> Training   99.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.385 Loss=1.513 Prec@1=63.614 Prec@5=85.119 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:06 IST
=> Training   99.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.559 DataTime=0.385 Loss=1.513 Prec@1=63.614 Prec@5=85.119 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:06 IST
=> Training   99.92% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.558 DataTime=0.385 Loss=1.513 Prec@1=63.612 Prec@5=85.117 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:06 IST
=> Training   100.00% of 1x2503...Epoch=36/150 LR=0.0872 Time=0.558 DataTime=0.385 Loss=1.513 Prec@1=63.612 Prec@5=85.117 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=06:06 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:06 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:06 IST
=> Validation 0.00% of 1x98...Epoch=36/150 LR=0.0872 Time=6.937 Loss=1.135 Prec@1=70.703 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=06:06 IST
=> Validation 1.02% of 1x98...Epoch=36/150 LR=0.0872 Time=6.937 Loss=1.135 Prec@1=70.703 Prec@5=89.453 rate=6003.88 Hz, eta=0:00:00, total=0:00:00, wall=06:06 IST
** Validation 1.02% of 1x98...Epoch=36/150 LR=0.0872 Time=6.937 Loss=1.135 Prec@1=70.703 Prec@5=89.453 rate=6003.88 Hz, eta=0:00:00, total=0:00:00, wall=06:07 IST
** Validation 1.02% of 1x98...Epoch=36/150 LR=0.0872 Time=0.633 Loss=1.674 Prec@1=60.314 Prec@5=83.090 rate=6003.88 Hz, eta=0:00:00, total=0:00:00, wall=06:07 IST
** Validation 100.00% of 1x98...Epoch=36/150 LR=0.0872 Time=0.633 Loss=1.674 Prec@1=60.314 Prec@5=83.090 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=06:07 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:07 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:07 IST
=> Training   0.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=5.264 DataTime=4.718 Loss=1.509 Prec@1=65.430 Prec@5=84.961 rate=0 Hz, eta=?, total=0:00:00, wall=06:07 IST
=> Training   0.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=5.264 DataTime=4.718 Loss=1.509 Prec@1=65.430 Prec@5=84.961 rate=119.51 Hz, eta=0:00:20, total=0:00:00, wall=06:07 IST
=> Training   0.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=5.264 DataTime=4.718 Loss=1.509 Prec@1=65.430 Prec@5=84.961 rate=119.51 Hz, eta=0:00:20, total=0:00:00, wall=06:08 IST
=> Training   0.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.596 DataTime=0.430 Loss=1.466 Prec@1=64.737 Prec@5=85.613 rate=119.51 Hz, eta=0:00:20, total=0:00:00, wall=06:08 IST
=> Training   4.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.596 DataTime=0.430 Loss=1.466 Prec@1=64.737 Prec@5=85.613 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=06:08 IST
=> Training   4.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.596 DataTime=0.430 Loss=1.466 Prec@1=64.737 Prec@5=85.613 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=06:09 IST
=> Training   4.04% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.472 Prec@1=64.582 Prec@5=85.564 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=06:09 IST
=> Training   8.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.472 Prec@1=64.582 Prec@5=85.564 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=06:09 IST
=> Training   8.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.472 Prec@1=64.582 Prec@5=85.564 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=06:10 IST
=> Training   8.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.573 DataTime=0.399 Loss=1.473 Prec@1=64.536 Prec@5=85.636 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=06:10 IST
=> Training   12.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.573 DataTime=0.399 Loss=1.473 Prec@1=64.536 Prec@5=85.636 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=06:10 IST
=> Training   12.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.573 DataTime=0.399 Loss=1.473 Prec@1=64.536 Prec@5=85.636 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=06:11 IST
=> Training   12.03% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.474 Prec@1=64.516 Prec@5=85.613 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=06:11 IST
=> Training   16.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.474 Prec@1=64.516 Prec@5=85.613 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=06:11 IST
=> Training   16.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.572 DataTime=0.400 Loss=1.474 Prec@1=64.516 Prec@5=85.613 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=06:12 IST
=> Training   16.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.477 Prec@1=64.479 Prec@5=85.573 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=06:12 IST
=> Training   20.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.477 Prec@1=64.479 Prec@5=85.573 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=06:12 IST
=> Training   20.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.477 Prec@1=64.479 Prec@5=85.573 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=06:13 IST
=> Training   20.02% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.480 Prec@1=64.420 Prec@5=85.566 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=06:13 IST
=> Training   24.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.480 Prec@1=64.420 Prec@5=85.566 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:13 IST
=> Training   24.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.568 DataTime=0.397 Loss=1.480 Prec@1=64.420 Prec@5=85.566 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:14 IST
=> Training   24.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.483 Prec@1=64.312 Prec@5=85.534 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:14 IST
=> Training   28.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.483 Prec@1=64.312 Prec@5=85.534 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=06:14 IST
=> Training   28.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.483 Prec@1=64.312 Prec@5=85.534 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=06:15 IST
=> Training   28.01% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.484 Prec@1=64.295 Prec@5=85.509 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=06:15 IST
=> Training   32.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.484 Prec@1=64.295 Prec@5=85.509 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=06:15 IST
=> Training   32.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.484 Prec@1=64.295 Prec@5=85.509 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=06:16 IST
=> Training   32.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.486 Prec@1=64.250 Prec@5=85.482 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=06:16 IST
=> Training   36.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.486 Prec@1=64.250 Prec@5=85.482 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=06:16 IST
=> Training   36.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.486 Prec@1=64.250 Prec@5=85.482 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=06:17 IST
=> Training   36.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.489 Prec@1=64.173 Prec@5=85.445 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=06:17 IST
=> Training   39.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.489 Prec@1=64.173 Prec@5=85.445 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=06:17 IST
=> Training   39.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.489 Prec@1=64.173 Prec@5=85.445 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=06:18 IST
=> Training   39.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.567 DataTime=0.396 Loss=1.491 Prec@1=64.119 Prec@5=85.397 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=06:18 IST
=> Training   43.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.567 DataTime=0.396 Loss=1.491 Prec@1=64.119 Prec@5=85.397 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=06:18 IST
=> Training   43.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.567 DataTime=0.396 Loss=1.491 Prec@1=64.119 Prec@5=85.397 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=06:18 IST
=> Training   43.99% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.105 Prec@5=85.388 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=06:18 IST
=> Training   47.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.105 Prec@5=85.388 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=06:18 IST
=> Training   47.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.105 Prec@5=85.388 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=06:19 IST
=> Training   47.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.082 Prec@5=85.377 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=06:19 IST
=> Training   51.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.082 Prec@5=85.377 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=06:19 IST
=> Training   51.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.492 Prec@1=64.082 Prec@5=85.377 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=06:20 IST
=> Training   51.98% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.493 Prec@1=64.042 Prec@5=85.366 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=06:20 IST
=> Training   55.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.493 Prec@1=64.042 Prec@5=85.366 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=06:20 IST
=> Training   55.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.493 Prec@1=64.042 Prec@5=85.366 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=06:21 IST
=> Training   55.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.494 Prec@1=64.017 Prec@5=85.356 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=06:21 IST
=> Training   59.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.494 Prec@1=64.017 Prec@5=85.356 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=06:21 IST
=> Training   59.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.395 Loss=1.494 Prec@1=64.017 Prec@5=85.356 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=06:22 IST
=> Training   59.97% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.495 Prec@1=63.980 Prec@5=85.338 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=06:22 IST
=> Training   63.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.495 Prec@1=63.980 Prec@5=85.338 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=06:22 IST
=> Training   63.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.495 Prec@1=63.980 Prec@5=85.338 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=06:23 IST
=> Training   63.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.497 Prec@1=63.973 Prec@5=85.319 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=06:23 IST
=> Training   67.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.497 Prec@1=63.973 Prec@5=85.319 rate=1.78 Hz, eta=0:07:31, total=0:15:58, wall=06:23 IST
=> Training   67.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.394 Loss=1.497 Prec@1=63.973 Prec@5=85.319 rate=1.78 Hz, eta=0:07:31, total=0:15:58, wall=06:24 IST
=> Training   67.96% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.498 Prec@1=63.934 Prec@5=85.305 rate=1.78 Hz, eta=0:07:31, total=0:15:58, wall=06:24 IST
=> Training   71.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.498 Prec@1=63.934 Prec@5=85.305 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:24 IST
=> Training   71.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.498 Prec@1=63.934 Prec@5=85.305 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:25 IST
=> Training   71.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.499 Prec@1=63.911 Prec@5=85.289 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:25 IST
=> Training   75.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.499 Prec@1=63.911 Prec@5=85.289 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:25 IST
=> Training   75.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.499 Prec@1=63.911 Prec@5=85.289 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:26 IST
=> Training   75.95% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.500 Prec@1=63.888 Prec@5=85.271 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:26 IST
=> Training   79.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.500 Prec@1=63.888 Prec@5=85.271 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:26 IST
=> Training   79.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.500 Prec@1=63.888 Prec@5=85.271 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:27 IST
=> Training   79.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.502 Prec@1=63.861 Prec@5=85.247 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:27 IST
=> Training   83.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.502 Prec@1=63.861 Prec@5=85.247 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=06:27 IST
=> Training   83.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.393 Loss=1.502 Prec@1=63.861 Prec@5=85.247 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=06:28 IST
=> Training   83.94% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.503 Prec@1=63.842 Prec@5=85.229 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=06:28 IST
=> Training   87.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.503 Prec@1=63.842 Prec@5=85.229 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:28 IST
=> Training   87.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.394 Loss=1.503 Prec@1=63.842 Prec@5=85.229 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:29 IST
=> Training   87.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.504 Prec@1=63.830 Prec@5=85.221 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:29 IST
=> Training   91.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.504 Prec@1=63.830 Prec@5=85.221 rate=1.78 Hz, eta=0:01:53, total=0:21:36, wall=06:29 IST
=> Training   91.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.566 DataTime=0.393 Loss=1.504 Prec@1=63.830 Prec@5=85.221 rate=1.78 Hz, eta=0:01:53, total=0:21:36, wall=06:30 IST
=> Training   91.93% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.392 Loss=1.504 Prec@1=63.819 Prec@5=85.211 rate=1.78 Hz, eta=0:01:53, total=0:21:36, wall=06:30 IST
=> Training   95.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.392 Loss=1.504 Prec@1=63.819 Prec@5=85.211 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=06:30 IST
=> Training   95.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.565 DataTime=0.392 Loss=1.504 Prec@1=63.819 Prec@5=85.211 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=06:31 IST
=> Training   95.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.505 Prec@1=63.797 Prec@5=85.202 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=06:31 IST
=> Training   99.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.505 Prec@1=63.797 Prec@5=85.202 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=06:31 IST
=> Training   99.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.505 Prec@1=63.797 Prec@5=85.202 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=06:31 IST
=> Training   99.92% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.505 Prec@1=63.797 Prec@5=85.202 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=06:31 IST
=> Training   100.00% of 1x2503...Epoch=37/150 LR=0.0864 Time=0.564 DataTime=0.392 Loss=1.505 Prec@1=63.797 Prec@5=85.202 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=06:31 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST
=> Validation 0.00% of 1x98...Epoch=37/150 LR=0.0864 Time=7.168 Loss=0.933 Prec@1=75.195 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=06:31 IST
=> Validation 1.02% of 1x98...Epoch=37/150 LR=0.0864 Time=7.168 Loss=0.933 Prec@1=75.195 Prec@5=93.555 rate=7041.81 Hz, eta=0:00:00, total=0:00:00, wall=06:31 IST
** Validation 1.02% of 1x98...Epoch=37/150 LR=0.0864 Time=7.168 Loss=0.933 Prec@1=75.195 Prec@5=93.555 rate=7041.81 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST
** Validation 1.02% of 1x98...Epoch=37/150 LR=0.0864 Time=0.635 Loss=1.585 Prec@1=61.792 Prec@5=84.548 rate=7041.81 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST
** Validation 100.00% of 1x98...Epoch=37/150 LR=0.0864 Time=0.635 Loss=1.585 Prec@1=61.792 Prec@5=84.548 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=06:32 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST
=> Training   0.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=5.419 DataTime=5.047 Loss=1.388 Prec@1=65.039 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=06:32 IST
=> Training   0.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=5.419 DataTime=5.047 Loss=1.388 Prec@1=65.039 Prec@5=87.500 rate=10189.73 Hz, eta=0:00:00, total=0:00:00, wall=06:32 IST
=> Training   0.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=5.419 DataTime=5.047 Loss=1.388 Prec@1=65.039 Prec@5=87.500 rate=10189.73 Hz, eta=0:00:00, total=0:00:00, wall=06:33 IST
=> Training   0.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.601 DataTime=0.432 Loss=1.459 Prec@1=64.782 Prec@5=85.762 rate=10189.73 Hz, eta=0:00:00, total=0:00:00, wall=06:33 IST
=> Training   4.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.601 DataTime=0.432 Loss=1.459 Prec@1=64.782 Prec@5=85.762 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=06:33 IST
=> Training   4.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.601 DataTime=0.432 Loss=1.459 Prec@1=64.782 Prec@5=85.762 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=06:34 IST
=> Training   4.04% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.580 DataTime=0.404 Loss=1.458 Prec@1=64.640 Prec@5=85.844 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=06:34 IST
=> Training   8.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.580 DataTime=0.404 Loss=1.458 Prec@1=64.640 Prec@5=85.844 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:34 IST
=> Training   8.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.580 DataTime=0.404 Loss=1.458 Prec@1=64.640 Prec@5=85.844 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:35 IST
=> Training   8.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.572 DataTime=0.398 Loss=1.463 Prec@1=64.584 Prec@5=85.736 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:35 IST
=> Training   12.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.572 DataTime=0.398 Loss=1.463 Prec@1=64.584 Prec@5=85.736 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=06:35 IST
=> Training   12.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.572 DataTime=0.398 Loss=1.463 Prec@1=64.584 Prec@5=85.736 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=06:36 IST
=> Training   12.03% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.568 DataTime=0.394 Loss=1.464 Prec@1=64.562 Prec@5=85.686 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=06:36 IST
=> Training   16.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.568 DataTime=0.394 Loss=1.464 Prec@1=64.562 Prec@5=85.686 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=06:36 IST
=> Training   16.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.568 DataTime=0.394 Loss=1.464 Prec@1=64.562 Prec@5=85.686 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=06:36 IST
=> Training   16.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.567 DataTime=0.394 Loss=1.467 Prec@1=64.494 Prec@5=85.676 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=06:36 IST
=> Training   20.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.567 DataTime=0.394 Loss=1.467 Prec@1=64.494 Prec@5=85.676 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=06:36 IST
=> Training   20.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.567 DataTime=0.394 Loss=1.467 Prec@1=64.494 Prec@5=85.676 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=06:37 IST
=> Training   20.02% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.564 DataTime=0.391 Loss=1.469 Prec@1=64.460 Prec@5=85.647 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=06:37 IST
=> Training   24.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.564 DataTime=0.391 Loss=1.469 Prec@1=64.460 Prec@5=85.647 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=06:37 IST
=> Training   24.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.564 DataTime=0.391 Loss=1.469 Prec@1=64.460 Prec@5=85.647 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=06:38 IST
=> Training   24.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.565 DataTime=0.392 Loss=1.472 Prec@1=64.433 Prec@5=85.630 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=06:38 IST
=> Training   28.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.565 DataTime=0.392 Loss=1.472 Prec@1=64.433 Prec@5=85.630 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=06:38 IST
=> Training   28.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.565 DataTime=0.392 Loss=1.472 Prec@1=64.433 Prec@5=85.630 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=06:39 IST
=> Training   28.01% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.389 Loss=1.475 Prec@1=64.388 Prec@5=85.598 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=06:39 IST
=> Training   32.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.389 Loss=1.475 Prec@1=64.388 Prec@5=85.598 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=06:39 IST
=> Training   32.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.389 Loss=1.475 Prec@1=64.388 Prec@5=85.598 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=06:40 IST
=> Training   32.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.389 Loss=1.477 Prec@1=64.320 Prec@5=85.579 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=06:40 IST
=> Training   36.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.389 Loss=1.477 Prec@1=64.320 Prec@5=85.579 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=06:40 IST
=> Training   36.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.389 Loss=1.477 Prec@1=64.320 Prec@5=85.579 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=06:41 IST
=> Training   36.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.481 Prec@1=64.268 Prec@5=85.532 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=06:41 IST
=> Training   39.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.481 Prec@1=64.268 Prec@5=85.532 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:41 IST
=> Training   39.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.481 Prec@1=64.268 Prec@5=85.532 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:42 IST
=> Training   39.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.388 Loss=1.481 Prec@1=64.256 Prec@5=85.529 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:42 IST
=> Training   43.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.388 Loss=1.481 Prec@1=64.256 Prec@5=85.529 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:42 IST
=> Training   43.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.563 DataTime=0.388 Loss=1.481 Prec@1=64.256 Prec@5=85.529 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:43 IST
=> Training   43.99% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.482 Prec@1=64.247 Prec@5=85.526 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:43 IST
=> Training   47.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.482 Prec@1=64.247 Prec@5=85.526 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:43 IST
=> Training   47.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.561 DataTime=0.387 Loss=1.482 Prec@1=64.247 Prec@5=85.526 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:44 IST
=> Training   47.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.388 Loss=1.484 Prec@1=64.188 Prec@5=85.487 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:44 IST
=> Training   51.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.388 Loss=1.484 Prec@1=64.188 Prec@5=85.487 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=06:44 IST
=> Training   51.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.562 DataTime=0.388 Loss=1.484 Prec@1=64.188 Prec@5=85.487 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=06:45 IST
=> Training   51.98% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.486 Prec@1=64.155 Prec@5=85.461 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=06:45 IST
=> Training   55.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.486 Prec@1=64.155 Prec@5=85.461 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=06:45 IST
=> Training   55.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.486 Prec@1=64.155 Prec@5=85.461 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=06:46 IST
=> Training   55.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.488 Prec@1=64.134 Prec@5=85.441 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=06:46 IST
=> Training   59.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.488 Prec@1=64.134 Prec@5=85.441 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=06:46 IST
=> Training   59.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.386 Loss=1.488 Prec@1=64.134 Prec@5=85.441 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=06:47 IST
=> Training   59.97% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.489 Prec@1=64.108 Prec@5=85.421 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=06:47 IST
=> Training   63.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.489 Prec@1=64.108 Prec@5=85.421 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=06:47 IST
=> Training   63.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.489 Prec@1=64.108 Prec@5=85.421 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=06:48 IST
=> Training   63.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.491 Prec@1=64.077 Prec@5=85.394 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=06:48 IST
=> Training   67.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.491 Prec@1=64.077 Prec@5=85.394 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=06:48 IST
=> Training   67.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.491 Prec@1=64.077 Prec@5=85.394 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=06:49 IST
=> Training   67.96% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.492 Prec@1=64.058 Prec@5=85.373 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=06:49 IST
=> Training   71.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.492 Prec@1=64.058 Prec@5=85.373 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=06:49 IST
=> Training   71.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.492 Prec@1=64.058 Prec@5=85.373 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=06:49 IST
=> Training   71.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.493 Prec@1=64.029 Prec@5=85.364 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=06:49 IST
=> Training   75.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.493 Prec@1=64.029 Prec@5=85.364 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=06:49 IST
=> Training   75.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.493 Prec@1=64.029 Prec@5=85.364 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=06:50 IST
=> Training   75.95% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.495 Prec@1=63.997 Prec@5=85.336 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=06:50 IST
=> Training   79.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.495 Prec@1=63.997 Prec@5=85.336 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=06:50 IST
=> Training   79.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.495 Prec@1=63.997 Prec@5=85.336 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=06:51 IST
=> Training   79.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.387 Loss=1.497 Prec@1=63.974 Prec@5=85.315 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=06:51 IST
=> Training   83.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.387 Loss=1.497 Prec@1=63.974 Prec@5=85.315 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:51 IST
=> Training   83.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.560 DataTime=0.387 Loss=1.497 Prec@1=63.974 Prec@5=85.315 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:52 IST
=> Training   83.94% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.966 Prec@5=85.317 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:52 IST
=> Training   87.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.966 Prec@5=85.317 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=06:52 IST
=> Training   87.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.966 Prec@5=85.317 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=06:53 IST
=> Training   87.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.959 Prec@5=85.321 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=06:53 IST
=> Training   91.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.959 Prec@5=85.321 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=06:53 IST
=> Training   91.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.497 Prec@1=63.959 Prec@5=85.321 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=06:54 IST
=> Training   91.93% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.499 Prec@1=63.924 Prec@5=85.288 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=06:54 IST
=> Training   95.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.499 Prec@1=63.924 Prec@5=85.288 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=06:54 IST
=> Training   95.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.387 Loss=1.499 Prec@1=63.924 Prec@5=85.288 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=06:55 IST
=> Training   95.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.500 Prec@1=63.917 Prec@5=85.276 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=06:55 IST
=> Training   99.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.500 Prec@1=63.917 Prec@5=85.276 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:55 IST
=> Training   99.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.500 Prec@1=63.917 Prec@5=85.276 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:55 IST
=> Training   99.92% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.500 Prec@1=63.917 Prec@5=85.277 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=06:55 IST
=> Training   100.00% of 1x2503...Epoch=38/150 LR=0.0857 Time=0.559 DataTime=0.386 Loss=1.500 Prec@1=63.917 Prec@5=85.277 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=06:55 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:55 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:55 IST
=> Validation 0.00% of 1x98...Epoch=38/150 LR=0.0857 Time=7.007 Loss=0.899 Prec@1=76.172 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=06:55 IST
=> Validation 1.02% of 1x98...Epoch=38/150 LR=0.0857 Time=7.007 Loss=0.899 Prec@1=76.172 Prec@5=92.383 rate=4738.87 Hz, eta=0:00:00, total=0:00:00, wall=06:55 IST
** Validation 1.02% of 1x98...Epoch=38/150 LR=0.0857 Time=7.007 Loss=0.899 Prec@1=76.172 Prec@5=92.383 rate=4738.87 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
** Validation 1.02% of 1x98...Epoch=38/150 LR=0.0857 Time=0.654 Loss=1.559 Prec@1=62.554 Prec@5=84.982 rate=4738.87 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
** Validation 100.00% of 1x98...Epoch=38/150 LR=0.0857 Time=0.654 Loss=1.559 Prec@1=62.554 Prec@5=84.982 rate=1.72 Hz, eta=0:00:00, total=0:00:57, wall=06:56 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> Training   0.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=6.051 DataTime=5.816 Loss=1.522 Prec@1=65.625 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=06:56 IST
=> Training   0.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=6.051 DataTime=5.816 Loss=1.522 Prec@1=65.625 Prec@5=86.523 rate=3944.15 Hz, eta=0:00:00, total=0:00:00, wall=06:56 IST
=> Training   0.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=6.051 DataTime=5.816 Loss=1.522 Prec@1=65.625 Prec@5=86.523 rate=3944.15 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST
=> Training   0.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.613 DataTime=0.449 Loss=1.450 Prec@1=64.960 Prec@5=85.943 rate=3944.15 Hz, eta=0:00:00, total=0:00:00, wall=06:57 IST
=> Training   4.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.613 DataTime=0.449 Loss=1.450 Prec@1=64.960 Prec@5=85.943 rate=1.81 Hz, eta=0:22:10, total=0:00:55, wall=06:57 IST
=> Training   4.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.613 DataTime=0.449 Loss=1.450 Prec@1=64.960 Prec@5=85.943 rate=1.81 Hz, eta=0:22:10, total=0:00:55, wall=06:58 IST
=> Training   4.04% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.583 DataTime=0.416 Loss=1.452 Prec@1=65.046 Prec@5=85.939 rate=1.81 Hz, eta=0:22:10, total=0:00:55, wall=06:58 IST
=> Training   8.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.583 DataTime=0.416 Loss=1.452 Prec@1=65.046 Prec@5=85.939 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:58 IST
=> Training   8.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.583 DataTime=0.416 Loss=1.452 Prec@1=65.046 Prec@5=85.939 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:59 IST
=> Training   8.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.454 Prec@1=65.014 Prec@5=85.887 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:59 IST
=> Training   12.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.454 Prec@1=65.014 Prec@5=85.887 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=06:59 IST
=> Training   12.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.454 Prec@1=65.014 Prec@5=85.887 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=07:00 IST
=> Training   12.03% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.456 Prec@1=64.891 Prec@5=85.870 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=07:00 IST
=> Training   16.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.456 Prec@1=64.891 Prec@5=85.870 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=07:00 IST
=> Training   16.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.573 DataTime=0.408 Loss=1.456 Prec@1=64.891 Prec@5=85.870 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=07:01 IST
=> Training   16.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.404 Loss=1.459 Prec@1=64.816 Prec@5=85.837 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=07:01 IST
=> Training   20.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.404 Loss=1.459 Prec@1=64.816 Prec@5=85.837 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:01 IST
=> Training   20.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.404 Loss=1.459 Prec@1=64.816 Prec@5=85.837 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:02 IST
=> Training   20.02% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.460 Prec@1=64.752 Prec@5=85.825 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:02 IST
=> Training   24.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.460 Prec@1=64.752 Prec@5=85.825 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:02 IST
=> Training   24.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.460 Prec@1=64.752 Prec@5=85.825 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:03 IST
=> Training   24.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.571 DataTime=0.404 Loss=1.461 Prec@1=64.699 Prec@5=85.823 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:03 IST
=> Training   28.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.571 DataTime=0.404 Loss=1.461 Prec@1=64.699 Prec@5=85.823 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=07:03 IST
=> Training   28.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.571 DataTime=0.404 Loss=1.461 Prec@1=64.699 Prec@5=85.823 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=07:04 IST
=> Training   28.01% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.403 Loss=1.463 Prec@1=64.642 Prec@5=85.801 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=07:04 IST
=> Training   32.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.403 Loss=1.463 Prec@1=64.642 Prec@5=85.801 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=07:04 IST
=> Training   32.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.570 DataTime=0.403 Loss=1.463 Prec@1=64.642 Prec@5=85.801 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=07:05 IST
=> Training   32.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.560 Prec@5=85.740 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=07:05 IST
=> Training   36.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.560 Prec@5=85.740 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=07:05 IST
=> Training   36.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.569 DataTime=0.402 Loss=1.467 Prec@1=64.560 Prec@5=85.740 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=07:06 IST
=> Training   36.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.399 Loss=1.469 Prec@1=64.518 Prec@5=85.706 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=07:06 IST
=> Training   39.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.399 Loss=1.469 Prec@1=64.518 Prec@5=85.706 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=07:06 IST
=> Training   39.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.399 Loss=1.469 Prec@1=64.518 Prec@5=85.706 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=07:07 IST
=> Training   39.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.400 Loss=1.471 Prec@1=64.466 Prec@5=85.682 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=07:07 IST
=> Training   43.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.400 Loss=1.471 Prec@1=64.466 Prec@5=85.682 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=07:07 IST
=> Training   43.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.567 DataTime=0.400 Loss=1.471 Prec@1=64.466 Prec@5=85.682 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=07:07 IST
=> Training   43.99% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.398 Loss=1.473 Prec@1=64.412 Prec@5=85.662 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=07:07 IST
=> Training   47.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.398 Loss=1.473 Prec@1=64.412 Prec@5=85.662 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=07:07 IST
=> Training   47.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.398 Loss=1.473 Prec@1=64.412 Prec@5=85.662 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=07:08 IST
=> Training   47.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.397 Loss=1.475 Prec@1=64.386 Prec@5=85.637 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=07:08 IST
=> Training   51.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.397 Loss=1.475 Prec@1=64.386 Prec@5=85.637 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=07:08 IST
=> Training   51.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.566 DataTime=0.397 Loss=1.475 Prec@1=64.386 Prec@5=85.637 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=07:09 IST
=> Training   51.98% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.396 Loss=1.476 Prec@1=64.362 Prec@5=85.617 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=07:09 IST
=> Training   55.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.396 Loss=1.476 Prec@1=64.362 Prec@5=85.617 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=07:09 IST
=> Training   55.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.396 Loss=1.476 Prec@1=64.362 Prec@5=85.617 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=07:10 IST
=> Training   55.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.395 Loss=1.478 Prec@1=64.331 Prec@5=85.584 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=07:10 IST
=> Training   59.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.395 Loss=1.478 Prec@1=64.331 Prec@5=85.584 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=07:10 IST
=> Training   59.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.395 Loss=1.478 Prec@1=64.331 Prec@5=85.584 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=07:11 IST
=> Training   59.97% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.394 Loss=1.479 Prec@1=64.293 Prec@5=85.552 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=07:11 IST
=> Training   63.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.394 Loss=1.479 Prec@1=64.293 Prec@5=85.552 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=07:11 IST
=> Training   63.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.394 Loss=1.479 Prec@1=64.293 Prec@5=85.552 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=07:12 IST
=> Training   63.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.394 Loss=1.480 Prec@1=64.255 Prec@5=85.528 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=07:12 IST
=> Training   67.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.394 Loss=1.480 Prec@1=64.255 Prec@5=85.528 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:12 IST
=> Training   67.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.565 DataTime=0.394 Loss=1.480 Prec@1=64.255 Prec@5=85.528 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:13 IST
=> Training   67.96% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.393 Loss=1.482 Prec@1=64.218 Prec@5=85.501 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:13 IST
=> Training   71.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.393 Loss=1.482 Prec@1=64.218 Prec@5=85.501 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:13 IST
=> Training   71.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.393 Loss=1.482 Prec@1=64.218 Prec@5=85.501 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:14 IST
=> Training   71.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.484 Prec@1=64.173 Prec@5=85.480 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:14 IST
=> Training   75.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.484 Prec@1=64.173 Prec@5=85.480 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=07:14 IST
=> Training   75.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.484 Prec@1=64.173 Prec@5=85.480 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=07:15 IST
=> Training   75.95% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.485 Prec@1=64.156 Prec@5=85.465 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=07:15 IST
=> Training   79.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.485 Prec@1=64.156 Prec@5=85.465 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:15 IST
=> Training   79.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.392 Loss=1.485 Prec@1=64.156 Prec@5=85.465 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:16 IST
=> Training   79.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.391 Loss=1.487 Prec@1=64.119 Prec@5=85.443 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:16 IST
=> Training   83.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.391 Loss=1.487 Prec@1=64.119 Prec@5=85.443 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:16 IST
=> Training   83.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.564 DataTime=0.391 Loss=1.487 Prec@1=64.119 Prec@5=85.443 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:17 IST
=> Training   83.94% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.391 Loss=1.487 Prec@1=64.102 Prec@5=85.428 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:17 IST
=> Training   87.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.391 Loss=1.487 Prec@1=64.102 Prec@5=85.428 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:17 IST
=> Training   87.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.391 Loss=1.487 Prec@1=64.102 Prec@5=85.428 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:18 IST
=> Training   87.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.488 Prec@1=64.096 Prec@5=85.417 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:18 IST
=> Training   91.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.488 Prec@1=64.096 Prec@5=85.417 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:18 IST
=> Training   91.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.488 Prec@1=64.096 Prec@5=85.417 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:19 IST
=> Training   91.93% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.489 Prec@1=64.078 Prec@5=85.406 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:19 IST
=> Training   95.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.489 Prec@1=64.078 Prec@5=85.406 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=07:19 IST
=> Training   95.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.563 DataTime=0.390 Loss=1.489 Prec@1=64.078 Prec@5=85.406 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=07:20 IST
=> Training   95.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.562 DataTime=0.390 Loss=1.491 Prec@1=64.067 Prec@5=85.389 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=07:20 IST
=> Training   99.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.562 DataTime=0.390 Loss=1.491 Prec@1=64.067 Prec@5=85.389 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:20 IST
=> Training   99.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.562 DataTime=0.390 Loss=1.491 Prec@1=64.067 Prec@5=85.389 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:20 IST
=> Training   99.92% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.562 DataTime=0.389 Loss=1.491 Prec@1=64.067 Prec@5=85.389 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:20 IST
=> Training   100.00% of 1x2503...Epoch=39/150 LR=0.0850 Time=0.562 DataTime=0.389 Loss=1.491 Prec@1=64.067 Prec@5=85.389 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=07:20 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:20 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:20 IST
=> Validation 0.00% of 1x98...Epoch=39/150 LR=0.0850 Time=6.917 Loss=1.062 Prec@1=74.023 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=07:20 IST
=> Validation 1.02% of 1x98...Epoch=39/150 LR=0.0850 Time=6.917 Loss=1.062 Prec@1=74.023 Prec@5=91.992 rate=812.73 Hz, eta=0:00:00, total=0:00:00, wall=07:20 IST
** Validation 1.02% of 1x98...Epoch=39/150 LR=0.0850 Time=6.917 Loss=1.062 Prec@1=74.023 Prec@5=91.992 rate=812.73 Hz, eta=0:00:00, total=0:00:00, wall=07:21 IST
** Validation 1.02% of 1x98...Epoch=39/150 LR=0.0850 Time=0.633 Loss=1.592 Prec@1=62.162 Prec@5=84.466 rate=812.73 Hz, eta=0:00:00, total=0:00:00, wall=07:21 IST
** Validation 100.00% of 1x98...Epoch=39/150 LR=0.0850 Time=0.633 Loss=1.592 Prec@1=62.162 Prec@5=84.466 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=07:21 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:21 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:21 IST
=> Training   0.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=4.499 DataTime=4.173 Loss=1.530 Prec@1=65.039 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=07:21 IST
=> Training   0.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=4.499 DataTime=4.173 Loss=1.530 Prec@1=65.039 Prec@5=84.570 rate=403.34 Hz, eta=0:00:06, total=0:00:00, wall=07:21 IST
=> Training   0.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=4.499 DataTime=4.173 Loss=1.530 Prec@1=65.039 Prec@5=84.570 rate=403.34 Hz, eta=0:00:06, total=0:00:00, wall=07:22 IST
=> Training   0.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.588 DataTime=0.419 Loss=1.433 Prec@1=65.219 Prec@5=86.158 rate=403.34 Hz, eta=0:00:06, total=0:00:00, wall=07:22 IST
=> Training   4.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.588 DataTime=0.419 Loss=1.433 Prec@1=65.219 Prec@5=86.158 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=07:22 IST
=> Training   4.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.588 DataTime=0.419 Loss=1.433 Prec@1=65.219 Prec@5=86.158 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=07:23 IST
=> Training   4.04% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.574 DataTime=0.402 Loss=1.438 Prec@1=65.122 Prec@5=86.163 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=07:23 IST
=> Training   8.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.574 DataTime=0.402 Loss=1.438 Prec@1=65.122 Prec@5=86.163 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:23 IST
=> Training   8.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.574 DataTime=0.402 Loss=1.438 Prec@1=65.122 Prec@5=86.163 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:24 IST
=> Training   8.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.571 DataTime=0.396 Loss=1.439 Prec@1=65.135 Prec@5=86.187 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:24 IST
=> Training   12.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.571 DataTime=0.396 Loss=1.439 Prec@1=65.135 Prec@5=86.187 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=07:24 IST
=> Training   12.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.571 DataTime=0.396 Loss=1.439 Prec@1=65.135 Prec@5=86.187 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=07:24 IST
=> Training   12.03% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.566 DataTime=0.390 Loss=1.442 Prec@1=65.029 Prec@5=86.107 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=07:24 IST
=> Training   16.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.566 DataTime=0.390 Loss=1.442 Prec@1=65.029 Prec@5=86.107 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=07:24 IST
=> Training   16.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.566 DataTime=0.390 Loss=1.442 Prec@1=65.029 Prec@5=86.107 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=07:25 IST
=> Training   16.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.389 Loss=1.445 Prec@1=64.961 Prec@5=86.039 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=07:25 IST
=> Training   20.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.389 Loss=1.445 Prec@1=64.961 Prec@5=86.039 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=07:25 IST
=> Training   20.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.389 Loss=1.445 Prec@1=64.961 Prec@5=86.039 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=07:26 IST
=> Training   20.02% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.567 DataTime=0.393 Loss=1.448 Prec@1=64.941 Prec@5=85.995 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=07:26 IST
=> Training   24.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.567 DataTime=0.393 Loss=1.448 Prec@1=64.941 Prec@5=85.995 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=07:26 IST
=> Training   24.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.567 DataTime=0.393 Loss=1.448 Prec@1=64.941 Prec@5=85.995 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=07:27 IST
=> Training   24.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.452 Prec@1=64.867 Prec@5=85.938 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=07:27 IST
=> Training   28.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.452 Prec@1=64.867 Prec@5=85.938 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=07:27 IST
=> Training   28.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.452 Prec@1=64.867 Prec@5=85.938 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=07:28 IST
=> Training   28.01% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.455 Prec@1=64.822 Prec@5=85.894 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=07:28 IST
=> Training   32.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.455 Prec@1=64.822 Prec@5=85.894 rate=1.79 Hz, eta=0:15:51, total=0:07:28, wall=07:28 IST
=> Training   32.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.565 DataTime=0.391 Loss=1.455 Prec@1=64.822 Prec@5=85.894 rate=1.79 Hz, eta=0:15:51, total=0:07:28, wall=07:29 IST
=> Training   32.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.389 Loss=1.458 Prec@1=64.752 Prec@5=85.848 rate=1.79 Hz, eta=0:15:51, total=0:07:28, wall=07:29 IST
=> Training   36.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.389 Loss=1.458 Prec@1=64.752 Prec@5=85.848 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=07:29 IST
=> Training   36.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.389 Loss=1.458 Prec@1=64.752 Prec@5=85.848 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=07:30 IST
=> Training   36.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.461 Prec@1=64.722 Prec@5=85.811 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=07:30 IST
=> Training   39.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.461 Prec@1=64.722 Prec@5=85.811 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=07:30 IST
=> Training   39.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.461 Prec@1=64.722 Prec@5=85.811 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=07:31 IST
=> Training   39.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.463 Prec@1=64.690 Prec@5=85.786 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=07:31 IST
=> Training   43.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.463 Prec@1=64.690 Prec@5=85.786 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=07:31 IST
=> Training   43.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.463 Prec@1=64.690 Prec@5=85.786 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=07:32 IST
=> Training   43.99% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.464 Prec@1=64.656 Prec@5=85.783 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=07:32 IST
=> Training   47.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.464 Prec@1=64.656 Prec@5=85.783 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=07:32 IST
=> Training   47.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.390 Loss=1.464 Prec@1=64.656 Prec@5=85.783 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=07:33 IST
=> Training   47.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.388 Loss=1.466 Prec@1=64.627 Prec@5=85.766 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=07:33 IST
=> Training   51.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.388 Loss=1.466 Prec@1=64.627 Prec@5=85.766 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=07:33 IST
=> Training   51.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.388 Loss=1.466 Prec@1=64.627 Prec@5=85.766 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=07:34 IST
=> Training   51.98% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.468 Prec@1=64.567 Prec@5=85.730 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=07:34 IST
=> Training   55.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.468 Prec@1=64.567 Prec@5=85.730 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=07:34 IST
=> Training   55.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.468 Prec@1=64.567 Prec@5=85.730 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=07:35 IST
=> Training   55.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.471 Prec@1=64.518 Prec@5=85.697 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=07:35 IST
=> Training   59.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.471 Prec@1=64.518 Prec@5=85.697 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=07:35 IST
=> Training   59.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.471 Prec@1=64.518 Prec@5=85.697 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=07:36 IST
=> Training   59.97% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.472 Prec@1=64.487 Prec@5=85.681 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=07:36 IST
=> Training   63.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.472 Prec@1=64.487 Prec@5=85.681 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=07:36 IST
=> Training   63.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.472 Prec@1=64.487 Prec@5=85.681 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=07:37 IST
=> Training   63.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.391 Loss=1.474 Prec@1=64.440 Prec@5=85.657 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=07:37 IST
=> Training   67.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.391 Loss=1.474 Prec@1=64.440 Prec@5=85.657 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:37 IST
=> Training   67.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.564 DataTime=0.391 Loss=1.474 Prec@1=64.440 Prec@5=85.657 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:38 IST
=> Training   67.96% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.476 Prec@1=64.390 Prec@5=85.644 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=07:38 IST
=> Training   71.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.476 Prec@1=64.390 Prec@5=85.644 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:38 IST
=> Training   71.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.476 Prec@1=64.390 Prec@5=85.644 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:39 IST
=> Training   71.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.477 Prec@1=64.369 Prec@5=85.611 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=07:39 IST
=> Training   75.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.477 Prec@1=64.369 Prec@5=85.611 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=07:39 IST
=> Training   75.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.477 Prec@1=64.369 Prec@5=85.611 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=07:39 IST
=> Training   75.95% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.479 Prec@1=64.337 Prec@5=85.596 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=07:39 IST
=> Training   79.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.479 Prec@1=64.337 Prec@5=85.596 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=07:39 IST
=> Training   79.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.391 Loss=1.479 Prec@1=64.337 Prec@5=85.596 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=07:40 IST
=> Training   79.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.480 Prec@1=64.334 Prec@5=85.578 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=07:40 IST
=> Training   83.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.480 Prec@1=64.334 Prec@5=85.578 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:40 IST
=> Training   83.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.480 Prec@1=64.334 Prec@5=85.578 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:41 IST
=> Training   83.94% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.480 Prec@1=64.313 Prec@5=85.559 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:41 IST
=> Training   87.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.480 Prec@1=64.313 Prec@5=85.559 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:41 IST
=> Training   87.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.480 Prec@1=64.313 Prec@5=85.559 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:42 IST
=> Training   87.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.482 Prec@1=64.272 Prec@5=85.537 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:42 IST
=> Training   91.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.482 Prec@1=64.272 Prec@5=85.537 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:42 IST
=> Training   91.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.390 Loss=1.482 Prec@1=64.272 Prec@5=85.537 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:43 IST
=> Training   91.93% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.483 Prec@1=64.253 Prec@5=85.525 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=07:43 IST
=> Training   95.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.483 Prec@1=64.253 Prec@5=85.525 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=07:43 IST
=> Training   95.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.563 DataTime=0.390 Loss=1.483 Prec@1=64.253 Prec@5=85.525 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=07:44 IST
=> Training   95.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.389 Loss=1.484 Prec@1=64.233 Prec@5=85.507 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=07:44 IST
=> Training   99.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.389 Loss=1.484 Prec@1=64.233 Prec@5=85.507 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:44 IST
=> Training   99.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.562 DataTime=0.389 Loss=1.484 Prec@1=64.233 Prec@5=85.507 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:44 IST
=> Training   99.92% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.561 DataTime=0.389 Loss=1.484 Prec@1=64.234 Prec@5=85.508 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=07:44 IST
=> Training   100.00% of 1x2503...Epoch=40/150 LR=0.0842 Time=0.561 DataTime=0.389 Loss=1.484 Prec@1=64.234 Prec@5=85.508 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=07:44 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:44 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:44 IST
=> Validation 0.00% of 1x98...Epoch=40/150 LR=0.0842 Time=6.957 Loss=0.964 Prec@1=73.438 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=07:44 IST
=> Validation 1.02% of 1x98...Epoch=40/150 LR=0.0842 Time=6.957 Loss=0.964 Prec@1=73.438 Prec@5=92.383 rate=2783.72 Hz, eta=0:00:00, total=0:00:00, wall=07:44 IST
** Validation 1.02% of 1x98...Epoch=40/150 LR=0.0842 Time=6.957 Loss=0.964 Prec@1=73.438 Prec@5=92.383 rate=2783.72 Hz, eta=0:00:00, total=0:00:00, wall=07:45 IST
** Validation 1.02% of 1x98...Epoch=40/150 LR=0.0842 Time=0.629 Loss=1.570 Prec@1=62.238 Prec@5=84.914 rate=2783.72 Hz, eta=0:00:00, total=0:00:00, wall=07:45 IST
** Validation 100.00% of 1x98...Epoch=40/150 LR=0.0842 Time=0.629 Loss=1.570 Prec@1=62.238 Prec@5=84.914 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=07:45 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST
=> Training   0.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=5.158 DataTime=4.831 Loss=1.459 Prec@1=66.602 Prec@5=86.328 rate=0 Hz, eta=?, total=0:00:00, wall=07:45 IST
=> Training   0.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=5.158 DataTime=4.831 Loss=1.459 Prec@1=66.602 Prec@5=86.328 rate=3144.13 Hz, eta=0:00:00, total=0:00:00, wall=07:45 IST
=> Training   0.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=5.158 DataTime=4.831 Loss=1.459 Prec@1=66.602 Prec@5=86.328 rate=3144.13 Hz, eta=0:00:00, total=0:00:00, wall=07:46 IST
=> Training   0.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.588 DataTime=0.423 Loss=1.429 Prec@1=65.453 Prec@5=86.355 rate=3144.13 Hz, eta=0:00:00, total=0:00:00, wall=07:46 IST
=> Training   4.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.588 DataTime=0.423 Loss=1.429 Prec@1=65.453 Prec@5=86.355 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=07:46 IST
=> Training   4.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.588 DataTime=0.423 Loss=1.429 Prec@1=65.453 Prec@5=86.355 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=07:47 IST
=> Training   4.04% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.572 DataTime=0.408 Loss=1.431 Prec@1=65.350 Prec@5=86.292 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=07:47 IST
=> Training   8.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.572 DataTime=0.408 Loss=1.431 Prec@1=65.350 Prec@5=86.292 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=07:47 IST
=> Training   8.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.572 DataTime=0.408 Loss=1.431 Prec@1=65.350 Prec@5=86.292 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=07:48 IST
=> Training   8.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.567 DataTime=0.401 Loss=1.431 Prec@1=65.373 Prec@5=86.335 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=07:48 IST
=> Training   12.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.567 DataTime=0.401 Loss=1.431 Prec@1=65.373 Prec@5=86.335 rate=1.82 Hz, eta=0:20:09, total=0:02:45, wall=07:48 IST
=> Training   12.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.567 DataTime=0.401 Loss=1.431 Prec@1=65.373 Prec@5=86.335 rate=1.82 Hz, eta=0:20:09, total=0:02:45, wall=07:49 IST
=> Training   12.03% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.399 Loss=1.434 Prec@1=65.349 Prec@5=86.261 rate=1.82 Hz, eta=0:20:09, total=0:02:45, wall=07:49 IST
=> Training   16.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.399 Loss=1.434 Prec@1=65.349 Prec@5=86.261 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=07:49 IST
=> Training   16.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.399 Loss=1.434 Prec@1=65.349 Prec@5=86.261 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=07:50 IST
=> Training   16.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.398 Loss=1.438 Prec@1=65.270 Prec@5=86.192 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=07:50 IST
=> Training   20.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.398 Loss=1.438 Prec@1=65.270 Prec@5=86.192 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=07:50 IST
=> Training   20.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.565 DataTime=0.398 Loss=1.438 Prec@1=65.270 Prec@5=86.192 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=07:51 IST
=> Training   20.02% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.441 Prec@1=65.189 Prec@5=86.098 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=07:51 IST
=> Training   24.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.441 Prec@1=65.189 Prec@5=86.098 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:51 IST
=> Training   24.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.441 Prec@1=65.189 Prec@5=86.098 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:52 IST
=> Training   24.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.444 Prec@1=65.084 Prec@5=86.061 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:52 IST
=> Training   28.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.444 Prec@1=65.084 Prec@5=86.061 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=07:52 IST
=> Training   28.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.444 Prec@1=65.084 Prec@5=86.061 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=07:53 IST
=> Training   28.01% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.395 Loss=1.449 Prec@1=64.961 Prec@5=85.994 rate=1.81 Hz, eta=0:16:36, total=0:06:27, wall=07:53 IST
=> Training   32.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.395 Loss=1.449 Prec@1=64.961 Prec@5=85.994 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=07:53 IST
=> Training   32.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.395 Loss=1.449 Prec@1=64.961 Prec@5=85.994 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=07:54 IST
=> Training   32.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.451 Prec@1=64.927 Prec@5=85.987 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=07:54 IST
=> Training   36.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.451 Prec@1=64.927 Prec@5=85.987 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=07:54 IST
=> Training   36.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.451 Prec@1=64.927 Prec@5=85.987 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=07:54 IST
=> Training   36.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.452 Prec@1=64.876 Prec@5=85.946 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=07:54 IST
=> Training   39.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.452 Prec@1=64.876 Prec@5=85.946 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=07:54 IST
=> Training   39.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.452 Prec@1=64.876 Prec@5=85.946 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=07:55 IST
=> Training   39.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.453 Prec@1=64.844 Prec@5=85.932 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=07:55 IST
=> Training   43.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.453 Prec@1=64.844 Prec@5=85.932 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=07:55 IST
=> Training   43.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.453 Prec@1=64.844 Prec@5=85.932 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=07:56 IST
=> Training   43.99% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.456 Prec@1=64.798 Prec@5=85.888 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=07:56 IST
=> Training   47.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.456 Prec@1=64.798 Prec@5=85.888 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=07:56 IST
=> Training   47.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.456 Prec@1=64.798 Prec@5=85.888 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=07:57 IST
=> Training   47.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.392 Loss=1.457 Prec@1=64.748 Prec@5=85.873 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=07:57 IST
=> Training   51.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.392 Loss=1.457 Prec@1=64.748 Prec@5=85.873 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=07:57 IST
=> Training   51.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.392 Loss=1.457 Prec@1=64.748 Prec@5=85.873 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=07:58 IST
=> Training   51.98% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.460 Prec@1=64.691 Prec@5=85.825 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=07:58 IST
=> Training   55.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.460 Prec@1=64.691 Prec@5=85.825 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:58 IST
=> Training   55.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.460 Prec@1=64.691 Prec@5=85.825 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:59 IST
=> Training   55.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.462 Prec@1=64.658 Prec@5=85.798 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:59 IST
=> Training   59.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.462 Prec@1=64.658 Prec@5=85.798 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=07:59 IST
=> Training   59.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.393 Loss=1.462 Prec@1=64.658 Prec@5=85.798 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=08:00 IST
=> Training   59.97% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.463 Prec@1=64.630 Prec@5=85.780 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=08:00 IST
=> Training   63.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.463 Prec@1=64.630 Prec@5=85.780 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=08:00 IST
=> Training   63.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.394 Loss=1.463 Prec@1=64.630 Prec@5=85.780 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=08:01 IST
=> Training   63.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.393 Loss=1.464 Prec@1=64.622 Prec@5=85.766 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=08:01 IST
=> Training   67.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.393 Loss=1.464 Prec@1=64.622 Prec@5=85.766 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:01 IST
=> Training   67.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.393 Loss=1.464 Prec@1=64.622 Prec@5=85.766 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:02 IST
=> Training   67.96% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.392 Loss=1.466 Prec@1=64.581 Prec@5=85.738 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:02 IST
=> Training   71.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.392 Loss=1.466 Prec@1=64.581 Prec@5=85.738 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=08:02 IST
=> Training   71.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.561 DataTime=0.392 Loss=1.466 Prec@1=64.581 Prec@5=85.738 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=08:03 IST
=> Training   71.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.468 Prec@1=64.553 Prec@5=85.723 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=08:03 IST
=> Training   75.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.468 Prec@1=64.553 Prec@5=85.723 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:03 IST
=> Training   75.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.468 Prec@1=64.553 Prec@5=85.723 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:04 IST
=> Training   75.95% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.469 Prec@1=64.529 Prec@5=85.712 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:04 IST
=> Training   79.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.469 Prec@1=64.529 Prec@5=85.712 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:04 IST
=> Training   79.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.469 Prec@1=64.529 Prec@5=85.712 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:05 IST
=> Training   79.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.471 Prec@1=64.483 Prec@5=85.680 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:05 IST
=> Training   83.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.471 Prec@1=64.483 Prec@5=85.680 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=08:05 IST
=> Training   83.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.471 Prec@1=64.483 Prec@5=85.680 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=08:06 IST
=> Training   83.94% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.472 Prec@1=64.455 Prec@5=85.667 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=08:06 IST
=> Training   87.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.472 Prec@1=64.455 Prec@5=85.667 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:06 IST
=> Training   87.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.472 Prec@1=64.455 Prec@5=85.667 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:07 IST
=> Training   87.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.473 Prec@1=64.447 Prec@5=85.661 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:07 IST
=> Training   91.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.473 Prec@1=64.447 Prec@5=85.661 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:07 IST
=> Training   91.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.473 Prec@1=64.447 Prec@5=85.661 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:08 IST
=> Training   91.93% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.475 Prec@1=64.420 Prec@5=85.632 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:08 IST
=> Training   95.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.475 Prec@1=64.420 Prec@5=85.632 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=08:08 IST
=> Training   95.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.560 DataTime=0.391 Loss=1.475 Prec@1=64.420 Prec@5=85.632 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=08:08 IST
=> Training   95.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.476 Prec@1=64.393 Prec@5=85.612 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=08:08 IST
=> Training   99.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.476 Prec@1=64.393 Prec@5=85.612 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=08:08 IST
=> Training   99.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.476 Prec@1=64.393 Prec@5=85.612 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=08:08 IST
=> Training   99.92% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.476 Prec@1=64.393 Prec@5=85.611 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=08:08 IST
=> Training   100.00% of 1x2503...Epoch=41/150 LR=0.0835 Time=0.559 DataTime=0.390 Loss=1.476 Prec@1=64.393 Prec@5=85.611 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=08:08 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:09 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:09 IST
=> Validation 0.00% of 1x98...Epoch=41/150 LR=0.0835 Time=7.457 Loss=0.908 Prec@1=77.148 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=08:09 IST
=> Validation 1.02% of 1x98...Epoch=41/150 LR=0.0835 Time=7.457 Loss=0.908 Prec@1=77.148 Prec@5=91.602 rate=5320.08 Hz, eta=0:00:00, total=0:00:00, wall=08:09 IST
** Validation 1.02% of 1x98...Epoch=41/150 LR=0.0835 Time=7.457 Loss=0.908 Prec@1=77.148 Prec@5=91.602 rate=5320.08 Hz, eta=0:00:00, total=0:00:00, wall=08:09 IST
** Validation 1.02% of 1x98...Epoch=41/150 LR=0.0835 Time=0.639 Loss=1.550 Prec@1=62.806 Prec@5=85.040 rate=5320.08 Hz, eta=0:00:00, total=0:00:00, wall=08:09 IST
** Validation 100.00% of 1x98...Epoch=41/150 LR=0.0835 Time=0.639 Loss=1.550 Prec@1=62.806 Prec@5=85.040 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=08:09 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> Training   0.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=4.989 DataTime=4.741 Loss=1.291 Prec@1=68.750 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=08:10 IST
=> Training   0.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=4.989 DataTime=4.741 Loss=1.291 Prec@1=68.750 Prec@5=88.477 rate=4955.94 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
=> Training   0.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=4.989 DataTime=4.741 Loss=1.291 Prec@1=68.750 Prec@5=88.477 rate=4955.94 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
=> Training   0.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.582 DataTime=0.423 Loss=1.437 Prec@1=65.027 Prec@5=86.100 rate=4955.94 Hz, eta=0:00:00, total=0:00:00, wall=08:10 IST
=> Training   4.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.582 DataTime=0.423 Loss=1.437 Prec@1=65.027 Prec@5=86.100 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=08:10 IST
=> Training   4.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.582 DataTime=0.423 Loss=1.437 Prec@1=65.027 Prec@5=86.100 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=08:11 IST
=> Training   4.04% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.570 DataTime=0.404 Loss=1.432 Prec@1=65.119 Prec@5=86.199 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=08:11 IST
=> Training   8.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.570 DataTime=0.404 Loss=1.432 Prec@1=65.119 Prec@5=86.199 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=08:11 IST
=> Training   8.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.570 DataTime=0.404 Loss=1.432 Prec@1=65.119 Prec@5=86.199 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=08:12 IST
=> Training   8.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.396 Loss=1.435 Prec@1=65.136 Prec@5=86.194 rate=1.84 Hz, eta=0:20:54, total=0:01:49, wall=08:12 IST
=> Training   12.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.396 Loss=1.435 Prec@1=65.136 Prec@5=86.194 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=08:12 IST
=> Training   12.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.396 Loss=1.435 Prec@1=65.136 Prec@5=86.194 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=08:13 IST
=> Training   12.03% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.433 Prec@1=65.173 Prec@5=86.190 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=08:13 IST
=> Training   16.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.433 Prec@1=65.173 Prec@5=86.190 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=08:13 IST
=> Training   16.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.433 Prec@1=65.173 Prec@5=86.190 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=08:14 IST
=> Training   16.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.439 Prec@1=65.082 Prec@5=86.122 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=08:14 IST
=> Training   20.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.439 Prec@1=65.082 Prec@5=86.122 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=08:14 IST
=> Training   20.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.393 Loss=1.439 Prec@1=65.082 Prec@5=86.122 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=08:15 IST
=> Training   20.02% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.392 Loss=1.442 Prec@1=65.048 Prec@5=86.070 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=08:15 IST
=> Training   24.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.392 Loss=1.442 Prec@1=65.048 Prec@5=86.070 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=08:15 IST
=> Training   24.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.392 Loss=1.442 Prec@1=65.048 Prec@5=86.070 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=08:16 IST
=> Training   24.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.392 Loss=1.447 Prec@1=64.963 Prec@5=86.013 rate=1.81 Hz, eta=0:17:32, total=0:05:32, wall=08:16 IST
=> Training   28.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.392 Loss=1.447 Prec@1=64.963 Prec@5=86.013 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=08:16 IST
=> Training   28.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.562 DataTime=0.392 Loss=1.447 Prec@1=64.963 Prec@5=86.013 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=08:17 IST
=> Training   28.01% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.390 Loss=1.447 Prec@1=64.956 Prec@5=85.997 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=08:17 IST
=> Training   32.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.390 Loss=1.447 Prec@1=64.956 Prec@5=85.997 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=08:17 IST
=> Training   32.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.561 DataTime=0.390 Loss=1.447 Prec@1=64.956 Prec@5=85.997 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=08:18 IST
=> Training   32.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.388 Loss=1.447 Prec@1=64.950 Prec@5=86.004 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=08:18 IST
=> Training   36.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.388 Loss=1.447 Prec@1=64.950 Prec@5=86.004 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=08:18 IST
=> Training   36.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.388 Loss=1.447 Prec@1=64.950 Prec@5=86.004 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=08:19 IST
=> Training   36.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.449 Prec@1=64.914 Prec@5=85.988 rate=1.81 Hz, eta=0:14:47, total=0:08:19, wall=08:19 IST
=> Training   39.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.449 Prec@1=64.914 Prec@5=85.988 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=08:19 IST
=> Training   39.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.449 Prec@1=64.914 Prec@5=85.988 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=08:20 IST
=> Training   39.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.452 Prec@1=64.832 Prec@5=85.945 rate=1.81 Hz, eta=0:13:50, total=0:09:13, wall=08:20 IST
=> Training   43.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.452 Prec@1=64.832 Prec@5=85.945 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=08:20 IST
=> Training   43.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.452 Prec@1=64.832 Prec@5=85.945 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=08:21 IST
=> Training   43.99% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.455 Prec@1=64.811 Prec@5=85.905 rate=1.80 Hz, eta=0:12:56, total=0:10:10, wall=08:21 IST
=> Training   47.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.455 Prec@1=64.811 Prec@5=85.905 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=08:21 IST
=> Training   47.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.455 Prec@1=64.811 Prec@5=85.905 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=08:22 IST
=> Training   47.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.458 Prec@1=64.744 Prec@5=85.861 rate=1.81 Hz, eta=0:12:01, total=0:11:05, wall=08:22 IST
=> Training   51.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.458 Prec@1=64.744 Prec@5=85.861 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=08:22 IST
=> Training   51.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.458 Prec@1=64.744 Prec@5=85.861 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=08:23 IST
=> Training   51.98% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.458 Prec@1=64.744 Prec@5=85.855 rate=1.80 Hz, eta=0:11:06, total=0:12:01, wall=08:23 IST
=> Training   55.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.458 Prec@1=64.744 Prec@5=85.855 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=08:23 IST
=> Training   55.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.458 Prec@1=64.744 Prec@5=85.855 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=08:23 IST
=> Training   55.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.459 Prec@1=64.718 Prec@5=85.844 rate=1.80 Hz, eta=0:10:11, total=0:12:56, wall=08:23 IST
=> Training   59.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.459 Prec@1=64.718 Prec@5=85.844 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=08:23 IST
=> Training   59.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.459 Prec@1=64.718 Prec@5=85.844 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=08:24 IST
=> Training   59.97% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.384 Loss=1.460 Prec@1=64.686 Prec@5=85.825 rate=1.80 Hz, eta=0:09:16, total=0:13:53, wall=08:24 IST
=> Training   63.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.384 Loss=1.460 Prec@1=64.686 Prec@5=85.825 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=08:24 IST
=> Training   63.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.384 Loss=1.460 Prec@1=64.686 Prec@5=85.825 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=08:25 IST
=> Training   63.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.462 Prec@1=64.656 Prec@5=85.802 rate=1.80 Hz, eta=0:08:19, total=0:14:47, wall=08:25 IST
=> Training   67.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.462 Prec@1=64.656 Prec@5=85.802 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=08:25 IST
=> Training   67.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.462 Prec@1=64.656 Prec@5=85.802 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=08:26 IST
=> Training   67.96% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.385 Loss=1.463 Prec@1=64.642 Prec@5=85.798 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=08:26 IST
=> Training   71.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.385 Loss=1.463 Prec@1=64.642 Prec@5=85.798 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=08:26 IST
=> Training   71.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.557 DataTime=0.385 Loss=1.463 Prec@1=64.642 Prec@5=85.798 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=08:27 IST
=> Training   71.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.464 Prec@1=64.620 Prec@5=85.770 rate=1.80 Hz, eta=0:06:29, total=0:16:38, wall=08:27 IST
=> Training   75.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.464 Prec@1=64.620 Prec@5=85.770 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=08:27 IST
=> Training   75.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.464 Prec@1=64.620 Prec@5=85.770 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=08:28 IST
=> Training   75.95% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.387 Loss=1.466 Prec@1=64.589 Prec@5=85.762 rate=1.80 Hz, eta=0:05:34, total=0:17:35, wall=08:28 IST
=> Training   79.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.387 Loss=1.466 Prec@1=64.589 Prec@5=85.762 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=08:28 IST
=> Training   79.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.387 Loss=1.466 Prec@1=64.589 Prec@5=85.762 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=08:29 IST
=> Training   79.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.467 Prec@1=64.569 Prec@5=85.736 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=08:29 IST
=> Training   83.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.467 Prec@1=64.569 Prec@5=85.736 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=08:29 IST
=> Training   83.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.386 Loss=1.467 Prec@1=64.569 Prec@5=85.736 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=08:30 IST
=> Training   83.94% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.468 Prec@1=64.561 Prec@5=85.724 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=08:30 IST
=> Training   87.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.468 Prec@1=64.561 Prec@5=85.724 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:30 IST
=> Training   87.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.387 Loss=1.468 Prec@1=64.561 Prec@5=85.724 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:31 IST
=> Training   87.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.386 Loss=1.468 Prec@1=64.552 Prec@5=85.720 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:31 IST
=> Training   91.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.386 Loss=1.468 Prec@1=64.552 Prec@5=85.720 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=08:31 IST
=> Training   91.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.559 DataTime=0.386 Loss=1.468 Prec@1=64.552 Prec@5=85.720 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=08:32 IST
=> Training   91.93% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.469 Prec@1=64.527 Prec@5=85.702 rate=1.80 Hz, eta=0:01:52, total=0:21:20, wall=08:32 IST
=> Training   95.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.469 Prec@1=64.527 Prec@5=85.702 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:32 IST
=> Training   95.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.469 Prec@1=64.527 Prec@5=85.702 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:33 IST
=> Training   95.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.470 Prec@1=64.511 Prec@5=85.698 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:33 IST
=> Training   99.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.470 Prec@1=64.511 Prec@5=85.698 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=08:33 IST
=> Training   99.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.385 Loss=1.470 Prec@1=64.511 Prec@5=85.698 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=08:33 IST
=> Training   99.92% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.384 Loss=1.470 Prec@1=64.510 Prec@5=85.697 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=08:33 IST
=> Training   100.00% of 1x2503...Epoch=42/150 LR=0.0827 Time=0.558 DataTime=0.384 Loss=1.470 Prec@1=64.510 Prec@5=85.697 rate=1.80 Hz, eta=0:00:00, total=0:23:10, wall=08:33 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:33 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:33 IST
=> Validation 0.00% of 1x98...Epoch=42/150 LR=0.0827 Time=7.317 Loss=0.955 Prec@1=73.438 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=08:33 IST
=> Validation 1.02% of 1x98...Epoch=42/150 LR=0.0827 Time=7.317 Loss=0.955 Prec@1=73.438 Prec@5=93.555 rate=7618.64 Hz, eta=0:00:00, total=0:00:00, wall=08:33 IST
** Validation 1.02% of 1x98...Epoch=42/150 LR=0.0827 Time=7.317 Loss=0.955 Prec@1=73.438 Prec@5=93.555 rate=7618.64 Hz, eta=0:00:00, total=0:00:00, wall=08:34 IST
** Validation 1.02% of 1x98...Epoch=42/150 LR=0.0827 Time=0.634 Loss=1.602 Prec@1=61.852 Prec@5=84.206 rate=7618.64 Hz, eta=0:00:00, total=0:00:00, wall=08:34 IST
** Validation 100.00% of 1x98...Epoch=42/150 LR=0.0827 Time=0.634 Loss=1.602 Prec@1=61.852 Prec@5=84.206 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=08:34 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:34 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:34 IST
=> Training   0.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=4.827 DataTime=4.270 Loss=1.466 Prec@1=66.016 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=08:34 IST
=> Training   0.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=4.827 DataTime=4.270 Loss=1.466 Prec@1=66.016 Prec@5=86.523 rate=3574.95 Hz, eta=0:00:00, total=0:00:00, wall=08:34 IST
=> Training   0.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=4.827 DataTime=4.270 Loss=1.466 Prec@1=66.016 Prec@5=86.523 rate=3574.95 Hz, eta=0:00:00, total=0:00:00, wall=08:35 IST
=> Training   0.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.584 DataTime=0.419 Loss=1.427 Prec@1=65.511 Prec@5=86.237 rate=3574.95 Hz, eta=0:00:00, total=0:00:00, wall=08:35 IST
=> Training   4.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.584 DataTime=0.419 Loss=1.427 Prec@1=65.511 Prec@5=86.237 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=08:35 IST
=> Training   4.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.584 DataTime=0.419 Loss=1.427 Prec@1=65.511 Prec@5=86.237 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=08:36 IST
=> Training   4.04% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.570 DataTime=0.401 Loss=1.419 Prec@1=65.631 Prec@5=86.404 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=08:36 IST
=> Training   8.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.570 DataTime=0.401 Loss=1.419 Prec@1=65.631 Prec@5=86.404 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=08:36 IST
=> Training   8.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.570 DataTime=0.401 Loss=1.419 Prec@1=65.631 Prec@5=86.404 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=08:37 IST
=> Training   8.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.394 Loss=1.426 Prec@1=65.465 Prec@5=86.265 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=08:37 IST
=> Training   12.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.394 Loss=1.426 Prec@1=65.465 Prec@5=86.265 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=08:37 IST
=> Training   12.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.394 Loss=1.426 Prec@1=65.465 Prec@5=86.265 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=08:38 IST
=> Training   12.03% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.566 DataTime=0.395 Loss=1.433 Prec@1=65.314 Prec@5=86.201 rate=1.82 Hz, eta=0:20:10, total=0:02:45, wall=08:38 IST
=> Training   16.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.566 DataTime=0.395 Loss=1.433 Prec@1=65.314 Prec@5=86.201 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=08:38 IST
=> Training   16.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.566 DataTime=0.395 Loss=1.433 Prec@1=65.314 Prec@5=86.201 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=08:39 IST
=> Training   16.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.567 DataTime=0.394 Loss=1.432 Prec@1=65.392 Prec@5=86.214 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=08:39 IST
=> Training   20.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.567 DataTime=0.394 Loss=1.432 Prec@1=65.392 Prec@5=86.214 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=08:39 IST
=> Training   20.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.567 DataTime=0.394 Loss=1.432 Prec@1=65.392 Prec@5=86.214 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=08:39 IST
=> Training   20.02% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.391 Loss=1.436 Prec@1=65.295 Prec@5=86.179 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=08:39 IST
=> Training   24.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.391 Loss=1.436 Prec@1=65.295 Prec@5=86.179 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=08:39 IST
=> Training   24.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.565 DataTime=0.391 Loss=1.436 Prec@1=65.295 Prec@5=86.179 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=08:40 IST
=> Training   24.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.564 DataTime=0.389 Loss=1.438 Prec@1=65.236 Prec@5=86.148 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=08:40 IST
=> Training   28.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.564 DataTime=0.389 Loss=1.438 Prec@1=65.236 Prec@5=86.148 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=08:40 IST
=> Training   28.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.564 DataTime=0.389 Loss=1.438 Prec@1=65.236 Prec@5=86.148 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=08:41 IST
=> Training   28.01% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.563 DataTime=0.387 Loss=1.441 Prec@1=65.139 Prec@5=86.113 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=08:41 IST
=> Training   32.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.563 DataTime=0.387 Loss=1.441 Prec@1=65.139 Prec@5=86.113 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=08:41 IST
=> Training   32.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.563 DataTime=0.387 Loss=1.441 Prec@1=65.139 Prec@5=86.113 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=08:42 IST
=> Training   32.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.562 DataTime=0.387 Loss=1.441 Prec@1=65.110 Prec@5=86.129 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=08:42 IST
=> Training   36.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.562 DataTime=0.387 Loss=1.441 Prec@1=65.110 Prec@5=86.129 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:42 IST
=> Training   36.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.562 DataTime=0.387 Loss=1.441 Prec@1=65.110 Prec@5=86.129 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:43 IST
=> Training   36.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.443 Prec@1=65.063 Prec@5=86.087 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:43 IST
=> Training   39.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.443 Prec@1=65.063 Prec@5=86.087 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=08:43 IST
=> Training   39.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.443 Prec@1=65.063 Prec@5=86.087 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=08:44 IST
=> Training   39.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.444 Prec@1=65.031 Prec@5=86.066 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=08:44 IST
=> Training   43.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.444 Prec@1=65.031 Prec@5=86.066 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=08:44 IST
=> Training   43.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.444 Prec@1=65.031 Prec@5=86.066 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=08:45 IST
=> Training   43.99% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.447 Prec@1=64.977 Prec@5=86.028 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=08:45 IST
=> Training   47.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.447 Prec@1=64.977 Prec@5=86.028 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=08:45 IST
=> Training   47.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.447 Prec@1=64.977 Prec@5=86.028 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=08:46 IST
=> Training   47.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.449 Prec@1=64.942 Prec@5=85.992 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=08:46 IST
=> Training   51.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.449 Prec@1=64.942 Prec@5=85.992 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=08:46 IST
=> Training   51.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.449 Prec@1=64.942 Prec@5=85.992 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=08:47 IST
=> Training   51.98% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.450 Prec@1=64.893 Prec@5=85.992 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=08:47 IST
=> Training   55.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.450 Prec@1=64.893 Prec@5=85.992 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=08:47 IST
=> Training   55.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.450 Prec@1=64.893 Prec@5=85.992 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=08:48 IST
=> Training   55.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.451 Prec@1=64.867 Prec@5=85.970 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=08:48 IST
=> Training   59.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.451 Prec@1=64.867 Prec@5=85.970 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=08:48 IST
=> Training   59.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.451 Prec@1=64.867 Prec@5=85.970 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=08:49 IST
=> Training   59.97% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.452 Prec@1=64.856 Prec@5=85.957 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=08:49 IST
=> Training   63.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.452 Prec@1=64.856 Prec@5=85.957 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=08:49 IST
=> Training   63.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.452 Prec@1=64.856 Prec@5=85.957 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=08:50 IST
=> Training   63.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.453 Prec@1=64.838 Prec@5=85.931 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=08:50 IST
=> Training   67.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.453 Prec@1=64.838 Prec@5=85.931 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:50 IST
=> Training   67.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.453 Prec@1=64.838 Prec@5=85.931 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:51 IST
=> Training   67.96% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.455 Prec@1=64.809 Prec@5=85.902 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=08:51 IST
=> Training   71.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.455 Prec@1=64.809 Prec@5=85.902 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=08:51 IST
=> Training   71.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.386 Loss=1.455 Prec@1=64.809 Prec@5=85.902 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=08:52 IST
=> Training   71.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.457 Prec@1=64.764 Prec@5=85.875 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=08:52 IST
=> Training   75.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.457 Prec@1=64.764 Prec@5=85.875 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=08:52 IST
=> Training   75.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.387 Loss=1.457 Prec@1=64.764 Prec@5=85.875 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=08:52 IST
=> Training   75.95% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.735 Prec@5=85.846 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=08:52 IST
=> Training   79.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.735 Prec@5=85.846 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:52 IST
=> Training   79.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.735 Prec@5=85.846 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:53 IST
=> Training   79.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.732 Prec@5=85.846 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=08:53 IST
=> Training   83.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.732 Prec@5=85.846 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=08:53 IST
=> Training   83.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.560 DataTime=0.386 Loss=1.458 Prec@1=64.732 Prec@5=85.846 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=08:54 IST
=> Training   83.94% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.459 Prec@1=64.721 Prec@5=85.838 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=08:54 IST
=> Training   87.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.459 Prec@1=64.721 Prec@5=85.838 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:54 IST
=> Training   87.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.459 Prec@1=64.721 Prec@5=85.838 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:55 IST
=> Training   87.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.460 Prec@1=64.717 Prec@5=85.827 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=08:55 IST
=> Training   91.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.460 Prec@1=64.717 Prec@5=85.827 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:55 IST
=> Training   91.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.460 Prec@1=64.717 Prec@5=85.827 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:56 IST
=> Training   91.93% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.462 Prec@1=64.678 Prec@5=85.805 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:56 IST
=> Training   95.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.462 Prec@1=64.678 Prec@5=85.805 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=08:56 IST
=> Training   95.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.462 Prec@1=64.678 Prec@5=85.805 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=08:57 IST
=> Training   95.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.463 Prec@1=64.656 Prec@5=85.794 rate=1.79 Hz, eta=0:00:56, total=0:22:17, wall=08:57 IST
=> Training   99.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.463 Prec@1=64.656 Prec@5=85.794 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=08:57 IST
=> Training   99.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.559 DataTime=0.385 Loss=1.463 Prec@1=64.656 Prec@5=85.794 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=08:57 IST
=> Training   99.92% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.558 DataTime=0.385 Loss=1.463 Prec@1=64.655 Prec@5=85.794 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=08:57 IST
=> Training   100.00% of 1x2503...Epoch=43/150 LR=0.0819 Time=0.558 DataTime=0.385 Loss=1.463 Prec@1=64.655 Prec@5=85.794 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=08:57 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:57 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:57 IST
=> Validation 0.00% of 1x98...Epoch=43/150 LR=0.0819 Time=6.786 Loss=0.965 Prec@1=75.391 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=08:57 IST
=> Validation 1.02% of 1x98...Epoch=43/150 LR=0.0819 Time=6.786 Loss=0.965 Prec@1=75.391 Prec@5=90.820 rate=3868.05 Hz, eta=0:00:00, total=0:00:00, wall=08:57 IST
** Validation 1.02% of 1x98...Epoch=43/150 LR=0.0819 Time=6.786 Loss=0.965 Prec@1=75.391 Prec@5=90.820 rate=3868.05 Hz, eta=0:00:00, total=0:00:00, wall=08:58 IST
** Validation 1.02% of 1x98...Epoch=43/150 LR=0.0819 Time=0.637 Loss=1.549 Prec@1=62.742 Prec@5=85.026 rate=3868.05 Hz, eta=0:00:00, total=0:00:00, wall=08:58 IST
** Validation 100.00% of 1x98...Epoch=43/150 LR=0.0819 Time=0.637 Loss=1.549 Prec@1=62.742 Prec@5=85.026 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=08:58 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> Training   0.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=4.479 DataTime=4.183 Loss=1.559 Prec@1=65.039 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=08:58 IST
=> Training   0.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=4.479 DataTime=4.183 Loss=1.559 Prec@1=65.039 Prec@5=85.547 rate=813.10 Hz, eta=0:00:03, total=0:00:00, wall=08:58 IST
=> Training   0.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=4.479 DataTime=4.183 Loss=1.559 Prec@1=65.039 Prec@5=85.547 rate=813.10 Hz, eta=0:00:03, total=0:00:00, wall=08:59 IST
=> Training   0.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.590 DataTime=0.421 Loss=1.420 Prec@1=65.443 Prec@5=86.597 rate=813.10 Hz, eta=0:00:03, total=0:00:00, wall=08:59 IST
=> Training   4.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.590 DataTime=0.421 Loss=1.420 Prec@1=65.443 Prec@5=86.597 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=08:59 IST
=> Training   4.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.590 DataTime=0.421 Loss=1.420 Prec@1=65.443 Prec@5=86.597 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=09:00 IST
=> Training   4.04% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.575 DataTime=0.405 Loss=1.411 Prec@1=65.710 Prec@5=86.523 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=09:00 IST
=> Training   8.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.575 DataTime=0.405 Loss=1.411 Prec@1=65.710 Prec@5=86.523 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=09:00 IST
=> Training   8.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.575 DataTime=0.405 Loss=1.411 Prec@1=65.710 Prec@5=86.523 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=09:01 IST
=> Training   8.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.568 DataTime=0.401 Loss=1.415 Prec@1=65.648 Prec@5=86.462 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=09:01 IST
=> Training   12.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.568 DataTime=0.401 Loss=1.415 Prec@1=65.648 Prec@5=86.462 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=09:01 IST
=> Training   12.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.568 DataTime=0.401 Loss=1.415 Prec@1=65.648 Prec@5=86.462 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=09:02 IST
=> Training   12.03% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.394 Loss=1.418 Prec@1=65.657 Prec@5=86.438 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=09:02 IST
=> Training   16.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.394 Loss=1.418 Prec@1=65.657 Prec@5=86.438 rate=1.81 Hz, eta=0:19:18, total=0:03:41, wall=09:02 IST
=> Training   16.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.394 Loss=1.418 Prec@1=65.657 Prec@5=86.438 rate=1.81 Hz, eta=0:19:18, total=0:03:41, wall=09:03 IST
=> Training   16.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.563 DataTime=0.394 Loss=1.420 Prec@1=65.609 Prec@5=86.387 rate=1.81 Hz, eta=0:19:18, total=0:03:41, wall=09:03 IST
=> Training   20.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.563 DataTime=0.394 Loss=1.420 Prec@1=65.609 Prec@5=86.387 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=09:03 IST
=> Training   20.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.563 DataTime=0.394 Loss=1.420 Prec@1=65.609 Prec@5=86.387 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=09:04 IST
=> Training   20.02% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.425 Prec@1=65.502 Prec@5=86.345 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=09:04 IST
=> Training   24.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.425 Prec@1=65.502 Prec@5=86.345 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=09:04 IST
=> Training   24.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.425 Prec@1=65.502 Prec@5=86.345 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=09:05 IST
=> Training   24.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.427 Prec@1=65.455 Prec@5=86.318 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=09:05 IST
=> Training   28.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.427 Prec@1=65.455 Prec@5=86.318 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:05 IST
=> Training   28.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.562 DataTime=0.392 Loss=1.427 Prec@1=65.455 Prec@5=86.318 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:06 IST
=> Training   28.01% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.391 Loss=1.429 Prec@1=65.388 Prec@5=86.295 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:06 IST
=> Training   32.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.391 Loss=1.429 Prec@1=65.388 Prec@5=86.295 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=09:06 IST
=> Training   32.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.391 Loss=1.429 Prec@1=65.388 Prec@5=86.295 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=09:07 IST
=> Training   32.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.392 Loss=1.431 Prec@1=65.357 Prec@5=86.247 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=09:07 IST
=> Training   36.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.392 Loss=1.431 Prec@1=65.357 Prec@5=86.247 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:07 IST
=> Training   36.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.561 DataTime=0.392 Loss=1.431 Prec@1=65.357 Prec@5=86.247 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:07 IST
=> Training   36.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.433 Prec@1=65.307 Prec@5=86.231 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:07 IST
=> Training   39.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.433 Prec@1=65.307 Prec@5=86.231 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=09:07 IST
=> Training   39.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.433 Prec@1=65.307 Prec@5=86.231 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=09:08 IST
=> Training   39.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.434 Prec@1=65.285 Prec@5=86.201 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=09:08 IST
=> Training   43.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.434 Prec@1=65.285 Prec@5=86.201 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:08 IST
=> Training   43.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.434 Prec@1=65.285 Prec@5=86.201 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:09 IST
=> Training   43.99% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.437 Prec@1=65.231 Prec@5=86.155 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:09 IST
=> Training   47.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.437 Prec@1=65.231 Prec@5=86.155 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=09:09 IST
=> Training   47.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.437 Prec@1=65.231 Prec@5=86.155 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=09:10 IST
=> Training   47.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.392 Loss=1.440 Prec@1=65.174 Prec@5=86.116 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=09:10 IST
=> Training   51.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.392 Loss=1.440 Prec@1=65.174 Prec@5=86.116 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=09:10 IST
=> Training   51.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.392 Loss=1.440 Prec@1=65.174 Prec@5=86.116 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=09:11 IST
=> Training   51.98% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.442 Prec@1=65.129 Prec@5=86.070 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=09:11 IST
=> Training   55.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.442 Prec@1=65.129 Prec@5=86.070 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=09:11 IST
=> Training   55.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.391 Loss=1.442 Prec@1=65.129 Prec@5=86.070 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=09:12 IST
=> Training   55.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.443 Prec@1=65.116 Prec@5=86.076 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=09:12 IST
=> Training   59.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.443 Prec@1=65.116 Prec@5=86.076 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=09:12 IST
=> Training   59.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.391 Loss=1.443 Prec@1=65.116 Prec@5=86.076 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=09:13 IST
=> Training   59.97% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.446 Prec@1=65.057 Prec@5=86.040 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=09:13 IST
=> Training   63.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.446 Prec@1=65.057 Prec@5=86.040 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=09:13 IST
=> Training   63.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.559 DataTime=0.390 Loss=1.446 Prec@1=65.057 Prec@5=86.040 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=09:14 IST
=> Training   63.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.447 Prec@1=65.019 Prec@5=86.012 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=09:14 IST
=> Training   67.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.447 Prec@1=65.019 Prec@5=86.012 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=09:14 IST
=> Training   67.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.447 Prec@1=65.019 Prec@5=86.012 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=09:15 IST
=> Training   67.96% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.448 Prec@1=65.008 Prec@5=85.997 rate=1.80 Hz, eta=0:07:25, total=0:15:44, wall=09:15 IST
=> Training   71.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.448 Prec@1=65.008 Prec@5=85.997 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=09:15 IST
=> Training   71.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.448 Prec@1=65.008 Prec@5=85.997 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=09:16 IST
=> Training   71.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.449 Prec@1=64.984 Prec@5=85.976 rate=1.80 Hz, eta=0:06:29, total=0:16:39, wall=09:16 IST
=> Training   75.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.449 Prec@1=64.984 Prec@5=85.976 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=09:16 IST
=> Training   75.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.449 Prec@1=64.984 Prec@5=85.976 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=09:17 IST
=> Training   75.95% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.451 Prec@1=64.956 Prec@5=85.958 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=09:17 IST
=> Training   79.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.451 Prec@1=64.956 Prec@5=85.958 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=09:17 IST
=> Training   79.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.451 Prec@1=64.956 Prec@5=85.958 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=09:18 IST
=> Training   79.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.453 Prec@1=64.917 Prec@5=85.918 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=09:18 IST
=> Training   83.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.453 Prec@1=64.917 Prec@5=85.918 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=09:18 IST
=> Training   83.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.453 Prec@1=64.917 Prec@5=85.918 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=09:19 IST
=> Training   83.94% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.454 Prec@1=64.889 Prec@5=85.907 rate=1.80 Hz, eta=0:03:43, total=0:19:27, wall=09:19 IST
=> Training   87.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.454 Prec@1=64.889 Prec@5=85.907 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=09:19 IST
=> Training   87.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.557 DataTime=0.388 Loss=1.454 Prec@1=64.889 Prec@5=85.907 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=09:20 IST
=> Training   87.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.388 Loss=1.455 Prec@1=64.861 Prec@5=85.889 rate=1.80 Hz, eta=0:02:47, total=0:20:22, wall=09:20 IST
=> Training   91.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.388 Loss=1.455 Prec@1=64.861 Prec@5=85.889 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=09:20 IST
=> Training   91.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.388 Loss=1.455 Prec@1=64.861 Prec@5=85.889 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=09:20 IST
=> Training   91.93% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.456 Prec@1=64.838 Prec@5=85.877 rate=1.80 Hz, eta=0:01:52, total=0:21:19, wall=09:20 IST
=> Training   95.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.456 Prec@1=64.838 Prec@5=85.877 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=09:20 IST
=> Training   95.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.456 Prec@1=64.838 Prec@5=85.877 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=09:21 IST
=> Training   95.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.457 Prec@1=64.822 Prec@5=85.866 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=09:21 IST
=> Training   99.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.457 Prec@1=64.822 Prec@5=85.866 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=09:21 IST
=> Training   99.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.457 Prec@1=64.822 Prec@5=85.866 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=09:21 IST
=> Training   99.92% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.457 Prec@1=64.823 Prec@5=85.866 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=09:21 IST
=> Training   100.00% of 1x2503...Epoch=44/150 LR=0.0811 Time=0.558 DataTime=0.389 Loss=1.457 Prec@1=64.823 Prec@5=85.866 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=09:21 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST
=> Validation 0.00% of 1x98...Epoch=44/150 LR=0.0811 Time=8.111 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=09:22 IST
=> Validation 1.02% of 1x98...Epoch=44/150 LR=0.0811 Time=8.111 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=5200.29 Hz, eta=0:00:00, total=0:00:00, wall=09:22 IST
** Validation 1.02% of 1x98...Epoch=44/150 LR=0.0811 Time=8.111 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=5200.29 Hz, eta=0:00:00, total=0:00:00, wall=09:23 IST
** Validation 1.02% of 1x98...Epoch=44/150 LR=0.0811 Time=0.679 Loss=1.560 Prec@1=62.826 Prec@5=84.952 rate=5200.29 Hz, eta=0:00:00, total=0:00:00, wall=09:23 IST
** Validation 100.00% of 1x98...Epoch=44/150 LR=0.0811 Time=0.679 Loss=1.560 Prec@1=62.826 Prec@5=84.952 rate=1.68 Hz, eta=0:00:00, total=0:00:58, wall=09:23 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST
=> Training   0.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=5.821 DataTime=5.595 Loss=1.516 Prec@1=63.086 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=09:23 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=5.821 DataTime=5.595 Loss=1.516 Prec@1=63.086 Prec@5=84.570 rate=7309.57 Hz, eta=0:00:00, total=0:00:00, wall=09:23 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=5.821 DataTime=5.595 Loss=1.516 Prec@1=63.086 Prec@5=84.570 rate=7309.57 Hz, eta=0:00:00, total=0:00:00, wall=09:24 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.631 DataTime=0.468 Loss=1.405 Prec@1=65.938 Prec@5=86.595 rate=7309.57 Hz, eta=0:00:00, total=0:00:00, wall=09:24 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.631 DataTime=0.468 Loss=1.405 Prec@1=65.938 Prec@5=86.595 rate=1.74 Hz, eta=0:22:57, total=0:00:57, wall=09:24 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.631 DataTime=0.468 Loss=1.405 Prec@1=65.938 Prec@5=86.595 rate=1.74 Hz, eta=0:22:57, total=0:00:57, wall=09:25 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.609 DataTime=0.443 Loss=1.417 Prec@1=65.704 Prec@5=86.400 rate=1.74 Hz, eta=0:22:57, total=0:00:57, wall=09:25 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.609 DataTime=0.443 Loss=1.417 Prec@1=65.704 Prec@5=86.400 rate=1.73 Hz, eta=0:22:14, total=0:01:56, wall=09:25 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.609 DataTime=0.443 Loss=1.417 Prec@1=65.704 Prec@5=86.400 rate=1.73 Hz, eta=0:22:14, total=0:01:56, wall=09:26 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.596 DataTime=0.427 Loss=1.414 Prec@1=65.763 Prec@5=86.459 rate=1.73 Hz, eta=0:22:14, total=0:01:56, wall=09:26 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.596 DataTime=0.427 Loss=1.414 Prec@1=65.763 Prec@5=86.459 rate=1.74 Hz, eta=0:21:09, total=0:02:53, wall=09:26 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.596 DataTime=0.427 Loss=1.414 Prec@1=65.763 Prec@5=86.459 rate=1.74 Hz, eta=0:21:09, total=0:02:53, wall=09:27 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.591 DataTime=0.421 Loss=1.415 Prec@1=65.673 Prec@5=86.405 rate=1.74 Hz, eta=0:21:09, total=0:02:53, wall=09:27 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.591 DataTime=0.421 Loss=1.415 Prec@1=65.673 Prec@5=86.405 rate=1.74 Hz, eta=0:20:10, total=0:03:51, wall=09:27 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.591 DataTime=0.421 Loss=1.415 Prec@1=65.673 Prec@5=86.405 rate=1.74 Hz, eta=0:20:10, total=0:03:51, wall=09:27 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.583 DataTime=0.412 Loss=1.419 Prec@1=65.602 Prec@5=86.362 rate=1.74 Hz, eta=0:20:10, total=0:03:51, wall=09:27 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.583 DataTime=0.412 Loss=1.419 Prec@1=65.602 Prec@5=86.362 rate=1.75 Hz, eta=0:19:04, total=0:04:46, wall=09:27 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.583 DataTime=0.412 Loss=1.419 Prec@1=65.602 Prec@5=86.362 rate=1.75 Hz, eta=0:19:04, total=0:04:46, wall=09:28 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.582 DataTime=0.411 Loss=1.420 Prec@1=65.566 Prec@5=86.367 rate=1.75 Hz, eta=0:19:04, total=0:04:46, wall=09:28 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.582 DataTime=0.411 Loss=1.420 Prec@1=65.566 Prec@5=86.367 rate=1.75 Hz, eta=0:18:09, total=0:05:44, wall=09:28 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.582 DataTime=0.411 Loss=1.420 Prec@1=65.566 Prec@5=86.367 rate=1.75 Hz, eta=0:18:09, total=0:05:44, wall=09:29 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.578 DataTime=0.406 Loss=1.421 Prec@1=65.557 Prec@5=86.357 rate=1.75 Hz, eta=0:18:09, total=0:05:44, wall=09:29 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.578 DataTime=0.406 Loss=1.421 Prec@1=65.557 Prec@5=86.357 rate=1.76 Hz, eta=0:17:06, total=0:06:39, wall=09:29 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.578 DataTime=0.406 Loss=1.421 Prec@1=65.557 Prec@5=86.357 rate=1.76 Hz, eta=0:17:06, total=0:06:39, wall=09:30 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.577 DataTime=0.406 Loss=1.424 Prec@1=65.495 Prec@5=86.312 rate=1.76 Hz, eta=0:17:06, total=0:06:39, wall=09:30 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.577 DataTime=0.406 Loss=1.424 Prec@1=65.495 Prec@5=86.312 rate=1.75 Hz, eta=0:16:10, total=0:07:36, wall=09:30 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.577 DataTime=0.406 Loss=1.424 Prec@1=65.495 Prec@5=86.312 rate=1.75 Hz, eta=0:16:10, total=0:07:36, wall=09:31 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.579 DataTime=0.408 Loss=1.429 Prec@1=65.390 Prec@5=86.269 rate=1.75 Hz, eta=0:16:10, total=0:07:36, wall=09:31 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.579 DataTime=0.408 Loss=1.429 Prec@1=65.390 Prec@5=86.269 rate=1.75 Hz, eta=0:15:17, total=0:08:36, wall=09:31 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.579 DataTime=0.408 Loss=1.429 Prec@1=65.390 Prec@5=86.269 rate=1.75 Hz, eta=0:15:17, total=0:08:36, wall=09:32 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.582 DataTime=0.410 Loss=1.430 Prec@1=65.377 Prec@5=86.240 rate=1.75 Hz, eta=0:15:17, total=0:08:36, wall=09:32 IST
=> Training   39.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.582 DataTime=0.410 Loss=1.430 Prec@1=65.377 Prec@5=86.240 rate=1.74 Hz, eta=0:14:25, total=0:09:36, wall=09:32 IST


=================================================================================================
RESTARTING TRAINING
=================================================================================================
[39m
=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'num_classes': 1000, 'strides': (2, 2, 2, 2, 2)}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv1_x1', 'dataset_name': 'image_folder_classification', 'workers': 8, 'logger': <modules.xtensor.xnn.utils.logger.TeeLogger object at 0x7ff8914e5048>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': 0.0001, 'print_freq': 100, 'resume': './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224/checkpoint.pth.tar', 'evaluate': False, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'date': None, 'save_path': './checkpoints/image_folder_classification/2019-09-06_17-15-44_image_folder_classification_mobilenetv1_x1_resize256_crop224', 'generate_onnx': False, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': [0.485, 0.456, 0.406], 'image_scale': [4.366812227074235, 4.464285714285714, 4.444444444444445], 'quantize': False, 'model_surgery': None, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'channelwise_q': False, 'bias_calibration': False, 'solver': 'sgd', 'num_inputs': 1, 'distributed': False}
=> creating model 'mobilenetv1_x1'
=> feature size is:  torch.Size([1, 1024, 7, 7])
=> Size = 224, GFLOPs = 1.135432704, GMACs = 0.567716352
MobileNetV1(
  (classifier): Sequential(
    (0): BypassBlock()
    (1): Linear(in_features=1024, out_features=1000, bias=True)
  )
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
    )
    (1): Sequential(
      (0): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (2): Sequential(
      (0): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (3): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (4): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (5): Sequential(
      (0): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (6): Sequential(
      (0): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (7): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (8): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (9): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (10): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (11): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (12): Sequential(
      (0): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (13): Sequential(
      (0): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (1): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
  )
)=> args:  {'model_config': {'input_channels': 3, 'output_type': 'classification', 'output_channels': None, 'num_classes': 1000, 'strides': (2, 2, 2, 2, 2)}, 'dataset_config': {}, 'data_path': './data/datasets/image_folder_classification', 'model_name': 'mobilenetv1_x1', 'dataset_name': 'image_folder_classification', 'workers': 8, 'logger': <modules.xtensor.xnn.utils.logger.TeeLogger object at 0x7ff8914e5048>, 'epochs': 150, 'warmup_epochs': 5, 'epoch_size': 0, 'start_epoch': 0, 'stop_epoch': 150, 'batch_size': 512, 'total_batch_size': 512, 'iter_size': 1, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 4e-05, 'bias_decay': 0.0001, 'print_freq': 100, 'resume': './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224/checkpoint.pth.tar', 'evaluate': False, 'world_size': 1, 'dist_url': 'tcp://224.66.41.62:23456', 'dist_backend': 'gloo', 'optimizer': 'sgd', 'scheduler': 'cosine', 'milestones': [30, 60, 90], 'multistep_gamma': 0.1, 'polystep_power': 1.0, 'step_size': 1, 'beta': 0.999, 'pretrained': None, 'img_resize': 256, 'img_crop': 224, 'rand_scale': (0.2, 1.0), 'data_augument': 'inception', 'count_flops': True, 'date': None, 'save_path': './checkpoints/image_folder_classification/2019-09-06_17-15-44_image_folder_classification_mobilenetv1_x1_resize256_crop224', 'generate_onnx': False, 'print_model': False, 'run_soon': True, 'multi_color_modes': None, 'image_mean': [0.485, 0.456, 0.406], 'image_scale': [4.366812227074235, 4.464285714285714, 4.444444444444445], 'quantize': False, 'model_surgery': None, 'bitwidth_weights': 8, 'bitwidth_activations': 8, 'histogram_range': True, 'channelwise_q': False, 'bias_calibration': False, 'solver': 'sgd', 'num_inputs': 1, 'distributed': False}
=> optimizer type   : sgd
=> learning rate    : 0.1
=> resize resolution: 256
=> crop resolution  : 224
=> batch size       : 512
=> total batch size : 512
=> epoch size       : 0
=> data augument    : inception
=> epochs           : 150
=> resuming from checkpoint './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224/checkpoint.pth.tar'
=> resuming from checkpoint './checkpoints/image_folder_classification/2019-09-05_15-25-39_image_folder_classification_mobilenetv1_x1_resize256_crop224/checkpoint.pth.tar' (epoch 43)
[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> Validation 0.00% of 1x98...Epoch=45/150 LR=0.0802 Time=13.568 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=17:16 IST
=> Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=13.568 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=3974.93 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
** Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=13.568 Loss=1.052 Prec@1=72.656 Prec@5=91.602 rate=3974.93 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
** Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=0.652 Loss=1.560 Prec@1=62.826 Prec@5=84.952 rate=3974.93 Hz, eta=0:00:00, total=0:00:00, wall=17:16 IST
** Validation 100.00% of 1x98...Epoch=45/150 LR=0.0802 Time=0.652 Loss=1.560 Prec@1=62.826 Prec@5=84.952 rate=1.95 Hz, eta=0:00:00, total=0:00:50, wall=17:16 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST
=> Training   0.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=7.215 DataTime=4.504 Loss=1.528 Prec@1=64.648 Prec@5=84.570 rate=0 Hz, eta=?, total=0:00:00, wall=17:17 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=7.215 DataTime=4.504 Loss=1.528 Prec@1=64.648 Prec@5=84.570 rate=7078.94 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=7.215 DataTime=4.504 Loss=1.528 Prec@1=64.648 Prec@5=84.570 rate=7078.94 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST
=> Training   0.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.581 DataTime=0.390 Loss=1.405 Prec@1=65.857 Prec@5=86.682 rate=7078.94 Hz, eta=0:00:00, total=0:00:00, wall=17:17 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.581 DataTime=0.390 Loss=1.405 Prec@1=65.857 Prec@5=86.682 rate=1.95 Hz, eta=0:20:28, total=0:00:51, wall=17:17 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.581 DataTime=0.390 Loss=1.405 Prec@1=65.857 Prec@5=86.682 rate=1.95 Hz, eta=0:20:28, total=0:00:51, wall=17:18 IST
=> Training   4.04% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.391 Loss=1.411 Prec@1=65.771 Prec@5=86.538 rate=1.95 Hz, eta=0:20:28, total=0:00:51, wall=17:18 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.391 Loss=1.411 Prec@1=65.771 Prec@5=86.538 rate=1.88 Hz, eta=0:20:26, total=0:01:47, wall=17:18 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.391 Loss=1.411 Prec@1=65.771 Prec@5=86.538 rate=1.88 Hz, eta=0:20:26, total=0:01:47, wall=17:19 IST
=> Training   8.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.569 DataTime=0.395 Loss=1.411 Prec@1=65.850 Prec@5=86.475 rate=1.88 Hz, eta=0:20:26, total=0:01:47, wall=17:19 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.569 DataTime=0.395 Loss=1.411 Prec@1=65.850 Prec@5=86.475 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=17:19 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.569 DataTime=0.395 Loss=1.411 Prec@1=65.850 Prec@5=86.475 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=17:20 IST
=> Training   12.03% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.392 Loss=1.414 Prec@1=65.776 Prec@5=86.437 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=17:20 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.392 Loss=1.414 Prec@1=65.776 Prec@5=86.437 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=17:20 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.392 Loss=1.414 Prec@1=65.776 Prec@5=86.437 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=17:21 IST
=> Training   16.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.571 DataTime=0.396 Loss=1.417 Prec@1=65.693 Prec@5=86.441 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=17:21 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.571 DataTime=0.396 Loss=1.417 Prec@1=65.693 Prec@5=86.441 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=17:21 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.571 DataTime=0.396 Loss=1.417 Prec@1=65.693 Prec@5=86.441 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=17:22 IST
=> Training   20.02% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.394 Loss=1.420 Prec@1=65.606 Prec@5=86.392 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=17:22 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.394 Loss=1.420 Prec@1=65.606 Prec@5=86.392 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=17:22 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.394 Loss=1.420 Prec@1=65.606 Prec@5=86.392 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=17:23 IST
=> Training   24.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.395 Loss=1.424 Prec@1=65.542 Prec@5=86.338 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=17:23 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.395 Loss=1.424 Prec@1=65.542 Prec@5=86.338 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=17:23 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.568 DataTime=0.395 Loss=1.424 Prec@1=65.542 Prec@5=86.338 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=17:24 IST
=> Training   28.01% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.427 Prec@1=65.453 Prec@5=86.292 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=17:24 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.427 Prec@1=65.453 Prec@5=86.292 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=17:24 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.427 Prec@1=65.453 Prec@5=86.292 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=17:25 IST
=> Training   32.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.567 DataTime=0.395 Loss=1.431 Prec@1=65.404 Prec@5=86.241 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=17:25 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.567 DataTime=0.395 Loss=1.431 Prec@1=65.404 Prec@5=86.241 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=17:25 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.567 DataTime=0.395 Loss=1.431 Prec@1=65.404 Prec@5=86.241 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=17:26 IST
=> Training   36.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.565 DataTime=0.393 Loss=1.432 Prec@1=65.396 Prec@5=86.220 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=17:26 IST
=> Training   39.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.565 DataTime=0.393 Loss=1.432 Prec@1=65.396 Prec@5=86.220 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=17:26 IST
=> Training   39.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.565 DataTime=0.393 Loss=1.432 Prec@1=65.396 Prec@5=86.220 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=17:27 IST
=> Training   39.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.434 Prec@1=65.332 Prec@5=86.183 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=17:27 IST
=> Training   43.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.434 Prec@1=65.332 Prec@5=86.183 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=17:27 IST
=> Training   43.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.566 DataTime=0.394 Loss=1.434 Prec@1=65.332 Prec@5=86.183 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=17:28 IST
=> Training   43.99% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.392 Loss=1.436 Prec@1=65.288 Prec@5=86.170 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=17:28 IST
=> Training   47.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.392 Loss=1.436 Prec@1=65.288 Prec@5=86.170 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=17:28 IST
=> Training   47.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.392 Loss=1.436 Prec@1=65.288 Prec@5=86.170 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=17:29 IST
=> Training   47.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.392 Loss=1.437 Prec@1=65.252 Prec@5=86.143 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=17:29 IST
=> Training   51.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.392 Loss=1.437 Prec@1=65.252 Prec@5=86.143 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=17:29 IST
=> Training   51.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.392 Loss=1.437 Prec@1=65.252 Prec@5=86.143 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=17:30 IST
=> Training   51.98% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.438 Prec@1=65.221 Prec@5=86.130 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=17:30 IST
=> Training   55.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.438 Prec@1=65.221 Prec@5=86.130 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=17:30 IST
=> Training   55.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.438 Prec@1=65.221 Prec@5=86.130 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=17:30 IST
=> Training   55.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.439 Prec@1=65.201 Prec@5=86.116 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=17:30 IST
=> Training   59.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.439 Prec@1=65.201 Prec@5=86.116 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=17:30 IST
=> Training   59.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.439 Prec@1=65.201 Prec@5=86.116 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=17:31 IST
=> Training   59.97% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.440 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=17:31 IST
=> Training   63.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.440 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=17:31 IST
=> Training   63.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.440 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=17:32 IST
=> Training   63.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.441 Prec@1=65.144 Prec@5=86.088 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=17:32 IST
=> Training   67.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.441 Prec@1=65.144 Prec@5=86.088 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=17:32 IST
=> Training   67.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.441 Prec@1=65.144 Prec@5=86.088 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=17:33 IST
=> Training   67.96% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.442 Prec@1=65.138 Prec@5=86.078 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=17:33 IST
=> Training   71.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.442 Prec@1=65.138 Prec@5=86.078 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=17:33 IST
=> Training   71.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.442 Prec@1=65.138 Prec@5=86.078 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=17:34 IST
=> Training   71.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.443 Prec@1=65.110 Prec@5=86.059 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=17:34 IST
=> Training   75.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.443 Prec@1=65.110 Prec@5=86.059 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=17:34 IST
=> Training   75.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.443 Prec@1=65.110 Prec@5=86.059 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=17:35 IST
=> Training   75.95% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.444 Prec@1=65.096 Prec@5=86.051 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=17:35 IST
=> Training   79.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.444 Prec@1=65.096 Prec@5=86.051 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=17:35 IST
=> Training   79.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.391 Loss=1.444 Prec@1=65.096 Prec@5=86.051 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=17:36 IST
=> Training   79.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.446 Prec@1=65.058 Prec@5=86.034 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=17:36 IST
=> Training   83.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.446 Prec@1=65.058 Prec@5=86.034 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=17:36 IST
=> Training   83.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.564 DataTime=0.391 Loss=1.446 Prec@1=65.058 Prec@5=86.034 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=17:37 IST
=> Training   83.94% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.389 Loss=1.447 Prec@1=65.040 Prec@5=86.023 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=17:37 IST
=> Training   87.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.389 Loss=1.447 Prec@1=65.040 Prec@5=86.023 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=17:37 IST
=> Training   87.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.389 Loss=1.447 Prec@1=65.040 Prec@5=86.023 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=17:38 IST
=> Training   87.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.448 Prec@1=65.005 Prec@5=86.003 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=17:38 IST
=> Training   91.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.448 Prec@1=65.005 Prec@5=86.003 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=17:38 IST
=> Training   91.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.563 DataTime=0.390 Loss=1.448 Prec@1=65.005 Prec@5=86.003 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=17:39 IST
=> Training   91.93% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.562 DataTime=0.389 Loss=1.449 Prec@1=64.974 Prec@5=85.986 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=17:39 IST
=> Training   95.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.562 DataTime=0.389 Loss=1.449 Prec@1=64.974 Prec@5=85.986 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=17:39 IST
=> Training   95.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.562 DataTime=0.389 Loss=1.449 Prec@1=64.974 Prec@5=85.986 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=17:40 IST
=> Training   95.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.561 DataTime=0.388 Loss=1.450 Prec@1=64.952 Prec@5=85.968 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=17:40 IST
=> Training   99.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.561 DataTime=0.388 Loss=1.450 Prec@1=64.952 Prec@5=85.968 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=17:40 IST
=> Training   99.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.561 DataTime=0.388 Loss=1.450 Prec@1=64.952 Prec@5=85.968 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=17:40 IST
=> Training   99.92% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.562 DataTime=0.388 Loss=1.450 Prec@1=64.951 Prec@5=85.968 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=17:40 IST
=> Training   100.00% of 1x2503...Epoch=45/150 LR=0.0802 Time=0.562 DataTime=0.388 Loss=1.450 Prec@1=64.951 Prec@5=85.968 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=17:40 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:40 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:40 IST
=> Validation 0.00% of 1x98...Epoch=45/150 LR=0.0802 Time=6.473 Loss=1.123 Prec@1=75.000 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=17:40 IST
=> Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=6.473 Loss=1.123 Prec@1=75.000 Prec@5=90.234 rate=4972.90 Hz, eta=0:00:00, total=0:00:00, wall=17:40 IST
** Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=6.473 Loss=1.123 Prec@1=75.000 Prec@5=90.234 rate=4972.90 Hz, eta=0:00:00, total=0:00:00, wall=17:41 IST
** Validation 1.02% of 1x98...Epoch=45/150 LR=0.0802 Time=0.630 Loss=1.579 Prec@1=62.194 Prec@5=84.618 rate=4972.90 Hz, eta=0:00:00, total=0:00:00, wall=17:41 IST
** Validation 100.00% of 1x98...Epoch=45/150 LR=0.0802 Time=0.630 Loss=1.579 Prec@1=62.194 Prec@5=84.618 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=17:41 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:41 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:41 IST
=> Training   0.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=5.234 DataTime=4.887 Loss=1.399 Prec@1=66.211 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=17:41 IST
=> Training   0.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=5.234 DataTime=4.887 Loss=1.399 Prec@1=66.211 Prec@5=87.109 rate=1517.50 Hz, eta=0:00:01, total=0:00:00, wall=17:41 IST
=> Training   0.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=5.234 DataTime=4.887 Loss=1.399 Prec@1=66.211 Prec@5=87.109 rate=1517.50 Hz, eta=0:00:01, total=0:00:00, wall=17:42 IST
=> Training   0.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.580 DataTime=0.409 Loss=1.396 Prec@1=66.186 Prec@5=86.783 rate=1517.50 Hz, eta=0:00:01, total=0:00:00, wall=17:42 IST
=> Training   4.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.580 DataTime=0.409 Loss=1.396 Prec@1=66.186 Prec@5=86.783 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=17:42 IST
=> Training   4.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.580 DataTime=0.409 Loss=1.396 Prec@1=66.186 Prec@5=86.783 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=17:43 IST
=> Training   4.04% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.575 DataTime=0.405 Loss=1.400 Prec@1=65.970 Prec@5=86.685 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=17:43 IST
=> Training   8.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.575 DataTime=0.405 Loss=1.400 Prec@1=65.970 Prec@5=86.685 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=17:43 IST
=> Training   8.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.575 DataTime=0.405 Loss=1.400 Prec@1=65.970 Prec@5=86.685 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=17:44 IST
=> Training   8.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.573 DataTime=0.401 Loss=1.411 Prec@1=65.794 Prec@5=86.506 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=17:44 IST
=> Training   12.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.573 DataTime=0.401 Loss=1.411 Prec@1=65.794 Prec@5=86.506 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=17:44 IST
=> Training   12.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.573 DataTime=0.401 Loss=1.411 Prec@1=65.794 Prec@5=86.506 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=17:45 IST
=> Training   12.03% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.570 DataTime=0.397 Loss=1.415 Prec@1=65.656 Prec@5=86.421 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=17:45 IST
=> Training   16.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.570 DataTime=0.397 Loss=1.415 Prec@1=65.656 Prec@5=86.421 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=17:45 IST
=> Training   16.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.570 DataTime=0.397 Loss=1.415 Prec@1=65.656 Prec@5=86.421 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=17:46 IST
=> Training   16.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.569 DataTime=0.395 Loss=1.417 Prec@1=65.625 Prec@5=86.442 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=17:46 IST
=> Training   20.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.569 DataTime=0.395 Loss=1.417 Prec@1=65.625 Prec@5=86.442 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=17:46 IST
=> Training   20.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.569 DataTime=0.395 Loss=1.417 Prec@1=65.625 Prec@5=86.442 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=17:47 IST
=> Training   20.02% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.392 Loss=1.418 Prec@1=65.588 Prec@5=86.404 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=17:47 IST
=> Training   24.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.392 Loss=1.418 Prec@1=65.588 Prec@5=86.404 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=17:47 IST
=> Training   24.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.392 Loss=1.418 Prec@1=65.588 Prec@5=86.404 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=17:48 IST
=> Training   24.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.391 Loss=1.421 Prec@1=65.527 Prec@5=86.397 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=17:48 IST
=> Training   28.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.391 Loss=1.421 Prec@1=65.527 Prec@5=86.397 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=17:48 IST
=> Training   28.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.567 DataTime=0.391 Loss=1.421 Prec@1=65.527 Prec@5=86.397 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=17:48 IST
=> Training   28.01% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.423 Prec@1=65.463 Prec@5=86.354 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=17:48 IST
=> Training   32.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.423 Prec@1=65.463 Prec@5=86.354 rate=1.79 Hz, eta=0:15:49, total=0:07:27, wall=17:48 IST
=> Training   32.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.423 Prec@1=65.463 Prec@5=86.354 rate=1.79 Hz, eta=0:15:49, total=0:07:27, wall=17:49 IST
=> Training   32.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.566 DataTime=0.392 Loss=1.424 Prec@1=65.459 Prec@5=86.354 rate=1.79 Hz, eta=0:15:49, total=0:07:27, wall=17:49 IST
=> Training   36.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.566 DataTime=0.392 Loss=1.424 Prec@1=65.459 Prec@5=86.354 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=17:49 IST
=> Training   36.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.566 DataTime=0.392 Loss=1.424 Prec@1=65.459 Prec@5=86.354 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=17:50 IST
=> Training   36.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.426 Prec@1=65.425 Prec@5=86.317 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=17:50 IST
=> Training   39.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.426 Prec@1=65.425 Prec@5=86.317 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:50 IST
=> Training   39.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.565 DataTime=0.390 Loss=1.426 Prec@1=65.425 Prec@5=86.317 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:51 IST
=> Training   39.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.564 DataTime=0.388 Loss=1.427 Prec@1=65.389 Prec@5=86.294 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:51 IST
=> Training   43.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.564 DataTime=0.388 Loss=1.427 Prec@1=65.389 Prec@5=86.294 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:51 IST
=> Training   43.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.564 DataTime=0.388 Loss=1.427 Prec@1=65.389 Prec@5=86.294 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:52 IST
=> Training   43.99% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.563 DataTime=0.386 Loss=1.428 Prec@1=65.389 Prec@5=86.288 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:52 IST
=> Training   47.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.563 DataTime=0.386 Loss=1.428 Prec@1=65.389 Prec@5=86.288 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=17:52 IST
=> Training   47.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.563 DataTime=0.386 Loss=1.428 Prec@1=65.389 Prec@5=86.288 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=17:53 IST
=> Training   47.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.430 Prec@1=65.338 Prec@5=86.241 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=17:53 IST
=> Training   51.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.430 Prec@1=65.338 Prec@5=86.241 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=17:53 IST
=> Training   51.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.430 Prec@1=65.338 Prec@5=86.241 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=17:54 IST
=> Training   51.98% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.432 Prec@1=65.307 Prec@5=86.198 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=17:54 IST
=> Training   55.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.432 Prec@1=65.307 Prec@5=86.198 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=17:54 IST
=> Training   55.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.432 Prec@1=65.307 Prec@5=86.198 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=17:55 IST
=> Training   55.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.433 Prec@1=65.283 Prec@5=86.180 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=17:55 IST
=> Training   59.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.433 Prec@1=65.283 Prec@5=86.180 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=17:55 IST
=> Training   59.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.562 DataTime=0.385 Loss=1.433 Prec@1=65.283 Prec@5=86.180 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=17:56 IST
=> Training   59.97% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.434 Prec@1=65.256 Prec@5=86.171 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=17:56 IST
=> Training   63.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.434 Prec@1=65.256 Prec@5=86.171 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=17:56 IST
=> Training   63.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.434 Prec@1=65.256 Prec@5=86.171 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=17:57 IST
=> Training   63.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.435 Prec@1=65.238 Prec@5=86.159 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=17:57 IST
=> Training   67.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.435 Prec@1=65.238 Prec@5=86.159 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=17:57 IST
=> Training   67.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.435 Prec@1=65.238 Prec@5=86.159 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=17:58 IST
=> Training   67.96% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.437 Prec@1=65.217 Prec@5=86.136 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=17:58 IST
=> Training   71.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.437 Prec@1=65.217 Prec@5=86.136 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=17:58 IST
=> Training   71.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.437 Prec@1=65.217 Prec@5=86.136 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=17:59 IST
=> Training   71.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.439 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=17:59 IST
=> Training   75.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.439 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=17:59 IST
=> Training   75.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.384 Loss=1.439 Prec@1=65.180 Prec@5=86.104 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=18:00 IST
=> Training   75.95% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.440 Prec@1=65.172 Prec@5=86.086 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=18:00 IST
=> Training   79.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.440 Prec@1=65.172 Prec@5=86.086 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=18:00 IST
=> Training   79.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.440 Prec@1=65.172 Prec@5=86.086 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=18:01 IST
=> Training   79.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.441 Prec@1=65.141 Prec@5=86.061 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=18:01 IST
=> Training   83.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.441 Prec@1=65.141 Prec@5=86.061 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=18:01 IST
=> Training   83.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.441 Prec@1=65.141 Prec@5=86.061 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=18:01 IST
=> Training   83.94% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.442 Prec@1=65.123 Prec@5=86.051 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=18:01 IST
=> Training   87.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.442 Prec@1=65.123 Prec@5=86.051 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=18:01 IST
=> Training   87.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.442 Prec@1=65.123 Prec@5=86.051 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=18:02 IST
=> Training   87.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.444 Prec@1=65.091 Prec@5=86.034 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=18:02 IST
=> Training   91.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.444 Prec@1=65.091 Prec@5=86.034 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:02 IST
=> Training   91.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.561 DataTime=0.385 Loss=1.444 Prec@1=65.091 Prec@5=86.034 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:03 IST
=> Training   91.93% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.445 Prec@1=65.085 Prec@5=86.012 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:03 IST
=> Training   95.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.445 Prec@1=65.085 Prec@5=86.012 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:03 IST
=> Training   95.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.445 Prec@1=65.085 Prec@5=86.012 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:04 IST
=> Training   95.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.385 Loss=1.446 Prec@1=65.069 Prec@5=85.998 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=18:04 IST
=> Training   99.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.385 Loss=1.446 Prec@1=65.069 Prec@5=85.998 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:04 IST
=> Training   99.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.385 Loss=1.446 Prec@1=65.069 Prec@5=85.998 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:04 IST
=> Training   99.92% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.446 Prec@1=65.068 Prec@5=85.998 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=18:04 IST
=> Training   100.00% of 1x2503...Epoch=46/150 LR=0.0794 Time=0.560 DataTime=0.384 Loss=1.446 Prec@1=65.068 Prec@5=85.998 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=18:04 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:04 IST
=> Validation 0.00% of 1x98...Epoch=46/150 LR=0.0794 Time=7.547 Loss=0.981 Prec@1=75.391 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=18:04 IST
=> Validation 1.02% of 1x98...Epoch=46/150 LR=0.0794 Time=7.547 Loss=0.981 Prec@1=75.391 Prec@5=92.188 rate=1567.59 Hz, eta=0:00:00, total=0:00:00, wall=18:04 IST
** Validation 1.02% of 1x98...Epoch=46/150 LR=0.0794 Time=7.547 Loss=0.981 Prec@1=75.391 Prec@5=92.188 rate=1567.59 Hz, eta=0:00:00, total=0:00:00, wall=18:05 IST
** Validation 1.02% of 1x98...Epoch=46/150 LR=0.0794 Time=0.639 Loss=1.511 Prec@1=63.754 Prec@5=85.688 rate=1567.59 Hz, eta=0:00:00, total=0:00:00, wall=18:05 IST
** Validation 100.00% of 1x98...Epoch=46/150 LR=0.0794 Time=0.639 Loss=1.511 Prec@1=63.754 Prec@5=85.688 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=18:05 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:05 IST
=> Training   0.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=5.018 DataTime=4.653 Loss=1.479 Prec@1=65.820 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=18:05 IST
=> Training   0.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=5.018 DataTime=4.653 Loss=1.479 Prec@1=65.820 Prec@5=85.156 rate=7874.64 Hz, eta=0:00:00, total=0:00:00, wall=18:05 IST
=> Training   0.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=5.018 DataTime=4.653 Loss=1.479 Prec@1=65.820 Prec@5=85.156 rate=7874.64 Hz, eta=0:00:00, total=0:00:00, wall=18:06 IST
=> Training   0.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.581 DataTime=0.419 Loss=1.403 Prec@1=65.772 Prec@5=86.634 rate=7874.64 Hz, eta=0:00:00, total=0:00:00, wall=18:06 IST
=> Training   4.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.581 DataTime=0.419 Loss=1.403 Prec@1=65.772 Prec@5=86.634 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=18:06 IST
=> Training   4.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.581 DataTime=0.419 Loss=1.403 Prec@1=65.772 Prec@5=86.634 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=18:07 IST
=> Training   4.04% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.574 DataTime=0.408 Loss=1.399 Prec@1=65.938 Prec@5=86.624 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=18:07 IST
=> Training   8.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.574 DataTime=0.408 Loss=1.399 Prec@1=65.938 Prec@5=86.624 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=18:07 IST
=> Training   8.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.574 DataTime=0.408 Loss=1.399 Prec@1=65.938 Prec@5=86.624 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=18:08 IST
=> Training   8.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.404 Loss=1.394 Prec@1=66.116 Prec@5=86.695 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=18:08 IST
=> Training   12.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.404 Loss=1.394 Prec@1=66.116 Prec@5=86.695 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=18:08 IST
=> Training   12.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.404 Loss=1.394 Prec@1=66.116 Prec@5=86.695 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=18:09 IST
=> Training   12.03% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.401 Loss=1.395 Prec@1=66.058 Prec@5=86.706 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=18:09 IST
=> Training   16.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.401 Loss=1.395 Prec@1=66.058 Prec@5=86.706 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=18:09 IST
=> Training   16.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.401 Loss=1.395 Prec@1=66.058 Prec@5=86.706 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=18:10 IST
=> Training   16.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.403 Loss=1.399 Prec@1=65.953 Prec@5=86.702 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=18:10 IST
=> Training   20.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.403 Loss=1.399 Prec@1=65.953 Prec@5=86.702 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=18:10 IST
=> Training   20.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.572 DataTime=0.403 Loss=1.399 Prec@1=65.953 Prec@5=86.702 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=18:11 IST
=> Training   20.02% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.398 Loss=1.405 Prec@1=65.892 Prec@5=86.633 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=18:11 IST
=> Training   24.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.398 Loss=1.405 Prec@1=65.892 Prec@5=86.633 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=18:11 IST
=> Training   24.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.398 Loss=1.405 Prec@1=65.892 Prec@5=86.633 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=18:12 IST
=> Training   24.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.399 Loss=1.406 Prec@1=65.880 Prec@5=86.613 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=18:12 IST
=> Training   28.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.399 Loss=1.406 Prec@1=65.880 Prec@5=86.613 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:12 IST
=> Training   28.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.568 DataTime=0.399 Loss=1.406 Prec@1=65.880 Prec@5=86.613 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:13 IST
=> Training   28.01% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.408 Prec@1=65.821 Prec@5=86.599 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:13 IST
=> Training   32.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.408 Prec@1=65.821 Prec@5=86.599 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=18:13 IST
=> Training   32.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.408 Prec@1=65.821 Prec@5=86.599 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=18:14 IST
=> Training   32.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.412 Prec@1=65.719 Prec@5=86.526 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=18:14 IST
=> Training   36.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.412 Prec@1=65.719 Prec@5=86.526 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:14 IST
=> Training   36.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.567 DataTime=0.397 Loss=1.412 Prec@1=65.719 Prec@5=86.526 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:15 IST
=> Training   36.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.396 Loss=1.416 Prec@1=65.618 Prec@5=86.467 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:15 IST
=> Training   39.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.396 Loss=1.416 Prec@1=65.618 Prec@5=86.467 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:15 IST
=> Training   39.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.396 Loss=1.416 Prec@1=65.618 Prec@5=86.467 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:16 IST
=> Training   39.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.395 Loss=1.419 Prec@1=65.566 Prec@5=86.418 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=18:16 IST
=> Training   43.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.395 Loss=1.419 Prec@1=65.566 Prec@5=86.418 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=18:16 IST
=> Training   43.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.395 Loss=1.419 Prec@1=65.566 Prec@5=86.418 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=18:17 IST
=> Training   43.99% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.394 Loss=1.422 Prec@1=65.498 Prec@5=86.387 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=18:17 IST
=> Training   47.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.394 Loss=1.422 Prec@1=65.498 Prec@5=86.387 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=18:17 IST
=> Training   47.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.565 DataTime=0.394 Loss=1.422 Prec@1=65.498 Prec@5=86.387 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=18:18 IST
=> Training   47.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.564 DataTime=0.393 Loss=1.423 Prec@1=65.462 Prec@5=86.368 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=18:18 IST
=> Training   51.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.564 DataTime=0.393 Loss=1.423 Prec@1=65.462 Prec@5=86.368 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=18:18 IST
=> Training   51.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.564 DataTime=0.393 Loss=1.423 Prec@1=65.462 Prec@5=86.368 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=18:18 IST
=> Training   51.98% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.424 Prec@1=65.449 Prec@5=86.361 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=18:18 IST
=> Training   55.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.424 Prec@1=65.449 Prec@5=86.361 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=18:18 IST
=> Training   55.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.424 Prec@1=65.449 Prec@5=86.361 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=18:19 IST
=> Training   55.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.426 Prec@1=65.420 Prec@5=86.334 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=18:19 IST
=> Training   59.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.426 Prec@1=65.420 Prec@5=86.334 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=18:19 IST
=> Training   59.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.392 Loss=1.426 Prec@1=65.420 Prec@5=86.334 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=18:20 IST
=> Training   59.97% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.427 Prec@1=65.379 Prec@5=86.319 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=18:20 IST
=> Training   63.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.427 Prec@1=65.379 Prec@5=86.319 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=18:20 IST
=> Training   63.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.427 Prec@1=65.379 Prec@5=86.319 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=18:21 IST
=> Training   63.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.429 Prec@1=65.332 Prec@5=86.286 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=18:21 IST
=> Training   67.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.429 Prec@1=65.332 Prec@5=86.286 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=18:21 IST
=> Training   67.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.391 Loss=1.429 Prec@1=65.332 Prec@5=86.286 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=18:22 IST
=> Training   67.96% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.431 Prec@1=65.296 Prec@5=86.269 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=18:22 IST
=> Training   71.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.431 Prec@1=65.296 Prec@5=86.269 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=18:22 IST
=> Training   71.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.431 Prec@1=65.296 Prec@5=86.269 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=18:23 IST
=> Training   71.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.391 Loss=1.432 Prec@1=65.264 Prec@5=86.255 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=18:23 IST
=> Training   75.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.391 Loss=1.432 Prec@1=65.264 Prec@5=86.255 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=18:23 IST
=> Training   75.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.391 Loss=1.432 Prec@1=65.264 Prec@5=86.255 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=18:24 IST
=> Training   75.95% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.433 Prec@1=65.245 Prec@5=86.235 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=18:24 IST
=> Training   79.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.433 Prec@1=65.245 Prec@5=86.235 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=18:24 IST
=> Training   79.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.433 Prec@1=65.245 Prec@5=86.235 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=18:25 IST
=> Training   79.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.220 Prec@5=86.212 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=18:25 IST
=> Training   83.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.220 Prec@5=86.212 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=18:25 IST
=> Training   83.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.220 Prec@5=86.212 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=18:26 IST
=> Training   83.94% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.211 Prec@5=86.211 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=18:26 IST
=> Training   87.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.211 Prec@5=86.211 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=18:26 IST
=> Training   87.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.390 Loss=1.434 Prec@1=65.211 Prec@5=86.211 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=18:27 IST
=> Training   87.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.436 Prec@1=65.182 Prec@5=86.189 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=18:27 IST
=> Training   91.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.436 Prec@1=65.182 Prec@5=86.189 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:27 IST
=> Training   91.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.436 Prec@1=65.182 Prec@5=86.189 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:28 IST
=> Training   91.93% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.437 Prec@1=65.162 Prec@5=86.172 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:28 IST
=> Training   95.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.437 Prec@1=65.162 Prec@5=86.172 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:28 IST
=> Training   95.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.563 DataTime=0.390 Loss=1.437 Prec@1=65.162 Prec@5=86.172 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:29 IST
=> Training   95.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.389 Loss=1.438 Prec@1=65.148 Prec@5=86.157 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:29 IST
=> Training   99.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.389 Loss=1.438 Prec@1=65.148 Prec@5=86.157 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:29 IST
=> Training   99.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.389 Loss=1.438 Prec@1=65.148 Prec@5=86.157 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:29 IST
=> Training   99.92% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.389 Loss=1.438 Prec@1=65.146 Prec@5=86.156 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:29 IST
=> Training   100.00% of 1x2503...Epoch=47/150 LR=0.0785 Time=0.562 DataTime=0.389 Loss=1.438 Prec@1=65.146 Prec@5=86.156 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=18:29 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:29 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:29 IST
=> Validation 0.00% of 1x98...Epoch=47/150 LR=0.0785 Time=7.357 Loss=0.975 Prec@1=75.781 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=18:29 IST
=> Validation 1.02% of 1x98...Epoch=47/150 LR=0.0785 Time=7.357 Loss=0.975 Prec@1=75.781 Prec@5=92.383 rate=2912.06 Hz, eta=0:00:00, total=0:00:00, wall=18:29 IST
** Validation 1.02% of 1x98...Epoch=47/150 LR=0.0785 Time=7.357 Loss=0.975 Prec@1=75.781 Prec@5=92.383 rate=2912.06 Hz, eta=0:00:00, total=0:00:00, wall=18:30 IST
** Validation 1.02% of 1x98...Epoch=47/150 LR=0.0785 Time=0.638 Loss=1.534 Prec@1=63.320 Prec@5=85.386 rate=2912.06 Hz, eta=0:00:00, total=0:00:00, wall=18:30 IST
** Validation 100.00% of 1x98...Epoch=47/150 LR=0.0785 Time=0.638 Loss=1.534 Prec@1=63.320 Prec@5=85.386 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=18:30 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:30 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:30 IST
=> Training   0.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=4.736 DataTime=4.414 Loss=1.369 Prec@1=64.648 Prec@5=87.695 rate=0 Hz, eta=?, total=0:00:00, wall=18:30 IST
=> Training   0.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=4.736 DataTime=4.414 Loss=1.369 Prec@1=64.648 Prec@5=87.695 rate=7501.14 Hz, eta=0:00:00, total=0:00:00, wall=18:30 IST
=> Training   0.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=4.736 DataTime=4.414 Loss=1.369 Prec@1=64.648 Prec@5=87.695 rate=7501.14 Hz, eta=0:00:00, total=0:00:00, wall=18:31 IST
=> Training   0.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.607 DataTime=0.439 Loss=1.396 Prec@1=66.105 Prec@5=86.814 rate=7501.14 Hz, eta=0:00:00, total=0:00:00, wall=18:31 IST
=> Training   4.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.607 DataTime=0.439 Loss=1.396 Prec@1=66.105 Prec@5=86.814 rate=1.78 Hz, eta=0:22:25, total=0:00:56, wall=18:31 IST
=> Training   4.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.607 DataTime=0.439 Loss=1.396 Prec@1=66.105 Prec@5=86.814 rate=1.78 Hz, eta=0:22:25, total=0:00:56, wall=18:32 IST
=> Training   4.04% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.589 DataTime=0.419 Loss=1.396 Prec@1=66.157 Prec@5=86.753 rate=1.78 Hz, eta=0:22:25, total=0:00:56, wall=18:32 IST
=> Training   8.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.589 DataTime=0.419 Loss=1.396 Prec@1=66.157 Prec@5=86.753 rate=1.77 Hz, eta=0:21:41, total=0:01:53, wall=18:32 IST
=> Training   8.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.589 DataTime=0.419 Loss=1.396 Prec@1=66.157 Prec@5=86.753 rate=1.77 Hz, eta=0:21:41, total=0:01:53, wall=18:33 IST
=> Training   8.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.582 DataTime=0.412 Loss=1.397 Prec@1=66.127 Prec@5=86.738 rate=1.77 Hz, eta=0:21:41, total=0:01:53, wall=18:33 IST
=> Training   12.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.582 DataTime=0.412 Loss=1.397 Prec@1=66.127 Prec@5=86.738 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=18:33 IST
=> Training   12.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.582 DataTime=0.412 Loss=1.397 Prec@1=66.127 Prec@5=86.738 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=18:34 IST
=> Training   12.03% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.579 DataTime=0.407 Loss=1.397 Prec@1=66.092 Prec@5=86.711 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=18:34 IST
=> Training   16.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.579 DataTime=0.407 Loss=1.397 Prec@1=66.092 Prec@5=86.711 rate=1.76 Hz, eta=0:19:51, total=0:03:47, wall=18:34 IST
=> Training   16.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.579 DataTime=0.407 Loss=1.397 Prec@1=66.092 Prec@5=86.711 rate=1.76 Hz, eta=0:19:51, total=0:03:47, wall=18:35 IST
=> Training   16.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.575 DataTime=0.403 Loss=1.400 Prec@1=66.082 Prec@5=86.684 rate=1.76 Hz, eta=0:19:51, total=0:03:47, wall=18:35 IST
=> Training   20.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.575 DataTime=0.403 Loss=1.400 Prec@1=66.082 Prec@5=86.684 rate=1.77 Hz, eta=0:18:53, total=0:04:43, wall=18:35 IST
=> Training   20.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.575 DataTime=0.403 Loss=1.400 Prec@1=66.082 Prec@5=86.684 rate=1.77 Hz, eta=0:18:53, total=0:04:43, wall=18:36 IST
=> Training   20.02% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.571 DataTime=0.399 Loss=1.403 Prec@1=66.032 Prec@5=86.652 rate=1.77 Hz, eta=0:18:53, total=0:04:43, wall=18:36 IST
=> Training   24.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.571 DataTime=0.399 Loss=1.403 Prec@1=66.032 Prec@5=86.652 rate=1.78 Hz, eta=0:17:50, total=0:05:38, wall=18:36 IST
=> Training   24.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.571 DataTime=0.399 Loss=1.403 Prec@1=66.032 Prec@5=86.652 rate=1.78 Hz, eta=0:17:50, total=0:05:38, wall=18:36 IST
=> Training   24.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.395 Loss=1.404 Prec@1=65.981 Prec@5=86.626 rate=1.78 Hz, eta=0:17:50, total=0:05:38, wall=18:36 IST
=> Training   28.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.395 Loss=1.404 Prec@1=65.981 Prec@5=86.626 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:36 IST
=> Training   28.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.395 Loss=1.404 Prec@1=65.981 Prec@5=86.626 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:37 IST
=> Training   28.01% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.394 Loss=1.407 Prec@1=65.927 Prec@5=86.573 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=18:37 IST
=> Training   32.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.394 Loss=1.407 Prec@1=65.927 Prec@5=86.573 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=18:37 IST
=> Training   32.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.568 DataTime=0.394 Loss=1.407 Prec@1=65.927 Prec@5=86.573 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=18:38 IST
=> Training   32.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.566 DataTime=0.392 Loss=1.407 Prec@1=65.888 Prec@5=86.563 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=18:38 IST
=> Training   36.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.566 DataTime=0.392 Loss=1.407 Prec@1=65.888 Prec@5=86.563 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=18:38 IST
=> Training   36.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.566 DataTime=0.392 Loss=1.407 Prec@1=65.888 Prec@5=86.563 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=18:39 IST
=> Training   36.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.565 DataTime=0.392 Loss=1.409 Prec@1=65.876 Prec@5=86.530 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=18:39 IST
=> Training   39.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.565 DataTime=0.392 Loss=1.409 Prec@1=65.876 Prec@5=86.530 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=18:39 IST
=> Training   39.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.565 DataTime=0.392 Loss=1.409 Prec@1=65.876 Prec@5=86.530 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=18:40 IST
=> Training   39.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.390 Loss=1.411 Prec@1=65.807 Prec@5=86.495 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=18:40 IST
=> Training   43.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.390 Loss=1.411 Prec@1=65.807 Prec@5=86.495 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:40 IST
=> Training   43.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.390 Loss=1.411 Prec@1=65.807 Prec@5=86.495 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:41 IST
=> Training   43.99% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.391 Loss=1.412 Prec@1=65.766 Prec@5=86.486 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=18:41 IST
=> Training   47.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.391 Loss=1.412 Prec@1=65.766 Prec@5=86.486 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:41 IST
=> Training   47.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.563 DataTime=0.391 Loss=1.412 Prec@1=65.766 Prec@5=86.486 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:42 IST
=> Training   47.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.413 Prec@1=65.739 Prec@5=86.454 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:42 IST
=> Training   51.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.413 Prec@1=65.739 Prec@5=86.454 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=18:42 IST
=> Training   51.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.413 Prec@1=65.739 Prec@5=86.454 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=18:43 IST
=> Training   51.98% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.415 Prec@1=65.686 Prec@5=86.432 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=18:43 IST
=> Training   55.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.415 Prec@1=65.686 Prec@5=86.432 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=18:43 IST
=> Training   55.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.415 Prec@1=65.686 Prec@5=86.432 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=18:44 IST
=> Training   55.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.417 Prec@1=65.646 Prec@5=86.412 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=18:44 IST
=> Training   59.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.417 Prec@1=65.646 Prec@5=86.412 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=18:44 IST
=> Training   59.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.417 Prec@1=65.646 Prec@5=86.412 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=18:45 IST
=> Training   59.97% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.418 Prec@1=65.638 Prec@5=86.400 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=18:45 IST
=> Training   63.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.418 Prec@1=65.638 Prec@5=86.400 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=18:45 IST
=> Training   63.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.418 Prec@1=65.638 Prec@5=86.400 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=18:46 IST
=> Training   63.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.420 Prec@1=65.615 Prec@5=86.390 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=18:46 IST
=> Training   67.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.420 Prec@1=65.615 Prec@5=86.390 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=18:46 IST
=> Training   67.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.420 Prec@1=65.615 Prec@5=86.390 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=18:47 IST
=> Training   67.96% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.421 Prec@1=65.580 Prec@5=86.368 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=18:47 IST
=> Training   71.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.421 Prec@1=65.580 Prec@5=86.368 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=18:47 IST
=> Training   71.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.390 Loss=1.421 Prec@1=65.580 Prec@5=86.368 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=18:48 IST
=> Training   71.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.423 Prec@1=65.541 Prec@5=86.346 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=18:48 IST
=> Training   75.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.423 Prec@1=65.541 Prec@5=86.346 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=18:48 IST
=> Training   75.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.423 Prec@1=65.541 Prec@5=86.346 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=18:49 IST
=> Training   75.95% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.425 Prec@1=65.498 Prec@5=86.318 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=18:49 IST
=> Training   79.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.425 Prec@1=65.498 Prec@5=86.318 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=18:49 IST
=> Training   79.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.425 Prec@1=65.498 Prec@5=86.318 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=18:49 IST
=> Training   79.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.427 Prec@1=65.462 Prec@5=86.288 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=18:49 IST
=> Training   83.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.427 Prec@1=65.462 Prec@5=86.288 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=18:49 IST
=> Training   83.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.562 DataTime=0.389 Loss=1.427 Prec@1=65.462 Prec@5=86.288 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=18:50 IST
=> Training   83.94% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.389 Loss=1.428 Prec@1=65.427 Prec@5=86.283 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=18:50 IST
=> Training   87.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.389 Loss=1.428 Prec@1=65.427 Prec@5=86.283 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=18:50 IST
=> Training   87.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.389 Loss=1.428 Prec@1=65.427 Prec@5=86.283 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=18:51 IST
=> Training   87.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.388 Loss=1.429 Prec@1=65.386 Prec@5=86.262 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=18:51 IST
=> Training   91.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.388 Loss=1.429 Prec@1=65.386 Prec@5=86.262 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:51 IST
=> Training   91.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.561 DataTime=0.388 Loss=1.429 Prec@1=65.386 Prec@5=86.262 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:52 IST
=> Training   91.93% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.430 Prec@1=65.372 Prec@5=86.245 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=18:52 IST
=> Training   95.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.430 Prec@1=65.372 Prec@5=86.245 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=18:52 IST
=> Training   95.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.430 Prec@1=65.372 Prec@5=86.245 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=18:53 IST
=> Training   95.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.431 Prec@1=65.355 Prec@5=86.233 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=18:53 IST
=> Training   99.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.431 Prec@1=65.355 Prec@5=86.233 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=18:53 IST
=> Training   99.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.431 Prec@1=65.355 Prec@5=86.233 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=18:53 IST
=> Training   99.92% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.431 Prec@1=65.353 Prec@5=86.233 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=18:53 IST
=> Training   100.00% of 1x2503...Epoch=48/150 LR=0.0777 Time=0.560 DataTime=0.388 Loss=1.431 Prec@1=65.353 Prec@5=86.233 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=18:53 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> Validation 0.00% of 1x98...Epoch=48/150 LR=0.0777 Time=7.016 Loss=1.056 Prec@1=72.070 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=18:53 IST
=> Validation 1.02% of 1x98...Epoch=48/150 LR=0.0777 Time=7.016 Loss=1.056 Prec@1=72.070 Prec@5=91.992 rate=7393.71 Hz, eta=0:00:00, total=0:00:00, wall=18:53 IST
** Validation 1.02% of 1x98...Epoch=48/150 LR=0.0777 Time=7.016 Loss=1.056 Prec@1=72.070 Prec@5=91.992 rate=7393.71 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** Validation 1.02% of 1x98...Epoch=48/150 LR=0.0777 Time=0.630 Loss=1.498 Prec@1=64.028 Prec@5=85.812 rate=7393.71 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
** Validation 100.00% of 1x98...Epoch=48/150 LR=0.0777 Time=0.630 Loss=1.498 Prec@1=64.028 Prec@5=85.812 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=18:54 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> Training   0.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=5.092 DataTime=4.779 Loss=1.461 Prec@1=64.453 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=18:54 IST
=> Training   0.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=5.092 DataTime=4.779 Loss=1.461 Prec@1=64.453 Prec@5=85.938 rate=8345.50 Hz, eta=0:00:00, total=0:00:00, wall=18:54 IST
=> Training   0.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=5.092 DataTime=4.779 Loss=1.461 Prec@1=64.453 Prec@5=85.938 rate=8345.50 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> Training   0.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.591 DataTime=0.420 Loss=1.384 Prec@1=66.404 Prec@5=86.893 rate=8345.50 Hz, eta=0:00:00, total=0:00:00, wall=18:55 IST
=> Training   4.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.591 DataTime=0.420 Loss=1.384 Prec@1=66.404 Prec@5=86.893 rate=1.85 Hz, eta=0:21:36, total=0:00:54, wall=18:55 IST
=> Training   4.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.591 DataTime=0.420 Loss=1.384 Prec@1=66.404 Prec@5=86.893 rate=1.85 Hz, eta=0:21:36, total=0:00:54, wall=18:56 IST
=> Training   4.04% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.571 DataTime=0.396 Loss=1.392 Prec@1=66.242 Prec@5=86.771 rate=1.85 Hz, eta=0:21:36, total=0:00:54, wall=18:56 IST
=> Training   8.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.571 DataTime=0.396 Loss=1.392 Prec@1=66.242 Prec@5=86.771 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:56 IST
=> Training   8.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.571 DataTime=0.396 Loss=1.392 Prec@1=66.242 Prec@5=86.771 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:57 IST
=> Training   8.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.564 DataTime=0.391 Loss=1.393 Prec@1=66.214 Prec@5=86.776 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=18:57 IST
=> Training   12.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.564 DataTime=0.391 Loss=1.393 Prec@1=66.214 Prec@5=86.776 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=18:57 IST
=> Training   12.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.564 DataTime=0.391 Loss=1.393 Prec@1=66.214 Prec@5=86.776 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=18:58 IST
=> Training   12.03% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.390 Prec@1=66.210 Prec@5=86.823 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=18:58 IST
=> Training   16.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.390 Prec@1=66.210 Prec@5=86.823 rate=1.83 Hz, eta=0:19:08, total=0:03:39, wall=18:58 IST
=> Training   16.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.390 Prec@1=66.210 Prec@5=86.823 rate=1.83 Hz, eta=0:19:08, total=0:03:39, wall=18:59 IST
=> Training   16.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.250 Prec@5=86.862 rate=1.83 Hz, eta=0:19:08, total=0:03:39, wall=18:59 IST
=> Training   20.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.250 Prec@5=86.862 rate=1.82 Hz, eta=0:18:18, total=0:04:34, wall=18:59 IST
=> Training   20.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.250 Prec@5=86.862 rate=1.82 Hz, eta=0:18:18, total=0:04:34, wall=19:00 IST
=> Training   20.02% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.557 DataTime=0.385 Loss=1.391 Prec@1=66.194 Prec@5=86.833 rate=1.82 Hz, eta=0:18:18, total=0:04:34, wall=19:00 IST
=> Training   24.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.557 DataTime=0.385 Loss=1.391 Prec@1=66.194 Prec@5=86.833 rate=1.82 Hz, eta=0:17:23, total=0:05:29, wall=19:00 IST
=> Training   24.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.557 DataTime=0.385 Loss=1.391 Prec@1=66.194 Prec@5=86.833 rate=1.82 Hz, eta=0:17:23, total=0:05:29, wall=19:01 IST
=> Training   24.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.394 Prec@1=66.145 Prec@5=86.786 rate=1.82 Hz, eta=0:17:23, total=0:05:29, wall=19:01 IST
=> Training   28.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.394 Prec@1=66.145 Prec@5=86.786 rate=1.81 Hz, eta=0:16:33, total=0:06:26, wall=19:01 IST
=> Training   28.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.394 Prec@1=66.145 Prec@5=86.786 rate=1.81 Hz, eta=0:16:33, total=0:06:26, wall=19:02 IST
=> Training   28.01% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.398 Prec@1=66.036 Prec@5=86.716 rate=1.81 Hz, eta=0:16:33, total=0:06:26, wall=19:02 IST
=> Training   32.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.398 Prec@1=66.036 Prec@5=86.716 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=19:02 IST
=> Training   32.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.398 Prec@1=66.036 Prec@5=86.716 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=19:03 IST
=> Training   32.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.399 Prec@1=66.018 Prec@5=86.701 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=19:03 IST
=> Training   36.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.399 Prec@1=66.018 Prec@5=86.701 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:03 IST
=> Training   36.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.399 Prec@1=66.018 Prec@5=86.701 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:04 IST
=> Training   36.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.401 Prec@1=65.990 Prec@5=86.686 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:04 IST
=> Training   39.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.401 Prec@1=65.990 Prec@5=86.686 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:04 IST
=> Training   39.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.401 Prec@1=65.990 Prec@5=86.686 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:04 IST
=> Training   39.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.402 Prec@1=65.935 Prec@5=86.661 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=19:04 IST
=> Training   43.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.402 Prec@1=65.935 Prec@5=86.661 rate=1.81 Hz, eta=0:12:56, total=0:10:09, wall=19:04 IST
=> Training   43.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.387 Loss=1.402 Prec@1=65.935 Prec@5=86.661 rate=1.81 Hz, eta=0:12:56, total=0:10:09, wall=19:05 IST
=> Training   43.99% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.406 Prec@1=65.855 Prec@5=86.611 rate=1.81 Hz, eta=0:12:56, total=0:10:09, wall=19:05 IST
=> Training   47.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.406 Prec@1=65.855 Prec@5=86.611 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=19:05 IST
=> Training   47.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.388 Loss=1.406 Prec@1=65.855 Prec@5=86.611 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=19:06 IST
=> Training   47.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.407 Prec@1=65.835 Prec@5=86.590 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=19:06 IST
=> Training   51.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.407 Prec@1=65.835 Prec@5=86.590 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=19:06 IST
=> Training   51.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.387 Loss=1.407 Prec@1=65.835 Prec@5=86.590 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=19:07 IST
=> Training   51.98% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.387 Loss=1.409 Prec@1=65.797 Prec@5=86.554 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=19:07 IST
=> Training   55.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.387 Loss=1.409 Prec@1=65.797 Prec@5=86.554 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:07 IST
=> Training   55.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.387 Loss=1.409 Prec@1=65.797 Prec@5=86.554 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:08 IST
=> Training   55.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.756 Prec@5=86.517 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:08 IST
=> Training   59.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.756 Prec@5=86.517 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:08 IST
=> Training   59.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.756 Prec@5=86.517 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:09 IST
=> Training   59.97% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.742 Prec@5=86.509 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:09 IST
=> Training   63.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.742 Prec@5=86.509 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:09 IST
=> Training   63.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.560 DataTime=0.386 Loss=1.412 Prec@1=65.742 Prec@5=86.509 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:10 IST
=> Training   63.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.412 Prec@1=65.732 Prec@5=86.506 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:10 IST
=> Training   67.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.412 Prec@1=65.732 Prec@5=86.506 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=19:10 IST
=> Training   67.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.412 Prec@1=65.732 Prec@5=86.506 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=19:11 IST
=> Training   67.96% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.414 Prec@1=65.704 Prec@5=86.481 rate=1.80 Hz, eta=0:07:25, total=0:15:45, wall=19:11 IST
=> Training   71.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.414 Prec@1=65.704 Prec@5=86.481 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=19:11 IST
=> Training   71.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.414 Prec@1=65.704 Prec@5=86.481 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=19:12 IST
=> Training   71.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.415 Prec@1=65.676 Prec@5=86.469 rate=1.80 Hz, eta=0:06:29, total=0:16:40, wall=19:12 IST
=> Training   75.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.415 Prec@1=65.676 Prec@5=86.469 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=19:12 IST
=> Training   75.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.415 Prec@1=65.676 Prec@5=86.469 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=19:13 IST
=> Training   75.95% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.417 Prec@1=65.644 Prec@5=86.442 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=19:13 IST
=> Training   79.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.417 Prec@1=65.644 Prec@5=86.442 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=19:13 IST
=> Training   79.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.417 Prec@1=65.644 Prec@5=86.442 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=19:14 IST
=> Training   79.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.418 Prec@1=65.628 Prec@5=86.418 rate=1.80 Hz, eta=0:04:38, total=0:18:31, wall=19:14 IST
=> Training   83.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.418 Prec@1=65.628 Prec@5=86.418 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=19:14 IST
=> Training   83.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.418 Prec@1=65.628 Prec@5=86.418 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=19:15 IST
=> Training   83.94% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.419 Prec@1=65.606 Prec@5=86.417 rate=1.80 Hz, eta=0:03:43, total=0:19:28, wall=19:15 IST
=> Training   87.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.419 Prec@1=65.606 Prec@5=86.417 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=19:15 IST
=> Training   87.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.419 Prec@1=65.606 Prec@5=86.417 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=19:16 IST
=> Training   87.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.421 Prec@1=65.559 Prec@5=86.387 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=19:16 IST
=> Training   91.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.421 Prec@1=65.559 Prec@5=86.387 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=19:16 IST
=> Training   91.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.385 Loss=1.421 Prec@1=65.559 Prec@5=86.387 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=19:17 IST
=> Training   91.93% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.423 Prec@1=65.520 Prec@5=86.366 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=19:17 IST
=> Training   95.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.423 Prec@1=65.520 Prec@5=86.366 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=19:17 IST
=> Training   95.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.559 DataTime=0.384 Loss=1.423 Prec@1=65.520 Prec@5=86.366 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=19:17 IST
=> Training   95.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.424 Prec@1=65.491 Prec@5=86.350 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=19:17 IST
=> Training   99.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.424 Prec@1=65.491 Prec@5=86.350 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=19:17 IST
=> Training   99.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.424 Prec@1=65.491 Prec@5=86.350 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=19:18 IST
=> Training   99.92% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.424 Prec@1=65.492 Prec@5=86.350 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=19:18 IST
=> Training   100.00% of 1x2503...Epoch=49/150 LR=0.0768 Time=0.558 DataTime=0.384 Loss=1.424 Prec@1=65.492 Prec@5=86.350 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=19:18 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:18 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:18 IST
=> Validation 0.00% of 1x98...Epoch=49/150 LR=0.0768 Time=6.536 Loss=0.881 Prec@1=77.344 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=19:18 IST
=> Validation 1.02% of 1x98...Epoch=49/150 LR=0.0768 Time=6.536 Loss=0.881 Prec@1=77.344 Prec@5=92.773 rate=407.77 Hz, eta=0:00:00, total=0:00:00, wall=19:18 IST
** Validation 1.02% of 1x98...Epoch=49/150 LR=0.0768 Time=6.536 Loss=0.881 Prec@1=77.344 Prec@5=92.773 rate=407.77 Hz, eta=0:00:00, total=0:00:00, wall=19:19 IST
** Validation 1.02% of 1x98...Epoch=49/150 LR=0.0768 Time=0.627 Loss=1.522 Prec@1=63.826 Prec@5=85.496 rate=407.77 Hz, eta=0:00:00, total=0:00:00, wall=19:19 IST
** Validation 100.00% of 1x98...Epoch=49/150 LR=0.0768 Time=0.627 Loss=1.522 Prec@1=63.826 Prec@5=85.496 rate=1.78 Hz, eta=0:00:00, total=0:00:54, wall=19:19 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> Training   0.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=4.956 DataTime=4.558 Loss=1.515 Prec@1=62.891 Prec@5=84.766 rate=0 Hz, eta=?, total=0:00:00, wall=19:19 IST
=> Training   0.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=4.956 DataTime=4.558 Loss=1.515 Prec@1=62.891 Prec@5=84.766 rate=4156.28 Hz, eta=0:00:00, total=0:00:00, wall=19:19 IST
=> Training   0.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=4.956 DataTime=4.558 Loss=1.515 Prec@1=62.891 Prec@5=84.766 rate=4156.28 Hz, eta=0:00:00, total=0:00:00, wall=19:20 IST
=> Training   0.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.587 DataTime=0.419 Loss=1.380 Prec@1=66.261 Prec@5=86.947 rate=4156.28 Hz, eta=0:00:00, total=0:00:00, wall=19:20 IST
=> Training   4.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.587 DataTime=0.419 Loss=1.380 Prec@1=66.261 Prec@5=86.947 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=19:20 IST
=> Training   4.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.587 DataTime=0.419 Loss=1.380 Prec@1=66.261 Prec@5=86.947 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=19:20 IST
=> Training   4.04% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.580 DataTime=0.410 Loss=1.381 Prec@1=66.299 Prec@5=86.950 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=19:20 IST
=> Training   8.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.580 DataTime=0.410 Loss=1.381 Prec@1=66.299 Prec@5=86.950 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=19:20 IST
=> Training   8.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.580 DataTime=0.410 Loss=1.381 Prec@1=66.299 Prec@5=86.950 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=19:21 IST
=> Training   8.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.572 DataTime=0.402 Loss=1.382 Prec@1=66.323 Prec@5=86.919 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=19:21 IST
=> Training   12.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.572 DataTime=0.402 Loss=1.382 Prec@1=66.323 Prec@5=86.919 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=19:21 IST
=> Training   12.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.572 DataTime=0.402 Loss=1.382 Prec@1=66.323 Prec@5=86.919 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=19:22 IST
=> Training   12.03% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.570 DataTime=0.400 Loss=1.380 Prec@1=66.384 Prec@5=86.988 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=19:22 IST
=> Training   16.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.570 DataTime=0.400 Loss=1.380 Prec@1=66.384 Prec@5=86.988 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=19:22 IST
=> Training   16.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.570 DataTime=0.400 Loss=1.380 Prec@1=66.384 Prec@5=86.988 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=19:23 IST
=> Training   16.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.567 DataTime=0.396 Loss=1.383 Prec@1=66.317 Prec@5=86.923 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=19:23 IST
=> Training   20.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.567 DataTime=0.396 Loss=1.383 Prec@1=66.317 Prec@5=86.923 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=19:23 IST
=> Training   20.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.567 DataTime=0.396 Loss=1.383 Prec@1=66.317 Prec@5=86.923 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=19:24 IST
=> Training   20.02% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.569 DataTime=0.398 Loss=1.388 Prec@1=66.251 Prec@5=86.852 rate=1.80 Hz, eta=0:18:35, total=0:04:39, wall=19:24 IST
=> Training   24.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.569 DataTime=0.398 Loss=1.388 Prec@1=66.251 Prec@5=86.852 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=19:24 IST
=> Training   24.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.569 DataTime=0.398 Loss=1.388 Prec@1=66.251 Prec@5=86.852 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=19:25 IST
=> Training   24.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.396 Loss=1.389 Prec@1=66.230 Prec@5=86.838 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=19:25 IST
=> Training   28.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.396 Loss=1.389 Prec@1=66.230 Prec@5=86.838 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=19:25 IST
=> Training   28.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.396 Loss=1.389 Prec@1=66.230 Prec@5=86.838 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=19:26 IST
=> Training   28.01% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.395 Loss=1.390 Prec@1=66.191 Prec@5=86.819 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=19:26 IST
=> Training   32.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.395 Loss=1.390 Prec@1=66.191 Prec@5=86.819 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=19:26 IST
=> Training   32.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.568 DataTime=0.395 Loss=1.390 Prec@1=66.191 Prec@5=86.819 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=19:27 IST
=> Training   32.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.566 DataTime=0.393 Loss=1.394 Prec@1=66.118 Prec@5=86.754 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=19:27 IST
=> Training   36.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.566 DataTime=0.393 Loss=1.394 Prec@1=66.118 Prec@5=86.754 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:27 IST
=> Training   36.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.566 DataTime=0.393 Loss=1.394 Prec@1=66.118 Prec@5=86.754 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:28 IST
=> Training   36.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.565 DataTime=0.392 Loss=1.396 Prec@1=66.081 Prec@5=86.746 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=19:28 IST
=> Training   39.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.565 DataTime=0.392 Loss=1.396 Prec@1=66.081 Prec@5=86.746 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=19:28 IST
=> Training   39.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.565 DataTime=0.392 Loss=1.396 Prec@1=66.081 Prec@5=86.746 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=19:29 IST
=> Training   39.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.399 Prec@1=66.015 Prec@5=86.707 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=19:29 IST
=> Training   43.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.399 Prec@1=66.015 Prec@5=86.707 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:29 IST
=> Training   43.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.399 Prec@1=66.015 Prec@5=86.707 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:30 IST
=> Training   43.99% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.400 Prec@1=65.991 Prec@5=86.682 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:30 IST
=> Training   47.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.400 Prec@1=65.991 Prec@5=86.682 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=19:30 IST
=> Training   47.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.400 Prec@1=65.991 Prec@5=86.682 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=19:31 IST
=> Training   47.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.402 Prec@1=65.933 Prec@5=86.656 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=19:31 IST
=> Training   51.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.402 Prec@1=65.933 Prec@5=86.656 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=19:31 IST
=> Training   51.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.391 Loss=1.402 Prec@1=65.933 Prec@5=86.656 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=19:32 IST
=> Training   51.98% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.404 Prec@1=65.916 Prec@5=86.624 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=19:32 IST
=> Training   55.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.404 Prec@1=65.916 Prec@5=86.624 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=19:32 IST
=> Training   55.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.404 Prec@1=65.916 Prec@5=86.624 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=19:33 IST
=> Training   55.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.406 Prec@1=65.880 Prec@5=86.595 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=19:33 IST
=> Training   59.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.406 Prec@1=65.880 Prec@5=86.595 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=19:33 IST
=> Training   59.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.563 DataTime=0.390 Loss=1.406 Prec@1=65.880 Prec@5=86.595 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=19:34 IST
=> Training   59.97% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.389 Loss=1.408 Prec@1=65.822 Prec@5=86.557 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=19:34 IST
=> Training   63.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.389 Loss=1.408 Prec@1=65.822 Prec@5=86.557 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:34 IST
=> Training   63.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.389 Loss=1.408 Prec@1=65.822 Prec@5=86.557 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:34 IST
=> Training   63.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.388 Loss=1.408 Prec@1=65.816 Prec@5=86.566 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=19:34 IST
=> Training   67.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.388 Loss=1.408 Prec@1=65.816 Prec@5=86.566 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=19:34 IST
=> Training   67.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.562 DataTime=0.388 Loss=1.408 Prec@1=65.816 Prec@5=86.566 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=19:35 IST
=> Training   67.96% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.408 Prec@1=65.810 Prec@5=86.559 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=19:35 IST
=> Training   71.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.408 Prec@1=65.810 Prec@5=86.559 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=19:35 IST
=> Training   71.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.408 Prec@1=65.810 Prec@5=86.559 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=19:36 IST
=> Training   71.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.410 Prec@1=65.787 Prec@5=86.539 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=19:36 IST
=> Training   75.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.410 Prec@1=65.787 Prec@5=86.539 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=19:36 IST
=> Training   75.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.410 Prec@1=65.787 Prec@5=86.539 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=19:37 IST
=> Training   75.95% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.410 Prec@1=65.762 Prec@5=86.544 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=19:37 IST
=> Training   79.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.410 Prec@1=65.762 Prec@5=86.544 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:37 IST
=> Training   79.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.410 Prec@1=65.762 Prec@5=86.544 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:38 IST
=> Training   79.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.412 Prec@1=65.741 Prec@5=86.521 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:38 IST
=> Training   83.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.412 Prec@1=65.741 Prec@5=86.521 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=19:38 IST
=> Training   83.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.561 DataTime=0.388 Loss=1.412 Prec@1=65.741 Prec@5=86.521 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=19:39 IST
=> Training   83.94% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.413 Prec@1=65.723 Prec@5=86.500 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=19:39 IST
=> Training   87.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.413 Prec@1=65.723 Prec@5=86.500 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:39 IST
=> Training   87.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.413 Prec@1=65.723 Prec@5=86.500 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:40 IST
=> Training   87.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.414 Prec@1=65.708 Prec@5=86.485 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:40 IST
=> Training   91.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.414 Prec@1=65.708 Prec@5=86.485 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:40 IST
=> Training   91.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.387 Loss=1.414 Prec@1=65.708 Prec@5=86.485 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:41 IST
=> Training   91.93% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.386 Loss=1.414 Prec@1=65.700 Prec@5=86.465 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=19:41 IST
=> Training   95.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.386 Loss=1.414 Prec@1=65.700 Prec@5=86.465 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:41 IST
=> Training   95.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.560 DataTime=0.386 Loss=1.414 Prec@1=65.700 Prec@5=86.465 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:42 IST
=> Training   95.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.559 DataTime=0.386 Loss=1.415 Prec@1=65.670 Prec@5=86.445 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:42 IST
=> Training   99.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.559 DataTime=0.386 Loss=1.415 Prec@1=65.670 Prec@5=86.445 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:42 IST
=> Training   99.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.559 DataTime=0.386 Loss=1.415 Prec@1=65.670 Prec@5=86.445 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:42 IST
=> Training   99.92% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.559 DataTime=0.386 Loss=1.416 Prec@1=65.670 Prec@5=86.445 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=19:42 IST
=> Training   100.00% of 1x2503...Epoch=50/150 LR=0.0759 Time=0.559 DataTime=0.386 Loss=1.416 Prec@1=65.670 Prec@5=86.445 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=19:42 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST
=> Validation 0.00% of 1x98...Epoch=50/150 LR=0.0759 Time=7.076 Loss=1.011 Prec@1=76.758 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=19:42 IST
=> Validation 1.02% of 1x98...Epoch=50/150 LR=0.0759 Time=7.076 Loss=1.011 Prec@1=76.758 Prec@5=91.602 rate=5455.57 Hz, eta=0:00:00, total=0:00:00, wall=19:42 IST
** Validation 1.02% of 1x98...Epoch=50/150 LR=0.0759 Time=7.076 Loss=1.011 Prec@1=76.758 Prec@5=91.602 rate=5455.57 Hz, eta=0:00:00, total=0:00:00, wall=19:43 IST
** Validation 1.02% of 1x98...Epoch=50/150 LR=0.0759 Time=0.631 Loss=1.544 Prec@1=63.014 Prec@5=85.026 rate=5455.57 Hz, eta=0:00:00, total=0:00:00, wall=19:43 IST
** Validation 100.00% of 1x98...Epoch=50/150 LR=0.0759 Time=0.631 Loss=1.544 Prec@1=63.014 Prec@5=85.026 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=19:43 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST
=> Training   0.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=5.014 DataTime=4.761 Loss=1.392 Prec@1=65.430 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=19:43 IST
=> Training   0.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=5.014 DataTime=4.761 Loss=1.392 Prec@1=65.430 Prec@5=87.305 rate=2530.91 Hz, eta=0:00:00, total=0:00:00, wall=19:43 IST
=> Training   0.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=5.014 DataTime=4.761 Loss=1.392 Prec@1=65.430 Prec@5=87.305 rate=2530.91 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST
=> Training   0.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.590 DataTime=0.424 Loss=1.386 Prec@1=66.132 Prec@5=86.939 rate=2530.91 Hz, eta=0:00:00, total=0:00:00, wall=19:44 IST
=> Training   4.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.590 DataTime=0.424 Loss=1.386 Prec@1=66.132 Prec@5=86.939 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=19:44 IST
=> Training   4.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.590 DataTime=0.424 Loss=1.386 Prec@1=66.132 Prec@5=86.939 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=19:45 IST
=> Training   4.04% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.574 DataTime=0.407 Loss=1.371 Prec@1=66.432 Prec@5=86.988 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=19:45 IST
=> Training   8.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.574 DataTime=0.407 Loss=1.371 Prec@1=66.432 Prec@5=86.988 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=19:45 IST
=> Training   8.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.574 DataTime=0.407 Loss=1.371 Prec@1=66.432 Prec@5=86.988 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=19:46 IST
=> Training   8.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.569 DataTime=0.398 Loss=1.373 Prec@1=66.445 Prec@5=87.015 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=19:46 IST
=> Training   12.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.569 DataTime=0.398 Loss=1.373 Prec@1=66.445 Prec@5=87.015 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=19:46 IST
=> Training   12.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.569 DataTime=0.398 Loss=1.373 Prec@1=66.445 Prec@5=87.015 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=19:47 IST
=> Training   12.03% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.566 DataTime=0.394 Loss=1.377 Prec@1=66.343 Prec@5=86.949 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=19:47 IST
=> Training   16.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.566 DataTime=0.394 Loss=1.377 Prec@1=66.343 Prec@5=86.949 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=19:47 IST
=> Training   16.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.566 DataTime=0.394 Loss=1.377 Prec@1=66.343 Prec@5=86.949 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=19:48 IST
=> Training   16.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.565 DataTime=0.393 Loss=1.376 Prec@1=66.386 Prec@5=86.962 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=19:48 IST
=> Training   20.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.565 DataTime=0.393 Loss=1.376 Prec@1=66.386 Prec@5=86.962 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=19:48 IST
=> Training   20.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.565 DataTime=0.393 Loss=1.376 Prec@1=66.386 Prec@5=86.962 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=19:49 IST
=> Training   20.02% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.562 DataTime=0.390 Loss=1.379 Prec@1=66.319 Prec@5=86.929 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=19:49 IST
=> Training   24.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.562 DataTime=0.390 Loss=1.379 Prec@1=66.319 Prec@5=86.929 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:49 IST
=> Training   24.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.562 DataTime=0.390 Loss=1.379 Prec@1=66.319 Prec@5=86.929 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:49 IST
=> Training   24.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.382 Prec@1=66.249 Prec@5=86.899 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:49 IST
=> Training   28.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.382 Prec@1=66.249 Prec@5=86.899 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=19:49 IST
=> Training   28.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.382 Prec@1=66.249 Prec@5=86.899 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=19:50 IST
=> Training   28.01% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.384 Prec@1=66.200 Prec@5=86.870 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=19:50 IST
=> Training   32.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.384 Prec@1=66.200 Prec@5=86.870 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=19:50 IST
=> Training   32.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.388 Loss=1.384 Prec@1=66.200 Prec@5=86.870 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=19:51 IST
=> Training   32.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.387 Prec@1=66.147 Prec@5=86.842 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=19:51 IST
=> Training   36.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.387 Prec@1=66.147 Prec@5=86.842 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:51 IST
=> Training   36.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.387 Prec@1=66.147 Prec@5=86.842 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:52 IST
=> Training   36.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.134 Prec@5=86.813 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=19:52 IST
=> Training   39.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.134 Prec@5=86.813 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=19:52 IST
=> Training   39.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.388 Prec@1=66.134 Prec@5=86.813 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=19:53 IST
=> Training   39.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.391 Prec@1=66.104 Prec@5=86.770 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=19:53 IST
=> Training   43.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.391 Prec@1=66.104 Prec@5=86.770 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=19:53 IST
=> Training   43.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.387 Loss=1.391 Prec@1=66.104 Prec@5=86.770 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=19:54 IST
=> Training   43.99% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.393 Prec@1=66.062 Prec@5=86.749 rate=1.80 Hz, eta=0:12:59, total=0:10:11, wall=19:54 IST
=> Training   47.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.393 Prec@1=66.062 Prec@5=86.749 rate=1.80 Hz, eta=0:12:03, total=0:11:06, wall=19:54 IST
=> Training   47.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.386 Loss=1.393 Prec@1=66.062 Prec@5=86.749 rate=1.80 Hz, eta=0:12:03, total=0:11:06, wall=19:55 IST
=> Training   47.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.395 Prec@1=66.020 Prec@5=86.739 rate=1.80 Hz, eta=0:12:03, total=0:11:06, wall=19:55 IST
=> Training   51.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.395 Prec@1=66.020 Prec@5=86.739 rate=1.80 Hz, eta=0:11:08, total=0:12:04, wall=19:55 IST
=> Training   51.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.395 Prec@1=66.020 Prec@5=86.739 rate=1.80 Hz, eta=0:11:08, total=0:12:04, wall=19:56 IST
=> Training   51.98% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.396 Prec@1=66.004 Prec@5=86.730 rate=1.80 Hz, eta=0:11:08, total=0:12:04, wall=19:56 IST
=> Training   55.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.396 Prec@1=66.004 Prec@5=86.730 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:56 IST
=> Training   55.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.396 Prec@1=66.004 Prec@5=86.730 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:57 IST
=> Training   55.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.397 Prec@1=66.007 Prec@5=86.719 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=19:57 IST
=> Training   59.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.397 Prec@1=66.007 Prec@5=86.719 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=19:57 IST
=> Training   59.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.386 Loss=1.397 Prec@1=66.007 Prec@5=86.719 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=19:58 IST
=> Training   59.97% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.400 Prec@1=65.959 Prec@5=86.690 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=19:58 IST
=> Training   63.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.400 Prec@1=65.959 Prec@5=86.690 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:58 IST
=> Training   63.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.400 Prec@1=65.959 Prec@5=86.690 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:59 IST
=> Training   63.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.561 DataTime=0.385 Loss=1.402 Prec@1=65.917 Prec@5=86.664 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=19:59 IST
=> Training   67.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.561 DataTime=0.385 Loss=1.402 Prec@1=65.917 Prec@5=86.664 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=19:59 IST
=> Training   67.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.561 DataTime=0.385 Loss=1.402 Prec@1=65.917 Prec@5=86.664 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=20:00 IST
=> Training   67.96% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.402 Prec@1=65.904 Prec@5=86.646 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=20:00 IST
=> Training   71.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.402 Prec@1=65.904 Prec@5=86.646 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=20:00 IST
=> Training   71.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.402 Prec@1=65.904 Prec@5=86.646 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=20:01 IST
=> Training   71.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.404 Prec@1=65.883 Prec@5=86.617 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=20:01 IST
=> Training   75.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.404 Prec@1=65.883 Prec@5=86.617 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=20:01 IST
=> Training   75.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.385 Loss=1.404 Prec@1=65.883 Prec@5=86.617 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=20:02 IST
=> Training   75.95% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.405 Prec@1=65.859 Prec@5=86.599 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=20:02 IST
=> Training   79.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.405 Prec@1=65.859 Prec@5=86.599 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:02 IST
=> Training   79.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.405 Prec@1=65.859 Prec@5=86.599 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:03 IST
=> Training   79.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.407 Prec@1=65.822 Prec@5=86.573 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:03 IST
=> Training   83.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.407 Prec@1=65.822 Prec@5=86.573 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:03 IST
=> Training   83.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.407 Prec@1=65.822 Prec@5=86.573 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:03 IST
=> Training   83.94% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.409 Prec@1=65.784 Prec@5=86.553 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:03 IST
=> Training   87.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.409 Prec@1=65.784 Prec@5=86.553 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=20:03 IST
=> Training   87.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.409 Prec@1=65.784 Prec@5=86.553 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=20:04 IST
=> Training   87.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.410 Prec@1=65.757 Prec@5=86.531 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=20:04 IST
=> Training   91.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.410 Prec@1=65.757 Prec@5=86.531 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:04 IST
=> Training   91.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.410 Prec@1=65.757 Prec@5=86.531 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:05 IST
=> Training   91.93% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.411 Prec@1=65.735 Prec@5=86.513 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:05 IST
=> Training   95.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.411 Prec@1=65.735 Prec@5=86.513 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:05 IST
=> Training   95.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.560 DataTime=0.384 Loss=1.411 Prec@1=65.735 Prec@5=86.513 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:06 IST
=> Training   95.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.384 Loss=1.412 Prec@1=65.734 Prec@5=86.507 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:06 IST
=> Training   99.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.384 Loss=1.412 Prec@1=65.734 Prec@5=86.507 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=20:06 IST
=> Training   99.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.384 Loss=1.412 Prec@1=65.734 Prec@5=86.507 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=20:06 IST
=> Training   99.92% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.383 Loss=1.412 Prec@1=65.733 Prec@5=86.507 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=20:06 IST
=> Training   100.00% of 1x2503...Epoch=51/150 LR=0.0750 Time=0.559 DataTime=0.383 Loss=1.412 Prec@1=65.733 Prec@5=86.507 rate=1.80 Hz, eta=0:00:00, total=0:23:14, wall=20:06 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:06 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:06 IST
=> Validation 0.00% of 1x98...Epoch=51/150 LR=0.0750 Time=7.046 Loss=1.006 Prec@1=72.656 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=20:06 IST
=> Validation 1.02% of 1x98...Epoch=51/150 LR=0.0750 Time=7.046 Loss=1.006 Prec@1=72.656 Prec@5=90.820 rate=5071.53 Hz, eta=0:00:00, total=0:00:00, wall=20:06 IST
** Validation 1.02% of 1x98...Epoch=51/150 LR=0.0750 Time=7.046 Loss=1.006 Prec@1=72.656 Prec@5=90.820 rate=5071.53 Hz, eta=0:00:00, total=0:00:00, wall=20:07 IST
** Validation 1.02% of 1x98...Epoch=51/150 LR=0.0750 Time=0.627 Loss=1.556 Prec@1=62.960 Prec@5=84.946 rate=5071.53 Hz, eta=0:00:00, total=0:00:00, wall=20:07 IST
** Validation 100.00% of 1x98...Epoch=51/150 LR=0.0750 Time=0.627 Loss=1.556 Prec@1=62.960 Prec@5=84.946 rate=1.80 Hz, eta=0:00:00, total=0:00:54, wall=20:07 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:07 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:07 IST
=> Training   0.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=4.594 DataTime=4.075 Loss=1.535 Prec@1=63.281 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=20:07 IST
=> Training   0.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=4.594 DataTime=4.075 Loss=1.535 Prec@1=63.281 Prec@5=85.156 rate=917.42 Hz, eta=0:00:02, total=0:00:00, wall=20:07 IST
=> Training   0.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=4.594 DataTime=4.075 Loss=1.535 Prec@1=63.281 Prec@5=85.156 rate=917.42 Hz, eta=0:00:02, total=0:00:00, wall=20:08 IST
=> Training   0.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.596 DataTime=0.425 Loss=1.362 Prec@1=66.969 Prec@5=87.129 rate=917.42 Hz, eta=0:00:02, total=0:00:00, wall=20:08 IST
=> Training   4.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.596 DataTime=0.425 Loss=1.362 Prec@1=66.969 Prec@5=87.129 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=20:08 IST
=> Training   4.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.596 DataTime=0.425 Loss=1.362 Prec@1=66.969 Prec@5=87.129 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=20:09 IST
=> Training   4.04% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.572 DataTime=0.403 Loss=1.363 Prec@1=66.750 Prec@5=87.122 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=20:09 IST
=> Training   8.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.572 DataTime=0.403 Loss=1.363 Prec@1=66.750 Prec@5=87.122 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:09 IST
=> Training   8.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.572 DataTime=0.403 Loss=1.363 Prec@1=66.750 Prec@5=87.122 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:10 IST
=> Training   8.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.571 DataTime=0.398 Loss=1.366 Prec@1=66.728 Prec@5=87.054 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=20:10 IST
=> Training   12.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.571 DataTime=0.398 Loss=1.366 Prec@1=66.728 Prec@5=87.054 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=20:10 IST
=> Training   12.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.571 DataTime=0.398 Loss=1.366 Prec@1=66.728 Prec@5=87.054 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=20:11 IST
=> Training   12.03% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.567 DataTime=0.394 Loss=1.372 Prec@1=66.615 Prec@5=86.989 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=20:11 IST
=> Training   16.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.567 DataTime=0.394 Loss=1.372 Prec@1=66.615 Prec@5=86.989 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:11 IST
=> Training   16.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.567 DataTime=0.394 Loss=1.372 Prec@1=66.615 Prec@5=86.989 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:12 IST
=> Training   16.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.568 DataTime=0.392 Loss=1.376 Prec@1=66.535 Prec@5=86.935 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:12 IST
=> Training   20.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.568 DataTime=0.392 Loss=1.376 Prec@1=66.535 Prec@5=86.935 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:12 IST
=> Training   20.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.568 DataTime=0.392 Loss=1.376 Prec@1=66.535 Prec@5=86.935 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:13 IST
=> Training   20.02% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.377 Prec@1=66.512 Prec@5=86.932 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:13 IST
=> Training   24.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.377 Prec@1=66.512 Prec@5=86.932 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=20:13 IST
=> Training   24.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.377 Prec@1=66.512 Prec@5=86.932 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=20:14 IST
=> Training   24.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.380 Prec@1=66.462 Prec@5=86.874 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=20:14 IST
=> Training   28.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.380 Prec@1=66.462 Prec@5=86.874 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=20:14 IST
=> Training   28.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.566 DataTime=0.389 Loss=1.380 Prec@1=66.462 Prec@5=86.874 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=20:15 IST
=> Training   28.01% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.387 Loss=1.382 Prec@1=66.404 Prec@5=86.876 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=20:15 IST
=> Training   32.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.387 Loss=1.382 Prec@1=66.404 Prec@5=86.876 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=20:15 IST
=> Training   32.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.387 Loss=1.382 Prec@1=66.404 Prec@5=86.876 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=20:16 IST
=> Training   32.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.386 Prec@1=66.316 Prec@5=86.830 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=20:16 IST
=> Training   36.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.386 Prec@1=66.316 Prec@5=86.830 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=20:16 IST
=> Training   36.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.386 Prec@1=66.316 Prec@5=86.830 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=20:17 IST
=> Training   36.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.388 Loss=1.387 Prec@1=66.278 Prec@5=86.833 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=20:17 IST
=> Training   39.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.388 Loss=1.387 Prec@1=66.278 Prec@5=86.833 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=20:17 IST
=> Training   39.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.563 DataTime=0.388 Loss=1.387 Prec@1=66.278 Prec@5=86.833 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=20:18 IST
=> Training   39.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.388 Prec@1=66.277 Prec@5=86.813 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=20:18 IST
=> Training   43.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.388 Prec@1=66.277 Prec@5=86.813 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:18 IST
=> Training   43.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.562 DataTime=0.387 Loss=1.388 Prec@1=66.277 Prec@5=86.813 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:19 IST
=> Training   43.99% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.391 Prec@1=66.211 Prec@5=86.764 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:19 IST
=> Training   47.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.391 Prec@1=66.211 Prec@5=86.764 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=20:19 IST
=> Training   47.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.391 Prec@1=66.211 Prec@5=86.764 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=20:19 IST
=> Training   47.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.177 Prec@5=86.749 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=20:19 IST
=> Training   51.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.177 Prec@5=86.749 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:19 IST
=> Training   51.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.177 Prec@5=86.749 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:20 IST
=> Training   51.98% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.129 Prec@5=86.712 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:20 IST
=> Training   55.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.129 Prec@5=86.712 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:20 IST
=> Training   55.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.129 Prec@5=86.712 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:21 IST
=> Training   55.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.113 Prec@5=86.705 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:21 IST
=> Training   59.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.113 Prec@5=86.705 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:21 IST
=> Training   59.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.113 Prec@5=86.705 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:22 IST
=> Training   59.97% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.396 Prec@1=66.094 Prec@5=86.697 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:22 IST
=> Training   63.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.396 Prec@1=66.094 Prec@5=86.697 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:22 IST
=> Training   63.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.396 Prec@1=66.094 Prec@5=86.697 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:23 IST
=> Training   63.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.397 Prec@1=66.082 Prec@5=86.699 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:23 IST
=> Training   67.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.397 Prec@1=66.082 Prec@5=86.699 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:23 IST
=> Training   67.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.388 Loss=1.397 Prec@1=66.082 Prec@5=86.699 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:24 IST
=> Training   67.96% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.056 Prec@5=86.672 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:24 IST
=> Training   71.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.056 Prec@5=86.672 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=20:24 IST
=> Training   71.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.056 Prec@5=86.672 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=20:25 IST
=> Training   71.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.386 Loss=1.399 Prec@1=66.053 Prec@5=86.671 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=20:25 IST
=> Training   75.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.386 Loss=1.399 Prec@1=66.053 Prec@5=86.671 rate=1.79 Hz, eta=0:05:35, total=0:17:41, wall=20:25 IST
=> Training   75.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.561 DataTime=0.386 Loss=1.399 Prec@1=66.053 Prec@5=86.671 rate=1.79 Hz, eta=0:05:35, total=0:17:41, wall=20:26 IST
=> Training   75.95% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.384 Loss=1.400 Prec@1=66.027 Prec@5=86.657 rate=1.79 Hz, eta=0:05:35, total=0:17:41, wall=20:26 IST
=> Training   79.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.384 Loss=1.400 Prec@1=66.027 Prec@5=86.657 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:26 IST
=> Training   79.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.560 DataTime=0.384 Loss=1.400 Prec@1=66.027 Prec@5=86.657 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:27 IST
=> Training   79.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.401 Prec@1=65.991 Prec@5=86.645 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=20:27 IST
=> Training   83.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.401 Prec@1=65.991 Prec@5=86.645 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=20:27 IST
=> Training   83.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.401 Prec@1=65.991 Prec@5=86.645 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=20:28 IST
=> Training   83.94% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.970 Prec@5=86.636 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=20:28 IST
=> Training   87.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.970 Prec@5=86.636 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=20:28 IST
=> Training   87.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.970 Prec@5=86.636 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=20:29 IST
=> Training   87.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.956 Prec@5=86.621 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=20:29 IST
=> Training   91.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.956 Prec@5=86.621 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=20:29 IST
=> Training   91.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.402 Prec@1=65.956 Prec@5=86.621 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=20:30 IST
=> Training   91.93% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.403 Prec@1=65.938 Prec@5=86.604 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=20:30 IST
=> Training   95.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.403 Prec@1=65.938 Prec@5=86.604 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:30 IST
=> Training   95.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.403 Prec@1=65.938 Prec@5=86.604 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:31 IST
=> Training   95.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.404 Prec@1=65.920 Prec@5=86.585 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=20:31 IST
=> Training   99.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.404 Prec@1=65.920 Prec@5=86.585 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:31 IST
=> Training   99.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.404 Prec@1=65.920 Prec@5=86.585 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:31 IST
=> Training   99.92% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.404 Prec@1=65.918 Prec@5=86.584 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:31 IST
=> Training   100.00% of 1x2503...Epoch=52/150 LR=0.0741 Time=0.559 DataTime=0.384 Loss=1.404 Prec@1=65.918 Prec@5=86.584 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=20:31 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:31 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:31 IST
=> Validation 0.00% of 1x98...Epoch=52/150 LR=0.0741 Time=7.467 Loss=0.906 Prec@1=77.930 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=20:31 IST
=> Validation 1.02% of 1x98...Epoch=52/150 LR=0.0741 Time=7.467 Loss=0.906 Prec@1=77.930 Prec@5=92.383 rate=5865.89 Hz, eta=0:00:00, total=0:00:00, wall=20:31 IST
** Validation 1.02% of 1x98...Epoch=52/150 LR=0.0741 Time=7.467 Loss=0.906 Prec@1=77.930 Prec@5=92.383 rate=5865.89 Hz, eta=0:00:00, total=0:00:00, wall=20:32 IST
** Validation 1.02% of 1x98...Epoch=52/150 LR=0.0741 Time=0.644 Loss=1.524 Prec@1=63.476 Prec@5=85.388 rate=5865.89 Hz, eta=0:00:00, total=0:00:00, wall=20:32 IST
** Validation 100.00% of 1x98...Epoch=52/150 LR=0.0741 Time=0.644 Loss=1.524 Prec@1=63.476 Prec@5=85.388 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=20:32 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:32 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:32 IST
=> Training   0.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=5.081 DataTime=4.600 Loss=1.354 Prec@1=69.336 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=20:32 IST
=> Training   0.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=5.081 DataTime=4.600 Loss=1.354 Prec@1=69.336 Prec@5=88.281 rate=2084.28 Hz, eta=0:00:01, total=0:00:00, wall=20:32 IST
=> Training   0.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=5.081 DataTime=4.600 Loss=1.354 Prec@1=69.336 Prec@5=88.281 rate=2084.28 Hz, eta=0:00:01, total=0:00:00, wall=20:33 IST
=> Training   0.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.580 DataTime=0.415 Loss=1.354 Prec@1=66.863 Prec@5=87.280 rate=2084.28 Hz, eta=0:00:01, total=0:00:00, wall=20:33 IST
=> Training   4.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.580 DataTime=0.415 Loss=1.354 Prec@1=66.863 Prec@5=87.280 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=20:33 IST
=> Training   4.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.580 DataTime=0.415 Loss=1.354 Prec@1=66.863 Prec@5=87.280 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=20:34 IST
=> Training   4.04% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.569 DataTime=0.404 Loss=1.353 Prec@1=66.985 Prec@5=87.377 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=20:34 IST
=> Training   8.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.569 DataTime=0.404 Loss=1.353 Prec@1=66.985 Prec@5=87.377 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=20:34 IST
=> Training   8.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.569 DataTime=0.404 Loss=1.353 Prec@1=66.985 Prec@5=87.377 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=20:35 IST
=> Training   8.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.400 Loss=1.358 Prec@1=66.908 Prec@5=87.256 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=20:35 IST
=> Training   12.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.400 Loss=1.358 Prec@1=66.908 Prec@5=87.256 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=20:35 IST
=> Training   12.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.400 Loss=1.358 Prec@1=66.908 Prec@5=87.256 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=20:35 IST
=> Training   12.03% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.568 DataTime=0.400 Loss=1.360 Prec@1=66.825 Prec@5=87.196 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=20:35 IST
=> Training   16.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.568 DataTime=0.400 Loss=1.360 Prec@1=66.825 Prec@5=87.196 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:35 IST
=> Training   16.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.568 DataTime=0.400 Loss=1.360 Prec@1=66.825 Prec@5=87.196 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:36 IST
=> Training   16.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.363 Prec@1=66.771 Prec@5=87.156 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=20:36 IST
=> Training   20.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.363 Prec@1=66.771 Prec@5=87.156 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=20:36 IST
=> Training   20.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.363 Prec@1=66.771 Prec@5=87.156 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=20:37 IST
=> Training   20.02% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.369 Prec@1=66.636 Prec@5=87.033 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=20:37 IST
=> Training   24.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.369 Prec@1=66.636 Prec@5=87.033 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:37 IST
=> Training   24.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.567 DataTime=0.396 Loss=1.369 Prec@1=66.636 Prec@5=87.033 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:38 IST
=> Training   24.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.371 Prec@1=66.582 Prec@5=87.021 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:38 IST
=> Training   28.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.371 Prec@1=66.582 Prec@5=87.021 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=20:38 IST
=> Training   28.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.371 Prec@1=66.582 Prec@5=87.021 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=20:39 IST
=> Training   28.01% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.396 Loss=1.374 Prec@1=66.521 Prec@5=87.003 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=20:39 IST
=> Training   32.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.396 Loss=1.374 Prec@1=66.521 Prec@5=87.003 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=20:39 IST
=> Training   32.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.396 Loss=1.374 Prec@1=66.521 Prec@5=87.003 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=20:40 IST
=> Training   32.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.395 Loss=1.377 Prec@1=66.437 Prec@5=86.973 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=20:40 IST
=> Training   36.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.395 Loss=1.377 Prec@1=66.437 Prec@5=86.973 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=20:40 IST
=> Training   36.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.566 DataTime=0.395 Loss=1.377 Prec@1=66.437 Prec@5=86.973 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=20:41 IST
=> Training   36.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.381 Prec@1=66.373 Prec@5=86.927 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=20:41 IST
=> Training   39.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.381 Prec@1=66.373 Prec@5=86.927 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=20:41 IST
=> Training   39.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.565 DataTime=0.394 Loss=1.381 Prec@1=66.373 Prec@5=86.927 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=20:42 IST
=> Training   39.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.383 Prec@1=66.323 Prec@5=86.901 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=20:42 IST
=> Training   43.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.383 Prec@1=66.323 Prec@5=86.901 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:42 IST
=> Training   43.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.383 Prec@1=66.323 Prec@5=86.901 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:43 IST
=> Training   43.99% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.384 Prec@1=66.302 Prec@5=86.899 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=20:43 IST
=> Training   47.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.384 Prec@1=66.302 Prec@5=86.899 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=20:43 IST
=> Training   47.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.563 DataTime=0.391 Loss=1.384 Prec@1=66.302 Prec@5=86.899 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=20:44 IST
=> Training   47.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.240 Prec@5=86.867 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=20:44 IST
=> Training   51.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.240 Prec@5=86.867 rate=1.79 Hz, eta=0:11:09, total=0:12:04, wall=20:44 IST
=> Training   51.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.240 Prec@5=86.867 rate=1.79 Hz, eta=0:11:09, total=0:12:04, wall=20:45 IST
=> Training   51.98% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.387 Prec@1=66.211 Prec@5=86.860 rate=1.79 Hz, eta=0:11:09, total=0:12:04, wall=20:45 IST
=> Training   55.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.387 Prec@1=66.211 Prec@5=86.860 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=20:45 IST
=> Training   55.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.387 Prec@1=66.211 Prec@5=86.860 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=20:46 IST
=> Training   55.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.388 Prec@1=66.192 Prec@5=86.862 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=20:46 IST
=> Training   59.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.388 Prec@1=66.192 Prec@5=86.862 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=20:46 IST
=> Training   59.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.388 Prec@1=66.192 Prec@5=86.862 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=20:47 IST
=> Training   59.97% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.389 Prec@1=66.170 Prec@5=86.837 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=20:47 IST
=> Training   63.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.389 Prec@1=66.170 Prec@5=86.837 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=20:47 IST
=> Training   63.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.389 Prec@1=66.170 Prec@5=86.837 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=20:48 IST
=> Training   63.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.152 Prec@5=86.818 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=20:48 IST
=> Training   67.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.152 Prec@5=86.818 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=20:48 IST
=> Training   67.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.152 Prec@5=86.818 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=20:49 IST
=> Training   67.96% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.391 Prec@1=66.131 Prec@5=86.805 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=20:49 IST
=> Training   71.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.391 Prec@1=66.131 Prec@5=86.805 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=20:49 IST
=> Training   71.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.389 Loss=1.391 Prec@1=66.131 Prec@5=86.805 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=20:49 IST
=> Training   71.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.393 Prec@1=66.106 Prec@5=86.778 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=20:49 IST
=> Training   75.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.393 Prec@1=66.106 Prec@5=86.778 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=20:49 IST
=> Training   75.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.393 Prec@1=66.106 Prec@5=86.778 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=20:50 IST
=> Training   75.95% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.394 Prec@1=66.083 Prec@5=86.760 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=20:50 IST
=> Training   79.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.394 Prec@1=66.083 Prec@5=86.760 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:50 IST
=> Training   79.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.394 Prec@1=66.083 Prec@5=86.760 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:51 IST
=> Training   79.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.069 Prec@5=86.746 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:51 IST
=> Training   83.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.069 Prec@5=86.746 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=20:51 IST
=> Training   83.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.561 DataTime=0.388 Loss=1.395 Prec@1=66.069 Prec@5=86.746 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=20:52 IST
=> Training   83.94% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.396 Prec@1=66.045 Prec@5=86.728 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=20:52 IST
=> Training   87.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.396 Prec@1=66.045 Prec@5=86.728 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:52 IST
=> Training   87.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.396 Prec@1=66.045 Prec@5=86.728 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:53 IST
=> Training   87.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.020 Prec@5=86.708 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:53 IST
=> Training   91.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.020 Prec@5=86.708 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:53 IST
=> Training   91.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.398 Prec@1=66.020 Prec@5=86.708 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:54 IST
=> Training   91.93% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.400 Prec@1=65.989 Prec@5=86.687 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=20:54 IST
=> Training   95.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.400 Prec@1=65.989 Prec@5=86.687 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:54 IST
=> Training   95.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.388 Loss=1.400 Prec@1=65.989 Prec@5=86.687 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:55 IST
=> Training   95.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.401 Prec@1=65.953 Prec@5=86.667 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:55 IST
=> Training   99.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.401 Prec@1=65.953 Prec@5=86.667 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:55 IST
=> Training   99.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.560 DataTime=0.387 Loss=1.401 Prec@1=65.953 Prec@5=86.667 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:55 IST
=> Training   99.92% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.559 DataTime=0.387 Loss=1.401 Prec@1=65.951 Prec@5=86.666 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=20:55 IST
=> Training   100.00% of 1x2503...Epoch=53/150 LR=0.0732 Time=0.559 DataTime=0.387 Loss=1.401 Prec@1=65.951 Prec@5=86.666 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=20:55 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST
=> Validation 0.00% of 1x98...Epoch=53/150 LR=0.0732 Time=7.187 Loss=0.950 Prec@1=76.562 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=20:55 IST
=> Validation 1.02% of 1x98...Epoch=53/150 LR=0.0732 Time=7.187 Loss=0.950 Prec@1=76.562 Prec@5=92.969 rate=2135.21 Hz, eta=0:00:00, total=0:00:00, wall=20:55 IST
** Validation 1.02% of 1x98...Epoch=53/150 LR=0.0732 Time=7.187 Loss=0.950 Prec@1=76.562 Prec@5=92.969 rate=2135.21 Hz, eta=0:00:00, total=0:00:00, wall=20:56 IST
** Validation 1.02% of 1x98...Epoch=53/150 LR=0.0732 Time=0.638 Loss=1.489 Prec@1=64.222 Prec@5=86.066 rate=2135.21 Hz, eta=0:00:00, total=0:00:00, wall=20:56 IST
** Validation 100.00% of 1x98...Epoch=53/150 LR=0.0732 Time=0.638 Loss=1.489 Prec@1=64.222 Prec@5=86.066 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=20:56 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> Training   0.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=4.968 DataTime=4.715 Loss=1.453 Prec@1=64.648 Prec@5=85.156 rate=0 Hz, eta=?, total=0:00:00, wall=20:56 IST
=> Training   0.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=4.968 DataTime=4.715 Loss=1.453 Prec@1=64.648 Prec@5=85.156 rate=1815.44 Hz, eta=0:00:01, total=0:00:00, wall=20:56 IST
=> Training   0.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=4.968 DataTime=4.715 Loss=1.453 Prec@1=64.648 Prec@5=85.156 rate=1815.44 Hz, eta=0:00:01, total=0:00:00, wall=20:57 IST
=> Training   0.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.593 DataTime=0.432 Loss=1.354 Prec@1=66.913 Prec@5=87.388 rate=1815.44 Hz, eta=0:00:01, total=0:00:00, wall=20:57 IST
=> Training   4.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.593 DataTime=0.432 Loss=1.354 Prec@1=66.913 Prec@5=87.388 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=20:57 IST
=> Training   4.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.593 DataTime=0.432 Loss=1.354 Prec@1=66.913 Prec@5=87.388 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=20:58 IST
=> Training   4.04% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.579 DataTime=0.416 Loss=1.350 Prec@1=66.916 Prec@5=87.414 rate=1.84 Hz, eta=0:21:47, total=0:00:54, wall=20:58 IST
=> Training   8.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.579 DataTime=0.416 Loss=1.350 Prec@1=66.916 Prec@5=87.414 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=20:58 IST
=> Training   8.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.579 DataTime=0.416 Loss=1.350 Prec@1=66.916 Prec@5=87.414 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=20:59 IST
=> Training   8.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.573 DataTime=0.407 Loss=1.354 Prec@1=66.894 Prec@5=87.342 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=20:59 IST
=> Training   12.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.573 DataTime=0.407 Loss=1.354 Prec@1=66.894 Prec@5=87.342 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=20:59 IST
=> Training   12.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.573 DataTime=0.407 Loss=1.354 Prec@1=66.894 Prec@5=87.342 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=21:00 IST
=> Training   12.03% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.568 DataTime=0.399 Loss=1.357 Prec@1=66.872 Prec@5=87.312 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=21:00 IST
=> Training   16.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.568 DataTime=0.399 Loss=1.357 Prec@1=66.872 Prec@5=87.312 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=21:00 IST
=> Training   16.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.568 DataTime=0.399 Loss=1.357 Prec@1=66.872 Prec@5=87.312 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=21:01 IST
=> Training   16.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.394 Loss=1.361 Prec@1=66.822 Prec@5=87.257 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=21:01 IST
=> Training   20.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.394 Loss=1.361 Prec@1=66.822 Prec@5=87.257 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=21:01 IST
=> Training   20.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.394 Loss=1.361 Prec@1=66.822 Prec@5=87.257 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=21:02 IST
=> Training   20.02% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.392 Loss=1.363 Prec@1=66.809 Prec@5=87.222 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=21:02 IST
=> Training   24.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.392 Loss=1.363 Prec@1=66.809 Prec@5=87.222 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:02 IST
=> Training   24.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.564 DataTime=0.392 Loss=1.363 Prec@1=66.809 Prec@5=87.222 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:03 IST
=> Training   24.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.367 Prec@1=66.730 Prec@5=87.157 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:03 IST
=> Training   28.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.367 Prec@1=66.730 Prec@5=87.157 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:03 IST
=> Training   28.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.367 Prec@1=66.730 Prec@5=87.157 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:04 IST
=> Training   28.01% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.565 DataTime=0.394 Loss=1.367 Prec@1=66.709 Prec@5=87.143 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=21:04 IST
=> Training   32.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.565 DataTime=0.394 Loss=1.367 Prec@1=66.709 Prec@5=87.143 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:04 IST
=> Training   32.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.565 DataTime=0.394 Loss=1.367 Prec@1=66.709 Prec@5=87.143 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:05 IST
=> Training   32.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.369 Prec@1=66.663 Prec@5=87.097 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:05 IST
=> Training   36.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.369 Prec@1=66.663 Prec@5=87.097 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=21:05 IST
=> Training   36.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.369 Prec@1=66.663 Prec@5=87.097 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=21:05 IST
=> Training   36.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.371 Prec@1=66.608 Prec@5=87.055 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=21:05 IST
=> Training   39.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.371 Prec@1=66.608 Prec@5=87.055 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=21:05 IST
=> Training   39.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.563 DataTime=0.392 Loss=1.371 Prec@1=66.608 Prec@5=87.055 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=21:06 IST
=> Training   39.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.373 Prec@1=66.579 Prec@5=87.039 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=21:06 IST
=> Training   43.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.373 Prec@1=66.579 Prec@5=87.039 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=21:06 IST
=> Training   43.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.373 Prec@1=66.579 Prec@5=87.039 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=21:07 IST
=> Training   43.99% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.374 Prec@1=66.549 Prec@5=87.013 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=21:07 IST
=> Training   47.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.374 Prec@1=66.549 Prec@5=87.013 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=21:07 IST
=> Training   47.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.374 Prec@1=66.549 Prec@5=87.013 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=21:08 IST
=> Training   47.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.388 Loss=1.377 Prec@1=66.486 Prec@5=86.971 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=21:08 IST
=> Training   51.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.388 Loss=1.377 Prec@1=66.486 Prec@5=86.971 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:08 IST
=> Training   51.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.388 Loss=1.377 Prec@1=66.486 Prec@5=86.971 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:09 IST
=> Training   51.98% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.389 Loss=1.380 Prec@1=66.410 Prec@5=86.936 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:09 IST
=> Training   55.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.389 Loss=1.380 Prec@1=66.410 Prec@5=86.936 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:09 IST
=> Training   55.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.560 DataTime=0.389 Loss=1.380 Prec@1=66.410 Prec@5=86.936 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:10 IST
=> Training   55.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.381 Prec@1=66.393 Prec@5=86.933 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:10 IST
=> Training   59.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.381 Prec@1=66.393 Prec@5=86.933 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=21:10 IST
=> Training   59.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.390 Loss=1.381 Prec@1=66.393 Prec@5=86.933 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=21:11 IST
=> Training   59.97% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.381 Prec@1=66.378 Prec@5=86.923 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=21:11 IST
=> Training   63.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.381 Prec@1=66.378 Prec@5=86.923 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=21:11 IST
=> Training   63.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.381 Prec@1=66.378 Prec@5=86.923 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=21:12 IST
=> Training   63.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.382 Prec@1=66.366 Prec@5=86.907 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=21:12 IST
=> Training   67.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.382 Prec@1=66.366 Prec@5=86.907 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=21:12 IST
=> Training   67.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.382 Prec@1=66.366 Prec@5=86.907 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=21:13 IST
=> Training   67.96% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.384 Prec@1=66.336 Prec@5=86.879 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=21:13 IST
=> Training   71.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.384 Prec@1=66.336 Prec@5=86.879 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=21:13 IST
=> Training   71.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.384 Prec@1=66.336 Prec@5=86.879 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=21:14 IST
=> Training   71.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.385 Prec@1=66.295 Prec@5=86.861 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=21:14 IST
=> Training   75.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.385 Prec@1=66.295 Prec@5=86.861 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=21:14 IST
=> Training   75.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.385 Prec@1=66.295 Prec@5=86.861 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=21:15 IST
=> Training   75.95% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.255 Prec@5=86.827 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=21:15 IST
=> Training   79.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.255 Prec@5=86.827 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=21:15 IST
=> Training   79.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.387 Prec@1=66.255 Prec@5=86.827 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=21:16 IST
=> Training   79.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.388 Prec@1=66.228 Prec@5=86.812 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=21:16 IST
=> Training   83.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.388 Prec@1=66.228 Prec@5=86.812 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:16 IST
=> Training   83.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.390 Loss=1.388 Prec@1=66.228 Prec@5=86.812 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:17 IST
=> Training   83.94% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.197 Prec@5=86.798 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:17 IST
=> Training   87.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.197 Prec@5=86.798 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=21:17 IST
=> Training   87.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.389 Loss=1.390 Prec@1=66.197 Prec@5=86.798 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=21:18 IST
=> Training   87.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.390 Prec@1=66.180 Prec@5=86.791 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=21:18 IST
=> Training   91.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.390 Prec@1=66.180 Prec@5=86.791 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=21:18 IST
=> Training   91.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.562 DataTime=0.389 Loss=1.390 Prec@1=66.180 Prec@5=86.791 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=21:19 IST
=> Training   91.93% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.388 Loss=1.392 Prec@1=66.157 Prec@5=86.767 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=21:19 IST
=> Training   95.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.388 Loss=1.392 Prec@1=66.157 Prec@5=86.767 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=21:19 IST
=> Training   95.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.388 Loss=1.392 Prec@1=66.157 Prec@5=86.767 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=21:19 IST
=> Training   95.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.123 Prec@5=86.755 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=21:19 IST
=> Training   99.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.123 Prec@5=86.755 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:19 IST
=> Training   99.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.123 Prec@5=86.755 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:19 IST
=> Training   99.92% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.123 Prec@5=86.754 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:19 IST
=> Training   100.00% of 1x2503...Epoch=54/150 LR=0.0722 Time=0.561 DataTime=0.387 Loss=1.393 Prec@1=66.123 Prec@5=86.754 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=21:19 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:20 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:20 IST
=> Validation 0.00% of 1x98...Epoch=54/150 LR=0.0722 Time=7.026 Loss=0.955 Prec@1=76.367 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=21:20 IST
=> Validation 1.02% of 1x98...Epoch=54/150 LR=0.0722 Time=7.026 Loss=0.955 Prec@1=76.367 Prec@5=92.773 rate=7851.02 Hz, eta=0:00:00, total=0:00:00, wall=21:20 IST
** Validation 1.02% of 1x98...Epoch=54/150 LR=0.0722 Time=7.026 Loss=0.955 Prec@1=76.367 Prec@5=92.773 rate=7851.02 Hz, eta=0:00:00, total=0:00:00, wall=21:21 IST
** Validation 1.02% of 1x98...Epoch=54/150 LR=0.0722 Time=0.637 Loss=1.466 Prec@1=64.754 Prec@5=86.300 rate=7851.02 Hz, eta=0:00:00, total=0:00:00, wall=21:21 IST
** Validation 100.00% of 1x98...Epoch=54/150 LR=0.0722 Time=0.637 Loss=1.466 Prec@1=64.754 Prec@5=86.300 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=21:21 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:21 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:21 IST
=> Training   0.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=5.370 DataTime=5.183 Loss=1.340 Prec@1=67.578 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=21:21 IST
=> Training   0.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=5.370 DataTime=5.183 Loss=1.340 Prec@1=67.578 Prec@5=85.938 rate=8477.23 Hz, eta=0:00:00, total=0:00:00, wall=21:21 IST
=> Training   0.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=5.370 DataTime=5.183 Loss=1.340 Prec@1=67.578 Prec@5=85.938 rate=8477.23 Hz, eta=0:00:00, total=0:00:00, wall=21:22 IST
=> Training   0.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.589 DataTime=0.426 Loss=1.334 Prec@1=67.450 Prec@5=87.523 rate=8477.23 Hz, eta=0:00:00, total=0:00:00, wall=21:22 IST
=> Training   4.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.589 DataTime=0.426 Loss=1.334 Prec@1=67.450 Prec@5=87.523 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=21:22 IST
=> Training   4.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.589 DataTime=0.426 Loss=1.334 Prec@1=67.450 Prec@5=87.523 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=21:22 IST
=> Training   4.04% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.573 DataTime=0.409 Loss=1.335 Prec@1=67.445 Prec@5=87.527 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=21:22 IST
=> Training   8.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.573 DataTime=0.409 Loss=1.335 Prec@1=67.445 Prec@5=87.527 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=21:22 IST
=> Training   8.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.573 DataTime=0.409 Loss=1.335 Prec@1=67.445 Prec@5=87.527 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=21:23 IST
=> Training   8.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.340 Prec@1=67.210 Prec@5=87.532 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=21:23 IST
=> Training   12.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.340 Prec@1=67.210 Prec@5=87.532 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:23 IST
=> Training   12.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.340 Prec@1=67.210 Prec@5=87.532 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:24 IST
=> Training   12.03% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.575 DataTime=0.409 Loss=1.346 Prec@1=67.080 Prec@5=87.460 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:24 IST
=> Training   16.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.575 DataTime=0.409 Loss=1.346 Prec@1=67.080 Prec@5=87.460 rate=1.78 Hz, eta=0:19:39, total=0:03:45, wall=21:24 IST
=> Training   16.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.575 DataTime=0.409 Loss=1.346 Prec@1=67.080 Prec@5=87.460 rate=1.78 Hz, eta=0:19:39, total=0:03:45, wall=21:25 IST
=> Training   16.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.405 Loss=1.352 Prec@1=66.977 Prec@5=87.353 rate=1.78 Hz, eta=0:19:39, total=0:03:45, wall=21:25 IST
=> Training   20.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.405 Loss=1.352 Prec@1=66.977 Prec@5=87.353 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=21:25 IST
=> Training   20.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.405 Loss=1.352 Prec@1=66.977 Prec@5=87.353 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=21:26 IST
=> Training   20.02% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.358 Prec@1=66.862 Prec@5=87.292 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=21:26 IST
=> Training   24.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.358 Prec@1=66.862 Prec@5=87.292 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=21:26 IST
=> Training   24.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.571 DataTime=0.406 Loss=1.358 Prec@1=66.862 Prec@5=87.292 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=21:27 IST
=> Training   24.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.570 DataTime=0.402 Loss=1.360 Prec@1=66.781 Prec@5=87.272 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=21:27 IST
=> Training   28.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.570 DataTime=0.402 Loss=1.360 Prec@1=66.781 Prec@5=87.272 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=21:27 IST
=> Training   28.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.570 DataTime=0.402 Loss=1.360 Prec@1=66.781 Prec@5=87.272 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=21:28 IST
=> Training   28.01% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.400 Loss=1.362 Prec@1=66.717 Prec@5=87.216 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=21:28 IST
=> Training   32.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.400 Loss=1.362 Prec@1=66.717 Prec@5=87.216 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=21:28 IST
=> Training   32.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.400 Loss=1.362 Prec@1=66.717 Prec@5=87.216 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=21:29 IST
=> Training   32.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.399 Loss=1.366 Prec@1=66.638 Prec@5=87.160 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=21:29 IST
=> Training   36.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.399 Loss=1.366 Prec@1=66.638 Prec@5=87.160 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=21:29 IST
=> Training   36.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.399 Loss=1.366 Prec@1=66.638 Prec@5=87.160 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=21:30 IST
=> Training   36.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.397 Loss=1.368 Prec@1=66.608 Prec@5=87.121 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=21:30 IST
=> Training   39.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.397 Loss=1.368 Prec@1=66.608 Prec@5=87.121 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=21:30 IST
=> Training   39.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.569 DataTime=0.397 Loss=1.368 Prec@1=66.608 Prec@5=87.121 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=21:31 IST
=> Training   39.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.395 Loss=1.371 Prec@1=66.566 Prec@5=87.073 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=21:31 IST
=> Training   43.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.395 Loss=1.371 Prec@1=66.566 Prec@5=87.073 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=21:31 IST
=> Training   43.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.395 Loss=1.371 Prec@1=66.566 Prec@5=87.073 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=21:32 IST
=> Training   43.99% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.372 Prec@1=66.531 Prec@5=87.066 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=21:32 IST
=> Training   47.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.372 Prec@1=66.531 Prec@5=87.066 rate=1.77 Hz, eta=0:12:13, total=0:11:17, wall=21:32 IST
=> Training   47.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.372 Prec@1=66.531 Prec@5=87.066 rate=1.77 Hz, eta=0:12:13, total=0:11:17, wall=21:33 IST
=> Training   47.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.373 Prec@1=66.531 Prec@5=87.068 rate=1.77 Hz, eta=0:12:13, total=0:11:17, wall=21:33 IST
=> Training   51.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.373 Prec@1=66.531 Prec@5=87.068 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=21:33 IST
=> Training   51.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.396 Loss=1.373 Prec@1=66.531 Prec@5=87.068 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=21:34 IST
=> Training   51.98% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.395 Loss=1.374 Prec@1=66.503 Prec@5=87.044 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=21:34 IST
=> Training   55.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.395 Loss=1.374 Prec@1=66.503 Prec@5=87.044 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=21:34 IST
=> Training   55.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.568 DataTime=0.395 Loss=1.374 Prec@1=66.503 Prec@5=87.044 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=21:35 IST
=> Training   55.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.376 Prec@1=66.461 Prec@5=87.032 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=21:35 IST
=> Training   59.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.376 Prec@1=66.461 Prec@5=87.032 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=21:35 IST
=> Training   59.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.376 Prec@1=66.461 Prec@5=87.032 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=21:36 IST
=> Training   59.97% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.378 Prec@1=66.424 Prec@5=87.008 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=21:36 IST
=> Training   63.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.378 Prec@1=66.424 Prec@5=87.008 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=21:36 IST
=> Training   63.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.378 Prec@1=66.424 Prec@5=87.008 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=21:37 IST
=> Training   63.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.379 Prec@1=66.401 Prec@5=86.976 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=21:37 IST
=> Training   67.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.379 Prec@1=66.401 Prec@5=86.976 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=21:37 IST
=> Training   67.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.379 Prec@1=66.401 Prec@5=86.976 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=21:38 IST
=> Training   67.96% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.380 Prec@1=66.374 Prec@5=86.948 rate=1.77 Hz, eta=0:07:31, total=0:15:58, wall=21:38 IST
=> Training   71.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.380 Prec@1=66.374 Prec@5=86.948 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=21:38 IST
=> Training   71.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.380 Prec@1=66.374 Prec@5=86.948 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=21:38 IST
=> Training   71.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.381 Prec@1=66.366 Prec@5=86.935 rate=1.77 Hz, eta=0:06:35, total=0:16:55, wall=21:38 IST
=> Training   75.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.381 Prec@1=66.366 Prec@5=86.935 rate=1.77 Hz, eta=0:05:39, total=0:17:51, wall=21:38 IST
=> Training   75.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.381 Prec@1=66.366 Prec@5=86.935 rate=1.77 Hz, eta=0:05:39, total=0:17:51, wall=21:39 IST
=> Training   75.95% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.382 Prec@1=66.337 Prec@5=86.918 rate=1.77 Hz, eta=0:05:39, total=0:17:51, wall=21:39 IST
=> Training   79.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.382 Prec@1=66.337 Prec@5=86.918 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=21:39 IST
=> Training   79.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.567 DataTime=0.394 Loss=1.382 Prec@1=66.337 Prec@5=86.918 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=21:40 IST
=> Training   79.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.566 DataTime=0.393 Loss=1.384 Prec@1=66.301 Prec@5=86.895 rate=1.77 Hz, eta=0:04:43, total=0:18:48, wall=21:40 IST
=> Training   83.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.566 DataTime=0.393 Loss=1.384 Prec@1=66.301 Prec@5=86.895 rate=1.78 Hz, eta=0:03:46, total=0:19:43, wall=21:40 IST
=> Training   83.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.566 DataTime=0.393 Loss=1.384 Prec@1=66.301 Prec@5=86.895 rate=1.78 Hz, eta=0:03:46, total=0:19:43, wall=21:41 IST
=> Training   83.94% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.392 Loss=1.384 Prec@1=66.282 Prec@5=86.890 rate=1.78 Hz, eta=0:03:46, total=0:19:43, wall=21:41 IST
=> Training   87.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.392 Loss=1.384 Prec@1=66.282 Prec@5=86.890 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=21:41 IST
=> Training   87.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.392 Loss=1.384 Prec@1=66.282 Prec@5=86.890 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=21:42 IST
=> Training   87.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.385 Prec@1=66.257 Prec@5=86.872 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=21:42 IST
=> Training   91.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.385 Prec@1=66.257 Prec@5=86.872 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=21:42 IST
=> Training   91.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.385 Prec@1=66.257 Prec@5=86.872 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=21:43 IST
=> Training   91.93% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.393 Loss=1.386 Prec@1=66.236 Prec@5=86.852 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=21:43 IST
=> Training   95.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.393 Loss=1.386 Prec@1=66.236 Prec@5=86.852 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=21:43 IST
=> Training   95.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.565 DataTime=0.393 Loss=1.386 Prec@1=66.236 Prec@5=86.852 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=21:44 IST
=> Training   95.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.388 Prec@1=66.207 Prec@5=86.832 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=21:44 IST
=> Training   99.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.388 Prec@1=66.207 Prec@5=86.832 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=21:44 IST
=> Training   99.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.564 DataTime=0.392 Loss=1.388 Prec@1=66.207 Prec@5=86.832 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=21:44 IST
=> Training   99.92% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.563 DataTime=0.392 Loss=1.388 Prec@1=66.205 Prec@5=86.831 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=21:44 IST
=> Training   100.00% of 1x2503...Epoch=55/150 LR=0.0713 Time=0.563 DataTime=0.392 Loss=1.388 Prec@1=66.205 Prec@5=86.831 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=21:44 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:44 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:44 IST
=> Validation 0.00% of 1x98...Epoch=55/150 LR=0.0713 Time=7.139 Loss=0.945 Prec@1=76.367 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=21:44 IST
=> Validation 1.02% of 1x98...Epoch=55/150 LR=0.0713 Time=7.139 Loss=0.945 Prec@1=76.367 Prec@5=92.969 rate=478.13 Hz, eta=0:00:00, total=0:00:00, wall=21:44 IST
** Validation 1.02% of 1x98...Epoch=55/150 LR=0.0713 Time=7.139 Loss=0.945 Prec@1=76.367 Prec@5=92.969 rate=478.13 Hz, eta=0:00:00, total=0:00:00, wall=21:45 IST
** Validation 1.02% of 1x98...Epoch=55/150 LR=0.0713 Time=0.635 Loss=1.484 Prec@1=64.296 Prec@5=85.938 rate=478.13 Hz, eta=0:00:00, total=0:00:00, wall=21:45 IST
** Validation 100.00% of 1x98...Epoch=55/150 LR=0.0713 Time=0.635 Loss=1.484 Prec@1=64.296 Prec@5=85.938 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=21:45 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:45 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:45 IST
=> Training   0.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=5.241 DataTime=4.984 Loss=1.275 Prec@1=68.750 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=21:45 IST
=> Training   0.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=5.241 DataTime=4.984 Loss=1.275 Prec@1=68.750 Prec@5=89.258 rate=8473.72 Hz, eta=0:00:00, total=0:00:00, wall=21:45 IST
=> Training   0.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=5.241 DataTime=4.984 Loss=1.275 Prec@1=68.750 Prec@5=89.258 rate=8473.72 Hz, eta=0:00:00, total=0:00:00, wall=21:46 IST
=> Training   0.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.598 DataTime=0.435 Loss=1.329 Prec@1=67.509 Prec@5=87.626 rate=8473.72 Hz, eta=0:00:00, total=0:00:00, wall=21:46 IST
=> Training   4.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.598 DataTime=0.435 Loss=1.329 Prec@1=67.509 Prec@5=87.626 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=21:46 IST
=> Training   4.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.598 DataTime=0.435 Loss=1.329 Prec@1=67.509 Prec@5=87.626 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=21:47 IST
=> Training   4.04% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.582 DataTime=0.411 Loss=1.339 Prec@1=67.322 Prec@5=87.585 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=21:47 IST
=> Training   8.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.582 DataTime=0.411 Loss=1.339 Prec@1=67.322 Prec@5=87.585 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=21:47 IST
=> Training   8.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.582 DataTime=0.411 Loss=1.339 Prec@1=67.322 Prec@5=87.585 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=21:48 IST
=> Training   8.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.575 DataTime=0.402 Loss=1.337 Prec@1=67.376 Prec@5=87.591 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=21:48 IST
=> Training   12.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.575 DataTime=0.402 Loss=1.337 Prec@1=67.376 Prec@5=87.591 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=21:48 IST
=> Training   12.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.575 DataTime=0.402 Loss=1.337 Prec@1=67.376 Prec@5=87.591 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=21:49 IST
=> Training   12.03% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.570 DataTime=0.397 Loss=1.341 Prec@1=67.281 Prec@5=87.512 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=21:49 IST
=> Training   16.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.570 DataTime=0.397 Loss=1.341 Prec@1=67.281 Prec@5=87.512 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=21:49 IST
=> Training   16.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.570 DataTime=0.397 Loss=1.341 Prec@1=67.281 Prec@5=87.512 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=21:50 IST
=> Training   16.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.565 DataTime=0.390 Loss=1.345 Prec@1=67.200 Prec@5=87.456 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=21:50 IST
=> Training   20.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.565 DataTime=0.390 Loss=1.345 Prec@1=67.200 Prec@5=87.456 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=21:50 IST
=> Training   20.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.565 DataTime=0.390 Loss=1.345 Prec@1=67.200 Prec@5=87.456 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=21:51 IST
=> Training   20.02% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.564 DataTime=0.389 Loss=1.347 Prec@1=67.147 Prec@5=87.415 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=21:51 IST
=> Training   24.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.564 DataTime=0.389 Loss=1.347 Prec@1=67.147 Prec@5=87.415 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:51 IST
=> Training   24.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.564 DataTime=0.389 Loss=1.347 Prec@1=67.147 Prec@5=87.415 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:52 IST
=> Training   24.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.388 Loss=1.352 Prec@1=67.006 Prec@5=87.357 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=21:52 IST
=> Training   28.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.388 Loss=1.352 Prec@1=67.006 Prec@5=87.357 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=21:52 IST
=> Training   28.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.388 Loss=1.352 Prec@1=67.006 Prec@5=87.357 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=21:53 IST
=> Training   28.01% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.387 Loss=1.353 Prec@1=66.979 Prec@5=87.340 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=21:53 IST
=> Training   32.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.387 Loss=1.353 Prec@1=66.979 Prec@5=87.340 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=21:53 IST
=> Training   32.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.562 DataTime=0.387 Loss=1.353 Prec@1=66.979 Prec@5=87.340 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=21:54 IST
=> Training   32.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.355 Prec@1=66.922 Prec@5=87.308 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=21:54 IST
=> Training   36.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.355 Prec@1=66.922 Prec@5=87.308 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=21:54 IST
=> Training   36.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.355 Prec@1=66.922 Prec@5=87.308 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=21:54 IST
=> Training   36.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.563 DataTime=0.389 Loss=1.357 Prec@1=66.891 Prec@5=87.290 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=21:54 IST
=> Training   39.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.563 DataTime=0.389 Loss=1.357 Prec@1=66.891 Prec@5=87.290 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=21:54 IST
=> Training   39.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.563 DataTime=0.389 Loss=1.357 Prec@1=66.891 Prec@5=87.290 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=21:55 IST
=> Training   39.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.359 Prec@1=66.848 Prec@5=87.267 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=21:55 IST
=> Training   43.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.359 Prec@1=66.848 Prec@5=87.267 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=21:55 IST
=> Training   43.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.359 Prec@1=66.848 Prec@5=87.267 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=21:56 IST
=> Training   43.99% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.388 Loss=1.361 Prec@1=66.787 Prec@5=87.224 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=21:56 IST
=> Training   47.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.388 Loss=1.361 Prec@1=66.787 Prec@5=87.224 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=21:56 IST
=> Training   47.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.388 Loss=1.361 Prec@1=66.787 Prec@5=87.224 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=21:57 IST
=> Training   47.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.363 Prec@1=66.740 Prec@5=87.203 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=21:57 IST
=> Training   51.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.363 Prec@1=66.740 Prec@5=87.203 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:57 IST
=> Training   51.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.363 Prec@1=66.740 Prec@5=87.203 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:58 IST
=> Training   51.98% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.365 Prec@1=66.697 Prec@5=87.176 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=21:58 IST
=> Training   55.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.365 Prec@1=66.697 Prec@5=87.176 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:58 IST
=> Training   55.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.365 Prec@1=66.697 Prec@5=87.176 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:59 IST
=> Training   55.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.366 Prec@1=66.676 Prec@5=87.159 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=21:59 IST
=> Training   59.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.366 Prec@1=66.676 Prec@5=87.159 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=21:59 IST
=> Training   59.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.366 Prec@1=66.676 Prec@5=87.159 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=22:00 IST
=> Training   59.97% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.386 Loss=1.369 Prec@1=66.619 Prec@5=87.119 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=22:00 IST
=> Training   63.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.386 Loss=1.369 Prec@1=66.619 Prec@5=87.119 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=22:00 IST
=> Training   63.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.386 Loss=1.369 Prec@1=66.619 Prec@5=87.119 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=22:01 IST
=> Training   63.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.370 Prec@1=66.599 Prec@5=87.098 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=22:01 IST
=> Training   67.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.370 Prec@1=66.599 Prec@5=87.098 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=22:01 IST
=> Training   67.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.370 Prec@1=66.599 Prec@5=87.098 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=22:02 IST
=> Training   67.96% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.371 Prec@1=66.560 Prec@5=87.065 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=22:02 IST
=> Training   71.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.371 Prec@1=66.560 Prec@5=87.065 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=22:02 IST
=> Training   71.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.387 Loss=1.371 Prec@1=66.560 Prec@5=87.065 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=22:03 IST
=> Training   71.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.372 Prec@1=66.539 Prec@5=87.043 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=22:03 IST
=> Training   75.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.372 Prec@1=66.539 Prec@5=87.043 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:03 IST
=> Training   75.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.372 Prec@1=66.539 Prec@5=87.043 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:04 IST
=> Training   75.95% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.374 Prec@1=66.514 Prec@5=87.019 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=22:04 IST
=> Training   79.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.374 Prec@1=66.514 Prec@5=87.019 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=22:04 IST
=> Training   79.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.374 Prec@1=66.514 Prec@5=87.019 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=22:05 IST
=> Training   79.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.376 Prec@1=66.471 Prec@5=86.990 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=22:05 IST
=> Training   83.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.376 Prec@1=66.471 Prec@5=86.990 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:05 IST
=> Training   83.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.376 Prec@1=66.471 Prec@5=86.990 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:06 IST
=> Training   83.94% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.377 Prec@1=66.462 Prec@5=86.973 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:06 IST
=> Training   87.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.377 Prec@1=66.462 Prec@5=86.973 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:06 IST
=> Training   87.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.377 Prec@1=66.462 Prec@5=86.973 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:07 IST
=> Training   87.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.378 Prec@1=66.440 Prec@5=86.946 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:07 IST
=> Training   91.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.378 Prec@1=66.440 Prec@5=86.946 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:07 IST
=> Training   91.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.378 Prec@1=66.440 Prec@5=86.946 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:08 IST
=> Training   91.93% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.379 Prec@1=66.419 Prec@5=86.923 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:08 IST
=> Training   95.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.379 Prec@1=66.419 Prec@5=86.923 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=22:08 IST
=> Training   95.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.387 Loss=1.379 Prec@1=66.419 Prec@5=86.923 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=22:08 IST
=> Training   95.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.386 Loss=1.381 Prec@1=66.398 Prec@5=86.908 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=22:08 IST
=> Training   99.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.386 Loss=1.381 Prec@1=66.398 Prec@5=86.908 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=22:08 IST
=> Training   99.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.561 DataTime=0.386 Loss=1.381 Prec@1=66.398 Prec@5=86.908 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=22:08 IST
=> Training   99.92% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.386 Loss=1.381 Prec@1=66.397 Prec@5=86.907 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=22:08 IST
=> Training   100.00% of 1x2503...Epoch=56/150 LR=0.0703 Time=0.560 DataTime=0.386 Loss=1.381 Prec@1=66.397 Prec@5=86.907 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=22:08 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST
=> Validation 0.00% of 1x98...Epoch=56/150 LR=0.0703 Time=6.956 Loss=1.069 Prec@1=76.172 Prec@5=90.039 rate=0 Hz, eta=?, total=0:00:00, wall=22:09 IST
=> Validation 1.02% of 1x98...Epoch=56/150 LR=0.0703 Time=6.956 Loss=1.069 Prec@1=76.172 Prec@5=90.039 rate=6654.60 Hz, eta=0:00:00, total=0:00:00, wall=22:09 IST
** Validation 1.02% of 1x98...Epoch=56/150 LR=0.0703 Time=6.956 Loss=1.069 Prec@1=76.172 Prec@5=90.039 rate=6654.60 Hz, eta=0:00:00, total=0:00:00, wall=22:10 IST
** Validation 1.02% of 1x98...Epoch=56/150 LR=0.0703 Time=0.638 Loss=1.491 Prec@1=64.230 Prec@5=85.918 rate=6654.60 Hz, eta=0:00:00, total=0:00:00, wall=22:10 IST
** Validation 100.00% of 1x98...Epoch=56/150 LR=0.0703 Time=0.638 Loss=1.491 Prec@1=64.230 Prec@5=85.918 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=22:10 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:10 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:10 IST
=> Training   0.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=4.861 DataTime=4.208 Loss=1.384 Prec@1=66.406 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=22:10 IST
=> Training   0.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=4.861 DataTime=4.208 Loss=1.384 Prec@1=66.406 Prec@5=87.305 rate=532.07 Hz, eta=0:00:04, total=0:00:00, wall=22:10 IST
=> Training   0.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=4.861 DataTime=4.208 Loss=1.384 Prec@1=66.406 Prec@5=87.305 rate=532.07 Hz, eta=0:00:04, total=0:00:00, wall=22:11 IST
=> Training   0.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.594 DataTime=0.425 Loss=1.323 Prec@1=67.232 Prec@5=87.751 rate=532.07 Hz, eta=0:00:04, total=0:00:00, wall=22:11 IST
=> Training   4.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.594 DataTime=0.425 Loss=1.323 Prec@1=67.232 Prec@5=87.751 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=22:11 IST
=> Training   4.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.594 DataTime=0.425 Loss=1.323 Prec@1=67.232 Prec@5=87.751 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=22:11 IST
=> Training   4.04% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.575 DataTime=0.408 Loss=1.325 Prec@1=67.252 Prec@5=87.716 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=22:11 IST
=> Training   8.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.575 DataTime=0.408 Loss=1.325 Prec@1=67.252 Prec@5=87.716 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:11 IST
=> Training   8.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.575 DataTime=0.408 Loss=1.325 Prec@1=67.252 Prec@5=87.716 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:12 IST
=> Training   8.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.335 Prec@1=67.090 Prec@5=87.551 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:12 IST
=> Training   12.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.335 Prec@1=67.090 Prec@5=87.551 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=22:12 IST
=> Training   12.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.335 Prec@1=67.090 Prec@5=87.551 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=22:13 IST
=> Training   12.03% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.399 Loss=1.338 Prec@1=67.139 Prec@5=87.524 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=22:13 IST
=> Training   16.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.399 Loss=1.338 Prec@1=67.139 Prec@5=87.524 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=22:13 IST
=> Training   16.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.399 Loss=1.338 Prec@1=67.139 Prec@5=87.524 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=22:14 IST
=> Training   16.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.566 DataTime=0.396 Loss=1.345 Prec@1=67.004 Prec@5=87.431 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=22:14 IST
=> Training   20.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.566 DataTime=0.396 Loss=1.345 Prec@1=67.004 Prec@5=87.431 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:14 IST
=> Training   20.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.566 DataTime=0.396 Loss=1.345 Prec@1=67.004 Prec@5=87.431 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:15 IST
=> Training   20.02% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.345 Prec@1=67.046 Prec@5=87.415 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=22:15 IST
=> Training   24.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.345 Prec@1=67.046 Prec@5=87.415 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:15 IST
=> Training   24.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.398 Loss=1.345 Prec@1=67.046 Prec@5=87.415 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:16 IST
=> Training   24.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.397 Loss=1.346 Prec@1=67.048 Prec@5=87.413 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=22:16 IST
=> Training   28.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.397 Loss=1.346 Prec@1=67.048 Prec@5=87.413 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:16 IST
=> Training   28.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.568 DataTime=0.397 Loss=1.346 Prec@1=67.048 Prec@5=87.413 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:17 IST
=> Training   28.01% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.349 Prec@1=67.019 Prec@5=87.368 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:17 IST
=> Training   32.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.349 Prec@1=67.019 Prec@5=87.368 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:17 IST
=> Training   32.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.349 Prec@1=67.019 Prec@5=87.368 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:18 IST
=> Training   32.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.351 Prec@1=66.996 Prec@5=87.347 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:18 IST
=> Training   36.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.351 Prec@1=66.996 Prec@5=87.347 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:18 IST
=> Training   36.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.567 DataTime=0.395 Loss=1.351 Prec@1=66.996 Prec@5=87.347 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:19 IST
=> Training   36.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.393 Loss=1.351 Prec@1=66.999 Prec@5=87.336 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:19 IST
=> Training   39.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.393 Loss=1.351 Prec@1=66.999 Prec@5=87.336 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=22:19 IST
=> Training   39.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.393 Loss=1.351 Prec@1=66.999 Prec@5=87.336 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=22:20 IST
=> Training   39.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.391 Loss=1.352 Prec@1=66.939 Prec@5=87.319 rate=1.78 Hz, eta=0:14:01, total=0:09:20, wall=22:20 IST
=> Training   43.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.391 Loss=1.352 Prec@1=66.939 Prec@5=87.319 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=22:20 IST
=> Training   43.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.391 Loss=1.352 Prec@1=66.939 Prec@5=87.319 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=22:21 IST
=> Training   43.99% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.353 Prec@1=66.918 Prec@5=87.307 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=22:21 IST
=> Training   47.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.353 Prec@1=66.918 Prec@5=87.307 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:21 IST
=> Training   47.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.353 Prec@1=66.918 Prec@5=87.307 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:22 IST
=> Training   47.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.355 Prec@1=66.872 Prec@5=87.288 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:22 IST
=> Training   51.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.355 Prec@1=66.872 Prec@5=87.288 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=22:22 IST
=> Training   51.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.355 Prec@1=66.872 Prec@5=87.288 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=22:23 IST
=> Training   51.98% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.357 Prec@1=66.824 Prec@5=87.259 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=22:23 IST
=> Training   55.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.357 Prec@1=66.824 Prec@5=87.259 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=22:23 IST
=> Training   55.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.565 DataTime=0.392 Loss=1.357 Prec@1=66.824 Prec@5=87.259 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=22:24 IST
=> Training   55.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.360 Prec@1=66.766 Prec@5=87.222 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=22:24 IST
=> Training   59.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.360 Prec@1=66.766 Prec@5=87.222 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=22:24 IST
=> Training   59.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.360 Prec@1=66.766 Prec@5=87.222 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=22:25 IST
=> Training   59.97% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.361 Prec@1=66.735 Prec@5=87.204 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=22:25 IST
=> Training   63.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.361 Prec@1=66.735 Prec@5=87.204 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=22:25 IST
=> Training   63.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.361 Prec@1=66.735 Prec@5=87.204 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=22:25 IST
=> Training   63.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.362 Prec@1=66.712 Prec@5=87.192 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=22:25 IST
=> Training   67.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.362 Prec@1=66.712 Prec@5=87.192 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=22:25 IST
=> Training   67.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.391 Loss=1.362 Prec@1=66.712 Prec@5=87.192 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=22:26 IST
=> Training   67.96% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.364 Prec@1=66.689 Prec@5=87.169 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=22:26 IST
=> Training   71.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.364 Prec@1=66.689 Prec@5=87.169 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=22:26 IST
=> Training   71.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.564 DataTime=0.391 Loss=1.364 Prec@1=66.689 Prec@5=87.169 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=22:27 IST
=> Training   71.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.365 Prec@1=66.680 Prec@5=87.162 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=22:27 IST
=> Training   75.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.365 Prec@1=66.680 Prec@5=87.162 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=22:27 IST
=> Training   75.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.365 Prec@1=66.680 Prec@5=87.162 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=22:28 IST
=> Training   75.95% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.367 Prec@1=66.645 Prec@5=87.136 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=22:28 IST
=> Training   79.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.367 Prec@1=66.645 Prec@5=87.136 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:28 IST
=> Training   79.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.367 Prec@1=66.645 Prec@5=87.136 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:29 IST
=> Training   79.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.368 Prec@1=66.632 Prec@5=87.118 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:29 IST
=> Training   83.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.368 Prec@1=66.632 Prec@5=87.118 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:29 IST
=> Training   83.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.368 Prec@1=66.632 Prec@5=87.118 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:30 IST
=> Training   83.94% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.369 Prec@1=66.609 Prec@5=87.096 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=22:30 IST
=> Training   87.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.369 Prec@1=66.609 Prec@5=87.096 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=22:30 IST
=> Training   87.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.563 DataTime=0.390 Loss=1.369 Prec@1=66.609 Prec@5=87.096 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=22:31 IST
=> Training   87.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.370 Prec@1=66.584 Prec@5=87.081 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=22:31 IST
=> Training   91.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.370 Prec@1=66.584 Prec@5=87.081 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:31 IST
=> Training   91.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.370 Prec@1=66.584 Prec@5=87.081 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:32 IST
=> Training   91.93% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.372 Prec@1=66.547 Prec@5=87.060 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=22:32 IST
=> Training   95.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.372 Prec@1=66.547 Prec@5=87.060 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=22:32 IST
=> Training   95.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.562 DataTime=0.389 Loss=1.372 Prec@1=66.547 Prec@5=87.060 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=22:33 IST
=> Training   95.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.561 DataTime=0.388 Loss=1.373 Prec@1=66.520 Prec@5=87.034 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=22:33 IST
=> Training   99.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.561 DataTime=0.388 Loss=1.373 Prec@1=66.520 Prec@5=87.034 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=22:33 IST
=> Training   99.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.561 DataTime=0.388 Loss=1.373 Prec@1=66.520 Prec@5=87.034 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=22:33 IST
=> Training   99.92% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.561 DataTime=0.388 Loss=1.373 Prec@1=66.521 Prec@5=87.034 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=22:33 IST
=> Training   100.00% of 1x2503...Epoch=57/150 LR=0.0694 Time=0.561 DataTime=0.388 Loss=1.373 Prec@1=66.521 Prec@5=87.034 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=22:33 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:33 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:33 IST
=> Validation 0.00% of 1x98...Epoch=57/150 LR=0.0694 Time=7.046 Loss=0.855 Prec@1=80.078 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=22:33 IST
=> Validation 1.02% of 1x98...Epoch=57/150 LR=0.0694 Time=7.046 Loss=0.855 Prec@1=80.078 Prec@5=93.750 rate=5731.28 Hz, eta=0:00:00, total=0:00:00, wall=22:33 IST
** Validation 1.02% of 1x98...Epoch=57/150 LR=0.0694 Time=7.046 Loss=0.855 Prec@1=80.078 Prec@5=93.750 rate=5731.28 Hz, eta=0:00:00, total=0:00:00, wall=22:34 IST
** Validation 1.02% of 1x98...Epoch=57/150 LR=0.0694 Time=0.635 Loss=1.521 Prec@1=63.570 Prec@5=85.486 rate=5731.28 Hz, eta=0:00:00, total=0:00:00, wall=22:34 IST
** Validation 100.00% of 1x98...Epoch=57/150 LR=0.0694 Time=0.635 Loss=1.521 Prec@1=63.570 Prec@5=85.486 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=22:34 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST
=> Training   0.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=4.764 DataTime=4.307 Loss=1.377 Prec@1=67.773 Prec@5=85.547 rate=0 Hz, eta=?, total=0:00:00, wall=22:34 IST
=> Training   0.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=4.764 DataTime=4.307 Loss=1.377 Prec@1=67.773 Prec@5=85.547 rate=3021.48 Hz, eta=0:00:00, total=0:00:00, wall=22:34 IST
=> Training   0.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=4.764 DataTime=4.307 Loss=1.377 Prec@1=67.773 Prec@5=85.547 rate=3021.48 Hz, eta=0:00:00, total=0:00:00, wall=22:35 IST
=> Training   0.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.591 DataTime=0.425 Loss=1.324 Prec@1=67.663 Prec@5=87.595 rate=3021.48 Hz, eta=0:00:00, total=0:00:00, wall=22:35 IST
=> Training   4.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.591 DataTime=0.425 Loss=1.324 Prec@1=67.663 Prec@5=87.595 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=22:35 IST
=> Training   4.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.591 DataTime=0.425 Loss=1.324 Prec@1=67.663 Prec@5=87.595 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=22:36 IST
=> Training   4.04% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.576 DataTime=0.408 Loss=1.323 Prec@1=67.560 Prec@5=87.622 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=22:36 IST
=> Training   8.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.576 DataTime=0.408 Loss=1.323 Prec@1=67.560 Prec@5=87.622 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:36 IST
=> Training   8.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.576 DataTime=0.408 Loss=1.323 Prec@1=67.560 Prec@5=87.622 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:37 IST
=> Training   8.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.575 DataTime=0.404 Loss=1.327 Prec@1=67.496 Prec@5=87.580 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=22:37 IST
=> Training   12.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.575 DataTime=0.404 Loss=1.327 Prec@1=67.496 Prec@5=87.580 rate=1.79 Hz, eta=0:20:31, total=0:02:48, wall=22:37 IST
=> Training   12.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.575 DataTime=0.404 Loss=1.327 Prec@1=67.496 Prec@5=87.580 rate=1.79 Hz, eta=0:20:31, total=0:02:48, wall=22:38 IST
=> Training   12.03% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.572 DataTime=0.398 Loss=1.333 Prec@1=67.495 Prec@5=87.534 rate=1.79 Hz, eta=0:20:31, total=0:02:48, wall=22:38 IST
=> Training   16.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.572 DataTime=0.398 Loss=1.333 Prec@1=67.495 Prec@5=87.534 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=22:38 IST
=> Training   16.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.572 DataTime=0.398 Loss=1.333 Prec@1=67.495 Prec@5=87.534 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=22:39 IST
=> Training   16.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.571 DataTime=0.397 Loss=1.337 Prec@1=67.378 Prec@5=87.491 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=22:39 IST
=> Training   20.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.571 DataTime=0.397 Loss=1.337 Prec@1=67.378 Prec@5=87.491 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=22:39 IST
=> Training   20.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.571 DataTime=0.397 Loss=1.337 Prec@1=67.378 Prec@5=87.491 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=22:40 IST
=> Training   20.02% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.569 DataTime=0.394 Loss=1.338 Prec@1=67.329 Prec@5=87.482 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=22:40 IST
=> Training   24.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.569 DataTime=0.394 Loss=1.338 Prec@1=67.329 Prec@5=87.482 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=22:40 IST
=> Training   24.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.569 DataTime=0.394 Loss=1.338 Prec@1=67.329 Prec@5=87.482 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=22:41 IST
=> Training   24.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.568 DataTime=0.393 Loss=1.340 Prec@1=67.299 Prec@5=87.464 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=22:41 IST
=> Training   28.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.568 DataTime=0.393 Loss=1.340 Prec@1=67.299 Prec@5=87.464 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:41 IST
=> Training   28.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.568 DataTime=0.393 Loss=1.340 Prec@1=67.299 Prec@5=87.464 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:42 IST
=> Training   28.01% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.392 Loss=1.343 Prec@1=67.221 Prec@5=87.423 rate=1.78 Hz, eta=0:16:51, total=0:06:33, wall=22:42 IST
=> Training   32.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.392 Loss=1.343 Prec@1=67.221 Prec@5=87.423 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:42 IST
=> Training   32.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.392 Loss=1.343 Prec@1=67.221 Prec@5=87.423 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:42 IST
=> Training   32.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.391 Loss=1.347 Prec@1=67.151 Prec@5=87.365 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=22:42 IST
=> Training   36.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.391 Loss=1.347 Prec@1=67.151 Prec@5=87.365 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:42 IST
=> Training   36.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.567 DataTime=0.391 Loss=1.347 Prec@1=67.151 Prec@5=87.365 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:43 IST
=> Training   36.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.347 Prec@1=67.124 Prec@5=87.358 rate=1.78 Hz, eta=0:14:59, total=0:08:26, wall=22:43 IST
=> Training   39.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.347 Prec@1=67.124 Prec@5=87.358 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=22:43 IST
=> Training   39.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.347 Prec@1=67.124 Prec@5=87.358 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=22:44 IST
=> Training   39.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.388 Loss=1.348 Prec@1=67.104 Prec@5=87.340 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=22:44 IST
=> Training   43.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.388 Loss=1.348 Prec@1=67.104 Prec@5=87.340 rate=1.78 Hz, eta=0:13:05, total=0:10:16, wall=22:44 IST
=> Training   43.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.388 Loss=1.348 Prec@1=67.104 Prec@5=87.340 rate=1.78 Hz, eta=0:13:05, total=0:10:16, wall=22:45 IST
=> Training   43.99% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.351 Prec@1=67.036 Prec@5=87.297 rate=1.78 Hz, eta=0:13:05, total=0:10:16, wall=22:45 IST
=> Training   47.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.351 Prec@1=67.036 Prec@5=87.297 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=22:45 IST
=> Training   47.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.351 Prec@1=67.036 Prec@5=87.297 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=22:46 IST
=> Training   47.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.353 Prec@1=67.011 Prec@5=87.276 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=22:46 IST
=> Training   51.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.353 Prec@1=67.011 Prec@5=87.276 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=22:46 IST
=> Training   51.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.353 Prec@1=67.011 Prec@5=87.276 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=22:47 IST
=> Training   51.98% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.355 Prec@1=66.956 Prec@5=87.247 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=22:47 IST
=> Training   55.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.355 Prec@1=66.956 Prec@5=87.247 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=22:47 IST
=> Training   55.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.355 Prec@1=66.956 Prec@5=87.247 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=22:48 IST
=> Training   55.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.358 Prec@1=66.901 Prec@5=87.207 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=22:48 IST
=> Training   59.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.358 Prec@1=66.901 Prec@5=87.207 rate=1.78 Hz, eta=0:09:22, total=0:14:03, wall=22:48 IST
=> Training   59.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.389 Loss=1.358 Prec@1=66.901 Prec@5=87.207 rate=1.78 Hz, eta=0:09:22, total=0:14:03, wall=22:49 IST
=> Training   59.97% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.358 Prec@1=66.890 Prec@5=87.196 rate=1.78 Hz, eta=0:09:22, total=0:14:03, wall=22:49 IST
=> Training   63.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.358 Prec@1=66.890 Prec@5=87.196 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=22:49 IST
=> Training   63.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.358 Prec@1=66.890 Prec@5=87.196 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=22:50 IST
=> Training   63.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.360 Prec@1=66.847 Prec@5=87.162 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=22:50 IST
=> Training   67.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.360 Prec@1=66.847 Prec@5=87.162 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=22:50 IST
=> Training   67.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.360 Prec@1=66.847 Prec@5=87.162 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=22:51 IST
=> Training   67.96% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.361 Prec@1=66.817 Prec@5=87.148 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=22:51 IST
=> Training   71.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.361 Prec@1=66.817 Prec@5=87.148 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=22:51 IST
=> Training   71.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.361 Prec@1=66.817 Prec@5=87.148 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=22:52 IST
=> Training   71.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.363 Prec@1=66.782 Prec@5=87.131 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=22:52 IST
=> Training   75.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.363 Prec@1=66.782 Prec@5=87.131 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=22:52 IST
=> Training   75.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.565 DataTime=0.390 Loss=1.363 Prec@1=66.782 Prec@5=87.131 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=22:53 IST
=> Training   75.95% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.778 Prec@5=87.117 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=22:53 IST
=> Training   79.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.778 Prec@5=87.117 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=22:53 IST
=> Training   79.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.778 Prec@5=87.117 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=22:54 IST
=> Training   79.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.767 Prec@5=87.114 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=22:54 IST
=> Training   83.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.767 Prec@5=87.114 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=22:54 IST
=> Training   83.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.364 Prec@1=66.767 Prec@5=87.114 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=22:55 IST
=> Training   83.94% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.365 Prec@1=66.746 Prec@5=87.098 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=22:55 IST
=> Training   87.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.365 Prec@1=66.746 Prec@5=87.098 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=22:55 IST
=> Training   87.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.389 Loss=1.365 Prec@1=66.746 Prec@5=87.098 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=22:56 IST
=> Training   87.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.367 Prec@1=66.718 Prec@5=87.084 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=22:56 IST
=> Training   91.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.367 Prec@1=66.718 Prec@5=87.084 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=22:56 IST
=> Training   91.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.564 DataTime=0.388 Loss=1.367 Prec@1=66.718 Prec@5=87.084 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=22:57 IST
=> Training   91.93% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.687 Prec@5=87.074 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=22:57 IST
=> Training   95.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.687 Prec@5=87.074 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:57 IST
=> Training   95.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.687 Prec@5=87.074 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:57 IST
=> Training   95.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.686 Prec@5=87.063 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:57 IST
=> Training   99.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.686 Prec@5=87.063 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:57 IST
=> Training   99.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.563 DataTime=0.388 Loss=1.368 Prec@1=66.686 Prec@5=87.063 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:57 IST
=> Training   99.92% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.562 DataTime=0.388 Loss=1.368 Prec@1=66.683 Prec@5=87.060 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=22:57 IST
=> Training   100.00% of 1x2503...Epoch=58/150 LR=0.0684 Time=0.562 DataTime=0.388 Loss=1.368 Prec@1=66.683 Prec@5=87.060 rate=1.78 Hz, eta=0:00:00, total=0:23:22, wall=22:57 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:58 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:58 IST
=> Validation 0.00% of 1x98...Epoch=58/150 LR=0.0684 Time=6.846 Loss=1.017 Prec@1=74.023 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=22:58 IST
=> Validation 1.02% of 1x98...Epoch=58/150 LR=0.0684 Time=6.846 Loss=1.017 Prec@1=74.023 Prec@5=90.625 rate=5803.12 Hz, eta=0:00:00, total=0:00:00, wall=22:58 IST
** Validation 1.02% of 1x98...Epoch=58/150 LR=0.0684 Time=6.846 Loss=1.017 Prec@1=74.023 Prec@5=90.625 rate=5803.12 Hz, eta=0:00:00, total=0:00:00, wall=22:58 IST
** Validation 1.02% of 1x98...Epoch=58/150 LR=0.0684 Time=0.628 Loss=1.496 Prec@1=64.078 Prec@5=85.710 rate=5803.12 Hz, eta=0:00:00, total=0:00:00, wall=22:58 IST
** Validation 100.00% of 1x98...Epoch=58/150 LR=0.0684 Time=0.628 Loss=1.496 Prec@1=64.078 Prec@5=85.710 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=22:58 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> Training   0.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=5.273 DataTime=5.095 Loss=1.304 Prec@1=69.727 Prec@5=88.867 rate=0 Hz, eta=?, total=0:00:00, wall=22:59 IST
=> Training   0.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=5.273 DataTime=5.095 Loss=1.304 Prec@1=69.727 Prec@5=88.867 rate=1077.83 Hz, eta=0:00:02, total=0:00:00, wall=22:59 IST
=> Training   0.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=5.273 DataTime=5.095 Loss=1.304 Prec@1=69.727 Prec@5=88.867 rate=1077.83 Hz, eta=0:00:02, total=0:00:00, wall=22:59 IST
=> Training   0.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.594 DataTime=0.435 Loss=1.328 Prec@1=67.499 Prec@5=87.664 rate=1077.83 Hz, eta=0:00:02, total=0:00:00, wall=22:59 IST
=> Training   4.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.594 DataTime=0.435 Loss=1.328 Prec@1=67.499 Prec@5=87.664 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=22:59 IST
=> Training   4.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.594 DataTime=0.435 Loss=1.328 Prec@1=67.499 Prec@5=87.664 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=23:00 IST
=> Training   4.04% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.573 DataTime=0.406 Loss=1.327 Prec@1=67.514 Prec@5=87.595 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=23:00 IST
=> Training   8.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.573 DataTime=0.406 Loss=1.327 Prec@1=67.514 Prec@5=87.595 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=23:00 IST
=> Training   8.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.573 DataTime=0.406 Loss=1.327 Prec@1=67.514 Prec@5=87.595 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=23:01 IST
=> Training   8.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.569 DataTime=0.399 Loss=1.326 Prec@1=67.538 Prec@5=87.665 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=23:01 IST
=> Training   12.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.569 DataTime=0.399 Loss=1.326 Prec@1=67.538 Prec@5=87.665 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=23:01 IST
=> Training   12.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.569 DataTime=0.399 Loss=1.326 Prec@1=67.538 Prec@5=87.665 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=23:02 IST
=> Training   12.03% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.395 Loss=1.330 Prec@1=67.445 Prec@5=87.670 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=23:02 IST
=> Training   16.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.395 Loss=1.330 Prec@1=67.445 Prec@5=87.670 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=23:02 IST
=> Training   16.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.395 Loss=1.330 Prec@1=67.445 Prec@5=87.670 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=23:03 IST
=> Training   16.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.566 DataTime=0.396 Loss=1.334 Prec@1=67.375 Prec@5=87.592 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=23:03 IST
=> Training   20.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.566 DataTime=0.396 Loss=1.334 Prec@1=67.375 Prec@5=87.592 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=23:03 IST
=> Training   20.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.566 DataTime=0.396 Loss=1.334 Prec@1=67.375 Prec@5=87.592 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=23:04 IST
=> Training   20.02% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.393 Loss=1.335 Prec@1=67.349 Prec@5=87.566 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=23:04 IST
=> Training   24.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.393 Loss=1.335 Prec@1=67.349 Prec@5=87.566 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:04 IST
=> Training   24.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.393 Loss=1.335 Prec@1=67.349 Prec@5=87.566 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:05 IST
=> Training   24.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.393 Loss=1.340 Prec@1=67.283 Prec@5=87.511 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=23:05 IST
=> Training   28.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.393 Loss=1.340 Prec@1=67.283 Prec@5=87.511 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=23:05 IST
=> Training   28.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.393 Loss=1.340 Prec@1=67.283 Prec@5=87.511 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=23:06 IST
=> Training   28.01% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.394 Loss=1.342 Prec@1=67.243 Prec@5=87.477 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=23:06 IST
=> Training   32.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.394 Loss=1.342 Prec@1=67.243 Prec@5=87.477 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=23:06 IST
=> Training   32.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.565 DataTime=0.394 Loss=1.342 Prec@1=67.243 Prec@5=87.477 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=23:07 IST
=> Training   32.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.344 Prec@1=67.176 Prec@5=87.436 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=23:07 IST
=> Training   36.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.344 Prec@1=67.176 Prec@5=87.436 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=23:07 IST
=> Training   36.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.344 Prec@1=67.176 Prec@5=87.436 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=23:08 IST
=> Training   36.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.346 Prec@1=67.114 Prec@5=87.420 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=23:08 IST
=> Training   39.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.346 Prec@1=67.114 Prec@5=87.420 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=23:08 IST
=> Training   39.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.346 Prec@1=67.114 Prec@5=87.420 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=23:09 IST
=> Training   39.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.348 Prec@1=67.055 Prec@5=87.397 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=23:09 IST
=> Training   43.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.348 Prec@1=67.055 Prec@5=87.397 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=23:09 IST
=> Training   43.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.348 Prec@1=67.055 Prec@5=87.397 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=23:10 IST
=> Training   43.99% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.350 Prec@1=67.050 Prec@5=87.361 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=23:10 IST
=> Training   47.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.350 Prec@1=67.050 Prec@5=87.361 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=23:10 IST
=> Training   47.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.350 Prec@1=67.050 Prec@5=87.361 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=23:11 IST
=> Training   47.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.351 Prec@1=67.011 Prec@5=87.353 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=23:11 IST
=> Training   51.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.351 Prec@1=67.011 Prec@5=87.353 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:11 IST
=> Training   51.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.351 Prec@1=67.011 Prec@5=87.353 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:12 IST
=> Training   51.98% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.353 Prec@1=66.986 Prec@5=87.318 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:12 IST
=> Training   55.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.353 Prec@1=66.986 Prec@5=87.318 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=23:12 IST
=> Training   55.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.564 DataTime=0.392 Loss=1.353 Prec@1=66.986 Prec@5=87.318 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=23:13 IST
=> Training   55.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.354 Prec@1=66.976 Prec@5=87.300 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=23:13 IST
=> Training   59.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.354 Prec@1=66.976 Prec@5=87.300 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:13 IST
=> Training   59.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.354 Prec@1=66.976 Prec@5=87.300 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:14 IST
=> Training   59.97% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.355 Prec@1=66.947 Prec@5=87.284 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:14 IST
=> Training   63.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.355 Prec@1=66.947 Prec@5=87.284 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:14 IST
=> Training   63.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.563 DataTime=0.391 Loss=1.355 Prec@1=66.947 Prec@5=87.284 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:14 IST
=> Training   63.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.355 Prec@1=66.957 Prec@5=87.280 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:14 IST
=> Training   67.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.355 Prec@1=66.957 Prec@5=87.280 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=23:14 IST
=> Training   67.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.355 Prec@1=66.957 Prec@5=87.280 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=23:15 IST
=> Training   67.96% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.357 Prec@1=66.927 Prec@5=87.250 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=23:15 IST
=> Training   71.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.357 Prec@1=66.927 Prec@5=87.250 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:15 IST
=> Training   71.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.357 Prec@1=66.927 Prec@5=87.250 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:16 IST
=> Training   71.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.561 DataTime=0.389 Loss=1.358 Prec@1=66.891 Prec@5=87.224 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:16 IST
=> Training   75.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.561 DataTime=0.389 Loss=1.358 Prec@1=66.891 Prec@5=87.224 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=23:16 IST
=> Training   75.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.561 DataTime=0.389 Loss=1.358 Prec@1=66.891 Prec@5=87.224 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=23:17 IST
=> Training   75.95% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.359 Prec@1=66.877 Prec@5=87.209 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=23:17 IST
=> Training   79.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.359 Prec@1=66.877 Prec@5=87.209 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:17 IST
=> Training   79.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.390 Loss=1.359 Prec@1=66.877 Prec@5=87.209 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:18 IST
=> Training   79.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.847 Prec@5=87.202 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:18 IST
=> Training   83.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.847 Prec@5=87.202 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=23:18 IST
=> Training   83.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.847 Prec@5=87.202 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=23:19 IST
=> Training   83.94% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.832 Prec@5=87.189 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=23:19 IST
=> Training   87.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.832 Prec@5=87.189 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:19 IST
=> Training   87.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.360 Prec@1=66.832 Prec@5=87.189 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:20 IST
=> Training   87.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.388 Loss=1.361 Prec@1=66.826 Prec@5=87.174 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:20 IST
=> Training   91.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.388 Loss=1.361 Prec@1=66.826 Prec@5=87.174 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:20 IST
=> Training   91.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.388 Loss=1.361 Prec@1=66.826 Prec@5=87.174 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:21 IST
=> Training   91.93% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.362 Prec@1=66.805 Prec@5=87.161 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:21 IST
=> Training   95.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.362 Prec@1=66.805 Prec@5=87.161 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=23:21 IST
=> Training   95.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.362 Prec@1=66.805 Prec@5=87.161 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=23:22 IST
=> Training   95.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.364 Prec@1=66.768 Prec@5=87.138 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=23:22 IST
=> Training   99.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.364 Prec@1=66.768 Prec@5=87.138 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:22 IST
=> Training   99.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.389 Loss=1.364 Prec@1=66.768 Prec@5=87.138 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:22 IST
=> Training   99.92% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.388 Loss=1.364 Prec@1=66.768 Prec@5=87.137 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:22 IST
=> Training   100.00% of 1x2503...Epoch=59/150 LR=0.0674 Time=0.562 DataTime=0.388 Loss=1.364 Prec@1=66.768 Prec@5=87.137 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=23:22 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST
=> Validation 0.00% of 1x98...Epoch=59/150 LR=0.0674 Time=7.297 Loss=0.980 Prec@1=76.758 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=23:22 IST
=> Validation 1.02% of 1x98...Epoch=59/150 LR=0.0674 Time=7.297 Loss=0.980 Prec@1=76.758 Prec@5=91.992 rate=8200.96 Hz, eta=0:00:00, total=0:00:00, wall=23:22 IST
** Validation 1.02% of 1x98...Epoch=59/150 LR=0.0674 Time=7.297 Loss=0.980 Prec@1=76.758 Prec@5=91.992 rate=8200.96 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST
** Validation 1.02% of 1x98...Epoch=59/150 LR=0.0674 Time=0.640 Loss=1.473 Prec@1=64.518 Prec@5=86.080 rate=8200.96 Hz, eta=0:00:00, total=0:00:00, wall=23:23 IST
** Validation 100.00% of 1x98...Epoch=59/150 LR=0.0674 Time=0.640 Loss=1.473 Prec@1=64.518 Prec@5=86.080 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=23:23 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST
=> Training   0.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=4.816 DataTime=4.317 Loss=1.402 Prec@1=66.406 Prec@5=88.086 rate=0 Hz, eta=?, total=0:00:00, wall=23:23 IST
=> Training   0.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=4.816 DataTime=4.317 Loss=1.402 Prec@1=66.406 Prec@5=88.086 rate=1647.76 Hz, eta=0:00:01, total=0:00:00, wall=23:23 IST
=> Training   0.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=4.816 DataTime=4.317 Loss=1.402 Prec@1=66.406 Prec@5=88.086 rate=1647.76 Hz, eta=0:00:01, total=0:00:00, wall=23:24 IST
=> Training   0.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.589 DataTime=0.424 Loss=1.316 Prec@1=67.820 Prec@5=87.777 rate=1647.76 Hz, eta=0:00:01, total=0:00:00, wall=23:24 IST
=> Training   4.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.589 DataTime=0.424 Loss=1.316 Prec@1=67.820 Prec@5=87.777 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=23:24 IST
=> Training   4.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.589 DataTime=0.424 Loss=1.316 Prec@1=67.820 Prec@5=87.777 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=23:25 IST
=> Training   4.04% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.585 DataTime=0.416 Loss=1.317 Prec@1=67.771 Prec@5=87.788 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=23:25 IST
=> Training   8.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.585 DataTime=0.416 Loss=1.317 Prec@1=67.771 Prec@5=87.788 rate=1.78 Hz, eta=0:21:32, total=0:01:52, wall=23:25 IST
=> Training   8.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.585 DataTime=0.416 Loss=1.317 Prec@1=67.771 Prec@5=87.788 rate=1.78 Hz, eta=0:21:32, total=0:01:52, wall=23:26 IST
=> Training   8.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.576 DataTime=0.405 Loss=1.317 Prec@1=67.746 Prec@5=87.786 rate=1.78 Hz, eta=0:21:32, total=0:01:52, wall=23:26 IST
=> Training   12.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.576 DataTime=0.405 Loss=1.317 Prec@1=67.746 Prec@5=87.786 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=23:26 IST
=> Training   12.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.576 DataTime=0.405 Loss=1.317 Prec@1=67.746 Prec@5=87.786 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=23:27 IST
=> Training   12.03% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.575 DataTime=0.404 Loss=1.322 Prec@1=67.614 Prec@5=87.701 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=23:27 IST
=> Training   16.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.575 DataTime=0.404 Loss=1.322 Prec@1=67.614 Prec@5=87.701 rate=1.78 Hz, eta=0:19:43, total=0:03:45, wall=23:27 IST
=> Training   16.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.575 DataTime=0.404 Loss=1.322 Prec@1=67.614 Prec@5=87.701 rate=1.78 Hz, eta=0:19:43, total=0:03:45, wall=23:28 IST
=> Training   16.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.568 DataTime=0.397 Loss=1.323 Prec@1=67.590 Prec@5=87.689 rate=1.78 Hz, eta=0:19:43, total=0:03:45, wall=23:28 IST
=> Training   20.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.568 DataTime=0.397 Loss=1.323 Prec@1=67.590 Prec@5=87.689 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=23:28 IST
=> Training   20.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.568 DataTime=0.397 Loss=1.323 Prec@1=67.590 Prec@5=87.689 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=23:29 IST
=> Training   20.02% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.565 DataTime=0.394 Loss=1.328 Prec@1=67.463 Prec@5=87.602 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=23:29 IST
=> Training   24.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.565 DataTime=0.394 Loss=1.328 Prec@1=67.463 Prec@5=87.602 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=23:29 IST
=> Training   24.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.565 DataTime=0.394 Loss=1.328 Prec@1=67.463 Prec@5=87.602 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=23:30 IST
=> Training   24.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.566 DataTime=0.393 Loss=1.330 Prec@1=67.422 Prec@5=87.616 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=23:30 IST
=> Training   28.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.566 DataTime=0.393 Loss=1.330 Prec@1=67.422 Prec@5=87.616 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=23:30 IST
=> Training   28.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.566 DataTime=0.393 Loss=1.330 Prec@1=67.422 Prec@5=87.616 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=23:30 IST
=> Training   28.01% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.329 Prec@1=67.442 Prec@5=87.623 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=23:30 IST
=> Training   32.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.329 Prec@1=67.442 Prec@5=87.623 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=23:30 IST
=> Training   32.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.329 Prec@1=67.442 Prec@5=87.623 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=23:31 IST
=> Training   32.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.564 DataTime=0.392 Loss=1.332 Prec@1=67.385 Prec@5=87.582 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=23:31 IST
=> Training   36.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.564 DataTime=0.392 Loss=1.332 Prec@1=67.385 Prec@5=87.582 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:31 IST
=> Training   36.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.564 DataTime=0.392 Loss=1.332 Prec@1=67.385 Prec@5=87.582 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:32 IST
=> Training   36.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.333 Prec@1=67.361 Prec@5=87.547 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:32 IST
=> Training   39.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.333 Prec@1=67.361 Prec@5=87.547 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=23:32 IST
=> Training   39.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.333 Prec@1=67.361 Prec@5=87.547 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=23:33 IST
=> Training   39.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.335 Prec@1=67.317 Prec@5=87.533 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=23:33 IST
=> Training   43.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.335 Prec@1=67.317 Prec@5=87.533 rate=1.79 Hz, eta=0:13:03, total=0:10:14, wall=23:33 IST
=> Training   43.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.335 Prec@1=67.317 Prec@5=87.533 rate=1.79 Hz, eta=0:13:03, total=0:10:14, wall=23:34 IST
=> Training   43.99% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.337 Prec@1=67.276 Prec@5=87.515 rate=1.79 Hz, eta=0:13:03, total=0:10:14, wall=23:34 IST
=> Training   47.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.337 Prec@1=67.276 Prec@5=87.515 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=23:34 IST
=> Training   47.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.337 Prec@1=67.276 Prec@5=87.515 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=23:35 IST
=> Training   47.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.338 Prec@1=67.240 Prec@5=87.506 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=23:35 IST
=> Training   51.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.338 Prec@1=67.240 Prec@5=87.506 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:35 IST
=> Training   51.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.338 Prec@1=67.240 Prec@5=87.506 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:36 IST
=> Training   51.98% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.340 Prec@1=67.220 Prec@5=87.481 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:36 IST
=> Training   55.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.340 Prec@1=67.220 Prec@5=87.481 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=23:36 IST
=> Training   55.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.340 Prec@1=67.220 Prec@5=87.481 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=23:37 IST
=> Training   55.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.392 Loss=1.342 Prec@1=67.200 Prec@5=87.461 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=23:37 IST
=> Training   59.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.392 Loss=1.342 Prec@1=67.200 Prec@5=87.461 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=23:37 IST
=> Training   59.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.392 Loss=1.342 Prec@1=67.200 Prec@5=87.461 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=23:38 IST
=> Training   59.97% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.343 Prec@1=67.177 Prec@5=87.452 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=23:38 IST
=> Training   63.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.343 Prec@1=67.177 Prec@5=87.452 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:38 IST
=> Training   63.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.391 Loss=1.343 Prec@1=67.177 Prec@5=87.452 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:39 IST
=> Training   63.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.345 Prec@1=67.148 Prec@5=87.431 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=23:39 IST
=> Training   67.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.345 Prec@1=67.148 Prec@5=87.431 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=23:39 IST
=> Training   67.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.563 DataTime=0.391 Loss=1.345 Prec@1=67.148 Prec@5=87.431 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=23:40 IST
=> Training   67.96% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.347 Prec@1=67.113 Prec@5=87.395 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=23:40 IST
=> Training   71.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.347 Prec@1=67.113 Prec@5=87.395 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:40 IST
=> Training   71.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.347 Prec@1=67.113 Prec@5=87.395 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:41 IST
=> Training   71.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.348 Prec@1=67.080 Prec@5=87.374 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=23:41 IST
=> Training   75.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.348 Prec@1=67.080 Prec@5=87.374 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=23:41 IST
=> Training   75.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.348 Prec@1=67.080 Prec@5=87.374 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=23:42 IST
=> Training   75.95% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.350 Prec@1=67.050 Prec@5=87.351 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=23:42 IST
=> Training   79.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.350 Prec@1=67.050 Prec@5=87.351 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:42 IST
=> Training   79.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.350 Prec@1=67.050 Prec@5=87.351 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:43 IST
=> Training   79.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.351 Prec@1=67.014 Prec@5=87.321 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=23:43 IST
=> Training   83.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.351 Prec@1=67.014 Prec@5=87.321 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=23:43 IST
=> Training   83.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.351 Prec@1=67.014 Prec@5=87.321 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=23:44 IST
=> Training   83.94% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.353 Prec@1=66.982 Prec@5=87.304 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=23:44 IST
=> Training   87.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.353 Prec@1=66.982 Prec@5=87.304 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:44 IST
=> Training   87.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.353 Prec@1=66.982 Prec@5=87.304 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:45 IST
=> Training   87.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.389 Loss=1.354 Prec@1=66.938 Prec@5=87.297 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=23:45 IST
=> Training   91.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.389 Loss=1.354 Prec@1=66.938 Prec@5=87.297 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:45 IST
=> Training   91.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.389 Loss=1.354 Prec@1=66.938 Prec@5=87.297 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:45 IST
=> Training   91.93% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.356 Prec@1=66.909 Prec@5=87.272 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=23:45 IST
=> Training   95.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.356 Prec@1=66.909 Prec@5=87.272 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=23:45 IST
=> Training   95.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.562 DataTime=0.390 Loss=1.356 Prec@1=66.909 Prec@5=87.272 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=23:46 IST
=> Training   95.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.561 DataTime=0.389 Loss=1.357 Prec@1=66.900 Prec@5=87.245 rate=1.79 Hz, eta=0:00:57, total=0:22:25, wall=23:46 IST
=> Training   99.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.561 DataTime=0.389 Loss=1.357 Prec@1=66.900 Prec@5=87.245 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:46 IST
=> Training   99.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.561 DataTime=0.389 Loss=1.357 Prec@1=66.900 Prec@5=87.245 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:46 IST
=> Training   99.92% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.561 DataTime=0.389 Loss=1.357 Prec@1=66.899 Prec@5=87.245 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=23:46 IST
=> Training   100.00% of 1x2503...Epoch=60/150 LR=0.0664 Time=0.561 DataTime=0.389 Loss=1.357 Prec@1=66.899 Prec@5=87.245 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=23:46 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:47 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:47 IST
=> Validation 0.00% of 1x98...Epoch=60/150 LR=0.0664 Time=7.517 Loss=0.903 Prec@1=75.391 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=23:47 IST
=> Validation 1.02% of 1x98...Epoch=60/150 LR=0.0664 Time=7.517 Loss=0.903 Prec@1=75.391 Prec@5=93.750 rate=7869.92 Hz, eta=0:00:00, total=0:00:00, wall=23:47 IST
** Validation 1.02% of 1x98...Epoch=60/150 LR=0.0664 Time=7.517 Loss=0.903 Prec@1=75.391 Prec@5=93.750 rate=7869.92 Hz, eta=0:00:00, total=0:00:00, wall=23:47 IST
** Validation 1.02% of 1x98...Epoch=60/150 LR=0.0664 Time=0.638 Loss=1.453 Prec@1=64.706 Prec@5=86.506 rate=7869.92 Hz, eta=0:00:00, total=0:00:00, wall=23:47 IST
** Validation 100.00% of 1x98...Epoch=60/150 LR=0.0664 Time=0.638 Loss=1.453 Prec@1=64.706 Prec@5=86.506 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=23:47 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:48 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:48 IST
=> Training   0.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=4.883 DataTime=4.528 Loss=1.294 Prec@1=68.945 Prec@5=86.719 rate=0 Hz, eta=?, total=0:00:00, wall=23:48 IST
=> Training   0.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=4.883 DataTime=4.528 Loss=1.294 Prec@1=68.945 Prec@5=86.719 rate=10719.49 Hz, eta=0:00:00, total=0:00:00, wall=23:48 IST
=> Training   0.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=4.883 DataTime=4.528 Loss=1.294 Prec@1=68.945 Prec@5=86.719 rate=10719.49 Hz, eta=0:00:00, total=0:00:00, wall=23:48 IST
=> Training   0.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.599 DataTime=0.436 Loss=1.301 Prec@1=68.056 Prec@5=88.038 rate=10719.49 Hz, eta=0:00:00, total=0:00:00, wall=23:48 IST
=> Training   4.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.599 DataTime=0.436 Loss=1.301 Prec@1=68.056 Prec@5=88.038 rate=1.82 Hz, eta=0:22:03, total=0:00:55, wall=23:48 IST
=> Training   4.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.599 DataTime=0.436 Loss=1.301 Prec@1=68.056 Prec@5=88.038 rate=1.82 Hz, eta=0:22:03, total=0:00:55, wall=23:49 IST
=> Training   4.04% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.579 DataTime=0.405 Loss=1.303 Prec@1=67.966 Prec@5=88.090 rate=1.82 Hz, eta=0:22:03, total=0:00:55, wall=23:49 IST
=> Training   8.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.579 DataTime=0.405 Loss=1.303 Prec@1=67.966 Prec@5=88.090 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=23:49 IST
=> Training   8.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.579 DataTime=0.405 Loss=1.303 Prec@1=67.966 Prec@5=88.090 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=23:50 IST
=> Training   8.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.571 DataTime=0.396 Loss=1.307 Prec@1=67.987 Prec@5=87.996 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=23:50 IST
=> Training   12.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.571 DataTime=0.396 Loss=1.307 Prec@1=67.987 Prec@5=87.996 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=23:50 IST
=> Training   12.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.571 DataTime=0.396 Loss=1.307 Prec@1=67.987 Prec@5=87.996 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=23:51 IST
=> Training   12.03% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.566 DataTime=0.388 Loss=1.310 Prec@1=67.850 Prec@5=87.952 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=23:51 IST
=> Training   16.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.566 DataTime=0.388 Loss=1.310 Prec@1=67.850 Prec@5=87.952 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=23:51 IST
=> Training   16.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.566 DataTime=0.388 Loss=1.310 Prec@1=67.850 Prec@5=87.952 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=23:52 IST
=> Training   16.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.565 DataTime=0.387 Loss=1.313 Prec@1=67.815 Prec@5=87.918 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=23:52 IST
=> Training   20.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.565 DataTime=0.387 Loss=1.313 Prec@1=67.815 Prec@5=87.918 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=23:52 IST
=> Training   20.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.565 DataTime=0.387 Loss=1.313 Prec@1=67.815 Prec@5=87.918 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=23:53 IST
=> Training   20.02% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.319 Prec@1=67.680 Prec@5=87.840 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=23:53 IST
=> Training   24.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.319 Prec@1=67.680 Prec@5=87.840 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=23:53 IST
=> Training   24.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.319 Prec@1=67.680 Prec@5=87.840 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=23:54 IST
=> Training   24.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.322 Prec@1=67.646 Prec@5=87.778 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=23:54 IST
=> Training   28.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.322 Prec@1=67.646 Prec@5=87.778 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=23:54 IST
=> Training   28.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.564 DataTime=0.385 Loss=1.322 Prec@1=67.646 Prec@5=87.778 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=23:55 IST
=> Training   28.01% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.323 Prec@1=67.622 Prec@5=87.748 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=23:55 IST
=> Training   32.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.323 Prec@1=67.622 Prec@5=87.748 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=23:55 IST
=> Training   32.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.323 Prec@1=67.622 Prec@5=87.748 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=23:56 IST
=> Training   32.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.326 Prec@1=67.533 Prec@5=87.707 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=23:56 IST
=> Training   36.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.326 Prec@1=67.533 Prec@5=87.707 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:56 IST
=> Training   36.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.383 Loss=1.326 Prec@1=67.533 Prec@5=87.707 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:57 IST
=> Training   36.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.329 Prec@1=67.475 Prec@5=87.667 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=23:57 IST
=> Training   39.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.329 Prec@1=67.475 Prec@5=87.667 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=23:57 IST
=> Training   39.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.329 Prec@1=67.475 Prec@5=87.667 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=23:58 IST
=> Training   39.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.331 Prec@1=67.432 Prec@5=87.631 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=23:58 IST
=> Training   43.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.331 Prec@1=67.432 Prec@5=87.631 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=23:58 IST
=> Training   43.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.384 Loss=1.331 Prec@1=67.432 Prec@5=87.631 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=23:59 IST
=> Training   43.99% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.383 Loss=1.333 Prec@1=67.389 Prec@5=87.588 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=23:59 IST
=> Training   47.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.383 Loss=1.333 Prec@1=67.389 Prec@5=87.588 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=23:59 IST
=> Training   47.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.383 Loss=1.333 Prec@1=67.389 Prec@5=87.588 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=00:00 IST
=> Training   47.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.385 Loss=1.334 Prec@1=67.365 Prec@5=87.567 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=00:00 IST
=> Training   51.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.385 Loss=1.334 Prec@1=67.365 Prec@5=87.567 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=00:00 IST
=> Training   51.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.385 Loss=1.334 Prec@1=67.365 Prec@5=87.567 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=00:01 IST
=> Training   51.98% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.336 Prec@1=67.324 Prec@5=87.538 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=00:01 IST
=> Training   55.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.336 Prec@1=67.324 Prec@5=87.538 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:01 IST
=> Training   55.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.336 Prec@1=67.324 Prec@5=87.538 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:02 IST
=> Training   55.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.337 Prec@1=67.294 Prec@5=87.515 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=00:02 IST
=> Training   59.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.337 Prec@1=67.294 Prec@5=87.515 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:02 IST
=> Training   59.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.337 Prec@1=67.294 Prec@5=87.515 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:02 IST
=> Training   59.97% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.339 Prec@1=67.261 Prec@5=87.496 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:02 IST
=> Training   63.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.339 Prec@1=67.261 Prec@5=87.496 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:02 IST
=> Training   63.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.339 Prec@1=67.261 Prec@5=87.496 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:03 IST
=> Training   63.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.340 Prec@1=67.241 Prec@5=87.475 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=00:03 IST
=> Training   67.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.340 Prec@1=67.241 Prec@5=87.475 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:03 IST
=> Training   67.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.340 Prec@1=67.241 Prec@5=87.475 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:04 IST
=> Training   67.96% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.341 Prec@1=67.218 Prec@5=87.447 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:04 IST
=> Training   71.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.341 Prec@1=67.218 Prec@5=87.447 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:04 IST
=> Training   71.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.562 DataTime=0.386 Loss=1.341 Prec@1=67.218 Prec@5=87.447 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:05 IST
=> Training   71.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.342 Prec@1=67.196 Prec@5=87.438 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:05 IST
=> Training   75.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.342 Prec@1=67.196 Prec@5=87.438 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=00:05 IST
=> Training   75.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.342 Prec@1=67.196 Prec@5=87.438 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=00:06 IST
=> Training   75.95% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.344 Prec@1=67.159 Prec@5=87.421 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=00:06 IST
=> Training   79.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.344 Prec@1=67.159 Prec@5=87.421 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:06 IST
=> Training   79.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.344 Prec@1=67.159 Prec@5=87.421 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:07 IST
=> Training   79.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.346 Prec@1=67.119 Prec@5=87.379 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=00:07 IST
=> Training   83.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.346 Prec@1=67.119 Prec@5=87.379 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=00:07 IST
=> Training   83.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.346 Prec@1=67.119 Prec@5=87.379 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=00:08 IST
=> Training   83.94% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.347 Prec@1=67.095 Prec@5=87.363 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=00:08 IST
=> Training   87.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.347 Prec@1=67.095 Prec@5=87.363 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:08 IST
=> Training   87.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.347 Prec@1=67.095 Prec@5=87.363 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:09 IST
=> Training   87.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.348 Prec@1=67.071 Prec@5=87.350 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:09 IST
=> Training   91.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.348 Prec@1=67.071 Prec@5=87.350 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:09 IST
=> Training   91.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.561 DataTime=0.386 Loss=1.348 Prec@1=67.071 Prec@5=87.350 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:10 IST
=> Training   91.93% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.349 Prec@1=67.040 Prec@5=87.337 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=00:10 IST
=> Training   95.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.349 Prec@1=67.040 Prec@5=87.337 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:10 IST
=> Training   95.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.349 Prec@1=67.040 Prec@5=87.337 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:11 IST
=> Training   95.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.351 Prec@1=67.008 Prec@5=87.324 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:11 IST
=> Training   99.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.351 Prec@1=67.008 Prec@5=87.324 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=00:11 IST
=> Training   99.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.351 Prec@1=67.008 Prec@5=87.324 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=00:11 IST
=> Training   99.92% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.351 Prec@1=67.008 Prec@5=87.323 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=00:11 IST
=> Training   100.00% of 1x2503...Epoch=61/150 LR=0.0655 Time=0.560 DataTime=0.385 Loss=1.351 Prec@1=67.008 Prec@5=87.323 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=00:11 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:11 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:11 IST
=> Validation 0.00% of 1x98...Epoch=61/150 LR=0.0655 Time=9.473 Loss=0.907 Prec@1=77.344 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=00:11 IST
=> Validation 1.02% of 1x98...Epoch=61/150 LR=0.0655 Time=9.473 Loss=0.907 Prec@1=77.344 Prec@5=93.359 rate=8374.02 Hz, eta=0:00:00, total=0:00:00, wall=00:11 IST
** Validation 1.02% of 1x98...Epoch=61/150 LR=0.0655 Time=9.473 Loss=0.907 Prec@1=77.344 Prec@5=93.359 rate=8374.02 Hz, eta=0:00:00, total=0:00:00, wall=00:12 IST
** Validation 1.02% of 1x98...Epoch=61/150 LR=0.0655 Time=0.651 Loss=1.496 Prec@1=64.116 Prec@5=85.854 rate=8374.02 Hz, eta=0:00:00, total=0:00:00, wall=00:12 IST
** Validation 100.00% of 1x98...Epoch=61/150 LR=0.0655 Time=0.651 Loss=1.496 Prec@1=64.116 Prec@5=85.854 rate=1.80 Hz, eta=0:00:00, total=0:00:54, wall=00:12 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:12 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:12 IST
=> Training   0.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=4.916 DataTime=4.382 Loss=1.325 Prec@1=67.773 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=00:12 IST
=> Training   0.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=4.916 DataTime=4.382 Loss=1.325 Prec@1=67.773 Prec@5=88.477 rate=6295.40 Hz, eta=0:00:00, total=0:00:00, wall=00:12 IST
=> Training   0.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=4.916 DataTime=4.382 Loss=1.325 Prec@1=67.773 Prec@5=88.477 rate=6295.40 Hz, eta=0:00:00, total=0:00:00, wall=00:13 IST
=> Training   0.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.583 DataTime=0.417 Loss=1.307 Prec@1=67.638 Prec@5=87.833 rate=6295.40 Hz, eta=0:00:00, total=0:00:00, wall=00:13 IST
=> Training   4.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.583 DataTime=0.417 Loss=1.307 Prec@1=67.638 Prec@5=87.833 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=00:13 IST
=> Training   4.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.583 DataTime=0.417 Loss=1.307 Prec@1=67.638 Prec@5=87.833 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=00:14 IST
=> Training   4.04% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.565 DataTime=0.400 Loss=1.304 Prec@1=67.880 Prec@5=87.900 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=00:14 IST
=> Training   8.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.565 DataTime=0.400 Loss=1.304 Prec@1=67.880 Prec@5=87.900 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=00:14 IST
=> Training   8.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.565 DataTime=0.400 Loss=1.304 Prec@1=67.880 Prec@5=87.900 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=00:15 IST
=> Training   8.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.567 DataTime=0.399 Loss=1.306 Prec@1=67.907 Prec@5=87.890 rate=1.85 Hz, eta=0:20:44, total=0:01:48, wall=00:15 IST
=> Training   12.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.567 DataTime=0.399 Loss=1.306 Prec@1=67.907 Prec@5=87.890 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=00:15 IST
=> Training   12.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.567 DataTime=0.399 Loss=1.306 Prec@1=67.907 Prec@5=87.890 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=00:16 IST
=> Training   12.03% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.562 DataTime=0.393 Loss=1.313 Prec@1=67.789 Prec@5=87.816 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=00:16 IST
=> Training   16.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.562 DataTime=0.393 Loss=1.313 Prec@1=67.789 Prec@5=87.816 rate=1.82 Hz, eta=0:19:16, total=0:03:40, wall=00:16 IST
=> Training   16.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.562 DataTime=0.393 Loss=1.313 Prec@1=67.789 Prec@5=87.816 rate=1.82 Hz, eta=0:19:16, total=0:03:40, wall=00:17 IST
=> Training   16.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.564 DataTime=0.393 Loss=1.314 Prec@1=67.773 Prec@5=87.782 rate=1.82 Hz, eta=0:19:16, total=0:03:40, wall=00:17 IST
=> Training   20.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.564 DataTime=0.393 Loss=1.314 Prec@1=67.773 Prec@5=87.782 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=00:17 IST
=> Training   20.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.564 DataTime=0.393 Loss=1.314 Prec@1=67.773 Prec@5=87.782 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=00:17 IST
=> Training   20.02% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.316 Prec@1=67.770 Prec@5=87.766 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=00:17 IST
=> Training   24.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.316 Prec@1=67.770 Prec@5=87.766 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=00:17 IST
=> Training   24.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.316 Prec@1=67.770 Prec@5=87.766 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=00:18 IST
=> Training   24.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.318 Prec@1=67.729 Prec@5=87.718 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=00:18 IST
=> Training   28.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.318 Prec@1=67.729 Prec@5=87.718 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=00:18 IST
=> Training   28.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.318 Prec@1=67.729 Prec@5=87.718 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=00:19 IST
=> Training   28.01% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.321 Prec@1=67.680 Prec@5=87.682 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=00:19 IST
=> Training   32.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.321 Prec@1=67.680 Prec@5=87.682 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=00:19 IST
=> Training   32.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.321 Prec@1=67.680 Prec@5=87.682 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=00:20 IST
=> Training   32.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.324 Prec@1=67.605 Prec@5=87.647 rate=1.80 Hz, eta=0:15:43, total=0:07:24, wall=00:20 IST
=> Training   36.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.324 Prec@1=67.605 Prec@5=87.647 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=00:20 IST
=> Training   36.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.324 Prec@1=67.605 Prec@5=87.647 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=00:21 IST
=> Training   36.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.387 Loss=1.327 Prec@1=67.547 Prec@5=87.608 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=00:21 IST
=> Training   39.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.387 Loss=1.327 Prec@1=67.547 Prec@5=87.608 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=00:21 IST
=> Training   39.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.387 Loss=1.327 Prec@1=67.547 Prec@5=87.608 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=00:22 IST
=> Training   39.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.329 Prec@1=67.538 Prec@5=87.583 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=00:22 IST
=> Training   43.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.329 Prec@1=67.538 Prec@5=87.583 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=00:22 IST
=> Training   43.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.329 Prec@1=67.538 Prec@5=87.583 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=00:23 IST
=> Training   43.99% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.330 Prec@1=67.501 Prec@5=87.565 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=00:23 IST
=> Training   47.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.330 Prec@1=67.501 Prec@5=87.565 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=00:23 IST
=> Training   47.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.330 Prec@1=67.501 Prec@5=87.565 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=00:24 IST
=> Training   47.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.331 Prec@1=67.472 Prec@5=87.555 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=00:24 IST
=> Training   51.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.331 Prec@1=67.472 Prec@5=87.555 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=00:24 IST
=> Training   51.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.331 Prec@1=67.472 Prec@5=87.555 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=00:25 IST
=> Training   51.98% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.333 Prec@1=67.438 Prec@5=87.523 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=00:25 IST
=> Training   55.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.333 Prec@1=67.438 Prec@5=87.523 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=00:25 IST
=> Training   55.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.333 Prec@1=67.438 Prec@5=87.523 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=00:26 IST
=> Training   55.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.335 Prec@1=67.399 Prec@5=87.488 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=00:26 IST
=> Training   59.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.335 Prec@1=67.399 Prec@5=87.488 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=00:26 IST
=> Training   59.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.388 Loss=1.335 Prec@1=67.399 Prec@5=87.488 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=00:27 IST
=> Training   59.97% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.336 Prec@1=67.379 Prec@5=87.476 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=00:27 IST
=> Training   63.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.336 Prec@1=67.379 Prec@5=87.476 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=00:27 IST
=> Training   63.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.336 Prec@1=67.379 Prec@5=87.476 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=00:28 IST
=> Training   63.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.337 Prec@1=67.337 Prec@5=87.477 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=00:28 IST
=> Training   67.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.337 Prec@1=67.337 Prec@5=87.477 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=00:28 IST
=> Training   67.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.337 Prec@1=67.337 Prec@5=87.477 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=00:29 IST
=> Training   67.96% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.389 Loss=1.338 Prec@1=67.308 Prec@5=87.449 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=00:29 IST
=> Training   71.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.389 Loss=1.338 Prec@1=67.308 Prec@5=87.449 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:29 IST
=> Training   71.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.389 Loss=1.338 Prec@1=67.308 Prec@5=87.449 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:30 IST
=> Training   71.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.339 Prec@1=67.283 Prec@5=87.438 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=00:30 IST
=> Training   75.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.339 Prec@1=67.283 Prec@5=87.438 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:30 IST
=> Training   75.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.561 DataTime=0.388 Loss=1.339 Prec@1=67.283 Prec@5=87.438 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:31 IST
=> Training   75.95% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.341 Prec@1=67.265 Prec@5=87.416 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=00:31 IST
=> Training   79.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.341 Prec@1=67.265 Prec@5=87.416 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=00:31 IST
=> Training   79.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.341 Prec@1=67.265 Prec@5=87.416 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=00:31 IST
=> Training   79.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.239 Prec@5=87.405 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=00:31 IST
=> Training   83.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.239 Prec@5=87.405 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=00:31 IST
=> Training   83.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.239 Prec@5=87.405 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=00:32 IST
=> Training   83.94% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.232 Prec@5=87.400 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=00:32 IST
=> Training   87.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.232 Prec@5=87.400 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:32 IST
=> Training   87.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.232 Prec@5=87.400 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:33 IST
=> Training   87.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.233 Prec@5=87.407 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:33 IST
=> Training   91.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.233 Prec@5=87.407 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:33 IST
=> Training   91.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.342 Prec@1=67.233 Prec@5=87.407 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:34 IST
=> Training   91.93% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.343 Prec@1=67.197 Prec@5=87.392 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:34 IST
=> Training   95.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.343 Prec@1=67.197 Prec@5=87.392 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:34 IST
=> Training   95.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.560 DataTime=0.387 Loss=1.343 Prec@1=67.197 Prec@5=87.392 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:35 IST
=> Training   95.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.386 Loss=1.344 Prec@1=67.177 Prec@5=87.381 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=00:35 IST
=> Training   99.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.386 Loss=1.344 Prec@1=67.177 Prec@5=87.381 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=00:35 IST
=> Training   99.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.386 Loss=1.344 Prec@1=67.177 Prec@5=87.381 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=00:35 IST
=> Training   99.92% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.386 Loss=1.344 Prec@1=67.177 Prec@5=87.381 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=00:35 IST
=> Training   100.00% of 1x2503...Epoch=62/150 LR=0.0645 Time=0.559 DataTime=0.386 Loss=1.344 Prec@1=67.177 Prec@5=87.381 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=00:35 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:35 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:35 IST
=> Validation 0.00% of 1x98...Epoch=62/150 LR=0.0645 Time=6.866 Loss=0.892 Prec@1=77.930 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=00:35 IST
=> Validation 1.02% of 1x98...Epoch=62/150 LR=0.0645 Time=6.866 Loss=0.892 Prec@1=77.930 Prec@5=93.164 rate=6812.69 Hz, eta=0:00:00, total=0:00:00, wall=00:35 IST
** Validation 1.02% of 1x98...Epoch=62/150 LR=0.0645 Time=6.866 Loss=0.892 Prec@1=77.930 Prec@5=93.164 rate=6812.69 Hz, eta=0:00:00, total=0:00:00, wall=00:36 IST
** Validation 1.02% of 1x98...Epoch=62/150 LR=0.0645 Time=0.634 Loss=1.448 Prec@1=65.258 Prec@5=86.408 rate=6812.69 Hz, eta=0:00:00, total=0:00:00, wall=00:36 IST
** Validation 100.00% of 1x98...Epoch=62/150 LR=0.0645 Time=0.634 Loss=1.448 Prec@1=65.258 Prec@5=86.408 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=00:36 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST
=> Training   0.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=4.868 DataTime=4.356 Loss=1.287 Prec@1=67.969 Prec@5=89.258 rate=0 Hz, eta=?, total=0:00:00, wall=00:36 IST
=> Training   0.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=4.868 DataTime=4.356 Loss=1.287 Prec@1=67.969 Prec@5=89.258 rate=2339.08 Hz, eta=0:00:01, total=0:00:00, wall=00:36 IST
=> Training   0.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=4.868 DataTime=4.356 Loss=1.287 Prec@1=67.969 Prec@5=89.258 rate=2339.08 Hz, eta=0:00:01, total=0:00:00, wall=00:37 IST
=> Training   0.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.579 DataTime=0.409 Loss=1.308 Prec@1=68.172 Prec@5=87.836 rate=2339.08 Hz, eta=0:00:01, total=0:00:00, wall=00:37 IST
=> Training   4.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.579 DataTime=0.409 Loss=1.308 Prec@1=68.172 Prec@5=87.836 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=00:37 IST
=> Training   4.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.579 DataTime=0.409 Loss=1.308 Prec@1=68.172 Prec@5=87.836 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=00:38 IST
=> Training   4.04% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.398 Loss=1.308 Prec@1=68.166 Prec@5=87.882 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=00:38 IST
=> Training   8.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.398 Loss=1.308 Prec@1=68.166 Prec@5=87.882 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=00:38 IST
=> Training   8.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.398 Loss=1.308 Prec@1=68.166 Prec@5=87.882 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=00:39 IST
=> Training   8.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.571 DataTime=0.399 Loss=1.307 Prec@1=68.018 Prec@5=87.921 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=00:39 IST
=> Training   12.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.571 DataTime=0.399 Loss=1.307 Prec@1=68.018 Prec@5=87.921 rate=1.80 Hz, eta=0:20:21, total=0:02:47, wall=00:39 IST
=> Training   12.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.571 DataTime=0.399 Loss=1.307 Prec@1=68.018 Prec@5=87.921 rate=1.80 Hz, eta=0:20:21, total=0:02:47, wall=00:40 IST
=> Training   12.03% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.394 Loss=1.309 Prec@1=67.946 Prec@5=87.928 rate=1.80 Hz, eta=0:20:21, total=0:02:47, wall=00:40 IST
=> Training   16.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.394 Loss=1.309 Prec@1=67.946 Prec@5=87.928 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:40 IST
=> Training   16.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.567 DataTime=0.394 Loss=1.309 Prec@1=67.946 Prec@5=87.928 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:41 IST
=> Training   16.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.564 DataTime=0.391 Loss=1.307 Prec@1=67.950 Prec@5=87.969 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=00:41 IST
=> Training   20.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.564 DataTime=0.391 Loss=1.307 Prec@1=67.950 Prec@5=87.969 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:41 IST
=> Training   20.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.564 DataTime=0.391 Loss=1.307 Prec@1=67.950 Prec@5=87.969 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:42 IST
=> Training   20.02% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.308 Prec@1=67.943 Prec@5=87.946 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:42 IST
=> Training   24.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.308 Prec@1=67.943 Prec@5=87.946 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=00:42 IST
=> Training   24.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.308 Prec@1=67.943 Prec@5=87.946 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=00:43 IST
=> Training   24.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.311 Prec@1=67.858 Prec@5=87.901 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=00:43 IST
=> Training   28.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.311 Prec@1=67.858 Prec@5=87.901 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=00:43 IST
=> Training   28.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.311 Prec@1=67.858 Prec@5=87.901 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=00:44 IST
=> Training   28.01% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.312 Prec@1=67.844 Prec@5=87.878 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=00:44 IST
=> Training   32.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.312 Prec@1=67.844 Prec@5=87.878 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=00:44 IST
=> Training   32.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.387 Loss=1.312 Prec@1=67.844 Prec@5=87.878 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=00:45 IST
=> Training   32.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.826 Prec@5=87.839 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=00:45 IST
=> Training   36.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.826 Prec@5=87.839 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=00:45 IST
=> Training   36.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.826 Prec@5=87.839 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=00:46 IST
=> Training   36.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.315 Prec@1=67.773 Prec@5=87.802 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=00:46 IST
=> Training   39.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.315 Prec@1=67.773 Prec@5=87.802 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=00:46 IST
=> Training   39.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.315 Prec@1=67.773 Prec@5=87.802 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=00:47 IST
=> Training   39.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.319 Prec@1=67.741 Prec@5=87.751 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=00:47 IST
=> Training   43.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.319 Prec@1=67.741 Prec@5=87.751 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:47 IST
=> Training   43.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.319 Prec@1=67.741 Prec@5=87.751 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:48 IST
=> Training   43.99% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.320 Prec@1=67.702 Prec@5=87.718 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:48 IST
=> Training   47.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.320 Prec@1=67.702 Prec@5=87.718 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:48 IST
=> Training   47.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.563 DataTime=0.388 Loss=1.320 Prec@1=67.702 Prec@5=87.718 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:48 IST
=> Training   47.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.321 Prec@1=67.682 Prec@5=87.704 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:48 IST
=> Training   51.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.321 Prec@1=67.682 Prec@5=87.704 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=00:48 IST
=> Training   51.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.321 Prec@1=67.682 Prec@5=87.704 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=00:49 IST
=> Training   51.98% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.386 Loss=1.323 Prec@1=67.626 Prec@5=87.673 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=00:49 IST
=> Training   55.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.386 Loss=1.323 Prec@1=67.626 Prec@5=87.673 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=00:49 IST
=> Training   55.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.386 Loss=1.323 Prec@1=67.626 Prec@5=87.673 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=00:50 IST
=> Training   55.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.324 Prec@1=67.610 Prec@5=87.669 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=00:50 IST
=> Training   59.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.324 Prec@1=67.610 Prec@5=87.669 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:50 IST
=> Training   59.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.324 Prec@1=67.610 Prec@5=87.669 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:51 IST
=> Training   59.97% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.326 Prec@1=67.584 Prec@5=87.642 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=00:51 IST
=> Training   63.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.326 Prec@1=67.584 Prec@5=87.642 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=00:51 IST
=> Training   63.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.326 Prec@1=67.584 Prec@5=87.642 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=00:52 IST
=> Training   63.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.327 Prec@1=67.539 Prec@5=87.630 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=00:52 IST
=> Training   67.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.327 Prec@1=67.539 Prec@5=87.630 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:52 IST
=> Training   67.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.327 Prec@1=67.539 Prec@5=87.630 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:53 IST
=> Training   67.96% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.330 Prec@1=67.482 Prec@5=87.601 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=00:53 IST
=> Training   71.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.330 Prec@1=67.482 Prec@5=87.601 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:53 IST
=> Training   71.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.562 DataTime=0.385 Loss=1.330 Prec@1=67.482 Prec@5=87.601 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:54 IST
=> Training   71.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.332 Prec@1=67.434 Prec@5=87.579 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:54 IST
=> Training   75.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.332 Prec@1=67.434 Prec@5=87.579 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=00:54 IST
=> Training   75.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.385 Loss=1.332 Prec@1=67.434 Prec@5=87.579 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=00:55 IST
=> Training   75.95% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.384 Loss=1.333 Prec@1=67.416 Prec@5=87.566 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=00:55 IST
=> Training   79.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.384 Loss=1.333 Prec@1=67.416 Prec@5=87.566 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=00:55 IST
=> Training   79.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.561 DataTime=0.384 Loss=1.333 Prec@1=67.416 Prec@5=87.566 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=00:56 IST
=> Training   79.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.383 Loss=1.334 Prec@1=67.396 Prec@5=87.547 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=00:56 IST
=> Training   83.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.383 Loss=1.334 Prec@1=67.396 Prec@5=87.547 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:56 IST
=> Training   83.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.383 Loss=1.334 Prec@1=67.396 Prec@5=87.547 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:57 IST
=> Training   83.94% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.335 Prec@1=67.367 Prec@5=87.543 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=00:57 IST
=> Training   87.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.335 Prec@1=67.367 Prec@5=87.543 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:57 IST
=> Training   87.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.335 Prec@1=67.367 Prec@5=87.543 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:58 IST
=> Training   87.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.336 Prec@1=67.335 Prec@5=87.529 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=00:58 IST
=> Training   91.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.336 Prec@1=67.335 Prec@5=87.529 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:58 IST
=> Training   91.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.336 Prec@1=67.335 Prec@5=87.529 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:59 IST
=> Training   91.93% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.338 Prec@1=67.305 Prec@5=87.505 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=00:59 IST
=> Training   95.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.338 Prec@1=67.305 Prec@5=87.505 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=00:59 IST
=> Training   95.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.560 DataTime=0.382 Loss=1.338 Prec@1=67.305 Prec@5=87.505 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=01:00 IST
=> Training   95.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.559 DataTime=0.381 Loss=1.339 Prec@1=67.282 Prec@5=87.491 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=01:00 IST
=> Training   99.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.559 DataTime=0.381 Loss=1.339 Prec@1=67.282 Prec@5=87.491 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=01:00 IST
=> Training   99.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.559 DataTime=0.381 Loss=1.339 Prec@1=67.282 Prec@5=87.491 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=01:00 IST
=> Training   99.92% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.559 DataTime=0.381 Loss=1.339 Prec@1=67.280 Prec@5=87.491 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=01:00 IST
=> Training   100.00% of 1x2503...Epoch=63/150 LR=0.0634 Time=0.559 DataTime=0.381 Loss=1.339 Prec@1=67.280 Prec@5=87.491 rate=1.79 Hz, eta=0:00:00, total=0:23:14, wall=01:00 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST
=> Validation 0.00% of 1x98...Epoch=63/150 LR=0.0634 Time=6.675 Loss=0.989 Prec@1=76.562 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=01:00 IST
=> Validation 1.02% of 1x98...Epoch=63/150 LR=0.0634 Time=6.675 Loss=0.989 Prec@1=76.562 Prec@5=92.773 rate=6352.64 Hz, eta=0:00:00, total=0:00:00, wall=01:00 IST
** Validation 1.02% of 1x98...Epoch=63/150 LR=0.0634 Time=6.675 Loss=0.989 Prec@1=76.562 Prec@5=92.773 rate=6352.64 Hz, eta=0:00:00, total=0:00:00, wall=01:01 IST
** Validation 1.02% of 1x98...Epoch=63/150 LR=0.0634 Time=0.635 Loss=1.457 Prec@1=64.976 Prec@5=86.386 rate=6352.64 Hz, eta=0:00:00, total=0:00:00, wall=01:01 IST
** Validation 100.00% of 1x98...Epoch=63/150 LR=0.0634 Time=0.635 Loss=1.457 Prec@1=64.976 Prec@5=86.386 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=01:01 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST
=> Training   0.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=4.920 DataTime=4.537 Loss=1.238 Prec@1=68.945 Prec@5=88.281 rate=0 Hz, eta=?, total=0:00:00, wall=01:01 IST
=> Training   0.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=4.920 DataTime=4.537 Loss=1.238 Prec@1=68.945 Prec@5=88.281 rate=1183.24 Hz, eta=0:00:02, total=0:00:00, wall=01:01 IST
=> Training   0.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=4.920 DataTime=4.537 Loss=1.238 Prec@1=68.945 Prec@5=88.281 rate=1183.24 Hz, eta=0:00:02, total=0:00:00, wall=01:02 IST
=> Training   0.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.593 DataTime=0.429 Loss=1.295 Prec@1=68.332 Prec@5=87.997 rate=1183.24 Hz, eta=0:00:02, total=0:00:00, wall=01:02 IST
=> Training   4.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.593 DataTime=0.429 Loss=1.295 Prec@1=68.332 Prec@5=87.997 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=01:02 IST
=> Training   4.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.593 DataTime=0.429 Loss=1.295 Prec@1=68.332 Prec@5=87.997 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=01:03 IST
=> Training   4.04% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.577 DataTime=0.413 Loss=1.301 Prec@1=68.214 Prec@5=87.960 rate=1.84 Hz, eta=0:21:48, total=0:00:55, wall=01:03 IST
=> Training   8.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.577 DataTime=0.413 Loss=1.301 Prec@1=68.214 Prec@5=87.960 rate=1.81 Hz, eta=0:21:11, total=0:01:50, wall=01:03 IST
=> Training   8.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.577 DataTime=0.413 Loss=1.301 Prec@1=68.214 Prec@5=87.960 rate=1.81 Hz, eta=0:21:11, total=0:01:50, wall=01:04 IST
=> Training   8.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.568 DataTime=0.400 Loss=1.302 Prec@1=68.233 Prec@5=87.961 rate=1.81 Hz, eta=0:21:11, total=0:01:50, wall=01:04 IST
=> Training   12.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.568 DataTime=0.400 Loss=1.302 Prec@1=68.233 Prec@5=87.961 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=01:04 IST
=> Training   12.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.568 DataTime=0.400 Loss=1.302 Prec@1=68.233 Prec@5=87.961 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=01:04 IST
=> Training   12.03% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.392 Loss=1.302 Prec@1=68.182 Prec@5=87.983 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=01:04 IST
=> Training   16.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.392 Loss=1.302 Prec@1=68.182 Prec@5=87.983 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:04 IST
=> Training   16.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.392 Loss=1.302 Prec@1=68.182 Prec@5=87.983 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:05 IST
=> Training   16.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.567 DataTime=0.393 Loss=1.303 Prec@1=68.181 Prec@5=87.964 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:05 IST
=> Training   20.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.567 DataTime=0.393 Loss=1.303 Prec@1=68.181 Prec@5=87.964 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=01:05 IST
=> Training   20.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.567 DataTime=0.393 Loss=1.303 Prec@1=68.181 Prec@5=87.964 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=01:06 IST
=> Training   20.02% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.304 Prec@1=68.174 Prec@5=87.951 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=01:06 IST
=> Training   24.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.304 Prec@1=68.174 Prec@5=87.951 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=01:06 IST
=> Training   24.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.304 Prec@1=68.174 Prec@5=87.951 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=01:07 IST
=> Training   24.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.390 Loss=1.306 Prec@1=68.082 Prec@5=87.948 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=01:07 IST
=> Training   28.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.390 Loss=1.306 Prec@1=68.082 Prec@5=87.948 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=01:07 IST
=> Training   28.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.564 DataTime=0.390 Loss=1.306 Prec@1=68.082 Prec@5=87.948 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=01:08 IST
=> Training   28.01% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.308 Prec@1=68.034 Prec@5=87.905 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=01:08 IST
=> Training   32.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.308 Prec@1=68.034 Prec@5=87.905 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=01:08 IST
=> Training   32.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.308 Prec@1=68.034 Prec@5=87.905 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=01:09 IST
=> Training   32.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.563 DataTime=0.389 Loss=1.309 Prec@1=67.998 Prec@5=87.886 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=01:09 IST
=> Training   36.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.563 DataTime=0.389 Loss=1.309 Prec@1=67.998 Prec@5=87.886 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=01:09 IST
=> Training   36.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.563 DataTime=0.389 Loss=1.309 Prec@1=67.998 Prec@5=87.886 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=01:10 IST
=> Training   36.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.312 Prec@1=67.923 Prec@5=87.825 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=01:10 IST
=> Training   39.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.312 Prec@1=67.923 Prec@5=87.825 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=01:10 IST
=> Training   39.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.312 Prec@1=67.923 Prec@5=87.825 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=01:11 IST
=> Training   39.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.386 Loss=1.314 Prec@1=67.889 Prec@5=87.799 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=01:11 IST
=> Training   43.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.386 Loss=1.314 Prec@1=67.889 Prec@5=87.799 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=01:11 IST
=> Training   43.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.386 Loss=1.314 Prec@1=67.889 Prec@5=87.799 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=01:12 IST
=> Training   43.99% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.858 Prec@5=87.794 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=01:12 IST
=> Training   47.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.858 Prec@5=87.794 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=01:12 IST
=> Training   47.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.562 DataTime=0.388 Loss=1.314 Prec@1=67.858 Prec@5=87.794 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=01:13 IST
=> Training   47.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.316 Prec@1=67.789 Prec@5=87.783 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=01:13 IST
=> Training   51.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.316 Prec@1=67.789 Prec@5=87.783 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=01:13 IST
=> Training   51.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.561 DataTime=0.387 Loss=1.316 Prec@1=67.789 Prec@5=87.783 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=01:14 IST
=> Training   51.98% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.318 Prec@1=67.760 Prec@5=87.761 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=01:14 IST
=> Training   55.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.318 Prec@1=67.760 Prec@5=87.761 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=01:14 IST
=> Training   55.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.318 Prec@1=67.760 Prec@5=87.761 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=01:15 IST
=> Training   55.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.319 Prec@1=67.712 Prec@5=87.740 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=01:15 IST
=> Training   59.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.319 Prec@1=67.712 Prec@5=87.740 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:15 IST
=> Training   59.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.386 Loss=1.319 Prec@1=67.712 Prec@5=87.740 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:16 IST
=> Training   59.97% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.321 Prec@1=67.681 Prec@5=87.723 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:16 IST
=> Training   63.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.321 Prec@1=67.681 Prec@5=87.723 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=01:16 IST
=> Training   63.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.321 Prec@1=67.681 Prec@5=87.723 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=01:17 IST
=> Training   63.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.387 Loss=1.322 Prec@1=67.650 Prec@5=87.709 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=01:17 IST
=> Training   67.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.387 Loss=1.322 Prec@1=67.650 Prec@5=87.709 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=01:17 IST
=> Training   67.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.387 Loss=1.322 Prec@1=67.650 Prec@5=87.709 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=01:17 IST
=> Training   67.96% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.323 Prec@1=67.633 Prec@5=87.702 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=01:17 IST
=> Training   71.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.323 Prec@1=67.633 Prec@5=87.702 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:17 IST
=> Training   71.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.386 Loss=1.323 Prec@1=67.633 Prec@5=87.702 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:18 IST
=> Training   71.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.324 Prec@1=67.604 Prec@5=87.676 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:18 IST
=> Training   75.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.324 Prec@1=67.604 Prec@5=87.676 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=01:18 IST
=> Training   75.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.324 Prec@1=67.604 Prec@5=87.676 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=01:19 IST
=> Training   75.95% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.325 Prec@1=67.592 Prec@5=87.664 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=01:19 IST
=> Training   79.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.325 Prec@1=67.592 Prec@5=87.664 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=01:19 IST
=> Training   79.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.325 Prec@1=67.592 Prec@5=87.664 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=01:20 IST
=> Training   79.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.327 Prec@1=67.555 Prec@5=87.640 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=01:20 IST
=> Training   83.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.327 Prec@1=67.555 Prec@5=87.640 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=01:20 IST
=> Training   83.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.327 Prec@1=67.555 Prec@5=87.640 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=01:21 IST
=> Training   83.94% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.328 Prec@1=67.529 Prec@5=87.617 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=01:21 IST
=> Training   87.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.328 Prec@1=67.529 Prec@5=87.617 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=01:21 IST
=> Training   87.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.560 DataTime=0.385 Loss=1.328 Prec@1=67.529 Prec@5=87.617 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=01:22 IST
=> Training   87.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.384 Loss=1.329 Prec@1=67.505 Prec@5=87.609 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=01:22 IST
=> Training   91.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.384 Loss=1.329 Prec@1=67.505 Prec@5=87.609 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=01:22 IST
=> Training   91.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.559 DataTime=0.384 Loss=1.329 Prec@1=67.505 Prec@5=87.609 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=01:23 IST
=> Training   91.93% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.477 Prec@5=87.593 rate=1.79 Hz, eta=0:01:52, total=0:21:21, wall=01:23 IST
=> Training   95.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.477 Prec@5=87.593 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=01:23 IST
=> Training   95.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.477 Prec@5=87.593 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=01:24 IST
=> Training   95.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.467 Prec@5=87.586 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=01:24 IST
=> Training   99.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.467 Prec@5=87.586 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=01:24 IST
=> Training   99.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.467 Prec@5=87.586 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=01:24 IST
=> Training   99.92% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.467 Prec@5=87.587 rate=1.80 Hz, eta=0:00:01, total=0:23:10, wall=01:24 IST
=> Training   100.00% of 1x2503...Epoch=64/150 LR=0.0624 Time=0.558 DataTime=0.383 Loss=1.331 Prec@1=67.467 Prec@5=87.587 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=01:24 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:24 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:24 IST
=> Validation 0.00% of 1x98...Epoch=64/150 LR=0.0624 Time=7.227 Loss=0.928 Prec@1=75.977 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=01:24 IST
=> Validation 1.02% of 1x98...Epoch=64/150 LR=0.0624 Time=7.227 Loss=0.928 Prec@1=75.977 Prec@5=92.773 rate=3195.47 Hz, eta=0:00:00, total=0:00:00, wall=01:24 IST
** Validation 1.02% of 1x98...Epoch=64/150 LR=0.0624 Time=7.227 Loss=0.928 Prec@1=75.977 Prec@5=92.773 rate=3195.47 Hz, eta=0:00:00, total=0:00:00, wall=01:25 IST
** Validation 1.02% of 1x98...Epoch=64/150 LR=0.0624 Time=0.644 Loss=1.452 Prec@1=65.146 Prec@5=86.412 rate=3195.47 Hz, eta=0:00:00, total=0:00:00, wall=01:25 IST
** Validation 100.00% of 1x98...Epoch=64/150 LR=0.0624 Time=0.644 Loss=1.452 Prec@1=65.146 Prec@5=86.412 rate=1.75 Hz, eta=0:00:00, total=0:00:55, wall=01:25 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST
=> Training   0.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=4.577 DataTime=4.211 Loss=1.441 Prec@1=64.648 Prec@5=87.109 rate=0 Hz, eta=?, total=0:00:00, wall=01:25 IST
=> Training   0.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=4.577 DataTime=4.211 Loss=1.441 Prec@1=64.648 Prec@5=87.109 rate=3752.64 Hz, eta=0:00:00, total=0:00:00, wall=01:25 IST
=> Training   0.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=4.577 DataTime=4.211 Loss=1.441 Prec@1=64.648 Prec@5=87.109 rate=3752.64 Hz, eta=0:00:00, total=0:00:00, wall=01:26 IST
=> Training   0.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.591 DataTime=0.426 Loss=1.288 Prec@1=68.317 Prec@5=88.078 rate=3752.64 Hz, eta=0:00:00, total=0:00:00, wall=01:26 IST
=> Training   4.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.591 DataTime=0.426 Loss=1.288 Prec@1=68.317 Prec@5=88.078 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=01:26 IST
=> Training   4.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.591 DataTime=0.426 Loss=1.288 Prec@1=68.317 Prec@5=88.078 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=01:27 IST
=> Training   4.04% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.577 DataTime=0.408 Loss=1.288 Prec@1=68.409 Prec@5=88.160 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=01:27 IST
=> Training   8.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.577 DataTime=0.408 Loss=1.288 Prec@1=68.409 Prec@5=88.160 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=01:27 IST
=> Training   8.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.577 DataTime=0.408 Loss=1.288 Prec@1=68.409 Prec@5=88.160 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=01:28 IST
=> Training   8.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.395 Loss=1.289 Prec@1=68.389 Prec@5=88.162 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=01:28 IST
=> Training   12.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.395 Loss=1.289 Prec@1=68.389 Prec@5=88.162 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=01:28 IST
=> Training   12.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.395 Loss=1.289 Prec@1=68.389 Prec@5=88.162 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=01:29 IST
=> Training   12.03% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.397 Loss=1.289 Prec@1=68.322 Prec@5=88.193 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=01:29 IST
=> Training   16.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.397 Loss=1.289 Prec@1=68.322 Prec@5=88.193 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=01:29 IST
=> Training   16.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.566 DataTime=0.397 Loss=1.289 Prec@1=68.322 Prec@5=88.193 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=01:30 IST
=> Training   16.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.563 DataTime=0.395 Loss=1.292 Prec@1=68.242 Prec@5=88.129 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=01:30 IST
=> Training   20.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.563 DataTime=0.395 Loss=1.292 Prec@1=68.242 Prec@5=88.129 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=01:30 IST
=> Training   20.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.563 DataTime=0.395 Loss=1.292 Prec@1=68.242 Prec@5=88.129 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=01:31 IST
=> Training   20.02% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.391 Loss=1.295 Prec@1=68.194 Prec@5=88.058 rate=1.80 Hz, eta=0:18:29, total=0:04:37, wall=01:31 IST
=> Training   24.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.391 Loss=1.295 Prec@1=68.194 Prec@5=88.058 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=01:31 IST
=> Training   24.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.391 Loss=1.295 Prec@1=68.194 Prec@5=88.058 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=01:32 IST
=> Training   24.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.298 Prec@1=68.160 Prec@5=88.010 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=01:32 IST
=> Training   28.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.298 Prec@1=68.160 Prec@5=88.010 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=01:32 IST
=> Training   28.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.298 Prec@1=68.160 Prec@5=88.010 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=01:32 IST
=> Training   28.01% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.389 Loss=1.297 Prec@1=68.180 Prec@5=88.026 rate=1.81 Hz, eta=0:16:37, total=0:06:27, wall=01:32 IST
=> Training   32.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.389 Loss=1.297 Prec@1=68.180 Prec@5=88.026 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=01:32 IST
=> Training   32.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.389 Loss=1.297 Prec@1=68.180 Prec@5=88.026 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=01:33 IST
=> Training   32.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.301 Prec@1=68.109 Prec@5=87.970 rate=1.81 Hz, eta=0:15:41, total=0:07:23, wall=01:33 IST
=> Training   36.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.301 Prec@1=68.109 Prec@5=87.970 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=01:33 IST
=> Training   36.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.301 Prec@1=68.109 Prec@5=87.970 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=01:34 IST
=> Training   36.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.304 Prec@1=68.055 Prec@5=87.922 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=01:34 IST
=> Training   39.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.304 Prec@1=68.055 Prec@5=87.922 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=01:34 IST
=> Training   39.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.304 Prec@1=68.055 Prec@5=87.922 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=01:35 IST
=> Training   39.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.306 Prec@1=68.004 Prec@5=87.887 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=01:35 IST
=> Training   43.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.306 Prec@1=68.004 Prec@5=87.887 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=01:35 IST
=> Training   43.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.561 DataTime=0.390 Loss=1.306 Prec@1=68.004 Prec@5=87.887 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=01:36 IST
=> Training   43.99% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.308 Prec@1=67.956 Prec@5=87.867 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=01:36 IST
=> Training   47.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.308 Prec@1=67.956 Prec@5=87.867 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=01:36 IST
=> Training   47.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.389 Loss=1.308 Prec@1=67.956 Prec@5=87.867 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=01:37 IST
=> Training   47.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.309 Prec@1=67.907 Prec@5=87.852 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=01:37 IST
=> Training   51.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.309 Prec@1=67.907 Prec@5=87.852 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=01:37 IST
=> Training   51.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.309 Prec@1=67.907 Prec@5=87.852 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=01:38 IST
=> Training   51.98% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.312 Prec@1=67.862 Prec@5=87.826 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=01:38 IST
=> Training   55.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.312 Prec@1=67.862 Prec@5=87.826 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=01:38 IST
=> Training   55.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.312 Prec@1=67.862 Prec@5=87.826 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=01:39 IST
=> Training   55.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.313 Prec@1=67.855 Prec@5=87.804 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=01:39 IST
=> Training   59.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.313 Prec@1=67.855 Prec@5=87.804 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:39 IST
=> Training   59.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.313 Prec@1=67.855 Prec@5=87.804 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:40 IST
=> Training   59.97% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.314 Prec@1=67.820 Prec@5=87.788 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=01:40 IST
=> Training   63.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.314 Prec@1=67.820 Prec@5=87.788 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=01:40 IST
=> Training   63.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.314 Prec@1=67.820 Prec@5=87.788 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=01:41 IST
=> Training   63.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.314 Prec@1=67.817 Prec@5=87.790 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=01:41 IST
=> Training   67.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.314 Prec@1=67.817 Prec@5=87.790 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=01:41 IST
=> Training   67.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.314 Prec@1=67.817 Prec@5=87.790 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=01:42 IST
=> Training   67.96% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.790 Prec@5=87.772 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=01:42 IST
=> Training   71.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.790 Prec@5=87.772 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:42 IST
=> Training   71.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.790 Prec@5=87.772 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:43 IST
=> Training   71.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.558 DataTime=0.386 Loss=1.317 Prec@1=67.776 Prec@5=87.761 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=01:43 IST
=> Training   75.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.558 DataTime=0.386 Loss=1.317 Prec@1=67.776 Prec@5=87.761 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=01:43 IST
=> Training   75.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.558 DataTime=0.386 Loss=1.317 Prec@1=67.776 Prec@5=87.761 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=01:44 IST
=> Training   75.95% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.747 Prec@5=87.742 rate=1.80 Hz, eta=0:05:34, total=0:17:36, wall=01:44 IST
=> Training   79.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.747 Prec@5=87.742 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=01:44 IST
=> Training   79.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.747 Prec@5=87.742 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=01:45 IST
=> Training   79.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.743 Prec@5=87.739 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=01:45 IST
=> Training   83.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.743 Prec@5=87.739 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=01:45 IST
=> Training   83.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.318 Prec@1=67.743 Prec@5=87.739 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=01:46 IST
=> Training   83.94% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.320 Prec@1=67.718 Prec@5=87.726 rate=1.79 Hz, eta=0:03:43, total=0:19:30, wall=01:46 IST
=> Training   87.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.320 Prec@1=67.718 Prec@5=87.726 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=01:46 IST
=> Training   87.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.560 DataTime=0.388 Loss=1.320 Prec@1=67.718 Prec@5=87.726 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=01:46 IST
=> Training   87.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.388 Loss=1.321 Prec@1=67.694 Prec@5=87.711 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=01:46 IST
=> Training   91.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.388 Loss=1.321 Prec@1=67.694 Prec@5=87.711 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=01:46 IST
=> Training   91.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.388 Loss=1.321 Prec@1=67.694 Prec@5=87.711 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=01:47 IST
=> Training   91.93% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.322 Prec@1=67.653 Prec@5=87.690 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=01:47 IST
=> Training   95.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.322 Prec@1=67.653 Prec@5=87.690 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=01:47 IST
=> Training   95.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.322 Prec@1=67.653 Prec@5=87.690 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=01:48 IST
=> Training   95.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.324 Prec@1=67.633 Prec@5=87.671 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=01:48 IST
=> Training   99.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.324 Prec@1=67.633 Prec@5=87.671 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=01:48 IST
=> Training   99.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.324 Prec@1=67.633 Prec@5=87.671 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=01:48 IST
=> Training   99.92% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.323 Prec@1=67.635 Prec@5=87.672 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=01:48 IST
=> Training   100.00% of 1x2503...Epoch=65/150 LR=0.0614 Time=0.559 DataTime=0.387 Loss=1.323 Prec@1=67.635 Prec@5=87.672 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=01:48 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> Validation 0.00% of 1x98...Epoch=65/150 LR=0.0614 Time=7.006 Loss=0.788 Prec@1=79.688 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=01:48 IST
=> Validation 1.02% of 1x98...Epoch=65/150 LR=0.0614 Time=7.006 Loss=0.788 Prec@1=79.688 Prec@5=94.727 rate=7525.14 Hz, eta=0:00:00, total=0:00:00, wall=01:48 IST
** Validation 1.02% of 1x98...Epoch=65/150 LR=0.0614 Time=7.006 Loss=0.788 Prec@1=79.688 Prec@5=94.727 rate=7525.14 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** Validation 1.02% of 1x98...Epoch=65/150 LR=0.0614 Time=0.631 Loss=1.446 Prec@1=65.232 Prec@5=86.626 rate=7525.14 Hz, eta=0:00:00, total=0:00:00, wall=01:49 IST
** Validation 100.00% of 1x98...Epoch=65/150 LR=0.0614 Time=0.631 Loss=1.446 Prec@1=65.232 Prec@5=86.626 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=01:49 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> Training   0.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=4.661 DataTime=4.436 Loss=1.302 Prec@1=68.164 Prec@5=86.914 rate=0 Hz, eta=?, total=0:00:00, wall=01:49 IST
=> Training   0.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=4.661 DataTime=4.436 Loss=1.302 Prec@1=68.164 Prec@5=86.914 rate=645.92 Hz, eta=0:00:03, total=0:00:00, wall=01:49 IST
=> Training   0.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=4.661 DataTime=4.436 Loss=1.302 Prec@1=68.164 Prec@5=86.914 rate=645.92 Hz, eta=0:00:03, total=0:00:00, wall=01:50 IST
=> Training   0.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.578 DataTime=0.409 Loss=1.284 Prec@1=68.487 Prec@5=88.268 rate=645.92 Hz, eta=0:00:03, total=0:00:00, wall=01:50 IST
=> Training   4.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.578 DataTime=0.409 Loss=1.284 Prec@1=68.487 Prec@5=88.268 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=01:50 IST
=> Training   4.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.578 DataTime=0.409 Loss=1.284 Prec@1=68.487 Prec@5=88.268 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=01:51 IST
=> Training   4.04% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.570 DataTime=0.398 Loss=1.284 Prec@1=68.576 Prec@5=88.282 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=01:51 IST
=> Training   8.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.570 DataTime=0.398 Loss=1.284 Prec@1=68.576 Prec@5=88.282 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=01:51 IST
=> Training   8.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.570 DataTime=0.398 Loss=1.284 Prec@1=68.576 Prec@5=88.282 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=01:52 IST
=> Training   8.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.567 DataTime=0.393 Loss=1.288 Prec@1=68.453 Prec@5=88.201 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=01:52 IST
=> Training   12.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.567 DataTime=0.393 Loss=1.288 Prec@1=68.453 Prec@5=88.201 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=01:52 IST
=> Training   12.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.567 DataTime=0.393 Loss=1.288 Prec@1=68.453 Prec@5=88.201 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=01:53 IST
=> Training   12.03% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.389 Loss=1.290 Prec@1=68.446 Prec@5=88.174 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=01:53 IST
=> Training   16.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.389 Loss=1.290 Prec@1=68.446 Prec@5=88.174 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:53 IST
=> Training   16.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.389 Loss=1.290 Prec@1=68.446 Prec@5=88.174 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:54 IST
=> Training   16.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.390 Loss=1.290 Prec@1=68.376 Prec@5=88.166 rate=1.81 Hz, eta=0:19:20, total=0:03:41, wall=01:54 IST
=> Training   20.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.390 Loss=1.290 Prec@1=68.376 Prec@5=88.166 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=01:54 IST
=> Training   20.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.564 DataTime=0.390 Loss=1.290 Prec@1=68.376 Prec@5=88.166 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=01:55 IST
=> Training   20.02% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.343 Prec@5=88.123 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=01:55 IST
=> Training   24.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.343 Prec@5=88.123 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=01:55 IST
=> Training   24.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.343 Prec@5=88.123 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=01:56 IST
=> Training   24.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.294 Prec@1=68.301 Prec@5=88.122 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=01:56 IST
=> Training   28.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.294 Prec@1=68.301 Prec@5=88.122 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=01:56 IST
=> Training   28.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.294 Prec@1=68.301 Prec@5=88.122 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=01:57 IST
=> Training   28.01% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.254 Prec@5=88.102 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=01:57 IST
=> Training   32.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.254 Prec@5=88.102 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=01:57 IST
=> Training   32.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.254 Prec@5=88.102 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=01:58 IST
=> Training   32.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.183 Prec@5=88.068 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=01:58 IST
=> Training   36.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.183 Prec@5=88.068 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=01:58 IST
=> Training   36.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.183 Prec@5=88.068 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=01:59 IST
=> Training   36.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.142 Prec@5=88.040 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=01:59 IST
=> Training   39.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.142 Prec@5=88.040 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=01:59 IST
=> Training   39.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.142 Prec@5=88.040 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=02:00 IST
=> Training   39.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.091 Prec@5=88.022 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=02:00 IST
=> Training   43.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.091 Prec@5=88.022 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=02:00 IST
=> Training   43.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.091 Prec@5=88.022 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=02:01 IST
=> Training   43.99% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.303 Prec@1=68.043 Prec@5=87.976 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=02:01 IST
=> Training   47.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.303 Prec@1=68.043 Prec@5=87.976 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=02:01 IST
=> Training   47.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.303 Prec@1=68.043 Prec@5=87.976 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=02:01 IST
=> Training   47.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.305 Prec@1=68.003 Prec@5=87.946 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=02:01 IST
=> Training   51.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.305 Prec@1=68.003 Prec@5=87.946 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=02:01 IST
=> Training   51.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.305 Prec@1=68.003 Prec@5=87.946 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=02:02 IST
=> Training   51.98% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.306 Prec@1=67.968 Prec@5=87.937 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=02:02 IST
=> Training   55.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.306 Prec@1=67.968 Prec@5=87.937 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=02:02 IST
=> Training   55.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.306 Prec@1=67.968 Prec@5=87.937 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=02:03 IST
=> Training   55.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.307 Prec@1=67.945 Prec@5=87.911 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=02:03 IST
=> Training   59.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.307 Prec@1=67.945 Prec@5=87.911 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=02:03 IST
=> Training   59.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.387 Loss=1.307 Prec@1=67.945 Prec@5=87.911 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=02:04 IST
=> Training   59.97% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.308 Prec@1=67.946 Prec@5=87.891 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=02:04 IST
=> Training   63.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.308 Prec@1=67.946 Prec@5=87.891 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:04 IST
=> Training   63.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.308 Prec@1=67.946 Prec@5=87.891 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:05 IST
=> Training   63.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.309 Prec@1=67.919 Prec@5=87.873 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:05 IST
=> Training   67.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.309 Prec@1=67.919 Prec@5=87.873 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=02:05 IST
=> Training   67.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.309 Prec@1=67.919 Prec@5=87.873 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=02:06 IST
=> Training   67.96% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.312 Prec@1=67.873 Prec@5=87.841 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=02:06 IST
=> Training   71.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.312 Prec@1=67.873 Prec@5=87.841 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=02:06 IST
=> Training   71.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.312 Prec@1=67.873 Prec@5=87.841 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=02:07 IST
=> Training   71.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.313 Prec@1=67.853 Prec@5=87.829 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=02:07 IST
=> Training   75.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.313 Prec@1=67.853 Prec@5=87.829 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=02:07 IST
=> Training   75.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.313 Prec@1=67.853 Prec@5=87.829 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=02:08 IST
=> Training   75.95% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.314 Prec@1=67.836 Prec@5=87.814 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=02:08 IST
=> Training   79.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.314 Prec@1=67.836 Prec@5=87.814 rate=1.79 Hz, eta=0:04:39, total=0:18:34, wall=02:08 IST
=> Training   79.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.560 DataTime=0.386 Loss=1.314 Prec@1=67.836 Prec@5=87.814 rate=1.79 Hz, eta=0:04:39, total=0:18:34, wall=02:09 IST
=> Training   79.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.314 Prec@1=67.813 Prec@5=87.803 rate=1.79 Hz, eta=0:04:39, total=0:18:34, wall=02:09 IST
=> Training   83.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.314 Prec@1=67.813 Prec@5=87.803 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=02:09 IST
=> Training   83.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.314 Prec@1=67.813 Prec@5=87.803 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=02:10 IST
=> Training   83.94% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.793 Prec@5=87.787 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=02:10 IST
=> Training   87.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.793 Prec@5=87.787 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=02:10 IST
=> Training   87.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.316 Prec@1=67.793 Prec@5=87.787 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=02:11 IST
=> Training   87.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.317 Prec@1=67.764 Prec@5=87.768 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=02:11 IST
=> Training   91.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.317 Prec@1=67.764 Prec@5=87.768 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=02:11 IST
=> Training   91.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.317 Prec@1=67.764 Prec@5=87.768 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=02:12 IST
=> Training   91.93% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.319 Prec@1=67.730 Prec@5=87.753 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=02:12 IST
=> Training   95.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.319 Prec@1=67.730 Prec@5=87.753 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:12 IST
=> Training   95.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.386 Loss=1.319 Prec@1=67.730 Prec@5=87.753 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:13 IST
=> Training   95.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.319 Prec@1=67.705 Prec@5=87.738 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:13 IST
=> Training   99.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.319 Prec@1=67.705 Prec@5=87.738 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=02:13 IST
=> Training   99.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.319 Prec@1=67.705 Prec@5=87.738 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=02:13 IST
=> Training   99.92% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.319 Prec@1=67.705 Prec@5=87.737 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=02:13 IST
=> Training   100.00% of 1x2503...Epoch=66/150 LR=0.0604 Time=0.559 DataTime=0.385 Loss=1.319 Prec@1=67.705 Prec@5=87.737 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=02:13 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST
=> Validation 0.00% of 1x98...Epoch=66/150 LR=0.0604 Time=6.780 Loss=0.974 Prec@1=76.172 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=02:13 IST
=> Validation 1.02% of 1x98...Epoch=66/150 LR=0.0604 Time=6.780 Loss=0.974 Prec@1=76.172 Prec@5=92.383 rate=2159.16 Hz, eta=0:00:00, total=0:00:00, wall=02:13 IST
** Validation 1.02% of 1x98...Epoch=66/150 LR=0.0604 Time=6.780 Loss=0.974 Prec@1=76.172 Prec@5=92.383 rate=2159.16 Hz, eta=0:00:00, total=0:00:00, wall=02:14 IST
** Validation 1.02% of 1x98...Epoch=66/150 LR=0.0604 Time=0.635 Loss=1.453 Prec@1=64.954 Prec@5=86.374 rate=2159.16 Hz, eta=0:00:00, total=0:00:00, wall=02:14 IST
** Validation 100.00% of 1x98...Epoch=66/150 LR=0.0604 Time=0.635 Loss=1.453 Prec@1=64.954 Prec@5=86.374 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=02:14 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:14 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:14 IST
=> Training   0.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=4.620 DataTime=3.968 Loss=1.090 Prec@1=72.852 Prec@5=89.844 rate=0 Hz, eta=?, total=0:00:00, wall=02:14 IST
=> Training   0.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=4.620 DataTime=3.968 Loss=1.090 Prec@1=72.852 Prec@5=89.844 rate=2264.11 Hz, eta=0:00:01, total=0:00:00, wall=02:14 IST
=> Training   0.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=4.620 DataTime=3.968 Loss=1.090 Prec@1=72.852 Prec@5=89.844 rate=2264.11 Hz, eta=0:00:01, total=0:00:00, wall=02:15 IST
=> Training   0.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.575 DataTime=0.402 Loss=1.271 Prec@1=68.760 Prec@5=88.347 rate=2264.11 Hz, eta=0:00:01, total=0:00:00, wall=02:15 IST
=> Training   4.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.575 DataTime=0.402 Loss=1.271 Prec@1=68.760 Prec@5=88.347 rate=1.89 Hz, eta=0:21:12, total=0:00:53, wall=02:15 IST
=> Training   4.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.575 DataTime=0.402 Loss=1.271 Prec@1=68.760 Prec@5=88.347 rate=1.89 Hz, eta=0:21:12, total=0:00:53, wall=02:16 IST
=> Training   4.04% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.570 DataTime=0.401 Loss=1.269 Prec@1=68.844 Prec@5=88.378 rate=1.89 Hz, eta=0:21:12, total=0:00:53, wall=02:16 IST
=> Training   8.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.570 DataTime=0.401 Loss=1.269 Prec@1=68.844 Prec@5=88.378 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=02:16 IST
=> Training   8.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.570 DataTime=0.401 Loss=1.269 Prec@1=68.844 Prec@5=88.378 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=02:17 IST
=> Training   8.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.568 DataTime=0.400 Loss=1.272 Prec@1=68.779 Prec@5=88.351 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=02:17 IST
=> Training   12.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.568 DataTime=0.400 Loss=1.272 Prec@1=68.779 Prec@5=88.351 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=02:17 IST
=> Training   12.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.568 DataTime=0.400 Loss=1.272 Prec@1=68.779 Prec@5=88.351 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=02:17 IST
=> Training   12.03% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.397 Loss=1.272 Prec@1=68.772 Prec@5=88.398 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=02:17 IST
=> Training   16.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.397 Loss=1.272 Prec@1=68.772 Prec@5=88.398 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=02:17 IST
=> Training   16.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.397 Loss=1.272 Prec@1=68.772 Prec@5=88.398 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=02:18 IST
=> Training   16.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.396 Loss=1.275 Prec@1=68.682 Prec@5=88.360 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=02:18 IST
=> Training   20.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.396 Loss=1.275 Prec@1=68.682 Prec@5=88.360 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:18 IST
=> Training   20.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.565 DataTime=0.396 Loss=1.275 Prec@1=68.682 Prec@5=88.360 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:19 IST
=> Training   20.02% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.393 Loss=1.278 Prec@1=68.616 Prec@5=88.307 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:19 IST
=> Training   24.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.393 Loss=1.278 Prec@1=68.616 Prec@5=88.307 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:19 IST
=> Training   24.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.393 Loss=1.278 Prec@1=68.616 Prec@5=88.307 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:20 IST
=> Training   24.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.391 Loss=1.281 Prec@1=68.568 Prec@5=88.287 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:20 IST
=> Training   28.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.391 Loss=1.281 Prec@1=68.568 Prec@5=88.287 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=02:20 IST
=> Training   28.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.391 Loss=1.281 Prec@1=68.568 Prec@5=88.287 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=02:21 IST
=> Training   28.01% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.284 Prec@1=68.504 Prec@5=88.247 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=02:21 IST
=> Training   32.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.284 Prec@1=68.504 Prec@5=88.247 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:21 IST
=> Training   32.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.284 Prec@1=68.504 Prec@5=88.247 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:22 IST
=> Training   32.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.287 Prec@1=68.439 Prec@5=88.206 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:22 IST
=> Training   36.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.287 Prec@1=68.439 Prec@5=88.206 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:22 IST
=> Training   36.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.287 Prec@1=68.439 Prec@5=88.206 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:23 IST
=> Training   36.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.290 Prec@1=68.375 Prec@5=88.162 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:23 IST
=> Training   39.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.290 Prec@1=68.375 Prec@5=88.162 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=02:23 IST
=> Training   39.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.290 Prec@1=68.375 Prec@5=88.162 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=02:24 IST
=> Training   39.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.292 Prec@1=68.334 Prec@5=88.142 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=02:24 IST
=> Training   43.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.292 Prec@1=68.334 Prec@5=88.142 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=02:24 IST
=> Training   43.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.392 Loss=1.292 Prec@1=68.334 Prec@5=88.142 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=02:25 IST
=> Training   43.99% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.391 Loss=1.294 Prec@1=68.302 Prec@5=88.102 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=02:25 IST
=> Training   47.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.391 Loss=1.294 Prec@1=68.302 Prec@5=88.102 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=02:25 IST
=> Training   47.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.391 Loss=1.294 Prec@1=68.302 Prec@5=88.102 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=02:26 IST
=> Training   47.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.295 Prec@1=68.258 Prec@5=88.079 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=02:26 IST
=> Training   51.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.295 Prec@1=68.258 Prec@5=88.079 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:26 IST
=> Training   51.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.295 Prec@1=68.258 Prec@5=88.079 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:27 IST
=> Training   51.98% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.297 Prec@1=68.221 Prec@5=88.051 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:27 IST
=> Training   55.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.297 Prec@1=68.221 Prec@5=88.051 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=02:27 IST
=> Training   55.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.297 Prec@1=68.221 Prec@5=88.051 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=02:28 IST
=> Training   55.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.391 Loss=1.299 Prec@1=68.195 Prec@5=88.025 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=02:28 IST
=> Training   59.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.391 Loss=1.299 Prec@1=68.195 Prec@5=88.025 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:28 IST
=> Training   59.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.563 DataTime=0.391 Loss=1.299 Prec@1=68.195 Prec@5=88.025 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:29 IST
=> Training   59.97% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.300 Prec@1=68.144 Prec@5=88.000 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:29 IST
=> Training   63.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.300 Prec@1=68.144 Prec@5=88.000 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=02:29 IST
=> Training   63.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.300 Prec@1=68.144 Prec@5=88.000 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=02:30 IST
=> Training   63.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.301 Prec@1=68.117 Prec@5=87.993 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=02:30 IST
=> Training   67.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.301 Prec@1=68.117 Prec@5=87.993 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:30 IST
=> Training   67.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.301 Prec@1=68.117 Prec@5=87.993 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:31 IST
=> Training   67.96% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.302 Prec@1=68.097 Prec@5=87.987 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:31 IST
=> Training   71.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.302 Prec@1=68.097 Prec@5=87.987 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:31 IST
=> Training   71.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.302 Prec@1=68.097 Prec@5=87.987 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:31 IST
=> Training   71.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.304 Prec@1=68.059 Prec@5=87.956 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:31 IST
=> Training   75.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.304 Prec@1=68.059 Prec@5=87.956 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:31 IST
=> Training   75.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.304 Prec@1=68.059 Prec@5=87.956 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:32 IST
=> Training   75.95% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.305 Prec@1=68.039 Prec@5=87.947 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:32 IST
=> Training   79.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.305 Prec@1=68.039 Prec@5=87.947 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=02:32 IST
=> Training   79.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.305 Prec@1=68.039 Prec@5=87.947 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=02:33 IST
=> Training   79.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.307 Prec@1=67.999 Prec@5=87.924 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=02:33 IST
=> Training   83.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.307 Prec@1=67.999 Prec@5=87.924 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=02:33 IST
=> Training   83.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.307 Prec@1=67.999 Prec@5=87.924 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=02:34 IST
=> Training   83.94% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.308 Prec@1=67.950 Prec@5=87.903 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=02:34 IST
=> Training   87.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.308 Prec@1=67.950 Prec@5=87.903 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=02:34 IST
=> Training   87.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.390 Loss=1.308 Prec@1=67.950 Prec@5=87.903 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=02:35 IST
=> Training   87.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.309 Prec@1=67.928 Prec@5=87.884 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=02:35 IST
=> Training   91.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.309 Prec@1=67.928 Prec@5=87.884 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=02:35 IST
=> Training   91.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.309 Prec@1=67.928 Prec@5=87.884 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=02:36 IST
=> Training   91.93% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.311 Prec@1=67.906 Prec@5=87.866 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=02:36 IST
=> Training   95.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.311 Prec@1=67.906 Prec@5=87.866 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=02:36 IST
=> Training   95.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.562 DataTime=0.390 Loss=1.311 Prec@1=67.906 Prec@5=87.866 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=02:37 IST
=> Training   95.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.311 Prec@1=67.883 Prec@5=87.857 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=02:37 IST
=> Training   99.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.311 Prec@1=67.883 Prec@5=87.857 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=02:37 IST
=> Training   99.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.311 Prec@1=67.883 Prec@5=87.857 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=02:37 IST
=> Training   99.92% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.311 Prec@1=67.881 Prec@5=87.855 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=02:37 IST
=> Training   100.00% of 1x2503...Epoch=67/150 LR=0.0594 Time=0.561 DataTime=0.389 Loss=1.311 Prec@1=67.881 Prec@5=87.855 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=02:37 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:37 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:37 IST
=> Validation 0.00% of 1x98...Epoch=67/150 LR=0.0594 Time=6.826 Loss=0.886 Prec@1=77.734 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=02:37 IST
=> Validation 1.02% of 1x98...Epoch=67/150 LR=0.0594 Time=6.826 Loss=0.886 Prec@1=77.734 Prec@5=92.969 rate=5313.50 Hz, eta=0:00:00, total=0:00:00, wall=02:37 IST
** Validation 1.02% of 1x98...Epoch=67/150 LR=0.0594 Time=6.826 Loss=0.886 Prec@1=77.734 Prec@5=92.969 rate=5313.50 Hz, eta=0:00:00, total=0:00:00, wall=02:38 IST
** Validation 1.02% of 1x98...Epoch=67/150 LR=0.0594 Time=0.627 Loss=1.420 Prec@1=65.370 Prec@5=87.008 rate=5313.50 Hz, eta=0:00:00, total=0:00:00, wall=02:38 IST
** Validation 100.00% of 1x98...Epoch=67/150 LR=0.0594 Time=0.627 Loss=1.420 Prec@1=65.370 Prec@5=87.008 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=02:38 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:38 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:38 IST
=> Training   0.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=4.522 DataTime=4.186 Loss=1.306 Prec@1=69.922 Prec@5=86.523 rate=0 Hz, eta=?, total=0:00:00, wall=02:38 IST
=> Training   0.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=4.522 DataTime=4.186 Loss=1.306 Prec@1=69.922 Prec@5=86.523 rate=5812.03 Hz, eta=0:00:00, total=0:00:00, wall=02:38 IST
=> Training   0.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=4.522 DataTime=4.186 Loss=1.306 Prec@1=69.922 Prec@5=86.523 rate=5812.03 Hz, eta=0:00:00, total=0:00:00, wall=02:39 IST
=> Training   0.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.587 DataTime=0.417 Loss=1.281 Prec@1=68.431 Prec@5=88.260 rate=5812.03 Hz, eta=0:00:00, total=0:00:00, wall=02:39 IST
=> Training   4.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.587 DataTime=0.417 Loss=1.281 Prec@1=68.431 Prec@5=88.260 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:39 IST
=> Training   4.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.587 DataTime=0.417 Loss=1.281 Prec@1=68.431 Prec@5=88.260 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:40 IST
=> Training   4.04% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.573 DataTime=0.396 Loss=1.267 Prec@1=68.756 Prec@5=88.433 rate=1.84 Hz, eta=0:21:44, total=0:00:54, wall=02:40 IST
=> Training   8.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.573 DataTime=0.396 Loss=1.267 Prec@1=68.756 Prec@5=88.433 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=02:40 IST
=> Training   8.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.573 DataTime=0.396 Loss=1.267 Prec@1=68.756 Prec@5=88.433 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=02:41 IST
=> Training   8.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.566 DataTime=0.390 Loss=1.268 Prec@1=68.729 Prec@5=88.399 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=02:41 IST
=> Training   12.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.566 DataTime=0.390 Loss=1.268 Prec@1=68.729 Prec@5=88.399 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=02:41 IST
=> Training   12.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.566 DataTime=0.390 Loss=1.268 Prec@1=68.729 Prec@5=88.399 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=02:42 IST
=> Training   12.03% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.565 DataTime=0.390 Loss=1.271 Prec@1=68.670 Prec@5=88.318 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=02:42 IST
=> Training   16.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.565 DataTime=0.390 Loss=1.271 Prec@1=68.670 Prec@5=88.318 rate=1.81 Hz, eta=0:19:23, total=0:03:42, wall=02:42 IST
=> Training   16.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.565 DataTime=0.390 Loss=1.271 Prec@1=68.670 Prec@5=88.318 rate=1.81 Hz, eta=0:19:23, total=0:03:42, wall=02:43 IST
=> Training   16.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.390 Loss=1.274 Prec@1=68.619 Prec@5=88.279 rate=1.81 Hz, eta=0:19:23, total=0:03:42, wall=02:43 IST
=> Training   20.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.390 Loss=1.274 Prec@1=68.619 Prec@5=88.279 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=02:43 IST
=> Training   20.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.390 Loss=1.274 Prec@1=68.619 Prec@5=88.279 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=02:44 IST
=> Training   20.02% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.523 Prec@5=88.239 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=02:44 IST
=> Training   24.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.523 Prec@5=88.239 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:44 IST
=> Training   24.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.523 Prec@5=88.239 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:45 IST
=> Training   24.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.392 Loss=1.281 Prec@1=68.461 Prec@5=88.193 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=02:45 IST
=> Training   28.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.392 Loss=1.281 Prec@1=68.461 Prec@5=88.193 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=02:45 IST
=> Training   28.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.564 DataTime=0.392 Loss=1.281 Prec@1=68.461 Prec@5=88.193 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=02:46 IST
=> Training   28.01% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.283 Prec@1=68.391 Prec@5=88.172 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=02:46 IST
=> Training   32.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.283 Prec@1=68.391 Prec@5=88.172 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=02:46 IST
=> Training   32.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.563 DataTime=0.390 Loss=1.283 Prec@1=68.391 Prec@5=88.172 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=02:47 IST
=> Training   32.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.285 Prec@1=68.382 Prec@5=88.151 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=02:47 IST
=> Training   36.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.285 Prec@1=68.382 Prec@5=88.151 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:47 IST
=> Training   36.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.285 Prec@1=68.382 Prec@5=88.151 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:48 IST
=> Training   36.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.286 Prec@1=68.362 Prec@5=88.145 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=02:48 IST
=> Training   39.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.286 Prec@1=68.362 Prec@5=88.145 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=02:48 IST
=> Training   39.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.286 Prec@1=68.362 Prec@5=88.145 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=02:48 IST
=> Training   39.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.390 Loss=1.287 Prec@1=68.342 Prec@5=88.135 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=02:48 IST
=> Training   43.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.390 Loss=1.287 Prec@1=68.342 Prec@5=88.135 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=02:48 IST
=> Training   43.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.390 Loss=1.287 Prec@1=68.342 Prec@5=88.135 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=02:49 IST
=> Training   43.99% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.289 Prec@1=68.297 Prec@5=88.111 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=02:49 IST
=> Training   47.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.289 Prec@1=68.297 Prec@5=88.111 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=02:49 IST
=> Training   47.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.389 Loss=1.289 Prec@1=68.297 Prec@5=88.111 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=02:50 IST
=> Training   47.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.290 Prec@1=68.275 Prec@5=88.089 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=02:50 IST
=> Training   51.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.290 Prec@1=68.275 Prec@5=88.089 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=02:50 IST
=> Training   51.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.290 Prec@1=68.275 Prec@5=88.089 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=02:51 IST
=> Training   51.98% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.235 Prec@5=88.055 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=02:51 IST
=> Training   55.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.235 Prec@5=88.055 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:51 IST
=> Training   55.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.235 Prec@5=88.055 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:52 IST
=> Training   55.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.293 Prec@1=68.223 Prec@5=88.053 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:52 IST
=> Training   59.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.293 Prec@1=68.223 Prec@5=88.053 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:52 IST
=> Training   59.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.389 Loss=1.293 Prec@1=68.223 Prec@5=88.053 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:53 IST
=> Training   59.97% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.295 Prec@1=68.199 Prec@5=88.037 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=02:53 IST
=> Training   63.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.295 Prec@1=68.199 Prec@5=88.037 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=02:53 IST
=> Training   63.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.295 Prec@1=68.199 Prec@5=88.037 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=02:54 IST
=> Training   63.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.169 Prec@5=88.020 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=02:54 IST
=> Training   67.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.169 Prec@5=88.020 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:54 IST
=> Training   67.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.562 DataTime=0.388 Loss=1.296 Prec@1=68.169 Prec@5=88.020 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:55 IST
=> Training   67.96% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.131 Prec@5=87.999 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=02:55 IST
=> Training   71.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.131 Prec@5=87.999 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:55 IST
=> Training   71.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.131 Prec@5=87.999 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:56 IST
=> Training   71.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.117 Prec@5=87.999 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=02:56 IST
=> Training   75.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.117 Prec@5=87.999 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:56 IST
=> Training   75.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.298 Prec@1=68.117 Prec@5=87.999 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:57 IST
=> Training   75.95% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.091 Prec@5=87.992 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=02:57 IST
=> Training   79.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.091 Prec@5=87.992 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=02:57 IST
=> Training   79.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.299 Prec@1=68.091 Prec@5=87.992 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=02:58 IST
=> Training   79.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.300 Prec@1=68.079 Prec@5=87.981 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=02:58 IST
=> Training   83.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.300 Prec@1=68.079 Prec@5=87.981 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=02:58 IST
=> Training   83.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.300 Prec@1=68.079 Prec@5=87.981 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=02:59 IST
=> Training   83.94% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.047 Prec@5=87.962 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=02:59 IST
=> Training   87.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.047 Prec@5=87.962 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=02:59 IST
=> Training   87.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.301 Prec@1=68.047 Prec@5=87.962 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=03:00 IST
=> Training   87.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.387 Loss=1.303 Prec@1=68.023 Prec@5=87.942 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=03:00 IST
=> Training   91.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.387 Loss=1.303 Prec@1=68.023 Prec@5=87.942 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:00 IST
=> Training   91.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.387 Loss=1.303 Prec@1=68.023 Prec@5=87.942 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:01 IST
=> Training   91.93% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.305 Prec@1=67.994 Prec@5=87.922 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=03:01 IST
=> Training   95.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.305 Prec@1=67.994 Prec@5=87.922 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:01 IST
=> Training   95.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.561 DataTime=0.387 Loss=1.305 Prec@1=67.994 Prec@5=87.922 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:01 IST
=> Training   95.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.306 Prec@1=67.965 Prec@5=87.900 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:01 IST
=> Training   99.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.306 Prec@1=67.965 Prec@5=87.900 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:01 IST
=> Training   99.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.306 Prec@1=67.965 Prec@5=87.900 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:01 IST
=> Training   99.92% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.306 Prec@1=67.963 Prec@5=87.899 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:01 IST
=> Training   100.00% of 1x2503...Epoch=68/150 LR=0.0583 Time=0.560 DataTime=0.386 Loss=1.306 Prec@1=67.963 Prec@5=87.899 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=03:01 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:02 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:02 IST
=> Validation 0.00% of 1x98...Epoch=68/150 LR=0.0583 Time=6.916 Loss=0.909 Prec@1=76.172 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=03:02 IST
=> Validation 1.02% of 1x98...Epoch=68/150 LR=0.0583 Time=6.916 Loss=0.909 Prec@1=76.172 Prec@5=92.969 rate=3808.70 Hz, eta=0:00:00, total=0:00:00, wall=03:02 IST
** Validation 1.02% of 1x98...Epoch=68/150 LR=0.0583 Time=6.916 Loss=0.909 Prec@1=76.172 Prec@5=92.969 rate=3808.70 Hz, eta=0:00:00, total=0:00:00, wall=03:03 IST
** Validation 1.02% of 1x98...Epoch=68/150 LR=0.0583 Time=0.636 Loss=1.423 Prec@1=65.440 Prec@5=86.772 rate=3808.70 Hz, eta=0:00:00, total=0:00:00, wall=03:03 IST
** Validation 100.00% of 1x98...Epoch=68/150 LR=0.0583 Time=0.636 Loss=1.423 Prec@1=65.440 Prec@5=86.772 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=03:03 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:03 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:03 IST
=> Training   0.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=4.867 DataTime=4.423 Loss=1.227 Prec@1=68.359 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=03:03 IST
=> Training   0.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=4.867 DataTime=4.423 Loss=1.227 Prec@1=68.359 Prec@5=90.430 rate=4930.43 Hz, eta=0:00:00, total=0:00:00, wall=03:03 IST
=> Training   0.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=4.867 DataTime=4.423 Loss=1.227 Prec@1=68.359 Prec@5=90.430 rate=4930.43 Hz, eta=0:00:00, total=0:00:00, wall=03:04 IST
=> Training   0.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.581 DataTime=0.416 Loss=1.258 Prec@1=69.038 Prec@5=88.589 rate=4930.43 Hz, eta=0:00:00, total=0:00:00, wall=03:04 IST
=> Training   4.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.581 DataTime=0.416 Loss=1.258 Prec@1=69.038 Prec@5=88.589 rate=1.87 Hz, eta=0:21:21, total=0:00:53, wall=03:04 IST
=> Training   4.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.581 DataTime=0.416 Loss=1.258 Prec@1=69.038 Prec@5=88.589 rate=1.87 Hz, eta=0:21:21, total=0:00:53, wall=03:04 IST
=> Training   4.04% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.569 DataTime=0.402 Loss=1.260 Prec@1=68.988 Prec@5=88.640 rate=1.87 Hz, eta=0:21:21, total=0:00:53, wall=03:04 IST
=> Training   8.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.569 DataTime=0.402 Loss=1.260 Prec@1=68.988 Prec@5=88.640 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=03:04 IST
=> Training   8.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.569 DataTime=0.402 Loss=1.260 Prec@1=68.988 Prec@5=88.640 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=03:05 IST
=> Training   8.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.571 DataTime=0.401 Loss=1.266 Prec@1=68.943 Prec@5=88.468 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=03:05 IST
=> Training   12.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.571 DataTime=0.401 Loss=1.266 Prec@1=68.943 Prec@5=88.468 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=03:05 IST
=> Training   12.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.571 DataTime=0.401 Loss=1.266 Prec@1=68.943 Prec@5=88.468 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=03:06 IST
=> Training   12.03% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.568 DataTime=0.397 Loss=1.269 Prec@1=68.862 Prec@5=88.435 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=03:06 IST
=> Training   16.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.568 DataTime=0.397 Loss=1.269 Prec@1=68.862 Prec@5=88.435 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=03:06 IST
=> Training   16.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.568 DataTime=0.397 Loss=1.269 Prec@1=68.862 Prec@5=88.435 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=03:07 IST
=> Training   16.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.570 DataTime=0.397 Loss=1.266 Prec@1=68.920 Prec@5=88.483 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=03:07 IST
=> Training   20.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.570 DataTime=0.397 Loss=1.266 Prec@1=68.920 Prec@5=88.483 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=03:07 IST
=> Training   20.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.570 DataTime=0.397 Loss=1.266 Prec@1=68.920 Prec@5=88.483 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=03:08 IST
=> Training   20.02% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.566 DataTime=0.394 Loss=1.270 Prec@1=68.842 Prec@5=88.428 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=03:08 IST
=> Training   24.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.566 DataTime=0.394 Loss=1.270 Prec@1=68.842 Prec@5=88.428 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=03:08 IST
=> Training   24.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.566 DataTime=0.394 Loss=1.270 Prec@1=68.842 Prec@5=88.428 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=03:09 IST
=> Training   24.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.567 DataTime=0.394 Loss=1.270 Prec@1=68.794 Prec@5=88.425 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=03:09 IST
=> Training   28.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.567 DataTime=0.394 Loss=1.270 Prec@1=68.794 Prec@5=88.425 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=03:09 IST
=> Training   28.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.567 DataTime=0.394 Loss=1.270 Prec@1=68.794 Prec@5=88.425 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=03:10 IST
=> Training   28.01% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.565 DataTime=0.392 Loss=1.273 Prec@1=68.733 Prec@5=88.377 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=03:10 IST
=> Training   32.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.565 DataTime=0.392 Loss=1.273 Prec@1=68.733 Prec@5=88.377 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=03:10 IST
=> Training   32.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.565 DataTime=0.392 Loss=1.273 Prec@1=68.733 Prec@5=88.377 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=03:11 IST
=> Training   32.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.392 Loss=1.277 Prec@1=68.632 Prec@5=88.327 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=03:11 IST
=> Training   36.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.392 Loss=1.277 Prec@1=68.632 Prec@5=88.327 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=03:11 IST
=> Training   36.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.392 Loss=1.277 Prec@1=68.632 Prec@5=88.327 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=03:12 IST
=> Training   36.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.580 Prec@5=88.287 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=03:12 IST
=> Training   39.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.580 Prec@5=88.287 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=03:12 IST
=> Training   39.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.279 Prec@1=68.580 Prec@5=88.287 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=03:13 IST
=> Training   39.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.280 Prec@1=68.561 Prec@5=88.270 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=03:13 IST
=> Training   43.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.280 Prec@1=68.561 Prec@5=88.270 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=03:13 IST
=> Training   43.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.280 Prec@1=68.561 Prec@5=88.270 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=03:14 IST
=> Training   43.99% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.282 Prec@1=68.508 Prec@5=88.247 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=03:14 IST
=> Training   47.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.282 Prec@1=68.508 Prec@5=88.247 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:14 IST
=> Training   47.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.282 Prec@1=68.508 Prec@5=88.247 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:15 IST
=> Training   47.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.283 Prec@1=68.477 Prec@5=88.252 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:15 IST
=> Training   51.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.283 Prec@1=68.477 Prec@5=88.252 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=03:15 IST
=> Training   51.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.283 Prec@1=68.477 Prec@5=88.252 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=03:16 IST
=> Training   51.98% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.284 Prec@1=68.435 Prec@5=88.240 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=03:16 IST
=> Training   55.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.284 Prec@1=68.435 Prec@5=88.240 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:16 IST
=> Training   55.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.391 Loss=1.284 Prec@1=68.435 Prec@5=88.240 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:17 IST
=> Training   55.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.286 Prec@1=68.387 Prec@5=88.213 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:17 IST
=> Training   59.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.286 Prec@1=68.387 Prec@5=88.213 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=03:17 IST
=> Training   59.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.390 Loss=1.286 Prec@1=68.387 Prec@5=88.213 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=03:18 IST
=> Training   59.97% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.288 Prec@1=68.351 Prec@5=88.189 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=03:18 IST
=> Training   63.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.288 Prec@1=68.351 Prec@5=88.189 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=03:18 IST
=> Training   63.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.288 Prec@1=68.351 Prec@5=88.189 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=03:19 IST
=> Training   63.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.389 Loss=1.289 Prec@1=68.338 Prec@5=88.170 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=03:19 IST
=> Training   67.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.389 Loss=1.289 Prec@1=68.338 Prec@5=88.170 rate=1.78 Hz, eta=0:07:29, total=0:15:52, wall=03:19 IST
=> Training   67.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.389 Loss=1.289 Prec@1=68.338 Prec@5=88.170 rate=1.78 Hz, eta=0:07:29, total=0:15:52, wall=03:19 IST
=> Training   67.96% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.291 Prec@1=68.306 Prec@5=88.148 rate=1.78 Hz, eta=0:07:29, total=0:15:52, wall=03:19 IST
=> Training   71.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.291 Prec@1=68.306 Prec@5=88.148 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=03:19 IST
=> Training   71.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.564 DataTime=0.389 Loss=1.291 Prec@1=68.306 Prec@5=88.148 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=03:20 IST
=> Training   71.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.293 Prec@1=68.265 Prec@5=88.122 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=03:20 IST
=> Training   75.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.293 Prec@1=68.265 Prec@5=88.122 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=03:20 IST
=> Training   75.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.293 Prec@1=68.265 Prec@5=88.122 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=03:21 IST
=> Training   75.95% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.232 Prec@5=88.107 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=03:21 IST
=> Training   79.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.232 Prec@5=88.107 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=03:21 IST
=> Training   79.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.294 Prec@1=68.232 Prec@5=88.107 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=03:22 IST
=> Training   79.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.295 Prec@1=68.205 Prec@5=88.087 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=03:22 IST
=> Training   83.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.295 Prec@1=68.205 Prec@5=88.087 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:22 IST
=> Training   83.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.295 Prec@1=68.205 Prec@5=88.087 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:23 IST
=> Training   83.94% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.296 Prec@1=68.187 Prec@5=88.074 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=03:23 IST
=> Training   87.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.296 Prec@1=68.187 Prec@5=88.074 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=03:23 IST
=> Training   87.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.563 DataTime=0.388 Loss=1.296 Prec@1=68.187 Prec@5=88.074 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=03:24 IST
=> Training   87.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.156 Prec@5=88.056 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=03:24 IST
=> Training   91.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.156 Prec@5=88.056 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=03:24 IST
=> Training   91.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.156 Prec@5=88.056 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=03:25 IST
=> Training   91.93% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.146 Prec@5=88.046 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=03:25 IST
=> Training   95.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.146 Prec@5=88.046 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=03:25 IST
=> Training   95.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.298 Prec@1=68.146 Prec@5=88.046 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=03:26 IST
=> Training   95.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.299 Prec@1=68.130 Prec@5=88.025 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=03:26 IST
=> Training   99.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.299 Prec@1=68.130 Prec@5=88.025 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=03:26 IST
=> Training   99.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.562 DataTime=0.387 Loss=1.299 Prec@1=68.130 Prec@5=88.025 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=03:26 IST
=> Training   99.92% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.561 DataTime=0.387 Loss=1.299 Prec@1=68.130 Prec@5=88.025 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=03:26 IST
=> Training   100.00% of 1x2503...Epoch=69/150 LR=0.0573 Time=0.561 DataTime=0.387 Loss=1.299 Prec@1=68.130 Prec@5=88.025 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=03:26 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST
=> Validation 0.00% of 1x98...Epoch=69/150 LR=0.0573 Time=6.816 Loss=0.892 Prec@1=76.562 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=03:26 IST
=> Validation 1.02% of 1x98...Epoch=69/150 LR=0.0573 Time=6.816 Loss=0.892 Prec@1=76.562 Prec@5=93.750 rate=5426.67 Hz, eta=0:00:00, total=0:00:00, wall=03:26 IST
** Validation 1.02% of 1x98...Epoch=69/150 LR=0.0573 Time=6.816 Loss=0.892 Prec@1=76.562 Prec@5=93.750 rate=5426.67 Hz, eta=0:00:00, total=0:00:00, wall=03:27 IST
** Validation 1.02% of 1x98...Epoch=69/150 LR=0.0573 Time=0.647 Loss=1.504 Prec@1=63.922 Prec@5=85.862 rate=5426.67 Hz, eta=0:00:00, total=0:00:00, wall=03:27 IST
** Validation 100.00% of 1x98...Epoch=69/150 LR=0.0573 Time=0.647 Loss=1.504 Prec@1=63.922 Prec@5=85.862 rate=1.73 Hz, eta=0:00:00, total=0:00:56, wall=03:27 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST
=> Training   0.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=4.434 DataTime=4.039 Loss=1.313 Prec@1=67.188 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=03:27 IST
=> Training   0.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=4.434 DataTime=4.039 Loss=1.313 Prec@1=67.188 Prec@5=87.305 rate=873.93 Hz, eta=0:00:02, total=0:00:00, wall=03:27 IST
=> Training   0.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=4.434 DataTime=4.039 Loss=1.313 Prec@1=67.188 Prec@5=87.305 rate=873.93 Hz, eta=0:00:02, total=0:00:00, wall=03:28 IST
=> Training   0.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.590 DataTime=0.427 Loss=1.249 Prec@1=69.226 Prec@5=88.707 rate=873.93 Hz, eta=0:00:02, total=0:00:00, wall=03:28 IST
=> Training   4.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.590 DataTime=0.427 Loss=1.249 Prec@1=69.226 Prec@5=88.707 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=03:28 IST
=> Training   4.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.590 DataTime=0.427 Loss=1.249 Prec@1=69.226 Prec@5=88.707 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=03:29 IST
=> Training   4.04% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.583 DataTime=0.420 Loss=1.254 Prec@1=69.002 Prec@5=88.631 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=03:29 IST
=> Training   8.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.583 DataTime=0.420 Loss=1.254 Prec@1=69.002 Prec@5=88.631 rate=1.78 Hz, eta=0:21:30, total=0:01:52, wall=03:29 IST
=> Training   8.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.583 DataTime=0.420 Loss=1.254 Prec@1=69.002 Prec@5=88.631 rate=1.78 Hz, eta=0:21:30, total=0:01:52, wall=03:30 IST
=> Training   8.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.407 Loss=1.251 Prec@1=69.132 Prec@5=88.704 rate=1.78 Hz, eta=0:21:30, total=0:01:52, wall=03:30 IST
=> Training   12.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.407 Loss=1.251 Prec@1=69.132 Prec@5=88.704 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=03:30 IST
=> Training   12.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.407 Loss=1.251 Prec@1=69.132 Prec@5=88.704 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=03:31 IST
=> Training   12.03% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.404 Loss=1.250 Prec@1=69.136 Prec@5=88.686 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=03:31 IST
=> Training   16.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.404 Loss=1.250 Prec@1=69.136 Prec@5=88.686 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=03:31 IST
=> Training   16.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.570 DataTime=0.404 Loss=1.250 Prec@1=69.136 Prec@5=88.686 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=03:32 IST
=> Training   16.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.401 Loss=1.253 Prec@1=69.069 Prec@5=88.628 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=03:32 IST
=> Training   20.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.401 Loss=1.253 Prec@1=69.069 Prec@5=88.628 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=03:32 IST
=> Training   20.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.401 Loss=1.253 Prec@1=69.069 Prec@5=88.628 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=03:33 IST
=> Training   20.02% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.400 Loss=1.257 Prec@1=69.018 Prec@5=88.559 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=03:33 IST
=> Training   24.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.400 Loss=1.257 Prec@1=69.018 Prec@5=88.559 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=03:33 IST
=> Training   24.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.569 DataTime=0.400 Loss=1.257 Prec@1=69.018 Prec@5=88.559 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=03:34 IST
=> Training   24.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.567 DataTime=0.397 Loss=1.261 Prec@1=68.953 Prec@5=88.491 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=03:34 IST
=> Training   28.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.567 DataTime=0.397 Loss=1.261 Prec@1=68.953 Prec@5=88.491 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=03:34 IST
=> Training   28.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.567 DataTime=0.397 Loss=1.261 Prec@1=68.953 Prec@5=88.491 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=03:35 IST
=> Training   28.01% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.566 DataTime=0.394 Loss=1.265 Prec@1=68.864 Prec@5=88.447 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=03:35 IST
=> Training   32.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.566 DataTime=0.394 Loss=1.265 Prec@1=68.864 Prec@5=88.447 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=03:35 IST
=> Training   32.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.566 DataTime=0.394 Loss=1.265 Prec@1=68.864 Prec@5=88.447 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=03:36 IST
=> Training   32.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.565 DataTime=0.391 Loss=1.267 Prec@1=68.806 Prec@5=88.433 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=03:36 IST
=> Training   36.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.565 DataTime=0.391 Loss=1.267 Prec@1=68.806 Prec@5=88.433 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:36 IST
=> Training   36.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.565 DataTime=0.391 Loss=1.267 Prec@1=68.806 Prec@5=88.433 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:36 IST
=> Training   36.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.390 Loss=1.270 Prec@1=68.757 Prec@5=88.405 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:36 IST
=> Training   39.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.390 Loss=1.270 Prec@1=68.757 Prec@5=88.405 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=03:36 IST
=> Training   39.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.390 Loss=1.270 Prec@1=68.757 Prec@5=88.405 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=03:37 IST
=> Training   39.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.732 Prec@5=88.384 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=03:37 IST
=> Training   43.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.732 Prec@5=88.384 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=03:37 IST
=> Training   43.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.732 Prec@5=88.384 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=03:38 IST
=> Training   43.99% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.719 Prec@5=88.377 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=03:38 IST
=> Training   47.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.719 Prec@5=88.377 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=03:38 IST
=> Training   47.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.388 Loss=1.272 Prec@1=68.719 Prec@5=88.377 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=03:39 IST
=> Training   47.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.389 Loss=1.274 Prec@1=68.676 Prec@5=88.356 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=03:39 IST
=> Training   51.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.389 Loss=1.274 Prec@1=68.676 Prec@5=88.356 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:39 IST
=> Training   51.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.389 Loss=1.274 Prec@1=68.676 Prec@5=88.356 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:40 IST
=> Training   51.98% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.276 Prec@1=68.631 Prec@5=88.325 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=03:40 IST
=> Training   55.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.276 Prec@1=68.631 Prec@5=88.325 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:40 IST
=> Training   55.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.276 Prec@1=68.631 Prec@5=88.325 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:41 IST
=> Training   55.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.279 Prec@1=68.578 Prec@5=88.287 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=03:41 IST
=> Training   59.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.279 Prec@1=68.578 Prec@5=88.287 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=03:41 IST
=> Training   59.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.279 Prec@1=68.578 Prec@5=88.287 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=03:42 IST
=> Training   59.97% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.281 Prec@1=68.525 Prec@5=88.273 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=03:42 IST
=> Training   63.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.281 Prec@1=68.525 Prec@5=88.273 rate=1.78 Hz, eta=0:08:25, total=0:14:58, wall=03:42 IST
=> Training   63.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.564 DataTime=0.389 Loss=1.281 Prec@1=68.525 Prec@5=88.273 rate=1.78 Hz, eta=0:08:25, total=0:14:58, wall=03:43 IST
=> Training   63.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.281 Prec@1=68.507 Prec@5=88.274 rate=1.78 Hz, eta=0:08:25, total=0:14:58, wall=03:43 IST
=> Training   67.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.281 Prec@1=68.507 Prec@5=88.274 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=03:43 IST
=> Training   67.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.281 Prec@1=68.507 Prec@5=88.274 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=03:44 IST
=> Training   67.96% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.387 Loss=1.283 Prec@1=68.459 Prec@5=88.250 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=03:44 IST
=> Training   71.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.387 Loss=1.283 Prec@1=68.459 Prec@5=88.250 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:44 IST
=> Training   71.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.563 DataTime=0.387 Loss=1.283 Prec@1=68.459 Prec@5=88.250 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:45 IST
=> Training   71.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.386 Loss=1.284 Prec@1=68.437 Prec@5=88.230 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=03:45 IST
=> Training   75.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.386 Loss=1.284 Prec@1=68.437 Prec@5=88.230 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=03:45 IST
=> Training   75.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.386 Loss=1.284 Prec@1=68.437 Prec@5=88.230 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=03:46 IST
=> Training   75.95% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.286 Prec@1=68.396 Prec@5=88.204 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=03:46 IST
=> Training   79.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.286 Prec@1=68.396 Prec@5=88.204 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:46 IST
=> Training   79.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.387 Loss=1.286 Prec@1=68.396 Prec@5=88.204 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:47 IST
=> Training   79.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.387 Loss=1.287 Prec@1=68.374 Prec@5=88.183 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:47 IST
=> Training   83.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.387 Loss=1.287 Prec@1=68.374 Prec@5=88.183 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:47 IST
=> Training   83.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.387 Loss=1.287 Prec@1=68.374 Prec@5=88.183 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:48 IST
=> Training   83.94% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.288 Prec@1=68.366 Prec@5=88.172 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:48 IST
=> Training   87.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.288 Prec@1=68.366 Prec@5=88.172 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=03:48 IST
=> Training   87.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.288 Prec@1=68.366 Prec@5=88.172 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=03:49 IST
=> Training   87.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.289 Prec@1=68.342 Prec@5=88.161 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=03:49 IST
=> Training   91.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.289 Prec@1=68.342 Prec@5=88.161 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:49 IST
=> Training   91.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.289 Prec@1=68.342 Prec@5=88.161 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:50 IST
=> Training   91.93% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.290 Prec@1=68.306 Prec@5=88.141 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=03:50 IST
=> Training   95.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.290 Prec@1=68.306 Prec@5=88.141 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=03:50 IST
=> Training   95.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.562 DataTime=0.388 Loss=1.290 Prec@1=68.306 Prec@5=88.141 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=03:50 IST
=> Training   95.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.278 Prec@5=88.126 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=03:50 IST
=> Training   99.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.278 Prec@5=88.126 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:50 IST
=> Training   99.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.388 Loss=1.292 Prec@1=68.278 Prec@5=88.126 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:50 IST
=> Training   99.92% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.387 Loss=1.292 Prec@1=68.277 Prec@5=88.125 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=03:50 IST
=> Training   100.00% of 1x2503...Epoch=70/150 LR=0.0563 Time=0.561 DataTime=0.387 Loss=1.292 Prec@1=68.277 Prec@5=88.125 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=03:50 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST
=> Validation 0.00% of 1x98...Epoch=70/150 LR=0.0563 Time=6.685 Loss=1.016 Prec@1=75.977 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=03:51 IST
=> Validation 1.02% of 1x98...Epoch=70/150 LR=0.0563 Time=6.685 Loss=1.016 Prec@1=75.977 Prec@5=91.016 rate=6637.81 Hz, eta=0:00:00, total=0:00:00, wall=03:51 IST
** Validation 1.02% of 1x98...Epoch=70/150 LR=0.0563 Time=6.685 Loss=1.016 Prec@1=75.977 Prec@5=91.016 rate=6637.81 Hz, eta=0:00:00, total=0:00:00, wall=03:51 IST
** Validation 1.02% of 1x98...Epoch=70/150 LR=0.0563 Time=0.631 Loss=1.526 Prec@1=63.698 Prec@5=85.300 rate=6637.81 Hz, eta=0:00:00, total=0:00:00, wall=03:51 IST
** Validation 100.00% of 1x98...Epoch=70/150 LR=0.0563 Time=0.631 Loss=1.526 Prec@1=63.698 Prec@5=85.300 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=03:51 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> Training   0.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=5.440 DataTime=5.190 Loss=1.285 Prec@1=69.141 Prec@5=87.305 rate=0 Hz, eta=?, total=0:00:00, wall=03:52 IST
=> Training   0.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=5.440 DataTime=5.190 Loss=1.285 Prec@1=69.141 Prec@5=87.305 rate=8757.80 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST
=> Training   0.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=5.440 DataTime=5.190 Loss=1.285 Prec@1=69.141 Prec@5=87.305 rate=8757.80 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST
=> Training   0.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.587 DataTime=0.423 Loss=1.250 Prec@1=69.197 Prec@5=88.732 rate=8757.80 Hz, eta=0:00:00, total=0:00:00, wall=03:52 IST
=> Training   4.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.587 DataTime=0.423 Loss=1.250 Prec@1=69.197 Prec@5=88.732 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=03:52 IST
=> Training   4.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.587 DataTime=0.423 Loss=1.250 Prec@1=69.197 Prec@5=88.732 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=03:53 IST
=> Training   4.04% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.574 DataTime=0.407 Loss=1.246 Prec@1=69.301 Prec@5=88.736 rate=1.88 Hz, eta=0:21:20, total=0:00:53, wall=03:53 IST
=> Training   8.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.574 DataTime=0.407 Loss=1.246 Prec@1=69.301 Prec@5=88.736 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=03:53 IST
=> Training   8.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.574 DataTime=0.407 Loss=1.246 Prec@1=69.301 Prec@5=88.736 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=03:54 IST
=> Training   8.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.401 Loss=1.249 Prec@1=69.232 Prec@5=88.713 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=03:54 IST
=> Training   12.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.401 Loss=1.249 Prec@1=69.232 Prec@5=88.713 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=03:54 IST
=> Training   12.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.401 Loss=1.249 Prec@1=69.232 Prec@5=88.713 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=03:55 IST
=> Training   12.03% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.397 Loss=1.252 Prec@1=69.148 Prec@5=88.691 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=03:55 IST
=> Training   16.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.397 Loss=1.252 Prec@1=69.148 Prec@5=88.691 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=03:55 IST
=> Training   16.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.397 Loss=1.252 Prec@1=69.148 Prec@5=88.691 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=03:56 IST
=> Training   16.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.398 Loss=1.256 Prec@1=69.052 Prec@5=88.638 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=03:56 IST
=> Training   20.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.398 Loss=1.256 Prec@1=69.052 Prec@5=88.638 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=03:56 IST
=> Training   20.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.570 DataTime=0.398 Loss=1.256 Prec@1=69.052 Prec@5=88.638 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=03:57 IST
=> Training   20.02% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.394 Loss=1.257 Prec@1=69.009 Prec@5=88.622 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=03:57 IST
=> Training   24.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.394 Loss=1.257 Prec@1=69.009 Prec@5=88.622 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=03:57 IST
=> Training   24.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.567 DataTime=0.394 Loss=1.257 Prec@1=69.009 Prec@5=88.622 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=03:58 IST
=> Training   24.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.566 DataTime=0.391 Loss=1.260 Prec@1=68.924 Prec@5=88.572 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=03:58 IST
=> Training   28.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.566 DataTime=0.391 Loss=1.260 Prec@1=68.924 Prec@5=88.572 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=03:58 IST
=> Training   28.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.566 DataTime=0.391 Loss=1.260 Prec@1=68.924 Prec@5=88.572 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=03:59 IST
=> Training   28.01% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.564 DataTime=0.388 Loss=1.262 Prec@1=68.836 Prec@5=88.547 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=03:59 IST
=> Training   32.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.564 DataTime=0.388 Loss=1.262 Prec@1=68.836 Prec@5=88.547 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:59 IST
=> Training   32.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.564 DataTime=0.388 Loss=1.262 Prec@1=68.836 Prec@5=88.547 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:00 IST
=> Training   32.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.263 Prec@1=68.859 Prec@5=88.537 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:00 IST
=> Training   36.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.263 Prec@1=68.859 Prec@5=88.537 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=04:00 IST
=> Training   36.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.263 Prec@1=68.859 Prec@5=88.537 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=04:01 IST
=> Training   36.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.264 Prec@1=68.820 Prec@5=88.496 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=04:01 IST
=> Training   39.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.264 Prec@1=68.820 Prec@5=88.496 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=04:01 IST
=> Training   39.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.264 Prec@1=68.820 Prec@5=88.496 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=04:02 IST
=> Training   39.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.266 Prec@1=68.764 Prec@5=88.464 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=04:02 IST
=> Training   43.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.266 Prec@1=68.764 Prec@5=88.464 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:02 IST
=> Training   43.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.266 Prec@1=68.764 Prec@5=88.464 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:03 IST
=> Training   43.99% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.736 Prec@5=88.423 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:03 IST
=> Training   47.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.736 Prec@5=88.423 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:03 IST
=> Training   47.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.736 Prec@5=88.423 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:04 IST
=> Training   47.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.269 Prec@1=68.712 Prec@5=88.397 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:04 IST
=> Training   51.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.269 Prec@1=68.712 Prec@5=88.397 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:04 IST
=> Training   51.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.269 Prec@1=68.712 Prec@5=88.397 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:05 IST
=> Training   51.98% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.679 Prec@5=88.370 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:05 IST
=> Training   55.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.679 Prec@5=88.370 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=04:05 IST
=> Training   55.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.679 Prec@5=88.370 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=04:06 IST
=> Training   55.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.273 Prec@1=68.644 Prec@5=88.346 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=04:06 IST
=> Training   59.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.273 Prec@1=68.644 Prec@5=88.346 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=04:06 IST
=> Training   59.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.273 Prec@1=68.644 Prec@5=88.346 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=04:06 IST
=> Training   59.97% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.274 Prec@1=68.608 Prec@5=88.323 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=04:06 IST
=> Training   63.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.274 Prec@1=68.608 Prec@5=88.323 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:06 IST
=> Training   63.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.274 Prec@1=68.608 Prec@5=88.323 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:07 IST
=> Training   63.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.276 Prec@1=68.589 Prec@5=88.311 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:07 IST
=> Training   67.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.276 Prec@1=68.589 Prec@5=88.311 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:07 IST
=> Training   67.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.276 Prec@1=68.589 Prec@5=88.311 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:08 IST
=> Training   67.96% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.387 Loss=1.277 Prec@1=68.586 Prec@5=88.311 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:08 IST
=> Training   71.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.387 Loss=1.277 Prec@1=68.586 Prec@5=88.311 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=04:08 IST
=> Training   71.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.387 Loss=1.277 Prec@1=68.586 Prec@5=88.311 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=04:09 IST
=> Training   71.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.279 Prec@1=68.536 Prec@5=88.284 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=04:09 IST
=> Training   75.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.279 Prec@1=68.536 Prec@5=88.284 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:09 IST
=> Training   75.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.387 Loss=1.279 Prec@1=68.536 Prec@5=88.284 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:10 IST
=> Training   75.95% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.280 Prec@1=68.514 Prec@5=88.274 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:10 IST
=> Training   79.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.280 Prec@1=68.514 Prec@5=88.274 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:10 IST
=> Training   79.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.280 Prec@1=68.514 Prec@5=88.274 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:11 IST
=> Training   79.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.281 Prec@1=68.488 Prec@5=88.260 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:11 IST
=> Training   83.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.281 Prec@1=68.488 Prec@5=88.260 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=04:11 IST
=> Training   83.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.563 DataTime=0.386 Loss=1.281 Prec@1=68.488 Prec@5=88.260 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=04:12 IST
=> Training   83.94% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.282 Prec@1=68.458 Prec@5=88.241 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=04:12 IST
=> Training   87.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.282 Prec@1=68.458 Prec@5=88.241 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=04:12 IST
=> Training   87.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.562 DataTime=0.386 Loss=1.282 Prec@1=68.458 Prec@5=88.241 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=04:13 IST
=> Training   87.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.426 Prec@5=88.222 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=04:13 IST
=> Training   91.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.426 Prec@5=88.222 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:13 IST
=> Training   91.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.426 Prec@5=88.222 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:14 IST
=> Training   91.93% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.413 Prec@5=88.209 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:14 IST
=> Training   95.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.413 Prec@5=88.209 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:14 IST
=> Training   95.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.284 Prec@1=68.413 Prec@5=88.209 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:15 IST
=> Training   95.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.285 Prec@1=68.393 Prec@5=88.197 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:15 IST
=> Training   99.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.285 Prec@1=68.393 Prec@5=88.197 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:15 IST
=> Training   99.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.285 Prec@1=68.393 Prec@5=88.197 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:15 IST
=> Training   99.92% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.285 Prec@1=68.393 Prec@5=88.197 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:15 IST
=> Training   100.00% of 1x2503...Epoch=71/150 LR=0.0552 Time=0.561 DataTime=0.385 Loss=1.285 Prec@1=68.393 Prec@5=88.197 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=04:15 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:15 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:15 IST
=> Validation 0.00% of 1x98...Epoch=71/150 LR=0.0552 Time=7.407 Loss=0.914 Prec@1=77.344 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=04:15 IST
=> Validation 1.02% of 1x98...Epoch=71/150 LR=0.0552 Time=7.407 Loss=0.914 Prec@1=77.344 Prec@5=92.773 rate=7261.58 Hz, eta=0:00:00, total=0:00:00, wall=04:15 IST
** Validation 1.02% of 1x98...Epoch=71/150 LR=0.0552 Time=7.407 Loss=0.914 Prec@1=77.344 Prec@5=92.773 rate=7261.58 Hz, eta=0:00:00, total=0:00:00, wall=04:16 IST
** Validation 1.02% of 1x98...Epoch=71/150 LR=0.0552 Time=0.638 Loss=1.419 Prec@1=65.752 Prec@5=86.916 rate=7261.58 Hz, eta=0:00:00, total=0:00:00, wall=04:16 IST
** Validation 100.00% of 1x98...Epoch=71/150 LR=0.0552 Time=0.638 Loss=1.419 Prec@1=65.752 Prec@5=86.916 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=04:16 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> Training   0.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=4.182 DataTime=3.799 Loss=1.230 Prec@1=68.359 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=04:16 IST
=> Training   0.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=4.182 DataTime=3.799 Loss=1.230 Prec@1=68.359 Prec@5=88.672 rate=4258.65 Hz, eta=0:00:00, total=0:00:00, wall=04:16 IST
=> Training   0.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=4.182 DataTime=3.799 Loss=1.230 Prec@1=68.359 Prec@5=88.672 rate=4258.65 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST
=> Training   0.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.600 DataTime=0.422 Loss=1.250 Prec@1=69.065 Prec@5=88.807 rate=4258.65 Hz, eta=0:00:00, total=0:00:00, wall=04:17 IST
=> Training   4.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.600 DataTime=0.422 Loss=1.250 Prec@1=69.065 Prec@5=88.807 rate=1.79 Hz, eta=0:22:21, total=0:00:56, wall=04:17 IST
=> Training   4.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.600 DataTime=0.422 Loss=1.250 Prec@1=69.065 Prec@5=88.807 rate=1.79 Hz, eta=0:22:21, total=0:00:56, wall=04:18 IST
=> Training   4.04% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.582 DataTime=0.401 Loss=1.240 Prec@1=69.312 Prec@5=88.925 rate=1.79 Hz, eta=0:22:21, total=0:00:56, wall=04:18 IST
=> Training   8.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.582 DataTime=0.401 Loss=1.240 Prec@1=69.312 Prec@5=88.925 rate=1.78 Hz, eta=0:21:31, total=0:01:52, wall=04:18 IST
=> Training   8.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.582 DataTime=0.401 Loss=1.240 Prec@1=69.312 Prec@5=88.925 rate=1.78 Hz, eta=0:21:31, total=0:01:52, wall=04:19 IST
=> Training   8.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.574 DataTime=0.392 Loss=1.241 Prec@1=69.402 Prec@5=88.831 rate=1.78 Hz, eta=0:21:31, total=0:01:52, wall=04:19 IST
=> Training   12.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.574 DataTime=0.392 Loss=1.241 Prec@1=69.402 Prec@5=88.831 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=04:19 IST
=> Training   12.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.574 DataTime=0.392 Loss=1.241 Prec@1=69.402 Prec@5=88.831 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=04:20 IST
=> Training   12.03% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.569 DataTime=0.387 Loss=1.242 Prec@1=69.410 Prec@5=88.825 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=04:20 IST
=> Training   16.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.569 DataTime=0.387 Loss=1.242 Prec@1=69.410 Prec@5=88.825 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=04:20 IST
=> Training   16.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.569 DataTime=0.387 Loss=1.242 Prec@1=69.410 Prec@5=88.825 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=04:21 IST
=> Training   16.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.565 DataTime=0.386 Loss=1.241 Prec@1=69.445 Prec@5=88.802 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=04:21 IST
=> Training   20.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.565 DataTime=0.386 Loss=1.241 Prec@1=69.445 Prec@5=88.802 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=04:21 IST
=> Training   20.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.565 DataTime=0.386 Loss=1.241 Prec@1=69.445 Prec@5=88.802 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=04:22 IST
=> Training   20.02% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.566 DataTime=0.389 Loss=1.245 Prec@1=69.367 Prec@5=88.739 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=04:22 IST
=> Training   24.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.566 DataTime=0.389 Loss=1.245 Prec@1=69.367 Prec@5=88.739 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=04:22 IST
=> Training   24.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.566 DataTime=0.389 Loss=1.245 Prec@1=69.367 Prec@5=88.739 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=04:23 IST
=> Training   24.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.564 DataTime=0.388 Loss=1.249 Prec@1=69.282 Prec@5=88.695 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=04:23 IST
=> Training   28.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.564 DataTime=0.388 Loss=1.249 Prec@1=69.282 Prec@5=88.695 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=04:23 IST
=> Training   28.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.564 DataTime=0.388 Loss=1.249 Prec@1=69.282 Prec@5=88.695 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=04:23 IST
=> Training   28.01% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.219 Prec@5=88.674 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=04:23 IST
=> Training   32.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.219 Prec@5=88.674 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:23 IST
=> Training   32.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.219 Prec@5=88.674 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:24 IST
=> Training   32.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.253 Prec@1=69.151 Prec@5=88.666 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:24 IST
=> Training   36.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.253 Prec@1=69.151 Prec@5=88.666 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=04:24 IST
=> Training   36.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.563 DataTime=0.387 Loss=1.253 Prec@1=69.151 Prec@5=88.666 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=04:25 IST
=> Training   36.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.255 Prec@1=69.080 Prec@5=88.631 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=04:25 IST
=> Training   39.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.255 Prec@1=69.080 Prec@5=88.631 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:25 IST
=> Training   39.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.255 Prec@1=69.080 Prec@5=88.631 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:26 IST
=> Training   39.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.257 Prec@1=69.030 Prec@5=88.610 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=04:26 IST
=> Training   43.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.257 Prec@1=69.030 Prec@5=88.610 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:26 IST
=> Training   43.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.385 Loss=1.257 Prec@1=69.030 Prec@5=88.610 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:27 IST
=> Training   43.99% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.260 Prec@1=68.965 Prec@5=88.560 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:27 IST
=> Training   47.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.260 Prec@1=68.965 Prec@5=88.560 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:27 IST
=> Training   47.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.260 Prec@1=68.965 Prec@5=88.560 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:28 IST
=> Training   47.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.262 Prec@1=68.918 Prec@5=88.541 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=04:28 IST
=> Training   51.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.262 Prec@1=68.918 Prec@5=88.541 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:28 IST
=> Training   51.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.262 Prec@1=68.918 Prec@5=88.541 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:29 IST
=> Training   51.98% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.263 Prec@1=68.886 Prec@5=88.525 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=04:29 IST
=> Training   55.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.263 Prec@1=68.886 Prec@5=88.525 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=04:29 IST
=> Training   55.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.263 Prec@1=68.886 Prec@5=88.525 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=04:30 IST
=> Training   55.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.265 Prec@1=68.849 Prec@5=88.496 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=04:30 IST
=> Training   59.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.265 Prec@1=68.849 Prec@5=88.496 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=04:30 IST
=> Training   59.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.265 Prec@1=68.849 Prec@5=88.496 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=04:31 IST
=> Training   59.97% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.267 Prec@1=68.817 Prec@5=88.465 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=04:31 IST
=> Training   63.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.267 Prec@1=68.817 Prec@5=88.465 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=04:31 IST
=> Training   63.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.267 Prec@1=68.817 Prec@5=88.465 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=04:32 IST
=> Training   63.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.779 Prec@5=88.441 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=04:32 IST
=> Training   67.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.779 Prec@5=88.441 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:32 IST
=> Training   67.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.562 DataTime=0.386 Loss=1.268 Prec@1=68.779 Prec@5=88.441 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:33 IST
=> Training   67.96% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.271 Prec@1=68.727 Prec@5=88.415 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:33 IST
=> Training   71.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.271 Prec@1=68.727 Prec@5=88.415 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:33 IST
=> Training   71.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.385 Loss=1.271 Prec@1=68.727 Prec@5=88.415 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:34 IST
=> Training   71.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.272 Prec@1=68.699 Prec@5=88.398 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:34 IST
=> Training   75.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.272 Prec@1=68.699 Prec@5=88.398 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:34 IST
=> Training   75.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.272 Prec@1=68.699 Prec@5=88.398 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:35 IST
=> Training   75.95% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.274 Prec@1=68.648 Prec@5=88.371 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=04:35 IST
=> Training   79.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.274 Prec@1=68.648 Prec@5=88.371 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:35 IST
=> Training   79.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.274 Prec@1=68.648 Prec@5=88.371 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:36 IST
=> Training   79.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.275 Prec@1=68.632 Prec@5=88.354 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=04:36 IST
=> Training   83.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.275 Prec@1=68.632 Prec@5=88.354 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:36 IST
=> Training   83.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.275 Prec@1=68.632 Prec@5=88.354 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:37 IST
=> Training   83.94% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.560 DataTime=0.385 Loss=1.275 Prec@1=68.631 Prec@5=88.344 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:37 IST
=> Training   87.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.560 DataTime=0.385 Loss=1.275 Prec@1=68.631 Prec@5=88.344 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:37 IST
=> Training   87.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.560 DataTime=0.385 Loss=1.275 Prec@1=68.631 Prec@5=88.344 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:37 IST
=> Training   87.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.276 Prec@1=68.619 Prec@5=88.325 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:37 IST
=> Training   91.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.276 Prec@1=68.619 Prec@5=88.325 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:37 IST
=> Training   91.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.386 Loss=1.276 Prec@1=68.619 Prec@5=88.325 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:38 IST
=> Training   91.93% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.278 Prec@1=68.584 Prec@5=88.304 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=04:38 IST
=> Training   95.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.278 Prec@1=68.584 Prec@5=88.304 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=04:38 IST
=> Training   95.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.278 Prec@1=68.584 Prec@5=88.304 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=04:39 IST
=> Training   95.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.279 Prec@1=68.554 Prec@5=88.282 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=04:39 IST
=> Training   99.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.279 Prec@1=68.554 Prec@5=88.282 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=04:39 IST
=> Training   99.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.279 Prec@1=68.554 Prec@5=88.282 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=04:39 IST
=> Training   99.92% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.279 Prec@1=68.553 Prec@5=88.281 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=04:39 IST
=> Training   100.00% of 1x2503...Epoch=72/150 LR=0.0542 Time=0.561 DataTime=0.387 Loss=1.279 Prec@1=68.553 Prec@5=88.281 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=04:39 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> Validation 0.00% of 1x98...Epoch=72/150 LR=0.0542 Time=7.257 Loss=0.867 Prec@1=79.102 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=04:39 IST
=> Validation 1.02% of 1x98...Epoch=72/150 LR=0.0542 Time=7.257 Loss=0.867 Prec@1=79.102 Prec@5=93.555 rate=7805.00 Hz, eta=0:00:00, total=0:00:00, wall=04:39 IST
** Validation 1.02% of 1x98...Epoch=72/150 LR=0.0542 Time=7.257 Loss=0.867 Prec@1=79.102 Prec@5=93.555 rate=7805.00 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST
** Validation 1.02% of 1x98...Epoch=72/150 LR=0.0542 Time=0.639 Loss=1.445 Prec@1=65.086 Prec@5=86.412 rate=7805.00 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST
** Validation 100.00% of 1x98...Epoch=72/150 LR=0.0542 Time=0.639 Loss=1.445 Prec@1=65.086 Prec@5=86.412 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=04:40 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST
=> Training   0.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=5.128 DataTime=4.721 Loss=1.171 Prec@1=71.484 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=04:40 IST
=> Training   0.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=5.128 DataTime=4.721 Loss=1.171 Prec@1=71.484 Prec@5=89.062 rate=7851.88 Hz, eta=0:00:00, total=0:00:00, wall=04:40 IST
=> Training   0.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=5.128 DataTime=4.721 Loss=1.171 Prec@1=71.484 Prec@5=89.062 rate=7851.88 Hz, eta=0:00:00, total=0:00:00, wall=04:41 IST
=> Training   0.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.604 DataTime=0.441 Loss=1.231 Prec@1=69.630 Prec@5=88.993 rate=7851.88 Hz, eta=0:00:00, total=0:00:00, wall=04:41 IST
=> Training   4.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.604 DataTime=0.441 Loss=1.231 Prec@1=69.630 Prec@5=88.993 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=04:41 IST
=> Training   4.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.604 DataTime=0.441 Loss=1.231 Prec@1=69.630 Prec@5=88.993 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=04:42 IST
=> Training   4.04% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.590 DataTime=0.419 Loss=1.234 Prec@1=69.494 Prec@5=88.832 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=04:42 IST
=> Training   8.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.590 DataTime=0.419 Loss=1.234 Prec@1=69.494 Prec@5=88.832 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=04:42 IST
=> Training   8.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.590 DataTime=0.419 Loss=1.234 Prec@1=69.494 Prec@5=88.832 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=04:43 IST
=> Training   8.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.576 DataTime=0.400 Loss=1.234 Prec@1=69.537 Prec@5=88.810 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=04:43 IST
=> Training   12.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.576 DataTime=0.400 Loss=1.234 Prec@1=69.537 Prec@5=88.810 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=04:43 IST
=> Training   12.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.576 DataTime=0.400 Loss=1.234 Prec@1=69.537 Prec@5=88.810 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=04:44 IST
=> Training   12.03% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.573 DataTime=0.396 Loss=1.234 Prec@1=69.554 Prec@5=88.802 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=04:44 IST
=> Training   16.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.573 DataTime=0.396 Loss=1.234 Prec@1=69.554 Prec@5=88.802 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=04:44 IST
=> Training   16.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.573 DataTime=0.396 Loss=1.234 Prec@1=69.554 Prec@5=88.802 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=04:45 IST
=> Training   16.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.569 DataTime=0.394 Loss=1.238 Prec@1=69.456 Prec@5=88.781 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=04:45 IST
=> Training   20.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.569 DataTime=0.394 Loss=1.238 Prec@1=69.456 Prec@5=88.781 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:45 IST
=> Training   20.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.569 DataTime=0.394 Loss=1.238 Prec@1=69.456 Prec@5=88.781 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:46 IST
=> Training   20.02% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.570 DataTime=0.396 Loss=1.243 Prec@1=69.317 Prec@5=88.740 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:46 IST
=> Training   24.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.570 DataTime=0.396 Loss=1.243 Prec@1=69.317 Prec@5=88.740 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=04:46 IST
=> Training   24.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.570 DataTime=0.396 Loss=1.243 Prec@1=69.317 Prec@5=88.740 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=04:47 IST
=> Training   24.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.568 DataTime=0.393 Loss=1.244 Prec@1=69.302 Prec@5=88.731 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=04:47 IST
=> Training   28.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.568 DataTime=0.393 Loss=1.244 Prec@1=69.302 Prec@5=88.731 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=04:47 IST
=> Training   28.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.568 DataTime=0.393 Loss=1.244 Prec@1=69.302 Prec@5=88.731 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=04:48 IST
=> Training   28.01% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.567 DataTime=0.393 Loss=1.248 Prec@1=69.238 Prec@5=88.683 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=04:48 IST
=> Training   32.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.567 DataTime=0.393 Loss=1.248 Prec@1=69.238 Prec@5=88.683 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:48 IST
=> Training   32.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.567 DataTime=0.393 Loss=1.248 Prec@1=69.238 Prec@5=88.683 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:49 IST
=> Training   32.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.566 DataTime=0.391 Loss=1.251 Prec@1=69.150 Prec@5=88.639 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:49 IST
=> Training   36.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.566 DataTime=0.391 Loss=1.251 Prec@1=69.150 Prec@5=88.639 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=04:49 IST
=> Training   36.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.566 DataTime=0.391 Loss=1.251 Prec@1=69.150 Prec@5=88.639 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=04:50 IST
=> Training   36.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.565 DataTime=0.390 Loss=1.252 Prec@1=69.132 Prec@5=88.615 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=04:50 IST
=> Training   39.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.565 DataTime=0.390 Loss=1.252 Prec@1=69.132 Prec@5=88.615 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=04:50 IST
=> Training   39.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.565 DataTime=0.390 Loss=1.252 Prec@1=69.132 Prec@5=88.615 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=04:51 IST
=> Training   39.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.390 Loss=1.254 Prec@1=69.101 Prec@5=88.585 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=04:51 IST
=> Training   43.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.390 Loss=1.254 Prec@1=69.101 Prec@5=88.585 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=04:51 IST
=> Training   43.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.390 Loss=1.254 Prec@1=69.101 Prec@5=88.585 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=04:52 IST
=> Training   43.99% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.255 Prec@1=69.058 Prec@5=88.577 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=04:52 IST
=> Training   47.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.255 Prec@1=69.058 Prec@5=88.577 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=04:52 IST
=> Training   47.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.255 Prec@1=69.058 Prec@5=88.577 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=04:53 IST
=> Training   47.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.257 Prec@1=69.038 Prec@5=88.565 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=04:53 IST
=> Training   51.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.257 Prec@1=69.038 Prec@5=88.565 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=04:53 IST
=> Training   51.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.257 Prec@1=69.038 Prec@5=88.565 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=04:54 IST
=> Training   51.98% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.387 Loss=1.258 Prec@1=68.998 Prec@5=88.545 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=04:54 IST
=> Training   55.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.387 Loss=1.258 Prec@1=68.998 Prec@5=88.545 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:54 IST
=> Training   55.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.387 Loss=1.258 Prec@1=68.998 Prec@5=88.545 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:55 IST
=> Training   55.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.259 Prec@1=68.984 Prec@5=88.541 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:55 IST
=> Training   59.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.259 Prec@1=68.984 Prec@5=88.541 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=04:55 IST
=> Training   59.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.259 Prec@1=68.984 Prec@5=88.541 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=04:55 IST
=> Training   59.97% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.933 Prec@5=88.507 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=04:55 IST
=> Training   63.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.933 Prec@5=88.507 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=04:55 IST
=> Training   63.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.933 Prec@5=88.507 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=04:56 IST
=> Training   63.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.262 Prec@1=68.914 Prec@5=88.486 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=04:56 IST
=> Training   67.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.262 Prec@1=68.914 Prec@5=88.486 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=04:56 IST
=> Training   67.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.262 Prec@1=68.914 Prec@5=88.486 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=04:57 IST
=> Training   67.96% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.264 Prec@1=68.894 Prec@5=88.469 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=04:57 IST
=> Training   71.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.264 Prec@1=68.894 Prec@5=88.469 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=04:57 IST
=> Training   71.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.564 DataTime=0.389 Loss=1.264 Prec@1=68.894 Prec@5=88.469 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=04:58 IST
=> Training   71.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.266 Prec@1=68.853 Prec@5=88.441 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=04:58 IST
=> Training   75.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.266 Prec@1=68.853 Prec@5=88.441 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=04:58 IST
=> Training   75.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.266 Prec@1=68.853 Prec@5=88.441 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=04:59 IST
=> Training   75.95% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.268 Prec@1=68.815 Prec@5=88.417 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=04:59 IST
=> Training   79.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.268 Prec@1=68.815 Prec@5=88.417 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=04:59 IST
=> Training   79.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.268 Prec@1=68.815 Prec@5=88.417 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=05:00 IST
=> Training   79.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.269 Prec@1=68.792 Prec@5=88.396 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=05:00 IST
=> Training   83.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.269 Prec@1=68.792 Prec@5=88.396 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:00 IST
=> Training   83.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.269 Prec@1=68.792 Prec@5=88.396 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:01 IST
=> Training   83.94% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.270 Prec@1=68.765 Prec@5=88.386 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:01 IST
=> Training   87.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.270 Prec@1=68.765 Prec@5=88.386 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:01 IST
=> Training   87.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.563 DataTime=0.388 Loss=1.270 Prec@1=68.765 Prec@5=88.386 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:02 IST
=> Training   87.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.736 Prec@5=88.371 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:02 IST
=> Training   91.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.736 Prec@5=88.371 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:02 IST
=> Training   91.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.386 Loss=1.271 Prec@1=68.736 Prec@5=88.371 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:03 IST
=> Training   91.93% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.715 Prec@5=88.367 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=05:03 IST
=> Training   95.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.715 Prec@5=88.367 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=05:03 IST
=> Training   95.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.715 Prec@5=88.367 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=05:04 IST
=> Training   95.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.708 Prec@5=88.355 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=05:04 IST
=> Training   99.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.708 Prec@5=88.355 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=05:04 IST
=> Training   99.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.708 Prec@5=88.355 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=05:04 IST
=> Training   99.92% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.707 Prec@5=88.356 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=05:04 IST
=> Training   100.00% of 1x2503...Epoch=73/150 LR=0.0531 Time=0.562 DataTime=0.387 Loss=1.272 Prec@1=68.707 Prec@5=88.356 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=05:04 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:04 IST
=> Validation 0.00% of 1x98...Epoch=73/150 LR=0.0531 Time=6.765 Loss=0.922 Prec@1=76.953 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=05:04 IST
=> Validation 1.02% of 1x98...Epoch=73/150 LR=0.0531 Time=6.765 Loss=0.922 Prec@1=76.953 Prec@5=93.555 rate=7333.26 Hz, eta=0:00:00, total=0:00:00, wall=05:04 IST
** Validation 1.02% of 1x98...Epoch=73/150 LR=0.0531 Time=6.765 Loss=0.922 Prec@1=76.953 Prec@5=93.555 rate=7333.26 Hz, eta=0:00:00, total=0:00:00, wall=05:05 IST
** Validation 1.02% of 1x98...Epoch=73/150 LR=0.0531 Time=0.633 Loss=1.490 Prec@1=64.564 Prec@5=86.006 rate=7333.26 Hz, eta=0:00:00, total=0:00:00, wall=05:05 IST
** Validation 100.00% of 1x98...Epoch=73/150 LR=0.0531 Time=0.633 Loss=1.490 Prec@1=64.564 Prec@5=86.006 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=05:05 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:05 IST
=> Training   0.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=4.162 DataTime=3.883 Loss=1.269 Prec@1=70.898 Prec@5=88.477 rate=0 Hz, eta=?, total=0:00:00, wall=05:05 IST
=> Training   0.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=4.162 DataTime=3.883 Loss=1.269 Prec@1=70.898 Prec@5=88.477 rate=6645.09 Hz, eta=0:00:00, total=0:00:00, wall=05:05 IST
=> Training   0.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=4.162 DataTime=3.883 Loss=1.269 Prec@1=70.898 Prec@5=88.477 rate=6645.09 Hz, eta=0:00:00, total=0:00:00, wall=05:06 IST
=> Training   0.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.585 DataTime=0.420 Loss=1.232 Prec@1=69.597 Prec@5=88.960 rate=6645.09 Hz, eta=0:00:00, total=0:00:00, wall=05:06 IST
=> Training   4.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.585 DataTime=0.420 Loss=1.232 Prec@1=69.597 Prec@5=88.960 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=05:06 IST
=> Training   4.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.585 DataTime=0.420 Loss=1.232 Prec@1=69.597 Prec@5=88.960 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=05:07 IST
=> Training   4.04% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.402 Loss=1.226 Prec@1=69.765 Prec@5=88.965 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=05:07 IST
=> Training   8.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.402 Loss=1.226 Prec@1=69.765 Prec@5=88.965 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=05:07 IST
=> Training   8.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.402 Loss=1.226 Prec@1=69.765 Prec@5=88.965 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=05:08 IST
=> Training   8.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.916 rate=1.82 Hz, eta=0:21:05, total=0:01:50, wall=05:08 IST
=> Training   12.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.916 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:08 IST
=> Training   12.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.570 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.916 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:09 IST
=> Training   12.03% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.391 Loss=1.231 Prec@1=69.647 Prec@5=88.872 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:09 IST
=> Training   16.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.391 Loss=1.231 Prec@1=69.647 Prec@5=88.872 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=05:09 IST
=> Training   16.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.391 Loss=1.231 Prec@1=69.647 Prec@5=88.872 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=05:10 IST
=> Training   16.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.566 DataTime=0.391 Loss=1.233 Prec@1=69.584 Prec@5=88.852 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=05:10 IST
=> Training   20.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.566 DataTime=0.391 Loss=1.233 Prec@1=69.584 Prec@5=88.852 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:10 IST
=> Training   20.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.566 DataTime=0.391 Loss=1.233 Prec@1=69.584 Prec@5=88.852 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:11 IST
=> Training   20.02% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.235 Prec@1=69.532 Prec@5=88.861 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:11 IST
=> Training   24.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.235 Prec@1=69.532 Prec@5=88.861 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:11 IST
=> Training   24.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.235 Prec@1=69.532 Prec@5=88.861 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:11 IST
=> Training   24.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.238 Prec@1=69.458 Prec@5=88.832 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=05:11 IST
=> Training   28.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.238 Prec@1=69.458 Prec@5=88.832 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=05:11 IST
=> Training   28.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.238 Prec@1=69.458 Prec@5=88.832 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=05:12 IST
=> Training   28.01% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.390 Loss=1.240 Prec@1=69.438 Prec@5=88.801 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=05:12 IST
=> Training   32.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.390 Loss=1.240 Prec@1=69.438 Prec@5=88.801 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:12 IST
=> Training   32.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.390 Loss=1.240 Prec@1=69.438 Prec@5=88.801 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:13 IST
=> Training   32.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.243 Prec@1=69.379 Prec@5=88.774 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:13 IST
=> Training   36.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.243 Prec@1=69.379 Prec@5=88.774 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=05:13 IST
=> Training   36.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.243 Prec@1=69.379 Prec@5=88.774 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=05:14 IST
=> Training   36.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.245 Prec@1=69.329 Prec@5=88.744 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=05:14 IST
=> Training   39.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.245 Prec@1=69.329 Prec@5=88.744 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=05:14 IST
=> Training   39.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.245 Prec@1=69.329 Prec@5=88.744 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=05:15 IST
=> Training   39.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.247 Prec@1=69.298 Prec@5=88.706 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=05:15 IST
=> Training   43.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.247 Prec@1=69.298 Prec@5=88.706 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=05:15 IST
=> Training   43.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.247 Prec@1=69.298 Prec@5=88.706 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=05:16 IST
=> Training   43.99% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.389 Loss=1.249 Prec@1=69.252 Prec@5=88.674 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=05:16 IST
=> Training   47.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.389 Loss=1.249 Prec@1=69.252 Prec@5=88.674 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:16 IST
=> Training   47.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.564 DataTime=0.389 Loss=1.249 Prec@1=69.252 Prec@5=88.674 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:17 IST
=> Training   47.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.250 Prec@1=69.201 Prec@5=88.658 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=05:17 IST
=> Training   51.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.250 Prec@1=69.201 Prec@5=88.658 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=05:17 IST
=> Training   51.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.250 Prec@1=69.201 Prec@5=88.658 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=05:18 IST
=> Training   51.98% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.252 Prec@1=69.168 Prec@5=88.632 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=05:18 IST
=> Training   55.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.252 Prec@1=69.168 Prec@5=88.632 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=05:18 IST
=> Training   55.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.252 Prec@1=69.168 Prec@5=88.632 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=05:19 IST
=> Training   55.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.254 Prec@1=69.132 Prec@5=88.602 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=05:19 IST
=> Training   59.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.254 Prec@1=69.132 Prec@5=88.602 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:19 IST
=> Training   59.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.254 Prec@1=69.132 Prec@5=88.602 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:20 IST
=> Training   59.97% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.389 Loss=1.255 Prec@1=69.098 Prec@5=88.594 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:20 IST
=> Training   63.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.389 Loss=1.255 Prec@1=69.098 Prec@5=88.594 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=05:20 IST
=> Training   63.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.389 Loss=1.255 Prec@1=69.098 Prec@5=88.594 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=05:21 IST
=> Training   63.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.256 Prec@1=69.064 Prec@5=88.573 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=05:21 IST
=> Training   67.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.256 Prec@1=69.064 Prec@5=88.573 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:21 IST
=> Training   67.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.256 Prec@1=69.064 Prec@5=88.573 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:22 IST
=> Training   67.96% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.258 Prec@1=69.021 Prec@5=88.544 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:22 IST
=> Training   71.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.258 Prec@1=69.021 Prec@5=88.544 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=05:22 IST
=> Training   71.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.390 Loss=1.258 Prec@1=69.021 Prec@5=88.544 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=05:23 IST
=> Training   71.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.259 Prec@1=69.003 Prec@5=88.533 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=05:23 IST
=> Training   75.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.259 Prec@1=69.003 Prec@5=88.533 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=05:23 IST
=> Training   75.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.259 Prec@1=69.003 Prec@5=88.533 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=05:24 IST
=> Training   75.95% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.260 Prec@1=68.990 Prec@5=88.525 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=05:24 IST
=> Training   79.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.260 Prec@1=68.990 Prec@5=88.525 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=05:24 IST
=> Training   79.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.389 Loss=1.260 Prec@1=68.990 Prec@5=88.525 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=05:25 IST
=> Training   79.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.961 Prec@5=88.510 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=05:25 IST
=> Training   83.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.961 Prec@5=88.510 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:25 IST
=> Training   83.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.261 Prec@1=68.961 Prec@5=88.510 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:26 IST
=> Training   83.94% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.262 Prec@1=68.939 Prec@5=88.500 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:26 IST
=> Training   87.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.262 Prec@1=68.939 Prec@5=88.500 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:26 IST
=> Training   87.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.387 Loss=1.262 Prec@1=68.939 Prec@5=88.500 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:26 IST
=> Training   87.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.264 Prec@1=68.899 Prec@5=88.485 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:26 IST
=> Training   91.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.264 Prec@1=68.899 Prec@5=88.485 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:26 IST
=> Training   91.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.264 Prec@1=68.899 Prec@5=88.485 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:27 IST
=> Training   91.93% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.264 Prec@1=68.878 Prec@5=88.475 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:27 IST
=> Training   95.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.264 Prec@1=68.878 Prec@5=88.475 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=05:27 IST
=> Training   95.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.563 DataTime=0.388 Loss=1.264 Prec@1=68.878 Prec@5=88.475 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=05:28 IST
=> Training   95.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.265 Prec@1=68.866 Prec@5=88.462 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=05:28 IST
=> Training   99.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.265 Prec@1=68.866 Prec@5=88.462 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=05:28 IST
=> Training   99.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.265 Prec@1=68.866 Prec@5=88.462 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=05:28 IST
=> Training   99.92% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.265 Prec@1=68.865 Prec@5=88.462 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=05:28 IST
=> Training   100.00% of 1x2503...Epoch=74/150 LR=0.0521 Time=0.562 DataTime=0.388 Loss=1.265 Prec@1=68.865 Prec@5=88.462 rate=1.78 Hz, eta=0:00:00, total=0:23:22, wall=05:28 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:28 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:28 IST
=> Validation 0.00% of 1x98...Epoch=74/150 LR=0.0521 Time=6.966 Loss=0.885 Prec@1=76.953 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=05:28 IST
=> Validation 1.02% of 1x98...Epoch=74/150 LR=0.0521 Time=6.966 Loss=0.885 Prec@1=76.953 Prec@5=92.773 rate=1654.26 Hz, eta=0:00:00, total=0:00:00, wall=05:28 IST
** Validation 1.02% of 1x98...Epoch=74/150 LR=0.0521 Time=6.966 Loss=0.885 Prec@1=76.953 Prec@5=92.773 rate=1654.26 Hz, eta=0:00:00, total=0:00:00, wall=05:29 IST
** Validation 1.02% of 1x98...Epoch=74/150 LR=0.0521 Time=0.636 Loss=1.417 Prec@1=65.900 Prec@5=86.918 rate=1654.26 Hz, eta=0:00:00, total=0:00:00, wall=05:29 IST
** Validation 100.00% of 1x98...Epoch=74/150 LR=0.0521 Time=0.636 Loss=1.417 Prec@1=65.900 Prec@5=86.918 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=05:29 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:29 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:29 IST
=> Training   0.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=5.098 DataTime=4.741 Loss=1.104 Prec@1=75.000 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=05:29 IST
=> Training   0.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=5.098 DataTime=4.741 Loss=1.104 Prec@1=75.000 Prec@5=90.430 rate=8089.64 Hz, eta=0:00:00, total=0:00:00, wall=05:29 IST
=> Training   0.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=5.098 DataTime=4.741 Loss=1.104 Prec@1=75.000 Prec@5=90.430 rate=8089.64 Hz, eta=0:00:00, total=0:00:00, wall=05:30 IST
=> Training   0.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.597 DataTime=0.418 Loss=1.214 Prec@1=69.790 Prec@5=89.182 rate=8089.64 Hz, eta=0:00:00, total=0:00:00, wall=05:30 IST
=> Training   4.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.597 DataTime=0.418 Loss=1.214 Prec@1=69.790 Prec@5=89.182 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=05:30 IST
=> Training   4.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.597 DataTime=0.418 Loss=1.214 Prec@1=69.790 Prec@5=89.182 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=05:31 IST
=> Training   4.04% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.578 DataTime=0.400 Loss=1.221 Prec@1=69.685 Prec@5=89.003 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=05:31 IST
=> Training   8.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.578 DataTime=0.400 Loss=1.221 Prec@1=69.685 Prec@5=89.003 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:31 IST
=> Training   8.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.578 DataTime=0.400 Loss=1.221 Prec@1=69.685 Prec@5=89.003 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:32 IST
=> Training   8.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.573 DataTime=0.398 Loss=1.224 Prec@1=69.634 Prec@5=88.982 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:32 IST
=> Training   12.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.573 DataTime=0.398 Loss=1.224 Prec@1=69.634 Prec@5=88.982 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:32 IST
=> Training   12.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.573 DataTime=0.398 Loss=1.224 Prec@1=69.634 Prec@5=88.982 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:33 IST
=> Training   12.03% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.568 DataTime=0.392 Loss=1.229 Prec@1=69.549 Prec@5=88.928 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=05:33 IST
=> Training   16.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.568 DataTime=0.392 Loss=1.229 Prec@1=69.549 Prec@5=88.928 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=05:33 IST
=> Training   16.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.568 DataTime=0.392 Loss=1.229 Prec@1=69.549 Prec@5=88.928 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=05:34 IST
=> Training   16.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.569 DataTime=0.394 Loss=1.229 Prec@1=69.560 Prec@5=88.934 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=05:34 IST
=> Training   20.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.569 DataTime=0.394 Loss=1.229 Prec@1=69.560 Prec@5=88.934 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=05:34 IST
=> Training   20.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.569 DataTime=0.394 Loss=1.229 Prec@1=69.560 Prec@5=88.934 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=05:35 IST
=> Training   20.02% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.566 DataTime=0.390 Loss=1.232 Prec@1=69.494 Prec@5=88.872 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=05:35 IST
=> Training   24.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.566 DataTime=0.390 Loss=1.232 Prec@1=69.494 Prec@5=88.872 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:35 IST
=> Training   24.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.566 DataTime=0.390 Loss=1.232 Prec@1=69.494 Prec@5=88.872 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:36 IST
=> Training   24.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.389 Loss=1.234 Prec@1=69.465 Prec@5=88.847 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=05:36 IST
=> Training   28.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.389 Loss=1.234 Prec@1=69.465 Prec@5=88.847 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=05:36 IST
=> Training   28.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.389 Loss=1.234 Prec@1=69.465 Prec@5=88.847 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=05:37 IST
=> Training   28.01% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.390 Loss=1.236 Prec@1=69.440 Prec@5=88.817 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=05:37 IST
=> Training   32.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.390 Loss=1.236 Prec@1=69.440 Prec@5=88.817 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:37 IST
=> Training   32.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.565 DataTime=0.390 Loss=1.236 Prec@1=69.440 Prec@5=88.817 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:38 IST
=> Training   32.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.238 Prec@1=69.414 Prec@5=88.808 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=05:38 IST
=> Training   36.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.238 Prec@1=69.414 Prec@5=88.808 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=05:38 IST
=> Training   36.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.238 Prec@1=69.414 Prec@5=88.808 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=05:39 IST
=> Training   36.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.240 Prec@1=69.375 Prec@5=88.792 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=05:39 IST
=> Training   39.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.240 Prec@1=69.375 Prec@5=88.792 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:39 IST
=> Training   39.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.240 Prec@1=69.375 Prec@5=88.792 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:40 IST
=> Training   39.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.241 Prec@1=69.364 Prec@5=88.762 rate=1.79 Hz, eta=0:13:57, total=0:09:17, wall=05:40 IST
=> Training   43.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.241 Prec@1=69.364 Prec@5=88.762 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=05:40 IST
=> Training   43.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.241 Prec@1=69.364 Prec@5=88.762 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=05:41 IST
=> Training   43.99% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.244 Prec@1=69.306 Prec@5=88.728 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=05:41 IST
=> Training   47.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.244 Prec@1=69.306 Prec@5=88.728 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=05:41 IST
=> Training   47.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.244 Prec@1=69.306 Prec@5=88.728 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=05:42 IST
=> Training   47.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.245 Prec@1=69.290 Prec@5=88.704 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=05:42 IST
=> Training   51.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.245 Prec@1=69.290 Prec@5=88.704 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=05:42 IST
=> Training   51.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.564 DataTime=0.389 Loss=1.245 Prec@1=69.290 Prec@5=88.704 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=05:43 IST
=> Training   51.98% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.246 Prec@1=69.287 Prec@5=88.681 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=05:43 IST
=> Training   55.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.246 Prec@1=69.287 Prec@5=88.681 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=05:43 IST
=> Training   55.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.246 Prec@1=69.287 Prec@5=88.681 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=05:43 IST
=> Training   55.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.247 Prec@1=69.260 Prec@5=88.668 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=05:43 IST
=> Training   59.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.247 Prec@1=69.260 Prec@5=88.668 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:43 IST
=> Training   59.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.247 Prec@1=69.260 Prec@5=88.668 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:44 IST
=> Training   59.97% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.249 Prec@1=69.211 Prec@5=88.630 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:44 IST
=> Training   63.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.249 Prec@1=69.211 Prec@5=88.630 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=05:44 IST
=> Training   63.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.387 Loss=1.249 Prec@1=69.211 Prec@5=88.630 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=05:45 IST
=> Training   63.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.173 Prec@5=88.613 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=05:45 IST
=> Training   67.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.173 Prec@5=88.613 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:45 IST
=> Training   67.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.387 Loss=1.251 Prec@1=69.173 Prec@5=88.613 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:46 IST
=> Training   67.96% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.252 Prec@1=69.148 Prec@5=88.595 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=05:46 IST
=> Training   71.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.252 Prec@1=69.148 Prec@5=88.595 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:46 IST
=> Training   71.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.252 Prec@1=69.148 Prec@5=88.595 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:47 IST
=> Training   71.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.254 Prec@1=69.101 Prec@5=88.564 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=05:47 IST
=> Training   75.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.254 Prec@1=69.101 Prec@5=88.564 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:47 IST
=> Training   75.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.254 Prec@1=69.101 Prec@5=88.564 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:48 IST
=> Training   75.95% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.255 Prec@1=69.069 Prec@5=88.550 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:48 IST
=> Training   79.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.255 Prec@1=69.069 Prec@5=88.550 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:48 IST
=> Training   79.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.255 Prec@1=69.069 Prec@5=88.550 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:49 IST
=> Training   79.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.256 Prec@1=69.060 Prec@5=88.541 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=05:49 IST
=> Training   83.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.256 Prec@1=69.060 Prec@5=88.541 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:49 IST
=> Training   83.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.256 Prec@1=69.060 Prec@5=88.541 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:50 IST
=> Training   83.94% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.257 Prec@1=69.043 Prec@5=88.518 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:50 IST
=> Training   87.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.257 Prec@1=69.043 Prec@5=88.518 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=05:50 IST
=> Training   87.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.257 Prec@1=69.043 Prec@5=88.518 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=05:51 IST
=> Training   87.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.258 Prec@1=69.021 Prec@5=88.502 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=05:51 IST
=> Training   91.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.258 Prec@1=69.021 Prec@5=88.502 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:51 IST
=> Training   91.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.563 DataTime=0.386 Loss=1.258 Prec@1=69.021 Prec@5=88.502 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:52 IST
=> Training   91.93% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.260 Prec@1=68.986 Prec@5=88.483 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=05:52 IST
=> Training   95.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.260 Prec@1=68.986 Prec@5=88.483 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:52 IST
=> Training   95.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.386 Loss=1.260 Prec@1=68.986 Prec@5=88.483 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:53 IST
=> Training   95.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.385 Loss=1.261 Prec@1=68.970 Prec@5=88.469 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=05:53 IST
=> Training   99.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.385 Loss=1.261 Prec@1=68.970 Prec@5=88.469 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=05:53 IST
=> Training   99.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.385 Loss=1.261 Prec@1=68.970 Prec@5=88.469 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=05:53 IST
=> Training   99.92% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.385 Loss=1.261 Prec@1=68.968 Prec@5=88.467 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=05:53 IST
=> Training   100.00% of 1x2503...Epoch=75/150 LR=0.0510 Time=0.562 DataTime=0.385 Loss=1.261 Prec@1=68.968 Prec@5=88.467 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=05:53 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST
=> Validation 0.00% of 1x98...Epoch=75/150 LR=0.0510 Time=6.756 Loss=0.872 Prec@1=77.148 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=05:53 IST
=> Validation 1.02% of 1x98...Epoch=75/150 LR=0.0510 Time=6.756 Loss=0.872 Prec@1=77.148 Prec@5=93.555 rate=1450.71 Hz, eta=0:00:00, total=0:00:00, wall=05:53 IST
** Validation 1.02% of 1x98...Epoch=75/150 LR=0.0510 Time=6.756 Loss=0.872 Prec@1=77.148 Prec@5=93.555 rate=1450.71 Hz, eta=0:00:00, total=0:00:00, wall=05:54 IST
** Validation 1.02% of 1x98...Epoch=75/150 LR=0.0510 Time=0.626 Loss=1.407 Prec@1=66.246 Prec@5=86.960 rate=1450.71 Hz, eta=0:00:00, total=0:00:00, wall=05:54 IST
** Validation 100.00% of 1x98...Epoch=75/150 LR=0.0510 Time=0.626 Loss=1.407 Prec@1=66.246 Prec@5=86.960 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=05:54 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST
=> Training   0.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=4.882 DataTime=4.388 Loss=1.415 Prec@1=66.602 Prec@5=85.938 rate=0 Hz, eta=?, total=0:00:00, wall=05:54 IST
=> Training   0.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=4.882 DataTime=4.388 Loss=1.415 Prec@1=66.602 Prec@5=85.938 rate=278.97 Hz, eta=0:00:08, total=0:00:00, wall=05:54 IST
=> Training   0.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=4.882 DataTime=4.388 Loss=1.415 Prec@1=66.602 Prec@5=85.938 rate=278.97 Hz, eta=0:00:08, total=0:00:00, wall=05:55 IST
=> Training   0.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.591 DataTime=0.431 Loss=1.227 Prec@1=69.947 Prec@5=88.912 rate=278.97 Hz, eta=0:00:08, total=0:00:00, wall=05:55 IST
=> Training   4.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.591 DataTime=0.431 Loss=1.227 Prec@1=69.947 Prec@5=88.912 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:55 IST
=> Training   4.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.591 DataTime=0.431 Loss=1.227 Prec@1=69.947 Prec@5=88.912 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:56 IST
=> Training   4.04% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.589 DataTime=0.427 Loss=1.224 Prec@1=69.971 Prec@5=88.955 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=05:56 IST
=> Training   8.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.589 DataTime=0.427 Loss=1.224 Prec@1=69.971 Prec@5=88.955 rate=1.77 Hz, eta=0:21:39, total=0:01:53, wall=05:56 IST
=> Training   8.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.589 DataTime=0.427 Loss=1.224 Prec@1=69.971 Prec@5=88.955 rate=1.77 Hz, eta=0:21:39, total=0:01:53, wall=05:57 IST
=> Training   8.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.573 DataTime=0.409 Loss=1.225 Prec@1=69.867 Prec@5=88.963 rate=1.77 Hz, eta=0:21:39, total=0:01:53, wall=05:57 IST
=> Training   12.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.573 DataTime=0.409 Loss=1.225 Prec@1=69.867 Prec@5=88.963 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=05:57 IST
=> Training   12.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.573 DataTime=0.409 Loss=1.225 Prec@1=69.867 Prec@5=88.963 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=05:58 IST
=> Training   12.03% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.570 DataTime=0.405 Loss=1.226 Prec@1=69.804 Prec@5=88.881 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=05:58 IST
=> Training   16.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.570 DataTime=0.405 Loss=1.226 Prec@1=69.804 Prec@5=88.881 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=05:58 IST
=> Training   16.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.570 DataTime=0.405 Loss=1.226 Prec@1=69.804 Prec@5=88.881 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=05:59 IST
=> Training   16.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.571 DataTime=0.404 Loss=1.227 Prec@1=69.753 Prec@5=88.906 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=05:59 IST
=> Training   20.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.571 DataTime=0.404 Loss=1.227 Prec@1=69.753 Prec@5=88.906 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=05:59 IST
=> Training   20.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.571 DataTime=0.404 Loss=1.227 Prec@1=69.753 Prec@5=88.906 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=06:00 IST
=> Training   20.02% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.569 DataTime=0.400 Loss=1.228 Prec@1=69.693 Prec@5=88.878 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=06:00 IST
=> Training   24.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.569 DataTime=0.400 Loss=1.228 Prec@1=69.693 Prec@5=88.878 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=06:00 IST
=> Training   24.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.569 DataTime=0.400 Loss=1.228 Prec@1=69.693 Prec@5=88.878 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=06:00 IST
=> Training   24.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.897 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=06:00 IST
=> Training   28.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.897 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:00 IST
=> Training   28.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.397 Loss=1.229 Prec@1=69.697 Prec@5=88.897 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:01 IST
=> Training   28.01% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.398 Loss=1.227 Prec@1=69.711 Prec@5=88.897 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:01 IST
=> Training   32.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.398 Loss=1.227 Prec@1=69.711 Prec@5=88.897 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:01 IST
=> Training   32.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.398 Loss=1.227 Prec@1=69.711 Prec@5=88.897 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:02 IST
=> Training   32.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.231 Prec@1=69.620 Prec@5=88.858 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:02 IST
=> Training   36.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.231 Prec@1=69.620 Prec@5=88.858 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:02 IST
=> Training   36.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.231 Prec@1=69.620 Prec@5=88.858 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:03 IST
=> Training   36.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.232 Prec@1=69.560 Prec@5=88.858 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:03 IST
=> Training   39.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.232 Prec@1=69.560 Prec@5=88.858 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=06:03 IST
=> Training   39.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.232 Prec@1=69.560 Prec@5=88.858 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=06:04 IST
=> Training   39.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.234 Prec@1=69.537 Prec@5=88.837 rate=1.78 Hz, eta=0:14:03, total=0:09:21, wall=06:04 IST
=> Training   43.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.234 Prec@1=69.537 Prec@5=88.837 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:04 IST
=> Training   43.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.234 Prec@1=69.537 Prec@5=88.837 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:05 IST
=> Training   43.99% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.236 Prec@1=69.503 Prec@5=88.827 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:05 IST
=> Training   47.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.236 Prec@1=69.503 Prec@5=88.827 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=06:05 IST
=> Training   47.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.236 Prec@1=69.503 Prec@5=88.827 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=06:06 IST
=> Training   47.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.396 Loss=1.237 Prec@1=69.472 Prec@5=88.815 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=06:06 IST
=> Training   51.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.396 Loss=1.237 Prec@1=69.472 Prec@5=88.815 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=06:06 IST
=> Training   51.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.568 DataTime=0.396 Loss=1.237 Prec@1=69.472 Prec@5=88.815 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=06:07 IST
=> Training   51.98% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.240 Prec@1=69.432 Prec@5=88.769 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=06:07 IST
=> Training   55.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.240 Prec@1=69.432 Prec@5=88.769 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=06:07 IST
=> Training   55.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.240 Prec@1=69.432 Prec@5=88.769 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=06:08 IST
=> Training   55.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.241 Prec@1=69.411 Prec@5=88.757 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=06:08 IST
=> Training   59.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.241 Prec@1=69.411 Prec@5=88.757 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=06:08 IST
=> Training   59.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.396 Loss=1.241 Prec@1=69.411 Prec@5=88.757 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=06:09 IST
=> Training   59.97% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.243 Prec@1=69.370 Prec@5=88.728 rate=1.77 Hz, eta=0:09:25, total=0:14:06, wall=06:09 IST
=> Training   63.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.243 Prec@1=69.370 Prec@5=88.728 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=06:09 IST
=> Training   63.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.243 Prec@1=69.370 Prec@5=88.728 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=06:10 IST
=> Training   63.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.244 Prec@1=69.331 Prec@5=88.712 rate=1.78 Hz, eta=0:08:27, total=0:15:01, wall=06:10 IST
=> Training   67.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.244 Prec@1=69.331 Prec@5=88.712 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=06:10 IST
=> Training   67.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.567 DataTime=0.395 Loss=1.244 Prec@1=69.331 Prec@5=88.712 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=06:11 IST
=> Training   67.96% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.245 Prec@1=69.314 Prec@5=88.696 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=06:11 IST
=> Training   71.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.245 Prec@1=69.314 Prec@5=88.696 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:11 IST
=> Training   71.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.245 Prec@1=69.314 Prec@5=88.696 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:12 IST
=> Training   71.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.246 Prec@1=69.292 Prec@5=88.687 rate=1.78 Hz, eta=0:06:35, total=0:16:53, wall=06:12 IST
=> Training   75.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.246 Prec@1=69.292 Prec@5=88.687 rate=1.78 Hz, eta=0:05:38, total=0:17:50, wall=06:12 IST
=> Training   75.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.246 Prec@1=69.292 Prec@5=88.687 rate=1.78 Hz, eta=0:05:38, total=0:17:50, wall=06:13 IST
=> Training   75.95% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.247 Prec@1=69.253 Prec@5=88.669 rate=1.78 Hz, eta=0:05:38, total=0:17:50, wall=06:13 IST
=> Training   79.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.247 Prec@1=69.253 Prec@5=88.669 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=06:13 IST
=> Training   79.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.247 Prec@1=69.253 Prec@5=88.669 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=06:14 IST
=> Training   79.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.248 Prec@1=69.224 Prec@5=88.655 rate=1.77 Hz, eta=0:04:42, total=0:18:47, wall=06:14 IST
=> Training   83.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.248 Prec@1=69.224 Prec@5=88.655 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=06:14 IST
=> Training   83.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.395 Loss=1.248 Prec@1=69.224 Prec@5=88.655 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=06:15 IST
=> Training   83.94% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.249 Prec@1=69.193 Prec@5=88.642 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=06:15 IST
=> Training   87.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.249 Prec@1=69.193 Prec@5=88.642 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=06:15 IST
=> Training   87.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.566 DataTime=0.394 Loss=1.249 Prec@1=69.193 Prec@5=88.642 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=06:16 IST
=> Training   87.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.251 Prec@1=69.176 Prec@5=88.629 rate=1.78 Hz, eta=0:02:50, total=0:20:39, wall=06:16 IST
=> Training   91.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.251 Prec@1=69.176 Prec@5=88.629 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=06:16 IST
=> Training   91.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.251 Prec@1=69.176 Prec@5=88.629 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=06:16 IST
=> Training   91.93% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.252 Prec@1=69.161 Prec@5=88.617 rate=1.78 Hz, eta=0:01:53, total=0:21:35, wall=06:16 IST
=> Training   95.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.252 Prec@1=69.161 Prec@5=88.617 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:16 IST
=> Training   95.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.252 Prec@1=69.161 Prec@5=88.617 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:17 IST
=> Training   95.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.253 Prec@1=69.142 Prec@5=88.603 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:17 IST
=> Training   99.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.253 Prec@1=69.142 Prec@5=88.603 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=06:17 IST
=> Training   99.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.253 Prec@1=69.142 Prec@5=88.603 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=06:17 IST
=> Training   99.92% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.253 Prec@1=69.142 Prec@5=88.602 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=06:17 IST
=> Training   100.00% of 1x2503...Epoch=76/150 LR=0.0500 Time=0.565 DataTime=0.394 Loss=1.253 Prec@1=69.142 Prec@5=88.602 rate=1.78 Hz, eta=0:00:00, total=0:23:29, wall=06:17 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST
=> Validation 0.00% of 1x98...Epoch=76/150 LR=0.0500 Time=6.846 Loss=0.882 Prec@1=76.367 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=06:18 IST
=> Validation 1.02% of 1x98...Epoch=76/150 LR=0.0500 Time=6.846 Loss=0.882 Prec@1=76.367 Prec@5=92.969 rate=7794.23 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST
** Validation 1.02% of 1x98...Epoch=76/150 LR=0.0500 Time=6.846 Loss=0.882 Prec@1=76.367 Prec@5=92.969 rate=7794.23 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST
** Validation 1.02% of 1x98...Epoch=76/150 LR=0.0500 Time=0.628 Loss=1.390 Prec@1=66.472 Prec@5=87.408 rate=7794.23 Hz, eta=0:00:00, total=0:00:00, wall=06:18 IST
** Validation 100.00% of 1x98...Epoch=76/150 LR=0.0500 Time=0.628 Loss=1.390 Prec@1=66.472 Prec@5=87.408 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=06:18 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:19 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:19 IST
=> Training   0.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=4.505 DataTime=4.053 Loss=1.181 Prec@1=71.680 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=06:19 IST
=> Training   0.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=4.505 DataTime=4.053 Loss=1.181 Prec@1=71.680 Prec@5=91.602 rate=605.23 Hz, eta=0:00:04, total=0:00:00, wall=06:19 IST
=> Training   0.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=4.505 DataTime=4.053 Loss=1.181 Prec@1=71.680 Prec@5=91.602 rate=605.23 Hz, eta=0:00:04, total=0:00:00, wall=06:19 IST
=> Training   0.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.591 DataTime=0.419 Loss=1.202 Prec@1=69.949 Prec@5=89.345 rate=605.23 Hz, eta=0:00:04, total=0:00:00, wall=06:19 IST
=> Training   4.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.591 DataTime=0.419 Loss=1.202 Prec@1=69.949 Prec@5=89.345 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=06:19 IST
=> Training   4.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.591 DataTime=0.419 Loss=1.202 Prec@1=69.949 Prec@5=89.345 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=06:20 IST
=> Training   4.04% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.403 Loss=1.203 Prec@1=69.937 Prec@5=89.337 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=06:20 IST
=> Training   8.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.403 Loss=1.203 Prec@1=69.937 Prec@5=89.337 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=06:20 IST
=> Training   8.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.403 Loss=1.203 Prec@1=69.937 Prec@5=89.337 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=06:21 IST
=> Training   8.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.402 Loss=1.209 Prec@1=69.981 Prec@5=89.177 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=06:21 IST
=> Training   12.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.402 Loss=1.209 Prec@1=69.981 Prec@5=89.177 rate=1.78 Hz, eta=0:20:36, total=0:02:49, wall=06:21 IST
=> Training   12.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.576 DataTime=0.402 Loss=1.209 Prec@1=69.981 Prec@5=89.177 rate=1.78 Hz, eta=0:20:36, total=0:02:49, wall=06:22 IST
=> Training   12.03% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.210 Prec@1=69.985 Prec@5=89.175 rate=1.78 Hz, eta=0:20:36, total=0:02:49, wall=06:22 IST
=> Training   16.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.210 Prec@1=69.985 Prec@5=89.175 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=06:22 IST
=> Training   16.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.210 Prec@1=69.985 Prec@5=89.175 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=06:23 IST
=> Training   16.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.209 Prec@1=70.019 Prec@5=89.201 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=06:23 IST
=> Training   20.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.209 Prec@1=70.019 Prec@5=89.201 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=06:23 IST
=> Training   20.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.570 DataTime=0.395 Loss=1.209 Prec@1=70.019 Prec@5=89.201 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=06:24 IST
=> Training   20.02% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.212 Prec@1=70.003 Prec@5=89.166 rate=1.78 Hz, eta=0:18:43, total=0:04:41, wall=06:24 IST
=> Training   24.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.212 Prec@1=70.003 Prec@5=89.166 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:24 IST
=> Training   24.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.212 Prec@1=70.003 Prec@5=89.166 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:25 IST
=> Training   24.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.214 Prec@1=69.950 Prec@5=89.134 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=06:25 IST
=> Training   28.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.214 Prec@1=69.950 Prec@5=89.134 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:25 IST
=> Training   28.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.567 DataTime=0.392 Loss=1.214 Prec@1=69.950 Prec@5=89.134 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:26 IST
=> Training   28.01% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.566 DataTime=0.391 Loss=1.218 Prec@1=69.887 Prec@5=89.072 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=06:26 IST
=> Training   32.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.566 DataTime=0.391 Loss=1.218 Prec@1=69.887 Prec@5=89.072 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=06:26 IST
=> Training   32.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.566 DataTime=0.391 Loss=1.218 Prec@1=69.887 Prec@5=89.072 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=06:27 IST
=> Training   32.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.221 Prec@1=69.832 Prec@5=89.041 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=06:27 IST
=> Training   36.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.221 Prec@1=69.832 Prec@5=89.041 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=06:27 IST
=> Training   36.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.221 Prec@1=69.832 Prec@5=89.041 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=06:28 IST
=> Training   36.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.223 Prec@1=69.765 Prec@5=89.000 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=06:28 IST
=> Training   39.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.223 Prec@1=69.765 Prec@5=89.000 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=06:28 IST
=> Training   39.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.223 Prec@1=69.765 Prec@5=89.000 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=06:29 IST
=> Training   39.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.224 Prec@1=69.749 Prec@5=88.984 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=06:29 IST
=> Training   43.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.224 Prec@1=69.749 Prec@5=88.984 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=06:29 IST
=> Training   43.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.388 Loss=1.224 Prec@1=69.749 Prec@5=88.984 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=06:30 IST
=> Training   43.99% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.388 Loss=1.226 Prec@1=69.686 Prec@5=88.945 rate=1.78 Hz, eta=0:13:05, total=0:10:17, wall=06:30 IST
=> Training   47.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.388 Loss=1.226 Prec@1=69.686 Prec@5=88.945 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:30 IST
=> Training   47.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.388 Loss=1.226 Prec@1=69.686 Prec@5=88.945 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:31 IST
=> Training   47.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.227 Prec@1=69.645 Prec@5=88.938 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:31 IST
=> Training   51.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.227 Prec@1=69.645 Prec@5=88.938 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=06:31 IST
=> Training   51.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.565 DataTime=0.389 Loss=1.227 Prec@1=69.645 Prec@5=88.938 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=06:32 IST
=> Training   51.98% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.229 Prec@1=69.616 Prec@5=88.919 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=06:32 IST
=> Training   55.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.229 Prec@1=69.616 Prec@5=88.919 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=06:32 IST
=> Training   55.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.229 Prec@1=69.616 Prec@5=88.919 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=06:33 IST
=> Training   55.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.230 Prec@1=69.595 Prec@5=88.890 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=06:33 IST
=> Training   59.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.230 Prec@1=69.595 Prec@5=88.890 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=06:33 IST
=> Training   59.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.564 DataTime=0.387 Loss=1.230 Prec@1=69.595 Prec@5=88.890 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=06:34 IST
=> Training   59.97% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.386 Loss=1.231 Prec@1=69.567 Prec@5=88.890 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=06:34 IST
=> Training   63.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.386 Loss=1.231 Prec@1=69.567 Prec@5=88.890 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=06:34 IST
=> Training   63.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.386 Loss=1.231 Prec@1=69.567 Prec@5=88.890 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=06:34 IST
=> Training   63.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.232 Prec@1=69.556 Prec@5=88.881 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=06:34 IST
=> Training   67.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.232 Prec@1=69.556 Prec@5=88.881 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=06:34 IST
=> Training   67.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.232 Prec@1=69.556 Prec@5=88.881 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=06:35 IST
=> Training   67.96% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.233 Prec@1=69.525 Prec@5=88.860 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=06:35 IST
=> Training   71.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.233 Prec@1=69.525 Prec@5=88.860 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=06:35 IST
=> Training   71.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.233 Prec@1=69.525 Prec@5=88.860 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=06:36 IST
=> Training   71.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.235 Prec@1=69.489 Prec@5=88.843 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=06:36 IST
=> Training   75.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.235 Prec@1=69.489 Prec@5=88.843 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=06:36 IST
=> Training   75.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.386 Loss=1.235 Prec@1=69.489 Prec@5=88.843 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=06:37 IST
=> Training   75.95% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.388 Loss=1.236 Prec@1=69.451 Prec@5=88.823 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=06:37 IST
=> Training   79.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.388 Loss=1.236 Prec@1=69.451 Prec@5=88.823 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=06:37 IST
=> Training   79.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.563 DataTime=0.388 Loss=1.236 Prec@1=69.451 Prec@5=88.823 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=06:38 IST
=> Training   79.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.387 Loss=1.238 Prec@1=69.430 Prec@5=88.813 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=06:38 IST
=> Training   83.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.387 Loss=1.238 Prec@1=69.430 Prec@5=88.813 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=06:38 IST
=> Training   83.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.387 Loss=1.238 Prec@1=69.430 Prec@5=88.813 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=06:39 IST
=> Training   83.94% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.239 Prec@1=69.403 Prec@5=88.796 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=06:39 IST
=> Training   87.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.239 Prec@1=69.403 Prec@5=88.796 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=06:39 IST
=> Training   87.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.239 Prec@1=69.403 Prec@5=88.796 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=06:40 IST
=> Training   87.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.240 Prec@1=69.381 Prec@5=88.782 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=06:40 IST
=> Training   91.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.240 Prec@1=69.381 Prec@5=88.782 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=06:40 IST
=> Training   91.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.240 Prec@1=69.381 Prec@5=88.782 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=06:41 IST
=> Training   91.93% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.241 Prec@1=69.353 Prec@5=88.767 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=06:41 IST
=> Training   95.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.241 Prec@1=69.353 Prec@5=88.767 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=06:41 IST
=> Training   95.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.562 DataTime=0.388 Loss=1.241 Prec@1=69.353 Prec@5=88.767 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=06:42 IST
=> Training   95.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.561 DataTime=0.387 Loss=1.242 Prec@1=69.321 Prec@5=88.748 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=06:42 IST
=> Training   99.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.561 DataTime=0.387 Loss=1.242 Prec@1=69.321 Prec@5=88.748 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=06:42 IST
=> Training   99.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.561 DataTime=0.387 Loss=1.242 Prec@1=69.321 Prec@5=88.748 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=06:42 IST
=> Training   99.92% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.561 DataTime=0.387 Loss=1.242 Prec@1=69.321 Prec@5=88.748 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=06:42 IST
=> Training   100.00% of 1x2503...Epoch=77/150 LR=0.0490 Time=0.561 DataTime=0.387 Loss=1.242 Prec@1=69.321 Prec@5=88.748 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=06:42 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST
=> Validation 0.00% of 1x98...Epoch=77/150 LR=0.0490 Time=6.815 Loss=0.939 Prec@1=77.148 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=06:42 IST
=> Validation 1.02% of 1x98...Epoch=77/150 LR=0.0490 Time=6.815 Loss=0.939 Prec@1=77.148 Prec@5=92.773 rate=6038.14 Hz, eta=0:00:00, total=0:00:00, wall=06:42 IST
** Validation 1.02% of 1x98...Epoch=77/150 LR=0.0490 Time=6.815 Loss=0.939 Prec@1=77.148 Prec@5=92.773 rate=6038.14 Hz, eta=0:00:00, total=0:00:00, wall=06:43 IST
** Validation 1.02% of 1x98...Epoch=77/150 LR=0.0490 Time=0.638 Loss=1.404 Prec@1=65.832 Prec@5=87.154 rate=6038.14 Hz, eta=0:00:00, total=0:00:00, wall=06:43 IST
** Validation 100.00% of 1x98...Epoch=77/150 LR=0.0490 Time=0.638 Loss=1.404 Prec@1=65.832 Prec@5=87.154 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=06:43 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST
=> Training   0.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=4.786 DataTime=4.447 Loss=1.109 Prec@1=72.656 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=06:43 IST
=> Training   0.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=4.786 DataTime=4.447 Loss=1.109 Prec@1=72.656 Prec@5=90.820 rate=394.55 Hz, eta=0:00:06, total=0:00:00, wall=06:43 IST
=> Training   0.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=4.786 DataTime=4.447 Loss=1.109 Prec@1=72.656 Prec@5=90.820 rate=394.55 Hz, eta=0:00:06, total=0:00:00, wall=06:44 IST
=> Training   0.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.588 DataTime=0.416 Loss=1.204 Prec@1=70.019 Prec@5=89.260 rate=394.55 Hz, eta=0:00:06, total=0:00:00, wall=06:44 IST
=> Training   4.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.588 DataTime=0.416 Loss=1.204 Prec@1=70.019 Prec@5=89.260 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=06:44 IST
=> Training   4.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.588 DataTime=0.416 Loss=1.204 Prec@1=70.019 Prec@5=89.260 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=06:45 IST
=> Training   4.04% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.572 DataTime=0.399 Loss=1.198 Prec@1=70.203 Prec@5=89.344 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=06:45 IST
=> Training   8.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.572 DataTime=0.399 Loss=1.198 Prec@1=70.203 Prec@5=89.344 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=06:45 IST
=> Training   8.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.572 DataTime=0.399 Loss=1.198 Prec@1=70.203 Prec@5=89.344 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=06:46 IST
=> Training   8.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.395 Loss=1.201 Prec@1=70.213 Prec@5=89.260 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=06:46 IST
=> Training   12.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.395 Loss=1.201 Prec@1=70.213 Prec@5=89.260 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=06:46 IST
=> Training   12.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.395 Loss=1.201 Prec@1=70.213 Prec@5=89.260 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=06:47 IST
=> Training   12.03% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.391 Loss=1.204 Prec@1=70.200 Prec@5=89.207 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=06:47 IST
=> Training   16.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.391 Loss=1.204 Prec@1=70.200 Prec@5=89.207 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:47 IST
=> Training   16.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.391 Loss=1.204 Prec@1=70.200 Prec@5=89.207 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:48 IST
=> Training   16.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.566 DataTime=0.392 Loss=1.208 Prec@1=70.120 Prec@5=89.158 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:48 IST
=> Training   20.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.566 DataTime=0.392 Loss=1.208 Prec@1=70.120 Prec@5=89.158 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=06:48 IST
=> Training   20.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.566 DataTime=0.392 Loss=1.208 Prec@1=70.120 Prec@5=89.158 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=06:49 IST
=> Training   20.02% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.210 Prec@1=70.092 Prec@5=89.155 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=06:49 IST
=> Training   24.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.210 Prec@1=70.092 Prec@5=89.155 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=06:49 IST
=> Training   24.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.210 Prec@1=70.092 Prec@5=89.155 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=06:50 IST
=> Training   24.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.568 DataTime=0.394 Loss=1.213 Prec@1=70.036 Prec@5=89.112 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=06:50 IST
=> Training   28.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.568 DataTime=0.394 Loss=1.213 Prec@1=70.036 Prec@5=89.112 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=06:50 IST
=> Training   28.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.568 DataTime=0.394 Loss=1.213 Prec@1=70.036 Prec@5=89.112 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=06:51 IST
=> Training   28.01% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.393 Loss=1.215 Prec@1=69.993 Prec@5=89.092 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=06:51 IST
=> Training   32.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.393 Loss=1.215 Prec@1=69.993 Prec@5=89.092 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:51 IST
=> Training   32.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.393 Loss=1.215 Prec@1=69.993 Prec@5=89.092 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:51 IST
=> Training   32.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.215 Prec@1=69.987 Prec@5=89.086 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:51 IST
=> Training   36.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.215 Prec@1=69.987 Prec@5=89.086 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:51 IST
=> Training   36.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.567 DataTime=0.394 Loss=1.215 Prec@1=69.987 Prec@5=89.086 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:52 IST
=> Training   36.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.391 Loss=1.218 Prec@1=69.933 Prec@5=89.051 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=06:52 IST
=> Training   39.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.391 Loss=1.218 Prec@1=69.933 Prec@5=89.051 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=06:52 IST
=> Training   39.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.391 Loss=1.218 Prec@1=69.933 Prec@5=89.051 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=06:53 IST
=> Training   39.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.390 Loss=1.220 Prec@1=69.876 Prec@5=89.012 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=06:53 IST
=> Training   43.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.390 Loss=1.220 Prec@1=69.876 Prec@5=89.012 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=06:53 IST
=> Training   43.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.565 DataTime=0.390 Loss=1.220 Prec@1=69.876 Prec@5=89.012 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=06:54 IST
=> Training   43.99% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.564 DataTime=0.389 Loss=1.221 Prec@1=69.867 Prec@5=89.011 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=06:54 IST
=> Training   47.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.564 DataTime=0.389 Loss=1.221 Prec@1=69.867 Prec@5=89.011 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=06:54 IST
=> Training   47.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.564 DataTime=0.389 Loss=1.221 Prec@1=69.867 Prec@5=89.011 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=06:55 IST
=> Training   47.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.849 Prec@5=89.006 rate=1.78 Hz, eta=0:12:09, total=0:11:12, wall=06:55 IST
=> Training   51.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.849 Prec@5=89.006 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=06:55 IST
=> Training   51.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.849 Prec@5=89.006 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=06:56 IST
=> Training   51.98% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.835 Prec@5=89.002 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=06:56 IST
=> Training   55.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.835 Prec@5=89.002 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=06:56 IST
=> Training   55.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.222 Prec@1=69.835 Prec@5=89.002 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=06:57 IST
=> Training   55.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.224 Prec@1=69.794 Prec@5=88.977 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=06:57 IST
=> Training   59.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.224 Prec@1=69.794 Prec@5=88.977 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=06:57 IST
=> Training   59.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.563 DataTime=0.388 Loss=1.224 Prec@1=69.794 Prec@5=88.977 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=06:58 IST
=> Training   59.97% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.226 Prec@1=69.753 Prec@5=88.959 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=06:58 IST
=> Training   63.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.226 Prec@1=69.753 Prec@5=88.959 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=06:58 IST
=> Training   63.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.226 Prec@1=69.753 Prec@5=88.959 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=06:59 IST
=> Training   63.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.227 Prec@1=69.723 Prec@5=88.932 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=06:59 IST
=> Training   67.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.227 Prec@1=69.723 Prec@5=88.932 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=06:59 IST
=> Training   67.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.227 Prec@1=69.723 Prec@5=88.932 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=07:00 IST
=> Training   67.96% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.228 Prec@1=69.678 Prec@5=88.920 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=07:00 IST
=> Training   71.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.228 Prec@1=69.678 Prec@5=88.920 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:00 IST
=> Training   71.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.228 Prec@1=69.678 Prec@5=88.920 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:01 IST
=> Training   71.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.386 Loss=1.229 Prec@1=69.657 Prec@5=88.906 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:01 IST
=> Training   75.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.386 Loss=1.229 Prec@1=69.657 Prec@5=88.906 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:01 IST
=> Training   75.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.386 Loss=1.229 Prec@1=69.657 Prec@5=88.906 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:02 IST
=> Training   75.95% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.231 Prec@1=69.626 Prec@5=88.893 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:02 IST
=> Training   79.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.231 Prec@1=69.626 Prec@5=88.893 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:02 IST
=> Training   79.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.562 DataTime=0.386 Loss=1.231 Prec@1=69.626 Prec@5=88.893 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:03 IST
=> Training   79.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.233 Prec@1=69.584 Prec@5=88.876 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:03 IST
=> Training   83.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.233 Prec@1=69.584 Prec@5=88.876 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=07:03 IST
=> Training   83.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.233 Prec@1=69.584 Prec@5=88.876 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=07:04 IST
=> Training   83.94% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.234 Prec@1=69.552 Prec@5=88.856 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=07:04 IST
=> Training   87.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.234 Prec@1=69.552 Prec@5=88.856 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:04 IST
=> Training   87.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.561 DataTime=0.385 Loss=1.234 Prec@1=69.552 Prec@5=88.856 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:04 IST
=> Training   87.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.235 Prec@1=69.533 Prec@5=88.839 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:04 IST
=> Training   91.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.235 Prec@1=69.533 Prec@5=88.839 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=07:04 IST
=> Training   91.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.235 Prec@1=69.533 Prec@5=88.839 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=07:05 IST
=> Training   91.93% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.237 Prec@1=69.505 Prec@5=88.819 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=07:05 IST
=> Training   95.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.237 Prec@1=69.505 Prec@5=88.819 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=07:05 IST
=> Training   95.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.385 Loss=1.237 Prec@1=69.505 Prec@5=88.819 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=07:06 IST
=> Training   95.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.384 Loss=1.238 Prec@1=69.471 Prec@5=88.806 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=07:06 IST
=> Training   99.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.384 Loss=1.238 Prec@1=69.471 Prec@5=88.806 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=07:06 IST
=> Training   99.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.384 Loss=1.238 Prec@1=69.471 Prec@5=88.806 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=07:06 IST
=> Training   99.92% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.384 Loss=1.238 Prec@1=69.469 Prec@5=88.806 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=07:06 IST
=> Training   100.00% of 1x2503...Epoch=78/150 LR=0.0479 Time=0.560 DataTime=0.384 Loss=1.238 Prec@1=69.469 Prec@5=88.806 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=07:06 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> Validation 0.00% of 1x98...Epoch=78/150 LR=0.0479 Time=6.866 Loss=0.929 Prec@1=76.172 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=07:06 IST
=> Validation 1.02% of 1x98...Epoch=78/150 LR=0.0479 Time=6.866 Loss=0.929 Prec@1=76.172 Prec@5=92.578 rate=7165.89 Hz, eta=0:00:00, total=0:00:00, wall=07:06 IST
** Validation 1.02% of 1x98...Epoch=78/150 LR=0.0479 Time=6.866 Loss=0.929 Prec@1=76.172 Prec@5=92.578 rate=7165.89 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST
** Validation 1.02% of 1x98...Epoch=78/150 LR=0.0479 Time=0.635 Loss=1.411 Prec@1=65.848 Prec@5=86.856 rate=7165.89 Hz, eta=0:00:00, total=0:00:00, wall=07:07 IST
** Validation 100.00% of 1x98...Epoch=78/150 LR=0.0479 Time=0.635 Loss=1.411 Prec@1=65.848 Prec@5=86.856 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=07:07 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> Training   0.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=4.219 DataTime=3.884 Loss=1.203 Prec@1=72.070 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=07:07 IST
=> Training   0.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=4.219 DataTime=3.884 Loss=1.203 Prec@1=72.070 Prec@5=88.672 rate=750.11 Hz, eta=0:00:03, total=0:00:00, wall=07:07 IST
=> Training   0.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=4.219 DataTime=3.884 Loss=1.203 Prec@1=72.070 Prec@5=88.672 rate=750.11 Hz, eta=0:00:03, total=0:00:00, wall=07:08 IST
=> Training   0.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.583 DataTime=0.409 Loss=1.196 Prec@1=70.314 Prec@5=89.482 rate=750.11 Hz, eta=0:00:03, total=0:00:00, wall=07:08 IST
=> Training   4.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.583 DataTime=0.409 Loss=1.196 Prec@1=70.314 Prec@5=89.482 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=07:08 IST
=> Training   4.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.583 DataTime=0.409 Loss=1.196 Prec@1=70.314 Prec@5=89.482 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=07:09 IST
=> Training   4.04% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.389 Loss=1.189 Prec@1=70.406 Prec@5=89.505 rate=1.85 Hz, eta=0:21:41, total=0:00:54, wall=07:09 IST
=> Training   8.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.389 Loss=1.189 Prec@1=70.406 Prec@5=89.505 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=07:09 IST
=> Training   8.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.389 Loss=1.189 Prec@1=70.406 Prec@5=89.505 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=07:10 IST
=> Training   8.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.393 Loss=1.192 Prec@1=70.392 Prec@5=89.430 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=07:10 IST
=> Training   12.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.393 Loss=1.192 Prec@1=70.392 Prec@5=89.430 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=07:10 IST
=> Training   12.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.568 DataTime=0.393 Loss=1.192 Prec@1=70.392 Prec@5=89.430 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=07:11 IST
=> Training   12.03% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.392 Loss=1.194 Prec@1=70.301 Prec@5=89.437 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=07:11 IST
=> Training   16.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.392 Loss=1.194 Prec@1=70.301 Prec@5=89.437 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=07:11 IST
=> Training   16.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.392 Loss=1.194 Prec@1=70.301 Prec@5=89.437 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=07:12 IST
=> Training   16.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.393 Loss=1.195 Prec@1=70.289 Prec@5=89.397 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=07:12 IST
=> Training   20.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.393 Loss=1.195 Prec@1=70.289 Prec@5=89.397 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=07:12 IST
=> Training   20.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.565 DataTime=0.393 Loss=1.195 Prec@1=70.289 Prec@5=89.397 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=07:13 IST
=> Training   20.02% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.390 Loss=1.198 Prec@1=70.216 Prec@5=89.386 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=07:13 IST
=> Training   24.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.390 Loss=1.198 Prec@1=70.216 Prec@5=89.386 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=07:13 IST
=> Training   24.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.390 Loss=1.198 Prec@1=70.216 Prec@5=89.386 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=07:14 IST
=> Training   24.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.389 Loss=1.199 Prec@1=70.226 Prec@5=89.363 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=07:14 IST
=> Training   28.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.389 Loss=1.199 Prec@1=70.226 Prec@5=89.363 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:14 IST
=> Training   28.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.564 DataTime=0.389 Loss=1.199 Prec@1=70.226 Prec@5=89.363 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:15 IST
=> Training   28.01% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.562 DataTime=0.386 Loss=1.202 Prec@1=70.166 Prec@5=89.306 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:15 IST
=> Training   32.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.562 DataTime=0.386 Loss=1.202 Prec@1=70.166 Prec@5=89.306 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=07:15 IST
=> Training   32.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.562 DataTime=0.386 Loss=1.202 Prec@1=70.166 Prec@5=89.306 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=07:16 IST
=> Training   32.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.203 Prec@1=70.170 Prec@5=89.290 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=07:16 IST
=> Training   36.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.203 Prec@1=70.170 Prec@5=89.290 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=07:16 IST
=> Training   36.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.203 Prec@1=70.170 Prec@5=89.290 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=07:17 IST
=> Training   36.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.206 Prec@1=70.090 Prec@5=89.250 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=07:17 IST
=> Training   39.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.206 Prec@1=70.090 Prec@5=89.250 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=07:17 IST
=> Training   39.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.385 Loss=1.206 Prec@1=70.090 Prec@5=89.250 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=07:18 IST
=> Training   39.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.384 Loss=1.207 Prec@1=70.071 Prec@5=89.240 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=07:18 IST
=> Training   43.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.384 Loss=1.207 Prec@1=70.071 Prec@5=89.240 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=07:18 IST
=> Training   43.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.561 DataTime=0.384 Loss=1.207 Prec@1=70.071 Prec@5=89.240 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=07:19 IST
=> Training   43.99% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.209 Prec@1=70.018 Prec@5=89.214 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=07:19 IST
=> Training   47.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.209 Prec@1=70.018 Prec@5=89.214 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=07:19 IST
=> Training   47.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.209 Prec@1=70.018 Prec@5=89.214 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=07:20 IST
=> Training   47.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.211 Prec@1=69.966 Prec@5=89.179 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=07:20 IST
=> Training   51.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.211 Prec@1=69.966 Prec@5=89.179 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=07:20 IST
=> Training   51.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.384 Loss=1.211 Prec@1=69.966 Prec@5=89.179 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=07:20 IST
=> Training   51.98% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.383 Loss=1.212 Prec@1=69.927 Prec@5=89.158 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=07:20 IST
=> Training   55.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.383 Loss=1.212 Prec@1=69.927 Prec@5=89.158 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:20 IST
=> Training   55.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.383 Loss=1.212 Prec@1=69.927 Prec@5=89.158 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:21 IST
=> Training   55.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.215 Prec@1=69.886 Prec@5=89.134 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=07:21 IST
=> Training   59.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.215 Prec@1=69.886 Prec@5=89.134 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=07:21 IST
=> Training   59.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.215 Prec@1=69.886 Prec@5=89.134 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=07:22 IST
=> Training   59.97% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.216 Prec@1=69.832 Prec@5=89.114 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=07:22 IST
=> Training   63.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.216 Prec@1=69.832 Prec@5=89.114 rate=1.79 Hz, eta=0:08:22, total=0:14:51, wall=07:22 IST
=> Training   63.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.216 Prec@1=69.832 Prec@5=89.114 rate=1.79 Hz, eta=0:08:22, total=0:14:51, wall=07:23 IST
=> Training   63.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.218 Prec@1=69.810 Prec@5=89.081 rate=1.79 Hz, eta=0:08:22, total=0:14:51, wall=07:23 IST
=> Training   67.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.218 Prec@1=69.810 Prec@5=89.081 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=07:23 IST
=> Training   67.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.560 DataTime=0.383 Loss=1.218 Prec@1=69.810 Prec@5=89.081 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=07:24 IST
=> Training   67.96% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.219 Prec@1=69.777 Prec@5=89.051 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=07:24 IST
=> Training   71.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.219 Prec@1=69.777 Prec@5=89.051 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=07:24 IST
=> Training   71.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.219 Prec@1=69.777 Prec@5=89.051 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=07:25 IST
=> Training   71.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.221 Prec@1=69.749 Prec@5=89.023 rate=1.80 Hz, eta=0:06:30, total=0:16:41, wall=07:25 IST
=> Training   75.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.221 Prec@1=69.749 Prec@5=89.023 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=07:25 IST
=> Training   75.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.221 Prec@1=69.749 Prec@5=89.023 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=07:26 IST
=> Training   75.95% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.222 Prec@1=69.729 Prec@5=89.005 rate=1.80 Hz, eta=0:05:34, total=0:17:37, wall=07:26 IST
=> Training   79.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.222 Prec@1=69.729 Prec@5=89.005 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=07:26 IST
=> Training   79.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.222 Prec@1=69.729 Prec@5=89.005 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=07:27 IST
=> Training   79.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.224 Prec@1=69.693 Prec@5=88.978 rate=1.80 Hz, eta=0:04:39, total=0:18:32, wall=07:27 IST
=> Training   83.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.224 Prec@1=69.693 Prec@5=88.978 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=07:27 IST
=> Training   83.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.224 Prec@1=69.693 Prec@5=88.978 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=07:28 IST
=> Training   83.94% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.226 Prec@1=69.664 Prec@5=88.956 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=07:28 IST
=> Training   87.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.226 Prec@1=69.664 Prec@5=88.956 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=07:28 IST
=> Training   87.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.226 Prec@1=69.664 Prec@5=88.956 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=07:29 IST
=> Training   87.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.227 Prec@1=69.641 Prec@5=88.949 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=07:29 IST
=> Training   91.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.227 Prec@1=69.641 Prec@5=88.949 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=07:29 IST
=> Training   91.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.382 Loss=1.227 Prec@1=69.641 Prec@5=88.949 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=07:30 IST
=> Training   91.93% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.228 Prec@1=69.630 Prec@5=88.941 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=07:30 IST
=> Training   95.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.228 Prec@1=69.630 Prec@5=88.941 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=07:30 IST
=> Training   95.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.559 DataTime=0.381 Loss=1.228 Prec@1=69.630 Prec@5=88.941 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=07:31 IST
=> Training   95.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.229 Prec@1=69.592 Prec@5=88.915 rate=1.80 Hz, eta=0:00:56, total=0:22:17, wall=07:31 IST
=> Training   99.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.229 Prec@1=69.592 Prec@5=88.915 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=07:31 IST
=> Training   99.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.229 Prec@1=69.592 Prec@5=88.915 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=07:31 IST
=> Training   99.92% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.229 Prec@1=69.591 Prec@5=88.914 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=07:31 IST
=> Training   100.00% of 1x2503...Epoch=79/150 LR=0.0469 Time=0.558 DataTime=0.381 Loss=1.229 Prec@1=69.591 Prec@5=88.914 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=07:31 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST
=> Validation 0.00% of 1x98...Epoch=79/150 LR=0.0469 Time=6.926 Loss=0.851 Prec@1=78.320 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=07:31 IST
=> Validation 1.02% of 1x98...Epoch=79/150 LR=0.0469 Time=6.926 Loss=0.851 Prec@1=78.320 Prec@5=92.969 rate=6017.61 Hz, eta=0:00:00, total=0:00:00, wall=07:31 IST
** Validation 1.02% of 1x98...Epoch=79/150 LR=0.0469 Time=6.926 Loss=0.851 Prec@1=78.320 Prec@5=92.969 rate=6017.61 Hz, eta=0:00:00, total=0:00:00, wall=07:32 IST
** Validation 1.02% of 1x98...Epoch=79/150 LR=0.0469 Time=0.638 Loss=1.399 Prec@1=66.130 Prec@5=87.062 rate=6017.61 Hz, eta=0:00:00, total=0:00:00, wall=07:32 IST
** Validation 100.00% of 1x98...Epoch=79/150 LR=0.0469 Time=0.638 Loss=1.399 Prec@1=66.130 Prec@5=87.062 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=07:32 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:32 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:32 IST
=> Training   0.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=4.661 DataTime=4.124 Loss=1.225 Prec@1=72.070 Prec@5=87.891 rate=0 Hz, eta=?, total=0:00:00, wall=07:32 IST
=> Training   0.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=4.661 DataTime=4.124 Loss=1.225 Prec@1=72.070 Prec@5=87.891 rate=459.67 Hz, eta=0:00:05, total=0:00:00, wall=07:32 IST
=> Training   0.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=4.661 DataTime=4.124 Loss=1.225 Prec@1=72.070 Prec@5=87.891 rate=459.67 Hz, eta=0:00:05, total=0:00:00, wall=07:33 IST
=> Training   0.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.588 DataTime=0.421 Loss=1.185 Prec@1=70.485 Prec@5=89.469 rate=459.67 Hz, eta=0:00:05, total=0:00:00, wall=07:33 IST
=> Training   4.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.588 DataTime=0.421 Loss=1.185 Prec@1=70.485 Prec@5=89.469 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=07:33 IST
=> Training   4.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.588 DataTime=0.421 Loss=1.185 Prec@1=70.485 Prec@5=89.469 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=07:34 IST
=> Training   4.04% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.568 DataTime=0.402 Loss=1.185 Prec@1=70.533 Prec@5=89.489 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=07:34 IST
=> Training   8.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.568 DataTime=0.402 Loss=1.185 Prec@1=70.533 Prec@5=89.489 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=07:34 IST
=> Training   8.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.568 DataTime=0.402 Loss=1.185 Prec@1=70.533 Prec@5=89.489 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=07:35 IST
=> Training   8.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.563 DataTime=0.396 Loss=1.188 Prec@1=70.571 Prec@5=89.476 rate=1.83 Hz, eta=0:20:54, total=0:01:49, wall=07:35 IST
=> Training   12.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.563 DataTime=0.396 Loss=1.188 Prec@1=70.571 Prec@5=89.476 rate=1.82 Hz, eta=0:20:06, total=0:02:44, wall=07:35 IST
=> Training   12.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.563 DataTime=0.396 Loss=1.188 Prec@1=70.571 Prec@5=89.476 rate=1.82 Hz, eta=0:20:06, total=0:02:44, wall=07:35 IST
=> Training   12.03% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.394 Loss=1.192 Prec@1=70.462 Prec@5=89.399 rate=1.82 Hz, eta=0:20:06, total=0:02:44, wall=07:35 IST
=> Training   16.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.394 Loss=1.192 Prec@1=70.462 Prec@5=89.399 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=07:35 IST
=> Training   16.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.394 Loss=1.192 Prec@1=70.462 Prec@5=89.399 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=07:36 IST
=> Training   16.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.391 Loss=1.196 Prec@1=70.374 Prec@5=89.356 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=07:36 IST
=> Training   20.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.391 Loss=1.196 Prec@1=70.374 Prec@5=89.356 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=07:36 IST
=> Training   20.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.391 Loss=1.196 Prec@1=70.374 Prec@5=89.356 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=07:37 IST
=> Training   20.02% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.390 Loss=1.198 Prec@1=70.314 Prec@5=89.312 rate=1.81 Hz, eta=0:18:24, total=0:04:36, wall=07:37 IST
=> Training   24.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.390 Loss=1.198 Prec@1=70.314 Prec@5=89.312 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:37 IST
=> Training   24.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.390 Loss=1.198 Prec@1=70.314 Prec@5=89.312 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:38 IST
=> Training   24.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.199 Prec@1=70.305 Prec@5=89.295 rate=1.81 Hz, eta=0:17:30, total=0:05:31, wall=07:38 IST
=> Training   28.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.199 Prec@1=70.305 Prec@5=89.295 rate=1.81 Hz, eta=0:16:35, total=0:06:27, wall=07:38 IST
=> Training   28.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.199 Prec@1=70.305 Prec@5=89.295 rate=1.81 Hz, eta=0:16:35, total=0:06:27, wall=07:39 IST
=> Training   28.01% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.558 DataTime=0.388 Loss=1.203 Prec@1=70.208 Prec@5=89.239 rate=1.81 Hz, eta=0:16:35, total=0:06:27, wall=07:39 IST
=> Training   32.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.558 DataTime=0.388 Loss=1.203 Prec@1=70.208 Prec@5=89.239 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=07:39 IST
=> Training   32.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.558 DataTime=0.388 Loss=1.203 Prec@1=70.208 Prec@5=89.239 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=07:40 IST
=> Training   32.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.204 Prec@1=70.185 Prec@5=89.226 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=07:40 IST
=> Training   36.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.204 Prec@1=70.185 Prec@5=89.226 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=07:40 IST
=> Training   36.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.388 Loss=1.204 Prec@1=70.185 Prec@5=89.226 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=07:41 IST
=> Training   36.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.154 Prec@5=89.207 rate=1.81 Hz, eta=0:14:46, total=0:08:18, wall=07:41 IST
=> Training   39.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.154 Prec@5=89.207 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=07:41 IST
=> Training   39.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.154 Prec@5=89.207 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=07:42 IST
=> Training   39.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.148 Prec@5=89.197 rate=1.80 Hz, eta=0:13:52, total=0:09:14, wall=07:42 IST
=> Training   43.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.148 Prec@5=89.197 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=07:42 IST
=> Training   43.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.559 DataTime=0.389 Loss=1.206 Prec@1=70.148 Prec@5=89.197 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=07:43 IST
=> Training   43.99% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.390 Loss=1.208 Prec@1=70.115 Prec@5=89.179 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=07:43 IST
=> Training   47.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.390 Loss=1.208 Prec@1=70.115 Prec@5=89.179 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=07:43 IST
=> Training   47.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.390 Loss=1.208 Prec@1=70.115 Prec@5=89.179 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=07:44 IST
=> Training   47.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.208 Prec@1=70.119 Prec@5=89.182 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=07:44 IST
=> Training   51.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.208 Prec@1=70.119 Prec@5=89.182 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=07:44 IST
=> Training   51.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.208 Prec@1=70.119 Prec@5=89.182 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=07:45 IST
=> Training   51.98% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.209 Prec@1=70.095 Prec@5=89.156 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=07:45 IST
=> Training   55.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.209 Prec@1=70.095 Prec@5=89.156 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=07:45 IST
=> Training   55.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.560 DataTime=0.389 Loss=1.209 Prec@1=70.095 Prec@5=89.156 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=07:46 IST
=> Training   55.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.210 Prec@1=70.072 Prec@5=89.141 rate=1.79 Hz, eta=0:10:13, total=0:13:00, wall=07:46 IST
=> Training   59.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.210 Prec@1=70.072 Prec@5=89.141 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=07:46 IST
=> Training   59.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.210 Prec@1=70.072 Prec@5=89.141 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=07:47 IST
=> Training   59.97% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.212 Prec@1=70.039 Prec@5=89.127 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=07:47 IST
=> Training   63.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.212 Prec@1=70.039 Prec@5=89.127 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:47 IST
=> Training   63.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.212 Prec@1=70.039 Prec@5=89.127 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:48 IST
=> Training   63.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.214 Prec@1=69.989 Prec@5=89.104 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:48 IST
=> Training   67.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.214 Prec@1=69.989 Prec@5=89.104 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:48 IST
=> Training   67.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.214 Prec@1=69.989 Prec@5=89.104 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:49 IST
=> Training   67.96% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.216 Prec@1=69.960 Prec@5=89.089 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:49 IST
=> Training   71.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.216 Prec@1=69.960 Prec@5=89.089 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:49 IST
=> Training   71.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.216 Prec@1=69.960 Prec@5=89.089 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:49 IST
=> Training   71.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.216 Prec@1=69.932 Prec@5=89.087 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:49 IST
=> Training   75.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.216 Prec@1=69.932 Prec@5=89.087 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:49 IST
=> Training   75.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.216 Prec@1=69.932 Prec@5=89.087 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:50 IST
=> Training   75.95% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.217 Prec@1=69.931 Prec@5=89.082 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=07:50 IST
=> Training   79.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.217 Prec@1=69.931 Prec@5=89.082 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:50 IST
=> Training   79.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.217 Prec@1=69.931 Prec@5=89.082 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:51 IST
=> Training   79.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.218 Prec@1=69.889 Prec@5=89.068 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:51 IST
=> Training   83.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.218 Prec@1=69.889 Prec@5=89.068 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=07:51 IST
=> Training   83.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.218 Prec@1=69.889 Prec@5=89.068 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=07:52 IST
=> Training   83.94% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.219 Prec@1=69.870 Prec@5=89.044 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=07:52 IST
=> Training   87.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.219 Prec@1=69.870 Prec@5=89.044 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=07:52 IST
=> Training   87.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.219 Prec@1=69.870 Prec@5=89.044 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=07:53 IST
=> Training   87.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.221 Prec@1=69.842 Prec@5=89.031 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=07:53 IST
=> Training   91.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.221 Prec@1=69.842 Prec@5=89.031 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:53 IST
=> Training   91.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.562 DataTime=0.389 Loss=1.221 Prec@1=69.842 Prec@5=89.031 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:54 IST
=> Training   91.93% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.222 Prec@1=69.820 Prec@5=89.017 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:54 IST
=> Training   95.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.222 Prec@1=69.820 Prec@5=89.017 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=07:54 IST
=> Training   95.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.389 Loss=1.222 Prec@1=69.820 Prec@5=89.017 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=07:55 IST
=> Training   95.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.223 Prec@1=69.790 Prec@5=89.001 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=07:55 IST
=> Training   99.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.223 Prec@1=69.790 Prec@5=89.001 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=07:55 IST
=> Training   99.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.223 Prec@1=69.790 Prec@5=89.001 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=07:55 IST
=> Training   99.92% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.223 Prec@1=69.792 Prec@5=89.003 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=07:55 IST
=> Training   100.00% of 1x2503...Epoch=80/150 LR=0.0458 Time=0.561 DataTime=0.388 Loss=1.223 Prec@1=69.792 Prec@5=89.003 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=07:55 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST
=> Validation 0.00% of 1x98...Epoch=80/150 LR=0.0458 Time=6.946 Loss=0.905 Prec@1=77.148 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=07:55 IST
=> Validation 1.02% of 1x98...Epoch=80/150 LR=0.0458 Time=6.946 Loss=0.905 Prec@1=77.148 Prec@5=93.555 rate=6231.77 Hz, eta=0:00:00, total=0:00:00, wall=07:55 IST
** Validation 1.02% of 1x98...Epoch=80/150 LR=0.0458 Time=6.946 Loss=0.905 Prec@1=77.148 Prec@5=93.555 rate=6231.77 Hz, eta=0:00:00, total=0:00:00, wall=07:56 IST
** Validation 1.02% of 1x98...Epoch=80/150 LR=0.0458 Time=0.632 Loss=1.380 Prec@1=66.826 Prec@5=87.394 rate=6231.77 Hz, eta=0:00:00, total=0:00:00, wall=07:56 IST
** Validation 100.00% of 1x98...Epoch=80/150 LR=0.0458 Time=0.632 Loss=1.380 Prec@1=66.826 Prec@5=87.394 rate=1.78 Hz, eta=0:00:00, total=0:00:54, wall=07:56 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:56 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:56 IST
=> Training   0.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=4.769 DataTime=4.302 Loss=1.092 Prec@1=71.289 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=07:56 IST
=> Training   0.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=4.769 DataTime=4.302 Loss=1.092 Prec@1=71.289 Prec@5=90.430 rate=4332.25 Hz, eta=0:00:00, total=0:00:00, wall=07:56 IST
=> Training   0.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=4.769 DataTime=4.302 Loss=1.092 Prec@1=71.289 Prec@5=90.430 rate=4332.25 Hz, eta=0:00:00, total=0:00:00, wall=07:57 IST
=> Training   0.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.598 DataTime=0.429 Loss=1.184 Prec@1=70.626 Prec@5=89.546 rate=4332.25 Hz, eta=0:00:00, total=0:00:00, wall=07:57 IST
=> Training   4.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.598 DataTime=0.429 Loss=1.184 Prec@1=70.626 Prec@5=89.546 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=07:57 IST
=> Training   4.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.598 DataTime=0.429 Loss=1.184 Prec@1=70.626 Prec@5=89.546 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=07:58 IST
=> Training   4.04% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.577 DataTime=0.404 Loss=1.175 Prec@1=70.825 Prec@5=89.631 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=07:58 IST
=> Training   8.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.577 DataTime=0.404 Loss=1.175 Prec@1=70.825 Prec@5=89.631 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=07:58 IST
=> Training   8.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.577 DataTime=0.404 Loss=1.175 Prec@1=70.825 Prec@5=89.631 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=07:59 IST
=> Training   8.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.572 DataTime=0.397 Loss=1.175 Prec@1=70.752 Prec@5=89.612 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=07:59 IST
=> Training   12.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.572 DataTime=0.397 Loss=1.175 Prec@1=70.752 Prec@5=89.612 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=07:59 IST
=> Training   12.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.572 DataTime=0.397 Loss=1.175 Prec@1=70.752 Prec@5=89.612 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=08:00 IST
=> Training   12.03% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.391 Loss=1.176 Prec@1=70.787 Prec@5=89.604 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=08:00 IST
=> Training   16.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.391 Loss=1.176 Prec@1=70.787 Prec@5=89.604 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=08:00 IST
=> Training   16.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.391 Loss=1.176 Prec@1=70.787 Prec@5=89.604 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=08:01 IST
=> Training   16.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.392 Loss=1.180 Prec@1=70.696 Prec@5=89.541 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=08:01 IST
=> Training   20.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.392 Loss=1.180 Prec@1=70.696 Prec@5=89.541 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=08:01 IST
=> Training   20.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.568 DataTime=0.392 Loss=1.180 Prec@1=70.696 Prec@5=89.541 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=08:02 IST
=> Training   20.02% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.501 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=08:02 IST
=> Training   24.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.501 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=08:02 IST
=> Training   24.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.501 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=08:03 IST
=> Training   24.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.566 DataTime=0.390 Loss=1.183 Prec@1=70.597 Prec@5=89.511 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=08:03 IST
=> Training   28.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.566 DataTime=0.390 Loss=1.183 Prec@1=70.597 Prec@5=89.511 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:03 IST
=> Training   28.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.566 DataTime=0.390 Loss=1.183 Prec@1=70.597 Prec@5=89.511 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:04 IST
=> Training   28.01% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.186 Prec@1=70.554 Prec@5=89.469 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:04 IST
=> Training   32.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.186 Prec@1=70.554 Prec@5=89.469 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=08:04 IST
=> Training   32.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.186 Prec@1=70.554 Prec@5=89.469 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=08:05 IST
=> Training   32.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.567 DataTime=0.392 Loss=1.186 Prec@1=70.526 Prec@5=89.450 rate=1.79 Hz, eta=0:15:52, total=0:07:28, wall=08:05 IST
=> Training   36.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.567 DataTime=0.392 Loss=1.186 Prec@1=70.526 Prec@5=89.450 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=08:05 IST
=> Training   36.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.567 DataTime=0.392 Loss=1.186 Prec@1=70.526 Prec@5=89.450 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=08:06 IST
=> Training   36.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.391 Loss=1.188 Prec@1=70.503 Prec@5=89.438 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=08:06 IST
=> Training   39.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.391 Loss=1.188 Prec@1=70.503 Prec@5=89.438 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:06 IST
=> Training   39.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.391 Loss=1.188 Prec@1=70.503 Prec@5=89.438 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:07 IST
=> Training   39.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.190 Prec@1=70.454 Prec@5=89.409 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:07 IST
=> Training   43.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.190 Prec@1=70.454 Prec@5=89.409 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=08:07 IST
=> Training   43.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.390 Loss=1.190 Prec@1=70.454 Prec@5=89.409 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=08:07 IST
=> Training   43.99% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.564 DataTime=0.388 Loss=1.194 Prec@1=70.361 Prec@5=89.361 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=08:07 IST
=> Training   47.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.564 DataTime=0.388 Loss=1.194 Prec@1=70.361 Prec@5=89.361 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=08:07 IST
=> Training   47.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.564 DataTime=0.388 Loss=1.194 Prec@1=70.361 Prec@5=89.361 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=08:08 IST
=> Training   47.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.388 Loss=1.196 Prec@1=70.316 Prec@5=89.313 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=08:08 IST
=> Training   51.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.388 Loss=1.196 Prec@1=70.316 Prec@5=89.313 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=08:08 IST
=> Training   51.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.565 DataTime=0.388 Loss=1.196 Prec@1=70.316 Prec@5=89.313 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=08:09 IST
=> Training   51.98% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.198 Prec@1=70.272 Prec@5=89.296 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=08:09 IST
=> Training   55.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.198 Prec@1=70.272 Prec@5=89.296 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:09 IST
=> Training   55.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.198 Prec@1=70.272 Prec@5=89.296 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:10 IST
=> Training   55.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.200 Prec@1=70.215 Prec@5=89.261 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:10 IST
=> Training   59.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.200 Prec@1=70.215 Prec@5=89.261 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:10 IST
=> Training   59.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.200 Prec@1=70.215 Prec@5=89.261 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:11 IST
=> Training   59.97% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.386 Loss=1.203 Prec@1=70.171 Prec@5=89.219 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:11 IST
=> Training   63.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.386 Loss=1.203 Prec@1=70.171 Prec@5=89.219 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:11 IST
=> Training   63.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.386 Loss=1.203 Prec@1=70.171 Prec@5=89.219 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:12 IST
=> Training   63.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.388 Loss=1.204 Prec@1=70.142 Prec@5=89.204 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:12 IST
=> Training   67.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.388 Loss=1.204 Prec@1=70.142 Prec@5=89.204 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=08:12 IST
=> Training   67.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.388 Loss=1.204 Prec@1=70.142 Prec@5=89.204 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=08:13 IST
=> Training   67.96% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.206 Prec@1=70.103 Prec@5=89.187 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=08:13 IST
=> Training   71.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.206 Prec@1=70.103 Prec@5=89.187 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=08:13 IST
=> Training   71.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.206 Prec@1=70.103 Prec@5=89.187 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=08:14 IST
=> Training   71.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.207 Prec@1=70.073 Prec@5=89.172 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=08:14 IST
=> Training   75.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.207 Prec@1=70.073 Prec@5=89.172 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=08:14 IST
=> Training   75.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.207 Prec@1=70.073 Prec@5=89.172 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=08:15 IST
=> Training   75.95% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.208 Prec@1=70.040 Prec@5=89.152 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=08:15 IST
=> Training   79.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.208 Prec@1=70.040 Prec@5=89.152 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=08:15 IST
=> Training   79.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.208 Prec@1=70.040 Prec@5=89.152 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=08:16 IST
=> Training   79.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.210 Prec@1=70.013 Prec@5=89.138 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=08:16 IST
=> Training   83.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.210 Prec@1=70.013 Prec@5=89.138 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=08:16 IST
=> Training   83.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.210 Prec@1=70.013 Prec@5=89.138 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=08:17 IST
=> Training   83.94% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.211 Prec@1=69.986 Prec@5=89.122 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=08:17 IST
=> Training   87.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.211 Prec@1=69.986 Prec@5=89.122 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:17 IST
=> Training   87.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.211 Prec@1=69.986 Prec@5=89.122 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:18 IST
=> Training   87.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.213 Prec@1=69.945 Prec@5=89.101 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:18 IST
=> Training   91.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.213 Prec@1=69.945 Prec@5=89.101 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=08:18 IST
=> Training   91.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.563 DataTime=0.387 Loss=1.213 Prec@1=69.945 Prec@5=89.101 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=08:19 IST
=> Training   91.93% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.214 Prec@1=69.923 Prec@5=89.084 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=08:19 IST
=> Training   95.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.214 Prec@1=69.923 Prec@5=89.084 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=08:19 IST
=> Training   95.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.214 Prec@1=69.923 Prec@5=89.084 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=08:20 IST
=> Training   95.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.216 Prec@1=69.893 Prec@5=89.066 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=08:20 IST
=> Training   99.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.216 Prec@1=69.893 Prec@5=89.066 rate=1.79 Hz, eta=0:00:01, total=0:23:21, wall=08:20 IST
=> Training   99.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.216 Prec@1=69.893 Prec@5=89.066 rate=1.79 Hz, eta=0:00:01, total=0:23:21, wall=08:20 IST
=> Training   99.92% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.216 Prec@1=69.892 Prec@5=89.064 rate=1.79 Hz, eta=0:00:01, total=0:23:21, wall=08:20 IST
=> Training   100.00% of 1x2503...Epoch=81/150 LR=0.0448 Time=0.562 DataTime=0.387 Loss=1.216 Prec@1=69.892 Prec@5=89.064 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=08:20 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:20 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:20 IST
=> Validation 0.00% of 1x98...Epoch=81/150 LR=0.0448 Time=7.056 Loss=0.871 Prec@1=77.148 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=08:20 IST
=> Validation 1.02% of 1x98...Epoch=81/150 LR=0.0448 Time=7.056 Loss=0.871 Prec@1=77.148 Prec@5=93.750 rate=4774.62 Hz, eta=0:00:00, total=0:00:00, wall=08:20 IST
** Validation 1.02% of 1x98...Epoch=81/150 LR=0.0448 Time=7.056 Loss=0.871 Prec@1=77.148 Prec@5=93.750 rate=4774.62 Hz, eta=0:00:00, total=0:00:00, wall=08:21 IST
** Validation 1.02% of 1x98...Epoch=81/150 LR=0.0448 Time=0.636 Loss=1.430 Prec@1=65.508 Prec@5=86.744 rate=4774.62 Hz, eta=0:00:00, total=0:00:00, wall=08:21 IST
** Validation 100.00% of 1x98...Epoch=81/150 LR=0.0448 Time=0.636 Loss=1.430 Prec@1=65.508 Prec@5=86.744 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=08:21 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST
=> Training   0.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=5.346 DataTime=4.950 Loss=1.162 Prec@1=72.461 Prec@5=91.016 rate=0 Hz, eta=?, total=0:00:00, wall=08:21 IST
=> Training   0.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=5.346 DataTime=4.950 Loss=1.162 Prec@1=72.461 Prec@5=91.016 rate=6902.88 Hz, eta=0:00:00, total=0:00:00, wall=08:21 IST
=> Training   0.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=5.346 DataTime=4.950 Loss=1.162 Prec@1=72.461 Prec@5=91.016 rate=6902.88 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST
=> Training   0.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.599 DataTime=0.431 Loss=1.163 Prec@1=71.268 Prec@5=89.793 rate=6902.88 Hz, eta=0:00:00, total=0:00:00, wall=08:22 IST
=> Training   4.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.599 DataTime=0.431 Loss=1.163 Prec@1=71.268 Prec@5=89.793 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=08:22 IST
=> Training   4.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.599 DataTime=0.431 Loss=1.163 Prec@1=71.268 Prec@5=89.793 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=08:23 IST
=> Training   4.04% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.576 DataTime=0.405 Loss=1.169 Prec@1=71.057 Prec@5=89.706 rate=1.83 Hz, eta=0:21:51, total=0:00:55, wall=08:23 IST
=> Training   8.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.576 DataTime=0.405 Loss=1.169 Prec@1=71.057 Prec@5=89.706 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=08:23 IST
=> Training   8.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.576 DataTime=0.405 Loss=1.169 Prec@1=71.057 Prec@5=89.706 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=08:23 IST
=> Training   8.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.394 Loss=1.178 Prec@1=70.746 Prec@5=89.609 rate=1.82 Hz, eta=0:21:04, total=0:01:50, wall=08:23 IST
=> Training   12.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.394 Loss=1.178 Prec@1=70.746 Prec@5=89.609 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=08:23 IST
=> Training   12.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.394 Loss=1.178 Prec@1=70.746 Prec@5=89.609 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=08:24 IST
=> Training   12.03% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.567 DataTime=0.397 Loss=1.180 Prec@1=70.710 Prec@5=89.584 rate=1.83 Hz, eta=0:20:03, total=0:02:44, wall=08:24 IST
=> Training   16.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.567 DataTime=0.397 Loss=1.180 Prec@1=70.710 Prec@5=89.584 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:24 IST
=> Training   16.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.567 DataTime=0.397 Loss=1.180 Prec@1=70.710 Prec@5=89.584 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:25 IST
=> Training   16.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.566 DataTime=0.394 Loss=1.183 Prec@1=70.688 Prec@5=89.567 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:25 IST
=> Training   20.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.566 DataTime=0.394 Loss=1.183 Prec@1=70.688 Prec@5=89.567 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=08:25 IST
=> Training   20.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.566 DataTime=0.394 Loss=1.183 Prec@1=70.688 Prec@5=89.567 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=08:26 IST
=> Training   20.02% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.391 Loss=1.183 Prec@1=70.624 Prec@5=89.583 rate=1.80 Hz, eta=0:18:30, total=0:04:38, wall=08:26 IST
=> Training   24.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.391 Loss=1.183 Prec@1=70.624 Prec@5=89.583 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:26 IST
=> Training   24.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.391 Loss=1.183 Prec@1=70.624 Prec@5=89.583 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:27 IST
=> Training   24.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.390 Loss=1.185 Prec@1=70.583 Prec@5=89.551 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:27 IST
=> Training   28.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.390 Loss=1.185 Prec@1=70.583 Prec@5=89.551 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=08:27 IST
=> Training   28.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.564 DataTime=0.390 Loss=1.185 Prec@1=70.583 Prec@5=89.551 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=08:28 IST
=> Training   28.01% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.388 Loss=1.188 Prec@1=70.560 Prec@5=89.518 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=08:28 IST
=> Training   32.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.388 Loss=1.188 Prec@1=70.560 Prec@5=89.518 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:28 IST
=> Training   32.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.388 Loss=1.188 Prec@1=70.560 Prec@5=89.518 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:29 IST
=> Training   32.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.387 Loss=1.189 Prec@1=70.511 Prec@5=89.488 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:29 IST
=> Training   36.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.387 Loss=1.189 Prec@1=70.511 Prec@5=89.488 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:29 IST
=> Training   36.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.387 Loss=1.189 Prec@1=70.511 Prec@5=89.488 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:30 IST
=> Training   36.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.387 Loss=1.190 Prec@1=70.504 Prec@5=89.472 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=08:30 IST
=> Training   39.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.387 Loss=1.190 Prec@1=70.504 Prec@5=89.472 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=08:30 IST
=> Training   39.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.563 DataTime=0.387 Loss=1.190 Prec@1=70.504 Prec@5=89.472 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=08:31 IST
=> Training   39.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.191 Prec@1=70.484 Prec@5=89.450 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=08:31 IST
=> Training   43.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.191 Prec@1=70.484 Prec@5=89.450 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=08:31 IST
=> Training   43.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.191 Prec@1=70.484 Prec@5=89.450 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=08:32 IST
=> Training   43.99% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.192 Prec@1=70.447 Prec@5=89.431 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=08:32 IST
=> Training   47.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.192 Prec@1=70.447 Prec@5=89.431 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=08:32 IST
=> Training   47.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.192 Prec@1=70.447 Prec@5=89.431 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=08:33 IST
=> Training   47.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.195 Prec@1=70.379 Prec@5=89.386 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=08:33 IST
=> Training   51.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.195 Prec@1=70.379 Prec@5=89.386 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=08:33 IST
=> Training   51.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.195 Prec@1=70.379 Prec@5=89.386 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=08:34 IST
=> Training   51.98% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.197 Prec@1=70.339 Prec@5=89.364 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=08:34 IST
=> Training   55.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.197 Prec@1=70.339 Prec@5=89.364 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=08:34 IST
=> Training   55.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.562 DataTime=0.385 Loss=1.197 Prec@1=70.339 Prec@5=89.364 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=08:35 IST
=> Training   55.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.198 Prec@1=70.316 Prec@5=89.345 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=08:35 IST
=> Training   59.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.198 Prec@1=70.316 Prec@5=89.345 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=08:35 IST
=> Training   59.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.198 Prec@1=70.316 Prec@5=89.345 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=08:36 IST
=> Training   59.97% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.561 DataTime=0.385 Loss=1.199 Prec@1=70.299 Prec@5=89.333 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=08:36 IST
=> Training   63.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.561 DataTime=0.385 Loss=1.199 Prec@1=70.299 Prec@5=89.333 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=08:36 IST
=> Training   63.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.561 DataTime=0.385 Loss=1.199 Prec@1=70.299 Prec@5=89.333 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=08:37 IST
=> Training   63.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.201 Prec@1=70.266 Prec@5=89.311 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=08:37 IST
=> Training   67.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.201 Prec@1=70.266 Prec@5=89.311 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=08:37 IST
=> Training   67.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.201 Prec@1=70.266 Prec@5=89.311 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=08:37 IST
=> Training   67.96% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.385 Loss=1.202 Prec@1=70.230 Prec@5=89.293 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=08:37 IST
=> Training   71.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.385 Loss=1.202 Prec@1=70.230 Prec@5=89.293 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=08:37 IST
=> Training   71.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.385 Loss=1.202 Prec@1=70.230 Prec@5=89.293 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=08:38 IST
=> Training   71.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.204 Prec@1=70.201 Prec@5=89.273 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=08:38 IST
=> Training   75.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.204 Prec@1=70.201 Prec@5=89.273 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:38 IST
=> Training   75.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.560 DataTime=0.384 Loss=1.204 Prec@1=70.201 Prec@5=89.273 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:39 IST
=> Training   75.95% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.204 Prec@1=70.179 Prec@5=89.261 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=08:39 IST
=> Training   79.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.204 Prec@1=70.179 Prec@5=89.261 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=08:39 IST
=> Training   79.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.204 Prec@1=70.179 Prec@5=89.261 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=08:40 IST
=> Training   79.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.205 Prec@1=70.173 Prec@5=89.241 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=08:40 IST
=> Training   83.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.205 Prec@1=70.173 Prec@5=89.241 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=08:40 IST
=> Training   83.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.205 Prec@1=70.173 Prec@5=89.241 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=08:41 IST
=> Training   83.94% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.382 Loss=1.206 Prec@1=70.145 Prec@5=89.213 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=08:41 IST
=> Training   87.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.382 Loss=1.206 Prec@1=70.145 Prec@5=89.213 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:41 IST
=> Training   87.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.382 Loss=1.206 Prec@1=70.145 Prec@5=89.213 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:42 IST
=> Training   87.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.207 Prec@1=70.123 Prec@5=89.206 rate=1.80 Hz, eta=0:02:48, total=0:20:24, wall=08:42 IST
=> Training   91.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.207 Prec@1=70.123 Prec@5=89.206 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:42 IST
=> Training   91.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.207 Prec@1=70.123 Prec@5=89.206 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:43 IST
=> Training   91.93% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.208 Prec@1=70.097 Prec@5=89.197 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=08:43 IST
=> Training   95.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.208 Prec@1=70.097 Prec@5=89.197 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:43 IST
=> Training   95.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.559 DataTime=0.383 Loss=1.208 Prec@1=70.097 Prec@5=89.197 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:44 IST
=> Training   95.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.558 DataTime=0.383 Loss=1.209 Prec@1=70.085 Prec@5=89.186 rate=1.80 Hz, eta=0:00:56, total=0:22:15, wall=08:44 IST
=> Training   99.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.558 DataTime=0.383 Loss=1.209 Prec@1=70.085 Prec@5=89.186 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=08:44 IST
=> Training   99.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.558 DataTime=0.383 Loss=1.209 Prec@1=70.085 Prec@5=89.186 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=08:44 IST
=> Training   99.92% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.558 DataTime=0.383 Loss=1.209 Prec@1=70.083 Prec@5=89.186 rate=1.80 Hz, eta=0:00:01, total=0:23:11, wall=08:44 IST
=> Training   100.00% of 1x2503...Epoch=82/150 LR=0.0437 Time=0.558 DataTime=0.383 Loss=1.209 Prec@1=70.083 Prec@5=89.186 rate=1.80 Hz, eta=0:00:00, total=0:23:11, wall=08:44 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:44 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:44 IST
=> Validation 0.00% of 1x98...Epoch=82/150 LR=0.0437 Time=6.886 Loss=1.166 Prec@1=72.461 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=08:44 IST
=> Validation 1.02% of 1x98...Epoch=82/150 LR=0.0437 Time=6.886 Loss=1.166 Prec@1=72.461 Prec@5=90.234 rate=5348.97 Hz, eta=0:00:00, total=0:00:00, wall=08:44 IST
** Validation 1.02% of 1x98...Epoch=82/150 LR=0.0437 Time=6.886 Loss=1.166 Prec@1=72.461 Prec@5=90.234 rate=5348.97 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST
** Validation 1.02% of 1x98...Epoch=82/150 LR=0.0437 Time=0.633 Loss=1.542 Prec@1=63.500 Prec@5=85.174 rate=5348.97 Hz, eta=0:00:00, total=0:00:00, wall=08:45 IST
** Validation 100.00% of 1x98...Epoch=82/150 LR=0.0437 Time=0.633 Loss=1.542 Prec@1=63.500 Prec@5=85.174 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=08:45 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST
=> Training   0.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=4.690 DataTime=4.460 Loss=1.168 Prec@1=71.484 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=08:45 IST
=> Training   0.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=4.690 DataTime=4.460 Loss=1.168 Prec@1=71.484 Prec@5=91.406 rate=2018.08 Hz, eta=0:00:01, total=0:00:00, wall=08:45 IST
=> Training   0.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=4.690 DataTime=4.460 Loss=1.168 Prec@1=71.484 Prec@5=91.406 rate=2018.08 Hz, eta=0:00:01, total=0:00:00, wall=08:46 IST
=> Training   0.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.587 DataTime=0.419 Loss=1.158 Prec@1=71.231 Prec@5=89.859 rate=2018.08 Hz, eta=0:00:01, total=0:00:00, wall=08:46 IST
=> Training   4.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.587 DataTime=0.419 Loss=1.158 Prec@1=71.231 Prec@5=89.859 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=08:46 IST
=> Training   4.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.587 DataTime=0.419 Loss=1.158 Prec@1=71.231 Prec@5=89.859 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=08:47 IST
=> Training   4.04% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.572 DataTime=0.400 Loss=1.157 Prec@1=71.202 Prec@5=89.813 rate=1.85 Hz, eta=0:21:38, total=0:00:54, wall=08:47 IST
=> Training   8.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.572 DataTime=0.400 Loss=1.157 Prec@1=71.202 Prec@5=89.813 rate=1.82 Hz, eta=0:21:03, total=0:01:50, wall=08:47 IST
=> Training   8.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.572 DataTime=0.400 Loss=1.157 Prec@1=71.202 Prec@5=89.813 rate=1.82 Hz, eta=0:21:03, total=0:01:50, wall=08:48 IST
=> Training   8.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.567 DataTime=0.394 Loss=1.161 Prec@1=71.157 Prec@5=89.770 rate=1.82 Hz, eta=0:21:03, total=0:01:50, wall=08:48 IST
=> Training   12.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.567 DataTime=0.394 Loss=1.161 Prec@1=71.157 Prec@5=89.770 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=08:48 IST
=> Training   12.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.567 DataTime=0.394 Loss=1.161 Prec@1=71.157 Prec@5=89.770 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=08:49 IST
=> Training   12.03% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.565 DataTime=0.392 Loss=1.164 Prec@1=71.101 Prec@5=89.713 rate=1.81 Hz, eta=0:20:14, total=0:02:45, wall=08:49 IST
=> Training   16.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.565 DataTime=0.392 Loss=1.164 Prec@1=71.101 Prec@5=89.713 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=08:49 IST
=> Training   16.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.565 DataTime=0.392 Loss=1.164 Prec@1=71.101 Prec@5=89.713 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=08:50 IST
=> Training   16.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.566 DataTime=0.392 Loss=1.168 Prec@1=71.024 Prec@5=89.646 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=08:50 IST
=> Training   20.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.566 DataTime=0.392 Loss=1.168 Prec@1=71.024 Prec@5=89.646 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=08:50 IST
=> Training   20.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.566 DataTime=0.392 Loss=1.168 Prec@1=71.024 Prec@5=89.646 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=08:51 IST
=> Training   20.02% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.172 Prec@1=70.937 Prec@5=89.604 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=08:51 IST
=> Training   24.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.172 Prec@1=70.937 Prec@5=89.604 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:51 IST
=> Training   24.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.172 Prec@1=70.937 Prec@5=89.604 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:52 IST
=> Training   24.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.564 DataTime=0.391 Loss=1.175 Prec@1=70.847 Prec@5=89.581 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=08:52 IST
=> Training   28.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.564 DataTime=0.391 Loss=1.175 Prec@1=70.847 Prec@5=89.581 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=08:52 IST
=> Training   28.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.564 DataTime=0.391 Loss=1.175 Prec@1=70.847 Prec@5=89.581 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=08:53 IST
=> Training   28.01% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.177 Prec@1=70.782 Prec@5=89.559 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=08:53 IST
=> Training   32.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.177 Prec@1=70.782 Prec@5=89.559 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=08:53 IST
=> Training   32.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.177 Prec@1=70.782 Prec@5=89.559 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=08:53 IST
=> Training   32.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.180 Prec@1=70.731 Prec@5=89.520 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=08:53 IST
=> Training   36.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.180 Prec@1=70.731 Prec@5=89.520 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:53 IST
=> Training   36.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.180 Prec@1=70.731 Prec@5=89.520 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:54 IST
=> Training   36.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.181 Prec@1=70.692 Prec@5=89.511 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:54 IST
=> Training   39.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.181 Prec@1=70.692 Prec@5=89.511 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=08:54 IST
=> Training   39.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.181 Prec@1=70.692 Prec@5=89.511 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=08:55 IST
=> Training   39.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.184 Prec@1=70.624 Prec@5=89.482 rate=1.79 Hz, eta=0:13:56, total=0:09:17, wall=08:55 IST
=> Training   43.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.184 Prec@1=70.624 Prec@5=89.482 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=08:55 IST
=> Training   43.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.390 Loss=1.184 Prec@1=70.624 Prec@5=89.482 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=08:56 IST
=> Training   43.99% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.185 Prec@1=70.620 Prec@5=89.481 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=08:56 IST
=> Training   47.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.185 Prec@1=70.620 Prec@5=89.481 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=08:56 IST
=> Training   47.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.563 DataTime=0.390 Loss=1.185 Prec@1=70.620 Prec@5=89.481 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=08:57 IST
=> Training   47.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.186 Prec@1=70.582 Prec@5=89.457 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=08:57 IST
=> Training   51.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.186 Prec@1=70.582 Prec@5=89.457 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=08:57 IST
=> Training   51.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.186 Prec@1=70.582 Prec@5=89.457 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=08:58 IST
=> Training   51.98% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.188 Prec@1=70.555 Prec@5=89.431 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=08:58 IST
=> Training   55.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.188 Prec@1=70.555 Prec@5=89.431 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=08:58 IST
=> Training   55.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.389 Loss=1.188 Prec@1=70.555 Prec@5=89.431 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=08:59 IST
=> Training   55.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.189 Prec@1=70.545 Prec@5=89.421 rate=1.79 Hz, eta=0:10:15, total=0:13:03, wall=08:59 IST
=> Training   59.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.189 Prec@1=70.545 Prec@5=89.421 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=08:59 IST
=> Training   59.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.189 Prec@1=70.545 Prec@5=89.421 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=09:00 IST
=> Training   59.97% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.191 Prec@1=70.537 Prec@5=89.409 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=09:00 IST
=> Training   63.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.191 Prec@1=70.537 Prec@5=89.409 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=09:00 IST
=> Training   63.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.191 Prec@1=70.537 Prec@5=89.409 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=09:01 IST
=> Training   63.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.192 Prec@1=70.509 Prec@5=89.393 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=09:01 IST
=> Training   67.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.192 Prec@1=70.509 Prec@5=89.393 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:01 IST
=> Training   67.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.192 Prec@1=70.509 Prec@5=89.393 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:02 IST
=> Training   67.96% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.193 Prec@1=70.481 Prec@5=89.381 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:02 IST
=> Training   71.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.193 Prec@1=70.481 Prec@5=89.381 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:02 IST
=> Training   71.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.562 DataTime=0.388 Loss=1.193 Prec@1=70.481 Prec@5=89.381 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:03 IST
=> Training   71.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.194 Prec@1=70.430 Prec@5=89.359 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:03 IST
=> Training   75.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.194 Prec@1=70.430 Prec@5=89.359 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:03 IST
=> Training   75.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.194 Prec@1=70.430 Prec@5=89.359 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:04 IST
=> Training   75.95% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.196 Prec@1=70.384 Prec@5=89.338 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:04 IST
=> Training   79.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.196 Prec@1=70.384 Prec@5=89.338 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:04 IST
=> Training   79.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.196 Prec@1=70.384 Prec@5=89.338 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:05 IST
=> Training   79.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.357 Prec@5=89.309 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:05 IST
=> Training   83.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.357 Prec@5=89.309 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:05 IST
=> Training   83.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.357 Prec@5=89.309 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:06 IST
=> Training   83.94% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.337 Prec@5=89.300 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:06 IST
=> Training   87.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.337 Prec@5=89.300 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=09:06 IST
=> Training   87.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.387 Loss=1.198 Prec@1=70.337 Prec@5=89.300 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=09:06 IST
=> Training   87.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.199 Prec@1=70.318 Prec@5=89.298 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=09:06 IST
=> Training   91.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.199 Prec@1=70.318 Prec@5=89.298 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=09:06 IST
=> Training   91.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.199 Prec@1=70.318 Prec@5=89.298 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=09:07 IST
=> Training   91.93% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.200 Prec@1=70.294 Prec@5=89.289 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=09:07 IST
=> Training   95.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.200 Prec@1=70.294 Prec@5=89.289 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:07 IST
=> Training   95.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.561 DataTime=0.386 Loss=1.200 Prec@1=70.294 Prec@5=89.289 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:08 IST
=> Training   95.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.560 DataTime=0.385 Loss=1.201 Prec@1=70.281 Prec@5=89.279 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:08 IST
=> Training   99.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.560 DataTime=0.385 Loss=1.201 Prec@1=70.281 Prec@5=89.279 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=09:08 IST
=> Training   99.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.560 DataTime=0.385 Loss=1.201 Prec@1=70.281 Prec@5=89.279 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=09:08 IST
=> Training   99.92% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.560 DataTime=0.385 Loss=1.201 Prec@1=70.279 Prec@5=89.279 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=09:08 IST
=> Training   100.00% of 1x2503...Epoch=83/150 LR=0.0427 Time=0.560 DataTime=0.385 Loss=1.201 Prec@1=70.279 Prec@5=89.279 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=09:08 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST
=> Validation 0.00% of 1x98...Epoch=83/150 LR=0.0427 Time=6.996 Loss=0.957 Prec@1=77.930 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=09:08 IST
=> Validation 1.02% of 1x98...Epoch=83/150 LR=0.0427 Time=6.996 Loss=0.957 Prec@1=77.930 Prec@5=91.602 rate=6757.35 Hz, eta=0:00:00, total=0:00:00, wall=09:08 IST
** Validation 1.02% of 1x98...Epoch=83/150 LR=0.0427 Time=6.996 Loss=0.957 Prec@1=77.930 Prec@5=91.602 rate=6757.35 Hz, eta=0:00:00, total=0:00:00, wall=09:09 IST
** Validation 1.02% of 1x98...Epoch=83/150 LR=0.0427 Time=0.638 Loss=1.412 Prec@1=65.912 Prec@5=86.838 rate=6757.35 Hz, eta=0:00:00, total=0:00:00, wall=09:09 IST
** Validation 100.00% of 1x98...Epoch=83/150 LR=0.0427 Time=0.638 Loss=1.412 Prec@1=65.912 Prec@5=86.838 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=09:09 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST
=> Training   0.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=5.113 DataTime=4.929 Loss=1.083 Prec@1=72.266 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=09:09 IST
=> Training   0.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=5.113 DataTime=4.929 Loss=1.083 Prec@1=72.266 Prec@5=91.211 rate=2272.14 Hz, eta=0:00:01, total=0:00:00, wall=09:09 IST
=> Training   0.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=5.113 DataTime=4.929 Loss=1.083 Prec@1=72.266 Prec@5=91.211 rate=2272.14 Hz, eta=0:00:01, total=0:00:00, wall=09:10 IST
=> Training   0.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.587 DataTime=0.425 Loss=1.168 Prec@1=70.926 Prec@5=89.699 rate=2272.14 Hz, eta=0:00:01, total=0:00:00, wall=09:10 IST
=> Training   4.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.587 DataTime=0.425 Loss=1.168 Prec@1=70.926 Prec@5=89.699 rate=1.86 Hz, eta=0:21:28, total=0:00:54, wall=09:10 IST
=> Training   4.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.587 DataTime=0.425 Loss=1.168 Prec@1=70.926 Prec@5=89.699 rate=1.86 Hz, eta=0:21:28, total=0:00:54, wall=09:11 IST
=> Training   4.04% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.571 DataTime=0.405 Loss=1.167 Prec@1=71.137 Prec@5=89.651 rate=1.86 Hz, eta=0:21:28, total=0:00:54, wall=09:11 IST
=> Training   8.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.571 DataTime=0.405 Loss=1.167 Prec@1=71.137 Prec@5=89.651 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=09:11 IST
=> Training   8.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.571 DataTime=0.405 Loss=1.167 Prec@1=71.137 Prec@5=89.651 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=09:12 IST
=> Training   8.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.395 Loss=1.166 Prec@1=71.122 Prec@5=89.730 rate=1.83 Hz, eta=0:20:56, total=0:01:49, wall=09:12 IST
=> Training   12.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.395 Loss=1.166 Prec@1=71.122 Prec@5=89.730 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=09:12 IST
=> Training   12.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.395 Loss=1.166 Prec@1=71.122 Prec@5=89.730 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=09:13 IST
=> Training   12.03% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.393 Loss=1.167 Prec@1=71.109 Prec@5=89.710 rate=1.83 Hz, eta=0:20:02, total=0:02:44, wall=09:13 IST
=> Training   16.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.393 Loss=1.167 Prec@1=71.109 Prec@5=89.710 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=09:13 IST
=> Training   16.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.393 Loss=1.167 Prec@1=71.109 Prec@5=89.710 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=09:14 IST
=> Training   16.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.392 Loss=1.168 Prec@1=71.058 Prec@5=89.671 rate=1.82 Hz, eta=0:19:15, total=0:03:40, wall=09:14 IST
=> Training   20.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.392 Loss=1.168 Prec@1=71.058 Prec@5=89.671 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=09:14 IST
=> Training   20.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.563 DataTime=0.392 Loss=1.168 Prec@1=71.058 Prec@5=89.671 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=09:15 IST
=> Training   20.02% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.391 Loss=1.169 Prec@1=71.024 Prec@5=89.671 rate=1.81 Hz, eta=0:18:25, total=0:04:36, wall=09:15 IST
=> Training   24.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.391 Loss=1.169 Prec@1=71.024 Prec@5=89.671 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=09:15 IST
=> Training   24.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.391 Loss=1.169 Prec@1=71.024 Prec@5=89.671 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=09:16 IST
=> Training   24.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.169 Prec@1=70.993 Prec@5=89.677 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=09:16 IST
=> Training   28.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.169 Prec@1=70.993 Prec@5=89.677 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=09:16 IST
=> Training   28.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.169 Prec@1=70.993 Prec@5=89.677 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=09:17 IST
=> Training   28.01% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.170 Prec@1=70.972 Prec@5=89.667 rate=1.80 Hz, eta=0:16:38, total=0:06:28, wall=09:17 IST
=> Training   32.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.170 Prec@1=70.972 Prec@5=89.667 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:17 IST
=> Training   32.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.388 Loss=1.170 Prec@1=70.972 Prec@5=89.667 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:18 IST
=> Training   32.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.389 Loss=1.173 Prec@1=70.915 Prec@5=89.619 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:18 IST
=> Training   36.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.389 Loss=1.173 Prec@1=70.915 Prec@5=89.619 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:18 IST
=> Training   36.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.389 Loss=1.173 Prec@1=70.915 Prec@5=89.619 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:19 IST
=> Training   36.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.387 Loss=1.175 Prec@1=70.891 Prec@5=89.602 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=09:19 IST
=> Training   39.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.387 Loss=1.175 Prec@1=70.891 Prec@5=89.602 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:19 IST
=> Training   39.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.387 Loss=1.175 Prec@1=70.891 Prec@5=89.602 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:20 IST
=> Training   39.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.387 Loss=1.176 Prec@1=70.867 Prec@5=89.591 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:20 IST
=> Training   43.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.387 Loss=1.176 Prec@1=70.867 Prec@5=89.591 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:20 IST
=> Training   43.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.562 DataTime=0.387 Loss=1.176 Prec@1=70.867 Prec@5=89.591 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:21 IST
=> Training   43.99% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.178 Prec@1=70.810 Prec@5=89.565 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:21 IST
=> Training   47.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.178 Prec@1=70.810 Prec@5=89.565 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:21 IST
=> Training   47.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.178 Prec@1=70.810 Prec@5=89.565 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:22 IST
=> Training   47.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.754 Prec@5=89.529 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:22 IST
=> Training   51.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.754 Prec@5=89.529 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:22 IST
=> Training   51.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.754 Prec@5=89.529 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:23 IST
=> Training   51.98% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.744 Prec@5=89.540 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:23 IST
=> Training   55.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.744 Prec@5=89.540 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:23 IST
=> Training   55.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.181 Prec@1=70.744 Prec@5=89.540 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:23 IST
=> Training   55.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.182 Prec@1=70.705 Prec@5=89.527 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:23 IST
=> Training   59.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.182 Prec@1=70.705 Prec@5=89.527 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=09:23 IST
=> Training   59.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.182 Prec@1=70.705 Prec@5=89.527 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=09:24 IST
=> Training   59.97% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.383 Loss=1.183 Prec@1=70.674 Prec@5=89.507 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=09:24 IST
=> Training   63.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.383 Loss=1.183 Prec@1=70.674 Prec@5=89.507 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=09:24 IST
=> Training   63.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.383 Loss=1.183 Prec@1=70.674 Prec@5=89.507 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=09:25 IST
=> Training   63.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.184 Prec@1=70.637 Prec@5=89.488 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=09:25 IST
=> Training   67.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.184 Prec@1=70.637 Prec@5=89.488 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:25 IST
=> Training   67.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.184 Prec@1=70.637 Prec@5=89.488 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:26 IST
=> Training   67.96% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.186 Prec@1=70.597 Prec@5=89.468 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:26 IST
=> Training   71.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.186 Prec@1=70.597 Prec@5=89.468 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=09:26 IST
=> Training   71.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.186 Prec@1=70.597 Prec@5=89.468 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=09:27 IST
=> Training   71.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.385 Loss=1.188 Prec@1=70.567 Prec@5=89.435 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=09:27 IST
=> Training   75.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.385 Loss=1.188 Prec@1=70.567 Prec@5=89.435 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=09:27 IST
=> Training   75.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.385 Loss=1.188 Prec@1=70.567 Prec@5=89.435 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=09:28 IST
=> Training   75.95% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.190 Prec@1=70.517 Prec@5=89.414 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=09:28 IST
=> Training   79.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.190 Prec@1=70.517 Prec@5=89.414 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=09:28 IST
=> Training   79.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.384 Loss=1.190 Prec@1=70.517 Prec@5=89.414 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=09:29 IST
=> Training   79.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.492 Prec@5=89.393 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=09:29 IST
=> Training   83.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.492 Prec@5=89.393 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=09:29 IST
=> Training   83.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.492 Prec@5=89.393 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=09:30 IST
=> Training   83.94% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.487 Prec@5=89.389 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=09:30 IST
=> Training   87.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.487 Prec@5=89.389 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=09:30 IST
=> Training   87.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.192 Prec@1=70.487 Prec@5=89.389 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=09:31 IST
=> Training   87.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.193 Prec@1=70.448 Prec@5=89.374 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=09:31 IST
=> Training   91.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.193 Prec@1=70.448 Prec@5=89.374 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=09:31 IST
=> Training   91.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.385 Loss=1.193 Prec@1=70.448 Prec@5=89.374 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=09:32 IST
=> Training   91.93% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.194 Prec@1=70.434 Prec@5=89.363 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=09:32 IST
=> Training   95.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.194 Prec@1=70.434 Prec@5=89.363 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:32 IST
=> Training   95.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.194 Prec@1=70.434 Prec@5=89.363 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:33 IST
=> Training   95.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.195 Prec@1=70.405 Prec@5=89.348 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=09:33 IST
=> Training   99.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.195 Prec@1=70.405 Prec@5=89.348 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=09:33 IST
=> Training   99.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.561 DataTime=0.386 Loss=1.195 Prec@1=70.405 Prec@5=89.348 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=09:33 IST
=> Training   99.92% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.385 Loss=1.195 Prec@1=70.403 Prec@5=89.347 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=09:33 IST
=> Training   100.00% of 1x2503...Epoch=84/150 LR=0.0417 Time=0.560 DataTime=0.385 Loss=1.195 Prec@1=70.403 Prec@5=89.347 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=09:33 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:33 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:33 IST
=> Validation 0.00% of 1x98...Epoch=84/150 LR=0.0417 Time=7.196 Loss=0.963 Prec@1=75.195 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=09:33 IST
=> Validation 1.02% of 1x98...Epoch=84/150 LR=0.0417 Time=7.196 Loss=0.963 Prec@1=75.195 Prec@5=92.773 rate=6967.33 Hz, eta=0:00:00, total=0:00:00, wall=09:33 IST
** Validation 1.02% of 1x98...Epoch=84/150 LR=0.0417 Time=7.196 Loss=0.963 Prec@1=75.195 Prec@5=92.773 rate=6967.33 Hz, eta=0:00:00, total=0:00:00, wall=09:34 IST
** Validation 1.02% of 1x98...Epoch=84/150 LR=0.0417 Time=0.639 Loss=1.393 Prec@1=66.438 Prec@5=87.422 rate=6967.33 Hz, eta=0:00:00, total=0:00:00, wall=09:34 IST
** Validation 100.00% of 1x98...Epoch=84/150 LR=0.0417 Time=0.639 Loss=1.393 Prec@1=66.438 Prec@5=87.422 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=09:34 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST
=> Training   0.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=4.620 DataTime=4.375 Loss=1.217 Prec@1=71.680 Prec@5=87.500 rate=0 Hz, eta=?, total=0:00:00, wall=09:34 IST
=> Training   0.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=4.620 DataTime=4.375 Loss=1.217 Prec@1=71.680 Prec@5=87.500 rate=683.35 Hz, eta=0:00:03, total=0:00:00, wall=09:34 IST
=> Training   0.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=4.620 DataTime=4.375 Loss=1.217 Prec@1=71.680 Prec@5=87.500 rate=683.35 Hz, eta=0:00:03, total=0:00:00, wall=09:35 IST
=> Training   0.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.575 DataTime=0.402 Loss=1.162 Prec@1=71.096 Prec@5=89.793 rate=683.35 Hz, eta=0:00:03, total=0:00:00, wall=09:35 IST
=> Training   4.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.575 DataTime=0.402 Loss=1.162 Prec@1=71.096 Prec@5=89.793 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=09:35 IST
=> Training   4.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.575 DataTime=0.402 Loss=1.162 Prec@1=71.096 Prec@5=89.793 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=09:36 IST
=> Training   4.04% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.156 Prec@1=71.336 Prec@5=89.848 rate=1.89 Hz, eta=0:21:11, total=0:00:53, wall=09:36 IST
=> Training   8.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.156 Prec@1=71.336 Prec@5=89.848 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=09:36 IST
=> Training   8.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.156 Prec@1=71.336 Prec@5=89.848 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=09:37 IST
=> Training   8.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.157 Prec@1=71.268 Prec@5=89.797 rate=1.83 Hz, eta=0:20:55, total=0:01:49, wall=09:37 IST
=> Training   12.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.157 Prec@1=71.268 Prec@5=89.797 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=09:37 IST
=> Training   12.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.399 Loss=1.157 Prec@1=71.268 Prec@5=89.797 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=09:38 IST
=> Training   12.03% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.396 Loss=1.157 Prec@1=71.235 Prec@5=89.810 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=09:38 IST
=> Training   16.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.396 Loss=1.157 Prec@1=71.235 Prec@5=89.810 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=09:38 IST
=> Training   16.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.396 Loss=1.157 Prec@1=71.235 Prec@5=89.810 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=09:39 IST
=> Training   16.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.395 Loss=1.160 Prec@1=71.178 Prec@5=89.784 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=09:39 IST
=> Training   20.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.395 Loss=1.160 Prec@1=71.178 Prec@5=89.784 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=09:39 IST
=> Training   20.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.569 DataTime=0.395 Loss=1.160 Prec@1=71.178 Prec@5=89.784 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=09:40 IST
=> Training   20.02% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.162 Prec@1=71.113 Prec@5=89.784 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=09:40 IST
=> Training   24.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.162 Prec@1=71.113 Prec@5=89.784 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=09:40 IST
=> Training   24.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.162 Prec@1=71.113 Prec@5=89.784 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=09:40 IST
=> Training   24.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.161 Prec@1=71.141 Prec@5=89.792 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=09:40 IST
=> Training   28.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.161 Prec@1=71.141 Prec@5=89.792 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=09:40 IST
=> Training   28.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.393 Loss=1.161 Prec@1=71.141 Prec@5=89.792 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=09:41 IST
=> Training   28.01% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.162 Prec@1=71.084 Prec@5=89.764 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=09:41 IST
=> Training   32.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.162 Prec@1=71.084 Prec@5=89.764 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=09:41 IST
=> Training   32.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.162 Prec@1=71.084 Prec@5=89.764 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=09:42 IST
=> Training   32.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.165 Prec@1=71.022 Prec@5=89.738 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=09:42 IST
=> Training   36.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.165 Prec@1=71.022 Prec@5=89.738 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=09:42 IST
=> Training   36.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.165 Prec@1=71.022 Prec@5=89.738 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=09:43 IST
=> Training   36.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.166 Prec@1=71.029 Prec@5=89.730 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=09:43 IST
=> Training   39.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.166 Prec@1=71.029 Prec@5=89.730 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=09:43 IST
=> Training   39.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.392 Loss=1.166 Prec@1=71.029 Prec@5=89.730 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=09:44 IST
=> Training   39.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.392 Loss=1.167 Prec@1=71.014 Prec@5=89.720 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=09:44 IST
=> Training   43.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.392 Loss=1.167 Prec@1=71.014 Prec@5=89.720 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=09:44 IST
=> Training   43.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.567 DataTime=0.392 Loss=1.167 Prec@1=71.014 Prec@5=89.720 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=09:45 IST
=> Training   43.99% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.167 Prec@1=71.012 Prec@5=89.703 rate=1.78 Hz, eta=0:13:09, total=0:10:19, wall=09:45 IST
=> Training   47.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.167 Prec@1=71.012 Prec@5=89.703 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=09:45 IST
=> Training   47.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.167 Prec@1=71.012 Prec@5=89.703 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=09:46 IST
=> Training   47.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.169 Prec@1=70.962 Prec@5=89.672 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=09:46 IST
=> Training   51.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.169 Prec@1=70.962 Prec@5=89.672 rate=1.78 Hz, eta=0:11:16, total=0:12:11, wall=09:46 IST
=> Training   51.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.169 Prec@1=70.962 Prec@5=89.672 rate=1.78 Hz, eta=0:11:16, total=0:12:11, wall=09:47 IST
=> Training   51.98% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.170 Prec@1=70.928 Prec@5=89.649 rate=1.78 Hz, eta=0:11:16, total=0:12:11, wall=09:47 IST
=> Training   55.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.170 Prec@1=70.928 Prec@5=89.649 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=09:47 IST
=> Training   55.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.391 Loss=1.170 Prec@1=70.928 Prec@5=89.649 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=09:48 IST
=> Training   55.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.172 Prec@1=70.874 Prec@5=89.622 rate=1.78 Hz, eta=0:10:19, total=0:13:07, wall=09:48 IST
=> Training   59.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.172 Prec@1=70.874 Prec@5=89.622 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=09:48 IST
=> Training   59.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.172 Prec@1=70.874 Prec@5=89.622 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=09:49 IST
=> Training   59.97% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.174 Prec@1=70.810 Prec@5=89.604 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=09:49 IST
=> Training   63.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.174 Prec@1=70.810 Prec@5=89.604 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=09:49 IST
=> Training   63.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.390 Loss=1.174 Prec@1=70.810 Prec@5=89.604 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=09:50 IST
=> Training   63.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.390 Loss=1.175 Prec@1=70.790 Prec@5=89.578 rate=1.78 Hz, eta=0:08:27, total=0:15:00, wall=09:50 IST
=> Training   67.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.390 Loss=1.175 Prec@1=70.790 Prec@5=89.578 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=09:50 IST
=> Training   67.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.566 DataTime=0.390 Loss=1.175 Prec@1=70.790 Prec@5=89.578 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=09:51 IST
=> Training   67.96% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.177 Prec@1=70.756 Prec@5=89.557 rate=1.78 Hz, eta=0:07:31, total=0:15:57, wall=09:51 IST
=> Training   71.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.177 Prec@1=70.756 Prec@5=89.557 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=09:51 IST
=> Training   71.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.177 Prec@1=70.756 Prec@5=89.557 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=09:52 IST
=> Training   71.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.178 Prec@1=70.740 Prec@5=89.545 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=09:52 IST
=> Training   75.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.178 Prec@1=70.740 Prec@5=89.545 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=09:52 IST
=> Training   75.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.178 Prec@1=70.740 Prec@5=89.545 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=09:53 IST
=> Training   75.95% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.179 Prec@1=70.713 Prec@5=89.534 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=09:53 IST
=> Training   79.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.179 Prec@1=70.713 Prec@5=89.534 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=09:53 IST
=> Training   79.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.179 Prec@1=70.713 Prec@5=89.534 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=09:54 IST
=> Training   79.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.181 Prec@1=70.679 Prec@5=89.512 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=09:54 IST
=> Training   83.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.181 Prec@1=70.679 Prec@5=89.512 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=09:54 IST
=> Training   83.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.565 DataTime=0.389 Loss=1.181 Prec@1=70.679 Prec@5=89.512 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=09:55 IST
=> Training   83.94% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.388 Loss=1.181 Prec@1=70.663 Prec@5=89.499 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=09:55 IST
=> Training   87.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.388 Loss=1.181 Prec@1=70.663 Prec@5=89.499 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=09:55 IST
=> Training   87.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.388 Loss=1.181 Prec@1=70.663 Prec@5=89.499 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=09:55 IST
=> Training   87.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.183 Prec@1=70.643 Prec@5=89.488 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=09:55 IST
=> Training   91.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.183 Prec@1=70.643 Prec@5=89.488 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=09:55 IST
=> Training   91.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.183 Prec@1=70.643 Prec@5=89.488 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=09:56 IST
=> Training   91.93% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.468 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=09:56 IST
=> Training   95.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.468 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=09:56 IST
=> Training   95.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.184 Prec@1=70.604 Prec@5=89.468 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=09:57 IST
=> Training   95.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.185 Prec@1=70.579 Prec@5=89.455 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=09:57 IST
=> Training   99.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.185 Prec@1=70.579 Prec@5=89.455 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=09:57 IST
=> Training   99.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.389 Loss=1.185 Prec@1=70.579 Prec@5=89.455 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=09:57 IST
=> Training   99.92% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.388 Loss=1.186 Prec@1=70.578 Prec@5=89.454 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=09:57 IST
=> Training   100.00% of 1x2503...Epoch=85/150 LR=0.0406 Time=0.564 DataTime=0.388 Loss=1.186 Prec@1=70.578 Prec@5=89.454 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=09:57 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> Validation 0.00% of 1x98...Epoch=85/150 LR=0.0406 Time=7.036 Loss=0.916 Prec@1=75.977 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=09:57 IST
=> Validation 1.02% of 1x98...Epoch=85/150 LR=0.0406 Time=7.036 Loss=0.916 Prec@1=75.977 Prec@5=93.945 rate=5206.22 Hz, eta=0:00:00, total=0:00:00, wall=09:57 IST
** Validation 1.02% of 1x98...Epoch=85/150 LR=0.0406 Time=7.036 Loss=0.916 Prec@1=75.977 Prec@5=93.945 rate=5206.22 Hz, eta=0:00:00, total=0:00:00, wall=09:58 IST
** Validation 1.02% of 1x98...Epoch=85/150 LR=0.0406 Time=0.637 Loss=1.378 Prec@1=66.708 Prec@5=87.504 rate=5206.22 Hz, eta=0:00:00, total=0:00:00, wall=09:58 IST
** Validation 100.00% of 1x98...Epoch=85/150 LR=0.0406 Time=0.637 Loss=1.378 Prec@1=66.708 Prec@5=87.504 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=09:58 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:58 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:58 IST
=> Training   0.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=4.460 DataTime=3.937 Loss=1.095 Prec@1=72.461 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=09:58 IST
=> Training   0.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=4.460 DataTime=3.937 Loss=1.095 Prec@1=72.461 Prec@5=90.430 rate=1344.69 Hz, eta=0:00:01, total=0:00:00, wall=09:58 IST
=> Training   0.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=4.460 DataTime=3.937 Loss=1.095 Prec@1=72.461 Prec@5=90.430 rate=1344.69 Hz, eta=0:00:01, total=0:00:00, wall=09:59 IST
=> Training   0.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.592 DataTime=0.417 Loss=1.130 Prec@1=71.956 Prec@5=90.200 rate=1344.69 Hz, eta=0:00:01, total=0:00:00, wall=09:59 IST
=> Training   4.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.592 DataTime=0.417 Loss=1.130 Prec@1=71.956 Prec@5=90.200 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=09:59 IST
=> Training   4.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.592 DataTime=0.417 Loss=1.130 Prec@1=71.956 Prec@5=90.200 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=10:00 IST
=> Training   4.04% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.576 DataTime=0.400 Loss=1.128 Prec@1=71.931 Prec@5=90.136 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=10:00 IST
=> Training   8.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.576 DataTime=0.400 Loss=1.128 Prec@1=71.931 Prec@5=90.136 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:00 IST
=> Training   8.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.576 DataTime=0.400 Loss=1.128 Prec@1=71.931 Prec@5=90.136 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:01 IST
=> Training   8.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.569 DataTime=0.394 Loss=1.135 Prec@1=71.709 Prec@5=90.089 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:01 IST
=> Training   12.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.569 DataTime=0.394 Loss=1.135 Prec@1=71.709 Prec@5=90.089 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:01 IST
=> Training   12.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.569 DataTime=0.394 Loss=1.135 Prec@1=71.709 Prec@5=90.089 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:02 IST
=> Training   12.03% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.564 DataTime=0.391 Loss=1.136 Prec@1=71.687 Prec@5=90.082 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:02 IST
=> Training   16.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.564 DataTime=0.391 Loss=1.136 Prec@1=71.687 Prec@5=90.082 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:02 IST
=> Training   16.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.564 DataTime=0.391 Loss=1.136 Prec@1=71.687 Prec@5=90.082 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:03 IST
=> Training   16.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.388 Loss=1.142 Prec@1=71.578 Prec@5=90.011 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:03 IST
=> Training   20.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.388 Loss=1.142 Prec@1=71.578 Prec@5=90.011 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:03 IST
=> Training   20.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.388 Loss=1.142 Prec@1=71.578 Prec@5=90.011 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:04 IST
=> Training   20.02% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.389 Loss=1.145 Prec@1=71.460 Prec@5=89.963 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:04 IST
=> Training   24.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.389 Loss=1.145 Prec@1=71.460 Prec@5=89.963 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:04 IST
=> Training   24.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.389 Loss=1.145 Prec@1=71.460 Prec@5=89.963 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:05 IST
=> Training   24.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.389 Loss=1.147 Prec@1=71.424 Prec@5=89.928 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:05 IST
=> Training   28.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.389 Loss=1.147 Prec@1=71.424 Prec@5=89.928 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:05 IST
=> Training   28.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.389 Loss=1.147 Prec@1=71.424 Prec@5=89.928 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:06 IST
=> Training   28.01% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.361 Prec@5=89.900 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:06 IST
=> Training   32.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.361 Prec@5=89.900 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:06 IST
=> Training   32.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.361 Prec@5=89.900 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:07 IST
=> Training   32.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.360 Prec@5=89.903 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:07 IST
=> Training   36.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.360 Prec@5=89.903 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=10:07 IST
=> Training   36.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.563 DataTime=0.388 Loss=1.151 Prec@1=71.360 Prec@5=89.903 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=10:08 IST
=> Training   36.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.153 Prec@1=71.325 Prec@5=89.880 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=10:08 IST
=> Training   39.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.153 Prec@1=71.325 Prec@5=89.880 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:08 IST
=> Training   39.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.153 Prec@1=71.325 Prec@5=89.880 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:09 IST
=> Training   39.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.155 Prec@1=71.258 Prec@5=89.851 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:09 IST
=> Training   43.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.155 Prec@1=71.258 Prec@5=89.851 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:09 IST
=> Training   43.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.155 Prec@1=71.258 Prec@5=89.851 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:10 IST
=> Training   43.99% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.158 Prec@1=71.213 Prec@5=89.812 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:10 IST
=> Training   47.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.158 Prec@1=71.213 Prec@5=89.812 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=10:10 IST
=> Training   47.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.386 Loss=1.158 Prec@1=71.213 Prec@5=89.812 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=10:11 IST
=> Training   47.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.161 Prec@1=71.156 Prec@5=89.767 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=10:11 IST
=> Training   51.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.161 Prec@1=71.156 Prec@5=89.767 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=10:11 IST
=> Training   51.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.161 Prec@1=71.156 Prec@5=89.767 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=10:12 IST
=> Training   51.98% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.162 Prec@1=71.105 Prec@5=89.742 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=10:12 IST
=> Training   55.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.162 Prec@1=71.105 Prec@5=89.742 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=10:12 IST
=> Training   55.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.387 Loss=1.162 Prec@1=71.105 Prec@5=89.742 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=10:12 IST
=> Training   55.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.164 Prec@1=71.076 Prec@5=89.718 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=10:12 IST
=> Training   59.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.164 Prec@1=71.076 Prec@5=89.718 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=10:12 IST
=> Training   59.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.164 Prec@1=71.076 Prec@5=89.718 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=10:13 IST
=> Training   59.97% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.165 Prec@1=71.028 Prec@5=89.711 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=10:13 IST
=> Training   63.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.165 Prec@1=71.028 Prec@5=89.711 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:13 IST
=> Training   63.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.386 Loss=1.165 Prec@1=71.028 Prec@5=89.711 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:14 IST
=> Training   63.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.167 Prec@1=70.989 Prec@5=89.695 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:14 IST
=> Training   67.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.167 Prec@1=70.989 Prec@5=89.695 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=10:14 IST
=> Training   67.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.167 Prec@1=70.989 Prec@5=89.695 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=10:15 IST
=> Training   67.96% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.169 Prec@1=70.941 Prec@5=89.657 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=10:15 IST
=> Training   71.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.169 Prec@1=70.941 Prec@5=89.657 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=10:15 IST
=> Training   71.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.169 Prec@1=70.941 Prec@5=89.657 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=10:16 IST
=> Training   71.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.171 Prec@1=70.892 Prec@5=89.640 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=10:16 IST
=> Training   75.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.171 Prec@1=70.892 Prec@5=89.640 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=10:16 IST
=> Training   75.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.171 Prec@1=70.892 Prec@5=89.640 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=10:17 IST
=> Training   75.95% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.173 Prec@1=70.856 Prec@5=89.618 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=10:17 IST
=> Training   79.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.173 Prec@1=70.856 Prec@5=89.618 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:17 IST
=> Training   79.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.173 Prec@1=70.856 Prec@5=89.618 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:18 IST
=> Training   79.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.385 Loss=1.174 Prec@1=70.820 Prec@5=89.598 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:18 IST
=> Training   83.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.385 Loss=1.174 Prec@1=70.820 Prec@5=89.598 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=10:18 IST
=> Training   83.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.562 DataTime=0.385 Loss=1.174 Prec@1=70.820 Prec@5=89.598 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=10:19 IST
=> Training   83.94% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.175 Prec@1=70.810 Prec@5=89.580 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=10:19 IST
=> Training   87.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.175 Prec@1=70.810 Prec@5=89.580 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=10:19 IST
=> Training   87.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.175 Prec@1=70.810 Prec@5=89.580 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=10:20 IST
=> Training   87.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.177 Prec@1=70.781 Prec@5=89.567 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=10:20 IST
=> Training   91.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.177 Prec@1=70.781 Prec@5=89.567 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:20 IST
=> Training   91.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.385 Loss=1.177 Prec@1=70.781 Prec@5=89.567 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:21 IST
=> Training   91.93% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.384 Loss=1.178 Prec@1=70.762 Prec@5=89.551 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:21 IST
=> Training   95.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.384 Loss=1.178 Prec@1=70.762 Prec@5=89.551 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=10:21 IST
=> Training   95.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.561 DataTime=0.384 Loss=1.178 Prec@1=70.762 Prec@5=89.551 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=10:22 IST
=> Training   95.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.560 DataTime=0.384 Loss=1.179 Prec@1=70.747 Prec@5=89.540 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=10:22 IST
=> Training   99.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.560 DataTime=0.384 Loss=1.179 Prec@1=70.747 Prec@5=89.540 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=10:22 IST
=> Training   99.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.560 DataTime=0.384 Loss=1.179 Prec@1=70.747 Prec@5=89.540 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=10:22 IST
=> Training   99.92% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.560 DataTime=0.384 Loss=1.179 Prec@1=70.745 Prec@5=89.539 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=10:22 IST
=> Training   100.00% of 1x2503...Epoch=86/150 LR=0.0396 Time=0.560 DataTime=0.384 Loss=1.179 Prec@1=70.745 Prec@5=89.539 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=10:22 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> Validation 0.00% of 1x98...Epoch=86/150 LR=0.0396 Time=6.916 Loss=0.962 Prec@1=77.344 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=10:22 IST
=> Validation 1.02% of 1x98...Epoch=86/150 LR=0.0396 Time=6.916 Loss=0.962 Prec@1=77.344 Prec@5=93.164 rate=3830.83 Hz, eta=0:00:00, total=0:00:00, wall=10:22 IST
** Validation 1.02% of 1x98...Epoch=86/150 LR=0.0396 Time=6.916 Loss=0.962 Prec@1=77.344 Prec@5=93.164 rate=3830.83 Hz, eta=0:00:00, total=0:00:00, wall=10:23 IST
** Validation 1.02% of 1x98...Epoch=86/150 LR=0.0396 Time=0.633 Loss=1.383 Prec@1=66.498 Prec@5=87.346 rate=3830.83 Hz, eta=0:00:00, total=0:00:00, wall=10:23 IST
** Validation 100.00% of 1x98...Epoch=86/150 LR=0.0396 Time=0.633 Loss=1.383 Prec@1=66.498 Prec@5=87.346 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=10:23 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:23 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:23 IST
=> Training   0.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=4.803 DataTime=4.294 Loss=1.150 Prec@1=72.656 Prec@5=88.672 rate=0 Hz, eta=?, total=0:00:00, wall=10:23 IST
=> Training   0.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=4.803 DataTime=4.294 Loss=1.150 Prec@1=72.656 Prec@5=88.672 rate=703.88 Hz, eta=0:00:03, total=0:00:00, wall=10:23 IST
=> Training   0.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=4.803 DataTime=4.294 Loss=1.150 Prec@1=72.656 Prec@5=88.672 rate=703.88 Hz, eta=0:00:03, total=0:00:00, wall=10:24 IST
=> Training   0.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.594 DataTime=0.432 Loss=1.130 Prec@1=71.858 Prec@5=90.200 rate=703.88 Hz, eta=0:00:03, total=0:00:00, wall=10:24 IST
=> Training   4.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.594 DataTime=0.432 Loss=1.130 Prec@1=71.858 Prec@5=90.200 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=10:24 IST
=> Training   4.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.594 DataTime=0.432 Loss=1.130 Prec@1=71.858 Prec@5=90.200 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=10:25 IST
=> Training   4.04% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.580 DataTime=0.411 Loss=1.139 Prec@1=71.672 Prec@5=90.091 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=10:25 IST
=> Training   8.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.580 DataTime=0.411 Loss=1.139 Prec@1=71.672 Prec@5=90.091 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=10:25 IST
=> Training   8.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.580 DataTime=0.411 Loss=1.139 Prec@1=71.672 Prec@5=90.091 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=10:26 IST
=> Training   8.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.574 DataTime=0.404 Loss=1.142 Prec@1=71.568 Prec@5=90.004 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=10:26 IST
=> Training   12.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.574 DataTime=0.404 Loss=1.142 Prec@1=71.568 Prec@5=90.004 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=10:26 IST
=> Training   12.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.574 DataTime=0.404 Loss=1.142 Prec@1=71.568 Prec@5=90.004 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=10:27 IST
=> Training   12.03% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.143 Prec@1=71.545 Prec@5=89.994 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=10:27 IST
=> Training   16.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.143 Prec@1=71.545 Prec@5=89.994 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=10:27 IST
=> Training   16.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.143 Prec@1=71.545 Prec@5=89.994 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=10:28 IST
=> Training   16.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.144 Prec@1=71.546 Prec@5=90.012 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=10:28 IST
=> Training   20.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.144 Prec@1=71.546 Prec@5=90.012 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=10:28 IST
=> Training   20.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.144 Prec@1=71.546 Prec@5=90.012 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=10:28 IST
=> Training   20.02% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.396 Loss=1.147 Prec@1=71.490 Prec@5=89.967 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=10:28 IST
=> Training   24.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.396 Loss=1.147 Prec@1=71.490 Prec@5=89.967 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=10:28 IST
=> Training   24.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.396 Loss=1.147 Prec@1=71.490 Prec@5=89.967 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=10:29 IST
=> Training   24.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.147 Prec@1=71.459 Prec@5=89.957 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=10:29 IST
=> Training   28.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.147 Prec@1=71.459 Prec@5=89.957 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=10:29 IST
=> Training   28.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.147 Prec@1=71.459 Prec@5=89.957 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=10:30 IST
=> Training   28.01% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.149 Prec@1=71.436 Prec@5=89.940 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=10:30 IST
=> Training   32.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.149 Prec@1=71.436 Prec@5=89.940 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=10:30 IST
=> Training   32.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.395 Loss=1.149 Prec@1=71.436 Prec@5=89.940 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=10:31 IST
=> Training   32.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.393 Loss=1.149 Prec@1=71.430 Prec@5=89.930 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=10:31 IST
=> Training   36.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.393 Loss=1.149 Prec@1=71.430 Prec@5=89.930 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=10:31 IST
=> Training   36.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.566 DataTime=0.393 Loss=1.149 Prec@1=71.430 Prec@5=89.930 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=10:32 IST
=> Training   36.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.387 Prec@5=89.918 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=10:32 IST
=> Training   39.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.387 Prec@5=89.918 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=10:32 IST
=> Training   39.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.387 Prec@5=89.918 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=10:33 IST
=> Training   39.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.349 Prec@5=89.893 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=10:33 IST
=> Training   43.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.349 Prec@5=89.893 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=10:33 IST
=> Training   43.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.349 Prec@5=89.893 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=10:34 IST
=> Training   43.99% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.315 Prec@5=89.888 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=10:34 IST
=> Training   47.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.315 Prec@5=89.888 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=10:34 IST
=> Training   47.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.390 Loss=1.153 Prec@1=71.315 Prec@5=89.888 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=10:35 IST
=> Training   47.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.389 Loss=1.155 Prec@1=71.278 Prec@5=89.859 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=10:35 IST
=> Training   51.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.389 Loss=1.155 Prec@1=71.278 Prec@5=89.859 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=10:35 IST
=> Training   51.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.389 Loss=1.155 Prec@1=71.278 Prec@5=89.859 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=10:36 IST
=> Training   51.98% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.388 Loss=1.157 Prec@1=71.239 Prec@5=89.829 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=10:36 IST
=> Training   55.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.388 Loss=1.157 Prec@1=71.239 Prec@5=89.829 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=10:36 IST
=> Training   55.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.564 DataTime=0.388 Loss=1.157 Prec@1=71.239 Prec@5=89.829 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=10:37 IST
=> Training   55.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.387 Loss=1.159 Prec@1=71.212 Prec@5=89.808 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=10:37 IST
=> Training   59.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.387 Loss=1.159 Prec@1=71.212 Prec@5=89.808 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=10:37 IST
=> Training   59.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.387 Loss=1.159 Prec@1=71.212 Prec@5=89.808 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=10:38 IST
=> Training   59.97% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.387 Loss=1.160 Prec@1=71.174 Prec@5=89.784 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=10:38 IST
=> Training   63.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.387 Loss=1.160 Prec@1=71.174 Prec@5=89.784 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:38 IST
=> Training   63.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.387 Loss=1.160 Prec@1=71.174 Prec@5=89.784 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:39 IST
=> Training   63.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.162 Prec@1=71.148 Prec@5=89.771 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=10:39 IST
=> Training   67.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.162 Prec@1=71.148 Prec@5=89.771 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:39 IST
=> Training   67.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.162 Prec@1=71.148 Prec@5=89.771 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:40 IST
=> Training   67.96% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.163 Prec@1=71.111 Prec@5=89.752 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:40 IST
=> Training   71.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.163 Prec@1=71.111 Prec@5=89.752 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=10:40 IST
=> Training   71.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.163 Prec@1=71.111 Prec@5=89.752 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=10:41 IST
=> Training   71.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.165 Prec@1=71.076 Prec@5=89.739 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=10:41 IST
=> Training   75.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.165 Prec@1=71.076 Prec@5=89.739 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=10:41 IST
=> Training   75.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.165 Prec@1=71.076 Prec@5=89.739 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=10:42 IST
=> Training   75.95% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.166 Prec@1=71.053 Prec@5=89.716 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=10:42 IST
=> Training   79.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.166 Prec@1=71.053 Prec@5=89.716 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=10:42 IST
=> Training   79.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.166 Prec@1=71.053 Prec@5=89.716 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=10:43 IST
=> Training   79.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.167 Prec@1=71.014 Prec@5=89.706 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=10:43 IST
=> Training   83.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.167 Prec@1=71.014 Prec@5=89.706 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=10:43 IST
=> Training   83.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.167 Prec@1=71.014 Prec@5=89.706 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=10:43 IST
=> Training   83.94% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.169 Prec@1=70.970 Prec@5=89.684 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=10:43 IST
=> Training   87.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.169 Prec@1=70.970 Prec@5=89.684 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=10:43 IST
=> Training   87.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.169 Prec@1=70.970 Prec@5=89.684 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=10:44 IST
=> Training   87.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.389 Loss=1.170 Prec@1=70.948 Prec@5=89.666 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=10:44 IST
=> Training   91.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.389 Loss=1.170 Prec@1=70.948 Prec@5=89.666 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=10:44 IST
=> Training   91.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.389 Loss=1.170 Prec@1=70.948 Prec@5=89.666 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=10:45 IST
=> Training   91.93% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.171 Prec@1=70.932 Prec@5=89.658 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=10:45 IST
=> Training   95.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.171 Prec@1=70.932 Prec@5=89.658 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=10:45 IST
=> Training   95.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.563 DataTime=0.388 Loss=1.171 Prec@1=70.932 Prec@5=89.658 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=10:46 IST
=> Training   95.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.172 Prec@1=70.908 Prec@5=89.646 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=10:46 IST
=> Training   99.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.172 Prec@1=70.908 Prec@5=89.646 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=10:46 IST
=> Training   99.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.388 Loss=1.172 Prec@1=70.908 Prec@5=89.646 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=10:46 IST
=> Training   99.92% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.387 Loss=1.172 Prec@1=70.907 Prec@5=89.645 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=10:46 IST
=> Training   100.00% of 1x2503...Epoch=87/150 LR=0.0386 Time=0.562 DataTime=0.387 Loss=1.172 Prec@1=70.907 Prec@5=89.645 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=10:46 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:46 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:46 IST
=> Validation 0.00% of 1x98...Epoch=87/150 LR=0.0386 Time=6.958 Loss=0.962 Prec@1=75.781 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=10:46 IST
=> Validation 1.02% of 1x98...Epoch=87/150 LR=0.0386 Time=6.958 Loss=0.962 Prec@1=75.781 Prec@5=91.211 rate=350.71 Hz, eta=0:00:00, total=0:00:00, wall=10:46 IST
** Validation 1.02% of 1x98...Epoch=87/150 LR=0.0386 Time=6.958 Loss=0.962 Prec@1=75.781 Prec@5=91.211 rate=350.71 Hz, eta=0:00:00, total=0:00:00, wall=10:47 IST
** Validation 1.02% of 1x98...Epoch=87/150 LR=0.0386 Time=0.628 Loss=1.355 Prec@1=67.056 Prec@5=87.706 rate=350.71 Hz, eta=0:00:00, total=0:00:00, wall=10:47 IST
** Validation 100.00% of 1x98...Epoch=87/150 LR=0.0386 Time=0.628 Loss=1.355 Prec@1=67.056 Prec@5=87.706 rate=1.80 Hz, eta=0:00:00, total=0:00:54, wall=10:47 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:47 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:47 IST
=> Training   0.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=5.116 DataTime=4.710 Loss=1.103 Prec@1=70.312 Prec@5=91.602 rate=0 Hz, eta=?, total=0:00:00, wall=10:47 IST
=> Training   0.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=5.116 DataTime=4.710 Loss=1.103 Prec@1=70.312 Prec@5=91.602 rate=5561.49 Hz, eta=0:00:00, total=0:00:00, wall=10:47 IST
=> Training   0.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=5.116 DataTime=4.710 Loss=1.103 Prec@1=70.312 Prec@5=91.602 rate=5561.49 Hz, eta=0:00:00, total=0:00:00, wall=10:48 IST
=> Training   0.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.591 DataTime=0.431 Loss=1.128 Prec@1=71.889 Prec@5=90.287 rate=5561.49 Hz, eta=0:00:00, total=0:00:00, wall=10:48 IST
=> Training   4.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.591 DataTime=0.431 Loss=1.128 Prec@1=71.889 Prec@5=90.287 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=10:48 IST
=> Training   4.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.591 DataTime=0.431 Loss=1.128 Prec@1=71.889 Prec@5=90.287 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=10:49 IST
=> Training   4.04% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.579 DataTime=0.416 Loss=1.135 Prec@1=71.714 Prec@5=90.196 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=10:49 IST
=> Training   8.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.579 DataTime=0.416 Loss=1.135 Prec@1=71.714 Prec@5=90.196 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:49 IST
=> Training   8.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.579 DataTime=0.416 Loss=1.135 Prec@1=71.714 Prec@5=90.196 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:50 IST
=> Training   8.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.567 DataTime=0.405 Loss=1.138 Prec@1=71.717 Prec@5=90.167 rate=1.81 Hz, eta=0:21:14, total=0:01:51, wall=10:50 IST
=> Training   12.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.567 DataTime=0.405 Loss=1.138 Prec@1=71.717 Prec@5=90.167 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=10:50 IST
=> Training   12.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.567 DataTime=0.405 Loss=1.138 Prec@1=71.717 Prec@5=90.167 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=10:51 IST
=> Training   12.03% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.566 DataTime=0.401 Loss=1.135 Prec@1=71.728 Prec@5=90.173 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=10:51 IST
=> Training   16.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.566 DataTime=0.401 Loss=1.135 Prec@1=71.728 Prec@5=90.173 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:51 IST
=> Training   16.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.566 DataTime=0.401 Loss=1.135 Prec@1=71.728 Prec@5=90.173 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:52 IST
=> Training   16.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.396 Loss=1.138 Prec@1=71.640 Prec@5=90.118 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:52 IST
=> Training   20.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.396 Loss=1.138 Prec@1=71.640 Prec@5=90.118 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:52 IST
=> Training   20.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.396 Loss=1.138 Prec@1=71.640 Prec@5=90.118 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:53 IST
=> Training   20.02% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.395 Loss=1.138 Prec@1=71.657 Prec@5=90.095 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=10:53 IST
=> Training   24.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.395 Loss=1.138 Prec@1=71.657 Prec@5=90.095 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=10:53 IST
=> Training   24.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.564 DataTime=0.395 Loss=1.138 Prec@1=71.657 Prec@5=90.095 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=10:54 IST
=> Training   24.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.140 Prec@1=71.597 Prec@5=90.082 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=10:54 IST
=> Training   28.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.140 Prec@1=71.597 Prec@5=90.082 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=10:54 IST
=> Training   28.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.140 Prec@1=71.597 Prec@5=90.082 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=10:55 IST
=> Training   28.01% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.140 Prec@1=71.589 Prec@5=90.067 rate=1.81 Hz, eta=0:16:38, total=0:06:28, wall=10:55 IST
=> Training   32.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.140 Prec@1=71.589 Prec@5=90.067 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=10:55 IST
=> Training   32.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.140 Prec@1=71.589 Prec@5=90.067 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=10:56 IST
=> Training   32.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.142 Prec@1=71.543 Prec@5=90.040 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=10:56 IST
=> Training   36.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.142 Prec@1=71.543 Prec@5=90.040 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=10:56 IST
=> Training   36.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.562 DataTime=0.391 Loss=1.142 Prec@1=71.543 Prec@5=90.040 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=10:57 IST
=> Training   36.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.144 Prec@1=71.475 Prec@5=90.009 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=10:57 IST
=> Training   39.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.144 Prec@1=71.475 Prec@5=90.009 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=10:57 IST
=> Training   39.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.144 Prec@1=71.475 Prec@5=90.009 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=10:58 IST
=> Training   39.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.146 Prec@1=71.446 Prec@5=89.978 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=10:58 IST
=> Training   43.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.146 Prec@1=71.446 Prec@5=89.978 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=10:58 IST
=> Training   43.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.146 Prec@1=71.446 Prec@5=89.978 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=10:59 IST
=> Training   43.99% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.147 Prec@1=71.409 Prec@5=89.952 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=10:59 IST
=> Training   47.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.147 Prec@1=71.409 Prec@5=89.952 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=10:59 IST
=> Training   47.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.147 Prec@1=71.409 Prec@5=89.952 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=10:59 IST
=> Training   47.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.149 Prec@1=71.383 Prec@5=89.938 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=10:59 IST
=> Training   51.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.149 Prec@1=71.383 Prec@5=89.938 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=10:59 IST
=> Training   51.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.149 Prec@1=71.383 Prec@5=89.938 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=11:00 IST
=> Training   51.98% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.558 DataTime=0.387 Loss=1.151 Prec@1=71.336 Prec@5=89.914 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=11:00 IST
=> Training   55.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.558 DataTime=0.387 Loss=1.151 Prec@1=71.336 Prec@5=89.914 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=11:00 IST
=> Training   55.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.558 DataTime=0.387 Loss=1.151 Prec@1=71.336 Prec@5=89.914 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=11:01 IST
=> Training   55.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.152 Prec@1=71.309 Prec@5=89.902 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=11:01 IST
=> Training   59.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.152 Prec@1=71.309 Prec@5=89.902 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=11:01 IST
=> Training   59.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.152 Prec@1=71.309 Prec@5=89.902 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=11:02 IST
=> Training   59.97% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.153 Prec@1=71.306 Prec@5=89.879 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=11:02 IST
=> Training   63.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.153 Prec@1=71.306 Prec@5=89.879 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=11:02 IST
=> Training   63.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.388 Loss=1.153 Prec@1=71.306 Prec@5=89.879 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=11:03 IST
=> Training   63.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.153 Prec@1=71.287 Prec@5=89.867 rate=1.80 Hz, eta=0:08:21, total=0:14:49, wall=11:03 IST
=> Training   67.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.153 Prec@1=71.287 Prec@5=89.867 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=11:03 IST
=> Training   67.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.153 Prec@1=71.287 Prec@5=89.867 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=11:04 IST
=> Training   67.96% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.389 Loss=1.155 Prec@1=71.263 Prec@5=89.848 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=11:04 IST
=> Training   71.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.389 Loss=1.155 Prec@1=71.263 Prec@5=89.848 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=11:04 IST
=> Training   71.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.559 DataTime=0.389 Loss=1.155 Prec@1=71.263 Prec@5=89.848 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=11:05 IST
=> Training   71.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.155 Prec@1=71.270 Prec@5=89.850 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=11:05 IST
=> Training   75.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.155 Prec@1=71.270 Prec@5=89.850 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=11:05 IST
=> Training   75.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.561 DataTime=0.390 Loss=1.155 Prec@1=71.270 Prec@5=89.850 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=11:06 IST
=> Training   75.95% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.156 Prec@1=71.232 Prec@5=89.825 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=11:06 IST
=> Training   79.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.156 Prec@1=71.232 Prec@5=89.825 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=11:06 IST
=> Training   79.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.389 Loss=1.156 Prec@1=71.232 Prec@5=89.825 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=11:07 IST
=> Training   79.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.158 Prec@1=71.194 Prec@5=89.800 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=11:07 IST
=> Training   83.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.158 Prec@1=71.194 Prec@5=89.800 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=11:07 IST
=> Training   83.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.158 Prec@1=71.194 Prec@5=89.800 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=11:08 IST
=> Training   83.94% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.159 Prec@1=71.179 Prec@5=89.794 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=11:08 IST
=> Training   87.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.159 Prec@1=71.179 Prec@5=89.794 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=11:08 IST
=> Training   87.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.159 Prec@1=71.179 Prec@5=89.794 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=11:09 IST
=> Training   87.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.160 Prec@1=71.168 Prec@5=89.777 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=11:09 IST
=> Training   91.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.160 Prec@1=71.168 Prec@5=89.777 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=11:09 IST
=> Training   91.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.160 Prec@1=71.168 Prec@5=89.777 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=11:10 IST
=> Training   91.93% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.161 Prec@1=71.141 Prec@5=89.760 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=11:10 IST
=> Training   95.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.161 Prec@1=71.141 Prec@5=89.760 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=11:10 IST
=> Training   95.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.387 Loss=1.161 Prec@1=71.141 Prec@5=89.760 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=11:11 IST
=> Training   95.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.162 Prec@1=71.119 Prec@5=89.739 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=11:11 IST
=> Training   99.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.162 Prec@1=71.119 Prec@5=89.739 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=11:11 IST
=> Training   99.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.162 Prec@1=71.119 Prec@5=89.739 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=11:11 IST
=> Training   99.92% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.162 Prec@1=71.118 Prec@5=89.739 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=11:11 IST
=> Training   100.00% of 1x2503...Epoch=88/150 LR=0.0376 Time=0.560 DataTime=0.388 Loss=1.162 Prec@1=71.118 Prec@5=89.739 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=11:11 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:11 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:11 IST
=> Validation 0.00% of 1x98...Epoch=88/150 LR=0.0376 Time=6.745 Loss=0.882 Prec@1=76.758 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=11:11 IST
=> Validation 1.02% of 1x98...Epoch=88/150 LR=0.0376 Time=6.745 Loss=0.882 Prec@1=76.758 Prec@5=92.188 rate=5552.60 Hz, eta=0:00:00, total=0:00:00, wall=11:11 IST
** Validation 1.02% of 1x98...Epoch=88/150 LR=0.0376 Time=6.745 Loss=0.882 Prec@1=76.758 Prec@5=92.188 rate=5552.60 Hz, eta=0:00:00, total=0:00:00, wall=11:12 IST
** Validation 1.02% of 1x98...Epoch=88/150 LR=0.0376 Time=0.641 Loss=1.349 Prec@1=67.410 Prec@5=87.874 rate=5552.60 Hz, eta=0:00:00, total=0:00:00, wall=11:12 IST
** Validation 100.00% of 1x98...Epoch=88/150 LR=0.0376 Time=0.641 Loss=1.349 Prec@1=67.410 Prec@5=87.874 rate=1.75 Hz, eta=0:00:00, total=0:00:56, wall=11:12 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:12 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:12 IST
=> Training   0.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=4.374 DataTime=4.129 Loss=1.071 Prec@1=69.727 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=11:12 IST
=> Training   0.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=4.374 DataTime=4.129 Loss=1.071 Prec@1=69.727 Prec@5=92.188 rate=4400.77 Hz, eta=0:00:00, total=0:00:00, wall=11:12 IST
=> Training   0.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=4.374 DataTime=4.129 Loss=1.071 Prec@1=69.727 Prec@5=92.188 rate=4400.77 Hz, eta=0:00:00, total=0:00:00, wall=11:13 IST
=> Training   0.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.594 DataTime=0.428 Loss=1.125 Prec@1=71.916 Prec@5=90.379 rate=4400.77 Hz, eta=0:00:00, total=0:00:00, wall=11:13 IST
=> Training   4.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.594 DataTime=0.428 Loss=1.125 Prec@1=71.916 Prec@5=90.379 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=11:13 IST
=> Training   4.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.594 DataTime=0.428 Loss=1.125 Prec@1=71.916 Prec@5=90.379 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=11:14 IST
=> Training   4.04% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.578 DataTime=0.411 Loss=1.123 Prec@1=71.863 Prec@5=90.315 rate=1.81 Hz, eta=0:22:03, total=0:00:55, wall=11:14 IST
=> Training   8.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.578 DataTime=0.411 Loss=1.123 Prec@1=71.863 Prec@5=90.315 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=11:14 IST
=> Training   8.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.578 DataTime=0.411 Loss=1.123 Prec@1=71.863 Prec@5=90.315 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=11:15 IST
=> Training   8.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.575 DataTime=0.403 Loss=1.126 Prec@1=71.871 Prec@5=90.265 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=11:15 IST
=> Training   12.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.575 DataTime=0.403 Loss=1.126 Prec@1=71.871 Prec@5=90.265 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=11:15 IST
=> Training   12.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.575 DataTime=0.403 Loss=1.126 Prec@1=71.871 Prec@5=90.265 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=11:16 IST
=> Training   12.03% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.399 Loss=1.129 Prec@1=71.832 Prec@5=90.238 rate=1.78 Hz, eta=0:20:34, total=0:02:48, wall=11:16 IST
=> Training   16.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.399 Loss=1.129 Prec@1=71.832 Prec@5=90.238 rate=1.78 Hz, eta=0:19:37, total=0:03:44, wall=11:16 IST
=> Training   16.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.399 Loss=1.129 Prec@1=71.832 Prec@5=90.238 rate=1.78 Hz, eta=0:19:37, total=0:03:44, wall=11:16 IST
=> Training   16.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.397 Loss=1.130 Prec@1=71.775 Prec@5=90.208 rate=1.78 Hz, eta=0:19:37, total=0:03:44, wall=11:16 IST
=> Training   20.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.397 Loss=1.130 Prec@1=71.775 Prec@5=90.208 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=11:16 IST
=> Training   20.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.397 Loss=1.130 Prec@1=71.775 Prec@5=90.208 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=11:17 IST
=> Training   20.02% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.398 Loss=1.133 Prec@1=71.752 Prec@5=90.190 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=11:17 IST
=> Training   24.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.398 Loss=1.133 Prec@1=71.752 Prec@5=90.190 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=11:17 IST
=> Training   24.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.571 DataTime=0.398 Loss=1.133 Prec@1=71.752 Prec@5=90.190 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=11:18 IST
=> Training   24.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.570 DataTime=0.398 Loss=1.133 Prec@1=71.713 Prec@5=90.186 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=11:18 IST
=> Training   28.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.570 DataTime=0.398 Loss=1.133 Prec@1=71.713 Prec@5=90.186 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=11:18 IST
=> Training   28.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.570 DataTime=0.398 Loss=1.133 Prec@1=71.713 Prec@5=90.186 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=11:19 IST
=> Training   28.01% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.396 Loss=1.134 Prec@1=71.665 Prec@5=90.166 rate=1.77 Hz, eta=0:16:55, total=0:06:35, wall=11:19 IST
=> Training   32.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.396 Loss=1.134 Prec@1=71.665 Prec@5=90.166 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=11:19 IST
=> Training   32.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.396 Loss=1.134 Prec@1=71.665 Prec@5=90.166 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=11:20 IST
=> Training   32.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.397 Loss=1.136 Prec@1=71.630 Prec@5=90.146 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=11:20 IST
=> Training   36.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.397 Loss=1.136 Prec@1=71.630 Prec@5=90.146 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=11:20 IST
=> Training   36.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.568 DataTime=0.397 Loss=1.136 Prec@1=71.630 Prec@5=90.146 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=11:21 IST
=> Training   36.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.395 Loss=1.138 Prec@1=71.572 Prec@5=90.095 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=11:21 IST
=> Training   39.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.395 Loss=1.138 Prec@1=71.572 Prec@5=90.095 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=11:21 IST
=> Training   39.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.395 Loss=1.138 Prec@1=71.572 Prec@5=90.095 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=11:22 IST
=> Training   39.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.394 Loss=1.139 Prec@1=71.531 Prec@5=90.076 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=11:22 IST
=> Training   43.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.394 Loss=1.139 Prec@1=71.531 Prec@5=90.076 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=11:22 IST
=> Training   43.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.394 Loss=1.139 Prec@1=71.531 Prec@5=90.076 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=11:23 IST
=> Training   43.99% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.140 Prec@1=71.523 Prec@5=90.060 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=11:23 IST
=> Training   47.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.140 Prec@1=71.523 Prec@5=90.060 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=11:23 IST
=> Training   47.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.140 Prec@1=71.523 Prec@5=90.060 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=11:24 IST
=> Training   47.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.141 Prec@1=71.511 Prec@5=90.034 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=11:24 IST
=> Training   51.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.141 Prec@1=71.511 Prec@5=90.034 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=11:24 IST
=> Training   51.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.393 Loss=1.141 Prec@1=71.511 Prec@5=90.034 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=11:25 IST
=> Training   51.98% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.392 Loss=1.142 Prec@1=71.502 Prec@5=90.028 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=11:25 IST
=> Training   55.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.392 Loss=1.142 Prec@1=71.502 Prec@5=90.028 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=11:25 IST
=> Training   55.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.565 DataTime=0.392 Loss=1.142 Prec@1=71.502 Prec@5=90.028 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=11:26 IST
=> Training   55.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.393 Loss=1.144 Prec@1=71.472 Prec@5=90.017 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=11:26 IST
=> Training   59.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.393 Loss=1.144 Prec@1=71.472 Prec@5=90.017 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=11:26 IST
=> Training   59.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.566 DataTime=0.393 Loss=1.144 Prec@1=71.472 Prec@5=90.017 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=11:27 IST
=> Training   59.97% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.145 Prec@1=71.436 Prec@5=90.002 rate=1.78 Hz, eta=0:09:23, total=0:14:04, wall=11:27 IST
=> Training   63.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.145 Prec@1=71.436 Prec@5=90.002 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=11:27 IST
=> Training   63.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.145 Prec@1=71.436 Prec@5=90.002 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=11:28 IST
=> Training   63.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.147 Prec@1=71.396 Prec@5=89.975 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=11:28 IST
=> Training   67.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.147 Prec@1=71.396 Prec@5=89.975 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=11:28 IST
=> Training   67.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.392 Loss=1.147 Prec@1=71.396 Prec@5=89.975 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=11:29 IST
=> Training   67.96% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.147 Prec@1=71.377 Prec@5=89.972 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=11:29 IST
=> Training   71.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.147 Prec@1=71.377 Prec@5=89.972 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=11:29 IST
=> Training   71.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.147 Prec@1=71.377 Prec@5=89.972 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=11:30 IST
=> Training   71.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.148 Prec@1=71.354 Prec@5=89.959 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=11:30 IST
=> Training   75.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.148 Prec@1=71.354 Prec@5=89.959 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=11:30 IST
=> Training   75.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.148 Prec@1=71.354 Prec@5=89.959 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=11:31 IST
=> Training   75.95% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.390 Loss=1.149 Prec@1=71.343 Prec@5=89.949 rate=1.78 Hz, eta=0:05:38, total=0:17:48, wall=11:31 IST
=> Training   79.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.390 Loss=1.149 Prec@1=71.343 Prec@5=89.949 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=11:31 IST
=> Training   79.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.390 Loss=1.149 Prec@1=71.343 Prec@5=89.949 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=11:31 IST
=> Training   79.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.150 Prec@1=71.335 Prec@5=89.946 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=11:31 IST
=> Training   83.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.150 Prec@1=71.335 Prec@5=89.946 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=11:31 IST
=> Training   83.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.150 Prec@1=71.335 Prec@5=89.946 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=11:32 IST
=> Training   83.94% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.322 Prec@5=89.930 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=11:32 IST
=> Training   87.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.322 Prec@5=89.930 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=11:32 IST
=> Training   87.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.151 Prec@1=71.322 Prec@5=89.930 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=11:33 IST
=> Training   87.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.152 Prec@1=71.294 Prec@5=89.914 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=11:33 IST
=> Training   91.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.152 Prec@1=71.294 Prec@5=89.914 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=11:33 IST
=> Training   91.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.564 DataTime=0.391 Loss=1.152 Prec@1=71.294 Prec@5=89.914 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=11:34 IST
=> Training   91.93% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.391 Loss=1.154 Prec@1=71.265 Prec@5=89.883 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=11:34 IST
=> Training   95.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.391 Loss=1.154 Prec@1=71.265 Prec@5=89.883 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=11:34 IST
=> Training   95.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.391 Loss=1.154 Prec@1=71.265 Prec@5=89.883 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=11:35 IST
=> Training   95.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.390 Loss=1.155 Prec@1=71.245 Prec@5=89.878 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=11:35 IST
=> Training   99.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.390 Loss=1.155 Prec@1=71.245 Prec@5=89.878 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=11:35 IST
=> Training   99.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.563 DataTime=0.390 Loss=1.155 Prec@1=71.245 Prec@5=89.878 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=11:35 IST
=> Training   99.92% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.562 DataTime=0.390 Loss=1.155 Prec@1=71.245 Prec@5=89.878 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=11:35 IST
=> Training   100.00% of 1x2503...Epoch=89/150 LR=0.0366 Time=0.562 DataTime=0.390 Loss=1.155 Prec@1=71.245 Prec@5=89.878 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=11:35 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST
=> Validation 0.00% of 1x98...Epoch=89/150 LR=0.0366 Time=7.106 Loss=0.969 Prec@1=75.391 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=11:35 IST
=> Validation 1.02% of 1x98...Epoch=89/150 LR=0.0366 Time=7.106 Loss=0.969 Prec@1=75.391 Prec@5=93.359 rate=3144.50 Hz, eta=0:00:00, total=0:00:00, wall=11:35 IST
** Validation 1.02% of 1x98...Epoch=89/150 LR=0.0366 Time=7.106 Loss=0.969 Prec@1=75.391 Prec@5=93.359 rate=3144.50 Hz, eta=0:00:00, total=0:00:00, wall=11:36 IST
** Validation 1.02% of 1x98...Epoch=89/150 LR=0.0366 Time=0.634 Loss=1.385 Prec@1=66.512 Prec@5=87.412 rate=3144.50 Hz, eta=0:00:00, total=0:00:00, wall=11:36 IST
** Validation 100.00% of 1x98...Epoch=89/150 LR=0.0366 Time=0.634 Loss=1.385 Prec@1=66.512 Prec@5=87.412 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=11:36 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:36 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:36 IST
=> Training   0.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=5.267 DataTime=4.858 Loss=1.185 Prec@1=71.289 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=11:36 IST
=> Training   0.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=5.267 DataTime=4.858 Loss=1.185 Prec@1=71.289 Prec@5=89.453 rate=1884.86 Hz, eta=0:00:01, total=0:00:00, wall=11:36 IST
=> Training   0.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=5.267 DataTime=4.858 Loss=1.185 Prec@1=71.289 Prec@5=89.453 rate=1884.86 Hz, eta=0:00:01, total=0:00:00, wall=11:37 IST
=> Training   0.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.590 DataTime=0.424 Loss=1.114 Prec@1=71.941 Prec@5=90.443 rate=1884.86 Hz, eta=0:00:01, total=0:00:00, wall=11:37 IST
=> Training   4.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.590 DataTime=0.424 Loss=1.114 Prec@1=71.941 Prec@5=90.443 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=11:37 IST
=> Training   4.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.590 DataTime=0.424 Loss=1.114 Prec@1=71.941 Prec@5=90.443 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=11:38 IST
=> Training   4.04% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.568 DataTime=0.399 Loss=1.111 Prec@1=72.139 Prec@5=90.424 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=11:38 IST
=> Training   8.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.568 DataTime=0.399 Loss=1.111 Prec@1=72.139 Prec@5=90.424 rate=1.85 Hz, eta=0:20:46, total=0:01:48, wall=11:38 IST
=> Training   8.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.568 DataTime=0.399 Loss=1.111 Prec@1=72.139 Prec@5=90.424 rate=1.85 Hz, eta=0:20:46, total=0:01:48, wall=11:39 IST
=> Training   8.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.111 Prec@1=72.248 Prec@5=90.375 rate=1.85 Hz, eta=0:20:46, total=0:01:48, wall=11:39 IST
=> Training   12.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.111 Prec@1=72.248 Prec@5=90.375 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=11:39 IST
=> Training   12.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.111 Prec@1=72.248 Prec@5=90.375 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=11:40 IST
=> Training   12.03% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.115 Prec@1=72.176 Prec@5=90.332 rate=1.83 Hz, eta=0:20:01, total=0:02:44, wall=11:40 IST
=> Training   16.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.115 Prec@1=72.176 Prec@5=90.332 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=11:40 IST
=> Training   16.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.115 Prec@1=72.176 Prec@5=90.332 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=11:41 IST
=> Training   16.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.566 DataTime=0.394 Loss=1.117 Prec@1=72.162 Prec@5=90.313 rate=1.82 Hz, eta=0:19:13, total=0:03:40, wall=11:41 IST
=> Training   20.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.566 DataTime=0.394 Loss=1.117 Prec@1=72.162 Prec@5=90.313 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=11:41 IST
=> Training   20.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.566 DataTime=0.394 Loss=1.117 Prec@1=72.162 Prec@5=90.313 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=11:42 IST
=> Training   20.02% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.564 DataTime=0.391 Loss=1.119 Prec@1=72.123 Prec@5=90.277 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=11:42 IST
=> Training   24.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.564 DataTime=0.391 Loss=1.119 Prec@1=72.123 Prec@5=90.277 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=11:42 IST
=> Training   24.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.564 DataTime=0.391 Loss=1.119 Prec@1=72.123 Prec@5=90.277 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=11:43 IST
=> Training   24.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.565 DataTime=0.393 Loss=1.120 Prec@1=72.114 Prec@5=90.269 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=11:43 IST
=> Training   28.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.565 DataTime=0.393 Loss=1.120 Prec@1=72.114 Prec@5=90.269 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=11:43 IST
=> Training   28.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.565 DataTime=0.393 Loss=1.120 Prec@1=72.114 Prec@5=90.269 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=11:44 IST
=> Training   28.01% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.124 Prec@1=72.036 Prec@5=90.218 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=11:44 IST
=> Training   32.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.124 Prec@1=72.036 Prec@5=90.218 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=11:44 IST
=> Training   32.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.392 Loss=1.124 Prec@1=72.036 Prec@5=90.218 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=11:45 IST
=> Training   32.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.391 Loss=1.124 Prec@1=72.011 Prec@5=90.208 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=11:45 IST
=> Training   36.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.391 Loss=1.124 Prec@1=72.011 Prec@5=90.208 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=11:45 IST
=> Training   36.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.563 DataTime=0.391 Loss=1.124 Prec@1=72.011 Prec@5=90.208 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=11:46 IST
=> Training   36.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.127 Prec@1=71.977 Prec@5=90.187 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=11:46 IST
=> Training   39.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.127 Prec@1=71.977 Prec@5=90.187 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:46 IST
=> Training   39.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.127 Prec@1=71.977 Prec@5=90.187 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:47 IST
=> Training   39.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.128 Prec@1=71.930 Prec@5=90.163 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:47 IST
=> Training   43.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.128 Prec@1=71.930 Prec@5=90.163 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=11:47 IST
=> Training   43.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.128 Prec@1=71.930 Prec@5=90.163 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=11:47 IST
=> Training   43.99% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.129 Prec@1=71.909 Prec@5=90.143 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=11:47 IST
=> Training   47.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.129 Prec@1=71.909 Prec@5=90.143 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=11:47 IST
=> Training   47.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.129 Prec@1=71.909 Prec@5=90.143 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=11:48 IST
=> Training   47.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.133 Prec@1=71.842 Prec@5=90.100 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=11:48 IST
=> Training   51.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.133 Prec@1=71.842 Prec@5=90.100 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=11:48 IST
=> Training   51.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.133 Prec@1=71.842 Prec@5=90.100 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=11:49 IST
=> Training   51.98% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.135 Prec@1=71.796 Prec@5=90.061 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=11:49 IST
=> Training   55.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.135 Prec@1=71.796 Prec@5=90.061 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:49 IST
=> Training   55.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.135 Prec@1=71.796 Prec@5=90.061 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:50 IST
=> Training   55.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.136 Prec@1=71.749 Prec@5=90.053 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:50 IST
=> Training   59.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.136 Prec@1=71.749 Prec@5=90.053 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=11:50 IST
=> Training   59.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.388 Loss=1.136 Prec@1=71.749 Prec@5=90.053 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=11:51 IST
=> Training   59.97% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.138 Prec@1=71.715 Prec@5=90.039 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=11:51 IST
=> Training   63.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.138 Prec@1=71.715 Prec@5=90.039 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:51 IST
=> Training   63.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.138 Prec@1=71.715 Prec@5=90.039 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:52 IST
=> Training   63.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.139 Prec@1=71.682 Prec@5=90.029 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:52 IST
=> Training   67.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.139 Prec@1=71.682 Prec@5=90.029 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=11:52 IST
=> Training   67.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.139 Prec@1=71.682 Prec@5=90.029 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=11:53 IST
=> Training   67.96% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.140 Prec@1=71.662 Prec@5=90.020 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=11:53 IST
=> Training   71.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.140 Prec@1=71.662 Prec@5=90.020 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=11:53 IST
=> Training   71.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.389 Loss=1.140 Prec@1=71.662 Prec@5=90.020 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=11:54 IST
=> Training   71.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.142 Prec@1=71.627 Prec@5=89.996 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=11:54 IST
=> Training   75.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.142 Prec@1=71.627 Prec@5=89.996 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:54 IST
=> Training   75.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.142 Prec@1=71.627 Prec@5=89.996 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:55 IST
=> Training   75.95% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.143 Prec@1=71.583 Prec@5=89.977 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:55 IST
=> Training   79.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.143 Prec@1=71.583 Prec@5=89.977 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=11:55 IST
=> Training   79.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.143 Prec@1=71.583 Prec@5=89.977 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=11:56 IST
=> Training   79.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.144 Prec@1=71.569 Prec@5=89.966 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=11:56 IST
=> Training   83.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.144 Prec@1=71.569 Prec@5=89.966 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=11:56 IST
=> Training   83.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.144 Prec@1=71.569 Prec@5=89.966 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=11:57 IST
=> Training   83.94% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.145 Prec@1=71.548 Prec@5=89.956 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=11:57 IST
=> Training   87.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.145 Prec@1=71.548 Prec@5=89.956 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:57 IST
=> Training   87.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.145 Prec@1=71.548 Prec@5=89.956 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:58 IST
=> Training   87.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.146 Prec@1=71.531 Prec@5=89.944 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:58 IST
=> Training   91.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.146 Prec@1=71.531 Prec@5=89.944 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:58 IST
=> Training   91.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.390 Loss=1.146 Prec@1=71.531 Prec@5=89.944 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:59 IST
=> Training   91.93% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.147 Prec@1=71.508 Prec@5=89.934 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:59 IST
=> Training   95.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.147 Prec@1=71.508 Prec@5=89.934 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=11:59 IST
=> Training   95.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.562 DataTime=0.390 Loss=1.147 Prec@1=71.508 Prec@5=89.934 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:00 IST
=> Training   95.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.148 Prec@1=71.481 Prec@5=89.916 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:00 IST
=> Training   99.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.148 Prec@1=71.481 Prec@5=89.916 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:00 IST
=> Training   99.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.148 Prec@1=71.481 Prec@5=89.916 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:00 IST
=> Training   99.92% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.148 Prec@1=71.480 Prec@5=89.915 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:00 IST
=> Training   100.00% of 1x2503...Epoch=90/150 LR=0.0355 Time=0.561 DataTime=0.389 Loss=1.148 Prec@1=71.480 Prec@5=89.915 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=12:00 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST
=> Validation 0.00% of 1x98...Epoch=90/150 LR=0.0355 Time=7.046 Loss=0.945 Prec@1=76.172 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=12:00 IST
=> Validation 1.02% of 1x98...Epoch=90/150 LR=0.0355 Time=7.046 Loss=0.945 Prec@1=76.172 Prec@5=91.406 rate=6469.14 Hz, eta=0:00:00, total=0:00:00, wall=12:00 IST
** Validation 1.02% of 1x98...Epoch=90/150 LR=0.0355 Time=7.046 Loss=0.945 Prec@1=76.172 Prec@5=91.406 rate=6469.14 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST
** Validation 1.02% of 1x98...Epoch=90/150 LR=0.0355 Time=0.630 Loss=1.374 Prec@1=66.720 Prec@5=87.498 rate=6469.14 Hz, eta=0:00:00, total=0:00:00, wall=12:01 IST
** Validation 100.00% of 1x98...Epoch=90/150 LR=0.0355 Time=0.630 Loss=1.374 Prec@1=66.720 Prec@5=87.498 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=12:01 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> Training   0.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=4.909 DataTime=4.236 Loss=1.151 Prec@1=72.070 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=12:01 IST
=> Training   0.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=4.909 DataTime=4.236 Loss=1.151 Prec@1=72.070 Prec@5=90.234 rate=2388.19 Hz, eta=0:00:01, total=0:00:00, wall=12:01 IST
=> Training   0.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=4.909 DataTime=4.236 Loss=1.151 Prec@1=72.070 Prec@5=90.234 rate=2388.19 Hz, eta=0:00:01, total=0:00:00, wall=12:02 IST
=> Training   0.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.595 DataTime=0.422 Loss=1.112 Prec@1=72.271 Prec@5=90.430 rate=2388.19 Hz, eta=0:00:01, total=0:00:00, wall=12:02 IST
=> Training   4.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.595 DataTime=0.422 Loss=1.112 Prec@1=72.271 Prec@5=90.430 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=12:02 IST
=> Training   4.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.595 DataTime=0.422 Loss=1.112 Prec@1=72.271 Prec@5=90.430 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=12:03 IST
=> Training   4.04% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.579 DataTime=0.400 Loss=1.102 Prec@1=72.509 Prec@5=90.554 rate=1.83 Hz, eta=0:21:52, total=0:00:55, wall=12:03 IST
=> Training   8.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.579 DataTime=0.400 Loss=1.102 Prec@1=72.509 Prec@5=90.554 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=12:03 IST
=> Training   8.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.579 DataTime=0.400 Loss=1.102 Prec@1=72.509 Prec@5=90.554 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=12:04 IST
=> Training   8.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.571 DataTime=0.394 Loss=1.104 Prec@1=72.471 Prec@5=90.553 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=12:04 IST
=> Training   12.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.571 DataTime=0.394 Loss=1.104 Prec@1=72.471 Prec@5=90.553 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=12:04 IST
=> Training   12.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.571 DataTime=0.394 Loss=1.104 Prec@1=72.471 Prec@5=90.553 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=12:05 IST
=> Training   12.03% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.570 DataTime=0.392 Loss=1.106 Prec@1=72.396 Prec@5=90.530 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=12:05 IST
=> Training   16.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.570 DataTime=0.392 Loss=1.106 Prec@1=72.396 Prec@5=90.530 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=12:05 IST
=> Training   16.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.570 DataTime=0.392 Loss=1.106 Prec@1=72.396 Prec@5=90.530 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=12:05 IST
=> Training   16.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.567 DataTime=0.389 Loss=1.106 Prec@1=72.404 Prec@5=90.529 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=12:05 IST
=> Training   20.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.567 DataTime=0.389 Loss=1.106 Prec@1=72.404 Prec@5=90.529 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=12:05 IST
=> Training   20.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.567 DataTime=0.389 Loss=1.106 Prec@1=72.404 Prec@5=90.529 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=12:06 IST
=> Training   20.02% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.566 DataTime=0.390 Loss=1.109 Prec@1=72.335 Prec@5=90.471 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=12:06 IST
=> Training   24.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.566 DataTime=0.390 Loss=1.109 Prec@1=72.335 Prec@5=90.471 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=12:06 IST
=> Training   24.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.566 DataTime=0.390 Loss=1.109 Prec@1=72.335 Prec@5=90.471 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=12:07 IST
=> Training   24.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.565 DataTime=0.389 Loss=1.111 Prec@1=72.278 Prec@5=90.457 rate=1.79 Hz, eta=0:17:40, total=0:05:35, wall=12:07 IST
=> Training   28.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.565 DataTime=0.389 Loss=1.111 Prec@1=72.278 Prec@5=90.457 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=12:07 IST
=> Training   28.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.565 DataTime=0.389 Loss=1.111 Prec@1=72.278 Prec@5=90.457 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=12:08 IST
=> Training   28.01% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.112 Prec@1=72.246 Prec@5=90.436 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=12:08 IST
=> Training   32.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.112 Prec@1=72.246 Prec@5=90.436 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=12:08 IST
=> Training   32.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.112 Prec@1=72.246 Prec@5=90.436 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=12:09 IST
=> Training   32.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.116 Prec@1=72.186 Prec@5=90.378 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=12:09 IST
=> Training   36.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.116 Prec@1=72.186 Prec@5=90.378 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=12:09 IST
=> Training   36.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.564 DataTime=0.388 Loss=1.116 Prec@1=72.186 Prec@5=90.378 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=12:10 IST
=> Training   36.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.388 Loss=1.117 Prec@1=72.152 Prec@5=90.351 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=12:10 IST
=> Training   39.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.388 Loss=1.117 Prec@1=72.152 Prec@5=90.351 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:10 IST
=> Training   39.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.388 Loss=1.117 Prec@1=72.152 Prec@5=90.351 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:11 IST
=> Training   39.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.387 Loss=1.120 Prec@1=72.064 Prec@5=90.325 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:11 IST
=> Training   43.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.387 Loss=1.120 Prec@1=72.064 Prec@5=90.325 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=12:11 IST
=> Training   43.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.563 DataTime=0.387 Loss=1.120 Prec@1=72.064 Prec@5=90.325 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=12:12 IST
=> Training   43.99% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.386 Loss=1.122 Prec@1=72.012 Prec@5=90.298 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=12:12 IST
=> Training   47.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.386 Loss=1.122 Prec@1=72.012 Prec@5=90.298 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=12:12 IST
=> Training   47.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.386 Loss=1.122 Prec@1=72.012 Prec@5=90.298 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=12:13 IST
=> Training   47.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.123 Prec@1=71.961 Prec@5=90.284 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=12:13 IST
=> Training   51.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.123 Prec@1=71.961 Prec@5=90.284 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=12:13 IST
=> Training   51.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.123 Prec@1=71.961 Prec@5=90.284 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=12:14 IST
=> Training   51.98% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.125 Prec@1=71.931 Prec@5=90.262 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=12:14 IST
=> Training   55.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.125 Prec@1=71.931 Prec@5=90.262 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=12:14 IST
=> Training   55.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.125 Prec@1=71.931 Prec@5=90.262 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=12:15 IST
=> Training   55.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.385 Loss=1.126 Prec@1=71.900 Prec@5=90.249 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=12:15 IST
=> Training   59.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.385 Loss=1.126 Prec@1=71.900 Prec@5=90.249 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=12:15 IST
=> Training   59.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.385 Loss=1.126 Prec@1=71.900 Prec@5=90.249 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=12:16 IST
=> Training   59.97% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.384 Loss=1.127 Prec@1=71.863 Prec@5=90.232 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=12:16 IST
=> Training   63.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.384 Loss=1.127 Prec@1=71.863 Prec@5=90.232 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=12:16 IST
=> Training   63.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.384 Loss=1.127 Prec@1=71.863 Prec@5=90.232 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=12:17 IST
=> Training   63.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.128 Prec@1=71.833 Prec@5=90.208 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=12:17 IST
=> Training   67.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.128 Prec@1=71.833 Prec@5=90.208 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=12:17 IST
=> Training   67.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.385 Loss=1.128 Prec@1=71.833 Prec@5=90.208 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=12:18 IST
=> Training   67.96% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.130 Prec@1=71.805 Prec@5=90.196 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=12:18 IST
=> Training   71.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.130 Prec@1=71.805 Prec@5=90.196 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:18 IST
=> Training   71.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.130 Prec@1=71.805 Prec@5=90.196 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:18 IST
=> Training   71.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.387 Loss=1.131 Prec@1=71.772 Prec@5=90.165 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:18 IST
=> Training   75.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.387 Loss=1.131 Prec@1=71.772 Prec@5=90.165 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:18 IST
=> Training   75.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.562 DataTime=0.387 Loss=1.131 Prec@1=71.772 Prec@5=90.165 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:19 IST
=> Training   75.95% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.132 Prec@1=71.764 Prec@5=90.155 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:19 IST
=> Training   79.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.132 Prec@1=71.764 Prec@5=90.155 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=12:19 IST
=> Training   79.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.132 Prec@1=71.764 Prec@5=90.155 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=12:20 IST
=> Training   79.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.387 Loss=1.133 Prec@1=71.751 Prec@5=90.148 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=12:20 IST
=> Training   83.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.387 Loss=1.133 Prec@1=71.751 Prec@5=90.148 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=12:20 IST
=> Training   83.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.387 Loss=1.133 Prec@1=71.751 Prec@5=90.148 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=12:21 IST
=> Training   83.94% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.134 Prec@1=71.731 Prec@5=90.139 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=12:21 IST
=> Training   87.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.134 Prec@1=71.731 Prec@5=90.139 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=12:21 IST
=> Training   87.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.134 Prec@1=71.731 Prec@5=90.139 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=12:22 IST
=> Training   87.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.135 Prec@1=71.700 Prec@5=90.116 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=12:22 IST
=> Training   91.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.135 Prec@1=71.700 Prec@5=90.116 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=12:22 IST
=> Training   91.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.135 Prec@1=71.700 Prec@5=90.116 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=12:23 IST
=> Training   91.93% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.137 Prec@1=71.670 Prec@5=90.096 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=12:23 IST
=> Training   95.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.137 Prec@1=71.670 Prec@5=90.096 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=12:23 IST
=> Training   95.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.561 DataTime=0.386 Loss=1.137 Prec@1=71.670 Prec@5=90.096 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=12:24 IST
=> Training   95.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.386 Loss=1.138 Prec@1=71.639 Prec@5=90.080 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=12:24 IST
=> Training   99.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.386 Loss=1.138 Prec@1=71.639 Prec@5=90.080 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=12:24 IST
=> Training   99.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.386 Loss=1.138 Prec@1=71.639 Prec@5=90.080 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=12:24 IST
=> Training   99.92% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.386 Loss=1.138 Prec@1=71.639 Prec@5=90.080 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=12:24 IST
=> Training   100.00% of 1x2503...Epoch=91/150 LR=0.0345 Time=0.560 DataTime=0.386 Loss=1.138 Prec@1=71.639 Prec@5=90.080 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=12:24 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> Validation 0.00% of 1x98...Epoch=91/150 LR=0.0345 Time=7.116 Loss=0.895 Prec@1=76.367 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=12:24 IST
=> Validation 1.02% of 1x98...Epoch=91/150 LR=0.0345 Time=7.116 Loss=0.895 Prec@1=76.367 Prec@5=94.141 rate=6171.43 Hz, eta=0:00:00, total=0:00:00, wall=12:24 IST
** Validation 1.02% of 1x98...Epoch=91/150 LR=0.0345 Time=7.116 Loss=0.895 Prec@1=76.367 Prec@5=94.141 rate=6171.43 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
** Validation 1.02% of 1x98...Epoch=91/150 LR=0.0345 Time=0.635 Loss=1.355 Prec@1=67.366 Prec@5=87.952 rate=6171.43 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
** Validation 100.00% of 1x98...Epoch=91/150 LR=0.0345 Time=0.635 Loss=1.355 Prec@1=67.366 Prec@5=87.952 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=12:25 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> Training   0.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=4.943 DataTime=4.716 Loss=1.187 Prec@1=69.727 Prec@5=90.820 rate=0 Hz, eta=?, total=0:00:00, wall=12:25 IST
=> Training   0.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=4.943 DataTime=4.716 Loss=1.187 Prec@1=69.727 Prec@5=90.820 rate=4683.07 Hz, eta=0:00:00, total=0:00:00, wall=12:25 IST
=> Training   0.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=4.943 DataTime=4.716 Loss=1.187 Prec@1=69.727 Prec@5=90.820 rate=4683.07 Hz, eta=0:00:00, total=0:00:00, wall=12:26 IST
=> Training   0.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.590 DataTime=0.426 Loss=1.102 Prec@1=72.507 Prec@5=90.540 rate=4683.07 Hz, eta=0:00:00, total=0:00:00, wall=12:26 IST
=> Training   4.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.590 DataTime=0.426 Loss=1.102 Prec@1=72.507 Prec@5=90.540 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=12:26 IST
=> Training   4.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.590 DataTime=0.426 Loss=1.102 Prec@1=72.507 Prec@5=90.540 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=12:27 IST
=> Training   4.04% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.575 DataTime=0.406 Loss=1.097 Prec@1=72.481 Prec@5=90.592 rate=1.85 Hz, eta=0:21:39, total=0:00:54, wall=12:27 IST
=> Training   8.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.575 DataTime=0.406 Loss=1.097 Prec@1=72.481 Prec@5=90.592 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=12:27 IST
=> Training   8.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.575 DataTime=0.406 Loss=1.097 Prec@1=72.481 Prec@5=90.592 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=12:28 IST
=> Training   8.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.570 DataTime=0.399 Loss=1.098 Prec@1=72.549 Prec@5=90.577 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=12:28 IST
=> Training   12.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.570 DataTime=0.399 Loss=1.098 Prec@1=72.549 Prec@5=90.577 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=12:28 IST
=> Training   12.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.570 DataTime=0.399 Loss=1.098 Prec@1=72.549 Prec@5=90.577 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=12:29 IST
=> Training   12.03% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.394 Loss=1.103 Prec@1=72.488 Prec@5=90.475 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=12:29 IST
=> Training   16.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.394 Loss=1.103 Prec@1=72.488 Prec@5=90.475 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=12:29 IST
=> Training   16.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.394 Loss=1.103 Prec@1=72.488 Prec@5=90.475 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=12:30 IST
=> Training   16.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.396 Loss=1.102 Prec@1=72.504 Prec@5=90.525 rate=1.81 Hz, eta=0:19:23, total=0:03:41, wall=12:30 IST
=> Training   20.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.396 Loss=1.102 Prec@1=72.504 Prec@5=90.525 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:30 IST
=> Training   20.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.566 DataTime=0.396 Loss=1.102 Prec@1=72.504 Prec@5=90.525 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:31 IST
=> Training   20.02% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.392 Loss=1.103 Prec@1=72.443 Prec@5=90.519 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:31 IST
=> Training   24.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.392 Loss=1.103 Prec@1=72.443 Prec@5=90.519 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:31 IST
=> Training   24.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.392 Loss=1.103 Prec@1=72.443 Prec@5=90.519 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:32 IST
=> Training   24.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.341 Prec@5=90.449 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:32 IST
=> Training   28.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.341 Prec@5=90.449 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=12:32 IST
=> Training   28.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.341 Prec@5=90.449 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=12:33 IST
=> Training   28.01% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.308 Prec@5=90.443 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=12:33 IST
=> Training   32.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.308 Prec@5=90.443 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=12:33 IST
=> Training   32.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.393 Loss=1.108 Prec@1=72.308 Prec@5=90.443 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=12:34 IST
=> Training   32.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.393 Loss=1.110 Prec@1=72.289 Prec@5=90.441 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=12:34 IST
=> Training   36.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.393 Loss=1.110 Prec@1=72.289 Prec@5=90.441 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:34 IST
=> Training   36.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.393 Loss=1.110 Prec@1=72.289 Prec@5=90.441 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:35 IST
=> Training   36.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.392 Loss=1.112 Prec@1=72.236 Prec@5=90.428 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:35 IST
=> Training   39.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.392 Loss=1.112 Prec@1=72.236 Prec@5=90.428 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:35 IST
=> Training   39.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.392 Loss=1.112 Prec@1=72.236 Prec@5=90.428 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:35 IST
=> Training   39.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.391 Loss=1.114 Prec@1=72.174 Prec@5=90.394 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=12:35 IST
=> Training   43.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.391 Loss=1.114 Prec@1=72.174 Prec@5=90.394 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=12:35 IST
=> Training   43.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.564 DataTime=0.391 Loss=1.114 Prec@1=72.174 Prec@5=90.394 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=12:36 IST
=> Training   43.99% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.115 Prec@1=72.156 Prec@5=90.383 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=12:36 IST
=> Training   47.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.115 Prec@1=72.156 Prec@5=90.383 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=12:36 IST
=> Training   47.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.115 Prec@1=72.156 Prec@5=90.383 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=12:37 IST
=> Training   47.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.116 Prec@1=72.127 Prec@5=90.360 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=12:37 IST
=> Training   51.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.116 Prec@1=72.127 Prec@5=90.360 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=12:37 IST
=> Training   51.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.116 Prec@1=72.127 Prec@5=90.360 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=12:38 IST
=> Training   51.98% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.117 Prec@1=72.101 Prec@5=90.350 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=12:38 IST
=> Training   55.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.117 Prec@1=72.101 Prec@5=90.350 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=12:38 IST
=> Training   55.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.117 Prec@1=72.101 Prec@5=90.350 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=12:39 IST
=> Training   55.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.118 Prec@1=72.087 Prec@5=90.334 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=12:39 IST
=> Training   59.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.118 Prec@1=72.087 Prec@5=90.334 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=12:39 IST
=> Training   59.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.118 Prec@1=72.087 Prec@5=90.334 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=12:40 IST
=> Training   59.97% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.119 Prec@1=72.046 Prec@5=90.316 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=12:40 IST
=> Training   63.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.119 Prec@1=72.046 Prec@5=90.316 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=12:40 IST
=> Training   63.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.563 DataTime=0.391 Loss=1.119 Prec@1=72.046 Prec@5=90.316 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=12:41 IST
=> Training   63.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.121 Prec@1=71.995 Prec@5=90.297 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=12:41 IST
=> Training   67.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.121 Prec@1=71.995 Prec@5=90.297 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=12:41 IST
=> Training   67.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.121 Prec@1=71.995 Prec@5=90.297 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=12:42 IST
=> Training   67.96% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.122 Prec@1=71.970 Prec@5=90.278 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=12:42 IST
=> Training   71.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.122 Prec@1=71.970 Prec@5=90.278 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:42 IST
=> Training   71.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.122 Prec@1=71.970 Prec@5=90.278 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:43 IST
=> Training   71.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.125 Prec@1=71.927 Prec@5=90.251 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=12:43 IST
=> Training   75.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.125 Prec@1=71.927 Prec@5=90.251 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:43 IST
=> Training   75.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.125 Prec@1=71.927 Prec@5=90.251 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:44 IST
=> Training   75.95% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.126 Prec@1=71.909 Prec@5=90.236 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=12:44 IST
=> Training   79.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.126 Prec@1=71.909 Prec@5=90.236 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=12:44 IST
=> Training   79.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.126 Prec@1=71.909 Prec@5=90.236 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=12:45 IST
=> Training   79.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.127 Prec@1=71.884 Prec@5=90.223 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=12:45 IST
=> Training   83.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.127 Prec@1=71.884 Prec@5=90.223 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:45 IST
=> Training   83.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.127 Prec@1=71.884 Prec@5=90.223 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:46 IST
=> Training   83.94% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.128 Prec@1=71.845 Prec@5=90.203 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:46 IST
=> Training   87.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.128 Prec@1=71.845 Prec@5=90.203 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=12:46 IST
=> Training   87.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.128 Prec@1=71.845 Prec@5=90.203 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=12:47 IST
=> Training   87.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.129 Prec@1=71.815 Prec@5=90.187 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=12:47 IST
=> Training   91.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.129 Prec@1=71.815 Prec@5=90.187 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:47 IST
=> Training   91.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.390 Loss=1.129 Prec@1=71.815 Prec@5=90.187 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:48 IST
=> Training   91.93% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.130 Prec@1=71.808 Prec@5=90.176 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:48 IST
=> Training   95.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.130 Prec@1=71.808 Prec@5=90.176 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:48 IST
=> Training   95.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.562 DataTime=0.389 Loss=1.130 Prec@1=71.808 Prec@5=90.176 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:49 IST
=> Training   95.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.561 DataTime=0.389 Loss=1.131 Prec@1=71.781 Prec@5=90.158 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:49 IST
=> Training   99.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.561 DataTime=0.389 Loss=1.131 Prec@1=71.781 Prec@5=90.158 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=12:49 IST
=> Training   99.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.561 DataTime=0.389 Loss=1.131 Prec@1=71.781 Prec@5=90.158 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=12:49 IST
=> Training   99.92% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.561 DataTime=0.389 Loss=1.131 Prec@1=71.782 Prec@5=90.158 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=12:49 IST
=> Training   100.00% of 1x2503...Epoch=92/150 LR=0.0336 Time=0.561 DataTime=0.389 Loss=1.131 Prec@1=71.782 Prec@5=90.158 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=12:49 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:49 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:49 IST
=> Validation 0.00% of 1x98...Epoch=92/150 LR=0.0336 Time=6.806 Loss=0.829 Prec@1=78.516 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=12:49 IST
=> Validation 1.02% of 1x98...Epoch=92/150 LR=0.0336 Time=6.806 Loss=0.829 Prec@1=78.516 Prec@5=93.359 rate=2821.11 Hz, eta=0:00:00, total=0:00:00, wall=12:49 IST
** Validation 1.02% of 1x98...Epoch=92/150 LR=0.0336 Time=6.806 Loss=0.829 Prec@1=78.516 Prec@5=93.359 rate=2821.11 Hz, eta=0:00:00, total=0:00:00, wall=12:50 IST
** Validation 1.02% of 1x98...Epoch=92/150 LR=0.0336 Time=0.635 Loss=1.352 Prec@1=67.594 Prec@5=87.988 rate=2821.11 Hz, eta=0:00:00, total=0:00:00, wall=12:50 IST
** Validation 100.00% of 1x98...Epoch=92/150 LR=0.0336 Time=0.635 Loss=1.352 Prec@1=67.594 Prec@5=87.988 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=12:50 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:50 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:50 IST
=> Training   0.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=4.827 DataTime=4.445 Loss=1.203 Prec@1=70.898 Prec@5=89.062 rate=0 Hz, eta=?, total=0:00:00, wall=12:50 IST
=> Training   0.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=4.827 DataTime=4.445 Loss=1.203 Prec@1=70.898 Prec@5=89.062 rate=1890.03 Hz, eta=0:00:01, total=0:00:00, wall=12:50 IST
=> Training   0.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=4.827 DataTime=4.445 Loss=1.203 Prec@1=70.898 Prec@5=89.062 rate=1890.03 Hz, eta=0:00:01, total=0:00:00, wall=12:51 IST
=> Training   0.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.579 DataTime=0.415 Loss=1.087 Prec@1=72.890 Prec@5=90.687 rate=1890.03 Hz, eta=0:00:01, total=0:00:00, wall=12:51 IST
=> Training   4.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.579 DataTime=0.415 Loss=1.087 Prec@1=72.890 Prec@5=90.687 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=12:51 IST
=> Training   4.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.579 DataTime=0.415 Loss=1.087 Prec@1=72.890 Prec@5=90.687 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=12:51 IST
=> Training   4.04% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.571 DataTime=0.405 Loss=1.092 Prec@1=72.679 Prec@5=90.613 rate=1.88 Hz, eta=0:21:15, total=0:00:53, wall=12:51 IST
=> Training   8.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.571 DataTime=0.405 Loss=1.092 Prec@1=72.679 Prec@5=90.613 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=12:51 IST
=> Training   8.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.571 DataTime=0.405 Loss=1.092 Prec@1=72.679 Prec@5=90.613 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=12:52 IST
=> Training   8.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.570 DataTime=0.399 Loss=1.092 Prec@1=72.737 Prec@5=90.641 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=12:52 IST
=> Training   12.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.570 DataTime=0.399 Loss=1.092 Prec@1=72.737 Prec@5=90.641 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=12:52 IST
=> Training   12.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.570 DataTime=0.399 Loss=1.092 Prec@1=72.737 Prec@5=90.641 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=12:53 IST
=> Training   12.03% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.397 Loss=1.093 Prec@1=72.726 Prec@5=90.621 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=12:53 IST
=> Training   16.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.397 Loss=1.093 Prec@1=72.726 Prec@5=90.621 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=12:53 IST
=> Training   16.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.397 Loss=1.093 Prec@1=72.726 Prec@5=90.621 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=12:54 IST
=> Training   16.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.395 Loss=1.095 Prec@1=72.707 Prec@5=90.632 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=12:54 IST
=> Training   20.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.395 Loss=1.095 Prec@1=72.707 Prec@5=90.632 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=12:54 IST
=> Training   20.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.568 DataTime=0.395 Loss=1.095 Prec@1=72.707 Prec@5=90.632 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=12:55 IST
=> Training   20.02% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.390 Loss=1.098 Prec@1=72.619 Prec@5=90.570 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=12:55 IST
=> Training   24.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.390 Loss=1.098 Prec@1=72.619 Prec@5=90.570 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=12:55 IST
=> Training   24.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.390 Loss=1.098 Prec@1=72.619 Prec@5=90.570 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=12:56 IST
=> Training   24.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.389 Loss=1.102 Prec@1=72.515 Prec@5=90.543 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=12:56 IST
=> Training   28.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.389 Loss=1.102 Prec@1=72.515 Prec@5=90.543 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=12:56 IST
=> Training   28.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.565 DataTime=0.389 Loss=1.102 Prec@1=72.515 Prec@5=90.543 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=12:57 IST
=> Training   28.01% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.102 Prec@1=72.523 Prec@5=90.536 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=12:57 IST
=> Training   32.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.102 Prec@1=72.523 Prec@5=90.536 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=12:57 IST
=> Training   32.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.102 Prec@1=72.523 Prec@5=90.536 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=12:58 IST
=> Training   32.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.103 Prec@1=72.486 Prec@5=90.519 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=12:58 IST
=> Training   36.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.103 Prec@1=72.486 Prec@5=90.519 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:58 IST
=> Training   36.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.103 Prec@1=72.486 Prec@5=90.519 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:59 IST
=> Training   36.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.106 Prec@1=72.421 Prec@5=90.470 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=12:59 IST
=> Training   39.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.106 Prec@1=72.421 Prec@5=90.470 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=12:59 IST
=> Training   39.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.106 Prec@1=72.421 Prec@5=90.470 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=13:00 IST
=> Training   39.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.107 Prec@1=72.414 Prec@5=90.453 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=13:00 IST
=> Training   43.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.107 Prec@1=72.414 Prec@5=90.453 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:00 IST
=> Training   43.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.564 DataTime=0.387 Loss=1.107 Prec@1=72.414 Prec@5=90.453 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:01 IST
=> Training   43.99% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.108 Prec@1=72.391 Prec@5=90.446 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:01 IST
=> Training   47.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.108 Prec@1=72.391 Prec@5=90.446 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=13:01 IST
=> Training   47.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.108 Prec@1=72.391 Prec@5=90.446 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=13:02 IST
=> Training   47.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.111 Prec@1=72.320 Prec@5=90.409 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=13:02 IST
=> Training   51.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.111 Prec@1=72.320 Prec@5=90.409 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:02 IST
=> Training   51.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.563 DataTime=0.386 Loss=1.111 Prec@1=72.320 Prec@5=90.409 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:03 IST
=> Training   51.98% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.111 Prec@1=72.299 Prec@5=90.404 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:03 IST
=> Training   55.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.111 Prec@1=72.299 Prec@5=90.404 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=13:03 IST
=> Training   55.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.111 Prec@1=72.299 Prec@5=90.404 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=13:04 IST
=> Training   55.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.112 Prec@1=72.283 Prec@5=90.399 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=13:04 IST
=> Training   59.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.112 Prec@1=72.283 Prec@5=90.399 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=13:04 IST
=> Training   59.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.562 DataTime=0.385 Loss=1.112 Prec@1=72.283 Prec@5=90.399 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=13:05 IST
=> Training   59.97% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.383 Loss=1.113 Prec@1=72.232 Prec@5=90.390 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=13:05 IST
=> Training   63.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.383 Loss=1.113 Prec@1=72.232 Prec@5=90.390 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=13:05 IST
=> Training   63.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.383 Loss=1.113 Prec@1=72.232 Prec@5=90.390 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=13:05 IST
=> Training   63.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.114 Prec@1=72.210 Prec@5=90.382 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=13:05 IST
=> Training   67.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.114 Prec@1=72.210 Prec@5=90.382 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=13:05 IST
=> Training   67.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.114 Prec@1=72.210 Prec@5=90.382 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=13:06 IST
=> Training   67.96% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.115 Prec@1=72.188 Prec@5=90.368 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=13:06 IST
=> Training   71.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.115 Prec@1=72.188 Prec@5=90.368 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=13:06 IST
=> Training   71.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.115 Prec@1=72.188 Prec@5=90.368 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=13:07 IST
=> Training   71.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.116 Prec@1=72.165 Prec@5=90.353 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=13:07 IST
=> Training   75.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.116 Prec@1=72.165 Prec@5=90.353 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=13:07 IST
=> Training   75.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.116 Prec@1=72.165 Prec@5=90.353 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=13:08 IST
=> Training   75.95% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.117 Prec@1=72.137 Prec@5=90.343 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=13:08 IST
=> Training   79.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.117 Prec@1=72.137 Prec@5=90.343 rate=1.79 Hz, eta=0:04:39, total=0:18:36, wall=13:08 IST
=> Training   79.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.117 Prec@1=72.137 Prec@5=90.343 rate=1.79 Hz, eta=0:04:39, total=0:18:36, wall=13:09 IST
=> Training   79.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.118 Prec@1=72.121 Prec@5=90.327 rate=1.79 Hz, eta=0:04:39, total=0:18:36, wall=13:09 IST
=> Training   83.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.118 Prec@1=72.121 Prec@5=90.327 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=13:09 IST
=> Training   83.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.118 Prec@1=72.121 Prec@5=90.327 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=13:10 IST
=> Training   83.94% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.119 Prec@1=72.096 Prec@5=90.300 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=13:10 IST
=> Training   87.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.119 Prec@1=72.096 Prec@5=90.300 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=13:10 IST
=> Training   87.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.119 Prec@1=72.096 Prec@5=90.300 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=13:11 IST
=> Training   87.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.120 Prec@1=72.078 Prec@5=90.296 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=13:11 IST
=> Training   91.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.120 Prec@1=72.078 Prec@5=90.296 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=13:11 IST
=> Training   91.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.120 Prec@1=72.078 Prec@5=90.296 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=13:12 IST
=> Training   91.93% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.121 Prec@1=72.056 Prec@5=90.282 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=13:12 IST
=> Training   95.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.121 Prec@1=72.056 Prec@5=90.282 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=13:12 IST
=> Training   95.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.121 Prec@1=72.056 Prec@5=90.282 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=13:13 IST
=> Training   95.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.122 Prec@1=72.026 Prec@5=90.273 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=13:13 IST
=> Training   99.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.122 Prec@1=72.026 Prec@5=90.273 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=13:13 IST
=> Training   99.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.560 DataTime=0.384 Loss=1.122 Prec@1=72.026 Prec@5=90.273 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=13:13 IST
=> Training   99.92% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.559 DataTime=0.384 Loss=1.122 Prec@1=72.026 Prec@5=90.272 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=13:13 IST
=> Training   100.00% of 1x2503...Epoch=93/150 LR=0.0326 Time=0.559 DataTime=0.384 Loss=1.122 Prec@1=72.026 Prec@5=90.272 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=13:13 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:13 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:13 IST
=> Validation 0.00% of 1x98...Epoch=93/150 LR=0.0326 Time=7.207 Loss=0.917 Prec@1=76.172 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=13:13 IST
=> Validation 1.02% of 1x98...Epoch=93/150 LR=0.0326 Time=7.207 Loss=0.917 Prec@1=76.172 Prec@5=93.164 rate=1111.55 Hz, eta=0:00:00, total=0:00:00, wall=13:13 IST
** Validation 1.02% of 1x98...Epoch=93/150 LR=0.0326 Time=7.207 Loss=0.917 Prec@1=76.172 Prec@5=93.164 rate=1111.55 Hz, eta=0:00:00, total=0:00:00, wall=13:14 IST
** Validation 1.02% of 1x98...Epoch=93/150 LR=0.0326 Time=0.635 Loss=1.330 Prec@1=67.760 Prec@5=88.218 rate=1111.55 Hz, eta=0:00:00, total=0:00:00, wall=13:14 IST
** Validation 100.00% of 1x98...Epoch=93/150 LR=0.0326 Time=0.635 Loss=1.330 Prec@1=67.760 Prec@5=88.218 rate=1.78 Hz, eta=0:00:00, total=0:00:54, wall=13:14 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:14 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:14 IST
=> Training   0.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=5.017 DataTime=4.761 Loss=1.108 Prec@1=71.875 Prec@5=90.234 rate=0 Hz, eta=?, total=0:00:00, wall=13:14 IST
=> Training   0.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=5.017 DataTime=4.761 Loss=1.108 Prec@1=71.875 Prec@5=90.234 rate=2579.85 Hz, eta=0:00:00, total=0:00:00, wall=13:14 IST
=> Training   0.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=5.017 DataTime=4.761 Loss=1.108 Prec@1=71.875 Prec@5=90.234 rate=2579.85 Hz, eta=0:00:00, total=0:00:00, wall=13:15 IST
=> Training   0.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.585 DataTime=0.416 Loss=1.093 Prec@1=72.710 Prec@5=90.687 rate=2579.85 Hz, eta=0:00:00, total=0:00:00, wall=13:15 IST
=> Training   4.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.585 DataTime=0.416 Loss=1.093 Prec@1=72.710 Prec@5=90.687 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=13:15 IST
=> Training   4.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.585 DataTime=0.416 Loss=1.093 Prec@1=72.710 Prec@5=90.687 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=13:16 IST
=> Training   4.04% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.574 DataTime=0.397 Loss=1.089 Prec@1=72.852 Prec@5=90.769 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=13:16 IST
=> Training   8.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.574 DataTime=0.397 Loss=1.089 Prec@1=72.852 Prec@5=90.769 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=13:16 IST
=> Training   8.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.574 DataTime=0.397 Loss=1.089 Prec@1=72.852 Prec@5=90.769 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=13:17 IST
=> Training   8.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.570 DataTime=0.395 Loss=1.091 Prec@1=72.811 Prec@5=90.732 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=13:17 IST
=> Training   12.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.570 DataTime=0.395 Loss=1.091 Prec@1=72.811 Prec@5=90.732 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=13:17 IST
=> Training   12.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.570 DataTime=0.395 Loss=1.091 Prec@1=72.811 Prec@5=90.732 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=13:18 IST
=> Training   12.03% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.392 Loss=1.090 Prec@1=72.848 Prec@5=90.700 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=13:18 IST
=> Training   16.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.392 Loss=1.090 Prec@1=72.848 Prec@5=90.700 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=13:18 IST
=> Training   16.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.392 Loss=1.090 Prec@1=72.848 Prec@5=90.700 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=13:19 IST
=> Training   16.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.393 Loss=1.089 Prec@1=72.839 Prec@5=90.694 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=13:19 IST
=> Training   20.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.393 Loss=1.089 Prec@1=72.839 Prec@5=90.694 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=13:19 IST
=> Training   20.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.567 DataTime=0.393 Loss=1.089 Prec@1=72.839 Prec@5=90.694 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=13:20 IST
=> Training   20.02% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.090 Prec@1=72.806 Prec@5=90.696 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=13:20 IST
=> Training   24.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.090 Prec@1=72.806 Prec@5=90.696 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=13:20 IST
=> Training   24.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.090 Prec@1=72.806 Prec@5=90.696 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=13:21 IST
=> Training   24.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.091 Prec@1=72.771 Prec@5=90.677 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=13:21 IST
=> Training   28.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.091 Prec@1=72.771 Prec@5=90.677 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=13:21 IST
=> Training   28.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.392 Loss=1.091 Prec@1=72.771 Prec@5=90.677 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=13:22 IST
=> Training   28.01% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.709 Prec@5=90.662 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=13:22 IST
=> Training   32.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.709 Prec@5=90.662 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:22 IST
=> Training   32.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.709 Prec@5=90.662 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:22 IST
=> Training   32.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.094 Prec@1=72.648 Prec@5=90.653 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:22 IST
=> Training   36.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.094 Prec@1=72.648 Prec@5=90.653 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=13:22 IST
=> Training   36.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.565 DataTime=0.391 Loss=1.094 Prec@1=72.648 Prec@5=90.653 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=13:23 IST
=> Training   36.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.618 Prec@5=90.640 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=13:23 IST
=> Training   39.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.618 Prec@5=90.640 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=13:23 IST
=> Training   39.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.618 Prec@5=90.640 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=13:24 IST
=> Training   39.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.097 Prec@1=72.578 Prec@5=90.611 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=13:24 IST
=> Training   43.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.097 Prec@1=72.578 Prec@5=90.611 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:24 IST
=> Training   43.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.097 Prec@1=72.578 Prec@5=90.611 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:25 IST
=> Training   43.99% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.098 Prec@1=72.559 Prec@5=90.606 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:25 IST
=> Training   47.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.098 Prec@1=72.559 Prec@5=90.606 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:25 IST
=> Training   47.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.098 Prec@1=72.559 Prec@5=90.606 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:26 IST
=> Training   47.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.098 Prec@1=72.545 Prec@5=90.594 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:26 IST
=> Training   51.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.098 Prec@1=72.545 Prec@5=90.594 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=13:26 IST
=> Training   51.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.098 Prec@1=72.545 Prec@5=90.594 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=13:27 IST
=> Training   51.98% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.391 Loss=1.100 Prec@1=72.505 Prec@5=90.561 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=13:27 IST
=> Training   55.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.391 Loss=1.100 Prec@1=72.505 Prec@5=90.561 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:27 IST
=> Training   55.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.391 Loss=1.100 Prec@1=72.505 Prec@5=90.561 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:28 IST
=> Training   55.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.100 Prec@1=72.501 Prec@5=90.555 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:28 IST
=> Training   59.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.100 Prec@1=72.501 Prec@5=90.555 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=13:28 IST
=> Training   59.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.100 Prec@1=72.501 Prec@5=90.555 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=13:29 IST
=> Training   59.97% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.469 Prec@5=90.543 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=13:29 IST
=> Training   63.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.469 Prec@5=90.543 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=13:29 IST
=> Training   63.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.469 Prec@5=90.543 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=13:30 IST
=> Training   63.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.448 Prec@5=90.527 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=13:30 IST
=> Training   67.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.448 Prec@5=90.527 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=13:30 IST
=> Training   67.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.448 Prec@5=90.527 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=13:31 IST
=> Training   67.96% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.104 Prec@1=72.411 Prec@5=90.514 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=13:31 IST
=> Training   71.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.104 Prec@1=72.411 Prec@5=90.514 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=13:31 IST
=> Training   71.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.104 Prec@1=72.411 Prec@5=90.514 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=13:32 IST
=> Training   71.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.105 Prec@1=72.378 Prec@5=90.497 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=13:32 IST
=> Training   75.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.105 Prec@1=72.378 Prec@5=90.497 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=13:32 IST
=> Training   75.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.105 Prec@1=72.378 Prec@5=90.497 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=13:33 IST
=> Training   75.95% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.106 Prec@1=72.367 Prec@5=90.490 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=13:33 IST
=> Training   79.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.106 Prec@1=72.367 Prec@5=90.490 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=13:33 IST
=> Training   79.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.106 Prec@1=72.367 Prec@5=90.490 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=13:34 IST
=> Training   79.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.107 Prec@1=72.339 Prec@5=90.463 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=13:34 IST
=> Training   83.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.107 Prec@1=72.339 Prec@5=90.463 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=13:34 IST
=> Training   83.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.107 Prec@1=72.339 Prec@5=90.463 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=13:35 IST
=> Training   83.94% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.108 Prec@1=72.317 Prec@5=90.453 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=13:35 IST
=> Training   87.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.108 Prec@1=72.317 Prec@5=90.453 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=13:35 IST
=> Training   87.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.108 Prec@1=72.317 Prec@5=90.453 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=13:36 IST
=> Training   87.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.110 Prec@1=72.284 Prec@5=90.432 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=13:36 IST
=> Training   91.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.110 Prec@1=72.284 Prec@5=90.432 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=13:36 IST
=> Training   91.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.390 Loss=1.110 Prec@1=72.284 Prec@5=90.432 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=13:37 IST
=> Training   91.93% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.111 Prec@1=72.254 Prec@5=90.415 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=13:37 IST
=> Training   95.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.111 Prec@1=72.254 Prec@5=90.415 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=13:37 IST
=> Training   95.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.564 DataTime=0.390 Loss=1.111 Prec@1=72.254 Prec@5=90.415 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=13:37 IST
=> Training   95.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.112 Prec@1=72.233 Prec@5=90.404 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=13:37 IST
=> Training   99.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.112 Prec@1=72.233 Prec@5=90.404 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=13:37 IST
=> Training   99.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.112 Prec@1=72.233 Prec@5=90.404 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=13:37 IST
=> Training   99.92% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.112 Prec@1=72.231 Prec@5=90.402 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=13:37 IST
=> Training   100.00% of 1x2503...Epoch=94/150 LR=0.0316 Time=0.563 DataTime=0.389 Loss=1.112 Prec@1=72.231 Prec@5=90.402 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=13:37 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:38 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=13:38 IST
=> Validation 0.00% of 1x98...Epoch=94/150 LR=0.0316 Time=6.841 Loss=0.900 Prec@1=76.758 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=13:38 IST
=> Validation 1.02% of 1x98...Epoch=94/150 LR=0.0316 Time=6.841 Loss=0.900 Prec@1=76.758 Prec@5=92.188 rate=938.55 Hz, eta=0:00:00, total=0:00:00, wall=13:38 IST
** Validation 1.02% of 1x98...Epoch=94/150 LR=0.0316 Time=6.841 Loss=0.900 Prec@1=76.758 Prec@5=92.188 rate=938.55 Hz, eta=0:00:00, total=0:00:00, wall=13:39 IST
** Validation 1.02% of 1x98...Epoch=94/150 LR=0.0316 Time=0.642 Loss=1.332 Prec@1=67.742 Prec@5=87.962 rate=938.55 Hz, eta=0:00:00, total=0:00:00, wall=13:39 IST
** Validation 100.00% of 1x98...Epoch=94/150 LR=0.0316 Time=0.642 Loss=1.332 Prec@1=67.742 Prec@5=87.962 rate=1.75 Hz, eta=0:00:00, total=0:00:56, wall=13:39 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:39 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=13:39 IST
=> Training   0.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=5.097 DataTime=4.657 Loss=1.200 Prec@1=69.336 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=13:39 IST
=> Training   0.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=5.097 DataTime=4.657 Loss=1.200 Prec@1=69.336 Prec@5=90.625 rate=2030.21 Hz, eta=0:00:01, total=0:00:00, wall=13:39 IST
=> Training   0.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=5.097 DataTime=4.657 Loss=1.200 Prec@1=69.336 Prec@5=90.625 rate=2030.21 Hz, eta=0:00:01, total=0:00:00, wall=13:40 IST
=> Training   0.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.598 DataTime=0.421 Loss=1.091 Prec@1=72.927 Prec@5=90.523 rate=2030.21 Hz, eta=0:00:01, total=0:00:00, wall=13:40 IST
=> Training   4.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.598 DataTime=0.421 Loss=1.091 Prec@1=72.927 Prec@5=90.523 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=13:40 IST
=> Training   4.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.598 DataTime=0.421 Loss=1.091 Prec@1=72.927 Prec@5=90.523 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=13:40 IST
=> Training   4.04% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.578 DataTime=0.401 Loss=1.087 Prec@1=72.800 Prec@5=90.657 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=13:40 IST
=> Training   8.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.578 DataTime=0.401 Loss=1.087 Prec@1=72.800 Prec@5=90.657 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=13:40 IST
=> Training   8.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.578 DataTime=0.401 Loss=1.087 Prec@1=72.800 Prec@5=90.657 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=13:41 IST
=> Training   8.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.569 DataTime=0.391 Loss=1.083 Prec@1=72.970 Prec@5=90.681 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=13:41 IST
=> Training   12.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.569 DataTime=0.391 Loss=1.083 Prec@1=72.970 Prec@5=90.681 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=13:41 IST
=> Training   12.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.569 DataTime=0.391 Loss=1.083 Prec@1=72.970 Prec@5=90.681 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=13:42 IST
=> Training   12.03% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.387 Loss=1.083 Prec@1=72.971 Prec@5=90.718 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=13:42 IST
=> Training   16.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.387 Loss=1.083 Prec@1=72.971 Prec@5=90.718 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=13:42 IST
=> Training   16.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.387 Loss=1.083 Prec@1=72.971 Prec@5=90.718 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=13:43 IST
=> Training   16.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.085 Prec@1=72.915 Prec@5=90.717 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=13:43 IST
=> Training   20.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.085 Prec@1=72.915 Prec@5=90.717 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=13:43 IST
=> Training   20.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.085 Prec@1=72.915 Prec@5=90.717 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=13:44 IST
=> Training   20.02% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.087 Prec@1=72.846 Prec@5=90.694 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=13:44 IST
=> Training   24.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.087 Prec@1=72.846 Prec@5=90.694 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=13:44 IST
=> Training   24.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.390 Loss=1.087 Prec@1=72.846 Prec@5=90.694 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=13:45 IST
=> Training   24.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.086 Prec@1=72.845 Prec@5=90.706 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=13:45 IST
=> Training   28.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.086 Prec@1=72.845 Prec@5=90.706 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=13:45 IST
=> Training   28.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.086 Prec@1=72.845 Prec@5=90.706 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=13:46 IST
=> Training   28.01% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.087 Prec@1=72.816 Prec@5=90.679 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=13:46 IST
=> Training   32.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.087 Prec@1=72.816 Prec@5=90.679 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:46 IST
=> Training   32.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.087 Prec@1=72.816 Prec@5=90.679 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:47 IST
=> Training   32.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.779 Prec@5=90.654 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=13:47 IST
=> Training   36.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.779 Prec@5=90.654 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=13:47 IST
=> Training   36.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.779 Prec@5=90.654 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=13:48 IST
=> Training   36.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.091 Prec@1=72.750 Prec@5=90.625 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=13:48 IST
=> Training   39.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.091 Prec@1=72.750 Prec@5=90.625 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=13:48 IST
=> Training   39.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.091 Prec@1=72.750 Prec@5=90.625 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=13:49 IST
=> Training   39.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.708 Prec@5=90.595 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=13:49 IST
=> Training   43.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.708 Prec@5=90.595 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:49 IST
=> Training   43.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.565 DataTime=0.391 Loss=1.092 Prec@1=72.708 Prec@5=90.595 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:50 IST
=> Training   43.99% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.092 Prec@1=72.695 Prec@5=90.590 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=13:50 IST
=> Training   47.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.092 Prec@1=72.695 Prec@5=90.590 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:50 IST
=> Training   47.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.092 Prec@1=72.695 Prec@5=90.590 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:51 IST
=> Training   47.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.390 Loss=1.094 Prec@1=72.647 Prec@5=90.574 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=13:51 IST
=> Training   51.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.390 Loss=1.094 Prec@1=72.647 Prec@5=90.574 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:51 IST
=> Training   51.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.390 Loss=1.094 Prec@1=72.647 Prec@5=90.574 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:52 IST
=> Training   51.98% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.627 Prec@5=90.555 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=13:52 IST
=> Training   55.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.627 Prec@5=90.555 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:52 IST
=> Training   55.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.390 Loss=1.095 Prec@1=72.627 Prec@5=90.555 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:53 IST
=> Training   55.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.097 Prec@1=72.585 Prec@5=90.531 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=13:53 IST
=> Training   59.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.097 Prec@1=72.585 Prec@5=90.531 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=13:53 IST
=> Training   59.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.097 Prec@1=72.585 Prec@5=90.531 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=13:54 IST
=> Training   59.97% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.099 Prec@1=72.539 Prec@5=90.500 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=13:54 IST
=> Training   63.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.099 Prec@1=72.539 Prec@5=90.500 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=13:54 IST
=> Training   63.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.389 Loss=1.099 Prec@1=72.539 Prec@5=90.500 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=13:54 IST
=> Training   63.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.561 DataTime=0.388 Loss=1.100 Prec@1=72.508 Prec@5=90.493 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=13:54 IST
=> Training   67.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.561 DataTime=0.388 Loss=1.100 Prec@1=72.508 Prec@5=90.493 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=13:54 IST
=> Training   67.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.561 DataTime=0.388 Loss=1.100 Prec@1=72.508 Prec@5=90.493 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=13:55 IST
=> Training   67.96% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.101 Prec@1=72.501 Prec@5=90.490 rate=1.79 Hz, eta=0:07:27, total=0:15:50, wall=13:55 IST
=> Training   71.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.101 Prec@1=72.501 Prec@5=90.490 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=13:55 IST
=> Training   71.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.101 Prec@1=72.501 Prec@5=90.490 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=13:56 IST
=> Training   71.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.474 Prec@5=90.479 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=13:56 IST
=> Training   75.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.474 Prec@5=90.479 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=13:56 IST
=> Training   75.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.102 Prec@1=72.474 Prec@5=90.479 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=13:57 IST
=> Training   75.95% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.389 Loss=1.102 Prec@1=72.464 Prec@5=90.474 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=13:57 IST
=> Training   79.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.389 Loss=1.102 Prec@1=72.464 Prec@5=90.474 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=13:57 IST
=> Training   79.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.564 DataTime=0.389 Loss=1.102 Prec@1=72.464 Prec@5=90.474 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=13:58 IST
=> Training   79.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.443 Prec@5=90.464 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=13:58 IST
=> Training   83.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.443 Prec@5=90.464 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=13:58 IST
=> Training   83.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.389 Loss=1.103 Prec@1=72.443 Prec@5=90.464 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=13:59 IST
=> Training   83.94% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.103 Prec@1=72.435 Prec@5=90.459 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=13:59 IST
=> Training   87.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.103 Prec@1=72.435 Prec@5=90.459 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=13:59 IST
=> Training   87.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.103 Prec@1=72.435 Prec@5=90.459 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=14:00 IST
=> Training   87.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.105 Prec@1=72.397 Prec@5=90.432 rate=1.78 Hz, eta=0:02:49, total=0:20:34, wall=14:00 IST
=> Training   91.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.105 Prec@1=72.397 Prec@5=90.432 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=14:00 IST
=> Training   91.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.105 Prec@1=72.397 Prec@5=90.432 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=14:01 IST
=> Training   91.93% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.106 Prec@1=72.366 Prec@5=90.419 rate=1.78 Hz, eta=0:01:53, total=0:21:29, wall=14:01 IST
=> Training   95.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.106 Prec@1=72.366 Prec@5=90.419 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=14:01 IST
=> Training   95.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.388 Loss=1.106 Prec@1=72.366 Prec@5=90.419 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=14:02 IST
=> Training   95.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.387 Loss=1.107 Prec@1=72.352 Prec@5=90.408 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=14:02 IST
=> Training   99.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.387 Loss=1.107 Prec@1=72.352 Prec@5=90.408 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=14:02 IST
=> Training   99.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.563 DataTime=0.387 Loss=1.107 Prec@1=72.352 Prec@5=90.408 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=14:02 IST
=> Training   99.92% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.387 Loss=1.107 Prec@1=72.352 Prec@5=90.408 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=14:02 IST
=> Training   100.00% of 1x2503...Epoch=95/150 LR=0.0306 Time=0.562 DataTime=0.387 Loss=1.107 Prec@1=72.352 Prec@5=90.408 rate=1.78 Hz, eta=0:00:00, total=0:23:22, wall=14:02 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST
=> Validation 0.00% of 1x98...Epoch=95/150 LR=0.0306 Time=6.726 Loss=0.912 Prec@1=75.781 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=14:02 IST
=> Validation 1.02% of 1x98...Epoch=95/150 LR=0.0306 Time=6.726 Loss=0.912 Prec@1=75.781 Prec@5=93.164 rate=3109.68 Hz, eta=0:00:00, total=0:00:00, wall=14:02 IST
** Validation 1.02% of 1x98...Epoch=95/150 LR=0.0306 Time=6.726 Loss=0.912 Prec@1=75.781 Prec@5=93.164 rate=3109.68 Hz, eta=0:00:00, total=0:00:00, wall=14:03 IST
** Validation 1.02% of 1x98...Epoch=95/150 LR=0.0306 Time=0.631 Loss=1.325 Prec@1=68.040 Prec@5=88.102 rate=3109.68 Hz, eta=0:00:00, total=0:00:00, wall=14:03 IST
** Validation 100.00% of 1x98...Epoch=95/150 LR=0.0306 Time=0.631 Loss=1.325 Prec@1=68.040 Prec@5=88.102 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=14:03 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:03 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:03 IST
=> Training   0.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=5.133 DataTime=4.716 Loss=1.112 Prec@1=70.703 Prec@5=90.430 rate=0 Hz, eta=?, total=0:00:00, wall=14:03 IST
=> Training   0.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=5.133 DataTime=4.716 Loss=1.112 Prec@1=70.703 Prec@5=90.430 rate=7702.91 Hz, eta=0:00:00, total=0:00:00, wall=14:03 IST
=> Training   0.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=5.133 DataTime=4.716 Loss=1.112 Prec@1=70.703 Prec@5=90.430 rate=7702.91 Hz, eta=0:00:00, total=0:00:00, wall=14:04 IST
=> Training   0.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.595 DataTime=0.422 Loss=1.067 Prec@1=73.178 Prec@5=90.971 rate=7702.91 Hz, eta=0:00:00, total=0:00:00, wall=14:04 IST
=> Training   4.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.595 DataTime=0.422 Loss=1.067 Prec@1=73.178 Prec@5=90.971 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=14:04 IST
=> Training   4.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.595 DataTime=0.422 Loss=1.067 Prec@1=73.178 Prec@5=90.971 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=14:05 IST
=> Training   4.04% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.582 DataTime=0.410 Loss=1.067 Prec@1=73.285 Prec@5=90.974 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=14:05 IST
=> Training   8.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.582 DataTime=0.410 Loss=1.067 Prec@1=73.285 Prec@5=90.974 rate=1.80 Hz, eta=0:21:20, total=0:01:51, wall=14:05 IST
=> Training   8.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.582 DataTime=0.410 Loss=1.067 Prec@1=73.285 Prec@5=90.974 rate=1.80 Hz, eta=0:21:20, total=0:01:51, wall=14:06 IST
=> Training   8.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.575 DataTime=0.403 Loss=1.070 Prec@1=73.175 Prec@5=90.906 rate=1.80 Hz, eta=0:21:20, total=0:01:51, wall=14:06 IST
=> Training   12.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.575 DataTime=0.403 Loss=1.070 Prec@1=73.175 Prec@5=90.906 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=14:06 IST
=> Training   12.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.575 DataTime=0.403 Loss=1.070 Prec@1=73.175 Prec@5=90.906 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=14:07 IST
=> Training   12.03% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.070 Prec@1=73.135 Prec@5=90.880 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=14:07 IST
=> Training   16.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.070 Prec@1=73.135 Prec@5=90.880 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=14:07 IST
=> Training   16.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.070 Prec@1=73.135 Prec@5=90.880 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=14:08 IST
=> Training   16.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.396 Loss=1.068 Prec@1=73.154 Prec@5=90.917 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=14:08 IST
=> Training   20.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.396 Loss=1.068 Prec@1=73.154 Prec@5=90.917 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:08 IST
=> Training   20.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.396 Loss=1.068 Prec@1=73.154 Prec@5=90.917 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:09 IST
=> Training   20.02% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.073 Prec@1=73.080 Prec@5=90.865 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:09 IST
=> Training   24.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.073 Prec@1=73.080 Prec@5=90.865 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=14:09 IST
=> Training   24.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.570 DataTime=0.397 Loss=1.073 Prec@1=73.080 Prec@5=90.865 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=14:10 IST
=> Training   24.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.076 Prec@1=73.016 Prec@5=90.848 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=14:10 IST
=> Training   28.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.076 Prec@1=73.016 Prec@5=90.848 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=14:10 IST
=> Training   28.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.076 Prec@1=73.016 Prec@5=90.848 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=14:11 IST
=> Training   28.01% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.078 Prec@1=72.957 Prec@5=90.813 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=14:11 IST
=> Training   32.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.078 Prec@1=72.957 Prec@5=90.813 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=14:11 IST
=> Training   32.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.568 DataTime=0.395 Loss=1.078 Prec@1=72.957 Prec@5=90.813 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=14:12 IST
=> Training   32.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.395 Loss=1.080 Prec@1=72.913 Prec@5=90.798 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=14:12 IST
=> Training   36.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.395 Loss=1.080 Prec@1=72.913 Prec@5=90.798 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=14:12 IST
=> Training   36.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.395 Loss=1.080 Prec@1=72.913 Prec@5=90.798 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=14:12 IST
=> Training   36.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.567 DataTime=0.394 Loss=1.081 Prec@1=72.890 Prec@5=90.790 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=14:12 IST
=> Training   39.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.567 DataTime=0.394 Loss=1.081 Prec@1=72.890 Prec@5=90.790 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=14:12 IST
=> Training   39.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.567 DataTime=0.394 Loss=1.081 Prec@1=72.890 Prec@5=90.790 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=14:13 IST
=> Training   39.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.393 Loss=1.083 Prec@1=72.849 Prec@5=90.767 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=14:13 IST
=> Training   43.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.393 Loss=1.083 Prec@1=72.849 Prec@5=90.767 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=14:13 IST
=> Training   43.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.566 DataTime=0.393 Loss=1.083 Prec@1=72.849 Prec@5=90.767 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=14:14 IST
=> Training   43.99% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.085 Prec@1=72.828 Prec@5=90.739 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=14:14 IST
=> Training   47.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.085 Prec@1=72.828 Prec@5=90.739 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=14:14 IST
=> Training   47.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.085 Prec@1=72.828 Prec@5=90.739 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=14:15 IST
=> Training   47.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.087 Prec@1=72.794 Prec@5=90.709 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=14:15 IST
=> Training   51.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.087 Prec@1=72.794 Prec@5=90.709 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=14:15 IST
=> Training   51.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.391 Loss=1.087 Prec@1=72.794 Prec@5=90.709 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=14:16 IST
=> Training   51.98% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.753 Prec@5=90.697 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=14:16 IST
=> Training   55.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.753 Prec@5=90.697 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=14:16 IST
=> Training   55.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.753 Prec@5=90.697 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=14:17 IST
=> Training   55.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.752 Prec@5=90.693 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=14:17 IST
=> Training   59.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.752 Prec@5=90.693 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=14:17 IST
=> Training   59.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.089 Prec@1=72.752 Prec@5=90.693 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=14:18 IST
=> Training   59.97% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.089 Prec@1=72.747 Prec@5=90.681 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=14:18 IST
=> Training   63.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.089 Prec@1=72.747 Prec@5=90.681 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=14:18 IST
=> Training   63.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.089 Prec@1=72.747 Prec@5=90.681 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=14:19 IST
=> Training   63.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.090 Prec@1=72.729 Prec@5=90.679 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=14:19 IST
=> Training   67.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.090 Prec@1=72.729 Prec@5=90.679 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=14:19 IST
=> Training   67.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.090 Prec@1=72.729 Prec@5=90.679 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=14:20 IST
=> Training   67.96% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.091 Prec@1=72.713 Prec@5=90.666 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=14:20 IST
=> Training   71.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.091 Prec@1=72.713 Prec@5=90.666 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=14:20 IST
=> Training   71.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.091 Prec@1=72.713 Prec@5=90.666 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=14:21 IST
=> Training   71.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.093 Prec@1=72.685 Prec@5=90.635 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=14:21 IST
=> Training   75.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.093 Prec@1=72.685 Prec@5=90.635 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=14:21 IST
=> Training   75.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.390 Loss=1.093 Prec@1=72.685 Prec@5=90.635 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=14:22 IST
=> Training   75.95% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.389 Loss=1.094 Prec@1=72.667 Prec@5=90.618 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=14:22 IST
=> Training   79.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.389 Loss=1.094 Prec@1=72.667 Prec@5=90.618 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=14:22 IST
=> Training   79.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.389 Loss=1.094 Prec@1=72.667 Prec@5=90.618 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=14:23 IST
=> Training   79.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.094 Prec@1=72.643 Prec@5=90.609 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=14:23 IST
=> Training   83.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.094 Prec@1=72.643 Prec@5=90.609 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=14:23 IST
=> Training   83.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.094 Prec@1=72.643 Prec@5=90.609 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=14:24 IST
=> Training   83.94% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.096 Prec@1=72.607 Prec@5=90.584 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=14:24 IST
=> Training   87.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.096 Prec@1=72.607 Prec@5=90.584 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=14:24 IST
=> Training   87.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.389 Loss=1.096 Prec@1=72.607 Prec@5=90.584 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=14:25 IST
=> Training   87.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.388 Loss=1.098 Prec@1=72.568 Prec@5=90.558 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=14:25 IST
=> Training   91.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.388 Loss=1.098 Prec@1=72.568 Prec@5=90.558 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=14:25 IST
=> Training   91.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.564 DataTime=0.388 Loss=1.098 Prec@1=72.568 Prec@5=90.558 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=14:26 IST
=> Training   91.93% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.546 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=14:26 IST
=> Training   95.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.546 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=14:26 IST
=> Training   95.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.546 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=14:26 IST
=> Training   95.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.549 rate=1.78 Hz, eta=0:00:57, total=0:22:26, wall=14:26 IST
=> Training   99.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.549 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=14:26 IST
=> Training   99.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.559 Prec@5=90.549 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=14:26 IST
=> Training   99.92% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.558 Prec@5=90.549 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=14:26 IST
=> Training   100.00% of 1x2503...Epoch=96/150 LR=0.0297 Time=0.563 DataTime=0.388 Loss=1.098 Prec@1=72.558 Prec@5=90.549 rate=1.78 Hz, eta=0:00:00, total=0:23:22, wall=14:26 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> Validation 0.00% of 1x98...Epoch=96/150 LR=0.0297 Time=8.350 Loss=0.922 Prec@1=77.148 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=14:27 IST
=> Validation 1.02% of 1x98...Epoch=96/150 LR=0.0297 Time=8.350 Loss=0.922 Prec@1=77.148 Prec@5=92.578 rate=5686.47 Hz, eta=0:00:00, total=0:00:00, wall=14:27 IST
** Validation 1.02% of 1x98...Epoch=96/150 LR=0.0297 Time=8.350 Loss=0.922 Prec@1=77.148 Prec@5=92.578 rate=5686.47 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** Validation 1.02% of 1x98...Epoch=96/150 LR=0.0297 Time=0.651 Loss=1.336 Prec@1=67.660 Prec@5=87.986 rate=5686.47 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
** Validation 100.00% of 1x98...Epoch=96/150 LR=0.0297 Time=0.651 Loss=1.336 Prec@1=67.660 Prec@5=87.986 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=14:28 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> Training   0.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=4.853 DataTime=4.295 Loss=1.053 Prec@1=76.172 Prec@5=90.625 rate=0 Hz, eta=?, total=0:00:00, wall=14:28 IST
=> Training   0.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=4.853 DataTime=4.295 Loss=1.053 Prec@1=76.172 Prec@5=90.625 rate=3161.33 Hz, eta=0:00:00, total=0:00:00, wall=14:28 IST
=> Training   0.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=4.853 DataTime=4.295 Loss=1.053 Prec@1=76.172 Prec@5=90.625 rate=3161.33 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> Training   0.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.586 DataTime=0.417 Loss=1.058 Prec@1=73.693 Prec@5=91.014 rate=3161.33 Hz, eta=0:00:00, total=0:00:00, wall=14:29 IST
=> Training   4.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.586 DataTime=0.417 Loss=1.058 Prec@1=73.693 Prec@5=91.014 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=14:29 IST
=> Training   4.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.586 DataTime=0.417 Loss=1.058 Prec@1=73.693 Prec@5=91.014 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=14:30 IST
=> Training   4.04% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.579 DataTime=0.412 Loss=1.060 Prec@1=73.425 Prec@5=91.001 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=14:30 IST
=> Training   8.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.579 DataTime=0.412 Loss=1.060 Prec@1=73.425 Prec@5=91.001 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=14:30 IST
=> Training   8.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.579 DataTime=0.412 Loss=1.060 Prec@1=73.425 Prec@5=91.001 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=14:30 IST
=> Training   8.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.571 DataTime=0.401 Loss=1.065 Prec@1=73.322 Prec@5=90.959 rate=1.80 Hz, eta=0:21:18, total=0:01:51, wall=14:30 IST
=> Training   12.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.571 DataTime=0.401 Loss=1.065 Prec@1=73.322 Prec@5=90.959 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=14:30 IST
=> Training   12.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.571 DataTime=0.401 Loss=1.065 Prec@1=73.322 Prec@5=90.959 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=14:31 IST
=> Training   12.03% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.396 Loss=1.064 Prec@1=73.288 Prec@5=90.942 rate=1.80 Hz, eta=0:20:22, total=0:02:47, wall=14:31 IST
=> Training   16.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.396 Loss=1.064 Prec@1=73.288 Prec@5=90.942 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:31 IST
=> Training   16.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.396 Loss=1.064 Prec@1=73.288 Prec@5=90.942 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:32 IST
=> Training   16.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.565 DataTime=0.394 Loss=1.066 Prec@1=73.267 Prec@5=90.916 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:32 IST
=> Training   20.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.565 DataTime=0.394 Loss=1.066 Prec@1=73.267 Prec@5=90.916 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=14:32 IST
=> Training   20.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.565 DataTime=0.394 Loss=1.066 Prec@1=73.267 Prec@5=90.916 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=14:33 IST
=> Training   20.02% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.397 Loss=1.068 Prec@1=73.177 Prec@5=90.923 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=14:33 IST
=> Training   24.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.397 Loss=1.068 Prec@1=73.177 Prec@5=90.923 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:33 IST
=> Training   24.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.567 DataTime=0.397 Loss=1.068 Prec@1=73.177 Prec@5=90.923 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:34 IST
=> Training   24.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.068 Prec@1=73.180 Prec@5=90.907 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:34 IST
=> Training   28.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.068 Prec@1=73.180 Prec@5=90.907 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=14:34 IST
=> Training   28.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.068 Prec@1=73.180 Prec@5=90.907 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=14:35 IST
=> Training   28.01% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.070 Prec@1=73.142 Prec@5=90.878 rate=1.80 Hz, eta=0:16:42, total=0:06:29, wall=14:35 IST
=> Training   32.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.070 Prec@1=73.142 Prec@5=90.878 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=14:35 IST
=> Training   32.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.070 Prec@1=73.142 Prec@5=90.878 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=14:36 IST
=> Training   32.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.071 Prec@1=73.126 Prec@5=90.849 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=14:36 IST
=> Training   36.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.071 Prec@1=73.126 Prec@5=90.849 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=14:36 IST
=> Training   36.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.393 Loss=1.071 Prec@1=73.126 Prec@5=90.849 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=14:37 IST
=> Training   36.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.392 Loss=1.072 Prec@1=73.112 Prec@5=90.842 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=14:37 IST
=> Training   39.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.392 Loss=1.072 Prec@1=73.112 Prec@5=90.842 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=14:37 IST
=> Training   39.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.563 DataTime=0.392 Loss=1.072 Prec@1=73.112 Prec@5=90.842 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=14:38 IST
=> Training   39.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.391 Loss=1.074 Prec@1=73.098 Prec@5=90.822 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=14:38 IST
=> Training   43.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.391 Loss=1.074 Prec@1=73.098 Prec@5=90.822 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=14:38 IST
=> Training   43.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.391 Loss=1.074 Prec@1=73.098 Prec@5=90.822 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=14:39 IST
=> Training   43.99% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.392 Loss=1.075 Prec@1=73.077 Prec@5=90.813 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=14:39 IST
=> Training   47.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.392 Loss=1.075 Prec@1=73.077 Prec@5=90.813 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=14:39 IST
=> Training   47.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.562 DataTime=0.392 Loss=1.075 Prec@1=73.077 Prec@5=90.813 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=14:40 IST
=> Training   47.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.076 Prec@1=73.044 Prec@5=90.796 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=14:40 IST
=> Training   51.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.076 Prec@1=73.044 Prec@5=90.796 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=14:40 IST
=> Training   51.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.076 Prec@1=73.044 Prec@5=90.796 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=14:41 IST
=> Training   51.98% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.077 Prec@1=73.030 Prec@5=90.784 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=14:41 IST
=> Training   55.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.077 Prec@1=73.030 Prec@5=90.784 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=14:41 IST
=> Training   55.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.077 Prec@1=73.030 Prec@5=90.784 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=14:42 IST
=> Training   55.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.078 Prec@1=73.002 Prec@5=90.777 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=14:42 IST
=> Training   59.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.078 Prec@1=73.002 Prec@5=90.777 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=14:42 IST
=> Training   59.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.078 Prec@1=73.002 Prec@5=90.777 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=14:43 IST
=> Training   59.97% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.079 Prec@1=72.985 Prec@5=90.759 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=14:43 IST
=> Training   63.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.079 Prec@1=72.985 Prec@5=90.759 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=14:43 IST
=> Training   63.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.390 Loss=1.079 Prec@1=72.985 Prec@5=90.759 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=14:43 IST
=> Training   63.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.389 Loss=1.081 Prec@1=72.957 Prec@5=90.751 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=14:43 IST
=> Training   67.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.389 Loss=1.081 Prec@1=72.957 Prec@5=90.751 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=14:43 IST
=> Training   67.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.561 DataTime=0.389 Loss=1.081 Prec@1=72.957 Prec@5=90.751 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=14:44 IST
=> Training   67.96% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.082 Prec@1=72.920 Prec@5=90.739 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=14:44 IST
=> Training   71.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.082 Prec@1=72.920 Prec@5=90.739 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=14:44 IST
=> Training   71.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.082 Prec@1=72.920 Prec@5=90.739 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=14:45 IST
=> Training   71.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.083 Prec@1=72.903 Prec@5=90.726 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=14:45 IST
=> Training   75.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.083 Prec@1=72.903 Prec@5=90.726 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=14:45 IST
=> Training   75.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.388 Loss=1.083 Prec@1=72.903 Prec@5=90.726 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=14:46 IST
=> Training   75.95% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.084 Prec@1=72.885 Prec@5=90.714 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=14:46 IST
=> Training   79.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.084 Prec@1=72.885 Prec@5=90.714 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=14:46 IST
=> Training   79.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.084 Prec@1=72.885 Prec@5=90.714 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=14:47 IST
=> Training   79.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.085 Prec@1=72.868 Prec@5=90.706 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=14:47 IST
=> Training   83.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.085 Prec@1=72.868 Prec@5=90.706 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=14:47 IST
=> Training   83.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.085 Prec@1=72.868 Prec@5=90.706 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=14:48 IST
=> Training   83.94% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.386 Loss=1.086 Prec@1=72.838 Prec@5=90.682 rate=1.79 Hz, eta=0:03:44, total=0:19:30, wall=14:48 IST
=> Training   87.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.386 Loss=1.086 Prec@1=72.838 Prec@5=90.682 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=14:48 IST
=> Training   87.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.386 Loss=1.086 Prec@1=72.838 Prec@5=90.682 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=14:49 IST
=> Training   87.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.088 Prec@1=72.805 Prec@5=90.655 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=14:49 IST
=> Training   91.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.088 Prec@1=72.805 Prec@5=90.655 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=14:49 IST
=> Training   91.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.088 Prec@1=72.805 Prec@5=90.655 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=14:50 IST
=> Training   91.93% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.089 Prec@1=72.779 Prec@5=90.636 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=14:50 IST
=> Training   95.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.089 Prec@1=72.779 Prec@5=90.636 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=14:50 IST
=> Training   95.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.560 DataTime=0.386 Loss=1.089 Prec@1=72.779 Prec@5=90.636 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=14:51 IST
=> Training   95.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.385 Loss=1.090 Prec@1=72.758 Prec@5=90.621 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=14:51 IST
=> Training   99.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.385 Loss=1.090 Prec@1=72.758 Prec@5=90.621 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=14:51 IST
=> Training   99.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.385 Loss=1.090 Prec@1=72.758 Prec@5=90.621 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=14:51 IST
=> Training   99.92% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.386 Loss=1.090 Prec@1=72.758 Prec@5=90.621 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=14:51 IST
=> Training   100.00% of 1x2503...Epoch=97/150 LR=0.0287 Time=0.559 DataTime=0.386 Loss=1.090 Prec@1=72.758 Prec@5=90.621 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=14:51 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST
=> Validation 0.00% of 1x98...Epoch=97/150 LR=0.0287 Time=6.896 Loss=0.832 Prec@1=77.930 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=14:51 IST
=> Validation 1.02% of 1x98...Epoch=97/150 LR=0.0287 Time=6.896 Loss=0.832 Prec@1=77.930 Prec@5=93.555 rate=5704.15 Hz, eta=0:00:00, total=0:00:00, wall=14:51 IST
** Validation 1.02% of 1x98...Epoch=97/150 LR=0.0287 Time=6.896 Loss=0.832 Prec@1=77.930 Prec@5=93.555 rate=5704.15 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST
** Validation 1.02% of 1x98...Epoch=97/150 LR=0.0287 Time=0.634 Loss=1.317 Prec@1=67.998 Prec@5=88.070 rate=5704.15 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST
** Validation 100.00% of 1x98...Epoch=97/150 LR=0.0287 Time=0.634 Loss=1.317 Prec@1=67.998 Prec@5=88.070 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=14:52 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST
=> Training   0.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=4.607 DataTime=4.234 Loss=1.043 Prec@1=73.828 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=14:52 IST
=> Training   0.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=4.607 DataTime=4.234 Loss=1.043 Prec@1=73.828 Prec@5=91.992 rate=2987.49 Hz, eta=0:00:00, total=0:00:00, wall=14:52 IST
=> Training   0.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=4.607 DataTime=4.234 Loss=1.043 Prec@1=73.828 Prec@5=91.992 rate=2987.49 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST
=> Training   0.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.588 DataTime=0.419 Loss=1.045 Prec@1=73.836 Prec@5=91.195 rate=2987.49 Hz, eta=0:00:00, total=0:00:00, wall=14:53 IST
=> Training   4.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.588 DataTime=0.419 Loss=1.045 Prec@1=73.836 Prec@5=91.195 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=14:53 IST
=> Training   4.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.588 DataTime=0.419 Loss=1.045 Prec@1=73.836 Prec@5=91.195 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=14:54 IST
=> Training   4.04% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.574 DataTime=0.405 Loss=1.040 Prec@1=73.914 Prec@5=91.254 rate=1.84 Hz, eta=0:21:43, total=0:00:54, wall=14:54 IST
=> Training   8.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.574 DataTime=0.405 Loss=1.040 Prec@1=73.914 Prec@5=91.254 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=14:54 IST
=> Training   8.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.574 DataTime=0.405 Loss=1.040 Prec@1=73.914 Prec@5=91.254 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=14:55 IST
=> Training   8.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.568 DataTime=0.398 Loss=1.047 Prec@1=73.705 Prec@5=91.194 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=14:55 IST
=> Training   12.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.568 DataTime=0.398 Loss=1.047 Prec@1=73.705 Prec@5=91.194 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=14:55 IST
=> Training   12.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.568 DataTime=0.398 Loss=1.047 Prec@1=73.705 Prec@5=91.194 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=14:56 IST
=> Training   12.03% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.394 Loss=1.049 Prec@1=73.678 Prec@5=91.154 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=14:56 IST
=> Training   16.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.394 Loss=1.049 Prec@1=73.678 Prec@5=91.154 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:56 IST
=> Training   16.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.394 Loss=1.049 Prec@1=73.678 Prec@5=91.154 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:57 IST
=> Training   16.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.569 DataTime=0.396 Loss=1.054 Prec@1=73.590 Prec@5=91.074 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=14:57 IST
=> Training   20.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.569 DataTime=0.396 Loss=1.054 Prec@1=73.590 Prec@5=91.074 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:57 IST
=> Training   20.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.569 DataTime=0.396 Loss=1.054 Prec@1=73.590 Prec@5=91.074 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:58 IST
=> Training   20.02% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.567 DataTime=0.394 Loss=1.054 Prec@1=73.589 Prec@5=91.098 rate=1.79 Hz, eta=0:18:40, total=0:04:40, wall=14:58 IST
=> Training   24.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.567 DataTime=0.394 Loss=1.054 Prec@1=73.589 Prec@5=91.098 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:58 IST
=> Training   24.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.567 DataTime=0.394 Loss=1.054 Prec@1=73.589 Prec@5=91.098 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:59 IST
=> Training   24.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.393 Loss=1.056 Prec@1=73.538 Prec@5=91.075 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=14:59 IST
=> Training   28.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.393 Loss=1.056 Prec@1=73.538 Prec@5=91.075 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=14:59 IST
=> Training   28.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.393 Loss=1.056 Prec@1=73.538 Prec@5=91.075 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=14:59 IST
=> Training   28.01% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.456 Prec@5=91.047 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=14:59 IST
=> Training   32.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.456 Prec@5=91.047 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=14:59 IST
=> Training   32.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.456 Prec@5=91.047 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=15:00 IST
=> Training   32.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.060 Prec@1=73.432 Prec@5=91.036 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=15:00 IST
=> Training   36.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.060 Prec@1=73.432 Prec@5=91.036 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=15:00 IST
=> Training   36.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.060 Prec@1=73.432 Prec@5=91.036 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=15:01 IST
=> Training   36.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.062 Prec@1=73.390 Prec@5=91.011 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=15:01 IST
=> Training   39.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.062 Prec@1=73.390 Prec@5=91.011 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=15:01 IST
=> Training   39.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.062 Prec@1=73.390 Prec@5=91.011 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=15:02 IST
=> Training   39.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.065 Prec@1=73.322 Prec@5=90.968 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=15:02 IST
=> Training   43.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.065 Prec@1=73.322 Prec@5=90.968 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=15:02 IST
=> Training   43.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.566 DataTime=0.390 Loss=1.065 Prec@1=73.322 Prec@5=90.968 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=15:03 IST
=> Training   43.99% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.066 Prec@1=73.298 Prec@5=90.964 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=15:03 IST
=> Training   47.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.066 Prec@1=73.298 Prec@5=90.964 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=15:03 IST
=> Training   47.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.066 Prec@1=73.298 Prec@5=90.964 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=15:04 IST
=> Training   47.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.068 Prec@1=73.238 Prec@5=90.935 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=15:04 IST
=> Training   51.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.068 Prec@1=73.238 Prec@5=90.935 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=15:04 IST
=> Training   51.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.390 Loss=1.068 Prec@1=73.238 Prec@5=90.935 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=15:05 IST
=> Training   51.98% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.564 DataTime=0.389 Loss=1.070 Prec@1=73.191 Prec@5=90.912 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=15:05 IST
=> Training   55.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.564 DataTime=0.389 Loss=1.070 Prec@1=73.191 Prec@5=90.912 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:05 IST
=> Training   55.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.564 DataTime=0.389 Loss=1.070 Prec@1=73.191 Prec@5=90.912 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:06 IST
=> Training   55.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.071 Prec@1=73.171 Prec@5=90.891 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:06 IST
=> Training   59.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.071 Prec@1=73.171 Prec@5=90.891 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=15:06 IST
=> Training   59.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.565 DataTime=0.389 Loss=1.071 Prec@1=73.171 Prec@5=90.891 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=15:07 IST
=> Training   59.97% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.073 Prec@1=73.166 Prec@5=90.869 rate=1.78 Hz, eta=0:09:23, total=0:14:03, wall=15:07 IST
=> Training   63.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.073 Prec@1=73.166 Prec@5=90.869 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:07 IST
=> Training   63.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.073 Prec@1=73.166 Prec@5=90.869 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:08 IST
=> Training   63.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.386 Loss=1.074 Prec@1=73.133 Prec@5=90.856 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:08 IST
=> Training   67.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.386 Loss=1.074 Prec@1=73.133 Prec@5=90.856 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=15:08 IST
=> Training   67.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.386 Loss=1.074 Prec@1=73.133 Prec@5=90.856 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=15:09 IST
=> Training   67.96% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.075 Prec@1=73.100 Prec@5=90.856 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=15:09 IST
=> Training   71.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.075 Prec@1=73.100 Prec@5=90.856 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=15:09 IST
=> Training   71.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.075 Prec@1=73.100 Prec@5=90.856 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=15:10 IST
=> Training   71.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.076 Prec@1=73.078 Prec@5=90.836 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=15:10 IST
=> Training   75.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.076 Prec@1=73.078 Prec@5=90.836 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:10 IST
=> Training   75.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.076 Prec@1=73.078 Prec@5=90.836 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:11 IST
=> Training   75.95% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.077 Prec@1=73.060 Prec@5=90.818 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=15:11 IST
=> Training   79.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.077 Prec@1=73.060 Prec@5=90.818 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=15:11 IST
=> Training   79.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.385 Loss=1.077 Prec@1=73.060 Prec@5=90.818 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=15:12 IST
=> Training   79.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.078 Prec@1=73.038 Prec@5=90.808 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=15:12 IST
=> Training   83.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.078 Prec@1=73.038 Prec@5=90.808 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=15:12 IST
=> Training   83.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.562 DataTime=0.386 Loss=1.078 Prec@1=73.038 Prec@5=90.808 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=15:13 IST
=> Training   83.94% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.079 Prec@1=73.008 Prec@5=90.792 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=15:13 IST
=> Training   87.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.079 Prec@1=73.008 Prec@5=90.792 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=15:13 IST
=> Training   87.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.079 Prec@1=73.008 Prec@5=90.792 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=15:14 IST
=> Training   87.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.080 Prec@1=72.975 Prec@5=90.775 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=15:14 IST
=> Training   91.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.080 Prec@1=72.975 Prec@5=90.775 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=15:14 IST
=> Training   91.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.080 Prec@1=72.975 Prec@5=90.775 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=15:15 IST
=> Training   91.93% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.955 Prec@5=90.760 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=15:15 IST
=> Training   95.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.955 Prec@5=90.760 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:15 IST
=> Training   95.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.955 Prec@5=90.760 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:15 IST
=> Training   95.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.943 Prec@5=90.763 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:15 IST
=> Training   99.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.943 Prec@5=90.763 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=15:15 IST
=> Training   99.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.943 Prec@5=90.763 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=15:15 IST
=> Training   99.92% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.942 Prec@5=90.764 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=15:15 IST
=> Training   100.00% of 1x2503...Epoch=98/150 LR=0.0278 Time=0.563 DataTime=0.387 Loss=1.081 Prec@1=72.942 Prec@5=90.764 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=15:15 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST
=> Validation 0.00% of 1x98...Epoch=98/150 LR=0.0278 Time=7.046 Loss=0.794 Prec@1=79.102 Prec@5=95.898 rate=0 Hz, eta=?, total=0:00:00, wall=15:16 IST
=> Validation 1.02% of 1x98...Epoch=98/150 LR=0.0278 Time=7.046 Loss=0.794 Prec@1=79.102 Prec@5=95.898 rate=5995.53 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST
** Validation 1.02% of 1x98...Epoch=98/150 LR=0.0278 Time=7.046 Loss=0.794 Prec@1=79.102 Prec@5=95.898 rate=5995.53 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST
** Validation 1.02% of 1x98...Epoch=98/150 LR=0.0278 Time=0.632 Loss=1.294 Prec@1=68.630 Prec@5=88.704 rate=5995.53 Hz, eta=0:00:00, total=0:00:00, wall=15:16 IST
** Validation 100.00% of 1x98...Epoch=98/150 LR=0.0278 Time=0.632 Loss=1.294 Prec@1=68.630 Prec@5=88.704 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=15:16 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:17 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:17 IST
=> Training   0.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=4.392 DataTime=3.984 Loss=0.934 Prec@1=75.391 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=15:17 IST
=> Training   0.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=4.392 DataTime=3.984 Loss=0.934 Prec@1=75.391 Prec@5=92.773 rate=1937.86 Hz, eta=0:00:01, total=0:00:00, wall=15:17 IST
=> Training   0.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=4.392 DataTime=3.984 Loss=0.934 Prec@1=75.391 Prec@5=92.773 rate=1937.86 Hz, eta=0:00:01, total=0:00:00, wall=15:18 IST
=> Training   0.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.611 DataTime=0.438 Loss=1.063 Prec@1=73.573 Prec@5=91.010 rate=1937.86 Hz, eta=0:00:01, total=0:00:00, wall=15:18 IST
=> Training   4.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.611 DataTime=0.438 Loss=1.063 Prec@1=73.573 Prec@5=91.010 rate=1.76 Hz, eta=0:22:44, total=0:00:57, wall=15:18 IST
=> Training   4.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.611 DataTime=0.438 Loss=1.063 Prec@1=73.573 Prec@5=91.010 rate=1.76 Hz, eta=0:22:44, total=0:00:57, wall=15:18 IST
=> Training   4.04% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.594 DataTime=0.419 Loss=1.056 Prec@1=73.628 Prec@5=91.103 rate=1.76 Hz, eta=0:22:44, total=0:00:57, wall=15:18 IST
=> Training   8.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.594 DataTime=0.419 Loss=1.056 Prec@1=73.628 Prec@5=91.103 rate=1.75 Hz, eta=0:21:56, total=0:01:54, wall=15:18 IST
=> Training   8.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.594 DataTime=0.419 Loss=1.056 Prec@1=73.628 Prec@5=91.103 rate=1.75 Hz, eta=0:21:56, total=0:01:54, wall=15:19 IST
=> Training   8.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.578 DataTime=0.404 Loss=1.053 Prec@1=73.666 Prec@5=91.114 rate=1.75 Hz, eta=0:21:56, total=0:01:54, wall=15:19 IST
=> Training   12.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.578 DataTime=0.404 Loss=1.053 Prec@1=73.666 Prec@5=91.114 rate=1.77 Hz, eta=0:20:40, total=0:02:49, wall=15:19 IST
=> Training   12.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.578 DataTime=0.404 Loss=1.053 Prec@1=73.666 Prec@5=91.114 rate=1.77 Hz, eta=0:20:40, total=0:02:49, wall=15:20 IST
=> Training   12.03% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.572 DataTime=0.397 Loss=1.052 Prec@1=73.622 Prec@5=91.100 rate=1.77 Hz, eta=0:20:40, total=0:02:49, wall=15:20 IST
=> Training   16.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.572 DataTime=0.397 Loss=1.052 Prec@1=73.622 Prec@5=91.100 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=15:20 IST
=> Training   16.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.572 DataTime=0.397 Loss=1.052 Prec@1=73.622 Prec@5=91.100 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=15:21 IST
=> Training   16.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.397 Loss=1.050 Prec@1=73.634 Prec@5=91.151 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=15:21 IST
=> Training   20.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.397 Loss=1.050 Prec@1=73.634 Prec@5=91.151 rate=1.78 Hz, eta=0:18:45, total=0:04:41, wall=15:21 IST
=> Training   20.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.397 Loss=1.050 Prec@1=73.634 Prec@5=91.151 rate=1.78 Hz, eta=0:18:45, total=0:04:41, wall=15:22 IST
=> Training   20.02% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.396 Loss=1.053 Prec@1=73.628 Prec@5=91.115 rate=1.78 Hz, eta=0:18:45, total=0:04:41, wall=15:22 IST
=> Training   24.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.396 Loss=1.053 Prec@1=73.628 Prec@5=91.115 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=15:22 IST
=> Training   24.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.571 DataTime=0.396 Loss=1.053 Prec@1=73.628 Prec@5=91.115 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=15:23 IST
=> Training   24.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.570 DataTime=0.395 Loss=1.054 Prec@1=73.562 Prec@5=91.098 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=15:23 IST
=> Training   28.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.570 DataTime=0.395 Loss=1.054 Prec@1=73.562 Prec@5=91.098 rate=1.77 Hz, eta=0:16:56, total=0:06:35, wall=15:23 IST
=> Training   28.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.570 DataTime=0.395 Loss=1.054 Prec@1=73.562 Prec@5=91.098 rate=1.77 Hz, eta=0:16:56, total=0:06:35, wall=15:24 IST
=> Training   28.01% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.569 DataTime=0.393 Loss=1.055 Prec@1=73.542 Prec@5=91.099 rate=1.77 Hz, eta=0:16:56, total=0:06:35, wall=15:24 IST
=> Training   32.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.569 DataTime=0.393 Loss=1.055 Prec@1=73.542 Prec@5=91.099 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=15:24 IST
=> Training   32.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.569 DataTime=0.393 Loss=1.055 Prec@1=73.542 Prec@5=91.099 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=15:25 IST
=> Training   32.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.568 DataTime=0.392 Loss=1.056 Prec@1=73.506 Prec@5=91.079 rate=1.77 Hz, eta=0:15:59, total=0:07:31, wall=15:25 IST
=> Training   36.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.568 DataTime=0.392 Loss=1.056 Prec@1=73.506 Prec@5=91.079 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=15:25 IST
=> Training   36.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.568 DataTime=0.392 Loss=1.056 Prec@1=73.506 Prec@5=91.079 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=15:26 IST
=> Training   36.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.567 DataTime=0.391 Loss=1.055 Prec@1=73.522 Prec@5=91.098 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=15:26 IST
=> Training   39.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.567 DataTime=0.391 Loss=1.055 Prec@1=73.522 Prec@5=91.098 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=15:26 IST
=> Training   39.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.567 DataTime=0.391 Loss=1.055 Prec@1=73.522 Prec@5=91.098 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=15:27 IST
=> Training   39.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.390 Loss=1.057 Prec@1=73.498 Prec@5=91.069 rate=1.78 Hz, eta=0:14:05, total=0:09:23, wall=15:27 IST
=> Training   43.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.390 Loss=1.057 Prec@1=73.498 Prec@5=91.069 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=15:27 IST
=> Training   43.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.390 Loss=1.057 Prec@1=73.498 Prec@5=91.069 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=15:28 IST
=> Training   43.99% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.389 Loss=1.059 Prec@1=73.451 Prec@5=91.047 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=15:28 IST
=> Training   47.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.389 Loss=1.059 Prec@1=73.451 Prec@5=91.047 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=15:28 IST
=> Training   47.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.566 DataTime=0.389 Loss=1.059 Prec@1=73.451 Prec@5=91.047 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=15:29 IST
=> Training   47.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.387 Loss=1.061 Prec@1=73.395 Prec@5=91.009 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=15:29 IST
=> Training   51.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.387 Loss=1.061 Prec@1=73.395 Prec@5=91.009 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=15:29 IST
=> Training   51.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.387 Loss=1.061 Prec@1=73.395 Prec@5=91.009 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=15:30 IST
=> Training   51.98% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.062 Prec@1=73.357 Prec@5=90.987 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=15:30 IST
=> Training   55.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.062 Prec@1=73.357 Prec@5=90.987 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=15:30 IST
=> Training   55.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.062 Prec@1=73.357 Prec@5=90.987 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=15:31 IST
=> Training   55.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.063 Prec@1=73.326 Prec@5=90.978 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=15:31 IST
=> Training   59.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.063 Prec@1=73.326 Prec@5=90.978 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=15:31 IST
=> Training   59.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.063 Prec@1=73.326 Prec@5=90.978 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=15:32 IST
=> Training   59.97% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.064 Prec@1=73.301 Prec@5=90.972 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=15:32 IST
=> Training   63.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.064 Prec@1=73.301 Prec@5=90.972 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:32 IST
=> Training   63.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.064 Prec@1=73.301 Prec@5=90.972 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:32 IST
=> Training   63.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.066 Prec@1=73.261 Prec@5=90.950 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=15:32 IST
=> Training   67.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.066 Prec@1=73.261 Prec@5=90.950 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=15:32 IST
=> Training   67.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.388 Loss=1.066 Prec@1=73.261 Prec@5=90.950 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=15:33 IST
=> Training   67.96% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.067 Prec@1=73.232 Prec@5=90.936 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=15:33 IST
=> Training   71.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.067 Prec@1=73.232 Prec@5=90.936 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=15:33 IST
=> Training   71.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.067 Prec@1=73.232 Prec@5=90.936 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=15:34 IST
=> Training   71.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.068 Prec@1=73.197 Prec@5=90.925 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=15:34 IST
=> Training   75.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.068 Prec@1=73.197 Prec@5=90.925 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=15:34 IST
=> Training   75.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.068 Prec@1=73.197 Prec@5=90.925 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=15:35 IST
=> Training   75.95% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.069 Prec@1=73.175 Prec@5=90.911 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=15:35 IST
=> Training   79.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.069 Prec@1=73.175 Prec@5=90.911 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=15:35 IST
=> Training   79.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.069 Prec@1=73.175 Prec@5=90.911 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=15:36 IST
=> Training   79.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.387 Loss=1.071 Prec@1=73.151 Prec@5=90.895 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=15:36 IST
=> Training   83.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.387 Loss=1.071 Prec@1=73.151 Prec@5=90.895 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:36 IST
=> Training   83.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.387 Loss=1.071 Prec@1=73.151 Prec@5=90.895 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:37 IST
=> Training   83.94% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.389 Loss=1.072 Prec@1=73.126 Prec@5=90.875 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=15:37 IST
=> Training   87.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.389 Loss=1.072 Prec@1=73.126 Prec@5=90.875 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=15:37 IST
=> Training   87.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.564 DataTime=0.389 Loss=1.072 Prec@1=73.126 Prec@5=90.875 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=15:38 IST
=> Training   87.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.073 Prec@1=73.095 Prec@5=90.864 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=15:38 IST
=> Training   91.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.073 Prec@1=73.095 Prec@5=90.864 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=15:38 IST
=> Training   91.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.073 Prec@1=73.095 Prec@5=90.864 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=15:39 IST
=> Training   91.93% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.389 Loss=1.073 Prec@1=73.089 Prec@5=90.860 rate=1.78 Hz, eta=0:01:53, total=0:21:31, wall=15:39 IST
=> Training   95.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.389 Loss=1.073 Prec@1=73.089 Prec@5=90.860 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:39 IST
=> Training   95.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.389 Loss=1.073 Prec@1=73.089 Prec@5=90.860 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:40 IST
=> Training   95.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.074 Prec@1=73.081 Prec@5=90.846 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=15:40 IST
=> Training   99.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.074 Prec@1=73.081 Prec@5=90.846 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=15:40 IST
=> Training   99.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.074 Prec@1=73.081 Prec@5=90.846 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=15:40 IST
=> Training   99.92% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.074 Prec@1=73.081 Prec@5=90.845 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=15:40 IST
=> Training   100.00% of 1x2503...Epoch=99/150 LR=0.0268 Time=0.563 DataTime=0.388 Loss=1.074 Prec@1=73.081 Prec@5=90.845 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=15:40 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:40 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=15:40 IST
=> Validation 0.00% of 1x98...Epoch=99/150 LR=0.0268 Time=6.958 Loss=0.882 Prec@1=77.930 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=15:40 IST
=> Validation 1.02% of 1x98...Epoch=99/150 LR=0.0268 Time=6.958 Loss=0.882 Prec@1=77.930 Prec@5=92.969 rate=1411.08 Hz, eta=0:00:00, total=0:00:00, wall=15:40 IST
** Validation 1.02% of 1x98...Epoch=99/150 LR=0.0268 Time=6.958 Loss=0.882 Prec@1=77.930 Prec@5=92.969 rate=1411.08 Hz, eta=0:00:00, total=0:00:00, wall=15:41 IST
** Validation 1.02% of 1x98...Epoch=99/150 LR=0.0268 Time=0.633 Loss=1.300 Prec@1=68.408 Prec@5=88.496 rate=1411.08 Hz, eta=0:00:00, total=0:00:00, wall=15:41 IST
** Validation 100.00% of 1x98...Epoch=99/150 LR=0.0268 Time=0.633 Loss=1.300 Prec@1=68.408 Prec@5=88.496 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=15:41 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:41 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=15:41 IST
=> Training   0.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=4.783 DataTime=4.177 Loss=0.905 Prec@1=75.000 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=15:41 IST
=> Training   0.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=4.783 DataTime=4.177 Loss=0.905 Prec@1=75.000 Prec@5=92.773 rate=5470.76 Hz, eta=0:00:00, total=0:00:00, wall=15:41 IST
=> Training   0.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=4.783 DataTime=4.177 Loss=0.905 Prec@1=75.000 Prec@5=92.773 rate=5470.76 Hz, eta=0:00:00, total=0:00:00, wall=15:42 IST
=> Training   0.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.591 DataTime=0.425 Loss=1.035 Prec@1=73.973 Prec@5=91.244 rate=5470.76 Hz, eta=0:00:00, total=0:00:00, wall=15:42 IST
=> Training   4.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.591 DataTime=0.425 Loss=1.035 Prec@1=73.973 Prec@5=91.244 rate=1.83 Hz, eta=0:21:49, total=0:00:55, wall=15:42 IST
=> Training   4.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.591 DataTime=0.425 Loss=1.035 Prec@1=73.973 Prec@5=91.244 rate=1.83 Hz, eta=0:21:49, total=0:00:55, wall=15:43 IST
=> Training   4.04% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.577 DataTime=0.410 Loss=1.033 Prec@1=74.057 Prec@5=91.303 rate=1.83 Hz, eta=0:21:49, total=0:00:55, wall=15:43 IST
=> Training   8.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.577 DataTime=0.410 Loss=1.033 Prec@1=74.057 Prec@5=91.303 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=15:43 IST
=> Training   8.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.577 DataTime=0.410 Loss=1.033 Prec@1=74.057 Prec@5=91.303 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=15:44 IST
=> Training   8.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.578 DataTime=0.411 Loss=1.033 Prec@1=73.979 Prec@5=91.339 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=15:44 IST
=> Training   12.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.578 DataTime=0.411 Loss=1.033 Prec@1=73.979 Prec@5=91.339 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=15:44 IST
=> Training   12.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.578 DataTime=0.411 Loss=1.033 Prec@1=73.979 Prec@5=91.339 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=15:45 IST
=> Training   12.03% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.574 DataTime=0.405 Loss=1.035 Prec@1=73.936 Prec@5=91.335 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=15:45 IST
=> Training   16.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.574 DataTime=0.405 Loss=1.035 Prec@1=73.936 Prec@5=91.335 rate=1.78 Hz, eta=0:19:41, total=0:03:45, wall=15:45 IST
=> Training   16.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.574 DataTime=0.405 Loss=1.035 Prec@1=73.936 Prec@5=91.335 rate=1.78 Hz, eta=0:19:41, total=0:03:45, wall=15:46 IST
=> Training   16.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.572 DataTime=0.402 Loss=1.039 Prec@1=73.878 Prec@5=91.257 rate=1.78 Hz, eta=0:19:41, total=0:03:45, wall=15:46 IST
=> Training   20.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.572 DataTime=0.402 Loss=1.039 Prec@1=73.878 Prec@5=91.257 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=15:46 IST
=> Training   20.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.572 DataTime=0.402 Loss=1.039 Prec@1=73.878 Prec@5=91.257 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=15:47 IST
=> Training   20.02% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.569 DataTime=0.396 Loss=1.040 Prec@1=73.897 Prec@5=91.255 rate=1.78 Hz, eta=0:18:47, total=0:04:42, wall=15:47 IST
=> Training   24.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.569 DataTime=0.396 Loss=1.040 Prec@1=73.897 Prec@5=91.255 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=15:47 IST
=> Training   24.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.569 DataTime=0.396 Loss=1.040 Prec@1=73.897 Prec@5=91.255 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=15:48 IST
=> Training   24.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.568 DataTime=0.394 Loss=1.041 Prec@1=73.845 Prec@5=91.245 rate=1.78 Hz, eta=0:17:46, total=0:05:37, wall=15:48 IST
=> Training   28.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.568 DataTime=0.394 Loss=1.041 Prec@1=73.845 Prec@5=91.245 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=15:48 IST
=> Training   28.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.568 DataTime=0.394 Loss=1.041 Prec@1=73.845 Prec@5=91.245 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=15:49 IST
=> Training   28.01% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.566 DataTime=0.391 Loss=1.041 Prec@1=73.823 Prec@5=91.225 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=15:49 IST
=> Training   32.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.566 DataTime=0.391 Loss=1.041 Prec@1=73.823 Prec@5=91.225 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=15:49 IST
=> Training   32.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.566 DataTime=0.391 Loss=1.041 Prec@1=73.823 Prec@5=91.225 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=15:50 IST
=> Training   32.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.044 Prec@1=73.751 Prec@5=91.197 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=15:50 IST
=> Training   36.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.044 Prec@1=73.751 Prec@5=91.197 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=15:50 IST
=> Training   36.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.044 Prec@1=73.751 Prec@5=91.197 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=15:50 IST
=> Training   36.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.389 Loss=1.045 Prec@1=73.731 Prec@5=91.163 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=15:50 IST
=> Training   39.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.389 Loss=1.045 Prec@1=73.731 Prec@5=91.163 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=15:50 IST
=> Training   39.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.389 Loss=1.045 Prec@1=73.731 Prec@5=91.163 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=15:51 IST
=> Training   39.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.391 Loss=1.046 Prec@1=73.729 Prec@5=91.156 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=15:51 IST
=> Training   43.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.391 Loss=1.046 Prec@1=73.729 Prec@5=91.156 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=15:51 IST
=> Training   43.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.391 Loss=1.046 Prec@1=73.729 Prec@5=91.156 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=15:52 IST
=> Training   43.99% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.049 Prec@1=73.689 Prec@5=91.107 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=15:52 IST
=> Training   47.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.049 Prec@1=73.689 Prec@5=91.107 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=15:52 IST
=> Training   47.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.049 Prec@1=73.689 Prec@5=91.107 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=15:53 IST
=> Training   47.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.051 Prec@1=73.648 Prec@5=91.100 rate=1.78 Hz, eta=0:12:09, total=0:11:13, wall=15:53 IST
=> Training   51.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.051 Prec@1=73.648 Prec@5=91.100 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=15:53 IST
=> Training   51.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.051 Prec@1=73.648 Prec@5=91.100 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=15:54 IST
=> Training   51.98% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.052 Prec@1=73.613 Prec@5=91.077 rate=1.78 Hz, eta=0:11:14, total=0:12:10, wall=15:54 IST
=> Training   55.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.052 Prec@1=73.613 Prec@5=91.077 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:54 IST
=> Training   55.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.052 Prec@1=73.613 Prec@5=91.077 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:55 IST
=> Training   55.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.053 Prec@1=73.585 Prec@5=91.076 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=15:55 IST
=> Training   59.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.053 Prec@1=73.585 Prec@5=91.076 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=15:55 IST
=> Training   59.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.053 Prec@1=73.585 Prec@5=91.076 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=15:56 IST
=> Training   59.97% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.054 Prec@1=73.564 Prec@5=91.073 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=15:56 IST
=> Training   63.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.054 Prec@1=73.564 Prec@5=91.073 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=15:56 IST
=> Training   63.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.054 Prec@1=73.564 Prec@5=91.073 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=15:57 IST
=> Training   63.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.055 Prec@1=73.530 Prec@5=91.066 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=15:57 IST
=> Training   67.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.055 Prec@1=73.530 Prec@5=91.066 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=15:57 IST
=> Training   67.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.055 Prec@1=73.530 Prec@5=91.066 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=15:58 IST
=> Training   67.96% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.056 Prec@1=73.495 Prec@5=91.044 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=15:58 IST
=> Training   71.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.056 Prec@1=73.495 Prec@5=91.044 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=15:58 IST
=> Training   71.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.056 Prec@1=73.495 Prec@5=91.044 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=15:59 IST
=> Training   71.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.464 Prec@5=91.023 rate=1.78 Hz, eta=0:06:34, total=0:16:51, wall=15:59 IST
=> Training   75.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.464 Prec@5=91.023 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=15:59 IST
=> Training   75.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.058 Prec@1=73.464 Prec@5=91.023 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=16:00 IST
=> Training   75.95% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.059 Prec@1=73.426 Prec@5=91.012 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=16:00 IST
=> Training   79.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.059 Prec@1=73.426 Prec@5=91.012 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=16:00 IST
=> Training   79.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.059 Prec@1=73.426 Prec@5=91.012 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=16:01 IST
=> Training   79.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.060 Prec@1=73.410 Prec@5=91.001 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=16:01 IST
=> Training   83.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.060 Prec@1=73.410 Prec@5=91.001 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=16:01 IST
=> Training   83.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.060 Prec@1=73.410 Prec@5=91.001 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=16:02 IST
=> Training   83.94% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.061 Prec@1=73.389 Prec@5=90.989 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=16:02 IST
=> Training   87.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.061 Prec@1=73.389 Prec@5=90.989 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=16:02 IST
=> Training   87.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.389 Loss=1.061 Prec@1=73.389 Prec@5=90.989 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=16:03 IST
=> Training   87.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.063 Prec@1=73.361 Prec@5=90.968 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=16:03 IST
=> Training   91.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.063 Prec@1=73.361 Prec@5=90.968 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=16:03 IST
=> Training   91.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.565 DataTime=0.390 Loss=1.063 Prec@1=73.361 Prec@5=90.968 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=16:04 IST
=> Training   91.93% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.064 Prec@1=73.335 Prec@5=90.954 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=16:04 IST
=> Training   95.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.064 Prec@1=73.335 Prec@5=90.954 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=16:04 IST
=> Training   95.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.064 Prec@1=73.335 Prec@5=90.954 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=16:05 IST
=> Training   95.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.065 Prec@1=73.311 Prec@5=90.946 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=16:05 IST
=> Training   99.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.065 Prec@1=73.311 Prec@5=90.946 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=16:05 IST
=> Training   99.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.065 Prec@1=73.311 Prec@5=90.946 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=16:05 IST
=> Training   99.92% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.065 Prec@1=73.311 Prec@5=90.945 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=16:05 IST
=> Training   100.00% of 1x2503...Epoch=100/150 LR=0.0259 Time=0.564 DataTime=0.390 Loss=1.065 Prec@1=73.311 Prec@5=90.945 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=16:05 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:05 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:05 IST
=> Validation 0.00% of 1x98...Epoch=100/150 LR=0.0259 Time=7.046 Loss=0.751 Prec@1=80.469 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=16:05 IST
=> Validation 1.02% of 1x98...Epoch=100/150 LR=0.0259 Time=7.046 Loss=0.751 Prec@1=80.469 Prec@5=94.727 rate=2893.23 Hz, eta=0:00:00, total=0:00:00, wall=16:05 IST
** Validation 1.02% of 1x98...Epoch=100/150 LR=0.0259 Time=7.046 Loss=0.751 Prec@1=80.469 Prec@5=94.727 rate=2893.23 Hz, eta=0:00:00, total=0:00:00, wall=16:06 IST
** Validation 1.02% of 1x98...Epoch=100/150 LR=0.0259 Time=0.631 Loss=1.293 Prec@1=68.496 Prec@5=88.658 rate=2893.23 Hz, eta=0:00:00, total=0:00:00, wall=16:06 IST
** Validation 100.00% of 1x98...Epoch=100/150 LR=0.0259 Time=0.631 Loss=1.293 Prec@1=68.496 Prec@5=88.658 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=16:06 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:06 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:06 IST
=> Training   0.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=4.774 DataTime=4.219 Loss=0.975 Prec@1=74.414 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=16:06 IST
=> Training   0.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=4.774 DataTime=4.219 Loss=0.975 Prec@1=74.414 Prec@5=92.969 rate=4307.48 Hz, eta=0:00:00, total=0:00:00, wall=16:06 IST
=> Training   0.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=4.774 DataTime=4.219 Loss=0.975 Prec@1=74.414 Prec@5=92.969 rate=4307.48 Hz, eta=0:00:00, total=0:00:00, wall=16:07 IST
=> Training   0.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.573 DataTime=0.406 Loss=1.026 Prec@1=74.114 Prec@5=91.495 rate=4307.48 Hz, eta=0:00:00, total=0:00:00, wall=16:07 IST
=> Training   4.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.573 DataTime=0.406 Loss=1.026 Prec@1=74.114 Prec@5=91.495 rate=1.90 Hz, eta=0:21:02, total=0:00:53, wall=16:07 IST
=> Training   4.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.573 DataTime=0.406 Loss=1.026 Prec@1=74.114 Prec@5=91.495 rate=1.90 Hz, eta=0:21:02, total=0:00:53, wall=16:08 IST
=> Training   4.04% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.574 DataTime=0.409 Loss=1.033 Prec@1=74.052 Prec@5=91.416 rate=1.90 Hz, eta=0:21:02, total=0:00:53, wall=16:08 IST
=> Training   8.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.574 DataTime=0.409 Loss=1.033 Prec@1=74.052 Prec@5=91.416 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=16:08 IST
=> Training   8.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.574 DataTime=0.409 Loss=1.033 Prec@1=74.052 Prec@5=91.416 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=16:08 IST
=> Training   8.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.401 Loss=1.035 Prec@1=73.986 Prec@5=91.335 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=16:08 IST
=> Training   12.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.401 Loss=1.035 Prec@1=73.986 Prec@5=91.335 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=16:08 IST
=> Training   12.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.401 Loss=1.035 Prec@1=73.986 Prec@5=91.335 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=16:09 IST
=> Training   12.03% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.402 Loss=1.036 Prec@1=73.988 Prec@5=91.316 rate=1.81 Hz, eta=0:20:15, total=0:02:46, wall=16:09 IST
=> Training   16.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.402 Loss=1.036 Prec@1=73.988 Prec@5=91.316 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=16:09 IST
=> Training   16.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.568 DataTime=0.402 Loss=1.036 Prec@1=73.988 Prec@5=91.316 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=16:10 IST
=> Training   16.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.397 Loss=1.033 Prec@1=74.071 Prec@5=91.338 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=16:10 IST
=> Training   20.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.397 Loss=1.033 Prec@1=74.071 Prec@5=91.338 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=16:10 IST
=> Training   20.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.397 Loss=1.033 Prec@1=74.071 Prec@5=91.338 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=16:11 IST
=> Training   20.02% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.396 Loss=1.035 Prec@1=74.042 Prec@5=91.297 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=16:11 IST
=> Training   24.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.396 Loss=1.035 Prec@1=74.042 Prec@5=91.297 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:11 IST
=> Training   24.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.396 Loss=1.035 Prec@1=74.042 Prec@5=91.297 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:12 IST
=> Training   24.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.398 Loss=1.036 Prec@1=73.995 Prec@5=91.307 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:12 IST
=> Training   28.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.398 Loss=1.036 Prec@1=73.995 Prec@5=91.307 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=16:12 IST
=> Training   28.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.398 Loss=1.036 Prec@1=73.995 Prec@5=91.307 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=16:13 IST
=> Training   28.01% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.567 DataTime=0.399 Loss=1.038 Prec@1=73.964 Prec@5=91.286 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=16:13 IST
=> Training   32.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.567 DataTime=0.399 Loss=1.038 Prec@1=73.964 Prec@5=91.286 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=16:13 IST
=> Training   32.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.567 DataTime=0.399 Loss=1.038 Prec@1=73.964 Prec@5=91.286 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=16:14 IST
=> Training   32.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.396 Loss=1.038 Prec@1=73.928 Prec@5=91.280 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=16:14 IST
=> Training   36.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.396 Loss=1.038 Prec@1=73.928 Prec@5=91.280 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=16:14 IST
=> Training   36.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.566 DataTime=0.396 Loss=1.038 Prec@1=73.928 Prec@5=91.280 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=16:15 IST
=> Training   36.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.039 Prec@1=73.912 Prec@5=91.278 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=16:15 IST
=> Training   39.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.039 Prec@1=73.912 Prec@5=91.278 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=16:15 IST
=> Training   39.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.039 Prec@1=73.912 Prec@5=91.278 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=16:16 IST
=> Training   39.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.040 Prec@1=73.882 Prec@5=91.275 rate=1.78 Hz, eta=0:14:01, total=0:09:21, wall=16:16 IST
=> Training   43.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.040 Prec@1=73.882 Prec@5=91.275 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=16:16 IST
=> Training   43.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.395 Loss=1.040 Prec@1=73.882 Prec@5=91.275 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=16:17 IST
=> Training   43.99% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.859 Prec@5=91.275 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=16:17 IST
=> Training   47.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.859 Prec@5=91.275 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=16:17 IST
=> Training   47.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.859 Prec@5=91.275 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=16:18 IST
=> Training   47.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.854 Prec@5=91.277 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=16:18 IST
=> Training   51.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.854 Prec@5=91.277 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=16:18 IST
=> Training   51.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.565 DataTime=0.394 Loss=1.041 Prec@1=73.854 Prec@5=91.277 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=16:19 IST
=> Training   51.98% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.393 Loss=1.043 Prec@1=73.810 Prec@5=91.250 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=16:19 IST
=> Training   55.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.393 Loss=1.043 Prec@1=73.810 Prec@5=91.250 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=16:19 IST
=> Training   55.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.393 Loss=1.043 Prec@1=73.810 Prec@5=91.250 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=16:20 IST
=> Training   55.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.044 Prec@1=73.783 Prec@5=91.229 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=16:20 IST
=> Training   59.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.044 Prec@1=73.783 Prec@5=91.229 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=16:20 IST
=> Training   59.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.044 Prec@1=73.783 Prec@5=91.229 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=16:21 IST
=> Training   59.97% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.392 Loss=1.047 Prec@1=73.721 Prec@5=91.197 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=16:21 IST
=> Training   63.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.392 Loss=1.047 Prec@1=73.721 Prec@5=91.197 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=16:21 IST
=> Training   63.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.392 Loss=1.047 Prec@1=73.721 Prec@5=91.197 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=16:22 IST
=> Training   63.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.047 Prec@1=73.708 Prec@5=91.197 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=16:22 IST
=> Training   67.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.047 Prec@1=73.708 Prec@5=91.197 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=16:22 IST
=> Training   67.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.392 Loss=1.047 Prec@1=73.708 Prec@5=91.197 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=16:23 IST
=> Training   67.96% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.391 Loss=1.047 Prec@1=73.704 Prec@5=91.191 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=16:23 IST
=> Training   71.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.391 Loss=1.047 Prec@1=73.704 Prec@5=91.191 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=16:23 IST
=> Training   71.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.564 DataTime=0.391 Loss=1.047 Prec@1=73.704 Prec@5=91.191 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=16:23 IST
=> Training   71.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.048 Prec@1=73.680 Prec@5=91.170 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=16:23 IST
=> Training   75.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.048 Prec@1=73.680 Prec@5=91.170 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=16:23 IST
=> Training   75.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.048 Prec@1=73.680 Prec@5=91.170 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=16:24 IST
=> Training   75.95% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.049 Prec@1=73.660 Prec@5=91.159 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=16:24 IST
=> Training   79.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.049 Prec@1=73.660 Prec@5=91.159 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=16:24 IST
=> Training   79.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.391 Loss=1.049 Prec@1=73.660 Prec@5=91.159 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=16:25 IST
=> Training   79.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.050 Prec@1=73.632 Prec@5=91.150 rate=1.78 Hz, eta=0:04:41, total=0:18:42, wall=16:25 IST
=> Training   83.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.050 Prec@1=73.632 Prec@5=91.150 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=16:25 IST
=> Training   83.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.050 Prec@1=73.632 Prec@5=91.150 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=16:26 IST
=> Training   83.94% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.051 Prec@1=73.620 Prec@5=91.146 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=16:26 IST
=> Training   87.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.051 Prec@1=73.620 Prec@5=91.146 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=16:26 IST
=> Training   87.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.051 Prec@1=73.620 Prec@5=91.146 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=16:27 IST
=> Training   87.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.052 Prec@1=73.599 Prec@5=91.133 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=16:27 IST
=> Training   91.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.052 Prec@1=73.599 Prec@5=91.133 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=16:27 IST
=> Training   91.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.563 DataTime=0.390 Loss=1.052 Prec@1=73.599 Prec@5=91.133 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=16:28 IST
=> Training   91.93% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.052 Prec@1=73.579 Prec@5=91.125 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=16:28 IST
=> Training   95.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.052 Prec@1=73.579 Prec@5=91.125 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=16:28 IST
=> Training   95.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.052 Prec@1=73.579 Prec@5=91.125 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=16:29 IST
=> Training   95.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.053 Prec@1=73.548 Prec@5=91.113 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=16:29 IST
=> Training   99.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.053 Prec@1=73.548 Prec@5=91.113 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=16:29 IST
=> Training   99.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.053 Prec@1=73.548 Prec@5=91.113 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=16:29 IST
=> Training   99.92% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.053 Prec@1=73.548 Prec@5=91.113 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=16:29 IST
=> Training   100.00% of 1x2503...Epoch=101/150 LR=0.0250 Time=0.562 DataTime=0.390 Loss=1.053 Prec@1=73.548 Prec@5=91.113 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=16:29 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST
=> Validation 0.00% of 1x98...Epoch=101/150 LR=0.0250 Time=7.257 Loss=0.741 Prec@1=81.055 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=16:29 IST
=> Validation 1.02% of 1x98...Epoch=101/150 LR=0.0250 Time=7.257 Loss=0.741 Prec@1=81.055 Prec@5=94.727 rate=7519.13 Hz, eta=0:00:00, total=0:00:00, wall=16:29 IST
** Validation 1.02% of 1x98...Epoch=101/150 LR=0.0250 Time=7.257 Loss=0.741 Prec@1=81.055 Prec@5=94.727 rate=7519.13 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST
** Validation 1.02% of 1x98...Epoch=101/150 LR=0.0250 Time=0.639 Loss=1.307 Prec@1=68.544 Prec@5=88.534 rate=7519.13 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST
** Validation 100.00% of 1x98...Epoch=101/150 LR=0.0250 Time=0.639 Loss=1.307 Prec@1=68.544 Prec@5=88.534 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=16:30 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> Training   0.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=4.381 DataTime=4.077 Loss=0.806 Prec@1=79.492 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=16:30 IST
=> Training   0.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=4.381 DataTime=4.077 Loss=0.806 Prec@1=79.492 Prec@5=92.383 rate=5205.32 Hz, eta=0:00:00, total=0:00:00, wall=16:30 IST
=> Training   0.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=4.381 DataTime=4.077 Loss=0.806 Prec@1=79.492 Prec@5=92.383 rate=5205.32 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
=> Training   0.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.587 DataTime=0.416 Loss=1.025 Prec@1=74.323 Prec@5=91.404 rate=5205.32 Hz, eta=0:00:00, total=0:00:00, wall=16:31 IST
=> Training   4.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.587 DataTime=0.416 Loss=1.025 Prec@1=74.323 Prec@5=91.404 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=16:31 IST
=> Training   4.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.587 DataTime=0.416 Loss=1.025 Prec@1=74.323 Prec@5=91.404 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=16:32 IST
=> Training   4.04% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.582 DataTime=0.409 Loss=1.023 Prec@1=74.368 Prec@5=91.413 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=16:32 IST
=> Training   8.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.582 DataTime=0.409 Loss=1.023 Prec@1=74.368 Prec@5=91.413 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=16:32 IST
=> Training   8.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.582 DataTime=0.409 Loss=1.023 Prec@1=74.368 Prec@5=91.413 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=16:33 IST
=> Training   8.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.572 DataTime=0.398 Loss=1.024 Prec@1=74.323 Prec@5=91.379 rate=1.79 Hz, eta=0:21:29, total=0:01:52, wall=16:33 IST
=> Training   12.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.572 DataTime=0.398 Loss=1.024 Prec@1=74.323 Prec@5=91.379 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=16:33 IST
=> Training   12.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.572 DataTime=0.398 Loss=1.024 Prec@1=74.323 Prec@5=91.379 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=16:34 IST
=> Training   12.03% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.570 DataTime=0.394 Loss=1.021 Prec@1=74.369 Prec@5=91.455 rate=1.79 Hz, eta=0:20:27, total=0:02:47, wall=16:34 IST
=> Training   16.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.570 DataTime=0.394 Loss=1.021 Prec@1=74.369 Prec@5=91.455 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=16:34 IST
=> Training   16.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.570 DataTime=0.394 Loss=1.021 Prec@1=74.369 Prec@5=91.455 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=16:35 IST
=> Training   16.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.566 DataTime=0.389 Loss=1.023 Prec@1=74.318 Prec@5=91.451 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=16:35 IST
=> Training   20.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.566 DataTime=0.389 Loss=1.023 Prec@1=74.318 Prec@5=91.451 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=16:35 IST
=> Training   20.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.566 DataTime=0.389 Loss=1.023 Prec@1=74.318 Prec@5=91.451 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=16:36 IST
=> Training   20.02% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.387 Loss=1.024 Prec@1=74.305 Prec@5=91.426 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=16:36 IST
=> Training   24.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.387 Loss=1.024 Prec@1=74.305 Prec@5=91.426 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:36 IST
=> Training   24.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.387 Loss=1.024 Prec@1=74.305 Prec@5=91.426 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:37 IST
=> Training   24.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.026 Prec@1=74.224 Prec@5=91.403 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=16:37 IST
=> Training   28.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.026 Prec@1=74.224 Prec@5=91.403 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=16:37 IST
=> Training   28.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.026 Prec@1=74.224 Prec@5=91.403 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=16:38 IST
=> Training   28.01% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.028 Prec@1=74.198 Prec@5=91.377 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=16:38 IST
=> Training   32.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.028 Prec@1=74.198 Prec@5=91.377 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=16:38 IST
=> Training   32.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.388 Loss=1.028 Prec@1=74.198 Prec@5=91.377 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=16:39 IST
=> Training   32.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.565 DataTime=0.388 Loss=1.028 Prec@1=74.168 Prec@5=91.373 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=16:39 IST
=> Training   36.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.565 DataTime=0.388 Loss=1.028 Prec@1=74.168 Prec@5=91.373 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=16:39 IST
=> Training   36.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.565 DataTime=0.388 Loss=1.028 Prec@1=74.168 Prec@5=91.373 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=16:40 IST
=> Training   36.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.029 Prec@1=74.150 Prec@5=91.349 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=16:40 IST
=> Training   39.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.029 Prec@1=74.150 Prec@5=91.349 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=16:40 IST
=> Training   39.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.029 Prec@1=74.150 Prec@5=91.349 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=16:40 IST
=> Training   39.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.387 Loss=1.030 Prec@1=74.102 Prec@5=91.334 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=16:40 IST
=> Training   43.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.387 Loss=1.030 Prec@1=74.102 Prec@5=91.334 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=16:40 IST
=> Training   43.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.387 Loss=1.030 Prec@1=74.102 Prec@5=91.334 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=16:41 IST
=> Training   43.99% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.385 Loss=1.031 Prec@1=74.078 Prec@5=91.325 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=16:41 IST
=> Training   47.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.385 Loss=1.031 Prec@1=74.078 Prec@5=91.325 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=16:41 IST
=> Training   47.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.385 Loss=1.031 Prec@1=74.078 Prec@5=91.325 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=16:42 IST
=> Training   47.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.386 Loss=1.034 Prec@1=74.025 Prec@5=91.295 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=16:42 IST
=> Training   51.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.386 Loss=1.034 Prec@1=74.025 Prec@5=91.295 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=16:42 IST
=> Training   51.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.564 DataTime=0.386 Loss=1.034 Prec@1=74.025 Prec@5=91.295 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=16:43 IST
=> Training   51.98% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.035 Prec@1=73.991 Prec@5=91.284 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=16:43 IST
=> Training   55.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.035 Prec@1=73.991 Prec@5=91.284 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=16:43 IST
=> Training   55.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.035 Prec@1=73.991 Prec@5=91.284 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=16:44 IST
=> Training   55.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.036 Prec@1=73.975 Prec@5=91.269 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=16:44 IST
=> Training   59.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.036 Prec@1=73.975 Prec@5=91.269 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=16:44 IST
=> Training   59.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.563 DataTime=0.386 Loss=1.036 Prec@1=73.975 Prec@5=91.269 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=16:45 IST
=> Training   59.97% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.038 Prec@1=73.931 Prec@5=91.260 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=16:45 IST
=> Training   63.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.038 Prec@1=73.931 Prec@5=91.260 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=16:45 IST
=> Training   63.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.038 Prec@1=73.931 Prec@5=91.260 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=16:46 IST
=> Training   63.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.039 Prec@1=73.903 Prec@5=91.241 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=16:46 IST
=> Training   67.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.039 Prec@1=73.903 Prec@5=91.241 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=16:46 IST
=> Training   67.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.039 Prec@1=73.903 Prec@5=91.241 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=16:47 IST
=> Training   67.96% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.041 Prec@1=73.868 Prec@5=91.217 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=16:47 IST
=> Training   71.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.041 Prec@1=73.868 Prec@5=91.217 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=16:47 IST
=> Training   71.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.041 Prec@1=73.868 Prec@5=91.217 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=16:48 IST
=> Training   71.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.042 Prec@1=73.840 Prec@5=91.204 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=16:48 IST
=> Training   75.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.042 Prec@1=73.840 Prec@5=91.204 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=16:48 IST
=> Training   75.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.042 Prec@1=73.840 Prec@5=91.204 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=16:49 IST
=> Training   75.95% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.042 Prec@1=73.851 Prec@5=91.199 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=16:49 IST
=> Training   79.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.042 Prec@1=73.851 Prec@5=91.199 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=16:49 IST
=> Training   79.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.385 Loss=1.042 Prec@1=73.851 Prec@5=91.199 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=16:50 IST
=> Training   79.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.043 Prec@1=73.827 Prec@5=91.195 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=16:50 IST
=> Training   83.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.043 Prec@1=73.827 Prec@5=91.195 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=16:50 IST
=> Training   83.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.043 Prec@1=73.827 Prec@5=91.195 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=16:51 IST
=> Training   83.94% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.043 Prec@1=73.826 Prec@5=91.195 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=16:51 IST
=> Training   87.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.043 Prec@1=73.826 Prec@5=91.195 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=16:51 IST
=> Training   87.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.562 DataTime=0.386 Loss=1.043 Prec@1=73.826 Prec@5=91.195 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=16:52 IST
=> Training   87.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.044 Prec@1=73.805 Prec@5=91.185 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=16:52 IST
=> Training   91.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.044 Prec@1=73.805 Prec@5=91.185 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=16:52 IST
=> Training   91.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.044 Prec@1=73.805 Prec@5=91.185 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=16:53 IST
=> Training   91.93% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.045 Prec@1=73.790 Prec@5=91.176 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=16:53 IST
=> Training   95.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.045 Prec@1=73.790 Prec@5=91.176 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=16:53 IST
=> Training   95.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.386 Loss=1.045 Prec@1=73.790 Prec@5=91.176 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=16:53 IST
=> Training   95.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.046 Prec@1=73.764 Prec@5=91.159 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=16:53 IST
=> Training   99.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.046 Prec@1=73.764 Prec@5=91.159 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=16:53 IST
=> Training   99.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.561 DataTime=0.385 Loss=1.046 Prec@1=73.764 Prec@5=91.159 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=16:53 IST
=> Training   99.92% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.560 DataTime=0.385 Loss=1.046 Prec@1=73.764 Prec@5=91.159 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=16:53 IST
=> Training   100.00% of 1x2503...Epoch=102/150 LR=0.0241 Time=0.560 DataTime=0.385 Loss=1.046 Prec@1=73.764 Prec@5=91.159 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=16:53 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:54 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=16:54 IST
=> Validation 0.00% of 1x98...Epoch=102/150 LR=0.0241 Time=7.096 Loss=0.779 Prec@1=78.125 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=16:54 IST
=> Validation 1.02% of 1x98...Epoch=102/150 LR=0.0241 Time=7.096 Loss=0.779 Prec@1=78.125 Prec@5=94.336 rate=7246.58 Hz, eta=0:00:00, total=0:00:00, wall=16:54 IST
** Validation 1.02% of 1x98...Epoch=102/150 LR=0.0241 Time=7.096 Loss=0.779 Prec@1=78.125 Prec@5=94.336 rate=7246.58 Hz, eta=0:00:00, total=0:00:00, wall=16:55 IST
** Validation 1.02% of 1x98...Epoch=102/150 LR=0.0241 Time=0.636 Loss=1.285 Prec@1=68.852 Prec@5=88.650 rate=7246.58 Hz, eta=0:00:00, total=0:00:00, wall=16:55 IST
** Validation 100.00% of 1x98...Epoch=102/150 LR=0.0241 Time=0.636 Loss=1.285 Prec@1=68.852 Prec@5=88.650 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=16:55 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> Training   0.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=4.980 DataTime=4.476 Loss=1.039 Prec@1=74.805 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=16:55 IST
=> Training   0.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=4.980 DataTime=4.476 Loss=1.039 Prec@1=74.805 Prec@5=91.211 rate=6842.57 Hz, eta=0:00:00, total=0:00:00, wall=16:55 IST
=> Training   0.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=4.980 DataTime=4.476 Loss=1.039 Prec@1=74.805 Prec@5=91.211 rate=6842.57 Hz, eta=0:00:00, total=0:00:00, wall=16:56 IST
=> Training   0.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.585 DataTime=0.419 Loss=1.000 Prec@1=74.838 Prec@5=91.787 rate=6842.57 Hz, eta=0:00:00, total=0:00:00, wall=16:56 IST
=> Training   4.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.585 DataTime=0.419 Loss=1.000 Prec@1=74.838 Prec@5=91.787 rate=1.87 Hz, eta=0:21:27, total=0:00:54, wall=16:56 IST
=> Training   4.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.585 DataTime=0.419 Loss=1.000 Prec@1=74.838 Prec@5=91.787 rate=1.87 Hz, eta=0:21:27, total=0:00:54, wall=16:56 IST
=> Training   4.04% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.575 DataTime=0.408 Loss=1.008 Prec@1=74.591 Prec@5=91.649 rate=1.87 Hz, eta=0:21:27, total=0:00:54, wall=16:56 IST
=> Training   8.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.575 DataTime=0.408 Loss=1.008 Prec@1=74.591 Prec@5=91.649 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=16:56 IST
=> Training   8.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.575 DataTime=0.408 Loss=1.008 Prec@1=74.591 Prec@5=91.649 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=16:57 IST
=> Training   8.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.567 DataTime=0.395 Loss=1.009 Prec@1=74.607 Prec@5=91.638 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=16:57 IST
=> Training   12.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.567 DataTime=0.395 Loss=1.009 Prec@1=74.607 Prec@5=91.638 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=16:57 IST
=> Training   12.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.567 DataTime=0.395 Loss=1.009 Prec@1=74.607 Prec@5=91.638 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=16:58 IST
=> Training   12.03% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.566 DataTime=0.394 Loss=1.013 Prec@1=74.545 Prec@5=91.539 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=16:58 IST
=> Training   16.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.566 DataTime=0.394 Loss=1.013 Prec@1=74.545 Prec@5=91.539 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=16:58 IST
=> Training   16.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.566 DataTime=0.394 Loss=1.013 Prec@1=74.545 Prec@5=91.539 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=16:59 IST
=> Training   16.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.579 Prec@5=91.553 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=16:59 IST
=> Training   20.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.579 Prec@5=91.553 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=16:59 IST
=> Training   20.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.579 Prec@5=91.553 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=17:00 IST
=> Training   20.02% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.014 Prec@1=74.524 Prec@5=91.515 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=17:00 IST
=> Training   24.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.014 Prec@1=74.524 Prec@5=91.515 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=17:00 IST
=> Training   24.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.014 Prec@1=74.524 Prec@5=91.515 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=17:01 IST
=> Training   24.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.017 Prec@1=74.484 Prec@5=91.470 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=17:01 IST
=> Training   28.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.017 Prec@1=74.484 Prec@5=91.470 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=17:01 IST
=> Training   28.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.562 DataTime=0.388 Loss=1.017 Prec@1=74.484 Prec@5=91.470 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=17:02 IST
=> Training   28.01% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.020 Prec@1=74.413 Prec@5=91.441 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=17:02 IST
=> Training   32.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.020 Prec@1=74.413 Prec@5=91.441 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=17:02 IST
=> Training   32.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.020 Prec@1=74.413 Prec@5=91.441 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=17:03 IST
=> Training   32.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.561 DataTime=0.386 Loss=1.020 Prec@1=74.383 Prec@5=91.424 rate=1.81 Hz, eta=0:15:42, total=0:07:23, wall=17:03 IST
=> Training   36.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.561 DataTime=0.386 Loss=1.020 Prec@1=74.383 Prec@5=91.424 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=17:03 IST
=> Training   36.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.561 DataTime=0.386 Loss=1.020 Prec@1=74.383 Prec@5=91.424 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=17:04 IST
=> Training   36.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.022 Prec@1=74.357 Prec@5=91.415 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=17:04 IST
=> Training   39.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.022 Prec@1=74.357 Prec@5=91.415 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=17:04 IST
=> Training   39.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.022 Prec@1=74.357 Prec@5=91.415 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=17:05 IST
=> Training   39.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.023 Prec@1=74.316 Prec@5=91.394 rate=1.80 Hz, eta=0:13:52, total=0:09:15, wall=17:05 IST
=> Training   43.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.023 Prec@1=74.316 Prec@5=91.394 rate=1.80 Hz, eta=0:12:58, total=0:10:10, wall=17:05 IST
=> Training   43.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.023 Prec@1=74.316 Prec@5=91.394 rate=1.80 Hz, eta=0:12:58, total=0:10:10, wall=17:06 IST
=> Training   43.99% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.025 Prec@1=74.263 Prec@5=91.373 rate=1.80 Hz, eta=0:12:58, total=0:10:10, wall=17:06 IST
=> Training   47.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.025 Prec@1=74.263 Prec@5=91.373 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=17:06 IST
=> Training   47.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.025 Prec@1=74.263 Prec@5=91.373 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=17:07 IST
=> Training   47.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.025 Prec@1=74.245 Prec@5=91.371 rate=1.80 Hz, eta=0:12:02, total=0:11:06, wall=17:07 IST
=> Training   51.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.025 Prec@1=74.245 Prec@5=91.371 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=17:07 IST
=> Training   51.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.025 Prec@1=74.245 Prec@5=91.371 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=17:08 IST
=> Training   51.98% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.026 Prec@1=74.225 Prec@5=91.369 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=17:08 IST
=> Training   55.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.026 Prec@1=74.225 Prec@5=91.369 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=17:08 IST
=> Training   55.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.026 Prec@1=74.225 Prec@5=91.369 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=17:09 IST
=> Training   55.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.027 Prec@1=74.192 Prec@5=91.350 rate=1.80 Hz, eta=0:10:11, total=0:12:57, wall=17:09 IST
=> Training   59.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.027 Prec@1=74.192 Prec@5=91.350 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=17:09 IST
=> Training   59.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.385 Loss=1.027 Prec@1=74.192 Prec@5=91.350 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=17:09 IST
=> Training   59.97% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.028 Prec@1=74.181 Prec@5=91.351 rate=1.80 Hz, eta=0:09:18, total=0:13:55, wall=17:09 IST
=> Training   63.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.028 Prec@1=74.181 Prec@5=91.351 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=17:09 IST
=> Training   63.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.028 Prec@1=74.181 Prec@5=91.351 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=17:10 IST
=> Training   63.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.029 Prec@1=74.156 Prec@5=91.341 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=17:10 IST
=> Training   67.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.029 Prec@1=74.156 Prec@5=91.341 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=17:10 IST
=> Training   67.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.029 Prec@1=74.156 Prec@5=91.341 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=17:11 IST
=> Training   67.96% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.031 Prec@1=74.108 Prec@5=91.324 rate=1.79 Hz, eta=0:07:26, total=0:15:47, wall=17:11 IST
=> Training   71.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.031 Prec@1=74.108 Prec@5=91.324 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=17:11 IST
=> Training   71.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.383 Loss=1.031 Prec@1=74.108 Prec@5=91.324 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=17:12 IST
=> Training   71.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.032 Prec@1=74.079 Prec@5=91.310 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=17:12 IST
=> Training   75.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.032 Prec@1=74.079 Prec@5=91.310 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=17:12 IST
=> Training   75.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.032 Prec@1=74.079 Prec@5=91.310 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=17:13 IST
=> Training   75.95% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.033 Prec@1=74.044 Prec@5=91.299 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=17:13 IST
=> Training   79.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.033 Prec@1=74.044 Prec@5=91.299 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=17:13 IST
=> Training   79.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.033 Prec@1=74.044 Prec@5=91.299 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=17:14 IST
=> Training   79.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.034 Prec@1=74.000 Prec@5=91.286 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=17:14 IST
=> Training   83.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.034 Prec@1=74.000 Prec@5=91.286 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=17:14 IST
=> Training   83.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.034 Prec@1=74.000 Prec@5=91.286 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=17:15 IST
=> Training   83.94% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.036 Prec@1=73.977 Prec@5=91.269 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=17:15 IST
=> Training   87.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.036 Prec@1=73.977 Prec@5=91.269 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=17:15 IST
=> Training   87.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.036 Prec@1=73.977 Prec@5=91.269 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=17:16 IST
=> Training   87.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.037 Prec@1=73.958 Prec@5=91.253 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=17:16 IST
=> Training   91.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.037 Prec@1=73.958 Prec@5=91.253 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=17:16 IST
=> Training   91.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.559 DataTime=0.383 Loss=1.037 Prec@1=73.958 Prec@5=91.253 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=17:17 IST
=> Training   91.93% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.038 Prec@1=73.921 Prec@5=91.237 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=17:17 IST
=> Training   95.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.038 Prec@1=73.921 Prec@5=91.237 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=17:17 IST
=> Training   95.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.038 Prec@1=73.921 Prec@5=91.237 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=17:18 IST
=> Training   95.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.039 Prec@1=73.909 Prec@5=91.231 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=17:18 IST
=> Training   99.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.039 Prec@1=73.909 Prec@5=91.231 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=17:18 IST
=> Training   99.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.039 Prec@1=73.909 Prec@5=91.231 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=17:18 IST
=> Training   99.92% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.039 Prec@1=73.908 Prec@5=91.229 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=17:18 IST
=> Training   100.00% of 1x2503...Epoch=103/150 LR=0.0232 Time=0.560 DataTime=0.384 Loss=1.039 Prec@1=73.908 Prec@5=91.229 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=17:18 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST
=> Validation 0.00% of 1x98...Epoch=103/150 LR=0.0232 Time=7.056 Loss=0.767 Prec@1=79.688 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=17:18 IST
=> Validation 1.02% of 1x98...Epoch=103/150 LR=0.0232 Time=7.056 Loss=0.767 Prec@1=79.688 Prec@5=94.531 rate=6157.52 Hz, eta=0:00:00, total=0:00:00, wall=17:18 IST
** Validation 1.02% of 1x98...Epoch=103/150 LR=0.0232 Time=7.056 Loss=0.767 Prec@1=79.688 Prec@5=94.531 rate=6157.52 Hz, eta=0:00:00, total=0:00:00, wall=17:19 IST
** Validation 1.02% of 1x98...Epoch=103/150 LR=0.0232 Time=0.631 Loss=1.306 Prec@1=68.344 Prec@5=88.442 rate=6157.52 Hz, eta=0:00:00, total=0:00:00, wall=17:19 IST
** Validation 100.00% of 1x98...Epoch=103/150 LR=0.0232 Time=0.631 Loss=1.306 Prec@1=68.344 Prec@5=88.442 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=17:19 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:19 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:19 IST
=> Training   0.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=5.016 DataTime=4.511 Loss=0.911 Prec@1=78.125 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=17:19 IST
=> Training   0.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=5.016 DataTime=4.511 Loss=0.911 Prec@1=78.125 Prec@5=92.188 rate=5886.78 Hz, eta=0:00:00, total=0:00:00, wall=17:19 IST
=> Training   0.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=5.016 DataTime=4.511 Loss=0.911 Prec@1=78.125 Prec@5=92.188 rate=5886.78 Hz, eta=0:00:00, total=0:00:00, wall=17:20 IST
=> Training   0.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.581 DataTime=0.409 Loss=0.996 Prec@1=74.919 Prec@5=91.859 rate=5886.78 Hz, eta=0:00:00, total=0:00:00, wall=17:20 IST
=> Training   4.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.581 DataTime=0.409 Loss=0.996 Prec@1=74.919 Prec@5=91.859 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=17:20 IST
=> Training   4.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.581 DataTime=0.409 Loss=0.996 Prec@1=74.919 Prec@5=91.859 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=17:21 IST
=> Training   4.04% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.572 DataTime=0.403 Loss=0.995 Prec@1=74.881 Prec@5=91.836 rate=1.88 Hz, eta=0:21:17, total=0:00:53, wall=17:21 IST
=> Training   8.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.572 DataTime=0.403 Loss=0.995 Prec@1=74.881 Prec@5=91.836 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=17:21 IST
=> Training   8.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.572 DataTime=0.403 Loss=0.995 Prec@1=74.881 Prec@5=91.836 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=17:22 IST
=> Training   8.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.399 Loss=1.000 Prec@1=74.811 Prec@5=91.794 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=17:22 IST
=> Training   12.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.399 Loss=1.000 Prec@1=74.811 Prec@5=91.794 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=17:22 IST
=> Training   12.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.399 Loss=1.000 Prec@1=74.811 Prec@5=91.794 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=17:23 IST
=> Training   12.03% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.000 Prec@1=74.846 Prec@5=91.810 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=17:23 IST
=> Training   16.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.000 Prec@1=74.846 Prec@5=91.810 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=17:23 IST
=> Training   16.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.000 Prec@1=74.846 Prec@5=91.810 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=17:24 IST
=> Training   16.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.002 Prec@1=74.760 Prec@5=91.788 rate=1.80 Hz, eta=0:19:25, total=0:03:42, wall=17:24 IST
=> Training   20.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.002 Prec@1=74.760 Prec@5=91.788 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=17:24 IST
=> Training   20.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.002 Prec@1=74.760 Prec@5=91.788 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=17:25 IST
=> Training   20.02% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.001 Prec@1=74.804 Prec@5=91.781 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=17:25 IST
=> Training   24.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.001 Prec@1=74.804 Prec@5=91.781 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=17:25 IST
=> Training   24.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.567 DataTime=0.398 Loss=1.001 Prec@1=74.804 Prec@5=91.781 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=17:26 IST
=> Training   24.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.566 DataTime=0.397 Loss=1.004 Prec@1=74.753 Prec@5=91.737 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=17:26 IST
=> Training   28.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.566 DataTime=0.397 Loss=1.004 Prec@1=74.753 Prec@5=91.737 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=17:26 IST
=> Training   28.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.566 DataTime=0.397 Loss=1.004 Prec@1=74.753 Prec@5=91.737 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=17:26 IST
=> Training   28.01% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.393 Loss=1.006 Prec@1=74.747 Prec@5=91.710 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=17:26 IST
=> Training   32.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.393 Loss=1.006 Prec@1=74.747 Prec@5=91.710 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:26 IST
=> Training   32.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.393 Loss=1.006 Prec@1=74.747 Prec@5=91.710 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:27 IST
=> Training   32.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.009 Prec@1=74.674 Prec@5=91.660 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=17:27 IST
=> Training   36.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.009 Prec@1=74.674 Prec@5=91.660 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=17:27 IST
=> Training   36.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.009 Prec@1=74.674 Prec@5=91.660 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=17:28 IST
=> Training   36.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.011 Prec@1=74.617 Prec@5=91.645 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=17:28 IST
=> Training   39.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.011 Prec@1=74.617 Prec@5=91.645 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:28 IST
=> Training   39.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.011 Prec@1=74.617 Prec@5=91.645 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:29 IST
=> Training   39.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.581 Prec@5=91.635 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=17:29 IST
=> Training   43.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.581 Prec@5=91.635 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:29 IST
=> Training   43.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.012 Prec@1=74.581 Prec@5=91.635 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:30 IST
=> Training   43.99% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.013 Prec@1=74.545 Prec@5=91.611 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=17:30 IST
=> Training   47.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.013 Prec@1=74.545 Prec@5=91.611 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:30 IST
=> Training   47.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.013 Prec@1=74.545 Prec@5=91.611 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:31 IST
=> Training   47.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.392 Loss=1.014 Prec@1=74.505 Prec@5=91.602 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:31 IST
=> Training   51.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.392 Loss=1.014 Prec@1=74.505 Prec@5=91.602 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=17:31 IST
=> Training   51.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.392 Loss=1.014 Prec@1=74.505 Prec@5=91.602 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=17:32 IST
=> Training   51.98% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.393 Loss=1.015 Prec@1=74.469 Prec@5=91.596 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=17:32 IST
=> Training   55.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.393 Loss=1.015 Prec@1=74.469 Prec@5=91.596 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=17:32 IST
=> Training   55.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.565 DataTime=0.393 Loss=1.015 Prec@1=74.469 Prec@5=91.596 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=17:33 IST
=> Training   55.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.450 Prec@5=91.590 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=17:33 IST
=> Training   59.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.450 Prec@5=91.590 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=17:33 IST
=> Training   59.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.450 Prec@5=91.590 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=17:34 IST
=> Training   59.97% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.399 Prec@5=91.575 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=17:34 IST
=> Training   63.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.399 Prec@5=91.575 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:34 IST
=> Training   63.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.399 Prec@5=91.575 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:35 IST
=> Training   63.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.374 Prec@5=91.572 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:35 IST
=> Training   67.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.374 Prec@5=91.572 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=17:35 IST
=> Training   67.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.374 Prec@5=91.572 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=17:36 IST
=> Training   67.96% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.390 Loss=1.019 Prec@1=74.339 Prec@5=91.550 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=17:36 IST
=> Training   71.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.390 Loss=1.019 Prec@1=74.339 Prec@5=91.550 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=17:36 IST
=> Training   71.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.390 Loss=1.019 Prec@1=74.339 Prec@5=91.550 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=17:37 IST
=> Training   71.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.020 Prec@1=74.312 Prec@5=91.527 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=17:37 IST
=> Training   75.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.020 Prec@1=74.312 Prec@5=91.527 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=17:37 IST
=> Training   75.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.020 Prec@1=74.312 Prec@5=91.527 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=17:38 IST
=> Training   75.95% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.021 Prec@1=74.290 Prec@5=91.512 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=17:38 IST
=> Training   79.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.021 Prec@1=74.290 Prec@5=91.512 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=17:38 IST
=> Training   79.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.021 Prec@1=74.290 Prec@5=91.512 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=17:39 IST
=> Training   79.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.022 Prec@1=74.256 Prec@5=91.494 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=17:39 IST
=> Training   83.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.022 Prec@1=74.256 Prec@5=91.494 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=17:39 IST
=> Training   83.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.022 Prec@1=74.256 Prec@5=91.494 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=17:40 IST
=> Training   83.94% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.024 Prec@1=74.226 Prec@5=91.474 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=17:40 IST
=> Training   87.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.024 Prec@1=74.226 Prec@5=91.474 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=17:40 IST
=> Training   87.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.391 Loss=1.024 Prec@1=74.226 Prec@5=91.474 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=17:41 IST
=> Training   87.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.025 Prec@1=74.208 Prec@5=91.458 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=17:41 IST
=> Training   91.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.025 Prec@1=74.208 Prec@5=91.458 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=17:41 IST
=> Training   91.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.025 Prec@1=74.208 Prec@5=91.458 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=17:41 IST
=> Training   91.93% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.026 Prec@1=74.184 Prec@5=91.438 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=17:41 IST
=> Training   95.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.026 Prec@1=74.184 Prec@5=91.438 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=17:41 IST
=> Training   95.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.564 DataTime=0.392 Loss=1.026 Prec@1=74.184 Prec@5=91.438 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=17:42 IST
=> Training   95.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.392 Loss=1.027 Prec@1=74.168 Prec@5=91.421 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=17:42 IST
=> Training   99.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.392 Loss=1.027 Prec@1=74.168 Prec@5=91.421 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=17:42 IST
=> Training   99.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.392 Loss=1.027 Prec@1=74.168 Prec@5=91.421 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=17:42 IST
=> Training   99.92% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.027 Prec@1=74.165 Prec@5=91.420 rate=1.78 Hz, eta=0:00:01, total=0:23:24, wall=17:42 IST
=> Training   100.00% of 1x2503...Epoch=104/150 LR=0.0223 Time=0.563 DataTime=0.391 Loss=1.027 Prec@1=74.165 Prec@5=91.420 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=17:42 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST
=> Validation 0.00% of 1x98...Epoch=104/150 LR=0.0223 Time=8.169 Loss=0.858 Prec@1=78.906 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=17:43 IST
=> Validation 1.02% of 1x98...Epoch=104/150 LR=0.0223 Time=8.169 Loss=0.858 Prec@1=78.906 Prec@5=93.359 rate=6832.75 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST
** Validation 1.02% of 1x98...Epoch=104/150 LR=0.0223 Time=8.169 Loss=0.858 Prec@1=78.906 Prec@5=93.359 rate=6832.75 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST
** Validation 1.02% of 1x98...Epoch=104/150 LR=0.0223 Time=0.640 Loss=1.298 Prec@1=68.756 Prec@5=88.604 rate=6832.75 Hz, eta=0:00:00, total=0:00:00, wall=17:43 IST
** Validation 100.00% of 1x98...Epoch=104/150 LR=0.0223 Time=0.640 Loss=1.298 Prec@1=68.756 Prec@5=88.604 rate=1.80 Hz, eta=0:00:00, total=0:00:54, wall=17:43 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST
=> Training   0.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=5.018 DataTime=4.770 Loss=0.958 Prec@1=75.781 Prec@5=91.797 rate=0 Hz, eta=?, total=0:00:00, wall=17:44 IST
=> Training   0.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=5.018 DataTime=4.770 Loss=0.958 Prec@1=75.781 Prec@5=91.797 rate=8766.78 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST
=> Training   0.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=5.018 DataTime=4.770 Loss=0.958 Prec@1=75.781 Prec@5=91.797 rate=8766.78 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST
=> Training   0.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.598 DataTime=0.432 Loss=0.986 Prec@1=75.104 Prec@5=91.843 rate=8766.78 Hz, eta=0:00:00, total=0:00:00, wall=17:44 IST
=> Training   4.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.598 DataTime=0.432 Loss=0.986 Prec@1=75.104 Prec@5=91.843 rate=1.83 Hz, eta=0:21:56, total=0:00:55, wall=17:44 IST
=> Training   4.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.598 DataTime=0.432 Loss=0.986 Prec@1=75.104 Prec@5=91.843 rate=1.83 Hz, eta=0:21:56, total=0:00:55, wall=17:45 IST
=> Training   4.04% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.576 DataTime=0.409 Loss=0.993 Prec@1=74.892 Prec@5=91.744 rate=1.83 Hz, eta=0:21:56, total=0:00:55, wall=17:45 IST
=> Training   8.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.576 DataTime=0.409 Loss=0.993 Prec@1=74.892 Prec@5=91.744 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=17:45 IST
=> Training   8.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.576 DataTime=0.409 Loss=0.993 Prec@1=74.892 Prec@5=91.744 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=17:46 IST
=> Training   8.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.399 Loss=0.996 Prec@1=74.790 Prec@5=91.739 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=17:46 IST
=> Training   12.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.399 Loss=0.996 Prec@1=74.790 Prec@5=91.739 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=17:46 IST
=> Training   12.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.399 Loss=0.996 Prec@1=74.790 Prec@5=91.739 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=17:47 IST
=> Training   12.03% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.396 Loss=0.998 Prec@1=74.806 Prec@5=91.755 rate=1.82 Hz, eta=0:20:11, total=0:02:45, wall=17:47 IST
=> Training   16.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.396 Loss=0.998 Prec@1=74.806 Prec@5=91.755 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=17:47 IST
=> Training   16.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.396 Loss=0.998 Prec@1=74.806 Prec@5=91.755 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=17:48 IST
=> Training   16.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.398 Loss=0.998 Prec@1=74.802 Prec@5=91.727 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=17:48 IST
=> Training   20.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.398 Loss=0.998 Prec@1=74.802 Prec@5=91.727 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=17:48 IST
=> Training   20.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.398 Loss=0.998 Prec@1=74.802 Prec@5=91.727 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=17:49 IST
=> Training   20.02% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.393 Loss=1.000 Prec@1=74.782 Prec@5=91.716 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=17:49 IST
=> Training   24.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.393 Loss=1.000 Prec@1=74.782 Prec@5=91.716 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=17:49 IST
=> Training   24.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.393 Loss=1.000 Prec@1=74.782 Prec@5=91.716 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=17:50 IST
=> Training   24.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.395 Loss=1.001 Prec@1=74.724 Prec@5=91.708 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=17:50 IST
=> Training   28.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.395 Loss=1.001 Prec@1=74.724 Prec@5=91.708 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=17:50 IST
=> Training   28.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.395 Loss=1.001 Prec@1=74.724 Prec@5=91.708 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=17:51 IST
=> Training   28.01% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.394 Loss=1.003 Prec@1=74.710 Prec@5=91.700 rate=1.79 Hz, eta=0:16:49, total=0:06:32, wall=17:51 IST
=> Training   32.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.394 Loss=1.003 Prec@1=74.710 Prec@5=91.700 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=17:51 IST
=> Training   32.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.567 DataTime=0.394 Loss=1.003 Prec@1=74.710 Prec@5=91.700 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=17:52 IST
=> Training   32.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.393 Loss=1.004 Prec@1=74.692 Prec@5=91.681 rate=1.78 Hz, eta=0:15:53, total=0:07:28, wall=17:52 IST
=> Training   36.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.393 Loss=1.004 Prec@1=74.692 Prec@5=91.681 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=17:52 IST
=> Training   36.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.393 Loss=1.004 Prec@1=74.692 Prec@5=91.681 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=17:53 IST
=> Training   36.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.005 Prec@1=74.694 Prec@5=91.672 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=17:53 IST
=> Training   39.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.005 Prec@1=74.694 Prec@5=91.672 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:53 IST
=> Training   39.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.005 Prec@1=74.694 Prec@5=91.672 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:54 IST
=> Training   39.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.394 Loss=1.006 Prec@1=74.676 Prec@5=91.671 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=17:54 IST
=> Training   43.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.394 Loss=1.006 Prec@1=74.676 Prec@5=91.671 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=17:54 IST
=> Training   43.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.566 DataTime=0.394 Loss=1.006 Prec@1=74.676 Prec@5=91.671 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=17:55 IST
=> Training   43.99% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.008 Prec@1=74.634 Prec@5=91.642 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=17:55 IST
=> Training   47.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.008 Prec@1=74.634 Prec@5=91.642 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:55 IST
=> Training   47.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.008 Prec@1=74.634 Prec@5=91.642 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:56 IST
=> Training   47.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.601 Prec@5=91.615 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=17:56 IST
=> Training   51.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.601 Prec@5=91.615 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=17:56 IST
=> Training   51.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.601 Prec@5=91.615 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=17:57 IST
=> Training   51.98% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.589 Prec@5=91.592 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=17:57 IST
=> Training   55.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.589 Prec@5=91.592 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=17:57 IST
=> Training   55.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.010 Prec@1=74.589 Prec@5=91.592 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=17:58 IST
=> Training   55.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.011 Prec@1=74.569 Prec@5=91.586 rate=1.79 Hz, eta=0:10:16, total=0:13:04, wall=17:58 IST
=> Training   59.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.011 Prec@1=74.569 Prec@5=91.586 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=17:58 IST
=> Training   59.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.391 Loss=1.011 Prec@1=74.569 Prec@5=91.586 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=17:59 IST
=> Training   59.97% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.393 Loss=1.012 Prec@1=74.534 Prec@5=91.573 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=17:59 IST
=> Training   63.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.393 Loss=1.012 Prec@1=74.534 Prec@5=91.573 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:59 IST
=> Training   63.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.393 Loss=1.012 Prec@1=74.534 Prec@5=91.573 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:59 IST
=> Training   63.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.014 Prec@1=74.503 Prec@5=91.564 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=17:59 IST
=> Training   67.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.014 Prec@1=74.503 Prec@5=91.564 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=17:59 IST
=> Training   67.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.014 Prec@1=74.503 Prec@5=91.564 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=18:00 IST
=> Training   67.96% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.492 Prec@5=91.547 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=18:00 IST
=> Training   71.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.492 Prec@5=91.547 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=18:00 IST
=> Training   71.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.492 Prec@5=91.547 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=18:01 IST
=> Training   71.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.473 Prec@5=91.538 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=18:01 IST
=> Training   75.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.473 Prec@5=91.538 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=18:01 IST
=> Training   75.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.392 Loss=1.015 Prec@1=74.473 Prec@5=91.538 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=18:02 IST
=> Training   75.95% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.015 Prec@1=74.477 Prec@5=91.533 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=18:02 IST
=> Training   79.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.015 Prec@1=74.477 Prec@5=91.533 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:02 IST
=> Training   79.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.565 DataTime=0.392 Loss=1.015 Prec@1=74.477 Prec@5=91.533 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:03 IST
=> Training   79.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.458 Prec@5=91.511 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:03 IST
=> Training   83.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.458 Prec@5=91.511 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=18:03 IST
=> Training   83.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.458 Prec@5=91.511 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=18:04 IST
=> Training   83.94% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.435 Prec@5=91.505 rate=1.78 Hz, eta=0:03:45, total=0:19:39, wall=18:04 IST
=> Training   87.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.435 Prec@5=91.505 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:04 IST
=> Training   87.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.017 Prec@1=74.435 Prec@5=91.505 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:05 IST
=> Training   87.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.018 Prec@1=74.405 Prec@5=91.486 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:05 IST
=> Training   91.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.018 Prec@1=74.405 Prec@5=91.486 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=18:05 IST
=> Training   91.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.391 Loss=1.018 Prec@1=74.405 Prec@5=91.486 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=18:06 IST
=> Training   91.93% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.390 Loss=1.019 Prec@1=74.386 Prec@5=91.476 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=18:06 IST
=> Training   95.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.390 Loss=1.019 Prec@1=74.386 Prec@5=91.476 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=18:06 IST
=> Training   95.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.564 DataTime=0.390 Loss=1.019 Prec@1=74.386 Prec@5=91.476 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=18:07 IST
=> Training   95.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.390 Loss=1.020 Prec@1=74.370 Prec@5=91.466 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=18:07 IST
=> Training   99.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.390 Loss=1.020 Prec@1=74.370 Prec@5=91.466 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=18:07 IST
=> Training   99.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.390 Loss=1.020 Prec@1=74.370 Prec@5=91.466 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=18:07 IST
=> Training   99.92% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.390 Loss=1.020 Prec@1=74.370 Prec@5=91.466 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=18:07 IST
=> Training   100.00% of 1x2503...Epoch=105/150 LR=0.0215 Time=0.563 DataTime=0.390 Loss=1.020 Prec@1=74.370 Prec@5=91.466 rate=1.78 Hz, eta=0:00:00, total=0:23:24, wall=18:07 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:07 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:07 IST
=> Validation 0.00% of 1x98...Epoch=105/150 LR=0.0215 Time=7.027 Loss=0.890 Prec@1=76.758 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=18:07 IST
=> Validation 1.02% of 1x98...Epoch=105/150 LR=0.0215 Time=7.027 Loss=0.890 Prec@1=76.758 Prec@5=94.141 rate=6007.02 Hz, eta=0:00:00, total=0:00:00, wall=18:07 IST
** Validation 1.02% of 1x98...Epoch=105/150 LR=0.0215 Time=7.027 Loss=0.890 Prec@1=76.758 Prec@5=94.141 rate=6007.02 Hz, eta=0:00:00, total=0:00:00, wall=18:08 IST
** Validation 1.02% of 1x98...Epoch=105/150 LR=0.0215 Time=0.630 Loss=1.326 Prec@1=68.298 Prec@5=88.282 rate=6007.02 Hz, eta=0:00:00, total=0:00:00, wall=18:08 IST
** Validation 100.00% of 1x98...Epoch=105/150 LR=0.0215 Time=0.630 Loss=1.326 Prec@1=68.298 Prec@5=88.282 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=18:08 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:08 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:08 IST
=> Training   0.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=4.662 DataTime=4.300 Loss=1.137 Prec@1=70.703 Prec@5=89.453 rate=0 Hz, eta=?, total=0:00:00, wall=18:08 IST
=> Training   0.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=4.662 DataTime=4.300 Loss=1.137 Prec@1=70.703 Prec@5=89.453 rate=6482.77 Hz, eta=0:00:00, total=0:00:00, wall=18:08 IST
=> Training   0.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=4.662 DataTime=4.300 Loss=1.137 Prec@1=70.703 Prec@5=89.453 rate=6482.77 Hz, eta=0:00:00, total=0:00:00, wall=18:09 IST
=> Training   0.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.596 DataTime=0.427 Loss=0.983 Prec@1=75.300 Prec@5=92.023 rate=6482.77 Hz, eta=0:00:00, total=0:00:00, wall=18:09 IST
=> Training   4.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.596 DataTime=0.427 Loss=0.983 Prec@1=75.300 Prec@5=92.023 rate=1.82 Hz, eta=0:22:00, total=0:00:55, wall=18:09 IST
=> Training   4.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.596 DataTime=0.427 Loss=0.983 Prec@1=75.300 Prec@5=92.023 rate=1.82 Hz, eta=0:22:00, total=0:00:55, wall=18:10 IST
=> Training   4.04% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.575 DataTime=0.400 Loss=0.980 Prec@1=75.318 Prec@5=92.077 rate=1.82 Hz, eta=0:22:00, total=0:00:55, wall=18:10 IST
=> Training   8.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.575 DataTime=0.400 Loss=0.980 Prec@1=75.318 Prec@5=92.077 rate=1.81 Hz, eta=0:21:09, total=0:01:50, wall=18:10 IST
=> Training   8.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.575 DataTime=0.400 Loss=0.980 Prec@1=75.318 Prec@5=92.077 rate=1.81 Hz, eta=0:21:09, total=0:01:50, wall=18:11 IST
=> Training   8.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.574 DataTime=0.397 Loss=0.981 Prec@1=75.260 Prec@5=92.082 rate=1.81 Hz, eta=0:21:09, total=0:01:50, wall=18:11 IST
=> Training   12.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.574 DataTime=0.397 Loss=0.981 Prec@1=75.260 Prec@5=92.082 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=18:11 IST
=> Training   12.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.574 DataTime=0.397 Loss=0.981 Prec@1=75.260 Prec@5=92.082 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=18:12 IST
=> Training   12.03% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.568 DataTime=0.393 Loss=0.982 Prec@1=75.203 Prec@5=92.080 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=18:12 IST
=> Training   16.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.568 DataTime=0.393 Loss=0.982 Prec@1=75.203 Prec@5=92.080 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=18:12 IST
=> Training   16.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.568 DataTime=0.393 Loss=0.982 Prec@1=75.203 Prec@5=92.080 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=18:13 IST
=> Training   16.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.174 Prec@5=91.993 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=18:13 IST
=> Training   20.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.174 Prec@5=91.993 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=18:13 IST
=> Training   20.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.174 Prec@5=91.993 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=18:14 IST
=> Training   20.02% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.120 Prec@5=91.956 rate=1.80 Hz, eta=0:18:34, total=0:04:38, wall=18:14 IST
=> Training   24.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.120 Prec@5=91.956 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=18:14 IST
=> Training   24.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.985 Prec@1=75.120 Prec@5=91.956 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=18:15 IST
=> Training   24.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.393 Loss=0.989 Prec@1=75.023 Prec@5=91.903 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=18:15 IST
=> Training   28.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.393 Loss=0.989 Prec@1=75.023 Prec@5=91.903 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=18:15 IST
=> Training   28.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.393 Loss=0.989 Prec@1=75.023 Prec@5=91.903 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=18:16 IST
=> Training   28.01% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.990 Prec@1=75.006 Prec@5=91.895 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=18:16 IST
=> Training   32.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.990 Prec@1=75.006 Prec@5=91.895 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=18:16 IST
=> Training   32.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.392 Loss=0.990 Prec@1=75.006 Prec@5=91.895 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=18:17 IST
=> Training   32.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.391 Loss=0.993 Prec@1=74.957 Prec@5=91.849 rate=1.79 Hz, eta=0:15:53, total=0:07:28, wall=18:17 IST
=> Training   36.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.391 Loss=0.993 Prec@1=74.957 Prec@5=91.849 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=18:17 IST
=> Training   36.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.566 DataTime=0.391 Loss=0.993 Prec@1=74.957 Prec@5=91.849 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=18:17 IST
=> Training   36.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.992 Prec@1=74.945 Prec@5=91.863 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=18:17 IST
=> Training   39.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.992 Prec@1=74.945 Prec@5=91.863 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=18:17 IST
=> Training   39.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.992 Prec@1=74.945 Prec@5=91.863 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=18:18 IST
=> Training   39.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.565 DataTime=0.391 Loss=0.995 Prec@1=74.910 Prec@5=91.843 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=18:18 IST
=> Training   43.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.565 DataTime=0.391 Loss=0.995 Prec@1=74.910 Prec@5=91.843 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:18 IST
=> Training   43.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.565 DataTime=0.391 Loss=0.995 Prec@1=74.910 Prec@5=91.843 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:19 IST
=> Training   43.99% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.390 Loss=0.997 Prec@1=74.874 Prec@5=91.802 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:19 IST
=> Training   47.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.390 Loss=0.997 Prec@1=74.874 Prec@5=91.802 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:19 IST
=> Training   47.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.390 Loss=0.997 Prec@1=74.874 Prec@5=91.802 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:20 IST
=> Training   47.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.998 Prec@1=74.873 Prec@5=91.774 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=18:20 IST
=> Training   51.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.998 Prec@1=74.873 Prec@5=91.774 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=18:20 IST
=> Training   51.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.998 Prec@1=74.873 Prec@5=91.774 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=18:21 IST
=> Training   51.98% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.999 Prec@1=74.836 Prec@5=91.759 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=18:21 IST
=> Training   55.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.999 Prec@1=74.836 Prec@5=91.759 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=18:21 IST
=> Training   55.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=0.999 Prec@1=74.836 Prec@5=91.759 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=18:22 IST
=> Training   55.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.001 Prec@1=74.796 Prec@5=91.738 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=18:22 IST
=> Training   59.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.001 Prec@1=74.796 Prec@5=91.738 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=18:22 IST
=> Training   59.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.001 Prec@1=74.796 Prec@5=91.738 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=18:23 IST
=> Training   59.97% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.003 Prec@1=74.758 Prec@5=91.724 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=18:23 IST
=> Training   63.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.003 Prec@1=74.758 Prec@5=91.724 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=18:23 IST
=> Training   63.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.003 Prec@1=74.758 Prec@5=91.724 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=18:24 IST
=> Training   63.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.003 Prec@1=74.741 Prec@5=91.715 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=18:24 IST
=> Training   67.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.003 Prec@1=74.741 Prec@5=91.715 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=18:24 IST
=> Training   67.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.564 DataTime=0.389 Loss=1.003 Prec@1=74.741 Prec@5=91.715 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=18:25 IST
=> Training   67.96% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.004 Prec@1=74.714 Prec@5=91.693 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=18:25 IST
=> Training   71.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.004 Prec@1=74.714 Prec@5=91.693 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=18:25 IST
=> Training   71.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.389 Loss=1.004 Prec@1=74.714 Prec@5=91.693 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=18:26 IST
=> Training   71.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.388 Loss=1.005 Prec@1=74.705 Prec@5=91.693 rate=1.78 Hz, eta=0:06:33, total=0:16:50, wall=18:26 IST
=> Training   75.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.388 Loss=1.005 Prec@1=74.705 Prec@5=91.693 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=18:26 IST
=> Training   75.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.388 Loss=1.005 Prec@1=74.705 Prec@5=91.693 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=18:27 IST
=> Training   75.95% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.680 Prec@5=91.685 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=18:27 IST
=> Training   79.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.680 Prec@5=91.685 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=18:27 IST
=> Training   79.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.680 Prec@5=91.685 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=18:28 IST
=> Training   79.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.657 Prec@5=91.674 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=18:28 IST
=> Training   83.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.657 Prec@5=91.674 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=18:28 IST
=> Training   83.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.006 Prec@1=74.657 Prec@5=91.674 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=18:29 IST
=> Training   83.94% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.007 Prec@1=74.636 Prec@5=91.660 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=18:29 IST
=> Training   87.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.007 Prec@1=74.636 Prec@5=91.660 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=18:29 IST
=> Training   87.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.007 Prec@1=74.636 Prec@5=91.660 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=18:30 IST
=> Training   87.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.008 Prec@1=74.611 Prec@5=91.641 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=18:30 IST
=> Training   91.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.008 Prec@1=74.611 Prec@5=91.641 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:30 IST
=> Training   91.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.563 DataTime=0.387 Loss=1.008 Prec@1=74.611 Prec@5=91.641 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:31 IST
=> Training   91.93% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.009 Prec@1=74.588 Prec@5=91.625 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=18:31 IST
=> Training   95.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.009 Prec@1=74.588 Prec@5=91.625 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:31 IST
=> Training   95.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.009 Prec@1=74.588 Prec@5=91.625 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:31 IST
=> Training   95.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.010 Prec@1=74.566 Prec@5=91.613 rate=1.78 Hz, eta=0:00:57, total=0:22:25, wall=18:31 IST
=> Training   99.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.010 Prec@1=74.566 Prec@5=91.613 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:31 IST
=> Training   99.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.010 Prec@1=74.566 Prec@5=91.613 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:31 IST
=> Training   99.92% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.010 Prec@1=74.567 Prec@5=91.613 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=18:31 IST
=> Training   100.00% of 1x2503...Epoch=106/150 LR=0.0206 Time=0.562 DataTime=0.386 Loss=1.010 Prec@1=74.567 Prec@5=91.613 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=18:31 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:32 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:32 IST
=> Validation 0.00% of 1x98...Epoch=106/150 LR=0.0206 Time=7.267 Loss=0.810 Prec@1=77.734 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=18:32 IST
=> Validation 1.02% of 1x98...Epoch=106/150 LR=0.0206 Time=7.267 Loss=0.810 Prec@1=77.734 Prec@5=94.336 rate=7920.86 Hz, eta=0:00:00, total=0:00:00, wall=18:32 IST
** Validation 1.02% of 1x98...Epoch=106/150 LR=0.0206 Time=7.267 Loss=0.810 Prec@1=77.734 Prec@5=94.336 rate=7920.86 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** Validation 1.02% of 1x98...Epoch=106/150 LR=0.0206 Time=0.634 Loss=1.283 Prec@1=69.044 Prec@5=88.724 rate=7920.86 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
** Validation 100.00% of 1x98...Epoch=106/150 LR=0.0206 Time=0.634 Loss=1.283 Prec@1=69.044 Prec@5=88.724 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=18:33 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> Training   0.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=5.093 DataTime=4.845 Loss=1.017 Prec@1=72.852 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=18:33 IST
=> Training   0.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=5.093 DataTime=4.845 Loss=1.017 Prec@1=72.852 Prec@5=92.383 rate=8261.46 Hz, eta=0:00:00, total=0:00:00, wall=18:33 IST
=> Training   0.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=5.093 DataTime=4.845 Loss=1.017 Prec@1=72.852 Prec@5=92.383 rate=8261.46 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
=> Training   0.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.597 DataTime=0.433 Loss=0.974 Prec@1=75.625 Prec@5=91.973 rate=8261.46 Hz, eta=0:00:00, total=0:00:00, wall=18:34 IST
=> Training   4.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.597 DataTime=0.433 Loss=0.974 Prec@1=75.625 Prec@5=91.973 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=18:34 IST
=> Training   4.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.597 DataTime=0.433 Loss=0.974 Prec@1=75.625 Prec@5=91.973 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=18:34 IST
=> Training   4.04% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.580 DataTime=0.409 Loss=0.967 Prec@1=75.674 Prec@5=92.098 rate=1.83 Hz, eta=0:21:53, total=0:00:55, wall=18:34 IST
=> Training   8.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.580 DataTime=0.409 Loss=0.967 Prec@1=75.674 Prec@5=92.098 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=18:34 IST
=> Training   8.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.580 DataTime=0.409 Loss=0.967 Prec@1=75.674 Prec@5=92.098 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=18:35 IST
=> Training   8.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.573 DataTime=0.401 Loss=0.973 Prec@1=75.484 Prec@5=92.032 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=18:35 IST
=> Training   12.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.573 DataTime=0.401 Loss=0.973 Prec@1=75.484 Prec@5=92.032 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=18:35 IST
=> Training   12.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.573 DataTime=0.401 Loss=0.973 Prec@1=75.484 Prec@5=92.032 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=18:36 IST
=> Training   12.03% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.399 Loss=0.976 Prec@1=75.386 Prec@5=92.009 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=18:36 IST
=> Training   16.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.399 Loss=0.976 Prec@1=75.386 Prec@5=92.009 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=18:36 IST
=> Training   16.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.399 Loss=0.976 Prec@1=75.386 Prec@5=92.009 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=18:37 IST
=> Training   16.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.398 Loss=0.976 Prec@1=75.365 Prec@5=92.038 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=18:37 IST
=> Training   20.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.398 Loss=0.976 Prec@1=75.365 Prec@5=92.038 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=18:37 IST
=> Training   20.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.570 DataTime=0.398 Loss=0.976 Prec@1=75.365 Prec@5=92.038 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=18:38 IST
=> Training   20.02% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.568 DataTime=0.395 Loss=0.978 Prec@1=75.326 Prec@5=92.013 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=18:38 IST
=> Training   24.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.568 DataTime=0.395 Loss=0.978 Prec@1=75.326 Prec@5=92.013 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:38 IST
=> Training   24.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.568 DataTime=0.395 Loss=0.978 Prec@1=75.326 Prec@5=92.013 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:39 IST
=> Training   24.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.978 Prec@1=75.299 Prec@5=92.022 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=18:39 IST
=> Training   28.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.978 Prec@1=75.299 Prec@5=92.022 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=18:39 IST
=> Training   28.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.978 Prec@1=75.299 Prec@5=92.022 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=18:40 IST
=> Training   28.01% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.981 Prec@1=75.233 Prec@5=91.980 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=18:40 IST
=> Training   32.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.981 Prec@1=75.233 Prec@5=91.980 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=18:40 IST
=> Training   32.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.981 Prec@1=75.233 Prec@5=91.980 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=18:41 IST
=> Training   32.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.983 Prec@1=75.190 Prec@5=91.951 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=18:41 IST
=> Training   36.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.983 Prec@1=75.190 Prec@5=91.951 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:41 IST
=> Training   36.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.393 Loss=0.983 Prec@1=75.190 Prec@5=91.951 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:42 IST
=> Training   36.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.392 Loss=0.985 Prec@1=75.123 Prec@5=91.924 rate=1.78 Hz, eta=0:14:59, total=0:08:25, wall=18:42 IST
=> Training   39.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.392 Loss=0.985 Prec@1=75.123 Prec@5=91.924 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=18:42 IST
=> Training   39.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.567 DataTime=0.392 Loss=0.985 Prec@1=75.123 Prec@5=91.924 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=18:43 IST
=> Training   39.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.986 Prec@1=75.089 Prec@5=91.921 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=18:43 IST
=> Training   43.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.986 Prec@1=75.089 Prec@5=91.921 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:43 IST
=> Training   43.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.986 Prec@1=75.089 Prec@5=91.921 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:44 IST
=> Training   43.99% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.987 Prec@1=75.079 Prec@5=91.901 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=18:44 IST
=> Training   47.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.987 Prec@1=75.079 Prec@5=91.901 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=18:44 IST
=> Training   47.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.566 DataTime=0.392 Loss=0.987 Prec@1=75.079 Prec@5=91.901 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=18:45 IST
=> Training   47.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.988 Prec@1=75.047 Prec@5=91.876 rate=1.78 Hz, eta=0:12:11, total=0:11:15, wall=18:45 IST
=> Training   51.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.988 Prec@1=75.047 Prec@5=91.876 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=18:45 IST
=> Training   51.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.988 Prec@1=75.047 Prec@5=91.876 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=18:46 IST
=> Training   51.98% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.990 Prec@1=75.015 Prec@5=91.860 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=18:46 IST
=> Training   55.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.990 Prec@1=75.015 Prec@5=91.860 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=18:46 IST
=> Training   55.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.990 Prec@1=75.015 Prec@5=91.860 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=18:47 IST
=> Training   55.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.994 Prec@5=91.852 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=18:47 IST
=> Training   59.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.994 Prec@5=91.852 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=18:47 IST
=> Training   59.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.994 Prec@5=91.852 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=18:48 IST
=> Training   59.97% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.987 Prec@5=91.842 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=18:48 IST
=> Training   63.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.987 Prec@5=91.842 rate=1.78 Hz, eta=0:08:26, total=0:14:59, wall=18:48 IST
=> Training   63.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.991 Prec@1=74.987 Prec@5=91.842 rate=1.78 Hz, eta=0:08:26, total=0:14:59, wall=18:49 IST
=> Training   63.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.965 Prec@5=91.822 rate=1.78 Hz, eta=0:08:26, total=0:14:59, wall=18:49 IST
=> Training   67.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.965 Prec@5=91.822 rate=1.78 Hz, eta=0:07:30, total=0:15:56, wall=18:49 IST
=> Training   67.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.965 Prec@5=91.822 rate=1.78 Hz, eta=0:07:30, total=0:15:56, wall=18:49 IST
=> Training   67.96% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.950 Prec@5=91.817 rate=1.78 Hz, eta=0:07:30, total=0:15:56, wall=18:49 IST
=> Training   71.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.950 Prec@5=91.817 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=18:49 IST
=> Training   71.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.993 Prec@1=74.950 Prec@5=91.817 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=18:50 IST
=> Training   71.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.994 Prec@1=74.922 Prec@5=91.801 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=18:50 IST
=> Training   75.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.994 Prec@1=74.922 Prec@5=91.801 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=18:50 IST
=> Training   75.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.565 DataTime=0.391 Loss=0.994 Prec@1=74.922 Prec@5=91.801 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=18:51 IST
=> Training   75.95% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.901 Prec@5=91.789 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=18:51 IST
=> Training   79.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.901 Prec@5=91.789 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:51 IST
=> Training   79.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.901 Prec@5=91.789 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:52 IST
=> Training   79.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.882 Prec@5=91.785 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=18:52 IST
=> Training   83.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.882 Prec@5=91.785 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=18:52 IST
=> Training   83.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.995 Prec@1=74.882 Prec@5=91.785 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=18:53 IST
=> Training   83.94% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.996 Prec@1=74.865 Prec@5=91.779 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=18:53 IST
=> Training   87.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.996 Prec@1=74.865 Prec@5=91.779 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:53 IST
=> Training   87.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.996 Prec@1=74.865 Prec@5=91.779 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:54 IST
=> Training   87.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.998 Prec@1=74.831 Prec@5=91.760 rate=1.78 Hz, eta=0:02:49, total=0:20:36, wall=18:54 IST
=> Training   91.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.998 Prec@1=74.831 Prec@5=91.760 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=18:54 IST
=> Training   91.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.390 Loss=0.998 Prec@1=74.831 Prec@5=91.760 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=18:55 IST
=> Training   91.93% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.998 Prec@1=74.828 Prec@5=91.748 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=18:55 IST
=> Training   95.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.998 Prec@1=74.828 Prec@5=91.748 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=18:55 IST
=> Training   95.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.998 Prec@1=74.828 Prec@5=91.748 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=18:56 IST
=> Training   95.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.999 Prec@1=74.805 Prec@5=91.730 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=18:56 IST
=> Training   99.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.999 Prec@1=74.805 Prec@5=91.730 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=18:56 IST
=> Training   99.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.999 Prec@1=74.805 Prec@5=91.730 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=18:56 IST
=> Training   99.92% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.999 Prec@1=74.804 Prec@5=91.732 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=18:56 IST
=> Training   100.00% of 1x2503...Epoch=107/150 LR=0.0198 Time=0.564 DataTime=0.391 Loss=0.999 Prec@1=74.804 Prec@5=91.732 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=18:56 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:56 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=18:56 IST
=> Validation 0.00% of 1x98...Epoch=107/150 LR=0.0198 Time=7.106 Loss=0.834 Prec@1=80.664 Prec@5=93.164 rate=0 Hz, eta=?, total=0:00:00, wall=18:56 IST
=> Validation 1.02% of 1x98...Epoch=107/150 LR=0.0198 Time=7.106 Loss=0.834 Prec@1=80.664 Prec@5=93.164 rate=6451.24 Hz, eta=0:00:00, total=0:00:00, wall=18:56 IST
** Validation 1.02% of 1x98...Epoch=107/150 LR=0.0198 Time=7.106 Loss=0.834 Prec@1=80.664 Prec@5=93.164 rate=6451.24 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST
** Validation 1.02% of 1x98...Epoch=107/150 LR=0.0198 Time=0.638 Loss=1.272 Prec@1=69.268 Prec@5=88.874 rate=6451.24 Hz, eta=0:00:00, total=0:00:00, wall=18:57 IST
** Validation 100.00% of 1x98...Epoch=107/150 LR=0.0198 Time=0.638 Loss=1.272 Prec@1=69.268 Prec@5=88.874 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=18:57 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST
=> Training   0.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=4.728 DataTime=4.349 Loss=0.811 Prec@1=82.812 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=18:57 IST
=> Training   0.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=4.728 DataTime=4.349 Loss=0.811 Prec@1=82.812 Prec@5=93.555 rate=1950.40 Hz, eta=0:00:01, total=0:00:00, wall=18:57 IST
=> Training   0.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=4.728 DataTime=4.349 Loss=0.811 Prec@1=82.812 Prec@5=93.555 rate=1950.40 Hz, eta=0:00:01, total=0:00:00, wall=18:58 IST
=> Training   0.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.584 DataTime=0.411 Loss=0.948 Prec@1=75.862 Prec@5=92.427 rate=1950.40 Hz, eta=0:00:01, total=0:00:00, wall=18:58 IST
=> Training   4.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.584 DataTime=0.411 Loss=0.948 Prec@1=75.862 Prec@5=92.427 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=18:58 IST
=> Training   4.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.584 DataTime=0.411 Loss=0.948 Prec@1=75.862 Prec@5=92.427 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=18:59 IST
=> Training   4.04% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.578 DataTime=0.409 Loss=0.953 Prec@1=75.807 Prec@5=92.332 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=18:59 IST
=> Training   8.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.578 DataTime=0.409 Loss=0.953 Prec@1=75.807 Prec@5=92.332 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=18:59 IST
=> Training   8.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.578 DataTime=0.409 Loss=0.953 Prec@1=75.807 Prec@5=92.332 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=19:00 IST
=> Training   8.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.572 DataTime=0.400 Loss=0.958 Prec@1=75.730 Prec@5=92.265 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=19:00 IST
=> Training   12.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.572 DataTime=0.400 Loss=0.958 Prec@1=75.730 Prec@5=92.265 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=19:00 IST
=> Training   12.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.572 DataTime=0.400 Loss=0.958 Prec@1=75.730 Prec@5=92.265 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=19:01 IST
=> Training   12.03% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.570 DataTime=0.394 Loss=0.959 Prec@1=75.698 Prec@5=92.266 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=19:01 IST
=> Training   16.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.570 DataTime=0.394 Loss=0.959 Prec@1=75.698 Prec@5=92.266 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=19:01 IST
=> Training   16.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.570 DataTime=0.394 Loss=0.959 Prec@1=75.698 Prec@5=92.266 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=19:02 IST
=> Training   16.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.962 Prec@1=75.664 Prec@5=92.226 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=19:02 IST
=> Training   20.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.962 Prec@1=75.664 Prec@5=92.226 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=19:02 IST
=> Training   20.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.962 Prec@1=75.664 Prec@5=92.226 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=19:03 IST
=> Training   20.02% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.964 Prec@1=75.625 Prec@5=92.169 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=19:03 IST
=> Training   24.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.964 Prec@1=75.625 Prec@5=92.169 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=19:03 IST
=> Training   24.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.392 Loss=0.964 Prec@1=75.625 Prec@5=92.169 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=19:04 IST
=> Training   24.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.391 Loss=0.967 Prec@1=75.560 Prec@5=92.147 rate=1.78 Hz, eta=0:17:48, total=0:05:37, wall=19:04 IST
=> Training   28.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.391 Loss=0.967 Prec@1=75.560 Prec@5=92.147 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=19:04 IST
=> Training   28.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.569 DataTime=0.391 Loss=0.967 Prec@1=75.560 Prec@5=92.147 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=19:05 IST
=> Training   28.01% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.568 DataTime=0.390 Loss=0.970 Prec@1=75.487 Prec@5=92.095 rate=1.78 Hz, eta=0:16:53, total=0:06:34, wall=19:05 IST
=> Training   32.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.568 DataTime=0.390 Loss=0.970 Prec@1=75.487 Prec@5=92.095 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=19:05 IST
=> Training   32.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.568 DataTime=0.390 Loss=0.970 Prec@1=75.487 Prec@5=92.095 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=19:06 IST
=> Training   32.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.973 Prec@1=75.433 Prec@5=92.062 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=19:06 IST
=> Training   36.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.973 Prec@1=75.433 Prec@5=92.062 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=19:06 IST
=> Training   36.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.973 Prec@1=75.433 Prec@5=92.062 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=19:07 IST
=> Training   36.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.974 Prec@1=75.396 Prec@5=92.055 rate=1.78 Hz, eta=0:14:57, total=0:08:24, wall=19:07 IST
=> Training   39.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.974 Prec@1=75.396 Prec@5=92.055 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:07 IST
=> Training   39.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.566 DataTime=0.388 Loss=0.974 Prec@1=75.396 Prec@5=92.055 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:07 IST
=> Training   39.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.387 Loss=0.976 Prec@1=75.351 Prec@5=92.032 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=19:07 IST
=> Training   43.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.387 Loss=0.976 Prec@1=75.351 Prec@5=92.032 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=19:07 IST
=> Training   43.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.387 Loss=0.976 Prec@1=75.351 Prec@5=92.032 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=19:08 IST
=> Training   43.99% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.388 Loss=0.978 Prec@1=75.308 Prec@5=92.005 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=19:08 IST
=> Training   47.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.388 Loss=0.978 Prec@1=75.308 Prec@5=92.005 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:08 IST
=> Training   47.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.388 Loss=0.978 Prec@1=75.308 Prec@5=92.005 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:09 IST
=> Training   47.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.389 Loss=0.979 Prec@1=75.287 Prec@5=91.993 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=19:09 IST
=> Training   51.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.389 Loss=0.979 Prec@1=75.287 Prec@5=91.993 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=19:09 IST
=> Training   51.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.389 Loss=0.979 Prec@1=75.287 Prec@5=91.993 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=19:10 IST
=> Training   51.98% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.979 Prec@1=75.279 Prec@5=91.984 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=19:10 IST
=> Training   55.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.979 Prec@1=75.279 Prec@5=91.984 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=19:10 IST
=> Training   55.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.979 Prec@1=75.279 Prec@5=91.984 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=19:11 IST
=> Training   55.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.981 Prec@1=75.240 Prec@5=91.968 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=19:11 IST
=> Training   59.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.981 Prec@1=75.240 Prec@5=91.968 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=19:11 IST
=> Training   59.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.390 Loss=0.981 Prec@1=75.240 Prec@5=91.968 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=19:12 IST
=> Training   59.97% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.982 Prec@1=75.229 Prec@5=91.952 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=19:12 IST
=> Training   63.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.982 Prec@1=75.229 Prec@5=91.952 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=19:12 IST
=> Training   63.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.982 Prec@1=75.229 Prec@5=91.952 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=19:13 IST
=> Training   63.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.983 Prec@1=75.203 Prec@5=91.939 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=19:13 IST
=> Training   67.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.983 Prec@1=75.203 Prec@5=91.939 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=19:13 IST
=> Training   67.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.983 Prec@1=75.203 Prec@5=91.939 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=19:14 IST
=> Training   67.96% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.984 Prec@1=75.199 Prec@5=91.934 rate=1.78 Hz, eta=0:07:29, total=0:15:54, wall=19:14 IST
=> Training   71.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.984 Prec@1=75.199 Prec@5=91.934 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=19:14 IST
=> Training   71.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.984 Prec@1=75.199 Prec@5=91.934 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=19:15 IST
=> Training   71.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.985 Prec@1=75.165 Prec@5=91.919 rate=1.78 Hz, eta=0:06:34, total=0:16:50, wall=19:15 IST
=> Training   75.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.985 Prec@1=75.165 Prec@5=91.919 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=19:15 IST
=> Training   75.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.985 Prec@1=75.165 Prec@5=91.919 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=19:16 IST
=> Training   75.95% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.986 Prec@1=75.133 Prec@5=91.895 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=19:16 IST
=> Training   79.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.986 Prec@1=75.133 Prec@5=91.895 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=19:16 IST
=> Training   79.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.986 Prec@1=75.133 Prec@5=91.895 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=19:17 IST
=> Training   79.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.988 Prec@1=75.095 Prec@5=91.873 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=19:17 IST
=> Training   83.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.988 Prec@1=75.095 Prec@5=91.873 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=19:17 IST
=> Training   83.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.988 Prec@1=75.095 Prec@5=91.873 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=19:18 IST
=> Training   83.94% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.989 Prec@1=75.057 Prec@5=91.863 rate=1.78 Hz, eta=0:03:46, total=0:19:41, wall=19:18 IST
=> Training   87.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.989 Prec@1=75.057 Prec@5=91.863 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=19:18 IST
=> Training   87.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.989 Prec@1=75.057 Prec@5=91.863 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=19:19 IST
=> Training   87.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.990 Prec@1=75.033 Prec@5=91.857 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=19:19 IST
=> Training   91.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.990 Prec@1=75.033 Prec@5=91.857 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=19:19 IST
=> Training   91.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.565 DataTime=0.391 Loss=0.990 Prec@1=75.033 Prec@5=91.857 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=19:20 IST
=> Training   91.93% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.991 Prec@1=75.018 Prec@5=91.847 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=19:20 IST
=> Training   95.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.991 Prec@1=75.018 Prec@5=91.847 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=19:20 IST
=> Training   95.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.391 Loss=0.991 Prec@1=75.018 Prec@5=91.847 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=19:21 IST
=> Training   95.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.992 Prec@1=74.995 Prec@5=91.830 rate=1.78 Hz, eta=0:00:57, total=0:22:30, wall=19:21 IST
=> Training   99.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.992 Prec@1=74.995 Prec@5=91.830 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=19:21 IST
=> Training   99.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.992 Prec@1=74.995 Prec@5=91.830 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=19:21 IST
=> Training   99.92% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.992 Prec@1=74.994 Prec@5=91.829 rate=1.78 Hz, eta=0:00:01, total=0:23:25, wall=19:21 IST
=> Training   100.00% of 1x2503...Epoch=108/150 LR=0.0189 Time=0.564 DataTime=0.390 Loss=0.992 Prec@1=74.994 Prec@5=91.829 rate=1.78 Hz, eta=0:00:00, total=0:23:25, wall=19:21 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:21 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:21 IST
=> Validation 0.00% of 1x98...Epoch=108/150 LR=0.0189 Time=7.187 Loss=0.854 Prec@1=77.539 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=19:21 IST
=> Validation 1.02% of 1x98...Epoch=108/150 LR=0.0189 Time=7.187 Loss=0.854 Prec@1=77.539 Prec@5=94.141 rate=3601.85 Hz, eta=0:00:00, total=0:00:00, wall=19:21 IST
** Validation 1.02% of 1x98...Epoch=108/150 LR=0.0189 Time=7.187 Loss=0.854 Prec@1=77.539 Prec@5=94.141 rate=3601.85 Hz, eta=0:00:00, total=0:00:00, wall=19:22 IST
** Validation 1.02% of 1x98...Epoch=108/150 LR=0.0189 Time=0.638 Loss=1.281 Prec@1=68.818 Prec@5=88.850 rate=3601.85 Hz, eta=0:00:00, total=0:00:00, wall=19:22 IST
** Validation 100.00% of 1x98...Epoch=108/150 LR=0.0189 Time=0.638 Loss=1.281 Prec@1=68.818 Prec@5=88.850 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=19:22 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:22 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:22 IST
=> Training   0.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=4.905 DataTime=4.558 Loss=1.002 Prec@1=76.172 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=19:22 IST
=> Training   0.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=4.905 DataTime=4.558 Loss=1.002 Prec@1=76.172 Prec@5=91.992 rate=3200.79 Hz, eta=0:00:00, total=0:00:00, wall=19:22 IST
=> Training   0.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=4.905 DataTime=4.558 Loss=1.002 Prec@1=76.172 Prec@5=91.992 rate=3200.79 Hz, eta=0:00:00, total=0:00:00, wall=19:23 IST
=> Training   0.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.597 DataTime=0.426 Loss=0.945 Prec@1=76.052 Prec@5=92.414 rate=3200.79 Hz, eta=0:00:00, total=0:00:00, wall=19:23 IST
=> Training   4.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.597 DataTime=0.426 Loss=0.945 Prec@1=76.052 Prec@5=92.414 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=19:23 IST
=> Training   4.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.597 DataTime=0.426 Loss=0.945 Prec@1=76.052 Prec@5=92.414 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=19:24 IST
=> Training   4.04% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.579 DataTime=0.403 Loss=0.950 Prec@1=76.025 Prec@5=92.395 rate=1.82 Hz, eta=0:21:58, total=0:00:55, wall=19:24 IST
=> Training   8.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.579 DataTime=0.403 Loss=0.950 Prec@1=76.025 Prec@5=92.395 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=19:24 IST
=> Training   8.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.579 DataTime=0.403 Loss=0.950 Prec@1=76.025 Prec@5=92.395 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=19:25 IST
=> Training   8.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.390 Loss=0.955 Prec@1=75.853 Prec@5=92.346 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=19:25 IST
=> Training   12.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.390 Loss=0.955 Prec@1=75.853 Prec@5=92.346 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=19:25 IST
=> Training   12.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.390 Loss=0.955 Prec@1=75.853 Prec@5=92.346 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=19:25 IST
=> Training   12.03% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.566 DataTime=0.388 Loss=0.957 Prec@1=75.810 Prec@5=92.334 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=19:25 IST
=> Training   16.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.566 DataTime=0.388 Loss=0.957 Prec@1=75.810 Prec@5=92.334 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:25 IST
=> Training   16.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.566 DataTime=0.388 Loss=0.957 Prec@1=75.810 Prec@5=92.334 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:26 IST
=> Training   16.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.389 Loss=0.955 Prec@1=75.889 Prec@5=92.338 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=19:26 IST
=> Training   20.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.389 Loss=0.955 Prec@1=75.889 Prec@5=92.338 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=19:26 IST
=> Training   20.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.568 DataTime=0.389 Loss=0.955 Prec@1=75.889 Prec@5=92.338 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=19:27 IST
=> Training   20.02% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.386 Loss=0.957 Prec@1=75.885 Prec@5=92.318 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=19:27 IST
=> Training   24.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.386 Loss=0.957 Prec@1=75.885 Prec@5=92.318 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=19:27 IST
=> Training   24.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.386 Loss=0.957 Prec@1=75.885 Prec@5=92.318 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=19:28 IST
=> Training   24.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.387 Loss=0.959 Prec@1=75.843 Prec@5=92.274 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=19:28 IST
=> Training   28.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.387 Loss=0.959 Prec@1=75.843 Prec@5=92.274 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=19:28 IST
=> Training   28.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.564 DataTime=0.387 Loss=0.959 Prec@1=75.843 Prec@5=92.274 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=19:29 IST
=> Training   28.01% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.562 DataTime=0.385 Loss=0.960 Prec@1=75.820 Prec@5=92.255 rate=1.79 Hz, eta=0:16:43, total=0:06:30, wall=19:29 IST
=> Training   32.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.562 DataTime=0.385 Loss=0.960 Prec@1=75.820 Prec@5=92.255 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=19:29 IST
=> Training   32.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.562 DataTime=0.385 Loss=0.960 Prec@1=75.820 Prec@5=92.255 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=19:30 IST
=> Training   32.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.563 DataTime=0.386 Loss=0.962 Prec@1=75.736 Prec@5=92.218 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=19:30 IST
=> Training   36.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.563 DataTime=0.386 Loss=0.962 Prec@1=75.736 Prec@5=92.218 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=19:30 IST
=> Training   36.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.563 DataTime=0.386 Loss=0.962 Prec@1=75.736 Prec@5=92.218 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=19:31 IST
=> Training   36.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.963 Prec@1=75.719 Prec@5=92.207 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=19:31 IST
=> Training   39.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.963 Prec@1=75.719 Prec@5=92.207 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:31 IST
=> Training   39.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.963 Prec@1=75.719 Prec@5=92.207 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:32 IST
=> Training   39.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.965 Prec@1=75.656 Prec@5=92.168 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:32 IST
=> Training   43.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.965 Prec@1=75.656 Prec@5=92.168 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=19:32 IST
=> Training   43.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.385 Loss=0.965 Prec@1=75.656 Prec@5=92.168 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=19:33 IST
=> Training   43.99% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.967 Prec@1=75.630 Prec@5=92.155 rate=1.80 Hz, eta=0:12:59, total=0:10:12, wall=19:33 IST
=> Training   47.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.967 Prec@1=75.630 Prec@5=92.155 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:33 IST
=> Training   47.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.967 Prec@1=75.630 Prec@5=92.155 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:34 IST
=> Training   47.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.968 Prec@1=75.590 Prec@5=92.133 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:34 IST
=> Training   51.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.968 Prec@1=75.590 Prec@5=92.133 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:34 IST
=> Training   51.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.968 Prec@1=75.590 Prec@5=92.133 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:35 IST
=> Training   51.98% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.969 Prec@1=75.559 Prec@5=92.113 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:35 IST
=> Training   55.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.969 Prec@1=75.559 Prec@5=92.113 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=19:35 IST
=> Training   55.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.969 Prec@1=75.559 Prec@5=92.113 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=19:36 IST
=> Training   55.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.970 Prec@1=75.524 Prec@5=92.099 rate=1.80 Hz, eta=0:10:13, total=0:13:00, wall=19:36 IST
=> Training   59.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.970 Prec@1=75.524 Prec@5=92.099 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:36 IST
=> Training   59.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.970 Prec@1=75.524 Prec@5=92.099 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:37 IST
=> Training   59.97% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.971 Prec@1=75.500 Prec@5=92.098 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=19:37 IST
=> Training   63.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.971 Prec@1=75.500 Prec@5=92.098 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=19:37 IST
=> Training   63.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.971 Prec@1=75.500 Prec@5=92.098 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=19:38 IST
=> Training   63.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.972 Prec@1=75.464 Prec@5=92.082 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=19:38 IST
=> Training   67.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.972 Prec@1=75.464 Prec@5=92.082 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=19:38 IST
=> Training   67.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.559 DataTime=0.386 Loss=0.972 Prec@1=75.464 Prec@5=92.082 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=19:38 IST
=> Training   67.96% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.974 Prec@1=75.431 Prec@5=92.059 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=19:38 IST
=> Training   71.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.974 Prec@1=75.431 Prec@5=92.059 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=19:38 IST
=> Training   71.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.974 Prec@1=75.431 Prec@5=92.059 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=19:39 IST
=> Training   71.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.976 Prec@1=75.388 Prec@5=92.041 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=19:39 IST
=> Training   75.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.976 Prec@1=75.388 Prec@5=92.041 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:39 IST
=> Training   75.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.387 Loss=0.976 Prec@1=75.388 Prec@5=92.041 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:40 IST
=> Training   75.95% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.977 Prec@1=75.362 Prec@5=92.023 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=19:40 IST
=> Training   79.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.977 Prec@1=75.362 Prec@5=92.023 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:40 IST
=> Training   79.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.977 Prec@1=75.362 Prec@5=92.023 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:41 IST
=> Training   79.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.978 Prec@1=75.346 Prec@5=92.012 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=19:41 IST
=> Training   83.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.978 Prec@1=75.346 Prec@5=92.012 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:41 IST
=> Training   83.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.561 DataTime=0.386 Loss=0.978 Prec@1=75.346 Prec@5=92.012 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:42 IST
=> Training   83.94% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.979 Prec@1=75.320 Prec@5=91.991 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=19:42 IST
=> Training   87.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.979 Prec@1=75.320 Prec@5=91.991 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:42 IST
=> Training   87.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.979 Prec@1=75.320 Prec@5=91.991 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:43 IST
=> Training   87.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.980 Prec@1=75.303 Prec@5=91.974 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=19:43 IST
=> Training   91.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.980 Prec@1=75.303 Prec@5=91.974 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:43 IST
=> Training   91.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.980 Prec@1=75.303 Prec@5=91.974 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:44 IST
=> Training   91.93% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.278 Prec@5=91.963 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=19:44 IST
=> Training   95.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.278 Prec@5=91.963 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:44 IST
=> Training   95.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.278 Prec@5=91.963 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:45 IST
=> Training   95.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.263 Prec@5=91.961 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=19:45 IST
=> Training   99.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.263 Prec@5=91.961 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:45 IST
=> Training   99.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.263 Prec@5=91.961 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:45 IST
=> Training   99.92% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.262 Prec@5=91.960 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=19:45 IST
=> Training   100.00% of 1x2503...Epoch=109/150 LR=0.0181 Time=0.560 DataTime=0.386 Loss=0.981 Prec@1=75.262 Prec@5=91.960 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=19:45 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:45 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=19:45 IST
=> Validation 0.00% of 1x98...Epoch=109/150 LR=0.0181 Time=7.157 Loss=0.788 Prec@1=81.055 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=19:45 IST
=> Validation 1.02% of 1x98...Epoch=109/150 LR=0.0181 Time=7.157 Loss=0.788 Prec@1=81.055 Prec@5=93.945 rate=5710.30 Hz, eta=0:00:00, total=0:00:00, wall=19:45 IST
** Validation 1.02% of 1x98...Epoch=109/150 LR=0.0181 Time=7.157 Loss=0.788 Prec@1=81.055 Prec@5=93.945 rate=5710.30 Hz, eta=0:00:00, total=0:00:00, wall=19:46 IST
** Validation 1.02% of 1x98...Epoch=109/150 LR=0.0181 Time=0.639 Loss=1.265 Prec@1=69.308 Prec@5=89.014 rate=5710.30 Hz, eta=0:00:00, total=0:00:00, wall=19:46 IST
** Validation 100.00% of 1x98...Epoch=109/150 LR=0.0181 Time=0.639 Loss=1.265 Prec@1=69.308 Prec@5=89.014 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=19:46 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:46 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=19:46 IST
=> Training   0.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=4.818 DataTime=4.228 Loss=0.953 Prec@1=77.148 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=19:46 IST
=> Training   0.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=4.818 DataTime=4.228 Loss=0.953 Prec@1=77.148 Prec@5=91.992 rate=1957.14 Hz, eta=0:00:01, total=0:00:00, wall=19:46 IST
=> Training   0.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=4.818 DataTime=4.228 Loss=0.953 Prec@1=77.148 Prec@5=91.992 rate=1957.14 Hz, eta=0:00:01, total=0:00:00, wall=19:47 IST
=> Training   0.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.582 DataTime=0.412 Loss=0.952 Prec@1=76.000 Prec@5=92.323 rate=1957.14 Hz, eta=0:00:01, total=0:00:00, wall=19:47 IST
=> Training   4.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.582 DataTime=0.412 Loss=0.952 Prec@1=76.000 Prec@5=92.323 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=19:47 IST
=> Training   4.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.582 DataTime=0.412 Loss=0.952 Prec@1=76.000 Prec@5=92.323 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=19:48 IST
=> Training   4.04% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.571 DataTime=0.404 Loss=0.946 Prec@1=76.129 Prec@5=92.408 rate=1.87 Hz, eta=0:21:26, total=0:00:54, wall=19:48 IST
=> Training   8.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.571 DataTime=0.404 Loss=0.946 Prec@1=76.129 Prec@5=92.408 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=19:48 IST
=> Training   8.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.571 DataTime=0.404 Loss=0.946 Prec@1=76.129 Prec@5=92.408 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=19:49 IST
=> Training   8.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.566 DataTime=0.396 Loss=0.945 Prec@1=76.152 Prec@5=92.443 rate=1.83 Hz, eta=0:21:00, total=0:01:50, wall=19:49 IST
=> Training   12.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.566 DataTime=0.396 Loss=0.945 Prec@1=76.152 Prec@5=92.443 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=19:49 IST
=> Training   12.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.566 DataTime=0.396 Loss=0.945 Prec@1=76.152 Prec@5=92.443 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=19:50 IST
=> Training   12.03% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.391 Loss=0.945 Prec@1=76.146 Prec@5=92.450 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=19:50 IST
=> Training   16.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.391 Loss=0.945 Prec@1=76.146 Prec@5=92.450 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=19:50 IST
=> Training   16.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.391 Loss=0.945 Prec@1=76.146 Prec@5=92.450 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=19:51 IST
=> Training   16.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.945 Prec@1=76.140 Prec@5=92.419 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=19:51 IST
=> Training   20.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.945 Prec@1=76.140 Prec@5=92.419 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=19:51 IST
=> Training   20.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.945 Prec@1=76.140 Prec@5=92.419 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=19:52 IST
=> Training   20.02% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.078 Prec@5=92.373 rate=1.81 Hz, eta=0:18:27, total=0:04:37, wall=19:52 IST
=> Training   24.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.078 Prec@5=92.373 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:52 IST
=> Training   24.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.078 Prec@5=92.373 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:53 IST
=> Training   24.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=76.003 Prec@5=92.331 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=19:53 IST
=> Training   28.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=76.003 Prec@5=92.331 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:53 IST
=> Training   28.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=76.003 Prec@5=92.331 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:54 IST
=> Training   28.01% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.954 Prec@1=75.942 Prec@5=92.284 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=19:54 IST
=> Training   32.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.954 Prec@1=75.942 Prec@5=92.284 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=19:54 IST
=> Training   32.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.954 Prec@1=75.942 Prec@5=92.284 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=19:55 IST
=> Training   32.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.956 Prec@1=75.887 Prec@5=92.258 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=19:55 IST
=> Training   36.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.956 Prec@1=75.887 Prec@5=92.258 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=19:55 IST
=> Training   36.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.956 Prec@1=75.887 Prec@5=92.258 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=19:55 IST
=> Training   36.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.957 Prec@1=75.847 Prec@5=92.239 rate=1.80 Hz, eta=0:14:50, total=0:08:21, wall=19:55 IST
=> Training   39.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.957 Prec@1=75.847 Prec@5=92.239 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:55 IST
=> Training   39.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.957 Prec@1=75.847 Prec@5=92.239 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:56 IST
=> Training   39.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.958 Prec@1=75.811 Prec@5=92.231 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=19:56 IST
=> Training   43.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.958 Prec@1=75.811 Prec@5=92.231 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:56 IST
=> Training   43.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.563 DataTime=0.391 Loss=0.958 Prec@1=75.811 Prec@5=92.231 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:57 IST
=> Training   43.99% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.960 Prec@1=75.768 Prec@5=92.208 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=19:57 IST
=> Training   47.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.960 Prec@1=75.768 Prec@5=92.208 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:57 IST
=> Training   47.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.960 Prec@1=75.768 Prec@5=92.208 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:58 IST
=> Training   47.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.962 Prec@1=75.733 Prec@5=92.193 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=19:58 IST
=> Training   51.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.962 Prec@1=75.733 Prec@5=92.193 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:58 IST
=> Training   51.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.962 Prec@1=75.733 Prec@5=92.193 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:59 IST
=> Training   51.98% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.963 Prec@1=75.686 Prec@5=92.173 rate=1.80 Hz, eta=0:11:09, total=0:12:04, wall=19:59 IST
=> Training   55.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.963 Prec@1=75.686 Prec@5=92.173 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=19:59 IST
=> Training   55.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.389 Loss=0.963 Prec@1=75.686 Prec@5=92.173 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:00 IST
=> Training   55.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.965 Prec@1=75.650 Prec@5=92.155 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:00 IST
=> Training   59.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.965 Prec@1=75.650 Prec@5=92.155 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=20:00 IST
=> Training   59.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.390 Loss=0.965 Prec@1=75.650 Prec@5=92.155 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=20:01 IST
=> Training   59.97% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.629 Prec@5=92.141 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=20:01 IST
=> Training   63.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.629 Prec@5=92.141 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=20:01 IST
=> Training   63.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.629 Prec@5=92.141 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=20:02 IST
=> Training   63.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.621 Prec@5=92.132 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=20:02 IST
=> Training   67.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.621 Prec@5=92.132 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=20:02 IST
=> Training   67.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.966 Prec@1=75.621 Prec@5=92.132 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=20:03 IST
=> Training   67.96% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.967 Prec@1=75.598 Prec@5=92.113 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=20:03 IST
=> Training   71.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.967 Prec@1=75.598 Prec@5=92.113 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=20:03 IST
=> Training   71.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.967 Prec@1=75.598 Prec@5=92.113 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=20:04 IST
=> Training   71.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.968 Prec@1=75.572 Prec@5=92.103 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=20:04 IST
=> Training   75.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.968 Prec@1=75.572 Prec@5=92.103 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=20:04 IST
=> Training   75.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.968 Prec@1=75.572 Prec@5=92.103 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=20:05 IST
=> Training   75.95% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.969 Prec@1=75.559 Prec@5=92.099 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=20:05 IST
=> Training   79.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.969 Prec@1=75.559 Prec@5=92.099 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=20:05 IST
=> Training   79.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.390 Loss=0.969 Prec@1=75.559 Prec@5=92.099 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=20:06 IST
=> Training   79.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.969 Prec@1=75.549 Prec@5=92.092 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=20:06 IST
=> Training   83.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.969 Prec@1=75.549 Prec@5=92.092 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=20:06 IST
=> Training   83.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.969 Prec@1=75.549 Prec@5=92.092 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=20:07 IST
=> Training   83.94% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.971 Prec@1=75.522 Prec@5=92.070 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=20:07 IST
=> Training   87.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.971 Prec@1=75.522 Prec@5=92.070 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=20:07 IST
=> Training   87.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.562 DataTime=0.389 Loss=0.971 Prec@1=75.522 Prec@5=92.070 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=20:08 IST
=> Training   87.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.971 Prec@1=75.502 Prec@5=92.065 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=20:08 IST
=> Training   91.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.971 Prec@1=75.502 Prec@5=92.065 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=20:08 IST
=> Training   91.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.971 Prec@1=75.502 Prec@5=92.065 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=20:09 IST
=> Training   91.93% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.972 Prec@1=75.482 Prec@5=92.055 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=20:09 IST
=> Training   95.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.972 Prec@1=75.482 Prec@5=92.055 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=20:09 IST
=> Training   95.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.972 Prec@1=75.482 Prec@5=92.055 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=20:09 IST
=> Training   95.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.973 Prec@1=75.469 Prec@5=92.047 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=20:09 IST
=> Training   99.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.973 Prec@1=75.469 Prec@5=92.047 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=20:09 IST
=> Training   99.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.561 DataTime=0.388 Loss=0.973 Prec@1=75.469 Prec@5=92.047 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=20:09 IST
=> Training   99.92% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.560 DataTime=0.388 Loss=0.973 Prec@1=75.469 Prec@5=92.046 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=20:09 IST
=> Training   100.00% of 1x2503...Epoch=110/150 LR=0.0173 Time=0.560 DataTime=0.388 Loss=0.973 Prec@1=75.469 Prec@5=92.046 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=20:09 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST
=> Validation 0.00% of 1x98...Epoch=110/150 LR=0.0173 Time=6.977 Loss=0.796 Prec@1=78.906 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=20:10 IST
=> Validation 1.02% of 1x98...Epoch=110/150 LR=0.0173 Time=6.977 Loss=0.796 Prec@1=78.906 Prec@5=94.922 rate=890.55 Hz, eta=0:00:00, total=0:00:00, wall=20:10 IST
** Validation 1.02% of 1x98...Epoch=110/150 LR=0.0173 Time=6.977 Loss=0.796 Prec@1=78.906 Prec@5=94.922 rate=890.55 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST
** Validation 1.02% of 1x98...Epoch=110/150 LR=0.0173 Time=0.637 Loss=1.262 Prec@1=69.528 Prec@5=89.064 rate=890.55 Hz, eta=0:00:00, total=0:00:00, wall=20:11 IST
** Validation 100.00% of 1x98...Epoch=110/150 LR=0.0173 Time=0.637 Loss=1.262 Prec@1=69.528 Prec@5=89.064 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=20:11 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST
=> Training   0.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=5.179 DataTime=4.626 Loss=0.880 Prec@1=77.930 Prec@5=92.383 rate=0 Hz, eta=?, total=0:00:00, wall=20:11 IST
=> Training   0.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=5.179 DataTime=4.626 Loss=0.880 Prec@1=77.930 Prec@5=92.383 rate=1796.65 Hz, eta=0:00:01, total=0:00:00, wall=20:11 IST
=> Training   0.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=5.179 DataTime=4.626 Loss=0.880 Prec@1=77.930 Prec@5=92.383 rate=1796.65 Hz, eta=0:00:01, total=0:00:00, wall=20:12 IST
=> Training   0.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.586 DataTime=0.420 Loss=0.937 Prec@1=76.379 Prec@5=92.605 rate=1796.65 Hz, eta=0:00:01, total=0:00:00, wall=20:12 IST
=> Training   4.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.586 DataTime=0.420 Loss=0.937 Prec@1=76.379 Prec@5=92.605 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=20:12 IST
=> Training   4.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.586 DataTime=0.420 Loss=0.937 Prec@1=76.379 Prec@5=92.605 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=20:12 IST
=> Training   4.04% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.574 DataTime=0.406 Loss=0.932 Prec@1=76.324 Prec@5=92.620 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=20:12 IST
=> Training   8.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.574 DataTime=0.406 Loss=0.932 Prec@1=76.324 Prec@5=92.620 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=20:12 IST
=> Training   8.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.574 DataTime=0.406 Loss=0.932 Prec@1=76.324 Prec@5=92.620 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=20:13 IST
=> Training   8.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.568 DataTime=0.399 Loss=0.934 Prec@1=76.290 Prec@5=92.589 rate=1.82 Hz, eta=0:21:01, total=0:01:50, wall=20:13 IST
=> Training   12.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.568 DataTime=0.399 Loss=0.934 Prec@1=76.290 Prec@5=92.589 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=20:13 IST
=> Training   12.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.568 DataTime=0.399 Loss=0.934 Prec@1=76.290 Prec@5=92.589 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=20:14 IST
=> Training   12.03% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.563 DataTime=0.392 Loss=0.936 Prec@1=76.221 Prec@5=92.519 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=20:14 IST
=> Training   16.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.563 DataTime=0.392 Loss=0.936 Prec@1=76.221 Prec@5=92.519 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=20:14 IST
=> Training   16.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.563 DataTime=0.392 Loss=0.936 Prec@1=76.221 Prec@5=92.519 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=20:15 IST
=> Training   16.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.565 DataTime=0.392 Loss=0.939 Prec@1=76.177 Prec@5=92.490 rate=1.82 Hz, eta=0:19:17, total=0:03:40, wall=20:15 IST
=> Training   20.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.565 DataTime=0.392 Loss=0.939 Prec@1=76.177 Prec@5=92.490 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:15 IST
=> Training   20.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.565 DataTime=0.392 Loss=0.939 Prec@1=76.177 Prec@5=92.490 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:16 IST
=> Training   20.02% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.939 Prec@1=76.199 Prec@5=92.488 rate=1.80 Hz, eta=0:18:30, total=0:04:37, wall=20:16 IST
=> Training   24.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.939 Prec@1=76.199 Prec@5=92.488 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=20:16 IST
=> Training   24.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.939 Prec@1=76.199 Prec@5=92.488 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=20:17 IST
=> Training   24.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.942 Prec@1=76.128 Prec@5=92.443 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=20:17 IST
=> Training   28.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.942 Prec@1=76.128 Prec@5=92.443 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:17 IST
=> Training   28.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.942 Prec@1=76.128 Prec@5=92.443 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:18 IST
=> Training   28.01% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.943 Prec@1=76.123 Prec@5=92.428 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=20:18 IST
=> Training   32.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.943 Prec@1=76.123 Prec@5=92.428 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=20:18 IST
=> Training   32.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.562 DataTime=0.389 Loss=0.943 Prec@1=76.123 Prec@5=92.428 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=20:19 IST
=> Training   32.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.944 Prec@1=76.112 Prec@5=92.426 rate=1.80 Hz, eta=0:15:45, total=0:07:25, wall=20:19 IST
=> Training   36.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.944 Prec@1=76.112 Prec@5=92.426 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=20:19 IST
=> Training   36.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.388 Loss=0.944 Prec@1=76.112 Prec@5=92.426 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=20:20 IST
=> Training   36.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.387 Loss=0.945 Prec@1=76.091 Prec@5=92.412 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=20:20 IST
=> Training   39.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.387 Loss=0.945 Prec@1=76.091 Prec@5=92.412 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:20 IST
=> Training   39.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.387 Loss=0.945 Prec@1=76.091 Prec@5=92.412 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:21 IST
=> Training   39.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.946 Prec@1=76.070 Prec@5=92.396 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=20:21 IST
=> Training   43.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.946 Prec@1=76.070 Prec@5=92.396 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=20:21 IST
=> Training   43.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.946 Prec@1=76.070 Prec@5=92.396 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=20:22 IST
=> Training   43.99% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.947 Prec@1=76.049 Prec@5=92.369 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=20:22 IST
=> Training   47.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.947 Prec@1=76.049 Prec@5=92.369 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=20:22 IST
=> Training   47.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.947 Prec@1=76.049 Prec@5=92.369 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=20:23 IST
=> Training   47.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.386 Loss=0.948 Prec@1=76.010 Prec@5=92.364 rate=1.80 Hz, eta=0:12:03, total=0:11:07, wall=20:23 IST
=> Training   51.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.386 Loss=0.948 Prec@1=76.010 Prec@5=92.364 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=20:23 IST
=> Training   51.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.386 Loss=0.948 Prec@1=76.010 Prec@5=92.364 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=20:24 IST
=> Training   51.98% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.387 Loss=0.949 Prec@1=75.980 Prec@5=92.352 rate=1.80 Hz, eta=0:11:07, total=0:12:02, wall=20:24 IST
=> Training   55.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.387 Loss=0.949 Prec@1=75.980 Prec@5=92.352 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:24 IST
=> Training   55.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.387 Loss=0.949 Prec@1=75.980 Prec@5=92.352 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:25 IST
=> Training   55.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.951 Prec@1=75.946 Prec@5=92.336 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=20:25 IST
=> Training   59.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.951 Prec@1=75.946 Prec@5=92.336 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=20:25 IST
=> Training   59.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.951 Prec@1=75.946 Prec@5=92.336 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=20:25 IST
=> Training   59.97% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.953 Prec@1=75.899 Prec@5=92.308 rate=1.80 Hz, eta=0:09:16, total=0:13:54, wall=20:25 IST
=> Training   63.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.953 Prec@1=75.899 Prec@5=92.308 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=20:25 IST
=> Training   63.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.953 Prec@1=75.899 Prec@5=92.308 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=20:26 IST
=> Training   63.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.954 Prec@1=75.879 Prec@5=92.290 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=20:26 IST
=> Training   67.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.954 Prec@1=75.879 Prec@5=92.290 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=20:26 IST
=> Training   67.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.954 Prec@1=75.879 Prec@5=92.290 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=20:27 IST
=> Training   67.96% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.955 Prec@1=75.868 Prec@5=92.285 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=20:27 IST
=> Training   71.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.955 Prec@1=75.868 Prec@5=92.285 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=20:27 IST
=> Training   71.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.955 Prec@1=75.868 Prec@5=92.285 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=20:28 IST
=> Training   71.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.956 Prec@1=75.821 Prec@5=92.263 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=20:28 IST
=> Training   75.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.956 Prec@1=75.821 Prec@5=92.263 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=20:28 IST
=> Training   75.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.559 DataTime=0.385 Loss=0.956 Prec@1=75.821 Prec@5=92.263 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=20:29 IST
=> Training   75.95% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.957 Prec@1=75.800 Prec@5=92.253 rate=1.80 Hz, eta=0:05:35, total=0:17:38, wall=20:29 IST
=> Training   79.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.957 Prec@1=75.800 Prec@5=92.253 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=20:29 IST
=> Training   79.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.957 Prec@1=75.800 Prec@5=92.253 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=20:30 IST
=> Training   79.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.958 Prec@1=75.763 Prec@5=92.242 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=20:30 IST
=> Training   83.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.958 Prec@1=75.763 Prec@5=92.242 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:30 IST
=> Training   83.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.958 Prec@1=75.763 Prec@5=92.242 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:31 IST
=> Training   83.94% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.959 Prec@1=75.734 Prec@5=92.223 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=20:31 IST
=> Training   87.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.959 Prec@1=75.734 Prec@5=92.223 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:31 IST
=> Training   87.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.959 Prec@1=75.734 Prec@5=92.223 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:32 IST
=> Training   87.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.960 Prec@1=75.716 Prec@5=92.203 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:32 IST
=> Training   91.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.960 Prec@1=75.716 Prec@5=92.203 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:32 IST
=> Training   91.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.386 Loss=0.960 Prec@1=75.716 Prec@5=92.203 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:33 IST
=> Training   91.93% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.961 Prec@1=75.697 Prec@5=92.182 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:33 IST
=> Training   95.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.961 Prec@1=75.697 Prec@5=92.182 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:33 IST
=> Training   95.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.561 DataTime=0.386 Loss=0.961 Prec@1=75.697 Prec@5=92.182 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:34 IST
=> Training   95.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.385 Loss=0.962 Prec@1=75.677 Prec@5=92.173 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=20:34 IST
=> Training   99.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.385 Loss=0.962 Prec@1=75.677 Prec@5=92.173 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:34 IST
=> Training   99.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.385 Loss=0.962 Prec@1=75.677 Prec@5=92.173 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:34 IST
=> Training   99.92% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.385 Loss=0.962 Prec@1=75.677 Prec@5=92.174 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:34 IST
=> Training   100.00% of 1x2503...Epoch=111/150 LR=0.0165 Time=0.560 DataTime=0.385 Loss=0.962 Prec@1=75.677 Prec@5=92.174 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=20:34 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:34 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:34 IST
=> Validation 0.00% of 1x98...Epoch=111/150 LR=0.0165 Time=6.916 Loss=0.843 Prec@1=76.953 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=20:34 IST
=> Validation 1.02% of 1x98...Epoch=111/150 LR=0.0165 Time=6.916 Loss=0.843 Prec@1=76.953 Prec@5=93.555 rate=7134.14 Hz, eta=0:00:00, total=0:00:00, wall=20:34 IST
** Validation 1.02% of 1x98...Epoch=111/150 LR=0.0165 Time=6.916 Loss=0.843 Prec@1=76.953 Prec@5=93.555 rate=7134.14 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST
** Validation 1.02% of 1x98...Epoch=111/150 LR=0.0165 Time=0.638 Loss=1.276 Prec@1=69.144 Prec@5=88.886 rate=7134.14 Hz, eta=0:00:00, total=0:00:00, wall=20:35 IST
** Validation 100.00% of 1x98...Epoch=111/150 LR=0.0165 Time=0.638 Loss=1.276 Prec@1=69.144 Prec@5=88.886 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=20:35 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> Training   0.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=4.794 DataTime=4.334 Loss=0.980 Prec@1=76.172 Prec@5=91.211 rate=0 Hz, eta=?, total=0:00:00, wall=20:35 IST
=> Training   0.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=4.794 DataTime=4.334 Loss=0.980 Prec@1=76.172 Prec@5=91.211 rate=327.05 Hz, eta=0:00:07, total=0:00:00, wall=20:35 IST
=> Training   0.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=4.794 DataTime=4.334 Loss=0.980 Prec@1=76.172 Prec@5=91.211 rate=327.05 Hz, eta=0:00:07, total=0:00:00, wall=20:36 IST
=> Training   0.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.587 DataTime=0.415 Loss=0.929 Prec@1=76.572 Prec@5=92.572 rate=327.05 Hz, eta=0:00:07, total=0:00:00, wall=20:36 IST
=> Training   4.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.587 DataTime=0.415 Loss=0.929 Prec@1=76.572 Prec@5=92.572 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=20:36 IST
=> Training   4.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.587 DataTime=0.415 Loss=0.929 Prec@1=76.572 Prec@5=92.572 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=20:37 IST
=> Training   4.04% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.579 DataTime=0.413 Loss=0.928 Prec@1=76.575 Prec@5=92.578 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=20:37 IST
=> Training   8.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.579 DataTime=0.413 Loss=0.928 Prec@1=76.575 Prec@5=92.578 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=20:37 IST
=> Training   8.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.579 DataTime=0.413 Loss=0.928 Prec@1=76.575 Prec@5=92.578 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=20:38 IST
=> Training   8.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.570 DataTime=0.405 Loss=0.930 Prec@1=76.453 Prec@5=92.531 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=20:38 IST
=> Training   12.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.570 DataTime=0.405 Loss=0.930 Prec@1=76.453 Prec@5=92.531 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=20:38 IST
=> Training   12.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.570 DataTime=0.405 Loss=0.930 Prec@1=76.453 Prec@5=92.531 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=20:39 IST
=> Training   12.03% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.571 DataTime=0.402 Loss=0.930 Prec@1=76.443 Prec@5=92.560 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=20:39 IST
=> Training   16.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.571 DataTime=0.402 Loss=0.930 Prec@1=76.443 Prec@5=92.560 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=20:39 IST
=> Training   16.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.571 DataTime=0.402 Loss=0.930 Prec@1=76.443 Prec@5=92.560 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=20:40 IST
=> Training   16.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.568 DataTime=0.399 Loss=0.931 Prec@1=76.437 Prec@5=92.567 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=20:40 IST
=> Training   20.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.568 DataTime=0.399 Loss=0.931 Prec@1=76.437 Prec@5=92.567 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:40 IST
=> Training   20.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.568 DataTime=0.399 Loss=0.931 Prec@1=76.437 Prec@5=92.567 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:41 IST
=> Training   20.02% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.567 DataTime=0.397 Loss=0.933 Prec@1=76.404 Prec@5=92.560 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=20:41 IST
=> Training   24.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.567 DataTime=0.397 Loss=0.933 Prec@1=76.404 Prec@5=92.560 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:41 IST
=> Training   24.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.567 DataTime=0.397 Loss=0.933 Prec@1=76.404 Prec@5=92.560 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:42 IST
=> Training   24.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.394 Loss=0.935 Prec@1=76.351 Prec@5=92.532 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=20:42 IST
=> Training   28.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.394 Loss=0.935 Prec@1=76.351 Prec@5=92.532 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=20:42 IST
=> Training   28.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.394 Loss=0.935 Prec@1=76.351 Prec@5=92.532 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=20:42 IST
=> Training   28.01% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.565 DataTime=0.396 Loss=0.937 Prec@1=76.317 Prec@5=92.488 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=20:42 IST
=> Training   32.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.565 DataTime=0.396 Loss=0.937 Prec@1=76.317 Prec@5=92.488 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=20:42 IST
=> Training   32.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.565 DataTime=0.396 Loss=0.937 Prec@1=76.317 Prec@5=92.488 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=20:43 IST
=> Training   32.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.563 DataTime=0.394 Loss=0.937 Prec@1=76.313 Prec@5=92.501 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=20:43 IST
=> Training   36.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.563 DataTime=0.394 Loss=0.937 Prec@1=76.313 Prec@5=92.501 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=20:43 IST
=> Training   36.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.563 DataTime=0.394 Loss=0.937 Prec@1=76.313 Prec@5=92.501 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=20:44 IST
=> Training   36.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.564 DataTime=0.395 Loss=0.937 Prec@1=76.303 Prec@5=92.501 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=20:44 IST
=> Training   39.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.564 DataTime=0.395 Loss=0.937 Prec@1=76.303 Prec@5=92.501 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=20:44 IST
=> Training   39.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.564 DataTime=0.395 Loss=0.937 Prec@1=76.303 Prec@5=92.501 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=20:45 IST
=> Training   39.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.939 Prec@1=76.249 Prec@5=92.470 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=20:45 IST
=> Training   43.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.939 Prec@1=76.249 Prec@5=92.470 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=20:45 IST
=> Training   43.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.939 Prec@1=76.249 Prec@5=92.470 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=20:46 IST
=> Training   43.99% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.940 Prec@1=76.200 Prec@5=92.460 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=20:46 IST
=> Training   47.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.940 Prec@1=76.200 Prec@5=92.460 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=20:46 IST
=> Training   47.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.562 DataTime=0.393 Loss=0.940 Prec@1=76.200 Prec@5=92.460 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=20:47 IST
=> Training   47.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.392 Loss=0.941 Prec@1=76.189 Prec@5=92.449 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=20:47 IST
=> Training   51.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.392 Loss=0.941 Prec@1=76.189 Prec@5=92.449 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:47 IST
=> Training   51.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.392 Loss=0.941 Prec@1=76.189 Prec@5=92.449 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:48 IST
=> Training   51.98% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.942 Prec@1=76.148 Prec@5=92.423 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=20:48 IST
=> Training   55.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.942 Prec@1=76.148 Prec@5=92.423 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:48 IST
=> Training   55.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.942 Prec@1=76.148 Prec@5=92.423 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:49 IST
=> Training   55.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.943 Prec@1=76.140 Prec@5=92.414 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=20:49 IST
=> Training   59.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.943 Prec@1=76.140 Prec@5=92.414 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:49 IST
=> Training   59.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.391 Loss=0.943 Prec@1=76.140 Prec@5=92.414 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:50 IST
=> Training   59.97% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.944 Prec@1=76.120 Prec@5=92.399 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=20:50 IST
=> Training   63.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.944 Prec@1=76.120 Prec@5=92.399 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:50 IST
=> Training   63.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.944 Prec@1=76.120 Prec@5=92.399 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:51 IST
=> Training   63.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.945 Prec@1=76.093 Prec@5=92.385 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=20:51 IST
=> Training   67.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.945 Prec@1=76.093 Prec@5=92.385 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:51 IST
=> Training   67.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.945 Prec@1=76.093 Prec@5=92.385 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:52 IST
=> Training   67.96% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.946 Prec@1=76.075 Prec@5=92.380 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=20:52 IST
=> Training   71.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.946 Prec@1=76.075 Prec@5=92.380 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=20:52 IST
=> Training   71.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.946 Prec@1=76.075 Prec@5=92.380 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=20:53 IST
=> Training   71.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.389 Loss=0.947 Prec@1=76.060 Prec@5=92.368 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=20:53 IST
=> Training   75.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.389 Loss=0.947 Prec@1=76.060 Prec@5=92.368 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=20:53 IST
=> Training   75.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.389 Loss=0.947 Prec@1=76.060 Prec@5=92.368 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=20:54 IST
=> Training   75.95% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.030 Prec@5=92.353 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=20:54 IST
=> Training   79.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.030 Prec@5=92.353 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:54 IST
=> Training   79.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.030 Prec@5=92.353 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:55 IST
=> Training   79.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.022 Prec@5=92.351 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=20:55 IST
=> Training   83.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.022 Prec@5=92.351 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=20:55 IST
=> Training   83.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.561 DataTime=0.390 Loss=0.948 Prec@1=76.022 Prec@5=92.351 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=20:56 IST
=> Training   83.94% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.999 Prec@5=92.342 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=20:56 IST
=> Training   87.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.999 Prec@5=92.342 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:56 IST
=> Training   87.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.999 Prec@5=92.342 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:56 IST
=> Training   87.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.992 Prec@5=92.333 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=20:56 IST
=> Training   91.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.992 Prec@5=92.333 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:56 IST
=> Training   91.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.949 Prec@1=75.992 Prec@5=92.333 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:57 IST
=> Training   91.93% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=75.958 Prec@5=92.315 rate=1.79 Hz, eta=0:01:52, total=0:21:24, wall=20:57 IST
=> Training   95.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=75.958 Prec@5=92.315 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=20:57 IST
=> Training   95.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.951 Prec@1=75.958 Prec@5=92.315 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=20:58 IST
=> Training   95.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.952 Prec@1=75.939 Prec@5=92.303 rate=1.79 Hz, eta=0:00:56, total=0:22:19, wall=20:58 IST
=> Training   99.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.952 Prec@1=75.939 Prec@5=92.303 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:58 IST
=> Training   99.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.560 DataTime=0.389 Loss=0.952 Prec@1=75.939 Prec@5=92.303 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:58 IST
=> Training   99.92% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.559 DataTime=0.388 Loss=0.952 Prec@1=75.939 Prec@5=92.303 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=20:58 IST
=> Training   100.00% of 1x2503...Epoch=112/150 LR=0.0158 Time=0.559 DataTime=0.388 Loss=0.952 Prec@1=75.939 Prec@5=92.303 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=20:58 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:58 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=20:58 IST
=> Validation 0.00% of 1x98...Epoch=112/150 LR=0.0158 Time=7.237 Loss=0.808 Prec@1=78.516 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=20:58 IST
=> Validation 1.02% of 1x98...Epoch=112/150 LR=0.0158 Time=7.237 Loss=0.808 Prec@1=78.516 Prec@5=94.531 rate=5992.65 Hz, eta=0:00:00, total=0:00:00, wall=20:58 IST
** Validation 1.02% of 1x98...Epoch=112/150 LR=0.0158 Time=7.237 Loss=0.808 Prec@1=78.516 Prec@5=94.531 rate=5992.65 Hz, eta=0:00:00, total=0:00:00, wall=20:59 IST
** Validation 1.02% of 1x98...Epoch=112/150 LR=0.0158 Time=0.635 Loss=1.266 Prec@1=69.410 Prec@5=88.864 rate=5992.65 Hz, eta=0:00:00, total=0:00:00, wall=20:59 IST
** Validation 100.00% of 1x98...Epoch=112/150 LR=0.0158 Time=0.635 Loss=1.266 Prec@1=69.410 Prec@5=88.864 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=20:59 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:59 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=20:59 IST
=> Training   0.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=4.887 DataTime=4.427 Loss=0.911 Prec@1=75.781 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=20:59 IST
=> Training   0.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=4.887 DataTime=4.427 Loss=0.911 Prec@1=75.781 Prec@5=93.555 rate=3151.75 Hz, eta=0:00:00, total=0:00:00, wall=20:59 IST
=> Training   0.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=4.887 DataTime=4.427 Loss=0.911 Prec@1=75.781 Prec@5=93.555 rate=3151.75 Hz, eta=0:00:00, total=0:00:00, wall=21:00 IST
=> Training   0.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.596 DataTime=0.431 Loss=0.917 Prec@1=76.738 Prec@5=92.785 rate=3151.75 Hz, eta=0:00:00, total=0:00:00, wall=21:00 IST
=> Training   4.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.596 DataTime=0.431 Loss=0.917 Prec@1=76.738 Prec@5=92.785 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=21:00 IST
=> Training   4.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.596 DataTime=0.431 Loss=0.917 Prec@1=76.738 Prec@5=92.785 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=21:01 IST
=> Training   4.04% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.579 DataTime=0.411 Loss=0.910 Prec@1=76.938 Prec@5=92.838 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=21:01 IST
=> Training   8.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.579 DataTime=0.411 Loss=0.910 Prec@1=76.938 Prec@5=92.838 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=21:01 IST
=> Training   8.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.579 DataTime=0.411 Loss=0.910 Prec@1=76.938 Prec@5=92.838 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=21:02 IST
=> Training   8.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.569 DataTime=0.398 Loss=0.911 Prec@1=76.863 Prec@5=92.827 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=21:02 IST
=> Training   12.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.569 DataTime=0.398 Loss=0.911 Prec@1=76.863 Prec@5=92.827 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:02 IST
=> Training   12.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.569 DataTime=0.398 Loss=0.911 Prec@1=76.863 Prec@5=92.827 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:03 IST
=> Training   12.03% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.571 DataTime=0.398 Loss=0.916 Prec@1=76.794 Prec@5=92.749 rate=1.81 Hz, eta=0:20:18, total=0:02:46, wall=21:03 IST
=> Training   16.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.571 DataTime=0.398 Loss=0.916 Prec@1=76.794 Prec@5=92.749 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=21:03 IST
=> Training   16.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.571 DataTime=0.398 Loss=0.916 Prec@1=76.794 Prec@5=92.749 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=21:04 IST
=> Training   16.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.566 DataTime=0.391 Loss=0.921 Prec@1=76.709 Prec@5=92.695 rate=1.79 Hz, eta=0:19:34, total=0:03:44, wall=21:04 IST
=> Training   20.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.566 DataTime=0.391 Loss=0.921 Prec@1=76.709 Prec@5=92.695 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=21:04 IST
=> Training   20.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.566 DataTime=0.391 Loss=0.921 Prec@1=76.709 Prec@5=92.695 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=21:05 IST
=> Training   20.02% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.567 DataTime=0.392 Loss=0.923 Prec@1=76.656 Prec@5=92.671 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=21:05 IST
=> Training   24.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.567 DataTime=0.392 Loss=0.923 Prec@1=76.656 Prec@5=92.671 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=21:05 IST
=> Training   24.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.567 DataTime=0.392 Loss=0.923 Prec@1=76.656 Prec@5=92.671 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=21:06 IST
=> Training   24.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.564 DataTime=0.390 Loss=0.925 Prec@1=76.611 Prec@5=92.647 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=21:06 IST
=> Training   28.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.564 DataTime=0.390 Loss=0.925 Prec@1=76.611 Prec@5=92.647 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=21:06 IST
=> Training   28.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.564 DataTime=0.390 Loss=0.925 Prec@1=76.611 Prec@5=92.647 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=21:07 IST
=> Training   28.01% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.565 DataTime=0.391 Loss=0.926 Prec@1=76.598 Prec@5=92.632 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=21:07 IST
=> Training   32.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.565 DataTime=0.391 Loss=0.926 Prec@1=76.598 Prec@5=92.632 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:07 IST
=> Training   32.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.565 DataTime=0.391 Loss=0.926 Prec@1=76.598 Prec@5=92.632 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:08 IST
=> Training   32.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.567 Prec@5=92.617 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=21:08 IST
=> Training   36.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.567 Prec@5=92.617 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=21:08 IST
=> Training   36.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.567 Prec@5=92.617 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=21:09 IST
=> Training   36.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.561 Prec@5=92.603 rate=1.79 Hz, eta=0:14:54, total=0:08:22, wall=21:09 IST
=> Training   39.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.561 Prec@5=92.603 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=21:09 IST
=> Training   39.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.391 Loss=0.927 Prec@1=76.561 Prec@5=92.603 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=21:10 IST
=> Training   39.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.928 Prec@1=76.553 Prec@5=92.593 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=21:10 IST
=> Training   43.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.928 Prec@1=76.553 Prec@5=92.593 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=21:10 IST
=> Training   43.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.928 Prec@1=76.553 Prec@5=92.593 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=21:11 IST
=> Training   43.99% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.390 Loss=0.930 Prec@1=76.495 Prec@5=92.575 rate=1.79 Hz, eta=0:13:01, total=0:10:14, wall=21:11 IST
=> Training   47.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.390 Loss=0.930 Prec@1=76.495 Prec@5=92.575 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=21:11 IST
=> Training   47.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.390 Loss=0.930 Prec@1=76.495 Prec@5=92.575 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=21:12 IST
=> Training   47.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.390 Loss=0.931 Prec@1=76.452 Prec@5=92.557 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=21:12 IST
=> Training   51.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.390 Loss=0.931 Prec@1=76.452 Prec@5=92.557 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=21:12 IST
=> Training   51.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.563 DataTime=0.390 Loss=0.931 Prec@1=76.452 Prec@5=92.557 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=21:12 IST
=> Training   51.98% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.932 Prec@1=76.420 Prec@5=92.547 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=21:12 IST
=> Training   55.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.932 Prec@1=76.420 Prec@5=92.547 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=21:12 IST
=> Training   55.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.932 Prec@1=76.420 Prec@5=92.547 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=21:13 IST
=> Training   55.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.933 Prec@1=76.399 Prec@5=92.527 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=21:13 IST
=> Training   59.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.933 Prec@1=76.399 Prec@5=92.527 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=21:13 IST
=> Training   59.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.389 Loss=0.933 Prec@1=76.399 Prec@5=92.527 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=21:14 IST
=> Training   59.97% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.388 Loss=0.934 Prec@1=76.376 Prec@5=92.511 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=21:14 IST
=> Training   63.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.388 Loss=0.934 Prec@1=76.376 Prec@5=92.511 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=21:14 IST
=> Training   63.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.562 DataTime=0.388 Loss=0.934 Prec@1=76.376 Prec@5=92.511 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=21:15 IST
=> Training   63.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.936 Prec@1=76.347 Prec@5=92.490 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=21:15 IST
=> Training   67.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.936 Prec@1=76.347 Prec@5=92.490 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=21:15 IST
=> Training   67.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.936 Prec@1=76.347 Prec@5=92.490 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=21:16 IST
=> Training   67.96% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.937 Prec@1=76.314 Prec@5=92.478 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=21:16 IST
=> Training   71.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.937 Prec@1=76.314 Prec@5=92.478 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=21:16 IST
=> Training   71.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.388 Loss=0.937 Prec@1=76.314 Prec@5=92.478 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=21:17 IST
=> Training   71.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.387 Loss=0.938 Prec@1=76.285 Prec@5=92.462 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=21:17 IST
=> Training   75.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.387 Loss=0.938 Prec@1=76.285 Prec@5=92.462 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:17 IST
=> Training   75.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.387 Loss=0.938 Prec@1=76.285 Prec@5=92.462 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:18 IST
=> Training   75.95% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.387 Loss=0.939 Prec@1=76.266 Prec@5=92.453 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=21:18 IST
=> Training   79.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.387 Loss=0.939 Prec@1=76.266 Prec@5=92.453 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=21:18 IST
=> Training   79.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.387 Loss=0.939 Prec@1=76.266 Prec@5=92.453 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=21:19 IST
=> Training   79.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.386 Loss=0.939 Prec@1=76.256 Prec@5=92.444 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=21:19 IST
=> Training   83.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.386 Loss=0.939 Prec@1=76.256 Prec@5=92.444 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:19 IST
=> Training   83.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.386 Loss=0.939 Prec@1=76.256 Prec@5=92.444 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:20 IST
=> Training   83.94% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.940 Prec@1=76.229 Prec@5=92.429 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=21:20 IST
=> Training   87.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.940 Prec@1=76.229 Prec@5=92.429 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=21:20 IST
=> Training   87.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.940 Prec@1=76.229 Prec@5=92.429 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=21:21 IST
=> Training   87.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.201 Prec@5=92.418 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=21:21 IST
=> Training   91.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.201 Prec@5=92.418 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=21:21 IST
=> Training   91.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.201 Prec@5=92.418 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=21:22 IST
=> Training   91.93% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.184 Prec@5=92.413 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=21:22 IST
=> Training   95.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.184 Prec@5=92.413 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=21:22 IST
=> Training   95.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.941 Prec@1=76.184 Prec@5=92.413 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=21:23 IST
=> Training   95.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.942 Prec@1=76.163 Prec@5=92.406 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=21:23 IST
=> Training   99.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.942 Prec@1=76.163 Prec@5=92.406 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:23 IST
=> Training   99.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.561 DataTime=0.386 Loss=0.942 Prec@1=76.163 Prec@5=92.406 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:23 IST
=> Training   99.92% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.386 Loss=0.942 Prec@1=76.162 Prec@5=92.405 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=21:23 IST
=> Training   100.00% of 1x2503...Epoch=113/150 LR=0.0150 Time=0.560 DataTime=0.386 Loss=0.942 Prec@1=76.162 Prec@5=92.405 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=21:23 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> Validation 0.00% of 1x98...Epoch=113/150 LR=0.0150 Time=6.946 Loss=0.741 Prec@1=80.469 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=21:23 IST
=> Validation 1.02% of 1x98...Epoch=113/150 LR=0.0150 Time=6.946 Loss=0.741 Prec@1=80.469 Prec@5=93.750 rate=6113.85 Hz, eta=0:00:00, total=0:00:00, wall=21:23 IST
** Validation 1.02% of 1x98...Epoch=113/150 LR=0.0150 Time=6.946 Loss=0.741 Prec@1=80.469 Prec@5=93.750 rate=6113.85 Hz, eta=0:00:00, total=0:00:00, wall=21:24 IST
** Validation 1.02% of 1x98...Epoch=113/150 LR=0.0150 Time=0.634 Loss=1.245 Prec@1=69.942 Prec@5=89.360 rate=6113.85 Hz, eta=0:00:00, total=0:00:00, wall=21:24 IST
** Validation 100.00% of 1x98...Epoch=113/150 LR=0.0150 Time=0.634 Loss=1.245 Prec@1=69.942 Prec@5=89.360 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=21:24 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST
=> Training   0.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=4.748 DataTime=4.154 Loss=0.808 Prec@1=78.906 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=21:24 IST
=> Training   0.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=4.748 DataTime=4.154 Loss=0.808 Prec@1=78.906 Prec@5=93.945 rate=2619.74 Hz, eta=0:00:00, total=0:00:00, wall=21:24 IST
=> Training   0.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=4.748 DataTime=4.154 Loss=0.808 Prec@1=78.906 Prec@5=93.945 rate=2619.74 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST
=> Training   0.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.602 DataTime=0.419 Loss=0.915 Prec@1=76.669 Prec@5=92.870 rate=2619.74 Hz, eta=0:00:00, total=0:00:00, wall=21:25 IST
=> Training   4.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.602 DataTime=0.419 Loss=0.915 Prec@1=76.669 Prec@5=92.870 rate=1.80 Hz, eta=0:22:14, total=0:00:56, wall=21:25 IST
=> Training   4.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.602 DataTime=0.419 Loss=0.915 Prec@1=76.669 Prec@5=92.870 rate=1.80 Hz, eta=0:22:14, total=0:00:56, wall=21:26 IST
=> Training   4.04% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.578 DataTime=0.397 Loss=0.910 Prec@1=76.818 Prec@5=92.903 rate=1.80 Hz, eta=0:22:14, total=0:00:56, wall=21:26 IST
=> Training   8.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.578 DataTime=0.397 Loss=0.910 Prec@1=76.818 Prec@5=92.903 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=21:26 IST
=> Training   8.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.578 DataTime=0.397 Loss=0.910 Prec@1=76.818 Prec@5=92.903 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=21:27 IST
=> Training   8.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.574 DataTime=0.395 Loss=0.914 Prec@1=76.787 Prec@5=92.823 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=21:27 IST
=> Training   12.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.574 DataTime=0.395 Loss=0.914 Prec@1=76.787 Prec@5=92.823 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=21:27 IST
=> Training   12.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.574 DataTime=0.395 Loss=0.914 Prec@1=76.787 Prec@5=92.823 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=21:28 IST
=> Training   12.03% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.570 DataTime=0.391 Loss=0.912 Prec@1=76.893 Prec@5=92.817 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=21:28 IST
=> Training   16.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.570 DataTime=0.391 Loss=0.912 Prec@1=76.893 Prec@5=92.817 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=21:28 IST
=> Training   16.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.570 DataTime=0.391 Loss=0.912 Prec@1=76.893 Prec@5=92.817 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=21:29 IST
=> Training   16.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.915 Prec@1=76.825 Prec@5=92.770 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=21:29 IST
=> Training   20.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.915 Prec@1=76.825 Prec@5=92.770 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=21:29 IST
=> Training   20.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.915 Prec@1=76.825 Prec@5=92.770 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=21:29 IST
=> Training   20.02% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.916 Prec@1=76.809 Prec@5=92.730 rate=1.79 Hz, eta=0:18:38, total=0:04:40, wall=21:29 IST
=> Training   24.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.916 Prec@1=76.809 Prec@5=92.730 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=21:29 IST
=> Training   24.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.568 DataTime=0.391 Loss=0.916 Prec@1=76.809 Prec@5=92.730 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=21:30 IST
=> Training   24.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.567 DataTime=0.390 Loss=0.917 Prec@1=76.790 Prec@5=92.712 rate=1.79 Hz, eta=0:17:44, total=0:05:36, wall=21:30 IST
=> Training   28.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.567 DataTime=0.390 Loss=0.917 Prec@1=76.790 Prec@5=92.712 rate=1.78 Hz, eta=0:16:50, total=0:06:32, wall=21:30 IST
=> Training   28.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.567 DataTime=0.390 Loss=0.917 Prec@1=76.790 Prec@5=92.712 rate=1.78 Hz, eta=0:16:50, total=0:06:32, wall=21:31 IST
=> Training   28.01% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.386 Loss=0.917 Prec@1=76.771 Prec@5=92.712 rate=1.78 Hz, eta=0:16:50, total=0:06:32, wall=21:31 IST
=> Training   32.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.386 Loss=0.917 Prec@1=76.771 Prec@5=92.712 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=21:31 IST
=> Training   32.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.386 Loss=0.917 Prec@1=76.771 Prec@5=92.712 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=21:32 IST
=> Training   32.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.388 Loss=0.918 Prec@1=76.751 Prec@5=92.713 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=21:32 IST
=> Training   36.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.388 Loss=0.918 Prec@1=76.751 Prec@5=92.713 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=21:32 IST
=> Training   36.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.565 DataTime=0.388 Loss=0.918 Prec@1=76.751 Prec@5=92.713 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=21:33 IST
=> Training   36.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.388 Loss=0.918 Prec@1=76.776 Prec@5=92.691 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=21:33 IST
=> Training   39.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.388 Loss=0.918 Prec@1=76.776 Prec@5=92.691 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=21:33 IST
=> Training   39.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.388 Loss=0.918 Prec@1=76.776 Prec@5=92.691 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=21:34 IST
=> Training   39.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.387 Loss=0.920 Prec@1=76.718 Prec@5=92.674 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=21:34 IST
=> Training   43.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.387 Loss=0.920 Prec@1=76.718 Prec@5=92.674 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=21:34 IST
=> Training   43.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.387 Loss=0.920 Prec@1=76.718 Prec@5=92.674 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=21:35 IST
=> Training   43.99% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.387 Loss=0.921 Prec@1=76.692 Prec@5=92.672 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=21:35 IST
=> Training   47.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.387 Loss=0.921 Prec@1=76.692 Prec@5=92.672 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=21:35 IST
=> Training   47.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.564 DataTime=0.387 Loss=0.921 Prec@1=76.692 Prec@5=92.672 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=21:36 IST
=> Training   47.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.922 Prec@1=76.647 Prec@5=92.655 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=21:36 IST
=> Training   51.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.922 Prec@1=76.647 Prec@5=92.655 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=21:36 IST
=> Training   51.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.922 Prec@1=76.647 Prec@5=92.655 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=21:37 IST
=> Training   51.98% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.923 Prec@1=76.609 Prec@5=92.633 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=21:37 IST
=> Training   55.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.923 Prec@1=76.609 Prec@5=92.633 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=21:37 IST
=> Training   55.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.563 DataTime=0.386 Loss=0.923 Prec@1=76.609 Prec@5=92.633 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=21:38 IST
=> Training   55.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.924 Prec@1=76.568 Prec@5=92.615 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=21:38 IST
=> Training   59.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.924 Prec@1=76.568 Prec@5=92.615 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=21:38 IST
=> Training   59.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.924 Prec@1=76.568 Prec@5=92.615 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=21:39 IST
=> Training   59.97% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.925 Prec@1=76.555 Prec@5=92.598 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=21:39 IST
=> Training   63.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.925 Prec@1=76.555 Prec@5=92.598 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=21:39 IST
=> Training   63.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.925 Prec@1=76.555 Prec@5=92.598 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=21:40 IST
=> Training   63.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.926 Prec@1=76.539 Prec@5=92.592 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=21:40 IST
=> Training   67.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.926 Prec@1=76.539 Prec@5=92.592 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=21:40 IST
=> Training   67.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.926 Prec@1=76.539 Prec@5=92.592 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=21:41 IST
=> Training   67.96% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.523 Prec@5=92.582 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=21:41 IST
=> Training   71.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.523 Prec@5=92.582 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=21:41 IST
=> Training   71.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.523 Prec@5=92.582 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=21:42 IST
=> Training   71.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.512 Prec@5=92.584 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=21:42 IST
=> Training   75.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.512 Prec@5=92.584 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=21:42 IST
=> Training   75.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.386 Loss=0.927 Prec@1=76.512 Prec@5=92.584 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=21:43 IST
=> Training   75.95% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.928 Prec@1=76.476 Prec@5=92.571 rate=1.79 Hz, eta=0:05:36, total=0:17:44, wall=21:43 IST
=> Training   79.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.928 Prec@1=76.476 Prec@5=92.571 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=21:43 IST
=> Training   79.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.928 Prec@1=76.476 Prec@5=92.571 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=21:43 IST
=> Training   79.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.929 Prec@1=76.455 Prec@5=92.564 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=21:43 IST
=> Training   83.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.929 Prec@1=76.455 Prec@5=92.564 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:43 IST
=> Training   83.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.929 Prec@1=76.455 Prec@5=92.564 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:44 IST
=> Training   83.94% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.930 Prec@1=76.428 Prec@5=92.552 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=21:44 IST
=> Training   87.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.930 Prec@1=76.428 Prec@5=92.552 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=21:44 IST
=> Training   87.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.930 Prec@1=76.428 Prec@5=92.552 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=21:45 IST
=> Training   87.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.931 Prec@1=76.406 Prec@5=92.546 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=21:45 IST
=> Training   91.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.931 Prec@1=76.406 Prec@5=92.546 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=21:45 IST
=> Training   91.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.931 Prec@1=76.406 Prec@5=92.546 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=21:46 IST
=> Training   91.93% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.932 Prec@1=76.381 Prec@5=92.531 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=21:46 IST
=> Training   95.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.932 Prec@1=76.381 Prec@5=92.531 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=21:46 IST
=> Training   95.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.932 Prec@1=76.381 Prec@5=92.531 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=21:47 IST
=> Training   95.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.933 Prec@1=76.364 Prec@5=92.519 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=21:47 IST
=> Training   99.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.933 Prec@1=76.364 Prec@5=92.519 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=21:47 IST
=> Training   99.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.562 DataTime=0.387 Loss=0.933 Prec@1=76.364 Prec@5=92.519 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=21:47 IST
=> Training   99.92% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.561 DataTime=0.387 Loss=0.933 Prec@1=76.364 Prec@5=92.519 rate=1.79 Hz, eta=0:00:01, total=0:23:20, wall=21:47 IST
=> Training   100.00% of 1x2503...Epoch=114/150 LR=0.0143 Time=0.561 DataTime=0.387 Loss=0.933 Prec@1=76.364 Prec@5=92.519 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=21:47 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:47 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=21:47 IST
=> Validation 0.00% of 1x98...Epoch=114/150 LR=0.0143 Time=6.826 Loss=0.832 Prec@1=79.297 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=21:47 IST
=> Validation 1.02% of 1x98...Epoch=114/150 LR=0.0143 Time=6.826 Loss=0.832 Prec@1=79.297 Prec@5=93.750 rate=5817.71 Hz, eta=0:00:00, total=0:00:00, wall=21:47 IST
** Validation 1.02% of 1x98...Epoch=114/150 LR=0.0143 Time=6.826 Loss=0.832 Prec@1=79.297 Prec@5=93.750 rate=5817.71 Hz, eta=0:00:00, total=0:00:00, wall=21:48 IST
** Validation 1.02% of 1x98...Epoch=114/150 LR=0.0143 Time=0.638 Loss=1.249 Prec@1=69.700 Prec@5=89.290 rate=5817.71 Hz, eta=0:00:00, total=0:00:00, wall=21:48 IST
** Validation 100.00% of 1x98...Epoch=114/150 LR=0.0143 Time=0.638 Loss=1.249 Prec@1=69.700 Prec@5=89.290 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=21:48 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:48 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=21:48 IST
=> Training   0.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=4.420 DataTime=4.012 Loss=0.891 Prec@1=79.492 Prec@5=92.773 rate=0 Hz, eta=?, total=0:00:00, wall=21:48 IST
=> Training   0.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=4.420 DataTime=4.012 Loss=0.891 Prec@1=79.492 Prec@5=92.773 rate=745.37 Hz, eta=0:00:03, total=0:00:00, wall=21:48 IST
=> Training   0.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=4.420 DataTime=4.012 Loss=0.891 Prec@1=79.492 Prec@5=92.773 rate=745.37 Hz, eta=0:00:03, total=0:00:00, wall=21:49 IST
=> Training   0.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.597 DataTime=0.424 Loss=0.919 Prec@1=76.688 Prec@5=92.640 rate=745.37 Hz, eta=0:00:03, total=0:00:00, wall=21:49 IST
=> Training   4.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.597 DataTime=0.424 Loss=0.919 Prec@1=76.688 Prec@5=92.640 rate=1.81 Hz, eta=0:22:08, total=0:00:55, wall=21:49 IST
=> Training   4.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.597 DataTime=0.424 Loss=0.919 Prec@1=76.688 Prec@5=92.640 rate=1.81 Hz, eta=0:22:08, total=0:00:55, wall=21:50 IST
=> Training   4.04% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.577 DataTime=0.404 Loss=0.908 Prec@1=77.002 Prec@5=92.797 rate=1.81 Hz, eta=0:22:08, total=0:00:55, wall=21:50 IST
=> Training   8.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.577 DataTime=0.404 Loss=0.908 Prec@1=77.002 Prec@5=92.797 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=21:50 IST
=> Training   8.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.577 DataTime=0.404 Loss=0.908 Prec@1=77.002 Prec@5=92.797 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=21:51 IST
=> Training   8.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.574 DataTime=0.399 Loss=0.908 Prec@1=77.006 Prec@5=92.813 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=21:51 IST
=> Training   12.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.574 DataTime=0.399 Loss=0.908 Prec@1=77.006 Prec@5=92.813 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=21:51 IST
=> Training   12.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.574 DataTime=0.399 Loss=0.908 Prec@1=77.006 Prec@5=92.813 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=21:52 IST
=> Training   12.03% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.568 DataTime=0.389 Loss=0.907 Prec@1=77.004 Prec@5=92.838 rate=1.79 Hz, eta=0:20:32, total=0:02:48, wall=21:52 IST
=> Training   16.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.568 DataTime=0.389 Loss=0.907 Prec@1=77.004 Prec@5=92.838 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=21:52 IST
=> Training   16.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.568 DataTime=0.389 Loss=0.907 Prec@1=77.004 Prec@5=92.838 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=21:53 IST
=> Training   16.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.566 DataTime=0.388 Loss=0.906 Prec@1=77.019 Prec@5=92.870 rate=1.80 Hz, eta=0:19:30, total=0:03:43, wall=21:53 IST
=> Training   20.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.566 DataTime=0.388 Loss=0.906 Prec@1=77.019 Prec@5=92.870 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=21:53 IST
=> Training   20.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.566 DataTime=0.388 Loss=0.906 Prec@1=77.019 Prec@5=92.870 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=21:54 IST
=> Training   20.02% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.907 Prec@1=77.004 Prec@5=92.849 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=21:54 IST
=> Training   24.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.907 Prec@1=77.004 Prec@5=92.849 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=21:54 IST
=> Training   24.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.907 Prec@1=77.004 Prec@5=92.849 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=21:55 IST
=> Training   24.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.909 Prec@1=76.939 Prec@5=92.804 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=21:55 IST
=> Training   28.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.909 Prec@1=76.939 Prec@5=92.804 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=21:55 IST
=> Training   28.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.564 DataTime=0.386 Loss=0.909 Prec@1=76.939 Prec@5=92.804 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=21:56 IST
=> Training   28.01% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.908 Prec@1=76.946 Prec@5=92.805 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=21:56 IST
=> Training   32.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.908 Prec@1=76.946 Prec@5=92.805 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=21:56 IST
=> Training   32.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.908 Prec@1=76.946 Prec@5=92.805 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=21:57 IST
=> Training   32.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.909 Prec@1=76.951 Prec@5=92.796 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=21:57 IST
=> Training   36.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.909 Prec@1=76.951 Prec@5=92.796 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:57 IST
=> Training   36.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.909 Prec@1=76.951 Prec@5=92.796 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:58 IST
=> Training   36.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.910 Prec@1=76.892 Prec@5=92.772 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=21:58 IST
=> Training   39.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.910 Prec@1=76.892 Prec@5=92.772 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=21:58 IST
=> Training   39.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.910 Prec@1=76.892 Prec@5=92.772 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=21:59 IST
=> Training   39.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.912 Prec@1=76.849 Prec@5=92.760 rate=1.80 Hz, eta=0:13:55, total=0:09:17, wall=21:59 IST
=> Training   43.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.912 Prec@1=76.849 Prec@5=92.760 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=21:59 IST
=> Training   43.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.912 Prec@1=76.849 Prec@5=92.760 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=22:00 IST
=> Training   43.99% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.386 Loss=0.912 Prec@1=76.866 Prec@5=92.756 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=22:00 IST
=> Training   47.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.386 Loss=0.912 Prec@1=76.866 Prec@5=92.756 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=22:00 IST
=> Training   47.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.386 Loss=0.912 Prec@1=76.866 Prec@5=92.756 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=22:00 IST
=> Training   47.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.912 Prec@1=76.842 Prec@5=92.757 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=22:00 IST
=> Training   51.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.912 Prec@1=76.842 Prec@5=92.757 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=22:00 IST
=> Training   51.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.912 Prec@1=76.842 Prec@5=92.757 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=22:01 IST
=> Training   51.98% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.913 Prec@1=76.813 Prec@5=92.745 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=22:01 IST
=> Training   55.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.913 Prec@1=76.813 Prec@5=92.745 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:01 IST
=> Training   55.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.913 Prec@1=76.813 Prec@5=92.745 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:02 IST
=> Training   55.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.915 Prec@1=76.789 Prec@5=92.728 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=22:02 IST
=> Training   59.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.915 Prec@1=76.789 Prec@5=92.728 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=22:02 IST
=> Training   59.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.915 Prec@1=76.789 Prec@5=92.728 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=22:03 IST
=> Training   59.97% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.915 Prec@1=76.788 Prec@5=92.719 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=22:03 IST
=> Training   63.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.915 Prec@1=76.788 Prec@5=92.719 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=22:03 IST
=> Training   63.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.562 DataTime=0.384 Loss=0.915 Prec@1=76.788 Prec@5=92.719 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=22:04 IST
=> Training   63.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.916 Prec@1=76.775 Prec@5=92.711 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=22:04 IST
=> Training   67.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.916 Prec@1=76.775 Prec@5=92.711 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=22:04 IST
=> Training   67.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.383 Loss=0.916 Prec@1=76.775 Prec@5=92.711 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=22:05 IST
=> Training   67.96% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.917 Prec@1=76.746 Prec@5=92.705 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=22:05 IST
=> Training   71.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.917 Prec@1=76.746 Prec@5=92.705 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=22:05 IST
=> Training   71.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.917 Prec@1=76.746 Prec@5=92.705 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=22:06 IST
=> Training   71.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.918 Prec@1=76.720 Prec@5=92.685 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=22:06 IST
=> Training   75.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.918 Prec@1=76.720 Prec@5=92.685 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=22:06 IST
=> Training   75.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.918 Prec@1=76.720 Prec@5=92.685 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=22:07 IST
=> Training   75.95% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.919 Prec@1=76.708 Prec@5=92.675 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=22:07 IST
=> Training   79.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.919 Prec@1=76.708 Prec@5=92.675 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=22:07 IST
=> Training   79.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.919 Prec@1=76.708 Prec@5=92.675 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=22:08 IST
=> Training   79.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.920 Prec@1=76.689 Prec@5=92.664 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=22:08 IST
=> Training   83.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.920 Prec@1=76.689 Prec@5=92.664 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:08 IST
=> Training   83.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.920 Prec@1=76.689 Prec@5=92.664 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:09 IST
=> Training   83.94% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.921 Prec@1=76.665 Prec@5=92.646 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=22:09 IST
=> Training   87.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.921 Prec@1=76.665 Prec@5=92.646 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:09 IST
=> Training   87.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.560 DataTime=0.383 Loss=0.921 Prec@1=76.665 Prec@5=92.646 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:10 IST
=> Training   87.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.922 Prec@1=76.649 Prec@5=92.626 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=22:10 IST
=> Training   91.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.922 Prec@1=76.649 Prec@5=92.626 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:10 IST
=> Training   91.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.922 Prec@1=76.649 Prec@5=92.626 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:11 IST
=> Training   91.93% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.923 Prec@1=76.625 Prec@5=92.613 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=22:11 IST
=> Training   95.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.923 Prec@1=76.625 Prec@5=92.613 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=22:11 IST
=> Training   95.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.923 Prec@1=76.625 Prec@5=92.613 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=22:12 IST
=> Training   95.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.924 Prec@1=76.595 Prec@5=92.609 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=22:12 IST
=> Training   99.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.924 Prec@1=76.595 Prec@5=92.609 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=22:12 IST
=> Training   99.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.385 Loss=0.924 Prec@1=76.595 Prec@5=92.609 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=22:12 IST
=> Training   99.92% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.924 Prec@1=76.594 Prec@5=92.609 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=22:12 IST
=> Training   100.00% of 1x2503...Epoch=115/150 LR=0.0136 Time=0.561 DataTime=0.384 Loss=0.924 Prec@1=76.594 Prec@5=92.609 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=22:12 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:12 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:12 IST
=> Validation 0.00% of 1x98...Epoch=115/150 LR=0.0136 Time=6.816 Loss=0.761 Prec@1=82.031 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=22:12 IST
=> Validation 1.02% of 1x98...Epoch=115/150 LR=0.0136 Time=6.816 Loss=0.761 Prec@1=82.031 Prec@5=94.141 rate=6683.29 Hz, eta=0:00:00, total=0:00:00, wall=22:12 IST
** Validation 1.02% of 1x98...Epoch=115/150 LR=0.0136 Time=6.816 Loss=0.761 Prec@1=82.031 Prec@5=94.141 rate=6683.29 Hz, eta=0:00:00, total=0:00:00, wall=22:13 IST
** Validation 1.02% of 1x98...Epoch=115/150 LR=0.0136 Time=0.632 Loss=1.248 Prec@1=69.706 Prec@5=89.262 rate=6683.29 Hz, eta=0:00:00, total=0:00:00, wall=22:13 IST
** Validation 100.00% of 1x98...Epoch=115/150 LR=0.0136 Time=0.632 Loss=1.248 Prec@1=69.706 Prec@5=89.262 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=22:13 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:13 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:13 IST
=> Training   0.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=4.692 DataTime=4.289 Loss=0.933 Prec@1=75.977 Prec@5=91.992 rate=0 Hz, eta=?, total=0:00:00, wall=22:13 IST
=> Training   0.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=4.692 DataTime=4.289 Loss=0.933 Prec@1=75.977 Prec@5=91.992 rate=1458.07 Hz, eta=0:00:01, total=0:00:00, wall=22:13 IST
=> Training   0.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=4.692 DataTime=4.289 Loss=0.933 Prec@1=75.977 Prec@5=91.992 rate=1458.07 Hz, eta=0:00:01, total=0:00:00, wall=22:14 IST
=> Training   0.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.584 DataTime=0.414 Loss=0.891 Prec@1=77.502 Prec@5=92.978 rate=1458.07 Hz, eta=0:00:01, total=0:00:00, wall=22:14 IST
=> Training   4.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.584 DataTime=0.414 Loss=0.891 Prec@1=77.502 Prec@5=92.978 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=22:14 IST
=> Training   4.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.584 DataTime=0.414 Loss=0.891 Prec@1=77.502 Prec@5=92.978 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=22:15 IST
=> Training   4.04% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.567 DataTime=0.398 Loss=0.890 Prec@1=77.524 Prec@5=92.980 rate=1.86 Hz, eta=0:21:31, total=0:00:54, wall=22:15 IST
=> Training   8.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.567 DataTime=0.398 Loss=0.890 Prec@1=77.524 Prec@5=92.980 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=22:15 IST
=> Training   8.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.567 DataTime=0.398 Loss=0.890 Prec@1=77.524 Prec@5=92.980 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=22:16 IST
=> Training   8.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.574 DataTime=0.404 Loss=0.894 Prec@1=77.383 Prec@5=92.949 rate=1.84 Hz, eta=0:20:52, total=0:01:49, wall=22:16 IST
=> Training   12.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.574 DataTime=0.404 Loss=0.894 Prec@1=77.383 Prec@5=92.949 rate=1.79 Hz, eta=0:20:28, total=0:02:47, wall=22:16 IST
=> Training   12.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.574 DataTime=0.404 Loss=0.894 Prec@1=77.383 Prec@5=92.949 rate=1.79 Hz, eta=0:20:28, total=0:02:47, wall=22:17 IST
=> Training   12.03% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.398 Loss=0.892 Prec@1=77.383 Prec@5=92.975 rate=1.79 Hz, eta=0:20:28, total=0:02:47, wall=22:17 IST
=> Training   16.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.398 Loss=0.892 Prec@1=77.383 Prec@5=92.975 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=22:17 IST
=> Training   16.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.398 Loss=0.892 Prec@1=77.383 Prec@5=92.975 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=22:17 IST
=> Training   16.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.396 Loss=0.892 Prec@1=77.339 Prec@5=92.984 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=22:17 IST
=> Training   20.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.396 Loss=0.892 Prec@1=77.339 Prec@5=92.984 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=22:17 IST
=> Training   20.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.568 DataTime=0.396 Loss=0.892 Prec@1=77.339 Prec@5=92.984 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=22:18 IST
=> Training   20.02% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.895 Prec@1=77.299 Prec@5=92.963 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=22:18 IST
=> Training   24.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.895 Prec@1=77.299 Prec@5=92.963 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=22:18 IST
=> Training   24.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.895 Prec@1=77.299 Prec@5=92.963 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=22:19 IST
=> Training   24.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.566 DataTime=0.394 Loss=0.896 Prec@1=77.262 Prec@5=92.941 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=22:19 IST
=> Training   28.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.566 DataTime=0.394 Loss=0.896 Prec@1=77.262 Prec@5=92.941 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=22:19 IST
=> Training   28.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.566 DataTime=0.394 Loss=0.896 Prec@1=77.262 Prec@5=92.941 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=22:20 IST
=> Training   28.01% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.898 Prec@1=77.247 Prec@5=92.915 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=22:20 IST
=> Training   32.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.898 Prec@1=77.247 Prec@5=92.915 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=22:20 IST
=> Training   32.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.898 Prec@1=77.247 Prec@5=92.915 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=22:21 IST
=> Training   32.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.901 Prec@1=77.161 Prec@5=92.871 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=22:21 IST
=> Training   36.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.901 Prec@1=77.161 Prec@5=92.871 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=22:21 IST
=> Training   36.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.901 Prec@1=77.161 Prec@5=92.871 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=22:22 IST
=> Training   36.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.902 Prec@1=77.154 Prec@5=92.871 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=22:22 IST
=> Training   39.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.902 Prec@1=77.154 Prec@5=92.871 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:22 IST
=> Training   39.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.902 Prec@1=77.154 Prec@5=92.871 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:23 IST
=> Training   39.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.903 Prec@1=77.117 Prec@5=92.835 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=22:23 IST
=> Training   43.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.903 Prec@1=77.117 Prec@5=92.835 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=22:23 IST
=> Training   43.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.903 Prec@1=77.117 Prec@5=92.835 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=22:24 IST
=> Training   43.99% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.089 Prec@5=92.815 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=22:24 IST
=> Training   47.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.089 Prec@5=92.815 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:24 IST
=> Training   47.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.089 Prec@5=92.815 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:25 IST
=> Training   47.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.068 Prec@5=92.815 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=22:25 IST
=> Training   51.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.068 Prec@5=92.815 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=22:25 IST
=> Training   51.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.391 Loss=0.905 Prec@1=77.068 Prec@5=92.815 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=22:26 IST
=> Training   51.98% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.905 Prec@1=77.057 Prec@5=92.814 rate=1.78 Hz, eta=0:11:13, total=0:12:08, wall=22:26 IST
=> Training   55.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.905 Prec@1=77.057 Prec@5=92.814 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=22:26 IST
=> Training   55.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.564 DataTime=0.392 Loss=0.905 Prec@1=77.057 Prec@5=92.814 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=22:27 IST
=> Training   55.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.906 Prec@1=77.037 Prec@5=92.801 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=22:27 IST
=> Training   59.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.906 Prec@1=77.037 Prec@5=92.801 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=22:27 IST
=> Training   59.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.390 Loss=0.906 Prec@1=77.037 Prec@5=92.801 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=22:28 IST
=> Training   59.97% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.906 Prec@1=77.017 Prec@5=92.796 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=22:28 IST
=> Training   63.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.906 Prec@1=77.017 Prec@5=92.796 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=22:28 IST
=> Training   63.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.906 Prec@1=77.017 Prec@5=92.796 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=22:29 IST
=> Training   63.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.907 Prec@1=77.000 Prec@5=92.791 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=22:29 IST
=> Training   67.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.907 Prec@1=77.000 Prec@5=92.791 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=22:29 IST
=> Training   67.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.907 Prec@1=77.000 Prec@5=92.791 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=22:30 IST
=> Training   67.96% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.968 Prec@5=92.785 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=22:30 IST
=> Training   71.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.968 Prec@5=92.785 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=22:30 IST
=> Training   71.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.968 Prec@5=92.785 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=22:31 IST
=> Training   71.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.951 Prec@5=92.790 rate=1.79 Hz, eta=0:06:33, total=0:16:48, wall=22:31 IST
=> Training   75.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.951 Prec@5=92.790 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=22:31 IST
=> Training   75.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.908 Prec@1=76.951 Prec@5=92.790 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=22:31 IST
=> Training   75.95% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.562 DataTime=0.391 Loss=0.909 Prec@1=76.924 Prec@5=92.776 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=22:31 IST
=> Training   79.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.562 DataTime=0.391 Loss=0.909 Prec@1=76.924 Prec@5=92.776 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:31 IST
=> Training   79.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.562 DataTime=0.391 Loss=0.909 Prec@1=76.924 Prec@5=92.776 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:32 IST
=> Training   79.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.392 Loss=0.910 Prec@1=76.907 Prec@5=92.761 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=22:32 IST
=> Training   83.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.392 Loss=0.910 Prec@1=76.907 Prec@5=92.761 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=22:32 IST
=> Training   83.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.392 Loss=0.910 Prec@1=76.907 Prec@5=92.761 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=22:33 IST
=> Training   83.94% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.911 Prec@1=76.885 Prec@5=92.743 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=22:33 IST
=> Training   87.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.911 Prec@1=76.885 Prec@5=92.743 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=22:33 IST
=> Training   87.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.911 Prec@1=76.885 Prec@5=92.743 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=22:34 IST
=> Training   87.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.912 Prec@1=76.865 Prec@5=92.729 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=22:34 IST
=> Training   91.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.912 Prec@1=76.865 Prec@5=92.729 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=22:34 IST
=> Training   91.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.912 Prec@1=76.865 Prec@5=92.729 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=22:35 IST
=> Training   91.93% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.913 Prec@1=76.844 Prec@5=92.723 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=22:35 IST
=> Training   95.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.913 Prec@1=76.844 Prec@5=92.723 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:35 IST
=> Training   95.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.913 Prec@1=76.844 Prec@5=92.723 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:36 IST
=> Training   95.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.914 Prec@1=76.821 Prec@5=92.710 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=22:36 IST
=> Training   99.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.914 Prec@1=76.821 Prec@5=92.710 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=22:36 IST
=> Training   99.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.914 Prec@1=76.821 Prec@5=92.710 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=22:36 IST
=> Training   99.92% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.914 Prec@1=76.819 Prec@5=92.710 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=22:36 IST
=> Training   100.00% of 1x2503...Epoch=116/150 LR=0.0128 Time=0.563 DataTime=0.391 Loss=0.914 Prec@1=76.819 Prec@5=92.710 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=22:36 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:36 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=22:36 IST
=> Validation 0.00% of 1x98...Epoch=116/150 LR=0.0128 Time=6.926 Loss=0.762 Prec@1=79.492 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=22:36 IST
=> Validation 1.02% of 1x98...Epoch=116/150 LR=0.0128 Time=6.926 Loss=0.762 Prec@1=79.492 Prec@5=94.727 rate=5828.66 Hz, eta=0:00:00, total=0:00:00, wall=22:36 IST
** Validation 1.02% of 1x98...Epoch=116/150 LR=0.0128 Time=6.926 Loss=0.762 Prec@1=79.492 Prec@5=94.727 rate=5828.66 Hz, eta=0:00:00, total=0:00:00, wall=22:37 IST
** Validation 1.02% of 1x98...Epoch=116/150 LR=0.0128 Time=0.639 Loss=1.238 Prec@1=69.890 Prec@5=89.350 rate=5828.66 Hz, eta=0:00:00, total=0:00:00, wall=22:37 IST
** Validation 100.00% of 1x98...Epoch=116/150 LR=0.0128 Time=0.639 Loss=1.238 Prec@1=69.890 Prec@5=89.350 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=22:37 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:37 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=22:37 IST
=> Training   0.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=4.680 DataTime=4.391 Loss=0.848 Prec@1=76.953 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=22:37 IST
=> Training   0.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=4.680 DataTime=4.391 Loss=0.848 Prec@1=76.953 Prec@5=93.750 rate=1388.94 Hz, eta=0:00:01, total=0:00:00, wall=22:37 IST
=> Training   0.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=4.680 DataTime=4.391 Loss=0.848 Prec@1=76.953 Prec@5=93.750 rate=1388.94 Hz, eta=0:00:01, total=0:00:00, wall=22:38 IST
=> Training   0.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.597 DataTime=0.428 Loss=0.880 Prec@1=77.632 Prec@5=93.058 rate=1388.94 Hz, eta=0:00:01, total=0:00:00, wall=22:38 IST
=> Training   4.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.597 DataTime=0.428 Loss=0.880 Prec@1=77.632 Prec@5=93.058 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=22:38 IST
=> Training   4.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.597 DataTime=0.428 Loss=0.880 Prec@1=77.632 Prec@5=93.058 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=22:39 IST
=> Training   4.04% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.591 DataTime=0.421 Loss=0.880 Prec@1=77.645 Prec@5=93.101 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=22:39 IST
=> Training   8.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.591 DataTime=0.421 Loss=0.880 Prec@1=77.645 Prec@5=93.101 rate=1.76 Hz, eta=0:21:45, total=0:01:53, wall=22:39 IST
=> Training   8.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.591 DataTime=0.421 Loss=0.880 Prec@1=77.645 Prec@5=93.101 rate=1.76 Hz, eta=0:21:45, total=0:01:53, wall=22:40 IST
=> Training   8.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.580 DataTime=0.410 Loss=0.883 Prec@1=77.555 Prec@5=93.076 rate=1.76 Hz, eta=0:21:45, total=0:01:53, wall=22:40 IST
=> Training   12.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.580 DataTime=0.410 Loss=0.883 Prec@1=77.555 Prec@5=93.076 rate=1.77 Hz, eta=0:20:41, total=0:02:49, wall=22:40 IST
=> Training   12.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.580 DataTime=0.410 Loss=0.883 Prec@1=77.555 Prec@5=93.076 rate=1.77 Hz, eta=0:20:41, total=0:02:49, wall=22:41 IST
=> Training   12.03% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.578 DataTime=0.407 Loss=0.885 Prec@1=77.556 Prec@5=93.064 rate=1.77 Hz, eta=0:20:41, total=0:02:49, wall=22:41 IST
=> Training   16.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.578 DataTime=0.407 Loss=0.885 Prec@1=77.556 Prec@5=93.064 rate=1.77 Hz, eta=0:19:50, total=0:03:47, wall=22:41 IST
=> Training   16.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.578 DataTime=0.407 Loss=0.885 Prec@1=77.556 Prec@5=93.064 rate=1.77 Hz, eta=0:19:50, total=0:03:47, wall=22:42 IST
=> Training   16.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.574 DataTime=0.401 Loss=0.889 Prec@1=77.488 Prec@5=93.001 rate=1.77 Hz, eta=0:19:50, total=0:03:47, wall=22:42 IST
=> Training   20.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.574 DataTime=0.401 Loss=0.889 Prec@1=77.488 Prec@5=93.001 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=22:42 IST
=> Training   20.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.574 DataTime=0.401 Loss=0.889 Prec@1=77.488 Prec@5=93.001 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=22:43 IST
=> Training   20.02% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.572 DataTime=0.400 Loss=0.889 Prec@1=77.484 Prec@5=92.992 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=22:43 IST
=> Training   24.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.572 DataTime=0.400 Loss=0.889 Prec@1=77.484 Prec@5=92.992 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=22:43 IST
=> Training   24.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.572 DataTime=0.400 Loss=0.889 Prec@1=77.484 Prec@5=92.992 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=22:44 IST
=> Training   24.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.570 DataTime=0.398 Loss=0.891 Prec@1=77.450 Prec@5=92.966 rate=1.77 Hz, eta=0:17:53, total=0:05:39, wall=22:44 IST
=> Training   28.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.570 DataTime=0.398 Loss=0.891 Prec@1=77.450 Prec@5=92.966 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=22:44 IST
=> Training   28.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.570 DataTime=0.398 Loss=0.891 Prec@1=77.450 Prec@5=92.966 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=22:45 IST
=> Training   28.01% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.893 Prec@1=77.426 Prec@5=92.925 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=22:45 IST
=> Training   32.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.893 Prec@1=77.426 Prec@5=92.925 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=22:45 IST
=> Training   32.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.893 Prec@1=77.426 Prec@5=92.925 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=22:46 IST
=> Training   32.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.894 Prec@1=77.402 Prec@5=92.915 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=22:46 IST
=> Training   36.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.894 Prec@1=77.402 Prec@5=92.915 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=22:46 IST
=> Training   36.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.396 Loss=0.894 Prec@1=77.402 Prec@5=92.915 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=22:47 IST
=> Training   36.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.895 Prec@1=77.385 Prec@5=92.914 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=22:47 IST
=> Training   39.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.895 Prec@1=77.385 Prec@5=92.914 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=22:47 IST
=> Training   39.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.895 Prec@1=77.385 Prec@5=92.914 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=22:48 IST
=> Training   39.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.895 Prec@1=77.377 Prec@5=92.915 rate=1.78 Hz, eta=0:14:04, total=0:09:22, wall=22:48 IST
=> Training   43.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.895 Prec@1=77.377 Prec@5=92.915 rate=1.77 Hz, eta=0:13:10, total=0:10:20, wall=22:48 IST
=> Training   43.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.895 Prec@1=77.377 Prec@5=92.915 rate=1.77 Hz, eta=0:13:10, total=0:10:20, wall=22:49 IST
=> Training   43.99% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.896 Prec@1=77.358 Prec@5=92.913 rate=1.77 Hz, eta=0:13:10, total=0:10:20, wall=22:49 IST
=> Training   47.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.896 Prec@1=77.358 Prec@5=92.913 rate=1.78 Hz, eta=0:12:13, total=0:11:16, wall=22:49 IST
=> Training   47.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.896 Prec@1=77.358 Prec@5=92.913 rate=1.78 Hz, eta=0:12:13, total=0:11:16, wall=22:50 IST
=> Training   47.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.393 Loss=0.897 Prec@1=77.313 Prec@5=92.899 rate=1.78 Hz, eta=0:12:13, total=0:11:16, wall=22:50 IST
=> Training   51.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.393 Loss=0.897 Prec@1=77.313 Prec@5=92.899 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=22:50 IST
=> Training   51.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.393 Loss=0.897 Prec@1=77.313 Prec@5=92.899 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=22:50 IST
=> Training   51.98% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.898 Prec@1=77.301 Prec@5=92.887 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=22:50 IST
=> Training   55.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.898 Prec@1=77.301 Prec@5=92.887 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=22:50 IST
=> Training   55.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.898 Prec@1=77.301 Prec@5=92.887 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=22:51 IST
=> Training   55.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.394 Loss=0.898 Prec@1=77.275 Prec@5=92.892 rate=1.77 Hz, eta=0:10:21, total=0:13:10, wall=22:51 IST
=> Training   59.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.394 Loss=0.898 Prec@1=77.275 Prec@5=92.892 rate=1.77 Hz, eta=0:09:25, total=0:14:07, wall=22:51 IST
=> Training   59.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.394 Loss=0.898 Prec@1=77.275 Prec@5=92.892 rate=1.77 Hz, eta=0:09:25, total=0:14:07, wall=22:52 IST
=> Training   59.97% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.899 Prec@1=77.250 Prec@5=92.882 rate=1.77 Hz, eta=0:09:25, total=0:14:07, wall=22:52 IST
=> Training   63.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.899 Prec@1=77.250 Prec@5=92.882 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=22:52 IST
=> Training   63.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.899 Prec@1=77.250 Prec@5=92.882 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=22:53 IST
=> Training   63.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.901 Prec@1=77.214 Prec@5=92.862 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=22:53 IST
=> Training   67.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.901 Prec@1=77.214 Prec@5=92.862 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=22:53 IST
=> Training   67.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.901 Prec@1=77.214 Prec@5=92.862 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=22:54 IST
=> Training   67.96% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.902 Prec@1=77.192 Prec@5=92.850 rate=1.77 Hz, eta=0:07:32, total=0:16:00, wall=22:54 IST
=> Training   71.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.902 Prec@1=77.192 Prec@5=92.850 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=22:54 IST
=> Training   71.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.902 Prec@1=77.192 Prec@5=92.850 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=22:55 IST
=> Training   71.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.168 Prec@5=92.837 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=22:55 IST
=> Training   75.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.168 Prec@5=92.837 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=22:55 IST
=> Training   75.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.168 Prec@5=92.837 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=22:56 IST
=> Training   75.95% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.903 Prec@1=77.151 Prec@5=92.838 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=22:56 IST
=> Training   79.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.903 Prec@1=77.151 Prec@5=92.838 rate=1.77 Hz, eta=0:04:43, total=0:18:51, wall=22:56 IST
=> Training   79.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.568 DataTime=0.395 Loss=0.903 Prec@1=77.151 Prec@5=92.838 rate=1.77 Hz, eta=0:04:43, total=0:18:51, wall=22:57 IST
=> Training   79.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.163 Prec@5=92.841 rate=1.77 Hz, eta=0:04:43, total=0:18:51, wall=22:57 IST
=> Training   83.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.163 Prec@5=92.841 rate=1.77 Hz, eta=0:03:47, total=0:19:46, wall=22:57 IST
=> Training   83.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.163 Prec@5=92.841 rate=1.77 Hz, eta=0:03:47, total=0:19:46, wall=22:58 IST
=> Training   83.94% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.149 Prec@5=92.834 rate=1.77 Hz, eta=0:03:47, total=0:19:46, wall=22:58 IST
=> Training   87.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.149 Prec@5=92.834 rate=1.77 Hz, eta=0:02:50, total=0:20:43, wall=22:58 IST
=> Training   87.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.903 Prec@1=77.149 Prec@5=92.834 rate=1.77 Hz, eta=0:02:50, total=0:20:43, wall=22:59 IST
=> Training   87.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.904 Prec@1=77.136 Prec@5=92.822 rate=1.77 Hz, eta=0:02:50, total=0:20:43, wall=22:59 IST
=> Training   91.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.904 Prec@1=77.136 Prec@5=92.822 rate=1.77 Hz, eta=0:01:54, total=0:21:39, wall=22:59 IST
=> Training   91.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.904 Prec@1=77.136 Prec@5=92.822 rate=1.77 Hz, eta=0:01:54, total=0:21:39, wall=23:00 IST
=> Training   91.93% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.905 Prec@1=77.112 Prec@5=92.812 rate=1.77 Hz, eta=0:01:54, total=0:21:39, wall=23:00 IST
=> Training   95.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.905 Prec@1=77.112 Prec@5=92.812 rate=1.77 Hz, eta=0:00:57, total=0:22:37, wall=23:00 IST
=> Training   95.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.394 Loss=0.905 Prec@1=77.112 Prec@5=92.812 rate=1.77 Hz, eta=0:00:57, total=0:22:37, wall=23:01 IST
=> Training   95.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.905 Prec@1=77.103 Prec@5=92.808 rate=1.77 Hz, eta=0:00:57, total=0:22:37, wall=23:01 IST
=> Training   99.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.905 Prec@1=77.103 Prec@5=92.808 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=23:01 IST
=> Training   99.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.567 DataTime=0.393 Loss=0.905 Prec@1=77.103 Prec@5=92.808 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=23:01 IST
=> Training   99.92% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.566 DataTime=0.393 Loss=0.905 Prec@1=77.102 Prec@5=92.808 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=23:01 IST
=> Training   100.00% of 1x2503...Epoch=117/150 LR=0.0122 Time=0.566 DataTime=0.393 Loss=0.905 Prec@1=77.102 Prec@5=92.808 rate=1.77 Hz, eta=0:00:00, total=0:23:32, wall=23:01 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST
=> Validation 0.00% of 1x98...Epoch=117/150 LR=0.0122 Time=7.076 Loss=0.813 Prec@1=78.320 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=23:01 IST
=> Validation 1.02% of 1x98...Epoch=117/150 LR=0.0122 Time=7.076 Loss=0.813 Prec@1=78.320 Prec@5=94.727 rate=7217.98 Hz, eta=0:00:00, total=0:00:00, wall=23:01 IST
** Validation 1.02% of 1x98...Epoch=117/150 LR=0.0122 Time=7.076 Loss=0.813 Prec@1=78.320 Prec@5=94.727 rate=7217.98 Hz, eta=0:00:00, total=0:00:00, wall=23:02 IST
** Validation 1.02% of 1x98...Epoch=117/150 LR=0.0122 Time=0.636 Loss=1.231 Prec@1=70.026 Prec@5=89.342 rate=7217.98 Hz, eta=0:00:00, total=0:00:00, wall=23:02 IST
** Validation 100.00% of 1x98...Epoch=117/150 LR=0.0122 Time=0.636 Loss=1.231 Prec@1=70.026 Prec@5=89.342 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=23:02 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:02 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:02 IST
=> Training   0.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=4.969 DataTime=4.547 Loss=0.838 Prec@1=77.930 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=23:02 IST
=> Training   0.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=4.969 DataTime=4.547 Loss=0.838 Prec@1=77.930 Prec@5=92.578 rate=7366.59 Hz, eta=0:00:00, total=0:00:00, wall=23:02 IST
=> Training   0.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=4.969 DataTime=4.547 Loss=0.838 Prec@1=77.930 Prec@5=92.578 rate=7366.59 Hz, eta=0:00:00, total=0:00:00, wall=23:03 IST
=> Training   0.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.600 DataTime=0.443 Loss=0.858 Prec@1=78.264 Prec@5=93.456 rate=7366.59 Hz, eta=0:00:00, total=0:00:00, wall=23:03 IST
=> Training   4.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.600 DataTime=0.443 Loss=0.858 Prec@1=78.264 Prec@5=93.456 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=23:03 IST
=> Training   4.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.600 DataTime=0.443 Loss=0.858 Prec@1=78.264 Prec@5=93.456 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=23:04 IST
=> Training   4.04% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.587 DataTime=0.424 Loss=0.864 Prec@1=78.078 Prec@5=93.322 rate=1.82 Hz, eta=0:22:02, total=0:00:55, wall=23:04 IST
=> Training   8.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.587 DataTime=0.424 Loss=0.864 Prec@1=78.078 Prec@5=93.322 rate=1.78 Hz, eta=0:21:33, total=0:01:52, wall=23:04 IST
=> Training   8.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.587 DataTime=0.424 Loss=0.864 Prec@1=78.078 Prec@5=93.322 rate=1.78 Hz, eta=0:21:33, total=0:01:52, wall=23:05 IST
=> Training   8.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.579 DataTime=0.414 Loss=0.866 Prec@1=78.020 Prec@5=93.299 rate=1.78 Hz, eta=0:21:33, total=0:01:52, wall=23:05 IST
=> Training   12.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.579 DataTime=0.414 Loss=0.866 Prec@1=78.020 Prec@5=93.299 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=23:05 IST
=> Training   12.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.579 DataTime=0.414 Loss=0.866 Prec@1=78.020 Prec@5=93.299 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=23:06 IST
=> Training   12.03% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.577 DataTime=0.409 Loss=0.870 Prec@1=77.943 Prec@5=93.208 rate=1.78 Hz, eta=0:20:39, total=0:02:49, wall=23:06 IST
=> Training   16.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.577 DataTime=0.409 Loss=0.870 Prec@1=77.943 Prec@5=93.208 rate=1.77 Hz, eta=0:19:47, total=0:03:46, wall=23:06 IST
=> Training   16.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.577 DataTime=0.409 Loss=0.870 Prec@1=77.943 Prec@5=93.208 rate=1.77 Hz, eta=0:19:47, total=0:03:46, wall=23:07 IST
=> Training   16.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.571 DataTime=0.402 Loss=0.871 Prec@1=77.879 Prec@5=93.193 rate=1.77 Hz, eta=0:19:47, total=0:03:46, wall=23:07 IST
=> Training   20.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.571 DataTime=0.402 Loss=0.871 Prec@1=77.879 Prec@5=93.193 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=23:07 IST
=> Training   20.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.571 DataTime=0.402 Loss=0.871 Prec@1=77.879 Prec@5=93.193 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=23:08 IST
=> Training   20.02% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.567 DataTime=0.397 Loss=0.873 Prec@1=77.847 Prec@5=93.186 rate=1.78 Hz, eta=0:18:44, total=0:04:41, wall=23:08 IST
=> Training   24.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.567 DataTime=0.397 Loss=0.873 Prec@1=77.847 Prec@5=93.186 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=23:08 IST
=> Training   24.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.567 DataTime=0.397 Loss=0.873 Prec@1=77.847 Prec@5=93.186 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=23:09 IST
=> Training   24.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.566 DataTime=0.397 Loss=0.875 Prec@1=77.802 Prec@5=93.160 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=23:09 IST
=> Training   28.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.566 DataTime=0.397 Loss=0.875 Prec@1=77.802 Prec@5=93.160 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=23:09 IST
=> Training   28.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.566 DataTime=0.397 Loss=0.875 Prec@1=77.802 Prec@5=93.160 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=23:09 IST
=> Training   28.01% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.395 Loss=0.877 Prec@1=77.757 Prec@5=93.132 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=23:09 IST
=> Training   32.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.395 Loss=0.877 Prec@1=77.757 Prec@5=93.132 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=23:09 IST
=> Training   32.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.395 Loss=0.877 Prec@1=77.757 Prec@5=93.132 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=23:10 IST
=> Training   32.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.879 Prec@1=77.704 Prec@5=93.114 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=23:10 IST
=> Training   36.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.879 Prec@1=77.704 Prec@5=93.114 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:10 IST
=> Training   36.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.879 Prec@1=77.704 Prec@5=93.114 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:11 IST
=> Training   36.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.393 Loss=0.880 Prec@1=77.658 Prec@5=93.093 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=23:11 IST
=> Training   39.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.393 Loss=0.880 Prec@1=77.658 Prec@5=93.093 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=23:11 IST
=> Training   39.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.393 Loss=0.880 Prec@1=77.658 Prec@5=93.093 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=23:12 IST
=> Training   39.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.882 Prec@1=77.617 Prec@5=93.088 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=23:12 IST
=> Training   43.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.882 Prec@1=77.617 Prec@5=93.088 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:12 IST
=> Training   43.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.565 DataTime=0.394 Loss=0.882 Prec@1=77.617 Prec@5=93.088 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:13 IST
=> Training   43.99% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.564 DataTime=0.392 Loss=0.882 Prec@1=77.615 Prec@5=93.081 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:13 IST
=> Training   47.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.564 DataTime=0.392 Loss=0.882 Prec@1=77.615 Prec@5=93.081 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=23:13 IST
=> Training   47.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.564 DataTime=0.392 Loss=0.882 Prec@1=77.615 Prec@5=93.081 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=23:14 IST
=> Training   47.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.392 Loss=0.883 Prec@1=77.580 Prec@5=93.059 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=23:14 IST
=> Training   51.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.392 Loss=0.883 Prec@1=77.580 Prec@5=93.059 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:14 IST
=> Training   51.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.392 Loss=0.883 Prec@1=77.580 Prec@5=93.059 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:15 IST
=> Training   51.98% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.884 Prec@1=77.574 Prec@5=93.050 rate=1.79 Hz, eta=0:11:12, total=0:12:07, wall=23:15 IST
=> Training   55.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.884 Prec@1=77.574 Prec@5=93.050 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:15 IST
=> Training   55.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.884 Prec@1=77.574 Prec@5=93.050 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:16 IST
=> Training   55.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.885 Prec@1=77.536 Prec@5=93.029 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=23:16 IST
=> Training   59.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.885 Prec@1=77.536 Prec@5=93.029 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:16 IST
=> Training   59.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.563 DataTime=0.391 Loss=0.885 Prec@1=77.536 Prec@5=93.029 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:17 IST
=> Training   59.97% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.886 Prec@1=77.506 Prec@5=93.021 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=23:17 IST
=> Training   63.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.886 Prec@1=77.506 Prec@5=93.021 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=23:17 IST
=> Training   63.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.886 Prec@1=77.506 Prec@5=93.021 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=23:18 IST
=> Training   63.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.888 Prec@1=77.476 Prec@5=93.007 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=23:18 IST
=> Training   67.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.888 Prec@1=77.476 Prec@5=93.007 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=23:18 IST
=> Training   67.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.391 Loss=0.888 Prec@1=77.476 Prec@5=93.007 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=23:19 IST
=> Training   67.96% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.888 Prec@1=77.459 Prec@5=92.997 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=23:19 IST
=> Training   71.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.888 Prec@1=77.459 Prec@5=92.997 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=23:19 IST
=> Training   71.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.888 Prec@1=77.459 Prec@5=92.997 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=23:20 IST
=> Training   71.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.889 Prec@1=77.434 Prec@5=92.985 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=23:20 IST
=> Training   75.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.889 Prec@1=77.434 Prec@5=92.985 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=23:20 IST
=> Training   75.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.889 Prec@1=77.434 Prec@5=92.985 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=23:21 IST
=> Training   75.95% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.890 Prec@1=77.408 Prec@5=92.969 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=23:21 IST
=> Training   79.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.890 Prec@1=77.408 Prec@5=92.969 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=23:21 IST
=> Training   79.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.562 DataTime=0.389 Loss=0.890 Prec@1=77.408 Prec@5=92.969 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=23:22 IST
=> Training   79.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.373 Prec@5=92.954 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=23:22 IST
=> Training   83.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.373 Prec@5=92.954 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=23:22 IST
=> Training   83.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.373 Prec@5=92.954 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=23:23 IST
=> Training   83.94% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.359 Prec@5=92.945 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=23:23 IST
=> Training   87.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.359 Prec@5=92.945 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=23:23 IST
=> Training   87.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.892 Prec@1=77.359 Prec@5=92.945 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=23:23 IST
=> Training   87.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.893 Prec@1=77.333 Prec@5=92.931 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=23:23 IST
=> Training   91.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.893 Prec@1=77.333 Prec@5=92.931 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=23:23 IST
=> Training   91.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.893 Prec@1=77.333 Prec@5=92.931 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=23:24 IST
=> Training   91.93% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.894 Prec@1=77.316 Prec@5=92.922 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=23:24 IST
=> Training   95.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.894 Prec@1=77.316 Prec@5=92.922 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=23:24 IST
=> Training   95.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.894 Prec@1=77.316 Prec@5=92.922 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=23:25 IST
=> Training   95.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.895 Prec@1=77.298 Prec@5=92.915 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=23:25 IST
=> Training   99.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.895 Prec@1=77.298 Prec@5=92.915 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=23:25 IST
=> Training   99.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.895 Prec@1=77.298 Prec@5=92.915 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=23:25 IST
=> Training   99.92% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.895 Prec@1=77.297 Prec@5=92.916 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=23:25 IST
=> Training   100.00% of 1x2503...Epoch=118/150 LR=0.0115 Time=0.561 DataTime=0.389 Loss=0.895 Prec@1=77.297 Prec@5=92.916 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=23:25 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> Validation 0.00% of 1x98...Epoch=118/150 LR=0.0115 Time=6.886 Loss=0.786 Prec@1=79.297 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=23:25 IST
=> Validation 1.02% of 1x98...Epoch=118/150 LR=0.0115 Time=6.886 Loss=0.786 Prec@1=79.297 Prec@5=94.141 rate=4927.90 Hz, eta=0:00:00, total=0:00:00, wall=23:25 IST
** Validation 1.02% of 1x98...Epoch=118/150 LR=0.0115 Time=6.886 Loss=0.786 Prec@1=79.297 Prec@5=94.141 rate=4927.90 Hz, eta=0:00:00, total=0:00:00, wall=23:26 IST
** Validation 1.02% of 1x98...Epoch=118/150 LR=0.0115 Time=0.638 Loss=1.244 Prec@1=69.874 Prec@5=89.366 rate=4927.90 Hz, eta=0:00:00, total=0:00:00, wall=23:26 IST
** Validation 100.00% of 1x98...Epoch=118/150 LR=0.0115 Time=0.638 Loss=1.244 Prec@1=69.874 Prec@5=89.366 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=23:26 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> Training   0.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=4.577 DataTime=4.072 Loss=0.902 Prec@1=77.930 Prec@5=92.188 rate=0 Hz, eta=?, total=0:00:00, wall=23:26 IST
=> Training   0.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=4.577 DataTime=4.072 Loss=0.902 Prec@1=77.930 Prec@5=92.188 rate=2225.96 Hz, eta=0:00:01, total=0:00:00, wall=23:26 IST
=> Training   0.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=4.577 DataTime=4.072 Loss=0.902 Prec@1=77.930 Prec@5=92.188 rate=2225.96 Hz, eta=0:00:01, total=0:00:00, wall=23:27 IST
=> Training   0.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.580 DataTime=0.406 Loss=0.866 Prec@1=78.013 Prec@5=93.361 rate=2225.96 Hz, eta=0:00:01, total=0:00:00, wall=23:27 IST
=> Training   4.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.580 DataTime=0.406 Loss=0.866 Prec@1=78.013 Prec@5=93.361 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=23:27 IST
=> Training   4.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.580 DataTime=0.406 Loss=0.866 Prec@1=78.013 Prec@5=93.361 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=23:28 IST
=> Training   4.04% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.574 DataTime=0.398 Loss=0.862 Prec@1=78.074 Prec@5=93.352 rate=1.87 Hz, eta=0:21:25, total=0:00:54, wall=23:28 IST
=> Training   8.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.574 DataTime=0.398 Loss=0.862 Prec@1=78.074 Prec@5=93.352 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=23:28 IST
=> Training   8.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.574 DataTime=0.398 Loss=0.862 Prec@1=78.074 Prec@5=93.352 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=23:29 IST
=> Training   8.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.396 Loss=0.862 Prec@1=78.109 Prec@5=93.370 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=23:29 IST
=> Training   12.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.396 Loss=0.862 Prec@1=78.109 Prec@5=93.370 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=23:29 IST
=> Training   12.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.396 Loss=0.862 Prec@1=78.109 Prec@5=93.370 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=23:30 IST
=> Training   12.03% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.572 DataTime=0.399 Loss=0.863 Prec@1=78.077 Prec@5=93.363 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=23:30 IST
=> Training   16.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.572 DataTime=0.399 Loss=0.863 Prec@1=78.077 Prec@5=93.363 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=23:30 IST
=> Training   16.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.572 DataTime=0.399 Loss=0.863 Prec@1=78.077 Prec@5=93.363 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=23:31 IST
=> Training   16.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.398 Loss=0.864 Prec@1=78.063 Prec@5=93.325 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=23:31 IST
=> Training   20.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.398 Loss=0.864 Prec@1=78.063 Prec@5=93.325 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=23:31 IST
=> Training   20.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.398 Loss=0.864 Prec@1=78.063 Prec@5=93.325 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=23:32 IST
=> Training   20.02% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.397 Loss=0.866 Prec@1=78.047 Prec@5=93.299 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=23:32 IST
=> Training   24.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.397 Loss=0.866 Prec@1=78.047 Prec@5=93.299 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=23:32 IST
=> Training   24.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.570 DataTime=0.397 Loss=0.866 Prec@1=78.047 Prec@5=93.299 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=23:33 IST
=> Training   24.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.396 Loss=0.869 Prec@1=78.003 Prec@5=93.265 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=23:33 IST
=> Training   28.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.396 Loss=0.869 Prec@1=78.003 Prec@5=93.265 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=23:33 IST
=> Training   28.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.569 DataTime=0.396 Loss=0.869 Prec@1=78.003 Prec@5=93.265 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=23:34 IST
=> Training   28.01% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.568 DataTime=0.394 Loss=0.869 Prec@1=78.001 Prec@5=93.260 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=23:34 IST
=> Training   32.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.568 DataTime=0.394 Loss=0.869 Prec@1=78.001 Prec@5=93.260 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:34 IST
=> Training   32.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.568 DataTime=0.394 Loss=0.869 Prec@1=78.001 Prec@5=93.260 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:35 IST
=> Training   32.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.567 DataTime=0.392 Loss=0.870 Prec@1=77.971 Prec@5=93.233 rate=1.78 Hz, eta=0:15:57, total=0:07:30, wall=23:35 IST
=> Training   36.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.567 DataTime=0.392 Loss=0.870 Prec@1=77.971 Prec@5=93.233 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=23:35 IST
=> Training   36.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.567 DataTime=0.392 Loss=0.870 Prec@1=77.971 Prec@5=93.233 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=23:36 IST
=> Training   36.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.390 Loss=0.871 Prec@1=77.950 Prec@5=93.230 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=23:36 IST
=> Training   39.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.390 Loss=0.871 Prec@1=77.950 Prec@5=93.230 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=23:36 IST
=> Training   39.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.390 Loss=0.871 Prec@1=77.950 Prec@5=93.230 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=23:37 IST
=> Training   39.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.565 DataTime=0.390 Loss=0.873 Prec@1=77.915 Prec@5=93.205 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=23:37 IST
=> Training   43.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.565 DataTime=0.390 Loss=0.873 Prec@1=77.915 Prec@5=93.205 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:37 IST
=> Training   43.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.565 DataTime=0.390 Loss=0.873 Prec@1=77.915 Prec@5=93.205 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:38 IST
=> Training   43.99% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.391 Loss=0.874 Prec@1=77.864 Prec@5=93.175 rate=1.78 Hz, eta=0:13:06, total=0:10:17, wall=23:38 IST
=> Training   47.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.391 Loss=0.874 Prec@1=77.864 Prec@5=93.175 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=23:38 IST
=> Training   47.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.566 DataTime=0.391 Loss=0.874 Prec@1=77.864 Prec@5=93.175 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=23:39 IST
=> Training   47.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.875 Prec@1=77.850 Prec@5=93.179 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=23:39 IST
=> Training   51.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.875 Prec@1=77.850 Prec@5=93.179 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=23:39 IST
=> Training   51.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.875 Prec@1=77.850 Prec@5=93.179 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=23:40 IST
=> Training   51.98% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.876 Prec@1=77.811 Prec@5=93.161 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=23:40 IST
=> Training   55.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.876 Prec@1=77.811 Prec@5=93.161 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=23:40 IST
=> Training   55.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.564 DataTime=0.389 Loss=0.876 Prec@1=77.811 Prec@5=93.161 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=23:40 IST
=> Training   55.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.877 Prec@1=77.788 Prec@5=93.153 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=23:40 IST
=> Training   59.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.877 Prec@1=77.788 Prec@5=93.153 rate=1.78 Hz, eta=0:09:21, total=0:14:00, wall=23:40 IST
=> Training   59.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.877 Prec@1=77.788 Prec@5=93.153 rate=1.78 Hz, eta=0:09:21, total=0:14:00, wall=23:41 IST
=> Training   59.97% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.878 Prec@1=77.752 Prec@5=93.140 rate=1.78 Hz, eta=0:09:21, total=0:14:00, wall=23:41 IST
=> Training   63.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.878 Prec@1=77.752 Prec@5=93.140 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:41 IST
=> Training   63.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.878 Prec@1=77.752 Prec@5=93.140 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:42 IST
=> Training   63.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.709 Prec@5=93.117 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=23:42 IST
=> Training   67.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.709 Prec@5=93.117 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=23:42 IST
=> Training   67.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.709 Prec@5=93.117 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=23:43 IST
=> Training   67.96% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.692 Prec@5=93.111 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=23:43 IST
=> Training   71.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.692 Prec@5=93.111 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=23:43 IST
=> Training   71.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.880 Prec@1=77.692 Prec@5=93.111 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=23:44 IST
=> Training   71.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.881 Prec@1=77.671 Prec@5=93.101 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=23:44 IST
=> Training   75.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.881 Prec@1=77.671 Prec@5=93.101 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=23:44 IST
=> Training   75.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.881 Prec@1=77.671 Prec@5=93.101 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=23:45 IST
=> Training   75.95% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.882 Prec@1=77.647 Prec@5=93.085 rate=1.78 Hz, eta=0:05:37, total=0:17:45, wall=23:45 IST
=> Training   79.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.882 Prec@1=77.647 Prec@5=93.085 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=23:45 IST
=> Training   79.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.882 Prec@1=77.647 Prec@5=93.085 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=23:46 IST
=> Training   79.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.624 Prec@5=93.077 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=23:46 IST
=> Training   83.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.624 Prec@5=93.077 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=23:46 IST
=> Training   83.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.624 Prec@5=93.077 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=23:47 IST
=> Training   83.94% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.612 Prec@5=93.070 rate=1.78 Hz, eta=0:03:45, total=0:19:38, wall=23:47 IST
=> Training   87.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.612 Prec@5=93.070 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:47 IST
=> Training   87.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.883 Prec@1=77.612 Prec@5=93.070 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:48 IST
=> Training   87.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.884 Prec@1=77.594 Prec@5=93.063 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=23:48 IST
=> Training   91.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.884 Prec@1=77.594 Prec@5=93.063 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=23:48 IST
=> Training   91.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.884 Prec@1=77.594 Prec@5=93.063 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=23:49 IST
=> Training   91.93% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.389 Loss=0.885 Prec@1=77.573 Prec@5=93.055 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=23:49 IST
=> Training   95.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.389 Loss=0.885 Prec@1=77.573 Prec@5=93.055 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=23:49 IST
=> Training   95.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.389 Loss=0.885 Prec@1=77.573 Prec@5=93.055 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=23:50 IST
=> Training   95.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.886 Prec@1=77.558 Prec@5=93.051 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=23:50 IST
=> Training   99.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.886 Prec@1=77.558 Prec@5=93.051 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=23:50 IST
=> Training   99.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.886 Prec@1=77.558 Prec@5=93.051 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=23:50 IST
=> Training   99.92% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.886 Prec@1=77.557 Prec@5=93.051 rate=1.78 Hz, eta=0:00:01, total=0:23:22, wall=23:50 IST
=> Training   100.00% of 1x2503...Epoch=119/150 LR=0.0108 Time=0.563 DataTime=0.388 Loss=0.886 Prec@1=77.557 Prec@5=93.051 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=23:50 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:50 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=23:50 IST
=> Validation 0.00% of 1x98...Epoch=119/150 LR=0.0108 Time=7.207 Loss=0.739 Prec@1=80.273 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=23:50 IST
=> Validation 1.02% of 1x98...Epoch=119/150 LR=0.0108 Time=7.207 Loss=0.739 Prec@1=80.273 Prec@5=94.141 rate=7044.14 Hz, eta=0:00:00, total=0:00:00, wall=23:50 IST
** Validation 1.02% of 1x98...Epoch=119/150 LR=0.0108 Time=7.207 Loss=0.739 Prec@1=80.273 Prec@5=94.141 rate=7044.14 Hz, eta=0:00:00, total=0:00:00, wall=23:51 IST
** Validation 1.02% of 1x98...Epoch=119/150 LR=0.0108 Time=0.632 Loss=1.232 Prec@1=70.230 Prec@5=89.486 rate=7044.14 Hz, eta=0:00:00, total=0:00:00, wall=23:51 IST
** Validation 100.00% of 1x98...Epoch=119/150 LR=0.0108 Time=0.632 Loss=1.232 Prec@1=70.230 Prec@5=89.486 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=23:51 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:51 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=23:51 IST
=> Training   0.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=4.369 DataTime=3.893 Loss=0.944 Prec@1=76.367 Prec@5=92.578 rate=0 Hz, eta=?, total=0:00:00, wall=23:51 IST
=> Training   0.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=4.369 DataTime=3.893 Loss=0.944 Prec@1=76.367 Prec@5=92.578 rate=5712.55 Hz, eta=0:00:00, total=0:00:00, wall=23:51 IST
=> Training   0.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=4.369 DataTime=3.893 Loss=0.944 Prec@1=76.367 Prec@5=92.578 rate=5712.55 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST
=> Training   0.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.596 DataTime=0.425 Loss=0.856 Prec@1=78.438 Prec@5=93.388 rate=5712.55 Hz, eta=0:00:00, total=0:00:00, wall=23:52 IST
=> Training   4.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.596 DataTime=0.425 Loss=0.856 Prec@1=78.438 Prec@5=93.388 rate=1.81 Hz, eta=0:22:07, total=0:00:55, wall=23:52 IST
=> Training   4.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.596 DataTime=0.425 Loss=0.856 Prec@1=78.438 Prec@5=93.388 rate=1.81 Hz, eta=0:22:07, total=0:00:55, wall=23:53 IST
=> Training   4.04% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.585 DataTime=0.412 Loss=0.852 Prec@1=78.426 Prec@5=93.490 rate=1.81 Hz, eta=0:22:07, total=0:00:55, wall=23:53 IST
=> Training   8.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.585 DataTime=0.412 Loss=0.852 Prec@1=78.426 Prec@5=93.490 rate=1.78 Hz, eta=0:21:36, total=0:01:53, wall=23:53 IST
=> Training   8.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.585 DataTime=0.412 Loss=0.852 Prec@1=78.426 Prec@5=93.490 rate=1.78 Hz, eta=0:21:36, total=0:01:53, wall=23:54 IST
=> Training   8.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.581 DataTime=0.408 Loss=0.852 Prec@1=78.416 Prec@5=93.467 rate=1.78 Hz, eta=0:21:36, total=0:01:53, wall=23:54 IST
=> Training   12.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.581 DataTime=0.408 Loss=0.852 Prec@1=78.416 Prec@5=93.467 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=23:54 IST
=> Training   12.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.581 DataTime=0.408 Loss=0.852 Prec@1=78.416 Prec@5=93.467 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=23:55 IST
=> Training   12.03% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.577 DataTime=0.405 Loss=0.855 Prec@1=78.330 Prec@5=93.399 rate=1.77 Hz, eta=0:20:46, total=0:02:50, wall=23:55 IST
=> Training   16.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.577 DataTime=0.405 Loss=0.855 Prec@1=78.330 Prec@5=93.399 rate=1.77 Hz, eta=0:19:49, total=0:03:46, wall=23:55 IST
=> Training   16.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.577 DataTime=0.405 Loss=0.855 Prec@1=78.330 Prec@5=93.399 rate=1.77 Hz, eta=0:19:49, total=0:03:46, wall=23:56 IST
=> Training   16.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.574 DataTime=0.401 Loss=0.858 Prec@1=78.279 Prec@5=93.375 rate=1.77 Hz, eta=0:19:49, total=0:03:46, wall=23:56 IST
=> Training   20.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.574 DataTime=0.401 Loss=0.858 Prec@1=78.279 Prec@5=93.375 rate=1.77 Hz, eta=0:18:51, total=0:04:43, wall=23:56 IST
=> Training   20.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.574 DataTime=0.401 Loss=0.858 Prec@1=78.279 Prec@5=93.375 rate=1.77 Hz, eta=0:18:51, total=0:04:43, wall=23:57 IST
=> Training   20.02% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.569 DataTime=0.396 Loss=0.858 Prec@1=78.271 Prec@5=93.364 rate=1.77 Hz, eta=0:18:51, total=0:04:43, wall=23:57 IST
=> Training   24.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.569 DataTime=0.396 Loss=0.858 Prec@1=78.271 Prec@5=93.364 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=23:57 IST
=> Training   24.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.569 DataTime=0.396 Loss=0.858 Prec@1=78.271 Prec@5=93.364 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=23:58 IST
=> Training   24.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.568 DataTime=0.395 Loss=0.861 Prec@1=78.216 Prec@5=93.341 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=23:58 IST
=> Training   28.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.568 DataTime=0.395 Loss=0.861 Prec@1=78.216 Prec@5=93.341 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=23:58 IST
=> Training   28.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.568 DataTime=0.395 Loss=0.861 Prec@1=78.216 Prec@5=93.341 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=23:58 IST
=> Training   28.01% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.393 Loss=0.862 Prec@1=78.155 Prec@5=93.311 rate=1.78 Hz, eta=0:16:52, total=0:06:33, wall=23:58 IST
=> Training   32.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.393 Loss=0.862 Prec@1=78.155 Prec@5=93.311 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=23:58 IST
=> Training   32.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.393 Loss=0.862 Prec@1=78.155 Prec@5=93.311 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=23:59 IST
=> Training   32.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.395 Loss=0.863 Prec@1=78.161 Prec@5=93.299 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=23:59 IST
=> Training   36.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.395 Loss=0.863 Prec@1=78.161 Prec@5=93.299 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=23:59 IST
=> Training   36.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.395 Loss=0.863 Prec@1=78.161 Prec@5=93.299 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=00:00 IST
=> Training   36.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.566 DataTime=0.394 Loss=0.863 Prec@1=78.186 Prec@5=93.293 rate=1.78 Hz, eta=0:15:01, total=0:08:26, wall=00:00 IST
=> Training   39.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.566 DataTime=0.394 Loss=0.863 Prec@1=78.186 Prec@5=93.293 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=00:00 IST
=> Training   39.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.566 DataTime=0.394 Loss=0.863 Prec@1=78.186 Prec@5=93.293 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=00:01 IST
=> Training   39.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.394 Loss=0.863 Prec@1=78.189 Prec@5=93.289 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=00:01 IST
=> Training   43.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.394 Loss=0.863 Prec@1=78.189 Prec@5=93.289 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:01 IST
=> Training   43.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.567 DataTime=0.394 Loss=0.863 Prec@1=78.189 Prec@5=93.289 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:02 IST
=> Training   43.99% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.391 Loss=0.864 Prec@1=78.152 Prec@5=93.274 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=00:02 IST
=> Training   47.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.391 Loss=0.864 Prec@1=78.152 Prec@5=93.274 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:02 IST
=> Training   47.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.391 Loss=0.864 Prec@1=78.152 Prec@5=93.274 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:03 IST
=> Training   47.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.864 Prec@1=78.129 Prec@5=93.268 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=00:03 IST
=> Training   51.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.864 Prec@1=78.129 Prec@5=93.268 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=00:03 IST
=> Training   51.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.864 Prec@1=78.129 Prec@5=93.268 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=00:04 IST
=> Training   51.98% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.388 Loss=0.865 Prec@1=78.099 Prec@5=93.256 rate=1.78 Hz, eta=0:11:14, total=0:12:09, wall=00:04 IST
=> Training   55.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.388 Loss=0.865 Prec@1=78.099 Prec@5=93.256 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=00:04 IST
=> Training   55.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.388 Loss=0.865 Prec@1=78.099 Prec@5=93.256 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=00:05 IST
=> Training   55.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.389 Loss=0.866 Prec@1=78.072 Prec@5=93.252 rate=1.78 Hz, eta=0:10:17, total=0:13:04, wall=00:05 IST
=> Training   59.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.389 Loss=0.866 Prec@1=78.072 Prec@5=93.252 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=00:05 IST
=> Training   59.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.563 DataTime=0.389 Loss=0.866 Prec@1=78.072 Prec@5=93.252 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=00:06 IST
=> Training   59.97% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.867 Prec@1=78.041 Prec@5=93.233 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=00:06 IST
=> Training   63.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.867 Prec@1=78.041 Prec@5=93.233 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=00:06 IST
=> Training   63.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.867 Prec@1=78.041 Prec@5=93.233 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=00:07 IST
=> Training   63.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.868 Prec@1=78.021 Prec@5=93.230 rate=1.78 Hz, eta=0:08:26, total=0:14:58, wall=00:07 IST
=> Training   67.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.868 Prec@1=78.021 Prec@5=93.230 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=00:07 IST
=> Training   67.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.390 Loss=0.868 Prec@1=78.021 Prec@5=93.230 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=00:08 IST
=> Training   67.96% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.869 Prec@1=77.990 Prec@5=93.219 rate=1.78 Hz, eta=0:07:30, total=0:15:55, wall=00:08 IST
=> Training   71.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.869 Prec@1=77.990 Prec@5=93.219 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=00:08 IST
=> Training   71.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.869 Prec@1=77.990 Prec@5=93.219 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=00:09 IST
=> Training   71.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.870 Prec@1=77.974 Prec@5=93.211 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=00:09 IST
=> Training   75.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.870 Prec@1=77.974 Prec@5=93.211 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:09 IST
=> Training   75.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.870 Prec@1=77.974 Prec@5=93.211 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:10 IST
=> Training   75.95% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.871 Prec@1=77.947 Prec@5=93.200 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=00:10 IST
=> Training   79.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.871 Prec@1=77.947 Prec@5=93.200 rate=1.78 Hz, eta=0:04:42, total=0:18:45, wall=00:10 IST
=> Training   79.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.389 Loss=0.871 Prec@1=77.947 Prec@5=93.200 rate=1.78 Hz, eta=0:04:42, total=0:18:45, wall=00:11 IST
=> Training   79.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.390 Loss=0.872 Prec@1=77.918 Prec@5=93.199 rate=1.78 Hz, eta=0:04:42, total=0:18:45, wall=00:11 IST
=> Training   83.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.390 Loss=0.872 Prec@1=77.918 Prec@5=93.199 rate=1.78 Hz, eta=0:03:46, total=0:19:42, wall=00:11 IST
=> Training   83.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.565 DataTime=0.390 Loss=0.872 Prec@1=77.918 Prec@5=93.199 rate=1.78 Hz, eta=0:03:46, total=0:19:42, wall=00:12 IST
=> Training   83.94% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.872 Prec@1=77.901 Prec@5=93.191 rate=1.78 Hz, eta=0:03:46, total=0:19:42, wall=00:12 IST
=> Training   87.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.872 Prec@1=77.901 Prec@5=93.191 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=00:12 IST
=> Training   87.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.872 Prec@1=77.901 Prec@5=93.191 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=00:13 IST
=> Training   87.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.874 Prec@1=77.865 Prec@5=93.173 rate=1.78 Hz, eta=0:02:49, total=0:20:37, wall=00:13 IST
=> Training   91.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.874 Prec@1=77.865 Prec@5=93.173 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=00:13 IST
=> Training   91.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.389 Loss=0.874 Prec@1=77.865 Prec@5=93.173 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=00:13 IST
=> Training   91.93% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.874 Prec@1=77.850 Prec@5=93.163 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=00:13 IST
=> Training   95.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.874 Prec@1=77.850 Prec@5=93.163 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=00:13 IST
=> Training   95.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.874 Prec@1=77.850 Prec@5=93.163 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=00:14 IST
=> Training   95.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.875 Prec@1=77.836 Prec@5=93.158 rate=1.78 Hz, eta=0:00:57, total=0:22:29, wall=00:14 IST
=> Training   99.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.875 Prec@1=77.836 Prec@5=93.158 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=00:14 IST
=> Training   99.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.875 Prec@1=77.836 Prec@5=93.158 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=00:14 IST
=> Training   99.92% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.875 Prec@1=77.836 Prec@5=93.158 rate=1.78 Hz, eta=0:00:01, total=0:23:26, wall=00:14 IST
=> Training   100.00% of 1x2503...Epoch=120/150 LR=0.0102 Time=0.564 DataTime=0.388 Loss=0.875 Prec@1=77.836 Prec@5=93.158 rate=1.78 Hz, eta=0:00:00, total=0:23:26, wall=00:14 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:15 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:15 IST
=> Validation 0.00% of 1x98...Epoch=120/150 LR=0.0102 Time=6.996 Loss=0.783 Prec@1=79.883 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=00:15 IST
=> Validation 1.02% of 1x98...Epoch=120/150 LR=0.0102 Time=6.996 Loss=0.783 Prec@1=79.883 Prec@5=94.141 rate=5969.69 Hz, eta=0:00:00, total=0:00:00, wall=00:15 IST
** Validation 1.02% of 1x98...Epoch=120/150 LR=0.0102 Time=6.996 Loss=0.783 Prec@1=79.883 Prec@5=94.141 rate=5969.69 Hz, eta=0:00:00, total=0:00:00, wall=00:15 IST
** Validation 1.02% of 1x98...Epoch=120/150 LR=0.0102 Time=0.643 Loss=1.230 Prec@1=70.220 Prec@5=89.494 rate=5969.69 Hz, eta=0:00:00, total=0:00:00, wall=00:15 IST
** Validation 100.00% of 1x98...Epoch=120/150 LR=0.0102 Time=0.643 Loss=1.230 Prec@1=70.220 Prec@5=89.494 rate=1.75 Hz, eta=0:00:00, total=0:00:55, wall=00:15 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:16 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:16 IST
=> Training   0.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=5.170 DataTime=4.858 Loss=0.907 Prec@1=76.562 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=00:16 IST
=> Training   0.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=5.170 DataTime=4.858 Loss=0.907 Prec@1=76.562 Prec@5=93.555 rate=2387.80 Hz, eta=0:00:01, total=0:00:00, wall=00:16 IST
=> Training   0.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=5.170 DataTime=4.858 Loss=0.907 Prec@1=76.562 Prec@5=93.555 rate=2387.80 Hz, eta=0:00:01, total=0:00:00, wall=00:17 IST
=> Training   0.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.603 DataTime=0.437 Loss=0.854 Prec@1=78.616 Prec@5=93.456 rate=2387.80 Hz, eta=0:00:01, total=0:00:00, wall=00:17 IST
=> Training   4.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.603 DataTime=0.437 Loss=0.854 Prec@1=78.616 Prec@5=93.456 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=00:17 IST
=> Training   4.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.603 DataTime=0.437 Loss=0.854 Prec@1=78.616 Prec@5=93.456 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=00:17 IST
=> Training   4.04% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.582 DataTime=0.417 Loss=0.850 Prec@1=78.575 Prec@5=93.564 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=00:17 IST
=> Training   8.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.582 DataTime=0.417 Loss=0.850 Prec@1=78.575 Prec@5=93.564 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=00:17 IST
=> Training   8.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.582 DataTime=0.417 Loss=0.850 Prec@1=78.575 Prec@5=93.564 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=00:18 IST
=> Training   8.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.572 DataTime=0.407 Loss=0.849 Prec@1=78.499 Prec@5=93.572 rate=1.80 Hz, eta=0:21:19, total=0:01:51, wall=00:18 IST
=> Training   12.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.572 DataTime=0.407 Loss=0.849 Prec@1=78.499 Prec@5=93.572 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=00:18 IST
=> Training   12.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.572 DataTime=0.407 Loss=0.849 Prec@1=78.499 Prec@5=93.572 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=00:19 IST
=> Training   12.03% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.571 DataTime=0.404 Loss=0.851 Prec@1=78.476 Prec@5=93.524 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=00:19 IST
=> Training   16.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.571 DataTime=0.404 Loss=0.851 Prec@1=78.476 Prec@5=93.524 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=00:19 IST
=> Training   16.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.571 DataTime=0.404 Loss=0.851 Prec@1=78.476 Prec@5=93.524 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=00:20 IST
=> Training   16.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.566 DataTime=0.399 Loss=0.852 Prec@1=78.446 Prec@5=93.508 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=00:20 IST
=> Training   20.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.566 DataTime=0.399 Loss=0.852 Prec@1=78.446 Prec@5=93.508 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:20 IST
=> Training   20.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.566 DataTime=0.399 Loss=0.852 Prec@1=78.446 Prec@5=93.508 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:21 IST
=> Training   20.02% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.854 Prec@1=78.384 Prec@5=93.487 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=00:21 IST
=> Training   24.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.854 Prec@1=78.384 Prec@5=93.487 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:21 IST
=> Training   24.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.854 Prec@1=78.384 Prec@5=93.487 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:22 IST
=> Training   24.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.398 Loss=0.856 Prec@1=78.341 Prec@5=93.443 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=00:22 IST
=> Training   28.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.398 Loss=0.856 Prec@1=78.341 Prec@5=93.443 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:22 IST
=> Training   28.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.398 Loss=0.856 Prec@1=78.341 Prec@5=93.443 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:23 IST
=> Training   28.01% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.858 Prec@1=78.318 Prec@5=93.419 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:23 IST
=> Training   32.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.858 Prec@1=78.318 Prec@5=93.419 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:23 IST
=> Training   32.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.397 Loss=0.858 Prec@1=78.318 Prec@5=93.419 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:24 IST
=> Training   32.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.396 Loss=0.859 Prec@1=78.281 Prec@5=93.405 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=00:24 IST
=> Training   36.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.396 Loss=0.859 Prec@1=78.281 Prec@5=93.405 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=00:24 IST
=> Training   36.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.396 Loss=0.859 Prec@1=78.281 Prec@5=93.405 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=00:25 IST
=> Training   36.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.255 Prec@5=93.389 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=00:25 IST
=> Training   39.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.255 Prec@5=93.389 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=00:25 IST
=> Training   39.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.255 Prec@5=93.389 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=00:26 IST
=> Training   39.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.252 Prec@5=93.375 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=00:26 IST
=> Training   43.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.252 Prec@5=93.375 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=00:26 IST
=> Training   43.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.860 Prec@1=78.252 Prec@5=93.375 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=00:27 IST
=> Training   43.99% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.861 Prec@1=78.235 Prec@5=93.356 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=00:27 IST
=> Training   47.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.861 Prec@1=78.235 Prec@5=93.356 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=00:27 IST
=> Training   47.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.565 DataTime=0.395 Loss=0.861 Prec@1=78.235 Prec@5=93.356 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=00:28 IST
=> Training   47.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.223 Prec@5=93.352 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=00:28 IST
=> Training   51.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.223 Prec@5=93.352 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:28 IST
=> Training   51.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.223 Prec@5=93.352 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:29 IST
=> Training   51.98% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.201 Prec@5=93.347 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:29 IST
=> Training   55.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.201 Prec@5=93.347 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:29 IST
=> Training   55.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.393 Loss=0.861 Prec@1=78.201 Prec@5=93.347 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:30 IST
=> Training   55.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.392 Loss=0.862 Prec@1=78.185 Prec@5=93.335 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:30 IST
=> Training   59.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.392 Loss=0.862 Prec@1=78.185 Prec@5=93.335 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=00:30 IST
=> Training   59.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.564 DataTime=0.392 Loss=0.862 Prec@1=78.185 Prec@5=93.335 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=00:31 IST
=> Training   59.97% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.862 Prec@1=78.184 Prec@5=93.335 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=00:31 IST
=> Training   63.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.862 Prec@1=78.184 Prec@5=93.335 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=00:31 IST
=> Training   63.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.862 Prec@1=78.184 Prec@5=93.335 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=00:31 IST
=> Training   63.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.863 Prec@1=78.166 Prec@5=93.327 rate=1.79 Hz, eta=0:08:25, total=0:14:56, wall=00:31 IST
=> Training   67.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.863 Prec@1=78.166 Prec@5=93.327 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=00:31 IST
=> Training   67.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.563 DataTime=0.391 Loss=0.863 Prec@1=78.166 Prec@5=93.327 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=00:32 IST
=> Training   67.96% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.864 Prec@1=78.122 Prec@5=93.310 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=00:32 IST
=> Training   71.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.864 Prec@1=78.122 Prec@5=93.310 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:32 IST
=> Training   71.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.864 Prec@1=78.122 Prec@5=93.310 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:33 IST
=> Training   71.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.865 Prec@1=78.112 Prec@5=93.309 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=00:33 IST
=> Training   75.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.865 Prec@1=78.112 Prec@5=93.309 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:33 IST
=> Training   75.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.390 Loss=0.865 Prec@1=78.112 Prec@5=93.309 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:34 IST
=> Training   75.95% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.865 Prec@1=78.106 Prec@5=93.303 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:34 IST
=> Training   79.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.865 Prec@1=78.106 Prec@5=93.303 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=00:34 IST
=> Training   79.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.865 Prec@1=78.106 Prec@5=93.303 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=00:35 IST
=> Training   79.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.866 Prec@1=78.093 Prec@5=93.292 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=00:35 IST
=> Training   83.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.866 Prec@1=78.093 Prec@5=93.292 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=00:35 IST
=> Training   83.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.866 Prec@1=78.093 Prec@5=93.292 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=00:36 IST
=> Training   83.94% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.070 Prec@5=93.280 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=00:36 IST
=> Training   87.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.070 Prec@5=93.280 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=00:36 IST
=> Training   87.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.070 Prec@5=93.280 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=00:37 IST
=> Training   87.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.061 Prec@5=93.271 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=00:37 IST
=> Training   91.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.061 Prec@5=93.271 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=00:37 IST
=> Training   91.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.061 Prec@5=93.271 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=00:38 IST
=> Training   91.93% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.046 Prec@5=93.263 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=00:38 IST
=> Training   95.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.046 Prec@5=93.263 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=00:38 IST
=> Training   95.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.389 Loss=0.867 Prec@1=78.046 Prec@5=93.263 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=00:39 IST
=> Training   95.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.388 Loss=0.868 Prec@1=78.026 Prec@5=93.252 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=00:39 IST
=> Training   99.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.388 Loss=0.868 Prec@1=78.026 Prec@5=93.252 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=00:39 IST
=> Training   99.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.562 DataTime=0.388 Loss=0.868 Prec@1=78.026 Prec@5=93.252 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=00:39 IST
=> Training   99.92% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.561 DataTime=0.388 Loss=0.868 Prec@1=78.025 Prec@5=93.251 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=00:39 IST
=> Training   100.00% of 1x2503...Epoch=121/150 LR=0.0095 Time=0.561 DataTime=0.388 Loss=0.868 Prec@1=78.025 Prec@5=93.251 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=00:39 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:39 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=00:39 IST
=> Validation 0.00% of 1x98...Epoch=121/150 LR=0.0095 Time=7.176 Loss=0.710 Prec@1=81.641 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=00:39 IST
=> Validation 1.02% of 1x98...Epoch=121/150 LR=0.0095 Time=7.176 Loss=0.710 Prec@1=81.641 Prec@5=94.922 rate=6485.55 Hz, eta=0:00:00, total=0:00:00, wall=00:39 IST
** Validation 1.02% of 1x98...Epoch=121/150 LR=0.0095 Time=7.176 Loss=0.710 Prec@1=81.641 Prec@5=94.922 rate=6485.55 Hz, eta=0:00:00, total=0:00:00, wall=00:40 IST
** Validation 1.02% of 1x98...Epoch=121/150 LR=0.0095 Time=0.636 Loss=1.230 Prec@1=70.246 Prec@5=89.490 rate=6485.55 Hz, eta=0:00:00, total=0:00:00, wall=00:40 IST
** Validation 100.00% of 1x98...Epoch=121/150 LR=0.0095 Time=0.636 Loss=1.230 Prec@1=70.246 Prec@5=89.490 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=00:40 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:40 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=00:40 IST
=> Training   0.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=4.529 DataTime=3.922 Loss=0.849 Prec@1=79.883 Prec@5=93.555 rate=0 Hz, eta=?, total=0:00:00, wall=00:40 IST
=> Training   0.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=4.529 DataTime=3.922 Loss=0.849 Prec@1=79.883 Prec@5=93.555 rate=1832.03 Hz, eta=0:00:01, total=0:00:00, wall=00:40 IST
=> Training   0.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=4.529 DataTime=3.922 Loss=0.849 Prec@1=79.883 Prec@5=93.555 rate=1832.03 Hz, eta=0:00:01, total=0:00:00, wall=00:41 IST
=> Training   0.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.586 DataTime=0.414 Loss=0.850 Prec@1=78.508 Prec@5=93.301 rate=1832.03 Hz, eta=0:00:01, total=0:00:00, wall=00:41 IST
=> Training   4.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.586 DataTime=0.414 Loss=0.850 Prec@1=78.508 Prec@5=93.301 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=00:41 IST
=> Training   4.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.586 DataTime=0.414 Loss=0.850 Prec@1=78.508 Prec@5=93.301 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=00:42 IST
=> Training   4.04% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.580 DataTime=0.410 Loss=0.844 Prec@1=78.541 Prec@5=93.468 rate=1.85 Hz, eta=0:21:40, total=0:00:54, wall=00:42 IST
=> Training   8.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.580 DataTime=0.410 Loss=0.844 Prec@1=78.541 Prec@5=93.468 rate=1.80 Hz, eta=0:21:22, total=0:01:51, wall=00:42 IST
=> Training   8.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.580 DataTime=0.410 Loss=0.844 Prec@1=78.541 Prec@5=93.468 rate=1.80 Hz, eta=0:21:22, total=0:01:51, wall=00:43 IST
=> Training   8.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.572 DataTime=0.402 Loss=0.845 Prec@1=78.621 Prec@5=93.443 rate=1.80 Hz, eta=0:21:22, total=0:01:51, wall=00:43 IST
=> Training   12.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.572 DataTime=0.402 Loss=0.845 Prec@1=78.621 Prec@5=93.443 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=00:43 IST
=> Training   12.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.572 DataTime=0.402 Loss=0.845 Prec@1=78.621 Prec@5=93.443 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=00:44 IST
=> Training   12.03% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.571 DataTime=0.400 Loss=0.843 Prec@1=78.629 Prec@5=93.470 rate=1.79 Hz, eta=0:20:26, total=0:02:47, wall=00:44 IST
=> Training   16.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.571 DataTime=0.400 Loss=0.843 Prec@1=78.629 Prec@5=93.470 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=00:44 IST
=> Training   16.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.571 DataTime=0.400 Loss=0.843 Prec@1=78.629 Prec@5=93.470 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=00:45 IST
=> Training   16.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.582 Prec@5=93.490 rate=1.79 Hz, eta=0:19:35, total=0:03:44, wall=00:45 IST
=> Training   20.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.582 Prec@5=93.490 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=00:45 IST
=> Training   20.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.582 Prec@5=93.490 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=00:46 IST
=> Training   20.02% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.547 Prec@5=93.527 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=00:46 IST
=> Training   24.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.547 Prec@5=93.527 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=00:46 IST
=> Training   24.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.566 DataTime=0.394 Loss=0.843 Prec@1=78.547 Prec@5=93.527 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=00:47 IST
=> Training   24.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.842 Prec@1=78.590 Prec@5=93.539 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=00:47 IST
=> Training   28.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.842 Prec@1=78.590 Prec@5=93.539 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:47 IST
=> Training   28.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.842 Prec@1=78.590 Prec@5=93.539 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:48 IST
=> Training   28.01% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.844 Prec@1=78.563 Prec@5=93.506 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=00:48 IST
=> Training   32.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.844 Prec@1=78.563 Prec@5=93.506 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=00:48 IST
=> Training   32.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.844 Prec@1=78.563 Prec@5=93.506 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=00:48 IST
=> Training   32.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.845 Prec@1=78.551 Prec@5=93.498 rate=1.79 Hz, eta=0:15:51, total=0:07:27, wall=00:48 IST
=> Training   36.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.845 Prec@1=78.551 Prec@5=93.498 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=00:48 IST
=> Training   36.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.565 DataTime=0.393 Loss=0.845 Prec@1=78.551 Prec@5=93.498 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=00:49 IST
=> Training   36.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.846 Prec@1=78.524 Prec@5=93.469 rate=1.79 Hz, eta=0:14:57, total=0:08:24, wall=00:49 IST
=> Training   39.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.846 Prec@1=78.524 Prec@5=93.469 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=00:49 IST
=> Training   39.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.846 Prec@1=78.524 Prec@5=93.469 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=00:50 IST
=> Training   39.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.847 Prec@1=78.524 Prec@5=93.454 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=00:50 IST
=> Training   43.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.847 Prec@1=78.524 Prec@5=93.454 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:50 IST
=> Training   43.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.847 Prec@1=78.524 Prec@5=93.454 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:51 IST
=> Training   43.99% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.848 Prec@1=78.501 Prec@5=93.440 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=00:51 IST
=> Training   47.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.848 Prec@1=78.501 Prec@5=93.440 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:51 IST
=> Training   47.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.848 Prec@1=78.501 Prec@5=93.440 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:52 IST
=> Training   47.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.849 Prec@1=78.486 Prec@5=93.431 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=00:52 IST
=> Training   51.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.849 Prec@1=78.486 Prec@5=93.431 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:52 IST
=> Training   51.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.849 Prec@1=78.486 Prec@5=93.431 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:53 IST
=> Training   51.98% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.850 Prec@1=78.466 Prec@5=93.424 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=00:53 IST
=> Training   55.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.850 Prec@1=78.466 Prec@5=93.424 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:53 IST
=> Training   55.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.850 Prec@1=78.466 Prec@5=93.424 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:54 IST
=> Training   55.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.851 Prec@1=78.434 Prec@5=93.408 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=00:54 IST
=> Training   59.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.851 Prec@1=78.434 Prec@5=93.408 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=00:54 IST
=> Training   59.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.564 DataTime=0.391 Loss=0.851 Prec@1=78.434 Prec@5=93.408 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=00:55 IST
=> Training   59.97% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.852 Prec@1=78.395 Prec@5=93.395 rate=1.78 Hz, eta=0:09:22, total=0:14:02, wall=00:55 IST
=> Training   63.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.852 Prec@1=78.395 Prec@5=93.395 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=00:55 IST
=> Training   63.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.390 Loss=0.852 Prec@1=78.395 Prec@5=93.395 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=00:56 IST
=> Training   63.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.853 Prec@1=78.369 Prec@5=93.385 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=00:56 IST
=> Training   67.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.853 Prec@1=78.369 Prec@5=93.385 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=00:56 IST
=> Training   67.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.563 DataTime=0.389 Loss=0.853 Prec@1=78.369 Prec@5=93.385 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=00:57 IST
=> Training   67.96% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.853 Prec@1=78.362 Prec@5=93.388 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=00:57 IST
=> Training   71.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.853 Prec@1=78.362 Prec@5=93.388 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=00:57 IST
=> Training   71.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.853 Prec@1=78.362 Prec@5=93.388 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=00:58 IST
=> Training   71.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.854 Prec@1=78.344 Prec@5=93.380 rate=1.79 Hz, eta=0:06:32, total=0:16:48, wall=00:58 IST
=> Training   75.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.854 Prec@1=78.344 Prec@5=93.380 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:58 IST
=> Training   75.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.854 Prec@1=78.344 Prec@5=93.380 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:59 IST
=> Training   75.95% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.339 Prec@5=93.373 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=00:59 IST
=> Training   79.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.339 Prec@5=93.373 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=00:59 IST
=> Training   79.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.339 Prec@5=93.373 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=01:00 IST
=> Training   79.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.331 Prec@5=93.369 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=01:00 IST
=> Training   83.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.331 Prec@5=93.369 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:00 IST
=> Training   83.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.388 Loss=0.855 Prec@1=78.331 Prec@5=93.369 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:01 IST
=> Training   83.94% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.855 Prec@1=78.329 Prec@5=93.367 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=01:01 IST
=> Training   87.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.855 Prec@1=78.329 Prec@5=93.367 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:01 IST
=> Training   87.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.562 DataTime=0.389 Loss=0.855 Prec@1=78.329 Prec@5=93.367 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:02 IST
=> Training   87.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.311 Prec@5=93.358 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:02 IST
=> Training   91.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.311 Prec@5=93.358 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=01:02 IST
=> Training   91.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.311 Prec@5=93.358 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=01:02 IST
=> Training   91.93% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.299 Prec@5=93.349 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=01:02 IST
=> Training   95.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.299 Prec@5=93.349 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=01:02 IST
=> Training   95.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.856 Prec@1=78.299 Prec@5=93.349 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=01:03 IST
=> Training   95.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.857 Prec@1=78.293 Prec@5=93.349 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=01:03 IST
=> Training   99.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.857 Prec@1=78.293 Prec@5=93.349 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=01:03 IST
=> Training   99.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.857 Prec@1=78.293 Prec@5=93.349 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=01:03 IST
=> Training   99.92% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.857 Prec@1=78.293 Prec@5=93.348 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=01:03 IST
=> Training   100.00% of 1x2503...Epoch=122/150 LR=0.0089 Time=0.561 DataTime=0.388 Loss=0.857 Prec@1=78.293 Prec@5=93.348 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=01:03 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:04 IST
=> Validation 0.00% of 1x98...Epoch=122/150 LR=0.0089 Time=6.946 Loss=0.773 Prec@1=79.492 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=01:04 IST
=> Validation 1.02% of 1x98...Epoch=122/150 LR=0.0089 Time=6.946 Loss=0.773 Prec@1=79.492 Prec@5=94.727 rate=4364.43 Hz, eta=0:00:00, total=0:00:00, wall=01:04 IST
** Validation 1.02% of 1x98...Epoch=122/150 LR=0.0089 Time=6.946 Loss=0.773 Prec@1=79.492 Prec@5=94.727 rate=4364.43 Hz, eta=0:00:00, total=0:00:00, wall=01:04 IST
** Validation 1.02% of 1x98...Epoch=122/150 LR=0.0089 Time=0.636 Loss=1.223 Prec@1=70.514 Prec@5=89.556 rate=4364.43 Hz, eta=0:00:00, total=0:00:00, wall=01:04 IST
** Validation 100.00% of 1x98...Epoch=122/150 LR=0.0089 Time=0.636 Loss=1.223 Prec@1=70.514 Prec@5=89.556 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=01:04 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:05 IST
=> Training   0.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=4.264 DataTime=3.961 Loss=0.821 Prec@1=78.516 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=01:05 IST
=> Training   0.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=4.264 DataTime=3.961 Loss=0.821 Prec@1=78.516 Prec@5=93.945 rate=5207.28 Hz, eta=0:00:00, total=0:00:00, wall=01:05 IST
=> Training   0.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=4.264 DataTime=3.961 Loss=0.821 Prec@1=78.516 Prec@5=93.945 rate=5207.28 Hz, eta=0:00:00, total=0:00:00, wall=01:05 IST
=> Training   0.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.582 DataTime=0.408 Loss=0.832 Prec@1=79.026 Prec@5=93.653 rate=5207.28 Hz, eta=0:00:00, total=0:00:00, wall=01:05 IST
=> Training   4.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.582 DataTime=0.408 Loss=0.832 Prec@1=79.026 Prec@5=93.653 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=01:05 IST
=> Training   4.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.582 DataTime=0.408 Loss=0.832 Prec@1=79.026 Prec@5=93.653 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=01:06 IST
=> Training   4.04% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.573 DataTime=0.400 Loss=0.834 Prec@1=78.858 Prec@5=93.617 rate=1.85 Hz, eta=0:21:37, total=0:00:54, wall=01:06 IST
=> Training   8.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.573 DataTime=0.400 Loss=0.834 Prec@1=78.858 Prec@5=93.617 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=01:06 IST
=> Training   8.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.573 DataTime=0.400 Loss=0.834 Prec@1=78.858 Prec@5=93.617 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=01:07 IST
=> Training   8.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.568 DataTime=0.396 Loss=0.833 Prec@1=78.817 Prec@5=93.644 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=01:07 IST
=> Training   12.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.568 DataTime=0.396 Loss=0.833 Prec@1=78.817 Prec@5=93.644 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=01:07 IST
=> Training   12.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.568 DataTime=0.396 Loss=0.833 Prec@1=78.817 Prec@5=93.644 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=01:08 IST
=> Training   12.03% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.567 DataTime=0.395 Loss=0.837 Prec@1=78.747 Prec@5=93.579 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=01:08 IST
=> Training   16.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.567 DataTime=0.395 Loss=0.837 Prec@1=78.747 Prec@5=93.579 rate=1.80 Hz, eta=0:19:28, total=0:03:43, wall=01:08 IST
=> Training   16.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.567 DataTime=0.395 Loss=0.837 Prec@1=78.747 Prec@5=93.579 rate=1.80 Hz, eta=0:19:28, total=0:03:43, wall=01:09 IST
=> Training   16.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.834 Prec@1=78.796 Prec@5=93.625 rate=1.80 Hz, eta=0:19:28, total=0:03:43, wall=01:09 IST
=> Training   20.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.834 Prec@1=78.796 Prec@5=93.625 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:09 IST
=> Training   20.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.834 Prec@1=78.796 Prec@5=93.625 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:10 IST
=> Training   20.02% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.835 Prec@1=78.755 Prec@5=93.607 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:10 IST
=> Training   24.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.835 Prec@1=78.755 Prec@5=93.607 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=01:10 IST
=> Training   24.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.835 Prec@1=78.755 Prec@5=93.607 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=01:11 IST
=> Training   24.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.835 Prec@1=78.752 Prec@5=93.617 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=01:11 IST
=> Training   28.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.835 Prec@1=78.752 Prec@5=93.617 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=01:11 IST
=> Training   28.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.835 Prec@1=78.752 Prec@5=93.617 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=01:12 IST
=> Training   28.01% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.836 Prec@1=78.748 Prec@5=93.589 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=01:12 IST
=> Training   32.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.836 Prec@1=78.748 Prec@5=93.589 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=01:12 IST
=> Training   32.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.392 Loss=0.836 Prec@1=78.748 Prec@5=93.589 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=01:13 IST
=> Training   32.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.837 Prec@1=78.732 Prec@5=93.577 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=01:13 IST
=> Training   36.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.837 Prec@1=78.732 Prec@5=93.577 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=01:13 IST
=> Training   36.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.837 Prec@1=78.732 Prec@5=93.577 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=01:14 IST
=> Training   36.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.838 Prec@1=78.720 Prec@5=93.572 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=01:14 IST
=> Training   39.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.838 Prec@1=78.720 Prec@5=93.572 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=01:14 IST
=> Training   39.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.393 Loss=0.838 Prec@1=78.720 Prec@5=93.572 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=01:15 IST
=> Training   39.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.566 DataTime=0.394 Loss=0.839 Prec@1=78.712 Prec@5=93.568 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=01:15 IST
=> Training   43.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.566 DataTime=0.394 Loss=0.839 Prec@1=78.712 Prec@5=93.568 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:15 IST
=> Training   43.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.566 DataTime=0.394 Loss=0.839 Prec@1=78.712 Prec@5=93.568 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:16 IST
=> Training   43.99% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.839 Prec@1=78.685 Prec@5=93.556 rate=1.78 Hz, eta=0:13:07, total=0:10:18, wall=01:16 IST
=> Training   47.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.839 Prec@1=78.685 Prec@5=93.556 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:16 IST
=> Training   47.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.839 Prec@1=78.685 Prec@5=93.556 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:17 IST
=> Training   47.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.840 Prec@1=78.671 Prec@5=93.558 rate=1.78 Hz, eta=0:12:11, total=0:11:14, wall=01:17 IST
=> Training   51.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.840 Prec@1=78.671 Prec@5=93.558 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=01:17 IST
=> Training   51.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.565 DataTime=0.393 Loss=0.840 Prec@1=78.671 Prec@5=93.558 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=01:18 IST
=> Training   51.98% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.391 Loss=0.842 Prec@1=78.643 Prec@5=93.538 rate=1.78 Hz, eta=0:11:15, total=0:12:11, wall=01:18 IST
=> Training   55.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.391 Loss=0.842 Prec@1=78.643 Prec@5=93.538 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:18 IST
=> Training   55.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.564 DataTime=0.391 Loss=0.842 Prec@1=78.643 Prec@5=93.538 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:19 IST
=> Training   55.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.391 Loss=0.842 Prec@1=78.639 Prec@5=93.531 rate=1.78 Hz, eta=0:10:18, total=0:13:05, wall=01:19 IST
=> Training   59.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.391 Loss=0.842 Prec@1=78.639 Prec@5=93.531 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=01:19 IST
=> Training   59.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.391 Loss=0.842 Prec@1=78.639 Prec@5=93.531 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=01:19 IST
=> Training   59.97% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.390 Loss=0.843 Prec@1=78.628 Prec@5=93.525 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=01:19 IST
=> Training   63.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.390 Loss=0.843 Prec@1=78.628 Prec@5=93.525 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:19 IST
=> Training   63.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.390 Loss=0.843 Prec@1=78.628 Prec@5=93.525 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:20 IST
=> Training   63.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.390 Loss=0.843 Prec@1=78.603 Prec@5=93.521 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:20 IST
=> Training   67.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.390 Loss=0.843 Prec@1=78.603 Prec@5=93.521 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:20 IST
=> Training   67.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.390 Loss=0.843 Prec@1=78.603 Prec@5=93.521 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:21 IST
=> Training   67.96% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.578 Prec@5=93.505 rate=1.78 Hz, eta=0:07:29, total=0:15:53, wall=01:21 IST
=> Training   71.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.578 Prec@5=93.505 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:21 IST
=> Training   71.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.578 Prec@5=93.505 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:22 IST
=> Training   71.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.571 Prec@5=93.501 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:22 IST
=> Training   75.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.571 Prec@5=93.501 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:22 IST
=> Training   75.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.563 DataTime=0.389 Loss=0.844 Prec@1=78.571 Prec@5=93.501 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:23 IST
=> Training   75.95% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.559 Prec@5=93.495 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:23 IST
=> Training   79.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.559 Prec@5=93.495 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:23 IST
=> Training   79.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.559 Prec@5=93.495 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:24 IST
=> Training   79.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.551 Prec@5=93.493 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=01:24 IST
=> Training   83.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.551 Prec@5=93.493 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=01:24 IST
=> Training   83.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.388 Loss=0.845 Prec@1=78.551 Prec@5=93.493 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=01:25 IST
=> Training   83.94% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.846 Prec@1=78.530 Prec@5=93.487 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=01:25 IST
=> Training   87.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.846 Prec@1=78.530 Prec@5=93.487 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:25 IST
=> Training   87.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.846 Prec@1=78.530 Prec@5=93.487 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:26 IST
=> Training   87.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.847 Prec@1=78.519 Prec@5=93.479 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=01:26 IST
=> Training   91.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.847 Prec@1=78.519 Prec@5=93.479 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=01:26 IST
=> Training   91.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.562 DataTime=0.387 Loss=0.847 Prec@1=78.519 Prec@5=93.479 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=01:27 IST
=> Training   91.93% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.561 DataTime=0.386 Loss=0.848 Prec@1=78.493 Prec@5=93.468 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=01:27 IST
=> Training   95.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.561 DataTime=0.386 Loss=0.848 Prec@1=78.493 Prec@5=93.468 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:27 IST
=> Training   95.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.561 DataTime=0.386 Loss=0.848 Prec@1=78.493 Prec@5=93.468 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:28 IST
=> Training   95.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.560 DataTime=0.386 Loss=0.848 Prec@1=78.485 Prec@5=93.461 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=01:28 IST
=> Training   99.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.560 DataTime=0.386 Loss=0.848 Prec@1=78.485 Prec@5=93.461 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=01:28 IST
=> Training   99.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.560 DataTime=0.386 Loss=0.848 Prec@1=78.485 Prec@5=93.461 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=01:28 IST
=> Training   99.92% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.560 DataTime=0.385 Loss=0.848 Prec@1=78.483 Prec@5=93.461 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=01:28 IST
=> Training   100.00% of 1x2503...Epoch=123/150 LR=0.0084 Time=0.560 DataTime=0.385 Loss=0.848 Prec@1=78.483 Prec@5=93.461 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=01:28 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> Validation 0.00% of 1x98...Epoch=123/150 LR=0.0084 Time=6.906 Loss=0.696 Prec@1=82.031 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=01:28 IST
=> Validation 1.02% of 1x98...Epoch=123/150 LR=0.0084 Time=6.906 Loss=0.696 Prec@1=82.031 Prec@5=94.922 rate=6168.08 Hz, eta=0:00:00, total=0:00:00, wall=01:28 IST
** Validation 1.02% of 1x98...Epoch=123/150 LR=0.0084 Time=6.906 Loss=0.696 Prec@1=82.031 Prec@5=94.922 rate=6168.08 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
** Validation 1.02% of 1x98...Epoch=123/150 LR=0.0084 Time=0.628 Loss=1.216 Prec@1=70.668 Prec@5=89.722 rate=6168.08 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
** Validation 100.00% of 1x98...Epoch=123/150 LR=0.0084 Time=0.628 Loss=1.216 Prec@1=70.668 Prec@5=89.722 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=01:29 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> Training   0.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=4.963 DataTime=4.515 Loss=0.719 Prec@1=82.812 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=01:29 IST
=> Training   0.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=4.963 DataTime=4.515 Loss=0.719 Prec@1=82.812 Prec@5=94.141 rate=2599.58 Hz, eta=0:00:00, total=0:00:00, wall=01:29 IST
=> Training   0.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=4.963 DataTime=4.515 Loss=0.719 Prec@1=82.812 Prec@5=94.141 rate=2599.58 Hz, eta=0:00:00, total=0:00:00, wall=01:30 IST
=> Training   0.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.586 DataTime=0.416 Loss=0.823 Prec@1=78.966 Prec@5=93.949 rate=2599.58 Hz, eta=0:00:00, total=0:00:00, wall=01:30 IST
=> Training   4.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.586 DataTime=0.416 Loss=0.823 Prec@1=78.966 Prec@5=93.949 rate=1.86 Hz, eta=0:21:29, total=0:00:54, wall=01:30 IST
=> Training   4.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.586 DataTime=0.416 Loss=0.823 Prec@1=78.966 Prec@5=93.949 rate=1.86 Hz, eta=0:21:29, total=0:00:54, wall=01:31 IST
=> Training   4.04% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.582 DataTime=0.414 Loss=0.823 Prec@1=79.156 Prec@5=93.870 rate=1.86 Hz, eta=0:21:29, total=0:00:54, wall=01:31 IST
=> Training   8.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.582 DataTime=0.414 Loss=0.823 Prec@1=79.156 Prec@5=93.870 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=01:31 IST
=> Training   8.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.582 DataTime=0.414 Loss=0.823 Prec@1=79.156 Prec@5=93.870 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=01:32 IST
=> Training   8.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.568 DataTime=0.400 Loss=0.824 Prec@1=79.167 Prec@5=93.823 rate=1.79 Hz, eta=0:21:22, total=0:01:51, wall=01:32 IST
=> Training   12.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.568 DataTime=0.400 Loss=0.824 Prec@1=79.167 Prec@5=93.823 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=01:32 IST
=> Training   12.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.568 DataTime=0.400 Loss=0.824 Prec@1=79.167 Prec@5=93.823 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=01:33 IST
=> Training   12.03% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.569 DataTime=0.399 Loss=0.825 Prec@1=79.136 Prec@5=93.814 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=01:33 IST
=> Training   16.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.569 DataTime=0.399 Loss=0.825 Prec@1=79.136 Prec@5=93.814 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=01:33 IST
=> Training   16.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.569 DataTime=0.399 Loss=0.825 Prec@1=79.136 Prec@5=93.814 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=01:34 IST
=> Training   16.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.393 Loss=0.824 Prec@1=79.133 Prec@5=93.783 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=01:34 IST
=> Training   20.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.393 Loss=0.824 Prec@1=79.133 Prec@5=93.783 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:34 IST
=> Training   20.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.393 Loss=0.824 Prec@1=79.133 Prec@5=93.783 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:35 IST
=> Training   20.02% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.393 Loss=0.826 Prec@1=79.084 Prec@5=93.752 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=01:35 IST
=> Training   24.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.393 Loss=0.826 Prec@1=79.084 Prec@5=93.752 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=01:35 IST
=> Training   24.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.393 Loss=0.826 Prec@1=79.084 Prec@5=93.752 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=01:35 IST
=> Training   24.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.394 Loss=0.826 Prec@1=79.068 Prec@5=93.757 rate=1.79 Hz, eta=0:17:39, total=0:05:34, wall=01:35 IST
=> Training   28.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.394 Loss=0.826 Prec@1=79.068 Prec@5=93.757 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=01:35 IST
=> Training   28.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.566 DataTime=0.394 Loss=0.826 Prec@1=79.068 Prec@5=93.757 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=01:36 IST
=> Training   28.01% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=79.056 Prec@5=93.737 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=01:36 IST
=> Training   32.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=79.056 Prec@5=93.737 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=01:36 IST
=> Training   32.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=79.056 Prec@5=93.737 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=01:37 IST
=> Training   32.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.827 Prec@1=79.051 Prec@5=93.715 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=01:37 IST
=> Training   36.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.827 Prec@1=79.051 Prec@5=93.715 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=01:37 IST
=> Training   36.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.827 Prec@1=79.051 Prec@5=93.715 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=01:38 IST
=> Training   36.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.829 Prec@1=79.016 Prec@5=93.690 rate=1.79 Hz, eta=0:14:56, total=0:08:23, wall=01:38 IST
=> Training   39.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.829 Prec@1=79.016 Prec@5=93.690 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=01:38 IST
=> Training   39.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.829 Prec@1=79.016 Prec@5=93.690 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=01:39 IST
=> Training   39.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.386 Loss=0.829 Prec@1=78.999 Prec@5=93.687 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=01:39 IST
=> Training   43.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.386 Loss=0.829 Prec@1=78.999 Prec@5=93.687 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=01:39 IST
=> Training   43.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.386 Loss=0.829 Prec@1=78.999 Prec@5=93.687 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=01:40 IST
=> Training   43.99% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.387 Loss=0.829 Prec@1=78.975 Prec@5=93.677 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=01:40 IST
=> Training   47.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.387 Loss=0.829 Prec@1=78.975 Prec@5=93.677 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=01:40 IST
=> Training   47.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.387 Loss=0.829 Prec@1=78.975 Prec@5=93.677 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=01:41 IST
=> Training   47.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.830 Prec@1=78.967 Prec@5=93.664 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=01:41 IST
=> Training   51.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.830 Prec@1=78.967 Prec@5=93.664 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=01:41 IST
=> Training   51.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.830 Prec@1=78.967 Prec@5=93.664 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=01:42 IST
=> Training   51.98% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.561 DataTime=0.384 Loss=0.830 Prec@1=78.955 Prec@5=93.662 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=01:42 IST
=> Training   55.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.561 DataTime=0.384 Loss=0.830 Prec@1=78.955 Prec@5=93.662 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=01:42 IST
=> Training   55.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.561 DataTime=0.384 Loss=0.830 Prec@1=78.955 Prec@5=93.662 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=01:43 IST
=> Training   55.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.831 Prec@1=78.932 Prec@5=93.649 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=01:43 IST
=> Training   59.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.831 Prec@1=78.932 Prec@5=93.649 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=01:43 IST
=> Training   59.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.831 Prec@1=78.932 Prec@5=93.649 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=01:44 IST
=> Training   59.97% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.385 Loss=0.832 Prec@1=78.908 Prec@5=93.645 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=01:44 IST
=> Training   63.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.385 Loss=0.832 Prec@1=78.908 Prec@5=93.645 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:44 IST
=> Training   63.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.385 Loss=0.832 Prec@1=78.908 Prec@5=93.645 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:45 IST
=> Training   63.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.832 Prec@1=78.890 Prec@5=93.641 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=01:45 IST
=> Training   67.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.832 Prec@1=78.890 Prec@5=93.641 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=01:45 IST
=> Training   67.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.562 DataTime=0.385 Loss=0.832 Prec@1=78.890 Prec@5=93.641 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=01:46 IST
=> Training   67.96% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.387 Loss=0.833 Prec@1=78.871 Prec@5=93.625 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=01:46 IST
=> Training   71.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.387 Loss=0.833 Prec@1=78.871 Prec@5=93.625 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:46 IST
=> Training   71.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.563 DataTime=0.387 Loss=0.833 Prec@1=78.871 Prec@5=93.625 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:47 IST
=> Training   71.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.851 Prec@5=93.614 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=01:47 IST
=> Training   75.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.851 Prec@5=93.614 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:47 IST
=> Training   75.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.851 Prec@5=93.614 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:48 IST
=> Training   75.95% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.846 Prec@5=93.605 rate=1.78 Hz, eta=0:05:37, total=0:17:46, wall=01:48 IST
=> Training   79.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.846 Prec@5=93.605 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=01:48 IST
=> Training   79.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.834 Prec@1=78.846 Prec@5=93.605 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=01:49 IST
=> Training   79.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.836 Prec@1=78.810 Prec@5=93.591 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=01:49 IST
=> Training   83.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.836 Prec@1=78.810 Prec@5=93.591 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=01:49 IST
=> Training   83.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.836 Prec@1=78.810 Prec@5=93.591 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=01:50 IST
=> Training   83.94% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.791 Prec@5=93.584 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=01:50 IST
=> Training   87.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.791 Prec@5=93.584 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=01:50 IST
=> Training   87.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.791 Prec@5=93.584 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=01:50 IST
=> Training   87.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.837 Prec@1=78.769 Prec@5=93.579 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=01:50 IST
=> Training   91.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.837 Prec@1=78.769 Prec@5=93.579 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=01:50 IST
=> Training   91.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.837 Prec@1=78.769 Prec@5=93.579 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=01:51 IST
=> Training   91.93% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.777 Prec@5=93.583 rate=1.78 Hz, eta=0:01:53, total=0:21:33, wall=01:51 IST
=> Training   95.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.777 Prec@5=93.583 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=01:51 IST
=> Training   95.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.389 Loss=0.837 Prec@1=78.777 Prec@5=93.583 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=01:52 IST
=> Training   95.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.838 Prec@1=78.750 Prec@5=93.570 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=01:52 IST
=> Training   99.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.838 Prec@1=78.750 Prec@5=93.570 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=01:52 IST
=> Training   99.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.565 DataTime=0.388 Loss=0.838 Prec@1=78.750 Prec@5=93.570 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=01:52 IST
=> Training   99.92% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.838 Prec@1=78.750 Prec@5=93.570 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=01:52 IST
=> Training   100.00% of 1x2503...Epoch=124/150 LR=0.0078 Time=0.564 DataTime=0.388 Loss=0.838 Prec@1=78.750 Prec@5=93.570 rate=1.78 Hz, eta=0:00:00, total=0:23:27, wall=01:52 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST
=> Validation 0.00% of 1x98...Epoch=124/150 LR=0.0078 Time=7.307 Loss=0.718 Prec@1=82.031 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=01:53 IST
=> Validation 1.02% of 1x98...Epoch=124/150 LR=0.0078 Time=7.307 Loss=0.718 Prec@1=82.031 Prec@5=94.922 rate=8397.58 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST
** Validation 1.02% of 1x98...Epoch=124/150 LR=0.0078 Time=7.307 Loss=0.718 Prec@1=82.031 Prec@5=94.922 rate=8397.58 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST
** Validation 1.02% of 1x98...Epoch=124/150 LR=0.0078 Time=0.643 Loss=1.211 Prec@1=70.706 Prec@5=89.630 rate=8397.58 Hz, eta=0:00:00, total=0:00:00, wall=01:53 IST
** Validation 100.00% of 1x98...Epoch=124/150 LR=0.0078 Time=0.643 Loss=1.211 Prec@1=70.706 Prec@5=89.630 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=01:53 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST
=> Training   0.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=5.931 DataTime=5.678 Loss=0.952 Prec@1=74.414 Prec@5=91.406 rate=0 Hz, eta=?, total=0:00:00, wall=01:54 IST
=> Training   0.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=5.931 DataTime=5.678 Loss=0.952 Prec@1=74.414 Prec@5=91.406 rate=9025.43 Hz, eta=0:00:00, total=0:00:00, wall=01:54 IST
=> Training   0.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=5.931 DataTime=5.678 Loss=0.952 Prec@1=74.414 Prec@5=91.406 rate=9025.43 Hz, eta=0:00:00, total=0:00:00, wall=01:54 IST
=> Training   0.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.610 DataTime=0.443 Loss=0.814 Prec@1=79.308 Prec@5=93.686 rate=9025.43 Hz, eta=0:00:00, total=0:00:00, wall=01:54 IST
=> Training   4.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.610 DataTime=0.443 Loss=0.814 Prec@1=79.308 Prec@5=93.686 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=01:54 IST
=> Training   4.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.610 DataTime=0.443 Loss=0.814 Prec@1=79.308 Prec@5=93.686 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=01:55 IST
=> Training   4.04% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.594 DataTime=0.423 Loss=0.808 Prec@1=79.533 Prec@5=93.776 rate=1.81 Hz, eta=0:22:04, total=0:00:55, wall=01:55 IST
=> Training   8.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.594 DataTime=0.423 Loss=0.808 Prec@1=79.533 Prec@5=93.776 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=01:55 IST
=> Training   8.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.594 DataTime=0.423 Loss=0.808 Prec@1=79.533 Prec@5=93.776 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=01:56 IST
=> Training   8.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.583 DataTime=0.409 Loss=0.807 Prec@1=79.561 Prec@5=93.844 rate=1.77 Hz, eta=0:21:40, total=0:01:53, wall=01:56 IST
=> Training   12.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.583 DataTime=0.409 Loss=0.807 Prec@1=79.561 Prec@5=93.844 rate=1.78 Hz, eta=0:20:40, total=0:02:49, wall=01:56 IST
=> Training   12.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.583 DataTime=0.409 Loss=0.807 Prec@1=79.561 Prec@5=93.844 rate=1.78 Hz, eta=0:20:40, total=0:02:49, wall=01:57 IST
=> Training   12.03% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.576 DataTime=0.401 Loss=0.810 Prec@1=79.518 Prec@5=93.803 rate=1.78 Hz, eta=0:20:40, total=0:02:49, wall=01:57 IST
=> Training   16.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.576 DataTime=0.401 Loss=0.810 Prec@1=79.518 Prec@5=93.803 rate=1.78 Hz, eta=0:19:40, total=0:03:45, wall=01:57 IST
=> Training   16.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.576 DataTime=0.401 Loss=0.810 Prec@1=79.518 Prec@5=93.803 rate=1.78 Hz, eta=0:19:40, total=0:03:45, wall=01:58 IST
=> Training   16.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.811 Prec@1=79.454 Prec@5=93.819 rate=1.78 Hz, eta=0:19:40, total=0:03:45, wall=01:58 IST
=> Training   20.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.811 Prec@1=79.454 Prec@5=93.819 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=01:58 IST
=> Training   20.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.811 Prec@1=79.454 Prec@5=93.819 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=01:59 IST
=> Training   20.02% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.814 Prec@1=79.370 Prec@5=93.779 rate=1.79 Hz, eta=0:18:41, total=0:04:40, wall=01:59 IST
=> Training   24.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.814 Prec@1=79.370 Prec@5=93.779 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=01:59 IST
=> Training   24.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.572 DataTime=0.396 Loss=0.814 Prec@1=79.370 Prec@5=93.779 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=02:00 IST
=> Training   24.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.571 DataTime=0.394 Loss=0.815 Prec@1=79.365 Prec@5=93.766 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=02:00 IST
=> Training   28.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.571 DataTime=0.394 Loss=0.815 Prec@1=79.365 Prec@5=93.766 rate=1.78 Hz, eta=0:16:52, total=0:06:34, wall=02:00 IST
=> Training   28.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.571 DataTime=0.394 Loss=0.815 Prec@1=79.365 Prec@5=93.766 rate=1.78 Hz, eta=0:16:52, total=0:06:34, wall=02:01 IST
=> Training   28.01% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.393 Loss=0.816 Prec@1=79.313 Prec@5=93.761 rate=1.78 Hz, eta=0:16:52, total=0:06:34, wall=02:01 IST
=> Training   32.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.393 Loss=0.816 Prec@1=79.313 Prec@5=93.761 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=02:01 IST
=> Training   32.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.393 Loss=0.816 Prec@1=79.313 Prec@5=93.761 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=02:02 IST
=> Training   32.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.394 Loss=0.818 Prec@1=79.266 Prec@5=93.756 rate=1.78 Hz, eta=0:15:56, total=0:07:30, wall=02:02 IST
=> Training   36.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.394 Loss=0.818 Prec@1=79.266 Prec@5=93.756 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=02:02 IST
=> Training   36.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.394 Loss=0.818 Prec@1=79.266 Prec@5=93.756 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=02:03 IST
=> Training   36.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.395 Loss=0.819 Prec@1=79.241 Prec@5=93.762 rate=1.78 Hz, eta=0:15:02, total=0:08:27, wall=02:03 IST
=> Training   39.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.395 Loss=0.819 Prec@1=79.241 Prec@5=93.762 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=02:03 IST
=> Training   39.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.570 DataTime=0.395 Loss=0.819 Prec@1=79.241 Prec@5=93.762 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=02:04 IST
=> Training   39.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.393 Loss=0.819 Prec@1=79.218 Prec@5=93.765 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=02:04 IST
=> Training   43.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.393 Loss=0.819 Prec@1=79.218 Prec@5=93.765 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=02:04 IST
=> Training   43.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.393 Loss=0.819 Prec@1=79.218 Prec@5=93.765 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=02:05 IST
=> Training   43.99% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.820 Prec@1=79.196 Prec@5=93.757 rate=1.78 Hz, eta=0:13:08, total=0:10:19, wall=02:05 IST
=> Training   47.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.820 Prec@1=79.196 Prec@5=93.757 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=02:05 IST
=> Training   47.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.820 Prec@1=79.196 Prec@5=93.757 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=02:06 IST
=> Training   47.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.392 Loss=0.821 Prec@1=79.155 Prec@5=93.743 rate=1.78 Hz, eta=0:12:12, total=0:11:15, wall=02:06 IST
=> Training   51.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.392 Loss=0.821 Prec@1=79.155 Prec@5=93.743 rate=1.78 Hz, eta=0:11:16, total=0:12:12, wall=02:06 IST
=> Training   51.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.392 Loss=0.821 Prec@1=79.155 Prec@5=93.743 rate=1.78 Hz, eta=0:11:16, total=0:12:12, wall=02:07 IST
=> Training   51.98% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.822 Prec@1=79.123 Prec@5=93.735 rate=1.78 Hz, eta=0:11:16, total=0:12:12, wall=02:07 IST
=> Training   55.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.822 Prec@1=79.123 Prec@5=93.735 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=02:07 IST
=> Training   55.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.392 Loss=0.822 Prec@1=79.123 Prec@5=93.735 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=02:08 IST
=> Training   55.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.087 Prec@5=93.723 rate=1.78 Hz, eta=0:10:20, total=0:13:08, wall=02:08 IST
=> Training   59.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.087 Prec@5=93.723 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:08 IST
=> Training   59.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.087 Prec@5=93.723 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:09 IST
=> Training   59.97% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.062 Prec@5=93.719 rate=1.77 Hz, eta=0:09:24, total=0:14:05, wall=02:09 IST
=> Training   63.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.062 Prec@5=93.719 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:09 IST
=> Training   63.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.823 Prec@1=79.062 Prec@5=93.719 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:10 IST
=> Training   63.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.824 Prec@1=79.051 Prec@5=93.717 rate=1.77 Hz, eta=0:08:28, total=0:15:02, wall=02:10 IST
=> Training   67.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.824 Prec@1=79.051 Prec@5=93.717 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:10 IST
=> Training   67.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.824 Prec@1=79.051 Prec@5=93.717 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:11 IST
=> Training   67.96% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.569 DataTime=0.395 Loss=0.824 Prec@1=79.037 Prec@5=93.706 rate=1.77 Hz, eta=0:07:32, total=0:15:59, wall=02:11 IST
=> Training   71.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.569 DataTime=0.395 Loss=0.824 Prec@1=79.037 Prec@5=93.706 rate=1.77 Hz, eta=0:06:36, total=0:16:58, wall=02:11 IST
=> Training   71.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.569 DataTime=0.395 Loss=0.824 Prec@1=79.037 Prec@5=93.706 rate=1.77 Hz, eta=0:06:36, total=0:16:58, wall=02:11 IST
=> Training   71.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.018 Prec@5=93.706 rate=1.77 Hz, eta=0:06:36, total=0:16:58, wall=02:11 IST
=> Training   75.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.018 Prec@5=93.706 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=02:11 IST
=> Training   75.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.018 Prec@5=93.706 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=02:12 IST
=> Training   75.95% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.013 Prec@5=93.710 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=02:12 IST
=> Training   79.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.013 Prec@5=93.710 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=02:12 IST
=> Training   79.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.568 DataTime=0.394 Loss=0.825 Prec@1=79.013 Prec@5=93.710 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=02:13 IST
=> Training   79.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.826 Prec@1=78.999 Prec@5=93.698 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=02:13 IST
=> Training   83.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.826 Prec@1=78.999 Prec@5=93.698 rate=1.77 Hz, eta=0:03:46, total=0:19:45, wall=02:13 IST
=> Training   83.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.394 Loss=0.826 Prec@1=78.999 Prec@5=93.698 rate=1.77 Hz, eta=0:03:46, total=0:19:45, wall=02:14 IST
=> Training   83.94% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.826 Prec@1=78.994 Prec@5=93.694 rate=1.77 Hz, eta=0:03:46, total=0:19:45, wall=02:14 IST
=> Training   87.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.826 Prec@1=78.994 Prec@5=93.694 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=02:14 IST
=> Training   87.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.826 Prec@1=78.994 Prec@5=93.694 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=02:15 IST
=> Training   87.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=78.983 Prec@5=93.686 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=02:15 IST
=> Training   91.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=78.983 Prec@5=93.686 rate=1.77 Hz, eta=0:01:53, total=0:21:38, wall=02:15 IST
=> Training   91.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.567 DataTime=0.393 Loss=0.827 Prec@1=78.983 Prec@5=93.686 rate=1.77 Hz, eta=0:01:53, total=0:21:38, wall=02:16 IST
=> Training   91.93% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.393 Loss=0.828 Prec@1=78.954 Prec@5=93.685 rate=1.77 Hz, eta=0:01:53, total=0:21:38, wall=02:16 IST
=> Training   95.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.393 Loss=0.828 Prec@1=78.954 Prec@5=93.685 rate=1.77 Hz, eta=0:00:57, total=0:22:34, wall=02:16 IST
=> Training   95.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.393 Loss=0.828 Prec@1=78.954 Prec@5=93.685 rate=1.77 Hz, eta=0:00:57, total=0:22:34, wall=02:17 IST
=> Training   95.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.392 Loss=0.828 Prec@1=78.939 Prec@5=93.682 rate=1.77 Hz, eta=0:00:57, total=0:22:34, wall=02:17 IST
=> Training   99.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.392 Loss=0.828 Prec@1=78.939 Prec@5=93.682 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=02:17 IST
=> Training   99.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.566 DataTime=0.392 Loss=0.828 Prec@1=78.939 Prec@5=93.682 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=02:17 IST
=> Training   99.92% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.565 DataTime=0.392 Loss=0.828 Prec@1=78.939 Prec@5=93.682 rate=1.78 Hz, eta=0:00:01, total=0:23:28, wall=02:17 IST
=> Training   100.00% of 1x2503...Epoch=125/150 LR=0.0072 Time=0.565 DataTime=0.392 Loss=0.828 Prec@1=78.939 Prec@5=93.682 rate=1.78 Hz, eta=0:00:00, total=0:23:29, wall=02:17 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:17 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:17 IST
=> Validation 0.00% of 1x98...Epoch=125/150 LR=0.0072 Time=7.026 Loss=0.752 Prec@1=80.859 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=02:17 IST
=> Validation 1.02% of 1x98...Epoch=125/150 LR=0.0072 Time=7.026 Loss=0.752 Prec@1=80.859 Prec@5=95.312 rate=5867.48 Hz, eta=0:00:00, total=0:00:00, wall=02:17 IST
** Validation 1.02% of 1x98...Epoch=125/150 LR=0.0072 Time=7.026 Loss=0.752 Prec@1=80.859 Prec@5=95.312 rate=5867.48 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST
** Validation 1.02% of 1x98...Epoch=125/150 LR=0.0072 Time=0.636 Loss=1.210 Prec@1=70.650 Prec@5=89.652 rate=5867.48 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST
** Validation 100.00% of 1x98...Epoch=125/150 LR=0.0072 Time=0.636 Loss=1.210 Prec@1=70.650 Prec@5=89.652 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=02:18 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST
=> Training   0.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=5.410 DataTime=5.153 Loss=0.876 Prec@1=77.539 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=02:18 IST
=> Training   0.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=5.410 DataTime=5.153 Loss=0.876 Prec@1=77.539 Prec@5=93.750 rate=9756.19 Hz, eta=0:00:00, total=0:00:00, wall=02:18 IST
=> Training   0.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=5.410 DataTime=5.153 Loss=0.876 Prec@1=77.539 Prec@5=93.750 rate=9756.19 Hz, eta=0:00:00, total=0:00:00, wall=02:19 IST
=> Training   0.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.586 DataTime=0.431 Loss=0.800 Prec@1=79.717 Prec@5=93.947 rate=9756.19 Hz, eta=0:00:00, total=0:00:00, wall=02:19 IST
=> Training   4.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.586 DataTime=0.431 Loss=0.800 Prec@1=79.717 Prec@5=93.947 rate=1.88 Hz, eta=0:21:18, total=0:00:53, wall=02:19 IST
=> Training   4.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.586 DataTime=0.431 Loss=0.800 Prec@1=79.717 Prec@5=93.947 rate=1.88 Hz, eta=0:21:18, total=0:00:53, wall=02:20 IST
=> Training   4.04% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.570 DataTime=0.409 Loss=0.798 Prec@1=79.716 Prec@5=93.960 rate=1.88 Hz, eta=0:21:18, total=0:00:53, wall=02:20 IST
=> Training   8.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.570 DataTime=0.409 Loss=0.798 Prec@1=79.716 Prec@5=93.960 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=02:20 IST
=> Training   8.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.570 DataTime=0.409 Loss=0.798 Prec@1=79.716 Prec@5=93.960 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=02:21 IST
=> Training   8.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.572 DataTime=0.405 Loss=0.800 Prec@1=79.624 Prec@5=93.930 rate=1.84 Hz, eta=0:20:50, total=0:01:49, wall=02:21 IST
=> Training   12.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.572 DataTime=0.405 Loss=0.800 Prec@1=79.624 Prec@5=93.930 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=02:21 IST
=> Training   12.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.572 DataTime=0.405 Loss=0.800 Prec@1=79.624 Prec@5=93.930 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=02:22 IST
=> Training   12.03% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.396 Loss=0.800 Prec@1=79.651 Prec@5=93.975 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=02:22 IST
=> Training   16.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.396 Loss=0.800 Prec@1=79.651 Prec@5=93.975 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=02:22 IST
=> Training   16.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.396 Loss=0.800 Prec@1=79.651 Prec@5=93.975 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=02:23 IST
=> Training   16.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.395 Loss=0.804 Prec@1=79.504 Prec@5=93.926 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=02:23 IST
=> Training   20.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.395 Loss=0.804 Prec@1=79.504 Prec@5=93.926 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:23 IST
=> Training   20.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.566 DataTime=0.395 Loss=0.804 Prec@1=79.504 Prec@5=93.926 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:24 IST
=> Training   20.02% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.392 Loss=0.806 Prec@1=79.532 Prec@5=93.864 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=02:24 IST
=> Training   24.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.392 Loss=0.806 Prec@1=79.532 Prec@5=93.864 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=02:24 IST
=> Training   24.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.392 Loss=0.806 Prec@1=79.532 Prec@5=93.864 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=02:25 IST
=> Training   24.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.807 Prec@1=79.525 Prec@5=93.844 rate=1.80 Hz, eta=0:17:33, total=0:05:32, wall=02:25 IST
=> Training   28.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.807 Prec@1=79.525 Prec@5=93.844 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=02:25 IST
=> Training   28.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.807 Prec@1=79.525 Prec@5=93.844 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=02:26 IST
=> Training   28.01% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.809 Prec@1=79.448 Prec@5=93.842 rate=1.80 Hz, eta=0:16:39, total=0:06:28, wall=02:26 IST
=> Training   32.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.809 Prec@1=79.448 Prec@5=93.842 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:26 IST
=> Training   32.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.391 Loss=0.809 Prec@1=79.448 Prec@5=93.842 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:27 IST
=> Training   32.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.390 Loss=0.809 Prec@1=79.454 Prec@5=93.838 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=02:27 IST
=> Training   36.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.390 Loss=0.809 Prec@1=79.454 Prec@5=93.838 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=02:27 IST
=> Training   36.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.390 Loss=0.809 Prec@1=79.454 Prec@5=93.838 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=02:27 IST
=> Training   36.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.388 Loss=0.811 Prec@1=79.431 Prec@5=93.831 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=02:27 IST
=> Training   39.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.388 Loss=0.811 Prec@1=79.431 Prec@5=93.831 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=02:27 IST
=> Training   39.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.388 Loss=0.811 Prec@1=79.431 Prec@5=93.831 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=02:28 IST
=> Training   39.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.811 Prec@1=79.430 Prec@5=93.830 rate=1.80 Hz, eta=0:13:54, total=0:09:16, wall=02:28 IST
=> Training   43.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.811 Prec@1=79.430 Prec@5=93.830 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=02:28 IST
=> Training   43.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.811 Prec@1=79.430 Prec@5=93.830 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=02:29 IST
=> Training   43.99% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.812 Prec@1=79.424 Prec@5=93.832 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=02:29 IST
=> Training   47.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.812 Prec@1=79.424 Prec@5=93.832 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=02:29 IST
=> Training   47.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.812 Prec@1=79.424 Prec@5=93.832 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=02:30 IST
=> Training   47.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.813 Prec@1=79.395 Prec@5=93.822 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=02:30 IST
=> Training   51.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.813 Prec@1=79.395 Prec@5=93.822 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:30 IST
=> Training   51.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.813 Prec@1=79.395 Prec@5=93.822 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:31 IST
=> Training   51.98% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.814 Prec@1=79.367 Prec@5=93.812 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=02:31 IST
=> Training   55.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.814 Prec@1=79.367 Prec@5=93.812 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:31 IST
=> Training   55.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.388 Loss=0.814 Prec@1=79.367 Prec@5=93.812 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:32 IST
=> Training   55.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.388 Loss=0.815 Prec@1=79.349 Prec@5=93.814 rate=1.79 Hz, eta=0:10:15, total=0:13:01, wall=02:32 IST
=> Training   59.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.388 Loss=0.815 Prec@1=79.349 Prec@5=93.814 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=02:32 IST
=> Training   59.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.563 DataTime=0.388 Loss=0.815 Prec@1=79.349 Prec@5=93.814 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=02:33 IST
=> Training   59.97% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.386 Loss=0.815 Prec@1=79.343 Prec@5=93.810 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=02:33 IST
=> Training   63.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.386 Loss=0.815 Prec@1=79.343 Prec@5=93.810 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=02:33 IST
=> Training   63.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.562 DataTime=0.386 Loss=0.815 Prec@1=79.343 Prec@5=93.810 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=02:34 IST
=> Training   63.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.386 Loss=0.815 Prec@1=79.327 Prec@5=93.816 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=02:34 IST
=> Training   67.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.386 Loss=0.815 Prec@1=79.327 Prec@5=93.816 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=02:34 IST
=> Training   67.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.386 Loss=0.815 Prec@1=79.327 Prec@5=93.816 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=02:35 IST
=> Training   67.96% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.815 Prec@1=79.309 Prec@5=93.811 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=02:35 IST
=> Training   71.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.815 Prec@1=79.309 Prec@5=93.811 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=02:35 IST
=> Training   71.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.815 Prec@1=79.309 Prec@5=93.811 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=02:36 IST
=> Training   71.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.299 Prec@5=93.810 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=02:36 IST
=> Training   75.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.299 Prec@5=93.810 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=02:36 IST
=> Training   75.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.299 Prec@5=93.810 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=02:37 IST
=> Training   75.95% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.263 Prec@5=93.805 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=02:37 IST
=> Training   79.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.263 Prec@5=93.805 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=02:37 IST
=> Training   79.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.816 Prec@1=79.263 Prec@5=93.805 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=02:38 IST
=> Training   79.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.817 Prec@1=79.256 Prec@5=93.802 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=02:38 IST
=> Training   83.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.817 Prec@1=79.256 Prec@5=93.802 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=02:38 IST
=> Training   83.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.817 Prec@1=79.256 Prec@5=93.802 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=02:39 IST
=> Training   83.94% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.818 Prec@1=79.233 Prec@5=93.787 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=02:39 IST
=> Training   87.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.818 Prec@1=79.233 Prec@5=93.787 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=02:39 IST
=> Training   87.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.818 Prec@1=79.233 Prec@5=93.787 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=02:40 IST
=> Training   87.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.818 Prec@1=79.219 Prec@5=93.775 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=02:40 IST
=> Training   91.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.818 Prec@1=79.219 Prec@5=93.775 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=02:40 IST
=> Training   91.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.561 DataTime=0.387 Loss=0.818 Prec@1=79.219 Prec@5=93.775 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=02:41 IST
=> Training   91.93% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.819 Prec@1=79.201 Prec@5=93.777 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=02:41 IST
=> Training   95.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.819 Prec@1=79.201 Prec@5=93.777 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:41 IST
=> Training   95.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.819 Prec@1=79.201 Prec@5=93.777 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:41 IST
=> Training   95.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.819 Prec@1=79.192 Prec@5=93.777 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=02:41 IST
=> Training   99.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.819 Prec@1=79.192 Prec@5=93.777 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=02:41 IST
=> Training   99.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.386 Loss=0.819 Prec@1=79.192 Prec@5=93.777 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=02:41 IST
=> Training   99.92% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.819 Prec@1=79.192 Prec@5=93.776 rate=1.79 Hz, eta=0:00:01, total=0:23:14, wall=02:41 IST
=> Training   100.00% of 1x2503...Epoch=126/150 LR=0.0067 Time=0.560 DataTime=0.385 Loss=0.819 Prec@1=79.192 Prec@5=93.776 rate=1.79 Hz, eta=0:00:00, total=0:23:15, wall=02:41 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST
=> Validation 0.00% of 1x98...Epoch=126/150 LR=0.0067 Time=6.957 Loss=0.758 Prec@1=79.297 Prec@5=94.531 rate=0 Hz, eta=?, total=0:00:00, wall=02:42 IST
=> Validation 1.02% of 1x98...Epoch=126/150 LR=0.0067 Time=6.957 Loss=0.758 Prec@1=79.297 Prec@5=94.531 rate=3171.80 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST
** Validation 1.02% of 1x98...Epoch=126/150 LR=0.0067 Time=6.957 Loss=0.758 Prec@1=79.297 Prec@5=94.531 rate=3171.80 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST
** Validation 1.02% of 1x98...Epoch=126/150 LR=0.0067 Time=0.638 Loss=1.208 Prec@1=70.744 Prec@5=89.712 rate=3171.80 Hz, eta=0:00:00, total=0:00:00, wall=02:42 IST
** Validation 100.00% of 1x98...Epoch=126/150 LR=0.0067 Time=0.638 Loss=1.208 Prec@1=70.744 Prec@5=89.712 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=02:42 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:43 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=02:43 IST
=> Training   0.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=4.811 DataTime=4.229 Loss=0.840 Prec@1=78.711 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=02:43 IST
=> Training   0.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=4.811 DataTime=4.229 Loss=0.840 Prec@1=78.711 Prec@5=94.141 rate=6378.41 Hz, eta=0:00:00, total=0:00:00, wall=02:43 IST
=> Training   0.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=4.811 DataTime=4.229 Loss=0.840 Prec@1=78.711 Prec@5=94.141 rate=6378.41 Hz, eta=0:00:00, total=0:00:00, wall=02:43 IST
=> Training   0.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.578 DataTime=0.408 Loss=0.809 Prec@1=79.633 Prec@5=93.827 rate=6378.41 Hz, eta=0:00:00, total=0:00:00, wall=02:43 IST
=> Training   4.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.578 DataTime=0.408 Loss=0.809 Prec@1=79.633 Prec@5=93.827 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=02:43 IST
=> Training   4.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.578 DataTime=0.408 Loss=0.809 Prec@1=79.633 Prec@5=93.827 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=02:44 IST
=> Training   4.04% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.566 DataTime=0.398 Loss=0.805 Prec@1=79.701 Prec@5=93.935 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=02:44 IST
=> Training   8.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.566 DataTime=0.398 Loss=0.805 Prec@1=79.701 Prec@5=93.935 rate=1.85 Hz, eta=0:20:47, total=0:01:48, wall=02:44 IST
=> Training   8.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.566 DataTime=0.398 Loss=0.805 Prec@1=79.701 Prec@5=93.935 rate=1.85 Hz, eta=0:20:47, total=0:01:48, wall=02:45 IST
=> Training   8.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.394 Loss=0.807 Prec@1=79.663 Prec@5=93.862 rate=1.85 Hz, eta=0:20:47, total=0:01:48, wall=02:45 IST
=> Training   12.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.394 Loss=0.807 Prec@1=79.663 Prec@5=93.862 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=02:45 IST
=> Training   12.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.394 Loss=0.807 Prec@1=79.663 Prec@5=93.862 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=02:46 IST
=> Training   12.03% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.391 Loss=0.802 Prec@1=79.720 Prec@5=93.924 rate=1.83 Hz, eta=0:20:04, total=0:02:44, wall=02:46 IST
=> Training   16.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.391 Loss=0.802 Prec@1=79.720 Prec@5=93.924 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=02:46 IST
=> Training   16.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.563 DataTime=0.391 Loss=0.802 Prec@1=79.720 Prec@5=93.924 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=02:47 IST
=> Training   16.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.562 DataTime=0.388 Loss=0.801 Prec@1=79.741 Prec@5=93.928 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=02:47 IST
=> Training   20.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.562 DataTime=0.388 Loss=0.801 Prec@1=79.741 Prec@5=93.928 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=02:47 IST
=> Training   20.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.562 DataTime=0.388 Loss=0.801 Prec@1=79.741 Prec@5=93.928 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=02:48 IST
=> Training   20.02% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.386 Loss=0.800 Prec@1=79.723 Prec@5=93.946 rate=1.81 Hz, eta=0:18:26, total=0:04:36, wall=02:48 IST
=> Training   24.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.386 Loss=0.800 Prec@1=79.723 Prec@5=93.946 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=02:48 IST
=> Training   24.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.386 Loss=0.800 Prec@1=79.723 Prec@5=93.946 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=02:49 IST
=> Training   24.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.385 Loss=0.801 Prec@1=79.698 Prec@5=93.942 rate=1.81 Hz, eta=0:17:31, total=0:05:32, wall=02:49 IST
=> Training   28.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.385 Loss=0.801 Prec@1=79.698 Prec@5=93.942 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=02:49 IST
=> Training   28.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.561 DataTime=0.385 Loss=0.801 Prec@1=79.698 Prec@5=93.942 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=02:50 IST
=> Training   28.01% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.383 Loss=0.802 Prec@1=79.667 Prec@5=93.944 rate=1.81 Hz, eta=0:16:37, total=0:06:28, wall=02:50 IST
=> Training   32.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.383 Loss=0.802 Prec@1=79.667 Prec@5=93.944 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=02:50 IST
=> Training   32.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.383 Loss=0.802 Prec@1=79.667 Prec@5=93.944 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=02:51 IST
=> Training   32.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.803 Prec@1=79.638 Prec@5=93.929 rate=1.81 Hz, eta=0:15:40, total=0:07:22, wall=02:51 IST
=> Training   36.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.803 Prec@1=79.638 Prec@5=93.929 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=02:51 IST
=> Training   36.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.803 Prec@1=79.638 Prec@5=93.929 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=02:52 IST
=> Training   36.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.803 Prec@1=79.641 Prec@5=93.925 rate=1.80 Hz, eta=0:14:47, total=0:08:19, wall=02:52 IST
=> Training   39.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.803 Prec@1=79.641 Prec@5=93.925 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=02:52 IST
=> Training   39.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.803 Prec@1=79.641 Prec@5=93.925 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=02:53 IST
=> Training   39.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.804 Prec@1=79.639 Prec@5=93.922 rate=1.80 Hz, eta=0:13:53, total=0:09:15, wall=02:53 IST
=> Training   43.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.804 Prec@1=79.639 Prec@5=93.922 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=02:53 IST
=> Training   43.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.804 Prec@1=79.639 Prec@5=93.922 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=02:54 IST
=> Training   43.99% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.803 Prec@1=79.645 Prec@5=93.925 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=02:54 IST
=> Training   47.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.803 Prec@1=79.645 Prec@5=93.925 rate=1.80 Hz, eta=0:12:04, total=0:11:07, wall=02:54 IST
=> Training   47.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.803 Prec@1=79.645 Prec@5=93.925 rate=1.80 Hz, eta=0:12:04, total=0:11:07, wall=02:55 IST
=> Training   47.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.604 Prec@5=93.931 rate=1.80 Hz, eta=0:12:04, total=0:11:07, wall=02:55 IST
=> Training   51.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.604 Prec@5=93.931 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=02:55 IST
=> Training   51.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.604 Prec@5=93.931 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=02:56 IST
=> Training   51.98% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.590 Prec@5=93.928 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=02:56 IST
=> Training   55.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.590 Prec@5=93.928 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=02:56 IST
=> Training   55.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.804 Prec@1=79.590 Prec@5=93.928 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=02:56 IST
=> Training   55.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.806 Prec@1=79.554 Prec@5=93.904 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=02:56 IST
=> Training   59.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.806 Prec@1=79.554 Prec@5=93.904 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=02:56 IST
=> Training   59.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.806 Prec@1=79.554 Prec@5=93.904 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=02:57 IST
=> Training   59.97% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.806 Prec@1=79.539 Prec@5=93.903 rate=1.80 Hz, eta=0:09:17, total=0:13:54, wall=02:57 IST
=> Training   63.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.806 Prec@1=79.539 Prec@5=93.903 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:57 IST
=> Training   63.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.806 Prec@1=79.539 Prec@5=93.903 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:58 IST
=> Training   63.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.384 Loss=0.807 Prec@1=79.515 Prec@5=93.893 rate=1.80 Hz, eta=0:08:22, total=0:14:51, wall=02:58 IST
=> Training   67.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.384 Loss=0.807 Prec@1=79.515 Prec@5=93.893 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=02:58 IST
=> Training   67.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.384 Loss=0.807 Prec@1=79.515 Prec@5=93.893 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=02:59 IST
=> Training   67.96% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.807 Prec@1=79.516 Prec@5=93.889 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=02:59 IST
=> Training   71.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.807 Prec@1=79.516 Prec@5=93.889 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=02:59 IST
=> Training   71.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.807 Prec@1=79.516 Prec@5=93.889 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=03:00 IST
=> Training   71.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.808 Prec@1=79.512 Prec@5=93.885 rate=1.80 Hz, eta=0:06:30, total=0:16:42, wall=03:00 IST
=> Training   75.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.808 Prec@1=79.512 Prec@5=93.885 rate=1.80 Hz, eta=0:05:35, total=0:17:37, wall=03:00 IST
=> Training   75.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.559 DataTime=0.385 Loss=0.808 Prec@1=79.512 Prec@5=93.885 rate=1.80 Hz, eta=0:05:35, total=0:17:37, wall=03:01 IST
=> Training   75.95% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.808 Prec@1=79.501 Prec@5=93.883 rate=1.80 Hz, eta=0:05:35, total=0:17:37, wall=03:01 IST
=> Training   79.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.808 Prec@1=79.501 Prec@5=93.883 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:01 IST
=> Training   79.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.808 Prec@1=79.501 Prec@5=93.883 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:02 IST
=> Training   79.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.808 Prec@1=79.489 Prec@5=93.873 rate=1.79 Hz, eta=0:04:39, total=0:18:35, wall=03:02 IST
=> Training   83.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.808 Prec@1=79.489 Prec@5=93.873 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:02 IST
=> Training   83.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.808 Prec@1=79.489 Prec@5=93.873 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:03 IST
=> Training   83.94% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.809 Prec@1=79.466 Prec@5=93.858 rate=1.79 Hz, eta=0:03:44, total=0:19:32, wall=03:03 IST
=> Training   87.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.809 Prec@1=79.466 Prec@5=93.858 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:03 IST
=> Training   87.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.386 Loss=0.809 Prec@1=79.466 Prec@5=93.858 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:04 IST
=> Training   87.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.447 Prec@5=93.860 rate=1.79 Hz, eta=0:02:48, total=0:20:28, wall=03:04 IST
=> Training   91.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.447 Prec@5=93.860 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:04 IST
=> Training   91.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.447 Prec@5=93.860 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:05 IST
=> Training   91.93% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.421 Prec@5=93.856 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:05 IST
=> Training   95.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.421 Prec@5=93.856 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:05 IST
=> Training   95.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.810 Prec@1=79.421 Prec@5=93.856 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:06 IST
=> Training   95.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.811 Prec@1=79.402 Prec@5=93.850 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=03:06 IST
=> Training   99.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.811 Prec@1=79.402 Prec@5=93.850 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:06 IST
=> Training   99.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.811 Prec@1=79.402 Prec@5=93.850 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:06 IST
=> Training   99.92% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.811 Prec@1=79.401 Prec@5=93.850 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:06 IST
=> Training   100.00% of 1x2503...Epoch=127/150 LR=0.0062 Time=0.560 DataTime=0.385 Loss=0.811 Prec@1=79.401 Prec@5=93.850 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=03:06 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:06 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:06 IST
=> Validation 0.00% of 1x98...Epoch=127/150 LR=0.0062 Time=7.136 Loss=0.758 Prec@1=78.906 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=03:06 IST
=> Validation 1.02% of 1x98...Epoch=127/150 LR=0.0062 Time=7.136 Loss=0.758 Prec@1=78.906 Prec@5=94.727 rate=3003.72 Hz, eta=0:00:00, total=0:00:00, wall=03:06 IST
** Validation 1.02% of 1x98...Epoch=127/150 LR=0.0062 Time=7.136 Loss=0.758 Prec@1=78.906 Prec@5=94.727 rate=3003.72 Hz, eta=0:00:00, total=0:00:00, wall=03:07 IST
** Validation 1.02% of 1x98...Epoch=127/150 LR=0.0062 Time=0.634 Loss=1.204 Prec@1=70.812 Prec@5=89.864 rate=3003.72 Hz, eta=0:00:00, total=0:00:00, wall=03:07 IST
** Validation 100.00% of 1x98...Epoch=127/150 LR=0.0062 Time=0.634 Loss=1.204 Prec@1=70.812 Prec@5=89.864 rate=1.78 Hz, eta=0:00:00, total=0:00:54, wall=03:07 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:07 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:07 IST
=> Training   0.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=5.281 DataTime=5.029 Loss=0.872 Prec@1=79.297 Prec@5=92.969 rate=0 Hz, eta=?, total=0:00:00, wall=03:07 IST
=> Training   0.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=5.281 DataTime=5.029 Loss=0.872 Prec@1=79.297 Prec@5=92.969 rate=8730.42 Hz, eta=0:00:00, total=0:00:00, wall=03:07 IST
=> Training   0.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=5.281 DataTime=5.029 Loss=0.872 Prec@1=79.297 Prec@5=92.969 rate=8730.42 Hz, eta=0:00:00, total=0:00:00, wall=03:08 IST
=> Training   0.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.589 DataTime=0.425 Loss=0.779 Prec@1=80.072 Prec@5=94.315 rate=8730.42 Hz, eta=0:00:00, total=0:00:00, wall=03:08 IST
=> Training   4.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.589 DataTime=0.425 Loss=0.779 Prec@1=80.072 Prec@5=94.315 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=03:08 IST
=> Training   4.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.589 DataTime=0.425 Loss=0.779 Prec@1=80.072 Prec@5=94.315 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=03:09 IST
=> Training   4.04% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.579 DataTime=0.409 Loss=0.785 Prec@1=79.916 Prec@5=94.214 rate=1.86 Hz, eta=0:21:30, total=0:00:54, wall=03:09 IST
=> Training   8.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.579 DataTime=0.409 Loss=0.785 Prec@1=79.916 Prec@5=94.214 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=03:09 IST
=> Training   8.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.579 DataTime=0.409 Loss=0.785 Prec@1=79.916 Prec@5=94.214 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=03:10 IST
=> Training   8.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.787 Prec@1=79.841 Prec@5=94.154 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=03:10 IST
=> Training   12.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.787 Prec@1=79.841 Prec@5=94.154 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=03:10 IST
=> Training   12.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.787 Prec@1=79.841 Prec@5=94.154 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=03:11 IST
=> Training   12.03% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.792 Prec@1=79.788 Prec@5=94.110 rate=1.82 Hz, eta=0:20:12, total=0:02:45, wall=03:11 IST
=> Training   16.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.792 Prec@1=79.788 Prec@5=94.110 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=03:11 IST
=> Training   16.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.568 DataTime=0.396 Loss=0.792 Prec@1=79.788 Prec@5=94.110 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=03:12 IST
=> Training   16.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.566 DataTime=0.393 Loss=0.793 Prec@1=79.829 Prec@5=94.092 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=03:12 IST
=> Training   20.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.566 DataTime=0.393 Loss=0.793 Prec@1=79.829 Prec@5=94.092 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=03:12 IST
=> Training   20.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.566 DataTime=0.393 Loss=0.793 Prec@1=79.829 Prec@5=94.092 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=03:13 IST
=> Training   20.02% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.792 Prec@1=79.864 Prec@5=94.074 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=03:13 IST
=> Training   24.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.792 Prec@1=79.864 Prec@5=94.074 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=03:13 IST
=> Training   24.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.792 Prec@1=79.864 Prec@5=94.074 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=03:14 IST
=> Training   24.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.793 Prec@1=79.874 Prec@5=94.068 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=03:14 IST
=> Training   28.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.793 Prec@1=79.874 Prec@5=94.068 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=03:14 IST
=> Training   28.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.565 DataTime=0.392 Loss=0.793 Prec@1=79.874 Prec@5=94.068 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=03:14 IST
=> Training   28.01% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.792 Prec@1=79.883 Prec@5=94.071 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=03:14 IST
=> Training   32.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.792 Prec@1=79.883 Prec@5=94.071 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:14 IST
=> Training   32.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.792 Prec@1=79.883 Prec@5=94.071 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:15 IST
=> Training   32.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.794 Prec@1=79.841 Prec@5=94.052 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=03:15 IST
=> Training   36.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.794 Prec@1=79.841 Prec@5=94.052 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:15 IST
=> Training   36.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.564 DataTime=0.391 Loss=0.794 Prec@1=79.841 Prec@5=94.052 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:16 IST
=> Training   36.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.795 Prec@1=79.823 Prec@5=94.036 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=03:16 IST
=> Training   39.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.795 Prec@1=79.823 Prec@5=94.036 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:16 IST
=> Training   39.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.795 Prec@1=79.823 Prec@5=94.036 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:17 IST
=> Training   39.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.796 Prec@1=79.807 Prec@5=94.047 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=03:17 IST
=> Training   43.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.796 Prec@1=79.807 Prec@5=94.047 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:17 IST
=> Training   43.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.563 DataTime=0.389 Loss=0.796 Prec@1=79.807 Prec@5=94.047 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:18 IST
=> Training   43.99% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.796 Prec@1=79.782 Prec@5=94.029 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=03:18 IST
=> Training   47.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.796 Prec@1=79.782 Prec@5=94.029 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=03:18 IST
=> Training   47.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.796 Prec@1=79.782 Prec@5=94.029 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=03:19 IST
=> Training   47.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.389 Loss=0.797 Prec@1=79.758 Prec@5=94.029 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=03:19 IST
=> Training   51.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.389 Loss=0.797 Prec@1=79.758 Prec@5=94.029 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=03:19 IST
=> Training   51.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.389 Loss=0.797 Prec@1=79.758 Prec@5=94.029 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=03:20 IST
=> Training   51.98% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.773 Prec@5=94.033 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=03:20 IST
=> Training   55.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.773 Prec@5=94.033 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=03:20 IST
=> Training   55.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.773 Prec@5=94.033 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=03:21 IST
=> Training   55.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.774 Prec@5=94.027 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=03:21 IST
=> Training   59.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.774 Prec@5=94.027 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=03:21 IST
=> Training   59.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.388 Loss=0.797 Prec@1=79.774 Prec@5=94.027 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=03:22 IST
=> Training   59.97% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.784 Prec@5=94.026 rate=1.79 Hz, eta=0:09:18, total=0:13:57, wall=03:22 IST
=> Training   63.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.784 Prec@5=94.026 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=03:22 IST
=> Training   63.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.784 Prec@5=94.026 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=03:23 IST
=> Training   63.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.770 Prec@5=94.032 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=03:23 IST
=> Training   67.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.770 Prec@5=94.032 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=03:23 IST
=> Training   67.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.561 DataTime=0.387 Loss=0.797 Prec@1=79.770 Prec@5=94.032 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=03:24 IST
=> Training   67.96% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.798 Prec@1=79.754 Prec@5=94.015 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=03:24 IST
=> Training   71.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.798 Prec@1=79.754 Prec@5=94.015 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:24 IST
=> Training   71.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.798 Prec@1=79.754 Prec@5=94.015 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:25 IST
=> Training   71.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.798 Prec@1=79.745 Prec@5=94.017 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=03:25 IST
=> Training   75.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.798 Prec@1=79.745 Prec@5=94.017 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=03:25 IST
=> Training   75.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.798 Prec@1=79.745 Prec@5=94.017 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=03:26 IST
=> Training   75.95% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.799 Prec@1=79.742 Prec@5=94.015 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=03:26 IST
=> Training   79.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.799 Prec@1=79.742 Prec@5=94.015 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=03:26 IST
=> Training   79.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.799 Prec@1=79.742 Prec@5=94.015 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=03:27 IST
=> Training   79.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.799 Prec@1=79.724 Prec@5=94.007 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=03:27 IST
=> Training   83.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.799 Prec@1=79.724 Prec@5=94.007 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=03:27 IST
=> Training   83.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.799 Prec@1=79.724 Prec@5=94.007 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=03:27 IST
=> Training   83.94% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.700 Prec@5=93.997 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=03:27 IST
=> Training   87.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.700 Prec@5=93.997 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=03:27 IST
=> Training   87.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.700 Prec@5=93.997 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=03:28 IST
=> Training   87.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.695 Prec@5=93.992 rate=1.79 Hz, eta=0:02:48, total=0:20:26, wall=03:28 IST
=> Training   91.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.695 Prec@5=93.992 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:28 IST
=> Training   91.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.387 Loss=0.800 Prec@1=79.695 Prec@5=93.992 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:29 IST
=> Training   91.93% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.800 Prec@1=79.698 Prec@5=93.993 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=03:29 IST
=> Training   95.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.800 Prec@1=79.698 Prec@5=93.993 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=03:29 IST
=> Training   95.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.560 DataTime=0.386 Loss=0.800 Prec@1=79.698 Prec@5=93.993 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=03:30 IST
=> Training   95.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.559 DataTime=0.386 Loss=0.801 Prec@1=79.699 Prec@5=93.986 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=03:30 IST
=> Training   99.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.559 DataTime=0.386 Loss=0.801 Prec@1=79.699 Prec@5=93.986 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=03:30 IST
=> Training   99.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.559 DataTime=0.386 Loss=0.801 Prec@1=79.699 Prec@5=93.986 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=03:30 IST
=> Training   99.92% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.559 DataTime=0.386 Loss=0.801 Prec@1=79.698 Prec@5=93.985 rate=1.79 Hz, eta=0:00:01, total=0:23:13, wall=03:30 IST
=> Training   100.00% of 1x2503...Epoch=128/150 LR=0.0057 Time=0.559 DataTime=0.386 Loss=0.801 Prec@1=79.698 Prec@5=93.985 rate=1.80 Hz, eta=0:00:00, total=0:23:14, wall=03:30 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:30 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:30 IST
=> Validation 0.00% of 1x98...Epoch=128/150 LR=0.0057 Time=6.655 Loss=0.687 Prec@1=80.664 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=03:30 IST
=> Validation 1.02% of 1x98...Epoch=128/150 LR=0.0057 Time=6.655 Loss=0.687 Prec@1=80.664 Prec@5=95.508 rate=7173.35 Hz, eta=0:00:00, total=0:00:00, wall=03:30 IST
** Validation 1.02% of 1x98...Epoch=128/150 LR=0.0057 Time=6.655 Loss=0.687 Prec@1=80.664 Prec@5=95.508 rate=7173.35 Hz, eta=0:00:00, total=0:00:00, wall=03:31 IST
** Validation 1.02% of 1x98...Epoch=128/150 LR=0.0057 Time=0.631 Loss=1.205 Prec@1=70.982 Prec@5=89.872 rate=7173.35 Hz, eta=0:00:00, total=0:00:00, wall=03:31 IST
** Validation 100.00% of 1x98...Epoch=128/150 LR=0.0057 Time=0.631 Loss=1.205 Prec@1=70.982 Prec@5=89.872 rate=1.78 Hz, eta=0:00:00, total=0:00:55, wall=03:31 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> Training   0.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=4.985 DataTime=4.630 Loss=0.761 Prec@1=80.859 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=03:31 IST
=> Training   0.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=4.985 DataTime=4.630 Loss=0.761 Prec@1=80.859 Prec@5=94.727 rate=6319.39 Hz, eta=0:00:00, total=0:00:00, wall=03:31 IST
=> Training   0.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=4.985 DataTime=4.630 Loss=0.761 Prec@1=80.859 Prec@5=94.727 rate=6319.39 Hz, eta=0:00:00, total=0:00:00, wall=03:32 IST
=> Training   0.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.588 DataTime=0.419 Loss=0.773 Prec@1=80.463 Prec@5=94.232 rate=6319.39 Hz, eta=0:00:00, total=0:00:00, wall=03:32 IST
=> Training   4.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.588 DataTime=0.419 Loss=0.773 Prec@1=80.463 Prec@5=94.232 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=03:32 IST
=> Training   4.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.588 DataTime=0.419 Loss=0.773 Prec@1=80.463 Prec@5=94.232 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=03:33 IST
=> Training   4.04% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.582 DataTime=0.411 Loss=0.773 Prec@1=80.353 Prec@5=94.288 rate=1.86 Hz, eta=0:21:34, total=0:00:54, wall=03:33 IST
=> Training   8.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.582 DataTime=0.411 Loss=0.773 Prec@1=80.353 Prec@5=94.288 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=03:33 IST
=> Training   8.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.582 DataTime=0.411 Loss=0.773 Prec@1=80.353 Prec@5=94.288 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=03:34 IST
=> Training   8.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.578 DataTime=0.405 Loss=0.777 Prec@1=80.231 Prec@5=94.244 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=03:34 IST
=> Training   12.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.578 DataTime=0.405 Loss=0.777 Prec@1=80.231 Prec@5=94.244 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=03:34 IST
=> Training   12.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.578 DataTime=0.405 Loss=0.777 Prec@1=80.231 Prec@5=94.244 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=03:35 IST
=> Training   12.03% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.570 DataTime=0.397 Loss=0.779 Prec@1=80.201 Prec@5=94.212 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=03:35 IST
=> Training   16.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.570 DataTime=0.397 Loss=0.779 Prec@1=80.201 Prec@5=94.212 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=03:35 IST
=> Training   16.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.570 DataTime=0.397 Loss=0.779 Prec@1=80.201 Prec@5=94.212 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=03:36 IST
=> Training   16.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.779 Prec@1=80.230 Prec@5=94.239 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=03:36 IST
=> Training   20.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.779 Prec@1=80.230 Prec@5=94.239 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=03:36 IST
=> Training   20.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.779 Prec@1=80.230 Prec@5=94.239 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=03:37 IST
=> Training   20.02% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.782 Prec@1=80.106 Prec@5=94.188 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=03:37 IST
=> Training   24.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.782 Prec@1=80.106 Prec@5=94.188 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=03:37 IST
=> Training   24.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.567 DataTime=0.394 Loss=0.782 Prec@1=80.106 Prec@5=94.188 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=03:38 IST
=> Training   24.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.568 DataTime=0.395 Loss=0.782 Prec@1=80.131 Prec@5=94.194 rate=1.79 Hz, eta=0:17:43, total=0:05:35, wall=03:38 IST
=> Training   28.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.568 DataTime=0.395 Loss=0.782 Prec@1=80.131 Prec@5=94.194 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=03:38 IST
=> Training   28.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.568 DataTime=0.395 Loss=0.782 Prec@1=80.131 Prec@5=94.194 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=03:39 IST
=> Training   28.01% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.391 Loss=0.783 Prec@1=80.085 Prec@5=94.185 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=03:39 IST
=> Training   32.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.391 Loss=0.783 Prec@1=80.085 Prec@5=94.185 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=03:39 IST
=> Training   32.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.391 Loss=0.783 Prec@1=80.085 Prec@5=94.185 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=03:40 IST
=> Training   32.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.784 Prec@1=80.079 Prec@5=94.179 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=03:40 IST
=> Training   36.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.784 Prec@1=80.079 Prec@5=94.179 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:40 IST
=> Training   36.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.784 Prec@1=80.079 Prec@5=94.179 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:41 IST
=> Training   36.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.786 Prec@1=80.040 Prec@5=94.155 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=03:41 IST
=> Training   39.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.786 Prec@1=80.040 Prec@5=94.155 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=03:41 IST
=> Training   39.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.786 Prec@1=80.040 Prec@5=94.155 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=03:42 IST
=> Training   39.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.787 Prec@1=80.008 Prec@5=94.148 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=03:42 IST
=> Training   43.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.787 Prec@1=80.008 Prec@5=94.148 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=03:42 IST
=> Training   43.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.565 DataTime=0.391 Loss=0.787 Prec@1=80.008 Prec@5=94.148 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=03:43 IST
=> Training   43.99% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.787 Prec@1=80.012 Prec@5=94.138 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=03:43 IST
=> Training   47.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.787 Prec@1=80.012 Prec@5=94.138 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:43 IST
=> Training   47.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.787 Prec@1=80.012 Prec@5=94.138 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:44 IST
=> Training   47.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.788 Prec@1=79.996 Prec@5=94.121 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=03:44 IST
=> Training   51.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.788 Prec@1=79.996 Prec@5=94.121 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=03:44 IST
=> Training   51.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.564 DataTime=0.390 Loss=0.788 Prec@1=79.996 Prec@5=94.121 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=03:44 IST
=> Training   51.98% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.788 Prec@1=79.981 Prec@5=94.110 rate=1.79 Hz, eta=0:11:12, total=0:12:08, wall=03:44 IST
=> Training   55.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.788 Prec@1=79.981 Prec@5=94.110 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:44 IST
=> Training   55.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.788 Prec@1=79.981 Prec@5=94.110 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:45 IST
=> Training   55.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.945 Prec@5=94.096 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=03:45 IST
=> Training   59.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.945 Prec@5=94.096 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:45 IST
=> Training   59.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.945 Prec@5=94.096 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:46 IST
=> Training   59.97% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.943 Prec@5=94.098 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=03:46 IST
=> Training   63.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.943 Prec@5=94.098 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=03:46 IST
=> Training   63.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.790 Prec@1=79.943 Prec@5=94.098 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=03:47 IST
=> Training   63.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.791 Prec@1=79.923 Prec@5=94.093 rate=1.79 Hz, eta=0:08:23, total=0:14:54, wall=03:47 IST
=> Training   67.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.791 Prec@1=79.923 Prec@5=94.093 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=03:47 IST
=> Training   67.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.388 Loss=0.791 Prec@1=79.923 Prec@5=94.093 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=03:48 IST
=> Training   67.96% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.916 Prec@5=94.086 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=03:48 IST
=> Training   71.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.916 Prec@5=94.086 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=03:48 IST
=> Training   71.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.916 Prec@5=94.086 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=03:49 IST
=> Training   71.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.792 Prec@1=79.908 Prec@5=94.084 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=03:49 IST
=> Training   75.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.792 Prec@1=79.908 Prec@5=94.084 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=03:49 IST
=> Training   75.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.792 Prec@1=79.908 Prec@5=94.084 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=03:50 IST
=> Training   75.95% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.903 Prec@5=94.085 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=03:50 IST
=> Training   79.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.903 Prec@5=94.085 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:50 IST
=> Training   79.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.562 DataTime=0.389 Loss=0.791 Prec@1=79.903 Prec@5=94.085 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:51 IST
=> Training   79.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.791 Prec@1=79.900 Prec@5=94.081 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=03:51 IST
=> Training   83.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.791 Prec@1=79.900 Prec@5=94.081 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:51 IST
=> Training   83.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.791 Prec@1=79.900 Prec@5=94.081 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:52 IST
=> Training   83.94% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.893 Prec@5=94.076 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=03:52 IST
=> Training   87.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.893 Prec@5=94.076 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=03:52 IST
=> Training   87.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.893 Prec@5=94.076 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=03:53 IST
=> Training   87.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.883 Prec@5=94.070 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=03:53 IST
=> Training   91.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.883 Prec@5=94.070 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=03:53 IST
=> Training   91.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.883 Prec@5=94.070 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=03:54 IST
=> Training   91.93% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.875 Prec@5=94.064 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=03:54 IST
=> Training   95.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.875 Prec@5=94.064 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:54 IST
=> Training   95.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.792 Prec@1=79.875 Prec@5=94.064 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:55 IST
=> Training   95.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.793 Prec@1=79.862 Prec@5=94.060 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=03:55 IST
=> Training   99.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.793 Prec@1=79.862 Prec@5=94.060 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:55 IST
=> Training   99.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.561 DataTime=0.388 Loss=0.793 Prec@1=79.862 Prec@5=94.060 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:55 IST
=> Training   99.92% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.560 DataTime=0.387 Loss=0.793 Prec@1=79.863 Prec@5=94.060 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=03:55 IST
=> Training   100.00% of 1x2503...Epoch=129/150 LR=0.0052 Time=0.560 DataTime=0.387 Loss=0.793 Prec@1=79.863 Prec@5=94.060 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=03:55 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> Validation 0.00% of 1x98...Epoch=129/150 LR=0.0052 Time=6.866 Loss=0.698 Prec@1=81.250 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=03:55 IST
=> Validation 1.02% of 1x98...Epoch=129/150 LR=0.0052 Time=6.866 Loss=0.698 Prec@1=81.250 Prec@5=94.727 rate=5835.63 Hz, eta=0:00:00, total=0:00:00, wall=03:55 IST
** Validation 1.02% of 1x98...Epoch=129/150 LR=0.0052 Time=6.866 Loss=0.698 Prec@1=81.250 Prec@5=94.727 rate=5835.63 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST
** Validation 1.02% of 1x98...Epoch=129/150 LR=0.0052 Time=0.629 Loss=1.201 Prec@1=71.092 Prec@5=89.992 rate=5835.63 Hz, eta=0:00:00, total=0:00:00, wall=03:56 IST
** Validation 100.00% of 1x98...Epoch=129/150 LR=0.0052 Time=0.629 Loss=1.201 Prec@1=71.092 Prec@5=89.992 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=03:56 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> Training   0.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=4.866 DataTime=4.539 Loss=0.802 Prec@1=81.055 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=03:56 IST
=> Training   0.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=4.866 DataTime=4.539 Loss=0.802 Prec@1=81.055 Prec@5=94.336 rate=2044.67 Hz, eta=0:00:01, total=0:00:00, wall=03:56 IST
=> Training   0.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=4.866 DataTime=4.539 Loss=0.802 Prec@1=81.055 Prec@5=94.336 rate=2044.67 Hz, eta=0:00:01, total=0:00:00, wall=03:57 IST
=> Training   0.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.596 DataTime=0.430 Loss=0.777 Prec@1=80.283 Prec@5=94.272 rate=2044.67 Hz, eta=0:00:01, total=0:00:00, wall=03:57 IST
=> Training   4.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.596 DataTime=0.430 Loss=0.777 Prec@1=80.283 Prec@5=94.272 rate=1.82 Hz, eta=0:21:56, total=0:00:55, wall=03:57 IST
=> Training   4.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.596 DataTime=0.430 Loss=0.777 Prec@1=80.283 Prec@5=94.272 rate=1.82 Hz, eta=0:21:56, total=0:00:55, wall=03:58 IST
=> Training   4.04% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.575 DataTime=0.404 Loss=0.774 Prec@1=80.359 Prec@5=94.267 rate=1.82 Hz, eta=0:21:56, total=0:00:55, wall=03:58 IST
=> Training   8.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.575 DataTime=0.404 Loss=0.774 Prec@1=80.359 Prec@5=94.267 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=03:58 IST
=> Training   8.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.575 DataTime=0.404 Loss=0.774 Prec@1=80.359 Prec@5=94.267 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=03:59 IST
=> Training   8.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.571 DataTime=0.397 Loss=0.773 Prec@1=80.384 Prec@5=94.283 rate=1.81 Hz, eta=0:21:08, total=0:01:50, wall=03:59 IST
=> Training   12.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.571 DataTime=0.397 Loss=0.773 Prec@1=80.384 Prec@5=94.283 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=03:59 IST
=> Training   12.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.571 DataTime=0.397 Loss=0.773 Prec@1=80.384 Prec@5=94.283 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=04:00 IST
=> Training   12.03% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.566 DataTime=0.392 Loss=0.773 Prec@1=80.434 Prec@5=94.261 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=04:00 IST
=> Training   16.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.566 DataTime=0.392 Loss=0.773 Prec@1=80.434 Prec@5=94.261 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=04:00 IST
=> Training   16.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.566 DataTime=0.392 Loss=0.773 Prec@1=80.434 Prec@5=94.261 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=04:00 IST
=> Training   16.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.567 DataTime=0.392 Loss=0.774 Prec@1=80.397 Prec@5=94.257 rate=1.80 Hz, eta=0:19:24, total=0:03:42, wall=04:00 IST
=> Training   20.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.567 DataTime=0.392 Loss=0.774 Prec@1=80.397 Prec@5=94.257 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=04:00 IST
=> Training   20.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.567 DataTime=0.392 Loss=0.774 Prec@1=80.397 Prec@5=94.257 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=04:01 IST
=> Training   20.02% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.774 Prec@1=80.398 Prec@5=94.261 rate=1.79 Hz, eta=0:18:35, total=0:04:39, wall=04:01 IST
=> Training   24.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.774 Prec@1=80.398 Prec@5=94.261 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=04:01 IST
=> Training   24.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.774 Prec@1=80.398 Prec@5=94.261 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=04:02 IST
=> Training   24.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.411 Prec@5=94.253 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=04:02 IST
=> Training   28.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.411 Prec@5=94.253 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=04:02 IST
=> Training   28.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.411 Prec@5=94.253 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=04:03 IST
=> Training   28.01% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.373 Prec@5=94.250 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=04:03 IST
=> Training   32.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.373 Prec@5=94.250 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:03 IST
=> Training   32.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.775 Prec@1=80.373 Prec@5=94.250 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:04 IST
=> Training   32.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.775 Prec@1=80.335 Prec@5=94.251 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=04:04 IST
=> Training   36.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.775 Prec@1=80.335 Prec@5=94.251 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:04 IST
=> Training   36.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.564 DataTime=0.389 Loss=0.775 Prec@1=80.335 Prec@5=94.251 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:05 IST
=> Training   36.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.299 Prec@5=94.232 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:05 IST
=> Training   39.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.299 Prec@5=94.232 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=04:05 IST
=> Training   39.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.299 Prec@5=94.232 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=04:06 IST
=> Training   39.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.285 Prec@5=94.236 rate=1.79 Hz, eta=0:13:58, total=0:09:18, wall=04:06 IST
=> Training   43.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.285 Prec@5=94.236 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:06 IST
=> Training   43.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.563 DataTime=0.388 Loss=0.777 Prec@1=80.285 Prec@5=94.236 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:07 IST
=> Training   43.99% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.778 Prec@1=80.268 Prec@5=94.224 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:07 IST
=> Training   47.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.778 Prec@1=80.268 Prec@5=94.224 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=04:07 IST
=> Training   47.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.778 Prec@1=80.268 Prec@5=94.224 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=04:08 IST
=> Training   47.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.779 Prec@1=80.233 Prec@5=94.205 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=04:08 IST
=> Training   51.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.779 Prec@1=80.233 Prec@5=94.205 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=04:08 IST
=> Training   51.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.779 Prec@1=80.233 Prec@5=94.205 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=04:09 IST
=> Training   51.98% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.208 Prec@5=94.191 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=04:09 IST
=> Training   55.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.208 Prec@5=94.191 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:09 IST
=> Training   55.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.208 Prec@5=94.191 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:10 IST
=> Training   55.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.207 Prec@5=94.185 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:10 IST
=> Training   59.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.207 Prec@5=94.185 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=04:10 IST
=> Training   59.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.386 Loss=0.780 Prec@1=80.207 Prec@5=94.185 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=04:11 IST
=> Training   59.97% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.189 Prec@5=94.176 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=04:11 IST
=> Training   63.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.189 Prec@5=94.176 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:11 IST
=> Training   63.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.189 Prec@5=94.176 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:12 IST
=> Training   63.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.175 Prec@5=94.172 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=04:12 IST
=> Training   67.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.175 Prec@5=94.172 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:12 IST
=> Training   67.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.781 Prec@1=80.175 Prec@5=94.172 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:13 IST
=> Training   67.96% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.782 Prec@1=80.158 Prec@5=94.166 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=04:13 IST
=> Training   71.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.782 Prec@1=80.158 Prec@5=94.166 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:13 IST
=> Training   71.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.562 DataTime=0.385 Loss=0.782 Prec@1=80.158 Prec@5=94.166 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:14 IST
=> Training   71.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.137 Prec@5=94.152 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=04:14 IST
=> Training   75.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.137 Prec@5=94.152 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=04:14 IST
=> Training   75.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.137 Prec@5=94.152 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=04:14 IST
=> Training   75.95% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.138 Prec@5=94.149 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=04:14 IST
=> Training   79.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.138 Prec@5=94.149 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:14 IST
=> Training   79.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.138 Prec@5=94.149 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:15 IST
=> Training   79.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.126 Prec@5=94.145 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:15 IST
=> Training   83.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.126 Prec@5=94.145 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:15 IST
=> Training   83.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.783 Prec@1=80.126 Prec@5=94.145 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:16 IST
=> Training   83.94% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.110 Prec@5=94.143 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:16 IST
=> Training   87.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.110 Prec@5=94.143 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:16 IST
=> Training   87.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.110 Prec@5=94.143 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:17 IST
=> Training   87.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.100 Prec@5=94.138 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=04:17 IST
=> Training   91.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.100 Prec@5=94.138 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:17 IST
=> Training   91.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.100 Prec@5=94.138 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:18 IST
=> Training   91.93% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.105 Prec@5=94.142 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:18 IST
=> Training   95.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.105 Prec@5=94.142 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:18 IST
=> Training   95.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.105 Prec@5=94.142 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:19 IST
=> Training   95.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.092 Prec@5=94.135 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=04:19 IST
=> Training   99.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.092 Prec@5=94.135 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:19 IST
=> Training   99.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.561 DataTime=0.385 Loss=0.784 Prec@1=80.092 Prec@5=94.135 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:19 IST
=> Training   99.92% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.560 DataTime=0.385 Loss=0.784 Prec@1=80.093 Prec@5=94.135 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=04:19 IST
=> Training   100.00% of 1x2503...Epoch=130/150 LR=0.0048 Time=0.560 DataTime=0.385 Loss=0.784 Prec@1=80.093 Prec@5=94.135 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=04:19 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:19 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:19 IST
=> Validation 0.00% of 1x98...Epoch=130/150 LR=0.0048 Time=6.705 Loss=0.687 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=04:19 IST
=> Validation 1.02% of 1x98...Epoch=130/150 LR=0.0048 Time=6.705 Loss=0.687 Prec@1=81.055 Prec@5=95.312 rate=6668.44 Hz, eta=0:00:00, total=0:00:00, wall=04:19 IST
** Validation 1.02% of 1x98...Epoch=130/150 LR=0.0048 Time=6.705 Loss=0.687 Prec@1=81.055 Prec@5=95.312 rate=6668.44 Hz, eta=0:00:00, total=0:00:00, wall=04:20 IST
** Validation 1.02% of 1x98...Epoch=130/150 LR=0.0048 Time=0.632 Loss=1.200 Prec@1=71.040 Prec@5=89.986 rate=6668.44 Hz, eta=0:00:00, total=0:00:00, wall=04:20 IST
** Validation 100.00% of 1x98...Epoch=130/150 LR=0.0048 Time=0.632 Loss=1.200 Prec@1=71.040 Prec@5=89.986 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=04:20 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:20 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:20 IST
=> Training   0.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=4.902 DataTime=4.695 Loss=0.727 Prec@1=79.688 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=04:20 IST
=> Training   0.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=4.902 DataTime=4.695 Loss=0.727 Prec@1=79.688 Prec@5=95.117 rate=1160.60 Hz, eta=0:00:02, total=0:00:00, wall=04:20 IST
=> Training   0.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=4.902 DataTime=4.695 Loss=0.727 Prec@1=79.688 Prec@5=95.117 rate=1160.60 Hz, eta=0:00:02, total=0:00:00, wall=04:21 IST
=> Training   0.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.583 DataTime=0.419 Loss=0.761 Prec@1=80.770 Prec@5=94.365 rate=1160.60 Hz, eta=0:00:02, total=0:00:00, wall=04:21 IST
=> Training   4.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.583 DataTime=0.419 Loss=0.761 Prec@1=80.770 Prec@5=94.365 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:21 IST
=> Training   4.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.583 DataTime=0.419 Loss=0.761 Prec@1=80.770 Prec@5=94.365 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:22 IST
=> Training   4.04% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.572 DataTime=0.399 Loss=0.758 Prec@1=80.748 Prec@5=94.450 rate=1.87 Hz, eta=0:21:23, total=0:00:53, wall=04:22 IST
=> Training   8.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.572 DataTime=0.399 Loss=0.758 Prec@1=80.748 Prec@5=94.450 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:22 IST
=> Training   8.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.572 DataTime=0.399 Loss=0.758 Prec@1=80.748 Prec@5=94.450 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:23 IST
=> Training   8.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.570 DataTime=0.397 Loss=0.758 Prec@1=80.692 Prec@5=94.413 rate=1.83 Hz, eta=0:20:59, total=0:01:49, wall=04:23 IST
=> Training   12.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.570 DataTime=0.397 Loss=0.758 Prec@1=80.692 Prec@5=94.413 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:23 IST
=> Training   12.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.570 DataTime=0.397 Loss=0.758 Prec@1=80.692 Prec@5=94.413 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:24 IST
=> Training   12.03% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.389 Loss=0.763 Prec@1=80.603 Prec@5=94.352 rate=1.81 Hz, eta=0:20:19, total=0:02:46, wall=04:24 IST
=> Training   16.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.389 Loss=0.763 Prec@1=80.603 Prec@5=94.352 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:24 IST
=> Training   16.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.389 Loss=0.763 Prec@1=80.603 Prec@5=94.352 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:25 IST
=> Training   16.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.566 DataTime=0.390 Loss=0.766 Prec@1=80.507 Prec@5=94.305 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=04:25 IST
=> Training   20.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.566 DataTime=0.390 Loss=0.766 Prec@1=80.507 Prec@5=94.305 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=04:25 IST
=> Training   20.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.566 DataTime=0.390 Loss=0.766 Prec@1=80.507 Prec@5=94.305 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=04:26 IST
=> Training   20.02% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.390 Loss=0.767 Prec@1=80.495 Prec@5=94.300 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=04:26 IST
=> Training   24.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.390 Loss=0.767 Prec@1=80.495 Prec@5=94.300 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:26 IST
=> Training   24.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.565 DataTime=0.390 Loss=0.767 Prec@1=80.495 Prec@5=94.300 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:27 IST
=> Training   24.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.564 DataTime=0.388 Loss=0.769 Prec@1=80.459 Prec@5=94.300 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=04:27 IST
=> Training   28.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.564 DataTime=0.388 Loss=0.769 Prec@1=80.459 Prec@5=94.300 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=04:27 IST
=> Training   28.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.564 DataTime=0.388 Loss=0.769 Prec@1=80.459 Prec@5=94.300 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=04:28 IST
=> Training   28.01% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.388 Loss=0.769 Prec@1=80.472 Prec@5=94.315 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=04:28 IST
=> Training   32.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.388 Loss=0.769 Prec@1=80.472 Prec@5=94.315 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=04:28 IST
=> Training   32.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.388 Loss=0.769 Prec@1=80.472 Prec@5=94.315 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=04:29 IST
=> Training   32.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.443 Prec@5=94.306 rate=1.80 Hz, eta=0:15:47, total=0:07:26, wall=04:29 IST
=> Training   36.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.443 Prec@5=94.306 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:29 IST
=> Training   36.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.443 Prec@5=94.306 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:30 IST
=> Training   36.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.769 Prec@1=80.440 Prec@5=94.318 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=04:30 IST
=> Training   39.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.769 Prec@1=80.440 Prec@5=94.318 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=04:30 IST
=> Training   39.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.769 Prec@1=80.440 Prec@5=94.318 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=04:30 IST
=> Training   39.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.428 Prec@5=94.318 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=04:30 IST
=> Training   43.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.428 Prec@5=94.318 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:30 IST
=> Training   43.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.387 Loss=0.770 Prec@1=80.428 Prec@5=94.318 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:31 IST
=> Training   43.99% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.385 Loss=0.771 Prec@1=80.396 Prec@5=94.298 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=04:31 IST
=> Training   47.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.385 Loss=0.771 Prec@1=80.396 Prec@5=94.298 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:31 IST
=> Training   47.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.385 Loss=0.771 Prec@1=80.396 Prec@5=94.298 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:32 IST
=> Training   47.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.386 Loss=0.772 Prec@1=80.370 Prec@5=94.281 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=04:32 IST
=> Training   51.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.386 Loss=0.772 Prec@1=80.370 Prec@5=94.281 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=04:32 IST
=> Training   51.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.563 DataTime=0.386 Loss=0.772 Prec@1=80.370 Prec@5=94.281 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=04:33 IST
=> Training   51.98% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.382 Prec@5=94.285 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=04:33 IST
=> Training   55.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.382 Prec@5=94.285 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:33 IST
=> Training   55.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.382 Prec@5=94.285 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:34 IST
=> Training   55.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.373 Prec@5=94.282 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=04:34 IST
=> Training   59.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.373 Prec@5=94.282 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=04:34 IST
=> Training   59.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.562 DataTime=0.386 Loss=0.773 Prec@1=80.373 Prec@5=94.282 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=04:35 IST
=> Training   59.97% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.773 Prec@1=80.376 Prec@5=94.278 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=04:35 IST
=> Training   63.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.773 Prec@1=80.376 Prec@5=94.278 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:35 IST
=> Training   63.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.773 Prec@1=80.376 Prec@5=94.278 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:36 IST
=> Training   63.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.774 Prec@1=80.374 Prec@5=94.271 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=04:36 IST
=> Training   67.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.774 Prec@1=80.374 Prec@5=94.271 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=04:36 IST
=> Training   67.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.774 Prec@1=80.374 Prec@5=94.271 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=04:37 IST
=> Training   67.96% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.774 Prec@1=80.369 Prec@5=94.265 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=04:37 IST
=> Training   71.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.774 Prec@1=80.369 Prec@5=94.265 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=04:37 IST
=> Training   71.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.774 Prec@1=80.369 Prec@5=94.265 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=04:38 IST
=> Training   71.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.345 Prec@5=94.255 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=04:38 IST
=> Training   75.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.345 Prec@5=94.255 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=04:38 IST
=> Training   75.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.345 Prec@5=94.255 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=04:39 IST
=> Training   75.95% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.343 Prec@5=94.256 rate=1.79 Hz, eta=0:05:35, total=0:17:40, wall=04:39 IST
=> Training   79.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.343 Prec@5=94.256 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:39 IST
=> Training   79.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.343 Prec@5=94.256 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:40 IST
=> Training   79.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.334 Prec@5=94.248 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=04:40 IST
=> Training   83.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.334 Prec@5=94.248 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:40 IST
=> Training   83.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.334 Prec@5=94.248 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:41 IST
=> Training   83.94% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.321 Prec@5=94.238 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=04:41 IST
=> Training   87.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.321 Prec@5=94.238 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=04:41 IST
=> Training   87.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.386 Loss=0.775 Prec@1=80.321 Prec@5=94.238 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=04:42 IST
=> Training   87.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.776 Prec@1=80.313 Prec@5=94.236 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=04:42 IST
=> Training   91.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.776 Prec@1=80.313 Prec@5=94.236 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:42 IST
=> Training   91.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.776 Prec@1=80.313 Prec@5=94.236 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:43 IST
=> Training   91.93% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.314 Prec@5=94.240 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=04:43 IST
=> Training   95.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.314 Prec@5=94.240 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:43 IST
=> Training   95.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.561 DataTime=0.385 Loss=0.775 Prec@1=80.314 Prec@5=94.240 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:43 IST
=> Training   95.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.776 Prec@1=80.297 Prec@5=94.240 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=04:43 IST
=> Training   99.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.776 Prec@1=80.297 Prec@5=94.240 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:43 IST
=> Training   99.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.776 Prec@1=80.297 Prec@5=94.240 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:44 IST
=> Training   99.92% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.776 Prec@1=80.295 Prec@5=94.240 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=04:44 IST
=> Training   100.00% of 1x2503...Epoch=131/150 LR=0.0043 Time=0.560 DataTime=0.384 Loss=0.776 Prec@1=80.295 Prec@5=94.240 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=04:44 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST
=> Validation 0.00% of 1x98...Epoch=131/150 LR=0.0043 Time=7.007 Loss=0.696 Prec@1=82.031 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=04:44 IST
=> Validation 1.02% of 1x98...Epoch=131/150 LR=0.0043 Time=7.007 Loss=0.696 Prec@1=82.031 Prec@5=95.508 rate=1626.50 Hz, eta=0:00:00, total=0:00:00, wall=04:44 IST
** Validation 1.02% of 1x98...Epoch=131/150 LR=0.0043 Time=7.007 Loss=0.696 Prec@1=82.031 Prec@5=95.508 rate=1626.50 Hz, eta=0:00:00, total=0:00:00, wall=04:45 IST
** Validation 1.02% of 1x98...Epoch=131/150 LR=0.0043 Time=0.646 Loss=1.193 Prec@1=71.072 Prec@5=89.922 rate=1626.50 Hz, eta=0:00:00, total=0:00:00, wall=04:45 IST
** Validation 100.00% of 1x98...Epoch=131/150 LR=0.0043 Time=0.646 Loss=1.193 Prec@1=71.072 Prec@5=89.922 rate=1.74 Hz, eta=0:00:00, total=0:00:56, wall=04:45 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST
=> Training   0.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=4.998 DataTime=4.757 Loss=0.729 Prec@1=81.055 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=04:45 IST
=> Training   0.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=4.998 DataTime=4.757 Loss=0.729 Prec@1=81.055 Prec@5=94.727 rate=7817.14 Hz, eta=0:00:00, total=0:00:00, wall=04:45 IST
=> Training   0.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=4.998 DataTime=4.757 Loss=0.729 Prec@1=81.055 Prec@5=94.727 rate=7817.14 Hz, eta=0:00:00, total=0:00:00, wall=04:46 IST
=> Training   0.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.589 DataTime=0.433 Loss=0.756 Prec@1=80.894 Prec@5=94.458 rate=7817.14 Hz, eta=0:00:00, total=0:00:00, wall=04:46 IST
=> Training   4.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.589 DataTime=0.433 Loss=0.756 Prec@1=80.894 Prec@5=94.458 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=04:46 IST
=> Training   4.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.589 DataTime=0.433 Loss=0.756 Prec@1=80.894 Prec@5=94.458 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=04:47 IST
=> Training   4.04% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.578 DataTime=0.415 Loss=0.756 Prec@1=80.895 Prec@5=94.391 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=04:47 IST
=> Training   8.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.578 DataTime=0.415 Loss=0.756 Prec@1=80.895 Prec@5=94.391 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=04:47 IST
=> Training   8.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.578 DataTime=0.415 Loss=0.756 Prec@1=80.895 Prec@5=94.391 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=04:47 IST
=> Training   8.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.573 DataTime=0.406 Loss=0.755 Prec@1=80.929 Prec@5=94.422 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=04:47 IST
=> Training   12.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.573 DataTime=0.406 Loss=0.755 Prec@1=80.929 Prec@5=94.422 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=04:47 IST
=> Training   12.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.573 DataTime=0.406 Loss=0.755 Prec@1=80.929 Prec@5=94.422 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=04:48 IST
=> Training   12.03% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.401 Loss=0.757 Prec@1=80.887 Prec@5=94.429 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=04:48 IST
=> Training   16.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.401 Loss=0.757 Prec@1=80.887 Prec@5=94.429 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=04:48 IST
=> Training   16.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.401 Loss=0.757 Prec@1=80.887 Prec@5=94.429 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=04:49 IST
=> Training   16.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.400 Loss=0.757 Prec@1=80.876 Prec@5=94.453 rate=1.80 Hz, eta=0:19:29, total=0:03:43, wall=04:49 IST
=> Training   20.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.400 Loss=0.757 Prec@1=80.876 Prec@5=94.453 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:49 IST
=> Training   20.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.569 DataTime=0.400 Loss=0.757 Prec@1=80.876 Prec@5=94.453 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:50 IST
=> Training   20.02% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.397 Loss=0.759 Prec@1=80.805 Prec@5=94.408 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=04:50 IST
=> Training   24.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.397 Loss=0.759 Prec@1=80.805 Prec@5=94.408 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=04:50 IST
=> Training   24.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.397 Loss=0.759 Prec@1=80.805 Prec@5=94.408 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=04:51 IST
=> Training   24.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.568 DataTime=0.399 Loss=0.759 Prec@1=80.791 Prec@5=94.401 rate=1.79 Hz, eta=0:17:41, total=0:05:35, wall=04:51 IST
=> Training   28.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.568 DataTime=0.399 Loss=0.759 Prec@1=80.791 Prec@5=94.401 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=04:51 IST
=> Training   28.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.568 DataTime=0.399 Loss=0.759 Prec@1=80.791 Prec@5=94.401 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=04:52 IST
=> Training   28.01% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.398 Loss=0.760 Prec@1=80.781 Prec@5=94.391 rate=1.78 Hz, eta=0:16:49, total=0:06:32, wall=04:52 IST
=> Training   32.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.398 Loss=0.760 Prec@1=80.781 Prec@5=94.391 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:52 IST
=> Training   32.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.567 DataTime=0.398 Loss=0.760 Prec@1=80.781 Prec@5=94.391 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:53 IST
=> Training   32.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.395 Loss=0.761 Prec@1=80.743 Prec@5=94.363 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=04:53 IST
=> Training   36.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.395 Loss=0.761 Prec@1=80.743 Prec@5=94.363 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:53 IST
=> Training   36.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.395 Loss=0.761 Prec@1=80.743 Prec@5=94.363 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:54 IST
=> Training   36.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.394 Loss=0.762 Prec@1=80.725 Prec@5=94.357 rate=1.79 Hz, eta=0:14:55, total=0:08:23, wall=04:54 IST
=> Training   39.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.394 Loss=0.762 Prec@1=80.725 Prec@5=94.357 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=04:54 IST
=> Training   39.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.394 Loss=0.762 Prec@1=80.725 Prec@5=94.357 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=04:55 IST
=> Training   39.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.393 Loss=0.763 Prec@1=80.694 Prec@5=94.343 rate=1.79 Hz, eta=0:13:58, total=0:09:19, wall=04:55 IST
=> Training   43.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.393 Loss=0.763 Prec@1=80.694 Prec@5=94.343 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:55 IST
=> Training   43.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.393 Loss=0.763 Prec@1=80.694 Prec@5=94.343 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:56 IST
=> Training   43.99% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.763 Prec@1=80.671 Prec@5=94.333 rate=1.79 Hz, eta=0:13:03, total=0:10:15, wall=04:56 IST
=> Training   47.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.763 Prec@1=80.671 Prec@5=94.333 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=04:56 IST
=> Training   47.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.763 Prec@1=80.671 Prec@5=94.333 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=04:57 IST
=> Training   47.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.393 Loss=0.764 Prec@1=80.675 Prec@5=94.321 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=04:57 IST
=> Training   51.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.393 Loss=0.764 Prec@1=80.675 Prec@5=94.321 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:57 IST
=> Training   51.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.393 Loss=0.764 Prec@1=80.675 Prec@5=94.321 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:58 IST
=> Training   51.98% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.764 Prec@1=80.683 Prec@5=94.323 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=04:58 IST
=> Training   55.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.764 Prec@1=80.683 Prec@5=94.323 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:58 IST
=> Training   55.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.392 Loss=0.764 Prec@1=80.683 Prec@5=94.323 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:59 IST
=> Training   55.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.562 DataTime=0.390 Loss=0.764 Prec@1=80.669 Prec@5=94.318 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=04:59 IST
=> Training   59.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.562 DataTime=0.390 Loss=0.764 Prec@1=80.669 Prec@5=94.318 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=04:59 IST
=> Training   59.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.562 DataTime=0.390 Loss=0.764 Prec@1=80.669 Prec@5=94.318 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:00 IST
=> Training   59.97% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.764 Prec@1=80.654 Prec@5=94.319 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=05:00 IST
=> Training   63.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.764 Prec@1=80.654 Prec@5=94.319 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=05:00 IST
=> Training   63.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.764 Prec@1=80.654 Prec@5=94.319 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=05:01 IST
=> Training   63.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.632 Prec@5=94.322 rate=1.79 Hz, eta=0:08:24, total=0:14:56, wall=05:01 IST
=> Training   67.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.632 Prec@5=94.322 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=05:01 IST
=> Training   67.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.632 Prec@5=94.322 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=05:01 IST
=> Training   67.96% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.631 Prec@5=94.330 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=05:01 IST
=> Training   71.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.631 Prec@5=94.330 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=05:01 IST
=> Training   71.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.391 Loss=0.765 Prec@1=80.631 Prec@5=94.330 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=05:02 IST
=> Training   71.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.614 Prec@5=94.324 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=05:02 IST
=> Training   75.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.614 Prec@5=94.324 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:02 IST
=> Training   75.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.614 Prec@5=94.324 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:03 IST
=> Training   75.95% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.610 Prec@5=94.328 rate=1.79 Hz, eta=0:05:37, total=0:17:44, wall=05:03 IST
=> Training   79.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.610 Prec@5=94.328 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=05:03 IST
=> Training   79.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.766 Prec@1=80.610 Prec@5=94.328 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=05:04 IST
=> Training   79.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.767 Prec@1=80.580 Prec@5=94.320 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=05:04 IST
=> Training   83.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.767 Prec@1=80.580 Prec@5=94.320 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:04 IST
=> Training   83.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.389 Loss=0.767 Prec@1=80.580 Prec@5=94.320 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:05 IST
=> Training   83.94% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.572 Prec@5=94.316 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=05:05 IST
=> Training   87.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.572 Prec@5=94.316 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:05 IST
=> Training   87.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.572 Prec@5=94.316 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:06 IST
=> Training   87.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.564 Prec@5=94.310 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=05:06 IST
=> Training   91.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.564 Prec@5=94.310 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=05:06 IST
=> Training   91.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.564 Prec@5=94.310 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=05:07 IST
=> Training   91.93% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.389 Loss=0.767 Prec@1=80.572 Prec@5=94.311 rate=1.78 Hz, eta=0:01:53, total=0:21:30, wall=05:07 IST
=> Training   95.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.389 Loss=0.767 Prec@1=80.572 Prec@5=94.311 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=05:07 IST
=> Training   95.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.564 DataTime=0.389 Loss=0.767 Prec@1=80.572 Prec@5=94.311 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=05:08 IST
=> Training   95.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.560 Prec@5=94.306 rate=1.78 Hz, eta=0:00:57, total=0:22:28, wall=05:08 IST
=> Training   99.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.560 Prec@5=94.306 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=05:08 IST
=> Training   99.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.560 Prec@5=94.306 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=05:08 IST
=> Training   99.92% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.560 Prec@5=94.307 rate=1.78 Hz, eta=0:00:01, total=0:23:23, wall=05:08 IST
=> Training   100.00% of 1x2503...Epoch=132/150 LR=0.0039 Time=0.563 DataTime=0.388 Loss=0.767 Prec@1=80.560 Prec@5=94.307 rate=1.78 Hz, eta=0:00:00, total=0:23:23, wall=05:08 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:08 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:08 IST
=> Validation 0.00% of 1x98...Epoch=132/150 LR=0.0039 Time=6.806 Loss=0.691 Prec@1=81.836 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=05:08 IST
=> Validation 1.02% of 1x98...Epoch=132/150 LR=0.0039 Time=6.806 Loss=0.691 Prec@1=81.836 Prec@5=94.727 rate=5201.75 Hz, eta=0:00:00, total=0:00:00, wall=05:08 IST
** Validation 1.02% of 1x98...Epoch=132/150 LR=0.0039 Time=6.806 Loss=0.691 Prec@1=81.836 Prec@5=94.727 rate=5201.75 Hz, eta=0:00:00, total=0:00:00, wall=05:09 IST
** Validation 1.02% of 1x98...Epoch=132/150 LR=0.0039 Time=0.639 Loss=1.187 Prec@1=71.312 Prec@5=90.010 rate=5201.75 Hz, eta=0:00:00, total=0:00:00, wall=05:09 IST
** Validation 100.00% of 1x98...Epoch=132/150 LR=0.0039 Time=0.639 Loss=1.187 Prec@1=71.312 Prec@5=90.010 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=05:09 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST
=> Training   0.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=4.660 DataTime=4.076 Loss=0.804 Prec@1=80.273 Prec@5=93.945 rate=0 Hz, eta=?, total=0:00:00, wall=05:09 IST
=> Training   0.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=4.660 DataTime=4.076 Loss=0.804 Prec@1=80.273 Prec@5=93.945 rate=1108.01 Hz, eta=0:00:02, total=0:00:00, wall=05:09 IST
=> Training   0.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=4.660 DataTime=4.076 Loss=0.804 Prec@1=80.273 Prec@5=93.945 rate=1108.01 Hz, eta=0:00:02, total=0:00:00, wall=05:10 IST
=> Training   0.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.598 DataTime=0.428 Loss=0.745 Prec@1=80.987 Prec@5=94.641 rate=1108.01 Hz, eta=0:00:02, total=0:00:00, wall=05:10 IST
=> Training   4.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.598 DataTime=0.428 Loss=0.745 Prec@1=80.987 Prec@5=94.641 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:10 IST
=> Training   4.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.598 DataTime=0.428 Loss=0.745 Prec@1=80.987 Prec@5=94.641 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:11 IST
=> Training   4.04% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.590 DataTime=0.417 Loss=0.747 Prec@1=80.961 Prec@5=94.602 rate=1.81 Hz, eta=0:22:05, total=0:00:55, wall=05:11 IST
=> Training   8.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.590 DataTime=0.417 Loss=0.747 Prec@1=80.961 Prec@5=94.602 rate=1.77 Hz, eta=0:21:44, total=0:01:53, wall=05:11 IST
=> Training   8.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.590 DataTime=0.417 Loss=0.747 Prec@1=80.961 Prec@5=94.602 rate=1.77 Hz, eta=0:21:44, total=0:01:53, wall=05:12 IST
=> Training   8.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.574 DataTime=0.399 Loss=0.748 Prec@1=80.905 Prec@5=94.573 rate=1.77 Hz, eta=0:21:44, total=0:01:53, wall=05:12 IST
=> Training   12.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.574 DataTime=0.399 Loss=0.748 Prec@1=80.905 Prec@5=94.573 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=05:12 IST
=> Training   12.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.574 DataTime=0.399 Loss=0.748 Prec@1=80.905 Prec@5=94.573 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=05:13 IST
=> Training   12.03% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.572 DataTime=0.395 Loss=0.751 Prec@1=80.895 Prec@5=94.549 rate=1.79 Hz, eta=0:20:29, total=0:02:48, wall=05:13 IST
=> Training   16.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.572 DataTime=0.395 Loss=0.751 Prec@1=80.895 Prec@5=94.549 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=05:13 IST
=> Training   16.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.572 DataTime=0.395 Loss=0.751 Prec@1=80.895 Prec@5=94.549 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=05:14 IST
=> Training   16.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.389 Loss=0.750 Prec@1=80.916 Prec@5=94.576 rate=1.78 Hz, eta=0:19:38, total=0:03:44, wall=05:14 IST
=> Training   20.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.389 Loss=0.750 Prec@1=80.916 Prec@5=94.576 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:14 IST
=> Training   20.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.389 Loss=0.750 Prec@1=80.916 Prec@5=94.576 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:15 IST
=> Training   20.02% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.390 Loss=0.753 Prec@1=80.874 Prec@5=94.542 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=05:15 IST
=> Training   24.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.390 Loss=0.753 Prec@1=80.874 Prec@5=94.542 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=05:15 IST
=> Training   24.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.390 Loss=0.753 Prec@1=80.874 Prec@5=94.542 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=05:16 IST
=> Training   24.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.754 Prec@1=80.861 Prec@5=94.515 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=05:16 IST
=> Training   28.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.754 Prec@1=80.861 Prec@5=94.515 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=05:16 IST
=> Training   28.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.754 Prec@1=80.861 Prec@5=94.515 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=05:17 IST
=> Training   28.01% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.754 Prec@1=80.869 Prec@5=94.517 rate=1.78 Hz, eta=0:16:50, total=0:06:33, wall=05:17 IST
=> Training   32.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.754 Prec@1=80.869 Prec@5=94.517 rate=1.78 Hz, eta=0:15:58, total=0:07:31, wall=05:17 IST
=> Training   32.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.754 Prec@1=80.869 Prec@5=94.517 rate=1.78 Hz, eta=0:15:58, total=0:07:31, wall=05:18 IST
=> Training   32.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.754 Prec@1=80.871 Prec@5=94.508 rate=1.78 Hz, eta=0:15:58, total=0:07:31, wall=05:18 IST
=> Training   36.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.754 Prec@1=80.871 Prec@5=94.508 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=05:18 IST
=> Training   36.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.754 Prec@1=80.871 Prec@5=94.508 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=05:19 IST
=> Training   36.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.755 Prec@1=80.868 Prec@5=94.485 rate=1.78 Hz, eta=0:15:00, total=0:08:26, wall=05:19 IST
=> Training   39.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.755 Prec@1=80.868 Prec@5=94.485 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=05:19 IST
=> Training   39.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.755 Prec@1=80.868 Prec@5=94.485 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=05:20 IST
=> Training   39.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.756 Prec@1=80.861 Prec@5=94.457 rate=1.77 Hz, eta=0:14:06, total=0:09:24, wall=05:20 IST
=> Training   43.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.756 Prec@1=80.861 Prec@5=94.457 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=05:20 IST
=> Training   43.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.394 Loss=0.756 Prec@1=80.861 Prec@5=94.457 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=05:21 IST
=> Training   43.99% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.756 Prec@1=80.862 Prec@5=94.451 rate=1.77 Hz, eta=0:13:11, total=0:10:21, wall=05:21 IST
=> Training   47.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.756 Prec@1=80.862 Prec@5=94.451 rate=1.77 Hz, eta=0:12:15, total=0:11:18, wall=05:21 IST
=> Training   47.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.569 DataTime=0.394 Loss=0.756 Prec@1=80.862 Prec@5=94.451 rate=1.77 Hz, eta=0:12:15, total=0:11:18, wall=05:21 IST
=> Training   47.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.757 Prec@1=80.849 Prec@5=94.444 rate=1.77 Hz, eta=0:12:15, total=0:11:18, wall=05:21 IST
=> Training   51.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.757 Prec@1=80.849 Prec@5=94.444 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=05:21 IST
=> Training   51.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.757 Prec@1=80.849 Prec@5=94.444 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=05:22 IST
=> Training   51.98% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.836 Prec@5=94.438 rate=1.77 Hz, eta=0:11:17, total=0:12:13, wall=05:22 IST
=> Training   55.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.836 Prec@5=94.438 rate=1.77 Hz, eta=0:10:22, total=0:13:11, wall=05:22 IST
=> Training   55.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.836 Prec@5=94.438 rate=1.77 Hz, eta=0:10:22, total=0:13:11, wall=05:23 IST
=> Training   55.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.829 Prec@5=94.436 rate=1.77 Hz, eta=0:10:22, total=0:13:11, wall=05:23 IST
=> Training   59.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.829 Prec@5=94.436 rate=1.77 Hz, eta=0:09:26, total=0:14:07, wall=05:23 IST
=> Training   59.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.757 Prec@1=80.829 Prec@5=94.436 rate=1.77 Hz, eta=0:09:26, total=0:14:07, wall=05:24 IST
=> Training   59.97% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.758 Prec@1=80.815 Prec@5=94.426 rate=1.77 Hz, eta=0:09:26, total=0:14:07, wall=05:24 IST
=> Training   63.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.758 Prec@1=80.815 Prec@5=94.426 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:24 IST
=> Training   63.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.393 Loss=0.758 Prec@1=80.815 Prec@5=94.426 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:25 IST
=> Training   63.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.797 Prec@5=94.419 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:25 IST
=> Training   67.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.797 Prec@5=94.419 rate=1.77 Hz, eta=0:07:33, total=0:16:01, wall=05:25 IST
=> Training   67.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.797 Prec@5=94.419 rate=1.77 Hz, eta=0:07:33, total=0:16:01, wall=05:26 IST
=> Training   67.96% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.812 Prec@5=94.426 rate=1.77 Hz, eta=0:07:33, total=0:16:01, wall=05:26 IST
=> Training   71.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.812 Prec@5=94.426 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=05:26 IST
=> Training   71.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.568 DataTime=0.392 Loss=0.758 Prec@1=80.812 Prec@5=94.426 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=05:27 IST
=> Training   71.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.811 Prec@5=94.431 rate=1.77 Hz, eta=0:06:36, total=0:16:57, wall=05:27 IST
=> Training   75.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.811 Prec@5=94.431 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:27 IST
=> Training   75.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.811 Prec@5=94.431 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:28 IST
=> Training   75.95% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.796 Prec@5=94.429 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:28 IST
=> Training   79.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.796 Prec@5=94.429 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:28 IST
=> Training   79.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.391 Loss=0.758 Prec@1=80.796 Prec@5=94.429 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:29 IST
=> Training   79.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.758 Prec@1=80.781 Prec@5=94.427 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:29 IST
=> Training   83.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.758 Prec@1=80.781 Prec@5=94.427 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=05:29 IST
=> Training   83.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.758 Prec@1=80.781 Prec@5=94.427 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=05:30 IST
=> Training   83.94% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.773 Prec@5=94.423 rate=1.77 Hz, eta=0:03:46, total=0:19:44, wall=05:30 IST
=> Training   87.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.773 Prec@5=94.423 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=05:30 IST
=> Training   87.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.773 Prec@5=94.423 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=05:31 IST
=> Training   87.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.751 Prec@5=94.413 rate=1.77 Hz, eta=0:02:50, total=0:20:40, wall=05:31 IST
=> Training   91.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.751 Prec@5=94.413 rate=1.77 Hz, eta=0:01:54, total=0:21:38, wall=05:31 IST
=> Training   91.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.391 Loss=0.759 Prec@1=80.751 Prec@5=94.413 rate=1.77 Hz, eta=0:01:54, total=0:21:38, wall=05:32 IST
=> Training   91.93% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.725 Prec@5=94.406 rate=1.77 Hz, eta=0:01:54, total=0:21:38, wall=05:32 IST
=> Training   95.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.725 Prec@5=94.406 rate=1.77 Hz, eta=0:00:57, total=0:22:36, wall=05:32 IST
=> Training   95.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.725 Prec@5=94.406 rate=1.77 Hz, eta=0:00:57, total=0:22:36, wall=05:33 IST
=> Training   95.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.735 Prec@5=94.406 rate=1.77 Hz, eta=0:00:57, total=0:22:36, wall=05:33 IST
=> Training   99.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.735 Prec@5=94.406 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=05:33 IST
=> Training   99.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.567 DataTime=0.392 Loss=0.760 Prec@1=80.735 Prec@5=94.406 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=05:33 IST
=> Training   99.92% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.392 Loss=0.760 Prec@1=80.733 Prec@5=94.405 rate=1.77 Hz, eta=0:00:01, total=0:23:32, wall=05:33 IST
=> Training   100.00% of 1x2503...Epoch=133/150 LR=0.0035 Time=0.566 DataTime=0.392 Loss=0.760 Prec@1=80.733 Prec@5=94.405 rate=1.77 Hz, eta=0:00:00, total=0:23:32, wall=05:33 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST
=> Validation 0.00% of 1x98...Epoch=133/150 LR=0.0035 Time=7.056 Loss=0.713 Prec@1=81.836 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=05:33 IST
=> Validation 1.02% of 1x98...Epoch=133/150 LR=0.0035 Time=7.056 Loss=0.713 Prec@1=81.836 Prec@5=95.117 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=05:33 IST
** Validation 1.02% of 1x98...Epoch=133/150 LR=0.0035 Time=7.056 Loss=0.713 Prec@1=81.836 Prec@5=95.117 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=05:34 IST
** Validation 1.02% of 1x98...Epoch=133/150 LR=0.0035 Time=0.641 Loss=1.188 Prec@1=71.414 Prec@5=90.042 rate=7370.39 Hz, eta=0:00:00, total=0:00:00, wall=05:34 IST
** Validation 100.00% of 1x98...Epoch=133/150 LR=0.0035 Time=0.641 Loss=1.188 Prec@1=71.414 Prec@5=90.042 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=05:34 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> Training   0.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=5.032 DataTime=4.579 Loss=0.740 Prec@1=82.812 Prec@5=94.141 rate=0 Hz, eta=?, total=0:00:00, wall=05:34 IST
=> Training   0.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=5.032 DataTime=4.579 Loss=0.740 Prec@1=82.812 Prec@5=94.141 rate=8511.72 Hz, eta=0:00:00, total=0:00:00, wall=05:34 IST
=> Training   0.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=5.032 DataTime=4.579 Loss=0.740 Prec@1=82.812 Prec@5=94.141 rate=8511.72 Hz, eta=0:00:00, total=0:00:00, wall=05:35 IST
=> Training   0.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.580 DataTime=0.409 Loss=0.739 Prec@1=81.128 Prec@5=94.622 rate=8511.72 Hz, eta=0:00:00, total=0:00:00, wall=05:35 IST
=> Training   4.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.580 DataTime=0.409 Loss=0.739 Prec@1=81.128 Prec@5=94.622 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=05:35 IST
=> Training   4.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.580 DataTime=0.409 Loss=0.739 Prec@1=81.128 Prec@5=94.622 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=05:36 IST
=> Training   4.04% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.579 DataTime=0.412 Loss=0.742 Prec@1=81.142 Prec@5=94.481 rate=1.88 Hz, eta=0:21:14, total=0:00:53, wall=05:36 IST
=> Training   8.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.579 DataTime=0.412 Loss=0.742 Prec@1=81.142 Prec@5=94.481 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:36 IST
=> Training   8.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.579 DataTime=0.412 Loss=0.742 Prec@1=81.142 Prec@5=94.481 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:37 IST
=> Training   8.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.744 Prec@1=81.140 Prec@5=94.483 rate=1.81 Hz, eta=0:21:13, total=0:01:51, wall=05:37 IST
=> Training   12.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.744 Prec@1=81.140 Prec@5=94.483 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=05:37 IST
=> Training   12.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.744 Prec@1=81.140 Prec@5=94.483 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=05:38 IST
=> Training   12.03% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.748 Prec@1=81.050 Prec@5=94.451 rate=1.80 Hz, eta=0:20:23, total=0:02:47, wall=05:38 IST
=> Training   16.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.748 Prec@1=81.050 Prec@5=94.451 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=05:38 IST
=> Training   16.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.405 Loss=0.748 Prec@1=81.050 Prec@5=94.451 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=05:39 IST
=> Training   16.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.574 DataTime=0.406 Loss=0.749 Prec@1=81.026 Prec@5=94.472 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=05:39 IST
=> Training   20.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.574 DataTime=0.406 Loss=0.749 Prec@1=81.026 Prec@5=94.472 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=05:39 IST
=> Training   20.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.574 DataTime=0.406 Loss=0.749 Prec@1=81.026 Prec@5=94.472 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=05:40 IST
=> Training   20.02% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.402 Loss=0.750 Prec@1=80.991 Prec@5=94.456 rate=1.77 Hz, eta=0:18:49, total=0:04:42, wall=05:40 IST
=> Training   24.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.402 Loss=0.750 Prec@1=80.991 Prec@5=94.456 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=05:40 IST
=> Training   24.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.402 Loss=0.750 Prec@1=80.991 Prec@5=94.456 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=05:41 IST
=> Training   24.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.402 Loss=0.750 Prec@1=80.989 Prec@5=94.465 rate=1.77 Hz, eta=0:17:52, total=0:05:38, wall=05:41 IST
=> Training   28.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.402 Loss=0.750 Prec@1=80.989 Prec@5=94.465 rate=1.77 Hz, eta=0:16:59, total=0:06:36, wall=05:41 IST
=> Training   28.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.402 Loss=0.750 Prec@1=80.989 Prec@5=94.465 rate=1.77 Hz, eta=0:16:59, total=0:06:36, wall=05:41 IST
=> Training   28.01% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.399 Loss=0.751 Prec@1=81.002 Prec@5=94.458 rate=1.77 Hz, eta=0:16:59, total=0:06:36, wall=05:41 IST
=> Training   32.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.399 Loss=0.751 Prec@1=81.002 Prec@5=94.458 rate=1.77 Hz, eta=0:16:02, total=0:07:32, wall=05:41 IST
=> Training   32.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.572 DataTime=0.399 Loss=0.751 Prec@1=81.002 Prec@5=94.458 rate=1.77 Hz, eta=0:16:02, total=0:07:32, wall=05:42 IST
=> Training   32.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.400 Loss=0.751 Prec@1=80.984 Prec@5=94.464 rate=1.77 Hz, eta=0:16:02, total=0:07:32, wall=05:42 IST
=> Training   36.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.400 Loss=0.751 Prec@1=80.984 Prec@5=94.464 rate=1.76 Hz, eta=0:15:08, total=0:08:30, wall=05:42 IST
=> Training   36.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.573 DataTime=0.400 Loss=0.751 Prec@1=80.984 Prec@5=94.464 rate=1.76 Hz, eta=0:15:08, total=0:08:30, wall=05:43 IST
=> Training   36.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.398 Loss=0.750 Prec@1=80.979 Prec@5=94.478 rate=1.76 Hz, eta=0:15:08, total=0:08:30, wall=05:43 IST
=> Training   39.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.398 Loss=0.750 Prec@1=80.979 Prec@5=94.478 rate=1.77 Hz, eta=0:14:10, total=0:09:27, wall=05:43 IST
=> Training   39.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.398 Loss=0.750 Prec@1=80.979 Prec@5=94.478 rate=1.77 Hz, eta=0:14:10, total=0:09:27, wall=05:44 IST
=> Training   39.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.991 Prec@5=94.479 rate=1.77 Hz, eta=0:14:10, total=0:09:27, wall=05:44 IST
=> Training   43.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.991 Prec@5=94.479 rate=1.77 Hz, eta=0:13:14, total=0:10:23, wall=05:44 IST
=> Training   43.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.991 Prec@5=94.479 rate=1.77 Hz, eta=0:13:14, total=0:10:23, wall=05:45 IST
=> Training   43.99% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.985 Prec@5=94.484 rate=1.77 Hz, eta=0:13:14, total=0:10:23, wall=05:45 IST
=> Training   47.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.985 Prec@5=94.484 rate=1.77 Hz, eta=0:12:17, total=0:11:20, wall=05:45 IST
=> Training   47.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.571 DataTime=0.397 Loss=0.750 Prec@1=80.985 Prec@5=94.484 rate=1.77 Hz, eta=0:12:17, total=0:11:20, wall=05:46 IST
=> Training   47.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.987 Prec@5=94.476 rate=1.77 Hz, eta=0:12:17, total=0:11:20, wall=05:46 IST
=> Training   51.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.987 Prec@5=94.476 rate=1.77 Hz, eta=0:11:20, total=0:12:16, wall=05:46 IST
=> Training   51.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.987 Prec@5=94.476 rate=1.77 Hz, eta=0:11:20, total=0:12:16, wall=05:47 IST
=> Training   51.98% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.995 Prec@5=94.464 rate=1.77 Hz, eta=0:11:20, total=0:12:16, wall=05:47 IST
=> Training   55.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.995 Prec@5=94.464 rate=1.77 Hz, eta=0:10:24, total=0:13:13, wall=05:47 IST
=> Training   55.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.396 Loss=0.751 Prec@1=80.995 Prec@5=94.464 rate=1.77 Hz, eta=0:10:24, total=0:13:13, wall=05:48 IST
=> Training   55.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.395 Loss=0.751 Prec@1=80.978 Prec@5=94.471 rate=1.77 Hz, eta=0:10:24, total=0:13:13, wall=05:48 IST
=> Training   59.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.395 Loss=0.751 Prec@1=80.978 Prec@5=94.471 rate=1.77 Hz, eta=0:09:27, total=0:14:10, wall=05:48 IST
=> Training   59.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.570 DataTime=0.395 Loss=0.751 Prec@1=80.978 Prec@5=94.471 rate=1.77 Hz, eta=0:09:27, total=0:14:10, wall=05:49 IST
=> Training   59.97% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.394 Loss=0.752 Prec@1=80.971 Prec@5=94.469 rate=1.77 Hz, eta=0:09:27, total=0:14:10, wall=05:49 IST
=> Training   63.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.394 Loss=0.752 Prec@1=80.971 Prec@5=94.469 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:49 IST
=> Training   63.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.394 Loss=0.752 Prec@1=80.971 Prec@5=94.469 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:50 IST
=> Training   63.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.393 Loss=0.751 Prec@1=80.980 Prec@5=94.483 rate=1.77 Hz, eta=0:08:30, total=0:15:05, wall=05:50 IST
=> Training   67.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.393 Loss=0.751 Prec@1=80.980 Prec@5=94.483 rate=1.77 Hz, eta=0:07:33, total=0:16:02, wall=05:50 IST
=> Training   67.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.569 DataTime=0.393 Loss=0.751 Prec@1=80.980 Prec@5=94.483 rate=1.77 Hz, eta=0:07:33, total=0:16:02, wall=05:51 IST
=> Training   67.96% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.392 Loss=0.751 Prec@1=80.978 Prec@5=94.482 rate=1.77 Hz, eta=0:07:33, total=0:16:02, wall=05:51 IST
=> Training   71.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.392 Loss=0.751 Prec@1=80.978 Prec@5=94.482 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=05:51 IST
=> Training   71.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.392 Loss=0.751 Prec@1=80.978 Prec@5=94.482 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=05:52 IST
=> Training   71.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.963 Prec@5=94.479 rate=1.77 Hz, eta=0:06:36, total=0:16:56, wall=05:52 IST
=> Training   75.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.963 Prec@5=94.479 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:52 IST
=> Training   75.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.963 Prec@5=94.479 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:53 IST
=> Training   75.95% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.964 Prec@5=94.489 rate=1.77 Hz, eta=0:05:39, total=0:17:53, wall=05:53 IST
=> Training   79.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.964 Prec@5=94.489 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:53 IST
=> Training   79.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.751 Prec@1=80.964 Prec@5=94.489 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:54 IST
=> Training   79.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.752 Prec@1=80.943 Prec@5=94.481 rate=1.77 Hz, eta=0:04:43, total=0:18:49, wall=05:54 IST
=> Training   83.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.752 Prec@1=80.943 Prec@5=94.481 rate=1.77 Hz, eta=0:03:46, total=0:19:46, wall=05:54 IST
=> Training   83.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.567 DataTime=0.391 Loss=0.752 Prec@1=80.943 Prec@5=94.481 rate=1.77 Hz, eta=0:03:46, total=0:19:46, wall=05:55 IST
=> Training   83.94% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.935 Prec@5=94.476 rate=1.77 Hz, eta=0:03:46, total=0:19:46, wall=05:55 IST
=> Training   87.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.935 Prec@5=94.476 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=05:55 IST
=> Training   87.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.935 Prec@5=94.476 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=05:56 IST
=> Training   87.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.923 Prec@5=94.477 rate=1.77 Hz, eta=0:02:50, total=0:20:41, wall=05:56 IST
=> Training   91.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.923 Prec@5=94.477 rate=1.77 Hz, eta=0:01:53, total=0:21:37, wall=05:56 IST
=> Training   91.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.566 DataTime=0.390 Loss=0.752 Prec@1=80.923 Prec@5=94.477 rate=1.77 Hz, eta=0:01:53, total=0:21:37, wall=05:56 IST
=> Training   91.93% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.752 Prec@1=80.909 Prec@5=94.476 rate=1.77 Hz, eta=0:01:53, total=0:21:37, wall=05:56 IST
=> Training   95.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.752 Prec@1=80.909 Prec@5=94.476 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=05:56 IST
=> Training   95.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.752 Prec@1=80.909 Prec@5=94.476 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=05:57 IST
=> Training   95.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.753 Prec@1=80.905 Prec@5=94.473 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=05:57 IST
=> Training   99.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.753 Prec@1=80.905 Prec@5=94.473 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=05:57 IST
=> Training   99.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.565 DataTime=0.389 Loss=0.753 Prec@1=80.905 Prec@5=94.473 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=05:57 IST
=> Training   99.92% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.564 DataTime=0.389 Loss=0.753 Prec@1=80.905 Prec@5=94.472 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=05:57 IST
=> Training   100.00% of 1x2503...Epoch=134/150 LR=0.0031 Time=0.564 DataTime=0.389 Loss=0.753 Prec@1=80.905 Prec@5=94.472 rate=1.78 Hz, eta=0:00:00, total=0:23:27, wall=05:57 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:57 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=05:57 IST
=> Validation 0.00% of 1x98...Epoch=134/150 LR=0.0031 Time=6.846 Loss=0.725 Prec@1=80.469 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=05:57 IST
=> Validation 1.02% of 1x98...Epoch=134/150 LR=0.0031 Time=6.846 Loss=0.725 Prec@1=80.469 Prec@5=94.727 rate=3170.34 Hz, eta=0:00:00, total=0:00:00, wall=05:57 IST
** Validation 1.02% of 1x98...Epoch=134/150 LR=0.0031 Time=6.846 Loss=0.725 Prec@1=80.469 Prec@5=94.727 rate=3170.34 Hz, eta=0:00:00, total=0:00:00, wall=05:58 IST
** Validation 1.02% of 1x98...Epoch=134/150 LR=0.0031 Time=0.634 Loss=1.184 Prec@1=71.506 Prec@5=90.044 rate=3170.34 Hz, eta=0:00:00, total=0:00:00, wall=05:58 IST
** Validation 100.00% of 1x98...Epoch=134/150 LR=0.0031 Time=0.634 Loss=1.184 Prec@1=71.506 Prec@5=90.044 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=05:58 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:58 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=05:58 IST
=> Training   0.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=4.989 DataTime=4.337 Loss=0.743 Prec@1=80.664 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=05:58 IST
=> Training   0.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=4.989 DataTime=4.337 Loss=0.743 Prec@1=80.664 Prec@5=94.727 rate=6359.70 Hz, eta=0:00:00, total=0:00:00, wall=05:58 IST
=> Training   0.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=4.989 DataTime=4.337 Loss=0.743 Prec@1=80.664 Prec@5=94.727 rate=6359.70 Hz, eta=0:00:00, total=0:00:00, wall=05:59 IST
=> Training   0.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.597 DataTime=0.430 Loss=0.732 Prec@1=81.530 Prec@5=94.541 rate=6359.70 Hz, eta=0:00:00, total=0:00:00, wall=05:59 IST
=> Training   4.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.597 DataTime=0.430 Loss=0.732 Prec@1=81.530 Prec@5=94.541 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=05:59 IST
=> Training   4.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.597 DataTime=0.430 Loss=0.732 Prec@1=81.530 Prec@5=94.541 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=06:00 IST
=> Training   4.04% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.578 DataTime=0.407 Loss=0.738 Prec@1=81.374 Prec@5=94.562 rate=1.83 Hz, eta=0:21:54, total=0:00:55, wall=06:00 IST
=> Training   8.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.578 DataTime=0.407 Loss=0.738 Prec@1=81.374 Prec@5=94.562 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:00 IST
=> Training   8.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.578 DataTime=0.407 Loss=0.738 Prec@1=81.374 Prec@5=94.562 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:01 IST
=> Training   8.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.573 DataTime=0.397 Loss=0.739 Prec@1=81.327 Prec@5=94.603 rate=1.81 Hz, eta=0:21:12, total=0:01:51, wall=06:01 IST
=> Training   12.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.573 DataTime=0.397 Loss=0.739 Prec@1=81.327 Prec@5=94.603 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=06:01 IST
=> Training   12.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.573 DataTime=0.397 Loss=0.739 Prec@1=81.327 Prec@5=94.603 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=06:02 IST
=> Training   12.03% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.395 Loss=0.739 Prec@1=81.291 Prec@5=94.625 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=06:02 IST
=> Training   16.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.395 Loss=0.739 Prec@1=81.291 Prec@5=94.625 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=06:02 IST
=> Training   16.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.395 Loss=0.739 Prec@1=81.291 Prec@5=94.625 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=06:03 IST
=> Training   16.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.572 DataTime=0.398 Loss=0.740 Prec@1=81.267 Prec@5=94.611 rate=1.79 Hz, eta=0:19:31, total=0:03:43, wall=06:03 IST
=> Training   20.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.572 DataTime=0.398 Loss=0.740 Prec@1=81.267 Prec@5=94.611 rate=1.78 Hz, eta=0:18:46, total=0:04:41, wall=06:03 IST
=> Training   20.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.572 DataTime=0.398 Loss=0.740 Prec@1=81.267 Prec@5=94.611 rate=1.78 Hz, eta=0:18:46, total=0:04:41, wall=06:04 IST
=> Training   20.02% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.571 DataTime=0.395 Loss=0.740 Prec@1=81.279 Prec@5=94.620 rate=1.78 Hz, eta=0:18:46, total=0:04:41, wall=06:04 IST
=> Training   24.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.571 DataTime=0.395 Loss=0.740 Prec@1=81.279 Prec@5=94.620 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=06:04 IST
=> Training   24.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.571 DataTime=0.395 Loss=0.740 Prec@1=81.279 Prec@5=94.620 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=06:05 IST
=> Training   24.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.394 Loss=0.741 Prec@1=81.238 Prec@5=94.584 rate=1.78 Hz, eta=0:17:49, total=0:05:37, wall=06:05 IST
=> Training   28.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.394 Loss=0.741 Prec@1=81.238 Prec@5=94.584 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=06:05 IST
=> Training   28.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.570 DataTime=0.394 Loss=0.741 Prec@1=81.238 Prec@5=94.584 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=06:06 IST
=> Training   28.01% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.391 Loss=0.742 Prec@1=81.233 Prec@5=94.580 rate=1.78 Hz, eta=0:16:54, total=0:06:34, wall=06:06 IST
=> Training   32.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.391 Loss=0.742 Prec@1=81.233 Prec@5=94.580 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:06 IST
=> Training   32.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.391 Loss=0.742 Prec@1=81.233 Prec@5=94.580 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:07 IST
=> Training   32.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.393 Loss=0.741 Prec@1=81.253 Prec@5=94.589 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=06:07 IST
=> Training   36.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.393 Loss=0.741 Prec@1=81.253 Prec@5=94.589 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=06:07 IST
=> Training   36.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.568 DataTime=0.393 Loss=0.741 Prec@1=81.253 Prec@5=94.589 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=06:08 IST
=> Training   36.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.390 Loss=0.741 Prec@1=81.246 Prec@5=94.590 rate=1.78 Hz, eta=0:15:01, total=0:08:27, wall=06:08 IST
=> Training   39.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.390 Loss=0.741 Prec@1=81.246 Prec@5=94.590 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=06:08 IST
=> Training   39.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.390 Loss=0.741 Prec@1=81.246 Prec@5=94.590 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=06:09 IST
=> Training   39.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.567 DataTime=0.390 Loss=0.740 Prec@1=81.255 Prec@5=94.592 rate=1.78 Hz, eta=0:14:03, total=0:09:22, wall=06:09 IST
=> Training   43.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.567 DataTime=0.390 Loss=0.740 Prec@1=81.255 Prec@5=94.592 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:09 IST
=> Training   43.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.567 DataTime=0.390 Loss=0.740 Prec@1=81.255 Prec@5=94.592 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:10 IST
=> Training   43.99% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.389 Loss=0.742 Prec@1=81.196 Prec@5=94.573 rate=1.78 Hz, eta=0:13:08, total=0:10:18, wall=06:10 IST
=> Training   47.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.389 Loss=0.742 Prec@1=81.196 Prec@5=94.573 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:10 IST
=> Training   47.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.389 Loss=0.742 Prec@1=81.196 Prec@5=94.573 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:11 IST
=> Training   47.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.190 Prec@5=94.565 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=06:11 IST
=> Training   51.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.190 Prec@5=94.565 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=06:11 IST
=> Training   51.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.190 Prec@5=94.565 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=06:12 IST
=> Training   51.98% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.178 Prec@5=94.557 rate=1.78 Hz, eta=0:11:15, total=0:12:10, wall=06:12 IST
=> Training   55.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.178 Prec@5=94.557 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=06:12 IST
=> Training   55.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.743 Prec@1=81.178 Prec@5=94.557 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=06:13 IST
=> Training   55.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.391 Loss=0.744 Prec@1=81.161 Prec@5=94.556 rate=1.78 Hz, eta=0:10:18, total=0:13:06, wall=06:13 IST
=> Training   59.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.391 Loss=0.744 Prec@1=81.161 Prec@5=94.556 rate=1.78 Hz, eta=0:09:24, total=0:14:04, wall=06:13 IST
=> Training   59.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.391 Loss=0.744 Prec@1=81.161 Prec@5=94.556 rate=1.78 Hz, eta=0:09:24, total=0:14:04, wall=06:14 IST
=> Training   59.97% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.392 Loss=0.744 Prec@1=81.146 Prec@5=94.555 rate=1.78 Hz, eta=0:09:24, total=0:14:04, wall=06:14 IST
=> Training   63.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.392 Loss=0.744 Prec@1=81.146 Prec@5=94.555 rate=1.78 Hz, eta=0:08:28, total=0:15:01, wall=06:14 IST
=> Training   63.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.566 DataTime=0.392 Loss=0.744 Prec@1=81.146 Prec@5=94.555 rate=1.78 Hz, eta=0:08:28, total=0:15:01, wall=06:14 IST
=> Training   63.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.126 Prec@5=94.555 rate=1.78 Hz, eta=0:08:28, total=0:15:01, wall=06:14 IST
=> Training   67.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.126 Prec@5=94.555 rate=1.78 Hz, eta=0:07:31, total=0:15:56, wall=06:14 IST
=> Training   67.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.126 Prec@5=94.555 rate=1.78 Hz, eta=0:07:31, total=0:15:56, wall=06:15 IST
=> Training   67.96% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.554 rate=1.78 Hz, eta=0:07:31, total=0:15:56, wall=06:15 IST
=> Training   71.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.554 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=06:15 IST
=> Training   71.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.554 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=06:16 IST
=> Training   71.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.555 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=06:16 IST
=> Training   75.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.555 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:16 IST
=> Training   75.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.745 Prec@1=81.122 Prec@5=94.555 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:17 IST
=> Training   75.95% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.120 Prec@5=94.556 rate=1.78 Hz, eta=0:05:38, total=0:17:49, wall=06:17 IST
=> Training   79.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.120 Prec@5=94.556 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:17 IST
=> Training   79.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.744 Prec@1=81.120 Prec@5=94.556 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:18 IST
=> Training   79.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.564 DataTime=0.390 Loss=0.745 Prec@1=81.110 Prec@5=94.553 rate=1.78 Hz, eta=0:04:42, total=0:18:44, wall=06:18 IST
=> Training   83.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.564 DataTime=0.390 Loss=0.745 Prec@1=81.110 Prec@5=94.553 rate=1.78 Hz, eta=0:03:45, total=0:19:41, wall=06:18 IST
=> Training   83.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.564 DataTime=0.390 Loss=0.745 Prec@1=81.110 Prec@5=94.553 rate=1.78 Hz, eta=0:03:45, total=0:19:41, wall=06:19 IST
=> Training   83.94% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.097 Prec@5=94.541 rate=1.78 Hz, eta=0:03:45, total=0:19:41, wall=06:19 IST
=> Training   87.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.097 Prec@5=94.541 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:19 IST
=> Training   87.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.097 Prec@5=94.541 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:20 IST
=> Training   87.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.095 Prec@5=94.534 rate=1.78 Hz, eta=0:02:49, total=0:20:38, wall=06:20 IST
=> Training   91.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.095 Prec@5=94.534 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=06:20 IST
=> Training   91.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.095 Prec@5=94.534 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=06:21 IST
=> Training   91.93% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.079 Prec@5=94.537 rate=1.78 Hz, eta=0:01:53, total=0:21:34, wall=06:21 IST
=> Training   95.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.079 Prec@5=94.537 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:21 IST
=> Training   95.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.746 Prec@1=81.079 Prec@5=94.537 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:22 IST
=> Training   95.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.747 Prec@1=81.054 Prec@5=94.526 rate=1.78 Hz, eta=0:00:57, total=0:22:31, wall=06:22 IST
=> Training   99.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.747 Prec@1=81.054 Prec@5=94.526 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=06:22 IST
=> Training   99.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.565 DataTime=0.390 Loss=0.747 Prec@1=81.054 Prec@5=94.526 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=06:22 IST
=> Training   99.92% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.564 DataTime=0.390 Loss=0.747 Prec@1=81.052 Prec@5=94.526 rate=1.78 Hz, eta=0:00:01, total=0:23:27, wall=06:22 IST
=> Training   100.00% of 1x2503...Epoch=135/150 LR=0.0028 Time=0.564 DataTime=0.390 Loss=0.747 Prec@1=81.052 Prec@5=94.526 rate=1.78 Hz, eta=0:00:00, total=0:23:27, wall=06:22 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:22 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:22 IST
=> Validation 0.00% of 1x98...Epoch=135/150 LR=0.0028 Time=6.990 Loss=0.725 Prec@1=80.664 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=06:22 IST
=> Validation 1.02% of 1x98...Epoch=135/150 LR=0.0028 Time=6.990 Loss=0.725 Prec@1=80.664 Prec@5=95.312 rate=2597.29 Hz, eta=0:00:00, total=0:00:00, wall=06:22 IST
** Validation 1.02% of 1x98...Epoch=135/150 LR=0.0028 Time=6.990 Loss=0.725 Prec@1=80.664 Prec@5=95.312 rate=2597.29 Hz, eta=0:00:00, total=0:00:00, wall=06:23 IST
** Validation 1.02% of 1x98...Epoch=135/150 LR=0.0028 Time=0.639 Loss=1.188 Prec@1=71.230 Prec@5=90.018 rate=2597.29 Hz, eta=0:00:00, total=0:00:00, wall=06:23 IST
** Validation 100.00% of 1x98...Epoch=135/150 LR=0.0028 Time=0.639 Loss=1.188 Prec@1=71.230 Prec@5=90.018 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=06:23 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST
=> Training   0.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=5.090 DataTime=4.829 Loss=0.663 Prec@1=82.617 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=06:23 IST
=> Training   0.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=5.090 DataTime=4.829 Loss=0.663 Prec@1=82.617 Prec@5=95.312 rate=2135.85 Hz, eta=0:00:01, total=0:00:00, wall=06:23 IST
=> Training   0.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=5.090 DataTime=4.829 Loss=0.663 Prec@1=82.617 Prec@5=95.312 rate=2135.85 Hz, eta=0:00:01, total=0:00:00, wall=06:24 IST
=> Training   0.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.599 DataTime=0.438 Loss=0.721 Prec@1=81.817 Prec@5=94.748 rate=2135.85 Hz, eta=0:00:01, total=0:00:00, wall=06:24 IST
=> Training   4.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.599 DataTime=0.438 Loss=0.721 Prec@1=81.817 Prec@5=94.748 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=06:24 IST
=> Training   4.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.599 DataTime=0.438 Loss=0.721 Prec@1=81.817 Prec@5=94.748 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=06:25 IST
=> Training   4.04% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.578 DataTime=0.411 Loss=0.727 Prec@1=81.632 Prec@5=94.712 rate=1.82 Hz, eta=0:21:57, total=0:00:55, wall=06:25 IST
=> Training   8.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.578 DataTime=0.411 Loss=0.727 Prec@1=81.632 Prec@5=94.712 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=06:25 IST
=> Training   8.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.578 DataTime=0.411 Loss=0.727 Prec@1=81.632 Prec@5=94.712 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=06:26 IST
=> Training   8.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.577 DataTime=0.408 Loss=0.729 Prec@1=81.591 Prec@5=94.695 rate=1.81 Hz, eta=0:21:11, total=0:01:51, wall=06:26 IST
=> Training   12.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.577 DataTime=0.408 Loss=0.729 Prec@1=81.591 Prec@5=94.695 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=06:26 IST
=> Training   12.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.577 DataTime=0.408 Loss=0.729 Prec@1=81.591 Prec@5=94.695 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=06:27 IST
=> Training   12.03% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.620 Prec@5=94.697 rate=1.79 Hz, eta=0:20:33, total=0:02:48, wall=06:27 IST
=> Training   16.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.620 Prec@5=94.697 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=06:27 IST
=> Training   16.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.620 Prec@5=94.697 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=06:28 IST
=> Training   16.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.556 Prec@5=94.701 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=06:28 IST
=> Training   20.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.556 Prec@5=94.701 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=06:28 IST
=> Training   20.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.568 DataTime=0.396 Loss=0.728 Prec@1=81.556 Prec@5=94.701 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=06:29 IST
=> Training   20.02% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.731 Prec@1=81.497 Prec@5=94.679 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=06:29 IST
=> Training   24.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.731 Prec@1=81.497 Prec@5=94.679 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=06:29 IST
=> Training   24.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.731 Prec@1=81.497 Prec@5=94.679 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=06:30 IST
=> Training   24.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.565 DataTime=0.393 Loss=0.731 Prec@1=81.488 Prec@5=94.674 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=06:30 IST
=> Training   28.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.565 DataTime=0.393 Loss=0.731 Prec@1=81.488 Prec@5=94.674 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=06:30 IST
=> Training   28.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.565 DataTime=0.393 Loss=0.731 Prec@1=81.488 Prec@5=94.674 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=06:31 IST
=> Training   28.01% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.732 Prec@1=81.460 Prec@5=94.671 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=06:31 IST
=> Training   32.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.732 Prec@1=81.460 Prec@5=94.671 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=06:31 IST
=> Training   32.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.564 DataTime=0.392 Loss=0.732 Prec@1=81.460 Prec@5=94.671 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=06:31 IST
=> Training   32.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.672 rate=1.79 Hz, eta=0:15:49, total=0:07:26, wall=06:31 IST
=> Training   36.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.672 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=06:31 IST
=> Training   36.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.672 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=06:32 IST
=> Training   36.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.666 rate=1.80 Hz, eta=0:14:51, total=0:08:21, wall=06:32 IST
=> Training   39.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.666 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:32 IST
=> Training   39.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.460 Prec@5=94.666 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:33 IST
=> Training   39.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.450 Prec@5=94.687 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=06:33 IST
=> Training   43.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.450 Prec@5=94.687 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=06:33 IST
=> Training   43.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.732 Prec@1=81.450 Prec@5=94.687 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=06:34 IST
=> Training   43.99% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.425 Prec@5=94.680 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=06:34 IST
=> Training   47.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.425 Prec@5=94.680 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:34 IST
=> Training   47.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.425 Prec@5=94.680 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:35 IST
=> Training   47.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.734 Prec@1=81.395 Prec@5=94.672 rate=1.80 Hz, eta=0:12:05, total=0:11:08, wall=06:35 IST
=> Training   51.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.734 Prec@1=81.395 Prec@5=94.672 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=06:35 IST
=> Training   51.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.562 DataTime=0.389 Loss=0.734 Prec@1=81.395 Prec@5=94.672 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=06:36 IST
=> Training   51.98% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.388 Loss=0.735 Prec@1=81.390 Prec@5=94.674 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=06:36 IST
=> Training   55.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.388 Loss=0.735 Prec@1=81.390 Prec@5=94.674 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=06:36 IST
=> Training   55.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.388 Loss=0.735 Prec@1=81.390 Prec@5=94.674 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=06:37 IST
=> Training   55.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.364 Prec@5=94.657 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=06:37 IST
=> Training   59.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.364 Prec@5=94.657 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=06:37 IST
=> Training   59.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.364 Prec@5=94.657 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=06:38 IST
=> Training   59.97% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.369 Prec@5=94.650 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=06:38 IST
=> Training   63.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.369 Prec@5=94.650 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=06:38 IST
=> Training   63.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.369 Prec@5=94.650 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=06:39 IST
=> Training   63.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.351 Prec@5=94.647 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=06:39 IST
=> Training   67.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.351 Prec@5=94.647 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=06:39 IST
=> Training   67.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.736 Prec@1=81.351 Prec@5=94.647 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=06:40 IST
=> Training   67.96% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.354 Prec@5=94.647 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=06:40 IST
=> Training   71.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.354 Prec@5=94.647 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=06:40 IST
=> Training   71.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.354 Prec@5=94.647 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=06:41 IST
=> Training   71.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.340 Prec@5=94.642 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=06:41 IST
=> Training   75.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.340 Prec@5=94.642 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=06:41 IST
=> Training   75.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.737 Prec@1=81.340 Prec@5=94.642 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=06:42 IST
=> Training   75.95% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.737 Prec@1=81.337 Prec@5=94.642 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=06:42 IST
=> Training   79.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.737 Prec@1=81.337 Prec@5=94.642 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=06:42 IST
=> Training   79.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.737 Prec@1=81.337 Prec@5=94.642 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=06:43 IST
=> Training   79.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.329 Prec@5=94.638 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=06:43 IST
=> Training   83.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.329 Prec@5=94.638 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:43 IST
=> Training   83.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.329 Prec@5=94.638 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:44 IST
=> Training   83.94% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.738 Prec@1=81.306 Prec@5=94.634 rate=1.79 Hz, eta=0:03:44, total=0:19:31, wall=06:44 IST
=> Training   87.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.738 Prec@1=81.306 Prec@5=94.634 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=06:44 IST
=> Training   87.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.387 Loss=0.738 Prec@1=81.306 Prec@5=94.634 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=06:45 IST
=> Training   87.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.386 Loss=0.738 Prec@1=81.305 Prec@5=94.631 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=06:45 IST
=> Training   91.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.386 Loss=0.738 Prec@1=81.305 Prec@5=94.631 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=06:45 IST
=> Training   91.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.561 DataTime=0.386 Loss=0.738 Prec@1=81.305 Prec@5=94.631 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=06:45 IST
=> Training   91.93% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.284 Prec@5=94.628 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=06:45 IST
=> Training   95.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.284 Prec@5=94.628 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=06:45 IST
=> Training   95.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.738 Prec@1=81.284 Prec@5=94.628 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=06:46 IST
=> Training   95.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.739 Prec@1=81.276 Prec@5=94.622 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=06:46 IST
=> Training   99.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.739 Prec@1=81.276 Prec@5=94.622 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=06:46 IST
=> Training   99.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.739 Prec@1=81.276 Prec@5=94.622 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=06:46 IST
=> Training   99.92% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.739 Prec@1=81.276 Prec@5=94.622 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=06:46 IST
=> Training   100.00% of 1x2503...Epoch=136/150 LR=0.0024 Time=0.560 DataTime=0.386 Loss=0.739 Prec@1=81.276 Prec@5=94.622 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=06:46 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> Validation 0.00% of 1x98...Epoch=136/150 LR=0.0024 Time=7.096 Loss=0.684 Prec@1=82.227 Prec@5=95.703 rate=0 Hz, eta=?, total=0:00:00, wall=06:46 IST
=> Validation 1.02% of 1x98...Epoch=136/150 LR=0.0024 Time=7.096 Loss=0.684 Prec@1=82.227 Prec@5=95.703 rate=7523.21 Hz, eta=0:00:00, total=0:00:00, wall=06:46 IST
** Validation 1.02% of 1x98...Epoch=136/150 LR=0.0024 Time=7.096 Loss=0.684 Prec@1=82.227 Prec@5=95.703 rate=7523.21 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST
** Validation 1.02% of 1x98...Epoch=136/150 LR=0.0024 Time=0.638 Loss=1.179 Prec@1=71.584 Prec@5=90.134 rate=7523.21 Hz, eta=0:00:00, total=0:00:00, wall=06:47 IST
** Validation 100.00% of 1x98...Epoch=136/150 LR=0.0024 Time=0.638 Loss=1.179 Prec@1=71.584 Prec@5=90.134 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=06:47 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:48 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=06:48 IST
=> Training   0.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=5.275 DataTime=5.033 Loss=0.827 Prec@1=78.711 Prec@5=93.750 rate=0 Hz, eta=?, total=0:00:00, wall=06:48 IST
=> Training   0.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=5.275 DataTime=5.033 Loss=0.827 Prec@1=78.711 Prec@5=93.750 rate=8428.65 Hz, eta=0:00:00, total=0:00:00, wall=06:48 IST
=> Training   0.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=5.275 DataTime=5.033 Loss=0.827 Prec@1=78.711 Prec@5=93.750 rate=8428.65 Hz, eta=0:00:00, total=0:00:00, wall=06:48 IST
=> Training   0.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.591 DataTime=0.423 Loss=0.729 Prec@1=81.592 Prec@5=94.825 rate=8428.65 Hz, eta=0:00:00, total=0:00:00, wall=06:48 IST
=> Training   4.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.591 DataTime=0.423 Loss=0.729 Prec@1=81.592 Prec@5=94.825 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=06:48 IST
=> Training   4.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.591 DataTime=0.423 Loss=0.729 Prec@1=81.592 Prec@5=94.825 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=06:49 IST
=> Training   4.04% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.573 DataTime=0.405 Loss=0.719 Prec@1=81.856 Prec@5=94.929 rate=1.86 Hz, eta=0:21:33, total=0:00:54, wall=06:49 IST
=> Training   8.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.573 DataTime=0.405 Loss=0.719 Prec@1=81.856 Prec@5=94.929 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=06:49 IST
=> Training   8.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.573 DataTime=0.405 Loss=0.719 Prec@1=81.856 Prec@5=94.929 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=06:50 IST
=> Training   8.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.570 DataTime=0.400 Loss=0.721 Prec@1=81.779 Prec@5=94.857 rate=1.83 Hz, eta=0:20:58, total=0:01:49, wall=06:50 IST
=> Training   12.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.570 DataTime=0.400 Loss=0.721 Prec@1=81.779 Prec@5=94.857 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=06:50 IST
=> Training   12.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.570 DataTime=0.400 Loss=0.721 Prec@1=81.779 Prec@5=94.857 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=06:51 IST
=> Training   12.03% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.565 DataTime=0.393 Loss=0.725 Prec@1=81.711 Prec@5=94.787 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=06:51 IST
=> Training   16.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.565 DataTime=0.393 Loss=0.725 Prec@1=81.711 Prec@5=94.787 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:51 IST
=> Training   16.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.565 DataTime=0.393 Loss=0.725 Prec@1=81.711 Prec@5=94.787 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:52 IST
=> Training   16.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.390 Loss=0.727 Prec@1=81.657 Prec@5=94.749 rate=1.81 Hz, eta=0:19:19, total=0:03:41, wall=06:52 IST
=> Training   20.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.390 Loss=0.727 Prec@1=81.657 Prec@5=94.749 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=06:52 IST
=> Training   20.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.390 Loss=0.727 Prec@1=81.657 Prec@5=94.749 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=06:53 IST
=> Training   20.02% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.727 Prec@1=81.638 Prec@5=94.751 rate=1.81 Hz, eta=0:18:28, total=0:04:37, wall=06:53 IST
=> Training   24.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.727 Prec@1=81.638 Prec@5=94.751 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=06:53 IST
=> Training   24.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.727 Prec@1=81.638 Prec@5=94.751 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=06:54 IST
=> Training   24.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.566 DataTime=0.393 Loss=0.729 Prec@1=81.623 Prec@5=94.711 rate=1.81 Hz, eta=0:17:33, total=0:05:32, wall=06:54 IST
=> Training   28.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.566 DataTime=0.393 Loss=0.729 Prec@1=81.623 Prec@5=94.711 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=06:54 IST
=> Training   28.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.566 DataTime=0.393 Loss=0.729 Prec@1=81.623 Prec@5=94.711 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=06:55 IST
=> Training   28.01% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.390 Loss=0.729 Prec@1=81.593 Prec@5=94.725 rate=1.79 Hz, eta=0:16:46, total=0:06:31, wall=06:55 IST
=> Training   32.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.390 Loss=0.729 Prec@1=81.593 Prec@5=94.725 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=06:55 IST
=> Training   32.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.390 Loss=0.729 Prec@1=81.593 Prec@5=94.725 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=06:56 IST
=> Training   32.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.391 Loss=0.730 Prec@1=81.564 Prec@5=94.725 rate=1.80 Hz, eta=0:15:47, total=0:07:25, wall=06:56 IST
=> Training   36.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.391 Loss=0.730 Prec@1=81.564 Prec@5=94.725 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=06:56 IST
=> Training   36.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.564 DataTime=0.391 Loss=0.730 Prec@1=81.564 Prec@5=94.725 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=06:57 IST
=> Training   36.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.731 Prec@1=81.554 Prec@5=94.717 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=06:57 IST
=> Training   39.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.731 Prec@1=81.554 Prec@5=94.717 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=06:57 IST
=> Training   39.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.731 Prec@1=81.554 Prec@5=94.717 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=06:58 IST
=> Training   39.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.515 Prec@5=94.696 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=06:58 IST
=> Training   43.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.515 Prec@5=94.696 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:58 IST
=> Training   43.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.515 Prec@5=94.696 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:59 IST
=> Training   43.99% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.504 Prec@5=94.690 rate=1.79 Hz, eta=0:13:01, total=0:10:13, wall=06:59 IST
=> Training   47.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.504 Prec@5=94.690 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=06:59 IST
=> Training   47.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.504 Prec@5=94.690 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=07:00 IST
=> Training   47.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.498 Prec@5=94.682 rate=1.79 Hz, eta=0:12:05, total=0:11:09, wall=07:00 IST
=> Training   51.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.498 Prec@5=94.682 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=07:00 IST
=> Training   51.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.732 Prec@1=81.498 Prec@5=94.682 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=07:01 IST
=> Training   51.98% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.490 Prec@5=94.676 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=07:01 IST
=> Training   55.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.490 Prec@5=94.676 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=07:01 IST
=> Training   55.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.389 Loss=0.733 Prec@1=81.490 Prec@5=94.676 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=07:02 IST
=> Training   55.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.733 Prec@1=81.484 Prec@5=94.675 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=07:02 IST
=> Training   59.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.733 Prec@1=81.484 Prec@5=94.675 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:02 IST
=> Training   59.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.389 Loss=0.733 Prec@1=81.484 Prec@5=94.675 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:02 IST
=> Training   59.97% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.488 Prec@5=94.679 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:02 IST
=> Training   63.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.488 Prec@5=94.679 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:02 IST
=> Training   63.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.488 Prec@5=94.679 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:03 IST
=> Training   63.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.733 Prec@1=81.476 Prec@5=94.673 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=07:03 IST
=> Training   67.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.733 Prec@1=81.476 Prec@5=94.673 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=07:03 IST
=> Training   67.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.563 DataTime=0.389 Loss=0.733 Prec@1=81.476 Prec@5=94.673 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=07:04 IST
=> Training   67.96% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.478 Prec@5=94.673 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=07:04 IST
=> Training   71.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.478 Prec@5=94.673 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:04 IST
=> Training   71.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.478 Prec@5=94.673 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:05 IST
=> Training   71.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.461 Prec@5=94.670 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:05 IST
=> Training   75.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.461 Prec@5=94.670 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:05 IST
=> Training   75.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.461 Prec@5=94.670 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:06 IST
=> Training   75.95% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.464 Prec@5=94.672 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:06 IST
=> Training   79.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.464 Prec@5=94.672 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:06 IST
=> Training   79.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.388 Loss=0.732 Prec@1=81.464 Prec@5=94.672 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:07 IST
=> Training   79.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.732 Prec@1=81.450 Prec@5=94.668 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=07:07 IST
=> Training   83.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.732 Prec@1=81.450 Prec@5=94.668 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:07 IST
=> Training   83.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.732 Prec@1=81.450 Prec@5=94.668 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:08 IST
=> Training   83.94% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.444 Prec@5=94.662 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:08 IST
=> Training   87.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.444 Prec@5=94.662 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:08 IST
=> Training   87.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.444 Prec@5=94.662 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:09 IST
=> Training   87.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.437 Prec@5=94.657 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=07:09 IST
=> Training   91.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.437 Prec@5=94.657 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:09 IST
=> Training   91.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.562 DataTime=0.387 Loss=0.733 Prec@1=81.437 Prec@5=94.657 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:10 IST
=> Training   91.93% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.386 Loss=0.733 Prec@1=81.427 Prec@5=94.659 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:10 IST
=> Training   95.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.386 Loss=0.733 Prec@1=81.427 Prec@5=94.659 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=07:10 IST
=> Training   95.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.386 Loss=0.733 Prec@1=81.427 Prec@5=94.659 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=07:11 IST
=> Training   95.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.385 Loss=0.733 Prec@1=81.426 Prec@5=94.660 rate=1.79 Hz, eta=0:00:56, total=0:22:21, wall=07:11 IST
=> Training   99.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.385 Loss=0.733 Prec@1=81.426 Prec@5=94.660 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=07:11 IST
=> Training   99.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.561 DataTime=0.385 Loss=0.733 Prec@1=81.426 Prec@5=94.660 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=07:11 IST
=> Training   99.92% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.560 DataTime=0.385 Loss=0.733 Prec@1=81.426 Prec@5=94.661 rate=1.79 Hz, eta=0:00:01, total=0:23:16, wall=07:11 IST
=> Training   100.00% of 1x2503...Epoch=137/150 LR=0.0021 Time=0.560 DataTime=0.385 Loss=0.733 Prec@1=81.426 Prec@5=94.661 rate=1.79 Hz, eta=0:00:00, total=0:23:17, wall=07:11 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST
=> Validation 0.00% of 1x98...Epoch=137/150 LR=0.0021 Time=6.886 Loss=0.724 Prec@1=80.859 Prec@5=94.727 rate=0 Hz, eta=?, total=0:00:00, wall=07:11 IST
=> Validation 1.02% of 1x98...Epoch=137/150 LR=0.0021 Time=6.886 Loss=0.724 Prec@1=80.859 Prec@5=94.727 rate=7337.73 Hz, eta=0:00:00, total=0:00:00, wall=07:11 IST
** Validation 1.02% of 1x98...Epoch=137/150 LR=0.0021 Time=6.886 Loss=0.724 Prec@1=80.859 Prec@5=94.727 rate=7337.73 Hz, eta=0:00:00, total=0:00:00, wall=07:12 IST
** Validation 1.02% of 1x98...Epoch=137/150 LR=0.0021 Time=0.636 Loss=1.178 Prec@1=71.588 Prec@5=90.234 rate=7337.73 Hz, eta=0:00:00, total=0:00:00, wall=07:12 IST
** Validation 100.00% of 1x98...Epoch=137/150 LR=0.0021 Time=0.636 Loss=1.178 Prec@1=71.588 Prec@5=90.234 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=07:12 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:12 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:12 IST
=> Training   0.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=4.793 DataTime=4.326 Loss=0.759 Prec@1=81.641 Prec@5=93.359 rate=0 Hz, eta=?, total=0:00:00, wall=07:12 IST
=> Training   0.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=4.793 DataTime=4.326 Loss=0.759 Prec@1=81.641 Prec@5=93.359 rate=5184.30 Hz, eta=0:00:00, total=0:00:00, wall=07:12 IST
=> Training   0.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=4.793 DataTime=4.326 Loss=0.759 Prec@1=81.641 Prec@5=93.359 rate=5184.30 Hz, eta=0:00:00, total=0:00:00, wall=07:13 IST
=> Training   0.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.600 DataTime=0.428 Loss=0.714 Prec@1=82.124 Prec@5=94.939 rate=5184.30 Hz, eta=0:00:00, total=0:00:00, wall=07:13 IST
=> Training   4.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.600 DataTime=0.428 Loss=0.714 Prec@1=82.124 Prec@5=94.939 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=07:13 IST
=> Training   4.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.600 DataTime=0.428 Loss=0.714 Prec@1=82.124 Prec@5=94.939 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=07:14 IST
=> Training   4.04% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.578 DataTime=0.402 Loss=0.718 Prec@1=82.020 Prec@5=94.821 rate=1.81 Hz, eta=0:22:09, total=0:00:55, wall=07:14 IST
=> Training   8.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.578 DataTime=0.402 Loss=0.718 Prec@1=82.020 Prec@5=94.821 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=07:14 IST
=> Training   8.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.578 DataTime=0.402 Loss=0.718 Prec@1=82.020 Prec@5=94.821 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=07:15 IST
=> Training   8.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.577 DataTime=0.400 Loss=0.720 Prec@1=81.924 Prec@5=94.817 rate=1.80 Hz, eta=0:21:15, total=0:01:51, wall=07:15 IST
=> Training   12.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.577 DataTime=0.400 Loss=0.720 Prec@1=81.924 Prec@5=94.817 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=07:15 IST
=> Training   12.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.577 DataTime=0.400 Loss=0.720 Prec@1=81.924 Prec@5=94.817 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=07:16 IST
=> Training   12.03% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.570 DataTime=0.393 Loss=0.723 Prec@1=81.810 Prec@5=94.781 rate=1.78 Hz, eta=0:20:35, total=0:02:48, wall=07:16 IST
=> Training   16.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.570 DataTime=0.393 Loss=0.723 Prec@1=81.810 Prec@5=94.781 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:16 IST
=> Training   16.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.570 DataTime=0.393 Loss=0.723 Prec@1=81.810 Prec@5=94.781 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:17 IST
=> Training   16.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.568 DataTime=0.391 Loss=0.724 Prec@1=81.789 Prec@5=94.790 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:17 IST
=> Training   20.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.568 DataTime=0.391 Loss=0.724 Prec@1=81.789 Prec@5=94.790 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=07:17 IST
=> Training   20.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.568 DataTime=0.391 Loss=0.724 Prec@1=81.789 Prec@5=94.790 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=07:18 IST
=> Training   20.02% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.384 Loss=0.725 Prec@1=81.754 Prec@5=94.777 rate=1.79 Hz, eta=0:18:37, total=0:04:39, wall=07:18 IST
=> Training   24.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.384 Loss=0.725 Prec@1=81.754 Prec@5=94.777 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=07:18 IST
=> Training   24.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.384 Loss=0.725 Prec@1=81.754 Prec@5=94.777 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=07:18 IST
=> Training   24.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.384 Loss=0.725 Prec@1=81.749 Prec@5=94.755 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=07:18 IST
=> Training   28.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.384 Loss=0.725 Prec@1=81.749 Prec@5=94.755 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=07:18 IST
=> Training   28.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.384 Loss=0.725 Prec@1=81.749 Prec@5=94.755 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=07:19 IST
=> Training   28.01% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.385 Loss=0.724 Prec@1=81.766 Prec@5=94.780 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=07:19 IST
=> Training   32.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.385 Loss=0.724 Prec@1=81.766 Prec@5=94.780 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=07:19 IST
=> Training   32.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.385 Loss=0.724 Prec@1=81.766 Prec@5=94.780 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=07:20 IST
=> Training   32.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.386 Loss=0.723 Prec@1=81.775 Prec@5=94.787 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=07:20 IST
=> Training   36.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.386 Loss=0.723 Prec@1=81.775 Prec@5=94.787 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:20 IST
=> Training   36.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.563 DataTime=0.386 Loss=0.723 Prec@1=81.775 Prec@5=94.787 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:21 IST
=> Training   36.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.565 DataTime=0.389 Loss=0.722 Prec@1=81.814 Prec@5=94.803 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:21 IST
=> Training   39.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.565 DataTime=0.389 Loss=0.722 Prec@1=81.814 Prec@5=94.803 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=07:21 IST
=> Training   39.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.565 DataTime=0.389 Loss=0.722 Prec@1=81.814 Prec@5=94.803 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=07:22 IST
=> Training   39.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.723 Prec@1=81.795 Prec@5=94.785 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=07:22 IST
=> Training   43.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.723 Prec@1=81.795 Prec@5=94.785 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:22 IST
=> Training   43.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.723 Prec@1=81.795 Prec@5=94.785 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:23 IST
=> Training   43.99% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.722 Prec@1=81.798 Prec@5=94.788 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:23 IST
=> Training   47.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.722 Prec@1=81.798 Prec@5=94.788 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=07:23 IST
=> Training   47.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.564 DataTime=0.388 Loss=0.722 Prec@1=81.798 Prec@5=94.788 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=07:24 IST
=> Training   47.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.779 Prec@5=94.778 rate=1.79 Hz, eta=0:12:09, total=0:11:12, wall=07:24 IST
=> Training   51.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.779 Prec@5=94.778 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=07:24 IST
=> Training   51.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.779 Prec@5=94.778 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=07:25 IST
=> Training   51.98% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.723 Prec@1=81.772 Prec@5=94.787 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=07:25 IST
=> Training   55.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.723 Prec@1=81.772 Prec@5=94.787 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=07:25 IST
=> Training   55.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.723 Prec@1=81.772 Prec@5=94.787 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=07:26 IST
=> Training   55.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.754 Prec@5=94.777 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=07:26 IST
=> Training   59.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.754 Prec@5=94.777 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:26 IST
=> Training   59.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.386 Loss=0.723 Prec@1=81.754 Prec@5=94.777 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:27 IST
=> Training   59.97% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.746 Prec@5=94.768 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=07:27 IST
=> Training   63.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.746 Prec@5=94.768 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:27 IST
=> Training   63.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.746 Prec@5=94.768 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:28 IST
=> Training   63.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.714 Prec@5=94.769 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:28 IST
=> Training   67.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.714 Prec@5=94.769 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:28 IST
=> Training   67.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.724 Prec@1=81.714 Prec@5=94.769 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:29 IST
=> Training   67.96% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.725 Prec@1=81.697 Prec@5=94.755 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=07:29 IST
=> Training   71.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.725 Prec@1=81.697 Prec@5=94.755 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:29 IST
=> Training   71.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.725 Prec@1=81.697 Prec@5=94.755 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:30 IST
=> Training   71.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.561 DataTime=0.387 Loss=0.726 Prec@1=81.686 Prec@5=94.749 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=07:30 IST
=> Training   75.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.561 DataTime=0.387 Loss=0.726 Prec@1=81.686 Prec@5=94.749 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:30 IST
=> Training   75.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.561 DataTime=0.387 Loss=0.726 Prec@1=81.686 Prec@5=94.749 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:31 IST
=> Training   75.95% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.753 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=07:31 IST
=> Training   79.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.753 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=07:31 IST
=> Training   79.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.753 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=07:32 IST
=> Training   79.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.750 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=07:32 IST
=> Training   83.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.750 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:32 IST
=> Training   83.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.685 Prec@5=94.750 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:32 IST
=> Training   83.94% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.677 Prec@5=94.749 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=07:32 IST
=> Training   87.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.677 Prec@5=94.749 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=07:32 IST
=> Training   87.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.677 Prec@5=94.749 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=07:33 IST
=> Training   87.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.650 Prec@5=94.747 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=07:33 IST
=> Training   91.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.650 Prec@5=94.747 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:33 IST
=> Training   91.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.726 Prec@1=81.650 Prec@5=94.747 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:34 IST
=> Training   91.93% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.727 Prec@1=81.631 Prec@5=94.739 rate=1.79 Hz, eta=0:01:53, total=0:21:27, wall=07:34 IST
=> Training   95.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.727 Prec@1=81.631 Prec@5=94.739 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=07:34 IST
=> Training   95.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.388 Loss=0.727 Prec@1=81.631 Prec@5=94.739 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=07:35 IST
=> Training   95.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.727 Prec@1=81.624 Prec@5=94.732 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=07:35 IST
=> Training   99.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.727 Prec@1=81.624 Prec@5=94.732 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=07:35 IST
=> Training   99.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.562 DataTime=0.387 Loss=0.727 Prec@1=81.624 Prec@5=94.732 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=07:35 IST
=> Training   99.92% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.561 DataTime=0.387 Loss=0.727 Prec@1=81.623 Prec@5=94.731 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=07:35 IST
=> Training   100.00% of 1x2503...Epoch=138/150 LR=0.0018 Time=0.561 DataTime=0.387 Loss=0.727 Prec@1=81.623 Prec@5=94.731 rate=1.79 Hz, eta=0:00:00, total=0:23:20, wall=07:35 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST
=> Validation 0.00% of 1x98...Epoch=138/150 LR=0.0018 Time=7.026 Loss=0.714 Prec@1=81.250 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=07:35 IST
=> Validation 1.02% of 1x98...Epoch=138/150 LR=0.0018 Time=7.026 Loss=0.714 Prec@1=81.250 Prec@5=95.117 rate=2820.22 Hz, eta=0:00:00, total=0:00:00, wall=07:35 IST
** Validation 1.02% of 1x98...Epoch=138/150 LR=0.0018 Time=7.026 Loss=0.714 Prec@1=81.250 Prec@5=95.117 rate=2820.22 Hz, eta=0:00:00, total=0:00:00, wall=07:36 IST
** Validation 1.02% of 1x98...Epoch=138/150 LR=0.0018 Time=0.636 Loss=1.177 Prec@1=71.546 Prec@5=90.218 rate=2820.22 Hz, eta=0:00:00, total=0:00:00, wall=07:36 IST
** Validation 100.00% of 1x98...Epoch=138/150 LR=0.0018 Time=0.636 Loss=1.177 Prec@1=71.546 Prec@5=90.218 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=07:36 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> Training   0.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=4.124 DataTime=3.815 Loss=0.672 Prec@1=83.594 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=07:36 IST
=> Training   0.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=4.124 DataTime=3.815 Loss=0.672 Prec@1=83.594 Prec@5=95.508 rate=313.96 Hz, eta=0:00:07, total=0:00:00, wall=07:36 IST
=> Training   0.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=4.124 DataTime=3.815 Loss=0.672 Prec@1=83.594 Prec@5=95.508 rate=313.96 Hz, eta=0:00:07, total=0:00:00, wall=07:37 IST
=> Training   0.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.590 DataTime=0.421 Loss=0.726 Prec@1=81.871 Prec@5=94.727 rate=313.96 Hz, eta=0:00:07, total=0:00:00, wall=07:37 IST
=> Training   4.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.590 DataTime=0.421 Loss=0.726 Prec@1=81.871 Prec@5=94.727 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=07:37 IST
=> Training   4.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.590 DataTime=0.421 Loss=0.726 Prec@1=81.871 Prec@5=94.727 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=07:38 IST
=> Training   4.04% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.573 DataTime=0.401 Loss=0.719 Prec@1=81.787 Prec@5=94.805 rate=1.82 Hz, eta=0:21:59, total=0:00:55, wall=07:38 IST
=> Training   8.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.573 DataTime=0.401 Loss=0.719 Prec@1=81.787 Prec@5=94.805 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:38 IST
=> Training   8.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.573 DataTime=0.401 Loss=0.719 Prec@1=81.787 Prec@5=94.805 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:39 IST
=> Training   8.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.565 DataTime=0.395 Loss=0.721 Prec@1=81.766 Prec@5=94.753 rate=1.81 Hz, eta=0:21:10, total=0:01:50, wall=07:39 IST
=> Training   12.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.565 DataTime=0.395 Loss=0.721 Prec@1=81.766 Prec@5=94.753 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=07:39 IST
=> Training   12.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.565 DataTime=0.395 Loss=0.721 Prec@1=81.766 Prec@5=94.753 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=07:40 IST
=> Training   12.03% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.569 DataTime=0.400 Loss=0.722 Prec@1=81.698 Prec@5=94.737 rate=1.81 Hz, eta=0:20:14, total=0:02:46, wall=07:40 IST
=> Training   16.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.569 DataTime=0.400 Loss=0.722 Prec@1=81.698 Prec@5=94.737 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:40 IST
=> Training   16.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.569 DataTime=0.400 Loss=0.722 Prec@1=81.698 Prec@5=94.737 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:41 IST
=> Training   16.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.719 Prec@1=81.777 Prec@5=94.750 rate=1.79 Hz, eta=0:19:33, total=0:03:43, wall=07:41 IST
=> Training   20.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.719 Prec@1=81.777 Prec@5=94.750 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:41 IST
=> Training   20.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.719 Prec@1=81.777 Prec@5=94.750 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:42 IST
=> Training   20.02% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.721 Prec@1=81.766 Prec@5=94.720 rate=1.79 Hz, eta=0:18:36, total=0:04:39, wall=07:42 IST
=> Training   24.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.721 Prec@1=81.766 Prec@5=94.720 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:42 IST
=> Training   24.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.566 DataTime=0.397 Loss=0.721 Prec@1=81.766 Prec@5=94.720 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:43 IST
=> Training   24.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.721 Prec@1=81.745 Prec@5=94.723 rate=1.79 Hz, eta=0:17:43, total=0:05:36, wall=07:43 IST
=> Training   28.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.721 Prec@1=81.745 Prec@5=94.723 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:43 IST
=> Training   28.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.721 Prec@1=81.745 Prec@5=94.723 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:44 IST
=> Training   28.01% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.722 Prec@1=81.744 Prec@5=94.710 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=07:44 IST
=> Training   32.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.722 Prec@1=81.744 Prec@5=94.710 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=07:44 IST
=> Training   32.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.394 Loss=0.722 Prec@1=81.744 Prec@5=94.710 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=07:45 IST
=> Training   32.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.392 Loss=0.721 Prec@1=81.741 Prec@5=94.731 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=07:45 IST
=> Training   36.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.392 Loss=0.721 Prec@1=81.741 Prec@5=94.731 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:45 IST
=> Training   36.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.392 Loss=0.721 Prec@1=81.741 Prec@5=94.731 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:46 IST
=> Training   36.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.392 Loss=0.720 Prec@1=81.777 Prec@5=94.740 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=07:46 IST
=> Training   39.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.392 Loss=0.720 Prec@1=81.777 Prec@5=94.740 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=07:46 IST
=> Training   39.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.392 Loss=0.720 Prec@1=81.777 Prec@5=94.740 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=07:47 IST
=> Training   39.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.393 Loss=0.720 Prec@1=81.797 Prec@5=94.750 rate=1.79 Hz, eta=0:14:00, total=0:09:19, wall=07:47 IST
=> Training   43.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.393 Loss=0.720 Prec@1=81.797 Prec@5=94.750 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:47 IST
=> Training   43.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.393 Loss=0.720 Prec@1=81.797 Prec@5=94.750 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:48 IST
=> Training   43.99% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.720 Prec@1=81.808 Prec@5=94.765 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=07:48 IST
=> Training   47.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.720 Prec@1=81.808 Prec@5=94.765 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=07:48 IST
=> Training   47.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.720 Prec@1=81.808 Prec@5=94.765 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=07:49 IST
=> Training   47.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.720 Prec@1=81.795 Prec@5=94.765 rate=1.79 Hz, eta=0:12:07, total=0:11:11, wall=07:49 IST
=> Training   51.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.720 Prec@1=81.795 Prec@5=94.765 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=07:49 IST
=> Training   51.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.720 Prec@1=81.795 Prec@5=94.765 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=07:49 IST
=> Training   51.98% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.721 Prec@1=81.766 Prec@5=94.751 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=07:49 IST
=> Training   55.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.721 Prec@1=81.766 Prec@5=94.751 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=07:49 IST
=> Training   55.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.564 DataTime=0.391 Loss=0.721 Prec@1=81.766 Prec@5=94.751 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=07:50 IST
=> Training   55.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.390 Loss=0.721 Prec@1=81.767 Prec@5=94.755 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=07:50 IST
=> Training   59.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.390 Loss=0.721 Prec@1=81.767 Prec@5=94.755 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=07:50 IST
=> Training   59.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.390 Loss=0.721 Prec@1=81.767 Prec@5=94.755 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=07:51 IST
=> Training   59.97% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.721 Prec@1=81.775 Prec@5=94.771 rate=1.79 Hz, eta=0:09:20, total=0:14:00, wall=07:51 IST
=> Training   63.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.721 Prec@1=81.775 Prec@5=94.771 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:51 IST
=> Training   63.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.721 Prec@1=81.775 Prec@5=94.771 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:52 IST
=> Training   63.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.721 Prec@1=81.761 Prec@5=94.764 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=07:52 IST
=> Training   67.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.721 Prec@1=81.761 Prec@5=94.764 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=07:52 IST
=> Training   67.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.721 Prec@1=81.761 Prec@5=94.764 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=07:53 IST
=> Training   67.96% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.747 Prec@5=94.752 rate=1.79 Hz, eta=0:07:28, total=0:15:52, wall=07:53 IST
=> Training   71.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.747 Prec@5=94.752 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:53 IST
=> Training   71.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.747 Prec@5=94.752 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:54 IST
=> Training   71.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.745 Prec@5=94.751 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=07:54 IST
=> Training   75.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.745 Prec@5=94.751 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=07:54 IST
=> Training   75.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.722 Prec@1=81.745 Prec@5=94.751 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=07:55 IST
=> Training   75.95% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.391 Loss=0.722 Prec@1=81.732 Prec@5=94.752 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=07:55 IST
=> Training   79.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.391 Loss=0.722 Prec@1=81.732 Prec@5=94.752 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:55 IST
=> Training   79.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.391 Loss=0.722 Prec@1=81.732 Prec@5=94.752 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:56 IST
=> Training   79.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.391 Loss=0.722 Prec@1=81.741 Prec@5=94.753 rate=1.78 Hz, eta=0:04:41, total=0:18:41, wall=07:56 IST
=> Training   83.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.391 Loss=0.722 Prec@1=81.741 Prec@5=94.753 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:56 IST
=> Training   83.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.563 DataTime=0.391 Loss=0.722 Prec@1=81.741 Prec@5=94.753 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:57 IST
=> Training   83.94% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.723 Prec@1=81.720 Prec@5=94.751 rate=1.78 Hz, eta=0:03:45, total=0:19:37, wall=07:57 IST
=> Training   87.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.723 Prec@1=81.720 Prec@5=94.751 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:57 IST
=> Training   87.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.390 Loss=0.723 Prec@1=81.720 Prec@5=94.751 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:58 IST
=> Training   87.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.714 Prec@5=94.757 rate=1.78 Hz, eta=0:02:49, total=0:20:33, wall=07:58 IST
=> Training   91.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.714 Prec@5=94.757 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=07:58 IST
=> Training   91.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.714 Prec@5=94.757 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=07:59 IST
=> Training   91.93% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.721 Prec@5=94.762 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=07:59 IST
=> Training   95.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.721 Prec@5=94.762 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=07:59 IST
=> Training   95.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.562 DataTime=0.389 Loss=0.723 Prec@1=81.721 Prec@5=94.762 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=08:00 IST
=> Training   95.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.561 DataTime=0.388 Loss=0.723 Prec@1=81.713 Prec@5=94.765 rate=1.79 Hz, eta=0:00:57, total=0:22:24, wall=08:00 IST
=> Training   99.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.561 DataTime=0.388 Loss=0.723 Prec@1=81.713 Prec@5=94.765 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=08:00 IST
=> Training   99.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.561 DataTime=0.388 Loss=0.723 Prec@1=81.713 Prec@5=94.765 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=08:00 IST
=> Training   99.92% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.561 DataTime=0.388 Loss=0.723 Prec@1=81.712 Prec@5=94.765 rate=1.79 Hz, eta=0:00:01, total=0:23:19, wall=08:00 IST
=> Training   100.00% of 1x2503...Epoch=139/150 LR=0.0016 Time=0.561 DataTime=0.388 Loss=0.723 Prec@1=81.712 Prec@5=94.765 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=08:00 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST
=> Validation 0.00% of 1x98...Epoch=139/150 LR=0.0016 Time=6.816 Loss=0.684 Prec@1=81.836 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=08:00 IST
=> Validation 1.02% of 1x98...Epoch=139/150 LR=0.0016 Time=6.816 Loss=0.684 Prec@1=81.836 Prec@5=95.312 rate=6388.68 Hz, eta=0:00:00, total=0:00:00, wall=08:00 IST
** Validation 1.02% of 1x98...Epoch=139/150 LR=0.0016 Time=6.816 Loss=0.684 Prec@1=81.836 Prec@5=95.312 rate=6388.68 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST
** Validation 1.02% of 1x98...Epoch=139/150 LR=0.0016 Time=0.639 Loss=1.177 Prec@1=71.618 Prec@5=90.238 rate=6388.68 Hz, eta=0:00:00, total=0:00:00, wall=08:01 IST
** Validation 100.00% of 1x98...Epoch=139/150 LR=0.0016 Time=0.639 Loss=1.177 Prec@1=71.618 Prec@5=90.238 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=08:01 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST
=> Training   0.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=4.940 DataTime=4.314 Loss=0.629 Prec@1=84.570 Prec@5=96.289 rate=0 Hz, eta=?, total=0:00:00, wall=08:01 IST
=> Training   0.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=4.940 DataTime=4.314 Loss=0.629 Prec@1=84.570 Prec@5=96.289 rate=1748.26 Hz, eta=0:00:01, total=0:00:00, wall=08:01 IST
=> Training   0.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=4.940 DataTime=4.314 Loss=0.629 Prec@1=84.570 Prec@5=96.289 rate=1748.26 Hz, eta=0:00:01, total=0:00:00, wall=08:02 IST
=> Training   0.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.580 DataTime=0.403 Loss=0.705 Prec@1=82.066 Prec@5=95.071 rate=1748.26 Hz, eta=0:00:01, total=0:00:00, wall=08:02 IST
=> Training   4.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.580 DataTime=0.403 Loss=0.705 Prec@1=82.066 Prec@5=95.071 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=08:02 IST
=> Training   4.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.580 DataTime=0.403 Loss=0.705 Prec@1=82.066 Prec@5=95.071 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=08:03 IST
=> Training   4.04% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.571 DataTime=0.397 Loss=0.711 Prec@1=81.968 Prec@5=94.914 rate=1.88 Hz, eta=0:21:16, total=0:00:53, wall=08:03 IST
=> Training   8.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.571 DataTime=0.397 Loss=0.711 Prec@1=81.968 Prec@5=94.914 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=08:03 IST
=> Training   8.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.571 DataTime=0.397 Loss=0.711 Prec@1=81.968 Prec@5=94.914 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=08:04 IST
=> Training   8.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.390 Loss=0.710 Prec@1=82.078 Prec@5=94.929 rate=1.83 Hz, eta=0:20:57, total=0:01:49, wall=08:04 IST
=> Training   12.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.390 Loss=0.710 Prec@1=82.078 Prec@5=94.929 rate=1.83 Hz, eta=0:20:05, total=0:02:44, wall=08:04 IST
=> Training   12.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.390 Loss=0.710 Prec@1=82.078 Prec@5=94.929 rate=1.83 Hz, eta=0:20:05, total=0:02:44, wall=08:05 IST
=> Training   12.03% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.394 Loss=0.710 Prec@1=82.064 Prec@5=94.922 rate=1.83 Hz, eta=0:20:05, total=0:02:44, wall=08:05 IST
=> Training   16.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.394 Loss=0.710 Prec@1=82.064 Prec@5=94.922 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:05 IST
=> Training   16.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.394 Loss=0.710 Prec@1=82.064 Prec@5=94.922 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:06 IST
=> Training   16.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.392 Loss=0.710 Prec@1=82.064 Prec@5=94.925 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=08:06 IST
=> Training   20.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.392 Loss=0.710 Prec@1=82.064 Prec@5=94.925 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:06 IST
=> Training   20.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.392 Loss=0.710 Prec@1=82.064 Prec@5=94.925 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:06 IST
=> Training   20.02% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.569 DataTime=0.395 Loss=0.710 Prec@1=82.080 Prec@5=94.916 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:06 IST
=> Training   24.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.569 DataTime=0.395 Loss=0.710 Prec@1=82.080 Prec@5=94.916 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=08:06 IST
=> Training   24.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.569 DataTime=0.395 Loss=0.710 Prec@1=82.080 Prec@5=94.916 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=08:07 IST
=> Training   24.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.391 Loss=0.711 Prec@1=82.085 Prec@5=94.907 rate=1.78 Hz, eta=0:17:46, total=0:05:36, wall=08:07 IST
=> Training   28.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.391 Loss=0.711 Prec@1=82.085 Prec@5=94.907 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=08:07 IST
=> Training   28.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.391 Loss=0.711 Prec@1=82.085 Prec@5=94.907 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=08:08 IST
=> Training   28.01% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.567 DataTime=0.391 Loss=0.712 Prec@1=82.063 Prec@5=94.897 rate=1.79 Hz, eta=0:16:47, total=0:06:32, wall=08:08 IST
=> Training   32.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.567 DataTime=0.391 Loss=0.712 Prec@1=82.063 Prec@5=94.897 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=08:08 IST
=> Training   32.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.567 DataTime=0.391 Loss=0.712 Prec@1=82.063 Prec@5=94.897 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=08:09 IST
=> Training   32.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.390 Loss=0.711 Prec@1=82.069 Prec@5=94.917 rate=1.78 Hz, eta=0:15:55, total=0:07:29, wall=08:09 IST
=> Training   36.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.390 Loss=0.711 Prec@1=82.069 Prec@5=94.917 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:09 IST
=> Training   36.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.390 Loss=0.711 Prec@1=82.069 Prec@5=94.917 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:10 IST
=> Training   36.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.389 Loss=0.713 Prec@1=82.018 Prec@5=94.888 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:10 IST
=> Training   39.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.389 Loss=0.713 Prec@1=82.018 Prec@5=94.888 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:10 IST
=> Training   39.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.566 DataTime=0.389 Loss=0.713 Prec@1=82.018 Prec@5=94.888 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:11 IST
=> Training   39.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.387 Loss=0.713 Prec@1=82.016 Prec@5=94.890 rate=1.78 Hz, eta=0:14:02, total=0:09:21, wall=08:11 IST
=> Training   43.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.387 Loss=0.713 Prec@1=82.016 Prec@5=94.890 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=08:11 IST
=> Training   43.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.387 Loss=0.713 Prec@1=82.016 Prec@5=94.890 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=08:12 IST
=> Training   43.99% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.388 Loss=0.713 Prec@1=82.012 Prec@5=94.884 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=08:12 IST
=> Training   47.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.388 Loss=0.713 Prec@1=82.012 Prec@5=94.884 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=08:12 IST
=> Training   47.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.565 DataTime=0.388 Loss=0.713 Prec@1=82.012 Prec@5=94.884 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=08:13 IST
=> Training   47.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.992 Prec@5=94.887 rate=1.78 Hz, eta=0:12:10, total=0:11:13, wall=08:13 IST
=> Training   51.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.992 Prec@5=94.887 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=08:13 IST
=> Training   51.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.992 Prec@5=94.887 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=08:14 IST
=> Training   51.98% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.984 Prec@5=94.883 rate=1.78 Hz, eta=0:11:13, total=0:12:09, wall=08:14 IST
=> Training   55.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.984 Prec@5=94.883 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=08:14 IST
=> Training   55.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.564 DataTime=0.388 Loss=0.714 Prec@1=81.984 Prec@5=94.883 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=08:15 IST
=> Training   55.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.387 Loss=0.715 Prec@1=81.960 Prec@5=94.874 rate=1.78 Hz, eta=0:10:17, total=0:13:05, wall=08:15 IST
=> Training   59.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.387 Loss=0.715 Prec@1=81.960 Prec@5=94.874 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=08:15 IST
=> Training   59.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.387 Loss=0.715 Prec@1=81.960 Prec@5=94.874 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=08:16 IST
=> Training   59.97% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.715 Prec@1=81.944 Prec@5=94.867 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=08:16 IST
=> Training   63.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.715 Prec@1=81.944 Prec@5=94.867 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:16 IST
=> Training   63.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.715 Prec@1=81.944 Prec@5=94.867 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:17 IST
=> Training   63.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.716 Prec@1=81.922 Prec@5=94.856 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=08:17 IST
=> Training   67.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.716 Prec@1=81.922 Prec@5=94.856 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=08:17 IST
=> Training   67.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.716 Prec@1=81.922 Prec@5=94.856 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=08:18 IST
=> Training   67.96% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.389 Loss=0.716 Prec@1=81.922 Prec@5=94.855 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=08:18 IST
=> Training   71.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.389 Loss=0.716 Prec@1=81.922 Prec@5=94.855 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=08:18 IST
=> Training   71.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.563 DataTime=0.389 Loss=0.716 Prec@1=81.922 Prec@5=94.855 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=08:19 IST
=> Training   71.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.716 Prec@1=81.914 Prec@5=94.847 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=08:19 IST
=> Training   75.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.716 Prec@1=81.914 Prec@5=94.847 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=08:19 IST
=> Training   75.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.716 Prec@1=81.914 Prec@5=94.847 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=08:20 IST
=> Training   75.95% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.899 Prec@5=94.839 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=08:20 IST
=> Training   79.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.899 Prec@5=94.839 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=08:20 IST
=> Training   79.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.899 Prec@5=94.839 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=08:20 IST
=> Training   79.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.885 Prec@5=94.830 rate=1.79 Hz, eta=0:04:40, total=0:18:39, wall=08:20 IST
=> Training   83.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.885 Prec@5=94.830 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=08:20 IST
=> Training   83.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.388 Loss=0.717 Prec@1=81.885 Prec@5=94.830 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=08:21 IST
=> Training   83.94% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.890 Prec@5=94.831 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=08:21 IST
=> Training   87.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.890 Prec@5=94.831 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:21 IST
=> Training   87.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.890 Prec@5=94.831 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:22 IST
=> Training   87.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.883 Prec@5=94.830 rate=1.79 Hz, eta=0:02:49, total=0:20:32, wall=08:22 IST
=> Training   91.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.883 Prec@5=94.830 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=08:22 IST
=> Training   91.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.562 DataTime=0.387 Loss=0.717 Prec@1=81.883 Prec@5=94.830 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=08:23 IST
=> Training   91.93% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.717 Prec@1=81.879 Prec@5=94.829 rate=1.79 Hz, eta=0:01:52, total=0:21:27, wall=08:23 IST
=> Training   95.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.717 Prec@1=81.879 Prec@5=94.829 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=08:23 IST
=> Training   95.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.717 Prec@1=81.879 Prec@5=94.829 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=08:24 IST
=> Training   95.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.718 Prec@1=81.877 Prec@5=94.820 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=08:24 IST
=> Training   99.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.718 Prec@1=81.877 Prec@5=94.820 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=08:24 IST
=> Training   99.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.718 Prec@1=81.877 Prec@5=94.820 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=08:24 IST
=> Training   99.92% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.718 Prec@1=81.877 Prec@5=94.820 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=08:24 IST
=> Training   100.00% of 1x2503...Epoch=140/150 LR=0.0013 Time=0.561 DataTime=0.386 Loss=0.718 Prec@1=81.877 Prec@5=94.820 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=08:24 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:24 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:24 IST
=> Validation 0.00% of 1x98...Epoch=140/150 LR=0.0013 Time=7.096 Loss=0.705 Prec@1=80.078 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=08:24 IST
=> Validation 1.02% of 1x98...Epoch=140/150 LR=0.0013 Time=7.096 Loss=0.705 Prec@1=80.078 Prec@5=95.117 rate=6043.39 Hz, eta=0:00:00, total=0:00:00, wall=08:24 IST
** Validation 1.02% of 1x98...Epoch=140/150 LR=0.0013 Time=7.096 Loss=0.705 Prec@1=80.078 Prec@5=95.117 rate=6043.39 Hz, eta=0:00:00, total=0:00:00, wall=08:25 IST
** Validation 1.02% of 1x98...Epoch=140/150 LR=0.0013 Time=0.637 Loss=1.176 Prec@1=71.574 Prec@5=90.182 rate=6043.39 Hz, eta=0:00:00, total=0:00:00, wall=08:25 IST
** Validation 100.00% of 1x98...Epoch=140/150 LR=0.0013 Time=0.637 Loss=1.176 Prec@1=71.574 Prec@5=90.182 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=08:25 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:25 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:25 IST
=> Training   0.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=5.102 DataTime=4.888 Loss=0.656 Prec@1=81.641 Prec@5=96.094 rate=0 Hz, eta=?, total=0:00:00, wall=08:25 IST
=> Training   0.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=5.102 DataTime=4.888 Loss=0.656 Prec@1=81.641 Prec@5=96.094 rate=2091.98 Hz, eta=0:00:01, total=0:00:00, wall=08:25 IST
=> Training   0.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=5.102 DataTime=4.888 Loss=0.656 Prec@1=81.641 Prec@5=96.094 rate=2091.98 Hz, eta=0:00:01, total=0:00:00, wall=08:26 IST
=> Training   0.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.598 DataTime=0.428 Loss=0.722 Prec@1=81.722 Prec@5=94.721 rate=2091.98 Hz, eta=0:00:01, total=0:00:00, wall=08:26 IST
=> Training   4.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.598 DataTime=0.428 Loss=0.722 Prec@1=81.722 Prec@5=94.721 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=08:26 IST
=> Training   4.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.598 DataTime=0.428 Loss=0.722 Prec@1=81.722 Prec@5=94.721 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=08:27 IST
=> Training   4.04% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.589 DataTime=0.415 Loss=0.719 Prec@1=81.842 Prec@5=94.786 rate=1.83 Hz, eta=0:21:55, total=0:00:55, wall=08:27 IST
=> Training   8.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.589 DataTime=0.415 Loss=0.719 Prec@1=81.842 Prec@5=94.786 rate=1.77 Hz, eta=0:21:38, total=0:01:53, wall=08:27 IST
=> Training   8.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.589 DataTime=0.415 Loss=0.719 Prec@1=81.842 Prec@5=94.786 rate=1.77 Hz, eta=0:21:38, total=0:01:53, wall=08:28 IST
=> Training   8.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.574 DataTime=0.399 Loss=0.716 Prec@1=81.877 Prec@5=94.819 rate=1.77 Hz, eta=0:21:38, total=0:01:53, wall=08:28 IST
=> Training   12.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.574 DataTime=0.399 Loss=0.716 Prec@1=81.877 Prec@5=94.819 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=08:28 IST
=> Training   12.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.574 DataTime=0.399 Loss=0.716 Prec@1=81.877 Prec@5=94.819 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=08:29 IST
=> Training   12.03% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.394 Loss=0.714 Prec@1=81.911 Prec@5=94.822 rate=1.80 Hz, eta=0:20:25, total=0:02:47, wall=08:29 IST
=> Training   16.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.394 Loss=0.714 Prec@1=81.911 Prec@5=94.822 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=08:29 IST
=> Training   16.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.394 Loss=0.714 Prec@1=81.911 Prec@5=94.822 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=08:30 IST
=> Training   16.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.395 Loss=0.714 Prec@1=81.927 Prec@5=94.834 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=08:30 IST
=> Training   20.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.395 Loss=0.714 Prec@1=81.927 Prec@5=94.834 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=08:30 IST
=> Training   20.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.569 DataTime=0.395 Loss=0.714 Prec@1=81.927 Prec@5=94.834 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=08:31 IST
=> Training   20.02% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.567 DataTime=0.393 Loss=0.713 Prec@1=81.969 Prec@5=94.846 rate=1.79 Hz, eta=0:18:39, total=0:04:40, wall=08:31 IST
=> Training   24.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.567 DataTime=0.393 Loss=0.713 Prec@1=81.969 Prec@5=94.846 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=08:31 IST
=> Training   24.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.567 DataTime=0.393 Loss=0.713 Prec@1=81.969 Prec@5=94.846 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=08:32 IST
=> Training   24.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.391 Loss=0.712 Prec@1=81.991 Prec@5=94.888 rate=1.79 Hz, eta=0:17:42, total=0:05:35, wall=08:32 IST
=> Training   28.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.391 Loss=0.712 Prec@1=81.991 Prec@5=94.888 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:32 IST
=> Training   28.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.391 Loss=0.712 Prec@1=81.991 Prec@5=94.888 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:33 IST
=> Training   28.01% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.390 Loss=0.713 Prec@1=82.018 Prec@5=94.853 rate=1.79 Hz, eta=0:16:47, total=0:06:31, wall=08:33 IST
=> Training   32.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.390 Loss=0.713 Prec@1=82.018 Prec@5=94.853 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=08:33 IST
=> Training   32.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.390 Loss=0.713 Prec@1=82.018 Prec@5=94.853 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=08:34 IST
=> Training   32.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.392 Loss=0.712 Prec@1=82.026 Prec@5=94.867 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=08:34 IST
=> Training   36.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.392 Loss=0.712 Prec@1=82.026 Prec@5=94.867 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:34 IST
=> Training   36.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.566 DataTime=0.392 Loss=0.712 Prec@1=82.026 Prec@5=94.867 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:35 IST
=> Training   36.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.711 Prec@1=82.022 Prec@5=94.883 rate=1.78 Hz, eta=0:14:58, total=0:08:25, wall=08:35 IST
=> Training   39.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.711 Prec@1=82.022 Prec@5=94.883 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=08:35 IST
=> Training   39.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.711 Prec@1=82.022 Prec@5=94.883 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=08:36 IST
=> Training   39.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.712 Prec@1=81.988 Prec@5=94.876 rate=1.79 Hz, eta=0:13:59, total=0:09:19, wall=08:36 IST
=> Training   43.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.712 Prec@1=81.988 Prec@5=94.876 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=08:36 IST
=> Training   43.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.712 Prec@1=81.988 Prec@5=94.876 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=08:37 IST
=> Training   43.99% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.978 Prec@5=94.867 rate=1.79 Hz, eta=0:13:04, total=0:10:16, wall=08:37 IST
=> Training   47.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.978 Prec@5=94.867 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=08:37 IST
=> Training   47.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.978 Prec@5=94.867 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=08:37 IST
=> Training   47.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.390 Loss=0.713 Prec@1=81.980 Prec@5=94.869 rate=1.79 Hz, eta=0:12:06, total=0:11:10, wall=08:37 IST
=> Training   51.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.390 Loss=0.713 Prec@1=81.980 Prec@5=94.869 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=08:37 IST
=> Training   51.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.390 Loss=0.713 Prec@1=81.980 Prec@5=94.869 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=08:38 IST
=> Training   51.98% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.712 Prec@1=81.982 Prec@5=94.877 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=08:38 IST
=> Training   55.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.712 Prec@1=81.982 Prec@5=94.877 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:38 IST
=> Training   55.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.712 Prec@1=81.982 Prec@5=94.877 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:39 IST
=> Training   55.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.391 Loss=0.713 Prec@1=81.977 Prec@5=94.877 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=08:39 IST
=> Training   59.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.391 Loss=0.713 Prec@1=81.977 Prec@5=94.877 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:39 IST
=> Training   59.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.391 Loss=0.713 Prec@1=81.977 Prec@5=94.877 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:40 IST
=> Training   59.97% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.713 Prec@1=81.961 Prec@5=94.884 rate=1.79 Hz, eta=0:09:21, total=0:14:00, wall=08:40 IST
=> Training   63.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.713 Prec@1=81.961 Prec@5=94.884 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=08:40 IST
=> Training   63.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.392 Loss=0.713 Prec@1=81.961 Prec@5=94.884 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=08:41 IST
=> Training   63.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.965 Prec@5=94.885 rate=1.78 Hz, eta=0:08:25, total=0:14:57, wall=08:41 IST
=> Training   67.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.965 Prec@5=94.885 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=08:41 IST
=> Training   67.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.965 Prec@5=94.885 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=08:42 IST
=> Training   67.96% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.392 Loss=0.713 Prec@1=81.959 Prec@5=94.885 rate=1.78 Hz, eta=0:07:30, total=0:15:54, wall=08:42 IST
=> Training   71.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.392 Loss=0.713 Prec@1=81.959 Prec@5=94.885 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=08:42 IST
=> Training   71.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.565 DataTime=0.392 Loss=0.713 Prec@1=81.959 Prec@5=94.885 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=08:43 IST
=> Training   71.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.949 Prec@5=94.880 rate=1.78 Hz, eta=0:06:34, total=0:16:52, wall=08:43 IST
=> Training   75.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.949 Prec@5=94.880 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=08:43 IST
=> Training   75.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.949 Prec@5=94.880 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=08:44 IST
=> Training   75.95% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.714 Prec@1=81.946 Prec@5=94.881 rate=1.78 Hz, eta=0:05:38, total=0:17:47, wall=08:44 IST
=> Training   79.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.714 Prec@1=81.946 Prec@5=94.881 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=08:44 IST
=> Training   79.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.714 Prec@1=81.946 Prec@5=94.881 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=08:45 IST
=> Training   79.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.964 Prec@5=94.884 rate=1.78 Hz, eta=0:04:41, total=0:18:43, wall=08:45 IST
=> Training   83.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.964 Prec@5=94.884 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=08:45 IST
=> Training   83.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.391 Loss=0.713 Prec@1=81.964 Prec@5=94.884 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=08:46 IST
=> Training   83.94% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.955 Prec@5=94.878 rate=1.78 Hz, eta=0:03:45, total=0:19:40, wall=08:46 IST
=> Training   87.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.955 Prec@5=94.878 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=08:46 IST
=> Training   87.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.955 Prec@5=94.878 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=08:47 IST
=> Training   87.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.964 Prec@5=94.878 rate=1.78 Hz, eta=0:02:49, total=0:20:35, wall=08:47 IST
=> Training   91.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.964 Prec@5=94.878 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=08:47 IST
=> Training   91.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.564 DataTime=0.390 Loss=0.713 Prec@1=81.964 Prec@5=94.878 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=08:48 IST
=> Training   91.93% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.952 Prec@5=94.879 rate=1.78 Hz, eta=0:01:53, total=0:21:32, wall=08:48 IST
=> Training   95.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.952 Prec@5=94.879 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=08:48 IST
=> Training   95.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.563 DataTime=0.389 Loss=0.713 Prec@1=81.952 Prec@5=94.879 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=08:49 IST
=> Training   95.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.562 DataTime=0.388 Loss=0.713 Prec@1=81.939 Prec@5=94.873 rate=1.78 Hz, eta=0:00:57, total=0:22:27, wall=08:49 IST
=> Training   99.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.562 DataTime=0.388 Loss=0.713 Prec@1=81.939 Prec@5=94.873 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=08:49 IST
=> Training   99.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.562 DataTime=0.388 Loss=0.713 Prec@1=81.939 Prec@5=94.873 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=08:49 IST
=> Training   99.92% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.562 DataTime=0.388 Loss=0.713 Prec@1=81.937 Prec@5=94.872 rate=1.78 Hz, eta=0:00:01, total=0:23:21, wall=08:49 IST
=> Training   100.00% of 1x2503...Epoch=141/150 LR=0.0011 Time=0.562 DataTime=0.388 Loss=0.713 Prec@1=81.937 Prec@5=94.872 rate=1.79 Hz, eta=0:00:00, total=0:23:21, wall=08:49 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:49 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=08:49 IST
=> Validation 0.00% of 1x98...Epoch=141/150 LR=0.0011 Time=7.397 Loss=0.689 Prec@1=81.836 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=08:49 IST
=> Validation 1.02% of 1x98...Epoch=141/150 LR=0.0011 Time=7.397 Loss=0.689 Prec@1=81.836 Prec@5=94.922 rate=7534.15 Hz, eta=0:00:00, total=0:00:00, wall=08:49 IST
** Validation 1.02% of 1x98...Epoch=141/150 LR=0.0011 Time=7.397 Loss=0.689 Prec@1=81.836 Prec@5=94.922 rate=7534.15 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST
** Validation 1.02% of 1x98...Epoch=141/150 LR=0.0011 Time=0.640 Loss=1.175 Prec@1=71.804 Prec@5=90.238 rate=7534.15 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST
** Validation 100.00% of 1x98...Epoch=141/150 LR=0.0011 Time=0.640 Loss=1.175 Prec@1=71.804 Prec@5=90.238 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=08:50 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST
=> Training   0.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=4.974 DataTime=4.260 Loss=0.713 Prec@1=81.445 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=08:50 IST
=> Training   0.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=4.974 DataTime=4.260 Loss=0.713 Prec@1=81.445 Prec@5=95.312 rate=3108.43 Hz, eta=0:00:00, total=0:00:00, wall=08:50 IST
=> Training   0.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=4.974 DataTime=4.260 Loss=0.713 Prec@1=81.445 Prec@5=95.312 rate=3108.43 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST
=> Training   0.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.577 DataTime=0.408 Loss=0.703 Prec@1=82.374 Prec@5=94.988 rate=3108.43 Hz, eta=0:00:00, total=0:00:00, wall=08:51 IST
=> Training   4.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.577 DataTime=0.408 Loss=0.703 Prec@1=82.374 Prec@5=94.988 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=08:51 IST
=> Training   4.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.577 DataTime=0.408 Loss=0.703 Prec@1=82.374 Prec@5=94.988 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=08:52 IST
=> Training   4.04% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.575 DataTime=0.407 Loss=0.706 Prec@1=82.222 Prec@5=94.939 rate=1.89 Hz, eta=0:21:09, total=0:00:53, wall=08:52 IST
=> Training   8.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.575 DataTime=0.407 Loss=0.706 Prec@1=82.222 Prec@5=94.939 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=08:52 IST
=> Training   8.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.575 DataTime=0.407 Loss=0.706 Prec@1=82.222 Prec@5=94.939 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=08:53 IST
=> Training   8.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.573 DataTime=0.400 Loss=0.703 Prec@1=82.301 Prec@5=94.993 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=08:53 IST
=> Training   12.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.573 DataTime=0.400 Loss=0.703 Prec@1=82.301 Prec@5=94.993 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=08:53 IST
=> Training   12.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.573 DataTime=0.400 Loss=0.703 Prec@1=82.301 Prec@5=94.993 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=08:54 IST
=> Training   12.03% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.568 DataTime=0.394 Loss=0.705 Prec@1=82.239 Prec@5=94.933 rate=1.80 Hz, eta=0:20:24, total=0:02:47, wall=08:54 IST
=> Training   16.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.568 DataTime=0.394 Loss=0.705 Prec@1=82.239 Prec@5=94.933 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=08:54 IST
=> Training   16.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.568 DataTime=0.394 Loss=0.705 Prec@1=82.239 Prec@5=94.933 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=08:54 IST
=> Training   16.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.393 Loss=0.705 Prec@1=82.231 Prec@5=94.935 rate=1.80 Hz, eta=0:19:27, total=0:03:42, wall=08:54 IST
=> Training   20.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.393 Loss=0.705 Prec@1=82.231 Prec@5=94.935 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:54 IST
=> Training   20.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.393 Loss=0.705 Prec@1=82.231 Prec@5=94.935 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:55 IST
=> Training   20.02% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.392 Loss=0.706 Prec@1=82.185 Prec@5=94.932 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=08:55 IST
=> Training   24.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.392 Loss=0.706 Prec@1=82.185 Prec@5=94.932 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=08:55 IST
=> Training   24.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.565 DataTime=0.392 Loss=0.706 Prec@1=82.185 Prec@5=94.932 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=08:56 IST
=> Training   24.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.706 Prec@1=82.178 Prec@5=94.932 rate=1.80 Hz, eta=0:17:38, total=0:05:34, wall=08:56 IST
=> Training   28.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.706 Prec@1=82.178 Prec@5=94.932 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=08:56 IST
=> Training   28.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.706 Prec@1=82.178 Prec@5=94.932 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=08:57 IST
=> Training   28.01% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.390 Loss=0.706 Prec@1=82.188 Prec@5=94.934 rate=1.80 Hz, eta=0:16:41, total=0:06:29, wall=08:57 IST
=> Training   32.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.390 Loss=0.706 Prec@1=82.188 Prec@5=94.934 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:57 IST
=> Training   32.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.390 Loss=0.706 Prec@1=82.188 Prec@5=94.934 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:58 IST
=> Training   32.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.708 Prec@1=82.154 Prec@5=94.915 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=08:58 IST
=> Training   36.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.708 Prec@1=82.154 Prec@5=94.915 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:58 IST
=> Training   36.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.708 Prec@1=82.154 Prec@5=94.915 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:59 IST
=> Training   36.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.124 Prec@5=94.901 rate=1.79 Hz, eta=0:14:52, total=0:08:22, wall=08:59 IST
=> Training   39.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.124 Prec@5=94.901 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=08:59 IST
=> Training   39.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.124 Prec@5=94.901 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=09:00 IST
=> Training   39.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.388 Loss=0.711 Prec@1=82.089 Prec@5=94.882 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=09:00 IST
=> Training   43.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.388 Loss=0.711 Prec@1=82.089 Prec@5=94.882 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:00 IST
=> Training   43.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.388 Loss=0.711 Prec@1=82.089 Prec@5=94.882 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:01 IST
=> Training   43.99% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.710 Prec@1=82.075 Prec@5=94.893 rate=1.80 Hz, eta=0:13:00, total=0:10:13, wall=09:01 IST
=> Training   47.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.710 Prec@1=82.075 Prec@5=94.893 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=09:01 IST
=> Training   47.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.563 DataTime=0.389 Loss=0.710 Prec@1=82.075 Prec@5=94.893 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=09:02 IST
=> Training   47.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.710 Prec@1=82.065 Prec@5=94.900 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=09:02 IST
=> Training   51.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.710 Prec@1=82.065 Prec@5=94.900 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=09:02 IST
=> Training   51.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.710 Prec@1=82.065 Prec@5=94.900 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=09:03 IST
=> Training   51.98% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.093 Prec@5=94.906 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=09:03 IST
=> Training   55.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.093 Prec@5=94.906 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=09:03 IST
=> Training   55.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.389 Loss=0.709 Prec@1=82.093 Prec@5=94.906 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=09:04 IST
=> Training   55.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.093 Prec@5=94.908 rate=1.79 Hz, eta=0:10:15, total=0:13:02, wall=09:04 IST
=> Training   59.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.093 Prec@5=94.908 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=09:04 IST
=> Training   59.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.093 Prec@5=94.908 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=09:05 IST
=> Training   59.97% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.085 Prec@5=94.904 rate=1.79 Hz, eta=0:09:19, total=0:13:58, wall=09:05 IST
=> Training   63.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.085 Prec@5=94.904 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=09:05 IST
=> Training   63.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.085 Prec@5=94.904 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=09:06 IST
=> Training   63.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.386 Loss=0.709 Prec@1=82.083 Prec@5=94.908 rate=1.79 Hz, eta=0:08:24, total=0:14:54, wall=09:06 IST
=> Training   67.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.386 Loss=0.709 Prec@1=82.083 Prec@5=94.908 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:06 IST
=> Training   67.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.386 Loss=0.709 Prec@1=82.083 Prec@5=94.908 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:07 IST
=> Training   67.96% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.091 Prec@5=94.903 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=09:07 IST
=> Training   71.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.091 Prec@5=94.903 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:07 IST
=> Training   71.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.562 DataTime=0.388 Loss=0.709 Prec@1=82.091 Prec@5=94.903 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:08 IST
=> Training   71.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.094 Prec@5=94.906 rate=1.79 Hz, eta=0:06:32, total=0:16:46, wall=09:08 IST
=> Training   75.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.094 Prec@5=94.906 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:08 IST
=> Training   75.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.094 Prec@5=94.906 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:08 IST
=> Training   75.95% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.096 Prec@5=94.904 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:08 IST
=> Training   79.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.096 Prec@5=94.904 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=09:08 IST
=> Training   79.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.096 Prec@5=94.904 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=09:09 IST
=> Training   79.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.090 Prec@5=94.902 rate=1.79 Hz, eta=0:04:40, total=0:18:37, wall=09:09 IST
=> Training   83.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.090 Prec@5=94.902 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:09 IST
=> Training   83.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.090 Prec@5=94.902 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:10 IST
=> Training   83.94% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.083 Prec@5=94.900 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=09:10 IST
=> Training   87.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.083 Prec@5=94.900 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=09:10 IST
=> Training   87.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.561 DataTime=0.387 Loss=0.709 Prec@1=82.083 Prec@5=94.900 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=09:11 IST
=> Training   87.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.078 Prec@5=94.905 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=09:11 IST
=> Training   91.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.078 Prec@5=94.905 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=09:11 IST
=> Training   91.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.078 Prec@5=94.905 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=09:12 IST
=> Training   91.93% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.075 Prec@5=94.906 rate=1.79 Hz, eta=0:01:52, total=0:21:23, wall=09:12 IST
=> Training   95.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.075 Prec@5=94.906 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=09:12 IST
=> Training   95.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.386 Loss=0.709 Prec@1=82.075 Prec@5=94.906 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=09:13 IST
=> Training   95.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.387 Loss=0.710 Prec@1=82.063 Prec@5=94.903 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=09:13 IST
=> Training   99.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.387 Loss=0.710 Prec@1=82.063 Prec@5=94.903 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=09:13 IST
=> Training   99.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.387 Loss=0.710 Prec@1=82.063 Prec@5=94.903 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=09:13 IST
=> Training   99.92% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.387 Loss=0.710 Prec@1=82.062 Prec@5=94.903 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=09:13 IST
=> Training   100.00% of 1x2503...Epoch=142/150 LR=0.0009 Time=0.560 DataTime=0.387 Loss=0.710 Prec@1=82.062 Prec@5=94.903 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=09:13 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> Validation 0.00% of 1x98...Epoch=142/150 LR=0.0009 Time=6.706 Loss=0.703 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=09:13 IST
=> Validation 1.02% of 1x98...Epoch=142/150 LR=0.0009 Time=6.706 Loss=0.703 Prec@1=81.055 Prec@5=95.312 rate=4252.22 Hz, eta=0:00:00, total=0:00:00, wall=09:13 IST
** Validation 1.02% of 1x98...Epoch=142/150 LR=0.0009 Time=6.706 Loss=0.703 Prec@1=81.055 Prec@5=95.312 rate=4252.22 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST
** Validation 1.02% of 1x98...Epoch=142/150 LR=0.0009 Time=0.632 Loss=1.174 Prec@1=71.660 Prec@5=90.306 rate=4252.22 Hz, eta=0:00:00, total=0:00:00, wall=09:14 IST
** Validation 100.00% of 1x98...Epoch=142/150 LR=0.0009 Time=0.632 Loss=1.174 Prec@1=71.660 Prec@5=90.306 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=09:14 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> Training   0.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=5.031 DataTime=4.812 Loss=0.583 Prec@1=85.352 Prec@5=96.680 rate=0 Hz, eta=?, total=0:00:00, wall=09:14 IST
=> Training   0.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=5.031 DataTime=4.812 Loss=0.583 Prec@1=85.352 Prec@5=96.680 rate=1536.45 Hz, eta=0:00:01, total=0:00:00, wall=09:14 IST
=> Training   0.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=5.031 DataTime=4.812 Loss=0.583 Prec@1=85.352 Prec@5=96.680 rate=1536.45 Hz, eta=0:00:01, total=0:00:00, wall=09:15 IST
=> Training   0.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.576 DataTime=0.412 Loss=0.707 Prec@1=82.000 Prec@5=95.021 rate=1536.45 Hz, eta=0:00:01, total=0:00:00, wall=09:15 IST
=> Training   4.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.576 DataTime=0.412 Loss=0.707 Prec@1=82.000 Prec@5=95.021 rate=1.90 Hz, eta=0:21:04, total=0:00:53, wall=09:15 IST
=> Training   4.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.576 DataTime=0.412 Loss=0.707 Prec@1=82.000 Prec@5=95.021 rate=1.90 Hz, eta=0:21:04, total=0:00:53, wall=09:16 IST
=> Training   4.04% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.569 DataTime=0.399 Loss=0.709 Prec@1=82.052 Prec@5=94.976 rate=1.90 Hz, eta=0:21:04, total=0:00:53, wall=09:16 IST
=> Training   8.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.569 DataTime=0.399 Loss=0.709 Prec@1=82.052 Prec@5=94.976 rate=1.84 Hz, eta=0:20:53, total=0:01:49, wall=09:16 IST
=> Training   8.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.569 DataTime=0.399 Loss=0.709 Prec@1=82.052 Prec@5=94.976 rate=1.84 Hz, eta=0:20:53, total=0:01:49, wall=09:17 IST
=> Training   8.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.571 DataTime=0.398 Loss=0.709 Prec@1=82.123 Prec@5=94.911 rate=1.84 Hz, eta=0:20:53, total=0:01:49, wall=09:17 IST
=> Training   12.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.571 DataTime=0.398 Loss=0.709 Prec@1=82.123 Prec@5=94.911 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=09:17 IST
=> Training   12.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.571 DataTime=0.398 Loss=0.709 Prec@1=82.123 Prec@5=94.911 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=09:18 IST
=> Training   12.03% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.073 Prec@5=94.908 rate=1.80 Hz, eta=0:20:21, total=0:02:46, wall=09:18 IST
=> Training   16.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.073 Prec@5=94.908 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=09:18 IST
=> Training   16.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.710 Prec@1=82.073 Prec@5=94.908 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=09:19 IST
=> Training   16.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.566 DataTime=0.391 Loss=0.707 Prec@1=82.172 Prec@5=94.937 rate=1.82 Hz, eta=0:19:14, total=0:03:40, wall=09:19 IST
=> Training   20.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.566 DataTime=0.391 Loss=0.707 Prec@1=82.172 Prec@5=94.937 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=09:19 IST
=> Training   20.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.566 DataTime=0.391 Loss=0.707 Prec@1=82.172 Prec@5=94.937 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=09:20 IST
=> Training   20.02% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.390 Loss=0.707 Prec@1=82.177 Prec@5=94.952 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=09:20 IST
=> Training   24.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.390 Loss=0.707 Prec@1=82.177 Prec@5=94.952 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=09:20 IST
=> Training   24.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.390 Loss=0.707 Prec@1=82.177 Prec@5=94.952 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=09:21 IST
=> Training   24.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.389 Loss=0.707 Prec@1=82.168 Prec@5=94.952 rate=1.80 Hz, eta=0:17:36, total=0:05:33, wall=09:21 IST
=> Training   28.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.389 Loss=0.707 Prec@1=82.168 Prec@5=94.952 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=09:21 IST
=> Training   28.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.564 DataTime=0.389 Loss=0.707 Prec@1=82.168 Prec@5=94.952 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=09:22 IST
=> Training   28.01% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.387 Loss=0.705 Prec@1=82.192 Prec@5=94.977 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=09:22 IST
=> Training   32.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.387 Loss=0.705 Prec@1=82.192 Prec@5=94.977 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=09:22 IST
=> Training   32.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.387 Loss=0.705 Prec@1=82.192 Prec@5=94.977 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=09:23 IST
=> Training   32.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.181 Prec@5=94.956 rate=1.80 Hz, eta=0:15:46, total=0:07:25, wall=09:23 IST
=> Training   36.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.181 Prec@5=94.956 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=09:23 IST
=> Training   36.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.181 Prec@5=94.956 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=09:24 IST
=> Training   36.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.386 Loss=0.707 Prec@1=82.160 Prec@5=94.945 rate=1.79 Hz, eta=0:14:53, total=0:08:22, wall=09:24 IST
=> Training   39.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.386 Loss=0.707 Prec@1=82.160 Prec@5=94.945 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:24 IST
=> Training   39.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.386 Loss=0.707 Prec@1=82.160 Prec@5=94.945 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:24 IST
=> Training   39.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.202 Prec@5=94.954 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:24 IST
=> Training   43.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.202 Prec@5=94.954 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=09:24 IST
=> Training   43.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.563 DataTime=0.388 Loss=0.706 Prec@1=82.202 Prec@5=94.954 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=09:25 IST
=> Training   43.99% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.386 Loss=0.705 Prec@1=82.222 Prec@5=94.968 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=09:25 IST
=> Training   47.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.386 Loss=0.705 Prec@1=82.222 Prec@5=94.968 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:25 IST
=> Training   47.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.386 Loss=0.705 Prec@1=82.222 Prec@5=94.968 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:26 IST
=> Training   47.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.705 Prec@1=82.204 Prec@5=94.966 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=09:26 IST
=> Training   51.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.705 Prec@1=82.204 Prec@5=94.966 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=09:26 IST
=> Training   51.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.562 DataTime=0.388 Loss=0.705 Prec@1=82.204 Prec@5=94.966 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=09:27 IST
=> Training   51.98% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.969 rate=1.79 Hz, eta=0:11:10, total=0:12:06, wall=09:27 IST
=> Training   55.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.969 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=09:27 IST
=> Training   55.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.561 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.969 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=09:28 IST
=> Training   55.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.965 rate=1.79 Hz, eta=0:10:14, total=0:13:00, wall=09:28 IST
=> Training   59.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.965 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=09:28 IST
=> Training   59.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.387 Loss=0.705 Prec@1=82.210 Prec@5=94.965 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=09:29 IST
=> Training   59.97% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.80 Hz, eta=0:09:18, total=0:13:56, wall=09:29 IST
=> Training   63.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=09:29 IST
=> Training   63.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=09:30 IST
=> Training   63.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.195 Prec@5=94.955 rate=1.80 Hz, eta=0:08:21, total=0:14:50, wall=09:30 IST
=> Training   67.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.195 Prec@5=94.955 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=09:30 IST
=> Training   67.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.195 Prec@5=94.955 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=09:31 IST
=> Training   67.96% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.196 Prec@5=94.956 rate=1.80 Hz, eta=0:07:26, total=0:15:46, wall=09:31 IST
=> Training   71.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.196 Prec@5=94.956 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=09:31 IST
=> Training   71.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.196 Prec@5=94.956 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=09:32 IST
=> Training   71.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.203 Prec@5=94.959 rate=1.80 Hz, eta=0:06:31, total=0:16:43, wall=09:32 IST
=> Training   75.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.203 Prec@5=94.959 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=09:32 IST
=> Training   75.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.705 Prec@1=82.203 Prec@5=94.959 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=09:33 IST
=> Training   75.95% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=09:33 IST
=> Training   79.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=09:33 IST
=> Training   79.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.190 Prec@5=94.955 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=09:34 IST
=> Training   79.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.386 Loss=0.705 Prec@1=82.197 Prec@5=94.955 rate=1.80 Hz, eta=0:04:39, total=0:18:34, wall=09:34 IST
=> Training   83.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.386 Loss=0.705 Prec@1=82.197 Prec@5=94.955 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=09:34 IST
=> Training   83.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.386 Loss=0.705 Prec@1=82.197 Prec@5=94.955 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=09:35 IST
=> Training   83.94% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.180 Prec@5=94.949 rate=1.80 Hz, eta=0:03:43, total=0:19:30, wall=09:35 IST
=> Training   87.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.180 Prec@5=94.949 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=09:35 IST
=> Training   87.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.560 DataTime=0.386 Loss=0.706 Prec@1=82.180 Prec@5=94.949 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=09:36 IST
=> Training   87.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.706 Prec@1=82.176 Prec@5=94.952 rate=1.79 Hz, eta=0:02:48, total=0:20:27, wall=09:36 IST
=> Training   91.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.706 Prec@1=82.176 Prec@5=94.952 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=09:36 IST
=> Training   91.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.706 Prec@1=82.176 Prec@5=94.952 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=09:37 IST
=> Training   91.93% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.705 Prec@1=82.181 Prec@5=94.954 rate=1.80 Hz, eta=0:01:52, total=0:21:21, wall=09:37 IST
=> Training   95.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.705 Prec@1=82.181 Prec@5=94.954 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=09:37 IST
=> Training   95.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.385 Loss=0.705 Prec@1=82.181 Prec@5=94.954 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=09:37 IST
=> Training   95.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.384 Loss=0.706 Prec@1=82.183 Prec@5=94.951 rate=1.80 Hz, eta=0:00:56, total=0:22:16, wall=09:37 IST
=> Training   99.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.384 Loss=0.706 Prec@1=82.183 Prec@5=94.951 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=09:37 IST
=> Training   99.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.559 DataTime=0.384 Loss=0.706 Prec@1=82.183 Prec@5=94.951 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=09:37 IST
=> Training   99.92% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.558 DataTime=0.384 Loss=0.706 Prec@1=82.183 Prec@5=94.951 rate=1.80 Hz, eta=0:00:01, total=0:23:12, wall=09:37 IST
=> Training   100.00% of 1x2503...Epoch=143/150 LR=0.0007 Time=0.558 DataTime=0.384 Loss=0.706 Prec@1=82.183 Prec@5=94.951 rate=1.80 Hz, eta=0:00:00, total=0:23:12, wall=09:37 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:38 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=09:38 IST
=> Validation 0.00% of 1x98...Epoch=143/150 LR=0.0007 Time=6.807 Loss=0.697 Prec@1=81.641 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=09:38 IST
=> Validation 1.02% of 1x98...Epoch=143/150 LR=0.0007 Time=6.807 Loss=0.697 Prec@1=81.641 Prec@5=95.312 rate=693.65 Hz, eta=0:00:00, total=0:00:00, wall=09:38 IST
** Validation 1.02% of 1x98...Epoch=143/150 LR=0.0007 Time=6.807 Loss=0.697 Prec@1=81.641 Prec@5=95.312 rate=693.65 Hz, eta=0:00:00, total=0:00:00, wall=09:38 IST
** Validation 1.02% of 1x98...Epoch=143/150 LR=0.0007 Time=0.629 Loss=1.171 Prec@1=71.722 Prec@5=90.256 rate=693.65 Hz, eta=0:00:00, total=0:00:00, wall=09:38 IST
** Validation 100.00% of 1x98...Epoch=143/150 LR=0.0007 Time=0.629 Loss=1.171 Prec@1=71.722 Prec@5=90.256 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=09:38 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:39 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=09:39 IST
=> Training   0.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=4.865 DataTime=4.580 Loss=0.694 Prec@1=82.812 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=09:39 IST
=> Training   0.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=4.865 DataTime=4.580 Loss=0.694 Prec@1=82.812 Prec@5=95.117 rate=1822.06 Hz, eta=0:00:01, total=0:00:00, wall=09:39 IST
=> Training   0.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=4.865 DataTime=4.580 Loss=0.694 Prec@1=82.812 Prec@5=95.117 rate=1822.06 Hz, eta=0:00:01, total=0:00:00, wall=09:40 IST
=> Training   0.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.592 DataTime=0.426 Loss=0.697 Prec@1=82.283 Prec@5=95.133 rate=1822.06 Hz, eta=0:00:01, total=0:00:00, wall=09:40 IST
=> Training   4.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.592 DataTime=0.426 Loss=0.697 Prec@1=82.283 Prec@5=95.133 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=09:40 IST
=> Training   4.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.592 DataTime=0.426 Loss=0.697 Prec@1=82.283 Prec@5=95.133 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=09:40 IST
=> Training   4.04% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.579 DataTime=0.407 Loss=0.701 Prec@1=82.269 Prec@5=95.096 rate=1.84 Hz, eta=0:21:46, total=0:00:54, wall=09:40 IST
=> Training   8.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.579 DataTime=0.407 Loss=0.701 Prec@1=82.269 Prec@5=95.096 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=09:40 IST
=> Training   8.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.579 DataTime=0.407 Loss=0.701 Prec@1=82.269 Prec@5=95.096 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=09:41 IST
=> Training   8.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.400 Loss=0.703 Prec@1=82.253 Prec@5=95.028 rate=1.80 Hz, eta=0:21:17, total=0:01:51, wall=09:41 IST
=> Training   12.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.400 Loss=0.703 Prec@1=82.253 Prec@5=95.028 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=09:41 IST
=> Training   12.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.400 Loss=0.703 Prec@1=82.253 Prec@5=95.028 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=09:42 IST
=> Training   12.03% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.399 Loss=0.699 Prec@1=82.352 Prec@5=95.078 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=09:42 IST
=> Training   16.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.399 Loss=0.699 Prec@1=82.352 Prec@5=95.078 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=09:42 IST
=> Training   16.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.570 DataTime=0.399 Loss=0.699 Prec@1=82.352 Prec@5=95.078 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=09:43 IST
=> Training   16.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.568 DataTime=0.396 Loss=0.699 Prec@1=82.377 Prec@5=95.068 rate=1.79 Hz, eta=0:19:32, total=0:03:43, wall=09:43 IST
=> Training   20.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.568 DataTime=0.396 Loss=0.699 Prec@1=82.377 Prec@5=95.068 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=09:43 IST
=> Training   20.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.568 DataTime=0.396 Loss=0.699 Prec@1=82.377 Prec@5=95.068 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=09:44 IST
=> Training   20.02% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.564 DataTime=0.390 Loss=0.699 Prec@1=82.348 Prec@5=95.041 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=09:44 IST
=> Training   24.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.564 DataTime=0.390 Loss=0.699 Prec@1=82.348 Prec@5=95.041 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=09:44 IST
=> Training   24.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.564 DataTime=0.390 Loss=0.699 Prec@1=82.348 Prec@5=95.041 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=09:45 IST
=> Training   24.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.700 Prec@1=82.327 Prec@5=95.020 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=09:45 IST
=> Training   28.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.700 Prec@1=82.327 Prec@5=95.020 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:45 IST
=> Training   28.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.700 Prec@1=82.327 Prec@5=95.020 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:46 IST
=> Training   28.01% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.701 Prec@1=82.348 Prec@5=95.003 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=09:46 IST
=> Training   32.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.701 Prec@1=82.348 Prec@5=95.003 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:46 IST
=> Training   32.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.701 Prec@1=82.348 Prec@5=95.003 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:47 IST
=> Training   32.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.385 Loss=0.701 Prec@1=82.340 Prec@5=94.990 rate=1.80 Hz, eta=0:15:45, total=0:07:24, wall=09:47 IST
=> Training   36.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.385 Loss=0.701 Prec@1=82.340 Prec@5=94.990 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=09:47 IST
=> Training   36.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.385 Loss=0.701 Prec@1=82.340 Prec@5=94.990 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=09:48 IST
=> Training   36.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.701 Prec@1=82.339 Prec@5=94.986 rate=1.80 Hz, eta=0:14:49, total=0:08:20, wall=09:48 IST
=> Training   39.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.701 Prec@1=82.339 Prec@5=94.986 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:48 IST
=> Training   39.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.701 Prec@1=82.339 Prec@5=94.986 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:49 IST
=> Training   39.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.386 Loss=0.702 Prec@1=82.321 Prec@5=94.980 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=09:49 IST
=> Training   43.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.386 Loss=0.702 Prec@1=82.321 Prec@5=94.980 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:49 IST
=> Training   43.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.386 Loss=0.702 Prec@1=82.321 Prec@5=94.980 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:50 IST
=> Training   43.99% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.703 Prec@1=82.301 Prec@5=94.986 rate=1.80 Hz, eta=0:12:58, total=0:10:11, wall=09:50 IST
=> Training   47.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.703 Prec@1=82.301 Prec@5=94.986 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=09:50 IST
=> Training   47.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.562 DataTime=0.387 Loss=0.703 Prec@1=82.301 Prec@5=94.986 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=09:51 IST
=> Training   47.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.305 Prec@5=94.980 rate=1.79 Hz, eta=0:12:06, total=0:11:09, wall=09:51 IST
=> Training   51.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.305 Prec@5=94.980 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:51 IST
=> Training   51.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.305 Prec@5=94.980 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:52 IST
=> Training   51.98% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.702 Prec@1=82.308 Prec@5=94.993 rate=1.79 Hz, eta=0:11:09, total=0:12:05, wall=09:52 IST
=> Training   55.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.702 Prec@1=82.308 Prec@5=94.993 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:52 IST
=> Training   55.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.702 Prec@1=82.308 Prec@5=94.993 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:53 IST
=> Training   55.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.293 Prec@5=94.988 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=09:53 IST
=> Training   59.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.293 Prec@5=94.988 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=09:53 IST
=> Training   59.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.293 Prec@5=94.988 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=09:53 IST
=> Training   59.97% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.283 Prec@5=94.985 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=09:53 IST
=> Training   63.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.283 Prec@5=94.985 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=09:53 IST
=> Training   63.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.283 Prec@5=94.985 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=09:54 IST
=> Training   63.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.300 Prec@5=94.989 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=09:54 IST
=> Training   67.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.300 Prec@5=94.989 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:54 IST
=> Training   67.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.300 Prec@5=94.989 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:55 IST
=> Training   67.96% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.287 Prec@5=94.986 rate=1.79 Hz, eta=0:07:27, total=0:15:49, wall=09:55 IST
=> Training   71.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.287 Prec@5=94.986 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=09:55 IST
=> Training   71.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.703 Prec@1=82.287 Prec@5=94.986 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=09:56 IST
=> Training   71.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.277 Prec@5=94.977 rate=1.79 Hz, eta=0:06:31, total=0:16:45, wall=09:56 IST
=> Training   75.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.277 Prec@5=94.977 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:56 IST
=> Training   75.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.277 Prec@5=94.977 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:57 IST
=> Training   75.95% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.261 Prec@5=94.979 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=09:57 IST
=> Training   79.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.261 Prec@5=94.979 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:57 IST
=> Training   79.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.387 Loss=0.704 Prec@1=82.261 Prec@5=94.979 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:58 IST
=> Training   79.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.258 Prec@5=94.977 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=09:58 IST
=> Training   83.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.258 Prec@5=94.977 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=09:58 IST
=> Training   83.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.258 Prec@5=94.977 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=09:59 IST
=> Training   83.94% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.266 Prec@5=94.980 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=09:59 IST
=> Training   87.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.266 Prec@5=94.980 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=09:59 IST
=> Training   87.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.386 Loss=0.704 Prec@1=82.266 Prec@5=94.980 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=10:00 IST
=> Training   87.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.385 Loss=0.703 Prec@1=82.266 Prec@5=94.988 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=10:00 IST
=> Training   91.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.385 Loss=0.703 Prec@1=82.266 Prec@5=94.988 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=10:00 IST
=> Training   91.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.561 DataTime=0.385 Loss=0.703 Prec@1=82.266 Prec@5=94.988 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=10:01 IST
=> Training   91.93% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.277 Prec@5=94.990 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=10:01 IST
=> Training   95.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.277 Prec@5=94.990 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=10:01 IST
=> Training   95.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.277 Prec@5=94.990 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=10:02 IST
=> Training   95.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.264 Prec@5=94.986 rate=1.79 Hz, eta=0:00:56, total=0:22:20, wall=10:02 IST
=> Training   99.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.264 Prec@5=94.986 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=10:02 IST
=> Training   99.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.264 Prec@5=94.986 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=10:02 IST
=> Training   99.92% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.263 Prec@5=94.985 rate=1.79 Hz, eta=0:00:01, total=0:23:15, wall=10:02 IST
=> Training   100.00% of 1x2503...Epoch=144/150 LR=0.0005 Time=0.560 DataTime=0.384 Loss=0.703 Prec@1=82.263 Prec@5=94.985 rate=1.79 Hz, eta=0:00:00, total=0:23:16, wall=10:02 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST
=> Validation 0.00% of 1x98...Epoch=144/150 LR=0.0005 Time=7.337 Loss=0.695 Prec@1=80.859 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=10:02 IST
=> Validation 1.02% of 1x98...Epoch=144/150 LR=0.0005 Time=7.337 Loss=0.695 Prec@1=80.859 Prec@5=94.922 rate=6010.05 Hz, eta=0:00:00, total=0:00:00, wall=10:02 IST
** Validation 1.02% of 1x98...Epoch=144/150 LR=0.0005 Time=7.337 Loss=0.695 Prec@1=80.859 Prec@5=94.922 rate=6010.05 Hz, eta=0:00:00, total=0:00:00, wall=10:03 IST
** Validation 1.02% of 1x98...Epoch=144/150 LR=0.0005 Time=0.635 Loss=1.170 Prec@1=71.706 Prec@5=90.322 rate=6010.05 Hz, eta=0:00:00, total=0:00:00, wall=10:03 IST
** Validation 100.00% of 1x98...Epoch=144/150 LR=0.0005 Time=0.635 Loss=1.170 Prec@1=71.706 Prec@5=90.322 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=10:03 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:03 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:03 IST
=> Training   0.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=5.669 DataTime=5.448 Loss=0.659 Prec@1=82.812 Prec@5=95.508 rate=0 Hz, eta=?, total=0:00:00, wall=10:03 IST
=> Training   0.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=5.669 DataTime=5.448 Loss=0.659 Prec@1=82.812 Prec@5=95.508 rate=8344.11 Hz, eta=0:00:00, total=0:00:00, wall=10:03 IST
=> Training   0.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=5.669 DataTime=5.448 Loss=0.659 Prec@1=82.812 Prec@5=95.508 rate=8344.11 Hz, eta=0:00:00, total=0:00:00, wall=10:04 IST
=> Training   0.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.600 DataTime=0.429 Loss=0.696 Prec@1=82.443 Prec@5=95.036 rate=8344.11 Hz, eta=0:00:00, total=0:00:00, wall=10:04 IST
=> Training   4.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.600 DataTime=0.429 Loss=0.696 Prec@1=82.443 Prec@5=95.036 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=10:04 IST
=> Training   4.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.600 DataTime=0.429 Loss=0.696 Prec@1=82.443 Prec@5=95.036 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=10:05 IST
=> Training   4.04% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.577 DataTime=0.406 Loss=0.700 Prec@1=82.300 Prec@5=94.972 rate=1.84 Hz, eta=0:21:45, total=0:00:54, wall=10:05 IST
=> Training   8.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.577 DataTime=0.406 Loss=0.700 Prec@1=82.300 Prec@5=94.972 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=10:05 IST
=> Training   8.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.577 DataTime=0.406 Loss=0.700 Prec@1=82.300 Prec@5=94.972 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=10:06 IST
=> Training   8.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.570 DataTime=0.398 Loss=0.700 Prec@1=82.344 Prec@5=94.993 rate=1.82 Hz, eta=0:21:02, total=0:01:50, wall=10:06 IST
=> Training   12.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.570 DataTime=0.398 Loss=0.700 Prec@1=82.344 Prec@5=94.993 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=10:06 IST
=> Training   12.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.570 DataTime=0.398 Loss=0.700 Prec@1=82.344 Prec@5=94.993 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=10:07 IST
=> Training   12.03% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.395 Loss=0.700 Prec@1=82.347 Prec@5=95.009 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=10:07 IST
=> Training   16.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.395 Loss=0.700 Prec@1=82.347 Prec@5=95.009 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:07 IST
=> Training   16.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.395 Loss=0.700 Prec@1=82.347 Prec@5=95.009 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:08 IST
=> Training   16.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.396 Loss=0.699 Prec@1=82.359 Prec@5=95.015 rate=1.81 Hz, eta=0:19:21, total=0:03:41, wall=10:08 IST
=> Training   20.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.396 Loss=0.699 Prec@1=82.359 Prec@5=95.015 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:08 IST
=> Training   20.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.567 DataTime=0.396 Loss=0.699 Prec@1=82.359 Prec@5=95.015 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:09 IST
=> Training   20.02% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.395 Loss=0.699 Prec@1=82.375 Prec@5=95.022 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:09 IST
=> Training   24.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.395 Loss=0.699 Prec@1=82.375 Prec@5=95.022 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=10:09 IST
=> Training   24.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.395 Loss=0.699 Prec@1=82.375 Prec@5=95.022 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=10:10 IST
=> Training   24.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.698 Prec@1=82.408 Prec@5=95.037 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=10:10 IST
=> Training   28.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.698 Prec@1=82.408 Prec@5=95.037 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:10 IST
=> Training   28.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.698 Prec@1=82.408 Prec@5=95.037 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:10 IST
=> Training   28.01% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.394 Loss=0.699 Prec@1=82.391 Prec@5=95.023 rate=1.80 Hz, eta=0:16:43, total=0:06:30, wall=10:10 IST
=> Training   32.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.394 Loss=0.699 Prec@1=82.391 Prec@5=95.023 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=10:10 IST
=> Training   32.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.394 Loss=0.699 Prec@1=82.391 Prec@5=95.023 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=10:11 IST
=> Training   32.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.357 Prec@5=95.030 rate=1.79 Hz, eta=0:15:48, total=0:07:26, wall=10:11 IST
=> Training   36.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.357 Prec@5=95.030 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=10:11 IST
=> Training   36.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.357 Prec@5=95.030 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=10:12 IST
=> Training   36.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.376 Prec@5=95.034 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=10:12 IST
=> Training   39.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.376 Prec@5=95.034 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=10:12 IST
=> Training   39.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.394 Loss=0.699 Prec@1=82.376 Prec@5=95.034 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=10:13 IST
=> Training   39.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.393 Loss=0.699 Prec@1=82.384 Prec@5=95.036 rate=1.79 Hz, eta=0:14:00, total=0:09:20, wall=10:13 IST
=> Training   43.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.393 Loss=0.699 Prec@1=82.384 Prec@5=95.036 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=10:13 IST
=> Training   43.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.565 DataTime=0.393 Loss=0.699 Prec@1=82.384 Prec@5=95.036 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=10:14 IST
=> Training   43.99% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.392 Prec@5=95.032 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=10:14 IST
=> Training   47.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.392 Prec@5=95.032 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=10:14 IST
=> Training   47.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.392 Prec@5=95.032 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=10:15 IST
=> Training   47.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.394 Prec@5=95.039 rate=1.79 Hz, eta=0:12:08, total=0:11:11, wall=10:15 IST
=> Training   51.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.394 Prec@5=95.039 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=10:15 IST
=> Training   51.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.394 Prec@5=95.039 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=10:16 IST
=> Training   51.98% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.373 Prec@5=95.034 rate=1.79 Hz, eta=0:11:11, total=0:12:07, wall=10:16 IST
=> Training   55.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.373 Prec@5=95.034 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=10:16 IST
=> Training   55.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.373 Prec@5=95.034 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=10:17 IST
=> Training   55.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.400 Prec@5=95.044 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=10:17 IST
=> Training   59.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.400 Prec@5=95.044 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=10:17 IST
=> Training   59.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.564 DataTime=0.392 Loss=0.699 Prec@1=82.400 Prec@5=95.044 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=10:18 IST
=> Training   59.97% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.384 Prec@5=95.035 rate=1.78 Hz, eta=0:09:21, total=0:14:01, wall=10:18 IST
=> Training   63.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.384 Prec@5=95.035 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=10:18 IST
=> Training   63.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.384 Prec@5=95.035 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=10:19 IST
=> Training   63.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.368 Prec@5=95.036 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=10:19 IST
=> Training   67.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.368 Prec@5=95.036 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:19 IST
=> Training   67.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.368 Prec@5=95.036 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:20 IST
=> Training   67.96% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.376 Prec@5=95.038 rate=1.79 Hz, eta=0:07:29, total=0:15:52, wall=10:20 IST
=> Training   71.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.376 Prec@5=95.038 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=10:20 IST
=> Training   71.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.563 DataTime=0.391 Loss=0.699 Prec@1=82.376 Prec@5=95.038 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=10:21 IST
=> Training   71.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.700 Prec@1=82.358 Prec@5=95.026 rate=1.78 Hz, eta=0:06:33, total=0:16:49, wall=10:21 IST
=> Training   75.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.700 Prec@1=82.358 Prec@5=95.026 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=10:21 IST
=> Training   75.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.700 Prec@1=82.358 Prec@5=95.026 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=10:22 IST
=> Training   75.95% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.357 Prec@5=95.022 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=10:22 IST
=> Training   79.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.357 Prec@5=95.022 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:22 IST
=> Training   79.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.357 Prec@5=95.022 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:23 IST
=> Training   79.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.699 Prec@1=82.371 Prec@5=95.023 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=10:23 IST
=> Training   83.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.699 Prec@1=82.371 Prec@5=95.023 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=10:23 IST
=> Training   83.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.699 Prec@1=82.371 Prec@5=95.023 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=10:24 IST
=> Training   83.94% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.699 Prec@1=82.365 Prec@5=95.020 rate=1.79 Hz, eta=0:03:44, total=0:19:34, wall=10:24 IST
=> Training   87.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.699 Prec@1=82.365 Prec@5=95.020 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=10:24 IST
=> Training   87.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.390 Loss=0.699 Prec@1=82.365 Prec@5=95.020 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=10:24 IST
=> Training   87.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.358 Prec@5=95.018 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=10:24 IST
=> Training   91.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.358 Prec@5=95.018 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:24 IST
=> Training   91.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.358 Prec@5=95.018 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:25 IST
=> Training   91.93% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.349 Prec@5=95.014 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=10:25 IST
=> Training   95.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.349 Prec@5=95.014 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=10:25 IST
=> Training   95.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.562 DataTime=0.389 Loss=0.700 Prec@1=82.349 Prec@5=95.014 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=10:26 IST
=> Training   95.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.346 Prec@5=95.013 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=10:26 IST
=> Training   99.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.346 Prec@5=95.013 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=10:26 IST
=> Training   99.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.346 Prec@5=95.013 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=10:26 IST
=> Training   99.92% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.344 Prec@5=95.012 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=10:26 IST
=> Training   100.00% of 1x2503...Epoch=145/150 LR=0.0004 Time=0.561 DataTime=0.389 Loss=0.700 Prec@1=82.344 Prec@5=95.012 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=10:26 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:26 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:26 IST
=> Validation 0.00% of 1x98...Epoch=145/150 LR=0.0004 Time=6.846 Loss=0.708 Prec@1=80.664 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=10:26 IST
=> Validation 1.02% of 1x98...Epoch=145/150 LR=0.0004 Time=6.846 Loss=0.708 Prec@1=80.664 Prec@5=94.922 rate=3406.03 Hz, eta=0:00:00, total=0:00:00, wall=10:26 IST
** Validation 1.02% of 1x98...Epoch=145/150 LR=0.0004 Time=6.846 Loss=0.708 Prec@1=80.664 Prec@5=94.922 rate=3406.03 Hz, eta=0:00:00, total=0:00:00, wall=10:27 IST
** Validation 1.02% of 1x98...Epoch=145/150 LR=0.0004 Time=0.630 Loss=1.170 Prec@1=71.780 Prec@5=90.328 rate=3406.03 Hz, eta=0:00:00, total=0:00:00, wall=10:27 IST
** Validation 100.00% of 1x98...Epoch=145/150 LR=0.0004 Time=0.630 Loss=1.170 Prec@1=71.780 Prec@5=90.328 rate=1.79 Hz, eta=0:00:00, total=0:00:54, wall=10:27 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST
=> Training   0.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=4.574 DataTime=4.133 Loss=0.762 Prec@1=82.617 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=10:27 IST
=> Training   0.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=4.574 DataTime=4.133 Loss=0.762 Prec@1=82.617 Prec@5=95.312 rate=936.49 Hz, eta=0:00:02, total=0:00:00, wall=10:27 IST
=> Training   0.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=4.574 DataTime=4.133 Loss=0.762 Prec@1=82.617 Prec@5=95.312 rate=936.49 Hz, eta=0:00:02, total=0:00:00, wall=10:28 IST
=> Training   0.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.587 DataTime=0.419 Loss=0.692 Prec@1=82.766 Prec@5=95.094 rate=936.49 Hz, eta=0:00:02, total=0:00:00, wall=10:28 IST
=> Training   4.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.587 DataTime=0.419 Loss=0.692 Prec@1=82.766 Prec@5=95.094 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=10:28 IST
=> Training   4.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.587 DataTime=0.419 Loss=0.692 Prec@1=82.766 Prec@5=95.094 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=10:29 IST
=> Training   4.04% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.573 DataTime=0.404 Loss=0.692 Prec@1=82.617 Prec@5=95.068 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=10:29 IST
=> Training   8.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.573 DataTime=0.404 Loss=0.692 Prec@1=82.617 Prec@5=95.068 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=10:29 IST
=> Training   8.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.573 DataTime=0.404 Loss=0.692 Prec@1=82.617 Prec@5=95.068 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=10:30 IST
=> Training   8.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.566 DataTime=0.395 Loss=0.696 Prec@1=82.535 Prec@5=94.998 rate=1.82 Hz, eta=0:21:06, total=0:01:50, wall=10:30 IST
=> Training   12.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.566 DataTime=0.395 Loss=0.696 Prec@1=82.535 Prec@5=94.998 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=10:30 IST
=> Training   12.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.566 DataTime=0.395 Loss=0.696 Prec@1=82.535 Prec@5=94.998 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=10:31 IST
=> Training   12.03% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.436 Prec@5=95.024 rate=1.81 Hz, eta=0:20:13, total=0:02:45, wall=10:31 IST
=> Training   16.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.436 Prec@5=95.024 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:31 IST
=> Training   16.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.436 Prec@5=95.024 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:32 IST
=> Training   16.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.696 Prec@1=82.476 Prec@5=95.028 rate=1.81 Hz, eta=0:19:22, total=0:03:41, wall=10:32 IST
=> Training   20.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.696 Prec@1=82.476 Prec@5=95.028 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:32 IST
=> Training   20.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.696 Prec@1=82.476 Prec@5=95.028 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:33 IST
=> Training   20.02% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.390 Loss=0.696 Prec@1=82.462 Prec@5=95.037 rate=1.80 Hz, eta=0:18:31, total=0:04:38, wall=10:33 IST
=> Training   24.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.390 Loss=0.696 Prec@1=82.462 Prec@5=95.037 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:33 IST
=> Training   24.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.390 Loss=0.696 Prec@1=82.462 Prec@5=95.037 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:34 IST
=> Training   24.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.695 Prec@1=82.463 Prec@5=95.058 rate=1.80 Hz, eta=0:17:35, total=0:05:33, wall=10:34 IST
=> Training   28.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.695 Prec@1=82.463 Prec@5=95.058 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=10:34 IST
=> Training   28.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.564 DataTime=0.391 Loss=0.695 Prec@1=82.463 Prec@5=95.058 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=10:35 IST
=> Training   28.01% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.468 Prec@5=95.067 rate=1.79 Hz, eta=0:16:44, total=0:06:30, wall=10:35 IST
=> Training   32.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.468 Prec@5=95.067 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:35 IST
=> Training   32.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.468 Prec@5=95.067 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:36 IST
=> Training   32.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.432 Prec@5=95.056 rate=1.80 Hz, eta=0:15:48, total=0:07:26, wall=10:36 IST
=> Training   36.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.432 Prec@5=95.056 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=10:36 IST
=> Training   36.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.432 Prec@5=95.056 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=10:37 IST
=> Training   36.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.445 Prec@5=95.055 rate=1.80 Hz, eta=0:14:52, total=0:08:21, wall=10:37 IST
=> Training   39.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.445 Prec@5=95.055 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:37 IST
=> Training   39.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.445 Prec@5=95.055 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:38 IST
=> Training   39.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.697 Prec@1=82.426 Prec@5=95.041 rate=1.80 Hz, eta=0:13:56, total=0:09:17, wall=10:38 IST
=> Training   43.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.697 Prec@1=82.426 Prec@5=95.041 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:38 IST
=> Training   43.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.562 DataTime=0.390 Loss=0.697 Prec@1=82.426 Prec@5=95.041 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:39 IST
=> Training   43.99% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.389 Loss=0.697 Prec@1=82.420 Prec@5=95.055 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=10:39 IST
=> Training   47.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.389 Loss=0.697 Prec@1=82.420 Prec@5=95.055 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=10:39 IST
=> Training   47.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.389 Loss=0.697 Prec@1=82.420 Prec@5=95.055 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=10:40 IST
=> Training   47.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.396 Prec@5=95.049 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=10:40 IST
=> Training   51.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.396 Prec@5=95.049 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=10:40 IST
=> Training   51.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.396 Prec@5=95.049 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=10:40 IST
=> Training   51.98% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.383 Prec@5=95.047 rate=1.79 Hz, eta=0:11:10, total=0:12:05, wall=10:40 IST
=> Training   55.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.383 Prec@5=95.047 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=10:40 IST
=> Training   55.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.383 Prec@5=95.047 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=10:41 IST
=> Training   55.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.371 Prec@5=95.039 rate=1.80 Hz, eta=0:10:12, total=0:12:59, wall=10:41 IST
=> Training   59.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.371 Prec@5=95.039 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=10:41 IST
=> Training   59.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.371 Prec@5=95.039 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=10:42 IST
=> Training   59.97% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.373 Prec@5=95.038 rate=1.79 Hz, eta=0:09:18, total=0:13:56, wall=10:42 IST
=> Training   63.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.373 Prec@5=95.038 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=10:42 IST
=> Training   63.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.388 Loss=0.698 Prec@1=82.373 Prec@5=95.038 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=10:43 IST
=> Training   63.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.033 rate=1.79 Hz, eta=0:08:22, total=0:14:52, wall=10:43 IST
=> Training   67.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.033 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=10:43 IST
=> Training   67.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.033 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=10:44 IST
=> Training   67.96% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.365 Prec@5=95.031 rate=1.80 Hz, eta=0:07:26, total=0:15:47, wall=10:44 IST
=> Training   71.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.365 Prec@5=95.031 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=10:44 IST
=> Training   71.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.698 Prec@1=82.365 Prec@5=95.031 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=10:45 IST
=> Training   71.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.699 Prec@1=82.364 Prec@5=95.025 rate=1.79 Hz, eta=0:06:31, total=0:16:43, wall=10:45 IST
=> Training   75.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.699 Prec@1=82.364 Prec@5=95.025 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=10:45 IST
=> Training   75.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.560 DataTime=0.387 Loss=0.699 Prec@1=82.364 Prec@5=95.025 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=10:46 IST
=> Training   75.95% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.368 Prec@5=95.030 rate=1.79 Hz, eta=0:05:35, total=0:17:39, wall=10:46 IST
=> Training   79.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.368 Prec@5=95.030 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=10:46 IST
=> Training   79.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.368 Prec@5=95.030 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=10:47 IST
=> Training   79.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.364 Prec@5=95.029 rate=1.80 Hz, eta=0:04:39, total=0:18:33, wall=10:47 IST
=> Training   83.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.364 Prec@5=95.029 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=10:47 IST
=> Training   83.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.364 Prec@5=95.029 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=10:48 IST
=> Training   83.94% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.367 Prec@5=95.033 rate=1.80 Hz, eta=0:03:43, total=0:19:29, wall=10:48 IST
=> Training   87.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.367 Prec@5=95.033 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=10:48 IST
=> Training   87.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.367 Prec@5=95.033 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=10:49 IST
=> Training   87.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.035 rate=1.80 Hz, eta=0:02:48, total=0:20:25, wall=10:49 IST
=> Training   91.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.035 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=10:49 IST
=> Training   91.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.698 Prec@1=82.373 Prec@5=95.035 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=10:50 IST
=> Training   91.93% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.697 Prec@1=82.384 Prec@5=95.042 rate=1.79 Hz, eta=0:01:52, total=0:21:22, wall=10:50 IST
=> Training   95.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.697 Prec@1=82.384 Prec@5=95.042 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=10:50 IST
=> Training   95.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.387 Loss=0.697 Prec@1=82.384 Prec@5=95.042 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=10:51 IST
=> Training   95.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.377 Prec@5=95.035 rate=1.79 Hz, eta=0:00:56, total=0:22:18, wall=10:51 IST
=> Training   99.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.377 Prec@5=95.035 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=10:51 IST
=> Training   99.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.377 Prec@5=95.035 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=10:51 IST
=> Training   99.92% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.377 Prec@5=95.036 rate=1.80 Hz, eta=0:00:01, total=0:23:13, wall=10:51 IST
=> Training   100.00% of 1x2503...Epoch=146/150 LR=0.0003 Time=0.559 DataTime=0.386 Loss=0.698 Prec@1=82.377 Prec@5=95.036 rate=1.80 Hz, eta=0:00:00, total=0:23:13, wall=10:51 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:51 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=10:51 IST
=> Validation 0.00% of 1x98...Epoch=146/150 LR=0.0003 Time=6.686 Loss=0.687 Prec@1=80.859 Prec@5=94.922 rate=0 Hz, eta=?, total=0:00:00, wall=10:51 IST
=> Validation 1.02% of 1x98...Epoch=146/150 LR=0.0003 Time=6.686 Loss=0.687 Prec@1=80.859 Prec@5=94.922 rate=1893.07 Hz, eta=0:00:00, total=0:00:00, wall=10:51 IST
** Validation 1.02% of 1x98...Epoch=146/150 LR=0.0003 Time=6.686 Loss=0.687 Prec@1=80.859 Prec@5=94.922 rate=1893.07 Hz, eta=0:00:00, total=0:00:00, wall=10:52 IST
** Validation 1.02% of 1x98...Epoch=146/150 LR=0.0003 Time=0.634 Loss=1.170 Prec@1=71.768 Prec@5=90.316 rate=1893.07 Hz, eta=0:00:00, total=0:00:00, wall=10:52 IST
** Validation 100.00% of 1x98...Epoch=146/150 LR=0.0003 Time=0.634 Loss=1.170 Prec@1=71.768 Prec@5=90.316 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=10:52 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:52 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=10:52 IST
=> Training   0.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=5.103 DataTime=4.888 Loss=0.682 Prec@1=83.008 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=10:52 IST
=> Training   0.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=5.103 DataTime=4.888 Loss=0.682 Prec@1=83.008 Prec@5=95.312 rate=873.04 Hz, eta=0:00:02, total=0:00:00, wall=10:52 IST
=> Training   0.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=5.103 DataTime=4.888 Loss=0.682 Prec@1=83.008 Prec@5=95.312 rate=873.04 Hz, eta=0:00:02, total=0:00:00, wall=10:53 IST
=> Training   0.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.590 DataTime=0.426 Loss=0.699 Prec@1=82.596 Prec@5=94.991 rate=873.04 Hz, eta=0:00:02, total=0:00:00, wall=10:53 IST
=> Training   4.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.590 DataTime=0.426 Loss=0.699 Prec@1=82.596 Prec@5=94.991 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=10:53 IST
=> Training   4.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.590 DataTime=0.426 Loss=0.699 Prec@1=82.596 Prec@5=94.991 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=10:54 IST
=> Training   4.04% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.582 DataTime=0.418 Loss=0.695 Prec@1=82.638 Prec@5=94.997 rate=1.85 Hz, eta=0:21:35, total=0:00:54, wall=10:54 IST
=> Training   8.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.582 DataTime=0.418 Loss=0.695 Prec@1=82.638 Prec@5=94.997 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=10:54 IST
=> Training   8.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.582 DataTime=0.418 Loss=0.695 Prec@1=82.638 Prec@5=94.997 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=10:55 IST
=> Training   8.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.571 DataTime=0.403 Loss=0.694 Prec@1=82.593 Prec@5=94.996 rate=1.80 Hz, eta=0:21:21, total=0:01:51, wall=10:55 IST
=> Training   12.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.571 DataTime=0.403 Loss=0.694 Prec@1=82.593 Prec@5=94.996 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:55 IST
=> Training   12.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.571 DataTime=0.403 Loss=0.694 Prec@1=82.593 Prec@5=94.996 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:56 IST
=> Training   12.03% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.572 DataTime=0.404 Loss=0.696 Prec@1=82.510 Prec@5=94.996 rate=1.80 Hz, eta=0:20:20, total=0:02:46, wall=10:56 IST
=> Training   16.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.572 DataTime=0.404 Loss=0.696 Prec@1=82.510 Prec@5=94.996 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=10:56 IST
=> Training   16.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.572 DataTime=0.404 Loss=0.696 Prec@1=82.510 Prec@5=94.996 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=10:56 IST
=> Training   16.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.569 DataTime=0.400 Loss=0.696 Prec@1=82.504 Prec@5=95.014 rate=1.79 Hz, eta=0:19:36, total=0:03:44, wall=10:56 IST
=> Training   20.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.569 DataTime=0.400 Loss=0.696 Prec@1=82.504 Prec@5=95.014 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=10:56 IST
=> Training   20.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.569 DataTime=0.400 Loss=0.696 Prec@1=82.504 Prec@5=95.014 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=10:57 IST
=> Training   20.02% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.499 Prec@5=95.005 rate=1.79 Hz, eta=0:18:38, total=0:04:39, wall=10:57 IST
=> Training   24.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.499 Prec@5=95.005 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=10:57 IST
=> Training   24.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.499 Prec@5=95.005 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=10:58 IST
=> Training   24.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.466 Prec@5=95.010 rate=1.78 Hz, eta=0:17:47, total=0:05:37, wall=10:58 IST
=> Training   28.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.466 Prec@5=95.010 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=10:58 IST
=> Training   28.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.466 Prec@5=95.010 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=10:59 IST
=> Training   28.01% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.456 Prec@5=95.013 rate=1.79 Hz, eta=0:16:48, total=0:06:32, wall=10:59 IST
=> Training   32.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.456 Prec@5=95.013 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=10:59 IST
=> Training   32.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.567 DataTime=0.395 Loss=0.697 Prec@1=82.456 Prec@5=95.013 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=11:00 IST
=> Training   32.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.498 Prec@5=95.028 rate=1.78 Hz, eta=0:15:54, total=0:07:29, wall=11:00 IST
=> Training   36.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.498 Prec@5=95.028 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=11:00 IST
=> Training   36.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.498 Prec@5=95.028 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=11:01 IST
=> Training   36.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.494 Prec@5=95.025 rate=1.79 Hz, eta=0:14:56, total=0:08:24, wall=11:01 IST
=> Training   39.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.494 Prec@5=95.025 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=11:01 IST
=> Training   39.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.565 DataTime=0.393 Loss=0.696 Prec@1=82.494 Prec@5=95.025 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=11:02 IST
=> Training   39.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.696 Prec@1=82.496 Prec@5=95.031 rate=1.79 Hz, eta=0:14:01, total=0:09:20, wall=11:02 IST
=> Training   43.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.696 Prec@1=82.496 Prec@5=95.031 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=11:02 IST
=> Training   43.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.696 Prec@1=82.496 Prec@5=95.031 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=11:03 IST
=> Training   43.99% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.473 Prec@5=95.010 rate=1.79 Hz, eta=0:13:04, total=0:10:15, wall=11:03 IST
=> Training   47.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.473 Prec@5=95.010 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=11:03 IST
=> Training   47.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.473 Prec@5=95.010 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=11:04 IST
=> Training   47.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.475 Prec@5=95.010 rate=1.79 Hz, eta=0:12:08, total=0:11:12, wall=11:04 IST
=> Training   51.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.475 Prec@5=95.010 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:04 IST
=> Training   51.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.393 Loss=0.697 Prec@1=82.475 Prec@5=95.010 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:05 IST
=> Training   51.98% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.470 Prec@5=95.027 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:05 IST
=> Training   55.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.470 Prec@5=95.027 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=11:05 IST
=> Training   55.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.564 DataTime=0.392 Loss=0.697 Prec@1=82.470 Prec@5=95.027 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=11:06 IST
=> Training   55.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.482 Prec@5=95.032 rate=1.79 Hz, eta=0:10:17, total=0:13:04, wall=11:06 IST
=> Training   59.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.482 Prec@5=95.032 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=11:06 IST
=> Training   59.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.482 Prec@5=95.032 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=11:07 IST
=> Training   59.97% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.489 Prec@5=95.031 rate=1.79 Hz, eta=0:09:20, total=0:13:58, wall=11:07 IST
=> Training   63.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.489 Prec@5=95.031 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=11:07 IST
=> Training   63.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.489 Prec@5=95.031 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=11:08 IST
=> Training   63.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.492 Prec@5=95.034 rate=1.79 Hz, eta=0:08:24, total=0:14:55, wall=11:08 IST
=> Training   67.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.492 Prec@5=95.034 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=11:08 IST
=> Training   67.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.492 Prec@5=95.034 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=11:09 IST
=> Training   67.96% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.473 Prec@5=95.041 rate=1.79 Hz, eta=0:07:28, total=0:15:51, wall=11:09 IST
=> Training   71.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.473 Prec@5=95.041 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=11:09 IST
=> Training   71.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.473 Prec@5=95.041 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=11:10 IST
=> Training   71.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.486 Prec@5=95.046 rate=1.79 Hz, eta=0:06:32, total=0:16:47, wall=11:10 IST
=> Training   75.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.486 Prec@5=95.046 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:10 IST
=> Training   75.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.486 Prec@5=95.046 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:10 IST
=> Training   75.95% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.485 Prec@5=95.047 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:10 IST
=> Training   79.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.485 Prec@5=95.047 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=11:10 IST
=> Training   79.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.391 Loss=0.696 Prec@1=82.485 Prec@5=95.047 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=11:11 IST
=> Training   79.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.494 Prec@5=95.045 rate=1.79 Hz, eta=0:04:41, total=0:18:40, wall=11:11 IST
=> Training   83.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.494 Prec@5=95.045 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=11:11 IST
=> Training   83.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.562 DataTime=0.390 Loss=0.696 Prec@1=82.494 Prec@5=95.045 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=11:12 IST
=> Training   83.94% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.480 Prec@5=95.038 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=11:12 IST
=> Training   87.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.480 Prec@5=95.038 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:12 IST
=> Training   87.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.480 Prec@5=95.038 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:13 IST
=> Training   87.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.481 Prec@5=95.039 rate=1.79 Hz, eta=0:02:48, total=0:20:30, wall=11:13 IST
=> Training   91.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.481 Prec@5=95.039 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:13 IST
=> Training   91.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.481 Prec@5=95.039 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:14 IST
=> Training   91.93% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.473 Prec@5=95.040 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:14 IST
=> Training   95.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.473 Prec@5=95.040 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:14 IST
=> Training   95.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.390 Loss=0.696 Prec@1=82.473 Prec@5=95.040 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:15 IST
=> Training   95.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.458 Prec@5=95.035 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:15 IST
=> Training   99.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.458 Prec@5=95.035 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=11:15 IST
=> Training   99.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.458 Prec@5=95.035 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=11:15 IST
=> Training   99.92% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.457 Prec@5=95.035 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=11:15 IST
=> Training   100.00% of 1x2503...Epoch=147/150 LR=0.0002 Time=0.561 DataTime=0.389 Loss=0.697 Prec@1=82.457 Prec@5=95.035 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=11:15 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:15 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:15 IST
=> Validation 0.00% of 1x98...Epoch=147/150 LR=0.0002 Time=7.227 Loss=0.697 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=11:15 IST
=> Validation 1.02% of 1x98...Epoch=147/150 LR=0.0002 Time=7.227 Loss=0.697 Prec@1=81.055 Prec@5=95.312 rate=5743.03 Hz, eta=0:00:00, total=0:00:00, wall=11:15 IST
** Validation 1.02% of 1x98...Epoch=147/150 LR=0.0002 Time=7.227 Loss=0.697 Prec@1=81.055 Prec@5=95.312 rate=5743.03 Hz, eta=0:00:00, total=0:00:00, wall=11:16 IST
** Validation 1.02% of 1x98...Epoch=147/150 LR=0.0002 Time=0.639 Loss=1.172 Prec@1=71.784 Prec@5=90.306 rate=5743.03 Hz, eta=0:00:00, total=0:00:00, wall=11:16 IST
** Validation 100.00% of 1x98...Epoch=147/150 LR=0.0002 Time=0.639 Loss=1.172 Prec@1=71.784 Prec@5=90.306 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=11:16 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:16 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:16 IST
=> Training   0.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=4.939 DataTime=4.554 Loss=0.742 Prec@1=82.031 Prec@5=94.336 rate=0 Hz, eta=?, total=0:00:00, wall=11:16 IST
=> Training   0.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=4.939 DataTime=4.554 Loss=0.742 Prec@1=82.031 Prec@5=94.336 rate=4971.74 Hz, eta=0:00:00, total=0:00:00, wall=11:16 IST
=> Training   0.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=4.939 DataTime=4.554 Loss=0.742 Prec@1=82.031 Prec@5=94.336 rate=4971.74 Hz, eta=0:00:00, total=0:00:00, wall=11:17 IST
=> Training   0.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.588 DataTime=0.424 Loss=0.697 Prec@1=82.430 Prec@5=94.928 rate=4971.74 Hz, eta=0:00:00, total=0:00:00, wall=11:17 IST
=> Training   4.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.588 DataTime=0.424 Loss=0.697 Prec@1=82.430 Prec@5=94.928 rate=1.85 Hz, eta=0:21:34, total=0:00:54, wall=11:17 IST
=> Training   4.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.588 DataTime=0.424 Loss=0.697 Prec@1=82.430 Prec@5=94.928 rate=1.85 Hz, eta=0:21:34, total=0:00:54, wall=11:18 IST
=> Training   4.04% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.579 DataTime=0.411 Loss=0.699 Prec@1=82.351 Prec@5=94.986 rate=1.85 Hz, eta=0:21:34, total=0:00:54, wall=11:18 IST
=> Training   8.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.579 DataTime=0.411 Loss=0.699 Prec@1=82.351 Prec@5=94.986 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=11:18 IST
=> Training   8.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.579 DataTime=0.411 Loss=0.699 Prec@1=82.351 Prec@5=94.986 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=11:19 IST
=> Training   8.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.567 DataTime=0.399 Loss=0.697 Prec@1=82.393 Prec@5=95.028 rate=1.80 Hz, eta=0:21:16, total=0:01:51, wall=11:19 IST
=> Training   12.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.567 DataTime=0.399 Loss=0.697 Prec@1=82.393 Prec@5=95.028 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=11:19 IST
=> Training   12.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.567 DataTime=0.399 Loss=0.697 Prec@1=82.393 Prec@5=95.028 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=11:20 IST
=> Training   12.03% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.566 DataTime=0.397 Loss=0.697 Prec@1=82.438 Prec@5=95.015 rate=1.82 Hz, eta=0:20:13, total=0:02:45, wall=11:20 IST
=> Training   16.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.566 DataTime=0.397 Loss=0.697 Prec@1=82.438 Prec@5=95.015 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=11:20 IST
=> Training   16.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.566 DataTime=0.397 Loss=0.697 Prec@1=82.438 Prec@5=95.015 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=11:21 IST
=> Training   16.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.394 Loss=0.696 Prec@1=82.492 Prec@5=95.027 rate=1.81 Hz, eta=0:19:24, total=0:03:42, wall=11:21 IST
=> Training   20.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.394 Loss=0.696 Prec@1=82.492 Prec@5=95.027 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=11:21 IST
=> Training   20.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.394 Loss=0.696 Prec@1=82.492 Prec@5=95.027 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=11:22 IST
=> Training   20.02% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.395 Loss=0.696 Prec@1=82.452 Prec@5=95.035 rate=1.81 Hz, eta=0:18:29, total=0:04:37, wall=11:22 IST
=> Training   24.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.395 Loss=0.696 Prec@1=82.452 Prec@5=95.035 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=11:22 IST
=> Training   24.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.564 DataTime=0.395 Loss=0.696 Prec@1=82.452 Prec@5=95.035 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=11:23 IST
=> Training   24.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.563 DataTime=0.395 Loss=0.695 Prec@1=82.447 Prec@5=95.055 rate=1.80 Hz, eta=0:17:37, total=0:05:34, wall=11:23 IST
=> Training   28.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.563 DataTime=0.395 Loss=0.695 Prec@1=82.447 Prec@5=95.055 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=11:23 IST
=> Training   28.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.563 DataTime=0.395 Loss=0.695 Prec@1=82.447 Prec@5=95.055 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=11:24 IST
=> Training   28.01% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.392 Loss=0.694 Prec@1=82.477 Prec@5=95.074 rate=1.80 Hz, eta=0:16:42, total=0:06:30, wall=11:24 IST
=> Training   32.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.392 Loss=0.694 Prec@1=82.477 Prec@5=95.074 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=11:24 IST
=> Training   32.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.392 Loss=0.694 Prec@1=82.477 Prec@5=95.074 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=11:25 IST
=> Training   32.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.560 DataTime=0.390 Loss=0.692 Prec@1=82.519 Prec@5=95.091 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=11:25 IST
=> Training   36.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.560 DataTime=0.390 Loss=0.692 Prec@1=82.519 Prec@5=95.091 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=11:25 IST
=> Training   36.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.560 DataTime=0.390 Loss=0.692 Prec@1=82.519 Prec@5=95.091 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=11:26 IST
=> Training   36.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.693 Prec@1=82.512 Prec@5=95.078 rate=1.80 Hz, eta=0:14:48, total=0:08:19, wall=11:26 IST
=> Training   39.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.693 Prec@1=82.512 Prec@5=95.078 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:26 IST
=> Training   39.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.693 Prec@1=82.512 Prec@5=95.078 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:26 IST
=> Training   39.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.693 Prec@1=82.530 Prec@5=95.078 rate=1.80 Hz, eta=0:13:55, total=0:09:16, wall=11:26 IST
=> Training   43.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.693 Prec@1=82.530 Prec@5=95.078 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=11:26 IST
=> Training   43.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.693 Prec@1=82.530 Prec@5=95.078 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=11:27 IST
=> Training   43.99% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.511 Prec@5=95.062 rate=1.79 Hz, eta=0:13:02, total=0:10:14, wall=11:27 IST
=> Training   47.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.511 Prec@5=95.062 rate=1.80 Hz, eta=0:12:05, total=0:11:09, wall=11:27 IST
=> Training   47.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.511 Prec@5=95.062 rate=1.80 Hz, eta=0:12:05, total=0:11:09, wall=11:28 IST
=> Training   47.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.694 Prec@1=82.501 Prec@5=95.054 rate=1.80 Hz, eta=0:12:05, total=0:11:09, wall=11:28 IST
=> Training   51.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.694 Prec@1=82.501 Prec@5=95.054 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=11:28 IST
=> Training   51.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.391 Loss=0.694 Prec@1=82.501 Prec@5=95.054 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=11:29 IST
=> Training   51.98% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.509 Prec@5=95.071 rate=1.79 Hz, eta=0:11:11, total=0:12:06, wall=11:29 IST
=> Training   55.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.509 Prec@5=95.071 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:29 IST
=> Training   55.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.509 Prec@5=95.071 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:30 IST
=> Training   55.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.507 Prec@5=95.072 rate=1.79 Hz, eta=0:10:14, total=0:13:01, wall=11:30 IST
=> Training   59.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.507 Prec@5=95.072 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=11:30 IST
=> Training   59.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.507 Prec@5=95.072 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=11:31 IST
=> Training   59.97% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.508 Prec@5=95.077 rate=1.79 Hz, eta=0:09:19, total=0:13:57, wall=11:31 IST
=> Training   63.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.508 Prec@5=95.077 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:31 IST
=> Training   63.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.390 Loss=0.694 Prec@1=82.508 Prec@5=95.077 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:32 IST
=> Training   63.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.503 Prec@5=95.070 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:32 IST
=> Training   67.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.503 Prec@5=95.070 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:32 IST
=> Training   67.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.503 Prec@5=95.070 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:33 IST
=> Training   67.96% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.490 Prec@5=95.069 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:33 IST
=> Training   71.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.490 Prec@5=95.069 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:33 IST
=> Training   71.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.490 Prec@5=95.069 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:34 IST
=> Training   71.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.495 Prec@5=95.069 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:34 IST
=> Training   75.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.495 Prec@5=95.069 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:34 IST
=> Training   75.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.390 Loss=0.694 Prec@1=82.495 Prec@5=95.069 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:35 IST
=> Training   75.95% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.482 Prec@5=95.060 rate=1.79 Hz, eta=0:05:36, total=0:17:43, wall=11:35 IST
=> Training   79.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.482 Prec@5=95.060 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=11:35 IST
=> Training   79.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.482 Prec@5=95.060 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=11:36 IST
=> Training   79.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.476 Prec@5=95.067 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=11:36 IST
=> Training   83.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.476 Prec@5=95.067 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=11:36 IST
=> Training   83.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.389 Loss=0.695 Prec@1=82.476 Prec@5=95.067 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=11:37 IST
=> Training   83.94% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.388 Loss=0.695 Prec@1=82.475 Prec@5=95.066 rate=1.79 Hz, eta=0:03:45, total=0:19:36, wall=11:37 IST
=> Training   87.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.388 Loss=0.695 Prec@1=82.475 Prec@5=95.066 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=11:37 IST
=> Training   87.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.562 DataTime=0.388 Loss=0.695 Prec@1=82.475 Prec@5=95.066 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=11:38 IST
=> Training   87.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.497 Prec@5=95.075 rate=1.79 Hz, eta=0:02:48, total=0:20:31, wall=11:38 IST
=> Training   91.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.497 Prec@5=95.075 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:38 IST
=> Training   91.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.497 Prec@5=95.075 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:39 IST
=> Training   91.93% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.494 Prec@5=95.072 rate=1.79 Hz, eta=0:01:52, total=0:21:26, wall=11:39 IST
=> Training   95.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.494 Prec@5=95.072 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:39 IST
=> Training   95.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.494 Prec@5=95.072 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:40 IST
=> Training   95.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.498 Prec@5=95.071 rate=1.79 Hz, eta=0:00:57, total=0:22:22, wall=11:40 IST
=> Training   99.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.498 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=11:40 IST
=> Training   99.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.498 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=11:40 IST
=> Training   99.92% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.387 Loss=0.694 Prec@1=82.498 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=11:40 IST
=> Training   100.00% of 1x2503...Epoch=148/150 LR=0.0001 Time=0.561 DataTime=0.387 Loss=0.694 Prec@1=82.498 Prec@5=95.071 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=11:40 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> Validation 0.00% of 1x98...Epoch=148/150 LR=0.0001 Time=7.066 Loss=0.692 Prec@1=81.055 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=11:40 IST
=> Validation 1.02% of 1x98...Epoch=148/150 LR=0.0001 Time=7.066 Loss=0.692 Prec@1=81.055 Prec@5=95.312 rate=7996.93 Hz, eta=0:00:00, total=0:00:00, wall=11:40 IST
** Validation 1.02% of 1x98...Epoch=148/150 LR=0.0001 Time=7.066 Loss=0.692 Prec@1=81.055 Prec@5=95.312 rate=7996.93 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
** Validation 1.02% of 1x98...Epoch=148/150 LR=0.0001 Time=0.628 Loss=1.167 Prec@1=71.824 Prec@5=90.370 rate=7996.93 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
** Validation 100.00% of 1x98...Epoch=148/150 LR=0.0001 Time=0.628 Loss=1.167 Prec@1=71.824 Prec@5=90.370 rate=1.80 Hz, eta=0:00:00, total=0:00:54, wall=11:41 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> Training   0.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=5.148 DataTime=4.914 Loss=0.676 Prec@1=83.398 Prec@5=96.094 rate=0 Hz, eta=?, total=0:00:00, wall=11:41 IST
=> Training   0.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=5.148 DataTime=4.914 Loss=0.676 Prec@1=83.398 Prec@5=96.094 rate=9778.71 Hz, eta=0:00:00, total=0:00:00, wall=11:41 IST
=> Training   0.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=5.148 DataTime=4.914 Loss=0.676 Prec@1=83.398 Prec@5=96.094 rate=9778.71 Hz, eta=0:00:00, total=0:00:00, wall=11:42 IST
=> Training   0.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.593 DataTime=0.427 Loss=0.685 Prec@1=82.832 Prec@5=95.131 rate=9778.71 Hz, eta=0:00:00, total=0:00:00, wall=11:42 IST
=> Training   4.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.593 DataTime=0.427 Loss=0.685 Prec@1=82.832 Prec@5=95.131 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=11:42 IST
=> Training   4.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.593 DataTime=0.427 Loss=0.685 Prec@1=82.832 Prec@5=95.131 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=11:43 IST
=> Training   4.04% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.576 DataTime=0.405 Loss=0.689 Prec@1=82.623 Prec@5=95.117 rate=1.84 Hz, eta=0:21:42, total=0:00:54, wall=11:43 IST
=> Training   8.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.576 DataTime=0.405 Loss=0.689 Prec@1=82.623 Prec@5=95.117 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=11:43 IST
=> Training   8.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.576 DataTime=0.405 Loss=0.689 Prec@1=82.623 Prec@5=95.117 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=11:43 IST
=> Training   8.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.569 DataTime=0.399 Loss=0.690 Prec@1=82.574 Prec@5=95.095 rate=1.82 Hz, eta=0:21:07, total=0:01:50, wall=11:43 IST
=> Training   12.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.569 DataTime=0.399 Loss=0.690 Prec@1=82.574 Prec@5=95.095 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=11:43 IST
=> Training   12.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.569 DataTime=0.399 Loss=0.690 Prec@1=82.574 Prec@5=95.095 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=11:44 IST
=> Training   12.03% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.568 DataTime=0.396 Loss=0.692 Prec@1=82.502 Prec@5=95.101 rate=1.81 Hz, eta=0:20:16, total=0:02:46, wall=11:44 IST
=> Training   16.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.568 DataTime=0.396 Loss=0.692 Prec@1=82.502 Prec@5=95.101 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=11:44 IST
=> Training   16.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.568 DataTime=0.396 Loss=0.692 Prec@1=82.502 Prec@5=95.101 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=11:45 IST
=> Training   16.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.567 DataTime=0.397 Loss=0.692 Prec@1=82.522 Prec@5=95.087 rate=1.80 Hz, eta=0:19:26, total=0:03:42, wall=11:45 IST
=> Training   20.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.567 DataTime=0.397 Loss=0.692 Prec@1=82.522 Prec@5=95.087 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=11:45 IST
=> Training   20.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.567 DataTime=0.397 Loss=0.692 Prec@1=82.522 Prec@5=95.087 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=11:46 IST
=> Training   20.02% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.396 Loss=0.693 Prec@1=82.476 Prec@5=95.065 rate=1.80 Hz, eta=0:18:33, total=0:04:38, wall=11:46 IST
=> Training   24.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.396 Loss=0.693 Prec@1=82.476 Prec@5=95.065 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=11:46 IST
=> Training   24.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.396 Loss=0.693 Prec@1=82.476 Prec@5=95.065 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=11:47 IST
=> Training   24.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.566 DataTime=0.396 Loss=0.693 Prec@1=82.488 Prec@5=95.066 rate=1.80 Hz, eta=0:17:39, total=0:05:34, wall=11:47 IST
=> Training   28.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.566 DataTime=0.396 Loss=0.693 Prec@1=82.488 Prec@5=95.066 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=11:47 IST
=> Training   28.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.566 DataTime=0.396 Loss=0.693 Prec@1=82.488 Prec@5=95.066 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=11:48 IST
=> Training   28.01% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.395 Loss=0.692 Prec@1=82.524 Prec@5=95.070 rate=1.79 Hz, eta=0:16:45, total=0:06:31, wall=11:48 IST
=> Training   32.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.395 Loss=0.692 Prec@1=82.524 Prec@5=95.070 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=11:48 IST
=> Training   32.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.395 Loss=0.692 Prec@1=82.524 Prec@5=95.070 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=11:49 IST
=> Training   32.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.393 Loss=0.692 Prec@1=82.533 Prec@5=95.067 rate=1.79 Hz, eta=0:15:50, total=0:07:27, wall=11:49 IST
=> Training   36.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.393 Loss=0.692 Prec@1=82.533 Prec@5=95.067 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=11:49 IST
=> Training   36.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.393 Loss=0.692 Prec@1=82.533 Prec@5=95.067 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=11:50 IST
=> Training   36.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.692 Prec@1=82.557 Prec@5=95.082 rate=1.79 Hz, eta=0:14:54, total=0:08:23, wall=11:50 IST
=> Training   39.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.692 Prec@1=82.557 Prec@5=95.082 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=11:50 IST
=> Training   39.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.692 Prec@1=82.557 Prec@5=95.082 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=11:51 IST
=> Training   39.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.393 Loss=0.692 Prec@1=82.523 Prec@5=95.072 rate=1.79 Hz, eta=0:13:57, total=0:09:18, wall=11:51 IST
=> Training   43.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.393 Loss=0.692 Prec@1=82.523 Prec@5=95.072 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=11:51 IST
=> Training   43.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.565 DataTime=0.393 Loss=0.692 Prec@1=82.523 Prec@5=95.072 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=11:52 IST
=> Training   43.99% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.693 Prec@1=82.523 Prec@5=95.071 rate=1.79 Hz, eta=0:13:05, total=0:10:16, wall=11:52 IST
=> Training   47.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.693 Prec@1=82.523 Prec@5=95.071 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=11:52 IST
=> Training   47.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.693 Prec@1=82.523 Prec@5=95.071 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=11:53 IST
=> Training   47.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.392 Loss=0.693 Prec@1=82.518 Prec@5=95.070 rate=1.79 Hz, eta=0:12:07, total=0:11:10, wall=11:53 IST
=> Training   51.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.392 Loss=0.693 Prec@1=82.518 Prec@5=95.070 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:53 IST
=> Training   51.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.564 DataTime=0.392 Loss=0.693 Prec@1=82.518 Prec@5=95.070 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:54 IST
=> Training   51.98% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.509 Prec@5=95.064 rate=1.79 Hz, eta=0:11:13, total=0:12:08, wall=11:54 IST
=> Training   55.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.509 Prec@5=95.064 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=11:54 IST
=> Training   55.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.694 Prec@1=82.509 Prec@5=95.064 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=11:55 IST
=> Training   55.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.390 Loss=0.693 Prec@1=82.520 Prec@5=95.075 rate=1.79 Hz, eta=0:10:16, total=0:13:03, wall=11:55 IST
=> Training   59.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.390 Loss=0.693 Prec@1=82.520 Prec@5=95.075 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=11:55 IST
=> Training   59.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.563 DataTime=0.390 Loss=0.693 Prec@1=82.520 Prec@5=95.075 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=11:56 IST
=> Training   59.97% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.525 Prec@5=95.081 rate=1.79 Hz, eta=0:09:20, total=0:13:59, wall=11:56 IST
=> Training   63.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.525 Prec@5=95.081 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:56 IST
=> Training   63.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.525 Prec@5=95.081 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:57 IST
=> Training   63.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.693 Prec@1=82.526 Prec@5=95.070 rate=1.79 Hz, eta=0:08:23, total=0:14:53, wall=11:57 IST
=> Training   67.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.693 Prec@1=82.526 Prec@5=95.070 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:57 IST
=> Training   67.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.693 Prec@1=82.526 Prec@5=95.070 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:57 IST
=> Training   67.96% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.514 Prec@5=95.067 rate=1.79 Hz, eta=0:07:28, total=0:15:50, wall=11:57 IST
=> Training   71.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.514 Prec@5=95.067 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:57 IST
=> Training   71.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.514 Prec@5=95.067 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:58 IST
=> Training   71.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.519 Prec@5=95.061 rate=1.79 Hz, eta=0:06:32, total=0:16:45, wall=11:58 IST
=> Training   75.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.519 Prec@5=95.061 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:58 IST
=> Training   75.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.519 Prec@5=95.061 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:59 IST
=> Training   75.95% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.528 Prec@5=95.067 rate=1.79 Hz, eta=0:05:36, total=0:17:42, wall=11:59 IST
=> Training   79.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.528 Prec@5=95.067 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=11:59 IST
=> Training   79.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.528 Prec@5=95.067 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=12:00 IST
=> Training   79.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.534 Prec@5=95.070 rate=1.79 Hz, eta=0:04:40, total=0:18:38, wall=12:00 IST
=> Training   83.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.534 Prec@5=95.070 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:00 IST
=> Training   83.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.534 Prec@5=95.070 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:01 IST
=> Training   83.94% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.531 Prec@5=95.075 rate=1.79 Hz, eta=0:03:44, total=0:19:35, wall=12:01 IST
=> Training   87.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.531 Prec@5=95.075 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=12:01 IST
=> Training   87.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.693 Prec@1=82.531 Prec@5=95.075 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=12:02 IST
=> Training   87.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.527 Prec@5=95.065 rate=1.79 Hz, eta=0:02:49, total=0:20:31, wall=12:02 IST
=> Training   91.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.527 Prec@5=95.065 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:02 IST
=> Training   91.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.389 Loss=0.694 Prec@1=82.527 Prec@5=95.065 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:03 IST
=> Training   91.93% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.068 rate=1.79 Hz, eta=0:01:53, total=0:21:28, wall=12:03 IST
=> Training   95.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.068 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:03 IST
=> Training   95.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.562 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.068 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:04 IST
=> Training   95.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.518 Prec@5=95.071 rate=1.79 Hz, eta=0:00:57, total=0:22:23, wall=12:04 IST
=> Training   99.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.518 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:04 IST
=> Training   99.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.518 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:04 IST
=> Training   99.92% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.519 Prec@5=95.071 rate=1.79 Hz, eta=0:00:01, total=0:23:18, wall=12:04 IST
=> Training   100.00% of 1x2503...Epoch=149/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.519 Prec@5=95.071 rate=1.79 Hz, eta=0:00:00, total=0:23:19, wall=12:04 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> Validation 0.00% of 1x98...Epoch=149/150 LR=0.0000 Time=6.986 Loss=0.696 Prec@1=81.445 Prec@5=95.312 rate=0 Hz, eta=?, total=0:00:00, wall=12:04 IST
=> Validation 1.02% of 1x98...Epoch=149/150 LR=0.0000 Time=6.986 Loss=0.696 Prec@1=81.445 Prec@5=95.312 rate=4183.87 Hz, eta=0:00:00, total=0:00:00, wall=12:04 IST
** Validation 1.02% of 1x98...Epoch=149/150 LR=0.0000 Time=6.986 Loss=0.696 Prec@1=81.445 Prec@5=95.312 rate=4183.87 Hz, eta=0:00:00, total=0:00:00, wall=12:05 IST
** Validation 1.02% of 1x98...Epoch=149/150 LR=0.0000 Time=0.636 Loss=1.168 Prec@1=71.794 Prec@5=90.368 rate=4183.87 Hz, eta=0:00:00, total=0:00:00, wall=12:05 IST
** Validation 100.00% of 1x98...Epoch=149/150 LR=0.0000 Time=0.636 Loss=1.168 Prec@1=71.794 Prec@5=90.368 rate=1.77 Hz, eta=0:00:00, total=0:00:55, wall=12:05 IST
[39m
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:05 IST
=> Training   0.00% of 1x2503... rate=0 Hz, eta=?, total=0:00:00, wall=12:05 IST
=> Training   0.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=5.033 DataTime=4.527 Loss=0.625 Prec@1=84.375 Prec@5=97.266 rate=0 Hz, eta=?, total=0:00:00, wall=12:05 IST
=> Training   0.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=5.033 DataTime=4.527 Loss=0.625 Prec@1=84.375 Prec@5=97.266 rate=1410.50 Hz, eta=0:00:01, total=0:00:00, wall=12:05 IST
=> Training   0.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=5.033 DataTime=4.527 Loss=0.625 Prec@1=84.375 Prec@5=97.266 rate=1410.50 Hz, eta=0:00:01, total=0:00:00, wall=12:06 IST
=> Training   0.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.610 DataTime=0.445 Loss=0.698 Prec@1=82.223 Prec@5=95.125 rate=1410.50 Hz, eta=0:00:01, total=0:00:00, wall=12:06 IST
=> Training   4.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.610 DataTime=0.445 Loss=0.698 Prec@1=82.223 Prec@5=95.125 rate=1.78 Hz, eta=0:22:26, total=0:00:56, wall=12:06 IST
=> Training   4.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.610 DataTime=0.445 Loss=0.698 Prec@1=82.223 Prec@5=95.125 rate=1.78 Hz, eta=0:22:26, total=0:00:56, wall=12:07 IST
=> Training   4.04% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.584 DataTime=0.409 Loss=0.696 Prec@1=82.344 Prec@5=95.134 rate=1.78 Hz, eta=0:22:26, total=0:00:56, wall=12:07 IST
=> Training   8.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.584 DataTime=0.409 Loss=0.696 Prec@1=82.344 Prec@5=95.134 rate=1.79 Hz, eta=0:21:26, total=0:01:52, wall=12:07 IST
=> Training   8.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.584 DataTime=0.409 Loss=0.696 Prec@1=82.344 Prec@5=95.134 rate=1.79 Hz, eta=0:21:26, total=0:01:52, wall=12:08 IST
=> Training   8.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.365 Prec@5=95.094 rate=1.79 Hz, eta=0:21:26, total=0:01:52, wall=12:08 IST
=> Training   12.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.365 Prec@5=95.094 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=12:08 IST
=> Training   12.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.570 DataTime=0.398 Loss=0.696 Prec@1=82.365 Prec@5=95.094 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=12:09 IST
=> Training   12.03% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.568 DataTime=0.397 Loss=0.696 Prec@1=82.412 Prec@5=95.069 rate=1.81 Hz, eta=0:20:17, total=0:02:46, wall=12:09 IST
=> Training   16.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.568 DataTime=0.397 Loss=0.696 Prec@1=82.412 Prec@5=95.069 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=12:09 IST
=> Training   16.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.568 DataTime=0.397 Loss=0.696 Prec@1=82.412 Prec@5=95.069 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=12:10 IST
=> Training   16.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.566 DataTime=0.395 Loss=0.695 Prec@1=82.461 Prec@5=95.075 rate=1.80 Hz, eta=0:19:28, total=0:03:42, wall=12:10 IST
=> Training   20.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.566 DataTime=0.395 Loss=0.695 Prec@1=82.461 Prec@5=95.075 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:10 IST
=> Training   20.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.566 DataTime=0.395 Loss=0.695 Prec@1=82.461 Prec@5=95.075 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:11 IST
=> Training   20.02% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.695 Prec@1=82.499 Prec@5=95.078 rate=1.80 Hz, eta=0:18:32, total=0:04:38, wall=12:11 IST
=> Training   24.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.695 Prec@1=82.499 Prec@5=95.078 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:11 IST
=> Training   24.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.563 DataTime=0.391 Loss=0.695 Prec@1=82.499 Prec@5=95.078 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:12 IST
=> Training   24.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.562 DataTime=0.390 Loss=0.695 Prec@1=82.462 Prec@5=95.076 rate=1.80 Hz, eta=0:17:34, total=0:05:33, wall=12:12 IST
=> Training   28.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.562 DataTime=0.390 Loss=0.695 Prec@1=82.462 Prec@5=95.076 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=12:12 IST
=> Training   28.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.562 DataTime=0.390 Loss=0.695 Prec@1=82.462 Prec@5=95.076 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=12:13 IST
=> Training   28.01% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.695 Prec@1=82.468 Prec@5=95.074 rate=1.80 Hz, eta=0:16:40, total=0:06:29, wall=12:13 IST
=> Training   32.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.695 Prec@1=82.468 Prec@5=95.074 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=12:13 IST
=> Training   32.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.695 Prec@1=82.468 Prec@5=95.074 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=12:14 IST
=> Training   32.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.482 Prec@5=95.098 rate=1.80 Hz, eta=0:15:44, total=0:07:24, wall=12:14 IST
=> Training   36.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.482 Prec@5=95.098 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=12:14 IST
=> Training   36.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.482 Prec@5=95.098 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=12:14 IST
=> Training   36.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.388 Loss=0.694 Prec@1=82.499 Prec@5=95.089 rate=1.80 Hz, eta=0:14:50, total=0:08:20, wall=12:14 IST
=> Training   39.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.388 Loss=0.694 Prec@1=82.499 Prec@5=95.089 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=12:14 IST
=> Training   39.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.388 Loss=0.694 Prec@1=82.499 Prec@5=95.089 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=12:15 IST
=> Training   39.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.510 Prec@5=95.072 rate=1.80 Hz, eta=0:13:54, total=0:09:15, wall=12:15 IST
=> Training   43.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.510 Prec@5=95.072 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=12:15 IST
=> Training   43.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.389 Loss=0.694 Prec@1=82.510 Prec@5=95.072 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=12:16 IST
=> Training   43.99% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.080 rate=1.80 Hz, eta=0:13:00, total=0:10:12, wall=12:16 IST
=> Training   47.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.080 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=12:16 IST
=> Training   47.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.694 Prec@1=82.521 Prec@5=95.080 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=12:17 IST
=> Training   47.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.80 Hz, eta=0:12:04, total=0:11:08, wall=12:17 IST
=> Training   51.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=12:17 IST
=> Training   51.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=12:18 IST
=> Training   51.98% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.514 Prec@5=95.085 rate=1.80 Hz, eta=0:11:08, total=0:12:03, wall=12:18 IST
=> Training   55.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.514 Prec@5=95.085 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=12:18 IST
=> Training   55.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.514 Prec@5=95.085 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=12:19 IST
=> Training   55.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.523 Prec@5=95.085 rate=1.80 Hz, eta=0:10:13, total=0:12:59, wall=12:19 IST
=> Training   59.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.523 Prec@5=95.085 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=12:19 IST
=> Training   59.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.694 Prec@1=82.523 Prec@5=95.085 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=12:20 IST
=> Training   59.97% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.80 Hz, eta=0:09:17, total=0:13:55, wall=12:20 IST
=> Training   63.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=12:20 IST
=> Training   63.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.693 Prec@1=82.524 Prec@5=95.087 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=12:21 IST
=> Training   63.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.537 Prec@5=95.095 rate=1.79 Hz, eta=0:08:23, total=0:14:52, wall=12:21 IST
=> Training   67.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.537 Prec@5=95.095 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=12:21 IST
=> Training   67.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.537 Prec@5=95.095 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=12:22 IST
=> Training   67.96% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.544 Prec@5=95.097 rate=1.79 Hz, eta=0:07:27, total=0:15:48, wall=12:22 IST
=> Training   71.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.544 Prec@5=95.097 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=12:22 IST
=> Training   71.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.544 Prec@5=95.097 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=12:23 IST
=> Training   71.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.532 Prec@5=95.090 rate=1.79 Hz, eta=0:06:31, total=0:16:44, wall=12:23 IST
=> Training   75.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.532 Prec@5=95.090 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=12:23 IST
=> Training   75.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.388 Loss=0.692 Prec@1=82.532 Prec@5=95.090 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=12:24 IST
=> Training   75.95% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.090 rate=1.79 Hz, eta=0:05:36, total=0:17:41, wall=12:24 IST
=> Training   79.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.090 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=12:24 IST
=> Training   79.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.560 DataTime=0.387 Loss=0.693 Prec@1=82.524 Prec@5=95.090 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=12:25 IST
=> Training   79.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.536 Prec@5=95.098 rate=1.79 Hz, eta=0:04:40, total=0:18:36, wall=12:25 IST
=> Training   83.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.536 Prec@5=95.098 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=12:25 IST
=> Training   83.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.536 Prec@5=95.098 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=12:26 IST
=> Training   83.94% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.544 Prec@5=95.099 rate=1.79 Hz, eta=0:03:44, total=0:19:33, wall=12:26 IST
=> Training   87.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.544 Prec@5=95.099 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=12:26 IST
=> Training   87.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.544 Prec@5=95.099 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=12:27 IST
=> Training   87.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.547 Prec@5=95.094 rate=1.79 Hz, eta=0:02:48, total=0:20:29, wall=12:27 IST
=> Training   91.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.547 Prec@5=95.094 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=12:27 IST
=> Training   91.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.692 Prec@1=82.547 Prec@5=95.094 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=12:28 IST
=> Training   91.93% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.539 Prec@5=95.088 rate=1.79 Hz, eta=0:01:52, total=0:21:25, wall=12:28 IST
=> Training   95.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.539 Prec@5=95.088 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=12:28 IST
=> Training   95.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.539 Prec@5=95.088 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=12:28 IST
=> Training   95.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.543 Prec@5=95.088 rate=1.79 Hz, eta=0:00:57, total=0:22:21, wall=12:28 IST
=> Training   99.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.543 Prec@5=95.088 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=12:28 IST
=> Training   99.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.543 Prec@5=95.088 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=12:28 IST
=> Training   99.92% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.543 Prec@5=95.089 rate=1.79 Hz, eta=0:00:01, total=0:23:17, wall=12:28 IST
=> Training   100.00% of 1x2503...Epoch=150/150 LR=0.0000 Time=0.561 DataTime=0.387 Loss=0.693 Prec@1=82.543 Prec@5=95.089 rate=1.79 Hz, eta=0:00:00, total=0:23:18, wall=12:28 IST
[39m[32m
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:29 IST
=> Validation 0.00% of 1x98... rate=0 Hz, eta=?, total=0:00:00, wall=12:29 IST
=> Validation 0.00% of 1x98...Epoch=150/150 LR=0.0000 Time=6.776 Loss=0.689 Prec@1=81.250 Prec@5=95.117 rate=0 Hz, eta=?, total=0:00:00, wall=12:29 IST
=> Validation 1.02% of 1x98...Epoch=150/150 LR=0.0000 Time=6.776 Loss=0.689 Prec@1=81.250 Prec@5=95.117 rate=758.37 Hz, eta=0:00:00, total=0:00:00, wall=12:29 IST
** Validation 1.02% of 1x98...Epoch=150/150 LR=0.0000 Time=6.776 Loss=0.689 Prec@1=81.250 Prec@5=95.117 rate=758.37 Hz, eta=0:00:00, total=0:00:00, wall=12:30 IST
** Validation 1.02% of 1x98...Epoch=150/150 LR=0.0000 Time=0.636 Loss=1.169 Prec@1=71.828 Prec@5=90.324 rate=758.37 Hz, eta=0:00:00, total=0:00:00, wall=12:30 IST
** Validation 100.00% of 1x98...Epoch=150/150 LR=0.0000 Time=0.636 Loss=1.169 Prec@1=71.828 Prec@5=90.324 rate=1.76 Hz, eta=0:00:00, total=0:00:55, wall=12:30 IST
[39m
