{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPythonNotebook can be used to experiment with the Post Training Calibration based method of quantization, where the user will be able to introduce quantization to any network. Further, the user can use the quantized model for evaluations on the pytorch framework itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netron in /home/a0491009/.pyenv/versions/3.10.13/envs/quant/lib/python3.10/site-packages (7.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a0491009/.pyenv/versions/quant/lib/python3.10/site-packages/tqdm-4.66.2-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/a0491009/quantization/edgeai-modeloptimization/torchmodelopt/edgeai_torchmodelopt/xmodelopt/quantization/v2/qconfig_types.py:51: UserWarning: could not find _get_default_qconfig_mapping_with_default_qconfig in torch.ao.quantization.qconfig_mapping\n",
      "  warnings.warn(\"could not find _get_default_qconfig_mapping_with_default_qconfig in torch.ao.quantization.qconfig_mapping\")\n"
     ]
    }
   ],
   "source": [
    "!pip install netron\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import edgeai_torchmodelopt\n",
    "import copy\n",
    "import netron\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape is : torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(weights='DEFAULT')\n",
    "example_input = torch.rand((1, 3, 224, 224))\n",
    "\n",
    "y = model(example_input)\n",
    "print(\"Output Shape is : {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './orig_simple_network_ptq.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_export_name = \"./orig_simple_network_ptq.onnx\"\n",
    "torch.onnx.export(model, example_input, model_export_name)\n",
    "netron.start(model_export_name, 8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be wrapping our model in the PTCFxModule which will be responsible for the calibration of the models and conversion to the final quantized network. It also enables bias calibration of the layers having a bias value, we can set a bias calibration factor (generally 0.01 works well) to enable it. Further, num_batch_norm_update_epochs and num_observer_update_epochs are used to define the epochs for which batch norm params and the observer are updated respectively. Each epoch is updated when a call to model.train() is done. Calibration is suggested to be performed in the training mode to utilise full functionality. Here, we do calibration for 3 epochs, and it can be done for very small examples from the distribution (generally 100 is good enough).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a0491009/quantization/edgeai-modeloptimization/torchmodelopt/edgeai_torchmodelopt/xmodelopt/quantization/v2/quant_fx_module.py:50: UserWarning: Fx based quantization wrapper will be depercated in the future after pt2e quantization wrapper is completed.\n",
      "  warnings.warn(\"Fx based quantization wrapper will be depercated in the future after pt2e quantization wrapper is completed.\")\n"
     ]
    }
   ],
   "source": [
    "model = edgeai_torchmodelopt.xmodelopt.quantization.v2.PTCFxModule(model, backend='qnnpack', bias_calibration_factor=0.01, num_batch_norm_update_epochs=1, num_observer_update_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Calibration Step for the network, where random data is used currently just for an example. **The data should be changed to your own dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing BN for subsequent epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing ranges for subsequent epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "num_calib_images = 10\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in tqdm(range(num_calib_images)):\n",
    "        output = model(torch.rand(1,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the quantized and calibrated 8-bit network now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTCFxModule(\n",
      "  (module): GraphModule(\n",
      "    (activation_post_process_0): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=1.8891554418587475e-06, max_val=0.9999961853027344)\n",
      "    )\n",
      "    (conv1): ConvReLU2d(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (activation_post_process_1): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0843]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=21.5041561126709)\n",
      "    )\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (activation_post_process_2): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0843]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=21.5041561126709)\n",
      "    )\n",
      "    (layer1): Module(\n",
      "      (0): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (downsample): Module(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (activation_post_process_3): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0424]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=10.819039344787598)\n",
      "    )\n",
      "    (activation_post_process_4): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=15.157670021057129)\n",
      "    )\n",
      "    (activation_post_process_5): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0849]), zero_point=tensor([105], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-8.898738861083984, max_val=12.750110626220703)\n",
      "    )\n",
      "    (activation_post_process_6): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951]), zero_point=tensor([151], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-14.393677711486816, max_val=9.866331100463867)\n",
      "    )\n",
      "    (activation_post_process_7): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0448]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=11.421177864074707)\n",
      "    )\n",
      "    (activation_post_process_8): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0351]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=8.949342727661133)\n",
      "    )\n",
      "    (activation_post_process_9): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0478]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=12.178536415100098)\n",
      "    )\n",
      "    (activation_post_process_10): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1416]), zero_point=tensor([204], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-28.909692764282227, max_val=7.205812454223633)\n",
      "    )\n",
      "    (activation_post_process_11): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0450]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=11.467218399047852)\n",
      "    )\n",
      "    (activation_post_process_12): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=6.039543151855469)\n",
      "    )\n",
      "    (activation_post_process_13): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0321]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=8.189811706542969)\n",
      "    )\n",
      "    (activation_post_process_14): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0946]), zero_point=tensor([156], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-14.790101051330566, max_val=9.333847999572754)\n",
      "    )\n",
      "    (activation_post_process_15): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0601]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=15.330673217773438)\n",
      "    )\n",
      "    (layer2): Module(\n",
      "      (0): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (downsample): Module(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (activation_post_process_16): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0352]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=8.9741792678833)\n",
      "    )\n",
      "    (activation_post_process_17): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=11.20594310760498)\n",
      "    )\n",
      "    (activation_post_process_18): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0765]), zero_point=tensor([142], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-10.886992454528809, max_val=8.620933532714844)\n",
      "    )\n",
      "    (activation_post_process_19): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0736]), zero_point=tensor([129], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-9.51778793334961, max_val=9.25764274597168)\n",
      "    )\n",
      "    (activation_post_process_20): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0437]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=11.147873878479004)\n",
      "    )\n",
      "    (activation_post_process_21): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=5.6915998458862305)\n",
      "    )\n",
      "    (activation_post_process_22): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0227]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=5.778574466705322)\n",
      "    )\n",
      "    (activation_post_process_23): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0758]), zero_point=tensor([171], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-12.9459867477417, max_val=6.382698059082031)\n",
      "    )\n",
      "    (activation_post_process_24): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0353]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.010711669921875)\n",
      "    )\n",
      "    (activation_post_process_25): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0197]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=5.028799057006836)\n",
      "    )\n",
      "    (activation_post_process_26): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0294]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=7.4887824058532715)\n",
      "    )\n",
      "    (activation_post_process_27): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0603]), zero_point=tensor([137], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-8.284296989440918, max_val=7.0965375900268555)\n",
      "    )\n",
      "    (activation_post_process_28): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0357]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.09596061706543)\n",
      "    )\n",
      "    (activation_post_process_29): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=5.005084991455078)\n",
      "    )\n",
      "    (activation_post_process_30): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0301]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=7.681105136871338)\n",
      "    )\n",
      "    (activation_post_process_31): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0553]), zero_point=tensor([146], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-8.054279327392578, max_val=6.041146278381348)\n",
      "    )\n",
      "    (activation_post_process_32): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0398]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=10.147117614746094)\n",
      "    )\n",
      "    (layer3): Module(\n",
      "      (0): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (downsample): Module(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (activation_post_process_33): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0247]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=6.303395748138428)\n",
      "    )\n",
      "    (activation_post_process_34): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0241]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=6.142226696014404)\n",
      "    )\n",
      "    (activation_post_process_35): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0704]), zero_point=tensor([128], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-9.01417350769043, max_val=8.928704261779785)\n",
      "    )\n",
      "    (activation_post_process_36): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0444]), zero_point=tensor([118], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-5.231438636779785, max_val=6.092741966247559)\n",
      "    )\n",
      "    (activation_post_process_37): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=10.911998748779297)\n",
      "    )\n",
      "    (activation_post_process_38): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0157]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=4.010509014129639)\n",
      "    )\n",
      "    (activation_post_process_39): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0169]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=4.310357570648193)\n",
      "    )\n",
      "    (activation_post_process_40): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0644]), zero_point=tensor([156], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-10.046839714050293, max_val=6.379068851470947)\n",
      "    )\n",
      "    (activation_post_process_41): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0392]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.989017486572266)\n",
      "    )\n",
      "    (activation_post_process_42): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=5.308193206787109)\n",
      "    )\n",
      "    (activation_post_process_43): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.39752197265625)\n",
      "    )\n",
      "    (activation_post_process_44): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0912]), zero_point=tensor([133], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-12.11152172088623, max_val=11.134442329406738)\n",
      "    )\n",
      "    (activation_post_process_45): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0673]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=17.15658950805664)\n",
      "    )\n",
      "    (activation_post_process_46): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1038]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=26.460182189941406)\n",
      "    )\n",
      "    (activation_post_process_47): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1187]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=30.268558502197266)\n",
      "    )\n",
      "    (activation_post_process_48): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3677]), zero_point=tensor([125], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-46.03974914550781, max_val=47.72956466674805)\n",
      "    )\n",
      "    (activation_post_process_49): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1873]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=47.770751953125)\n",
      "    )\n",
      "    (activation_post_process_50): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1684]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=42.933860778808594)\n",
      "    )\n",
      "    (activation_post_process_51): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0857]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=21.853893280029297)\n",
      "    )\n",
      "    (activation_post_process_52): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2448]), zero_point=tensor([163], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-39.890777587890625, max_val=22.521398544311523)\n",
      "    )\n",
      "    (activation_post_process_53): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1428]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=36.42178726196289)\n",
      "    )\n",
      "    (activation_post_process_54): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0607]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=15.472661018371582)\n",
      "    )\n",
      "    (activation_post_process_55): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0607]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=15.471393585205078)\n",
      "    )\n",
      "    (activation_post_process_56): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1292]), zero_point=tensor([194], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-25.022972106933594, max_val=7.927775859832764)\n",
      "    )\n",
      "    (activation_post_process_57): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1239]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=31.600114822387695)\n",
      "    )\n",
      "    (layer4): Module(\n",
      "      (0): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (downsample): Module(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Module(\n",
      "        (conv1): ConvReLU2d(\n",
      "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): ConvReLU2d(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (activation_post_process_58): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0378]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.643021583557129)\n",
      "    )\n",
      "    (activation_post_process_59): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0380]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=9.6787109375)\n",
      "    )\n",
      "    (activation_post_process_60): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1125]), zero_point=tensor([161], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-18.1208553314209, max_val=10.555363655090332)\n",
      "    )\n",
      "    (activation_post_process_61): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4417]), zero_point=tensor([81], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-35.584083557128906, max_val=77.05138397216797)\n",
      "    )\n",
      "    (activation_post_process_62): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3218]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=82.05553436279297)\n",
      "    )\n",
      "    (activation_post_process_63): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0985]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=25.128995895385742)\n",
      "    )\n",
      "    (activation_post_process_64): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0647]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=16.496028900146484)\n",
      "    )\n",
      "    (activation_post_process_65): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3042]), zero_point=tensor([108], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-33.00143051147461, max_val=44.56034851074219)\n",
      "    )\n",
      "    (activation_post_process_66): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2719]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=69.33836364746094)\n",
      "    )\n",
      "    (activation_post_process_67): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1052]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=26.829391479492188)\n",
      "    )\n",
      "    (activation_post_process_68): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0519]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=13.224039077758789)\n",
      "    )\n",
      "    (activation_post_process_69): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2260]), zero_point=tensor([235], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-53.02952575683594, max_val=4.588865756988525)\n",
      "    )\n",
      "    (activation_post_process_70): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=14.04137897491455)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (activation_post_process_71): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=14.04137897491455)\n",
      "    )\n",
      "    (activation_post_process_72): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551]), zero_point=tensor([0], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=0.0, max_val=14.04137897491455)\n",
      "    )\n",
      "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "    (activation_post_process_73): AdaptiveActivationFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([0], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0258]), zero_point=tensor([68], dtype=torch.int32)\n",
      "      (activation_post_process): CustomAdaptiveActivationObserverqscheme_torch_per_tensor_affine__range_shrink_percentile_0(min_val=-1.7499958276748657, max_val=4.834081649780273)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export_name = \"./converted_simple_network_ptq.onnx\"\n",
    "model.export(example_input, model_export_name)\n",
    "netron.start(model_export_name, 8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netron might show the quantized fused operators as separate because the fake-quantized (Q-DQ) models are exported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
